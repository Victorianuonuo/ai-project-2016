Speculative execution of information gathering 
plans can dramatically reduce the effect of 
source I/O latencies on overall performance. 
However, the utility of speculation is closely tied 
to how accurately data values are predicted at 
runtime. Caching is one approach that can be 
used to issue future predictions, but it scales 
poorly with large data sources and is unable to 
make intelligent predictions given previously 
unseen input data, even when there is an obvious 
relationship between past input and the output it 
generated. In this paper, we describe a novel 
way to combine classification and transduction 
for a more efficient and accurate value prediction 
strategy, one capable of issuing predictions 
about previously unseen hints. We show how 
our approach results in significant speedups for 
plans that query multiple sources or sources that 
require multi-page navigation. 

1 Introduction 

The performance of Web information gathering plans can 
suffer because of I/O latencies associated with the remote 
sources queried by these plans. A single slow Web 
source can bottleneck an entire plan and lead to poor 
execution time. When a plan requires multiple queries 
(either to the same source or to multiple sources), 
performance can be even worse, where the overhead is a 
function of the slowest sequence of sources queried. 

When multiple queries are required, speculative plan 
execution (Barish and Knoblock 2002b) can be used to 
dramatically reduce the impact of aggregate source latencies. 
The idea involves using data seen early in plan execution 
as a basis for issuing predictions about data likely 
to be needed during later parts of execution. This allows 
data dependency chains within the plan to be broken and 
parallelized, leading to significant speedups. 

To maximize the utility of speculative execution, a 
good value prediction strategy is necessary. The basic 
problem involves being able to use some hint h as the 
basis for issuing a predicted value v. One approach involves 
caching: we can note that particular hint hx corre


sponds to a particular value vv so that future receipt of hx 
can lead to prediction of vy. As a result, a plan that normally 
queries source S1 with hx and subsequently source 
Sz with vv can be parallelized so that both SI and S2 are 
queried in parallel, the latter speculatively. Unfortunately, 
caching has two major drawbacks. First, it does 
not scale well when the domain of hints is large. A second 
drawback is the inability to deal with novel (previously 
unseen) hints, even when an obvious relationship 
exists between hint and predicted value. 

In this paper, we present an alternative to caching that 
involves automatically learning predictors that combine 
classification and transduction in order to generate predictions 
from hints. Our approach succeeds where caching 
fails: the predictors learned usually consume less 
space than that demanded by caching and they are capable 
of making reasonable predictions when presented 
with novel hints, the latter leading to better speedups. 
Specifically, our work contributes the following: 

. An algorithm that learns efficient transducers capable 
of a variety of string transformations. 
. An algorithm that combines classification and transduction 
to learn value predictors 
The rest of this paper is organized as follows. The next 
section briefly reviews information gathering and provides 
a motivating example for speculative execution. In 
Section 3, we describe how classification and transduction 
can be used to build efficient and intelligent predictors. 
Section 4 describes our learning algorithms that 
combine both techniques. Section 5 describes experimental 
results of using our approach. Finally, Section 6 details 
the related work. 

2 Preliminaries 

Web information gathering plans retrieve, combine, and 
manipulate data located in remote Web sources. Such 
plans consist of a partially-ordered graph of operators 
O1.Ott connected in a producer/consumer fashion. Each 
operator O, consumes a set of inputs a1.ap, fetches data 
or performs a computation based on that input, and produces 
one or more outputs b1.bq The types of operators 
used in information gathering plans vary, but most either 
retrieve or perform computations on data. 

Al AND DATA INTEGRATION 


To better illustrate a Web information gathering plan, 
we consider the example plan Carlnfo, shown in Figure 

1. Given constraints on car type and pricing, Carlnfo locates 
the make and model which has a median price closest 
to that specified and then retrieves the full review of 
that car from the Web site ConsumerGuide.com. The 
plan consists of four Wrapper operators that fetch and 
extract data from various parts of the remote source. 
Specifically, the plan involves: (a) querying CarsDirect.
com for the car make and model having a median 
price closest to that specified, (b) querying Consumer-
Guide for the resulting make and model, (c) retrieving 
the link to the summary page for that car (using the link 
provided in the search results), and (d) retrieving the full 
review using the link provided on the summary page. 
Figure 1: The Carlnfo plan 

For example, for the input (Sedan, $19000), the car returned 
is (Honda Accord), the summary URL for this car 
is (http://cg.com/summ/2289.htm) and the full review 
URL (http://cg.com/full/2289.htm).l Once at the full review 
URL, the review text can be extracted. 

Existing research on information agent executors 
(Barish and Knoblock 2002a) and network query engines 
(Hellerstein et al. 2000, Naughton et al 2001, Ives et al 
2002) has suggested a number of techniques for the efficient 
execution of information gathering plans. Most of 
these systems work on the principle of streaming data-
flow, which executes plan operators as fast as their data 
dependencies allow and supports the pipelining of data 
between operators. Despite the benefits of streaming 
dataflow, plan execution can still be slow if there are 
many binding pattern relationships between sources in 
the plan. For example, notice that since steps (b), (c), 
and (d) are dependent on the steps that precede them, the 
plan in Figure 1 must be executed sequentially and profits 
little from the benefit of streaming dataflow. Thus, if 
each source in Figure 1 has an average latency of 2s, than 
the average plan execution time is the summation of 
source latencies, or (4*2s =) 8s. 

2.1 Speculative Plan Execution 
Speculative execution is one technique that can be used 
to overcome the effects of aggregate latencies in information 
gathering plans containing multiple binding patterns. 
As described in (Barish and Knoblock 2002b), a plan 
is transformed into one capable of speculative plan execution 
by the insertion of two additional operators Speculate 
and Confirm - at various points the plan, 

1 For the sake of brevity, we have abbreviated the URLs asso


ciated with ConsumerGuide.com (although we retain the key 

aspects of its structure). 


Figure 2: Carlnfo transformed for speculative execution 
based on an iterative analysis of the most expensive path 
to execute within that plan. For example, one possible 
result of transforming the plan in Figure 1 for speculative 
execution is shown in Figure 2. 
As shown, the Speculate operator receives copies of 
data sent to operators executing earlier in the plan. 
Based on the hints it receives, Speculate can generate 
predicted values for later operators that can be transmitted 
immediately to those operators. Thus, the earlier and 
later parts of the plan can be parallelized. After the earlier 
operators finish executing, Speculate can assess 
whether or not its initial predictions were correct and 
forward the results onto a Confirm operator, which ensures 
that speculative data does not prematurely exit the 
plan or cause some other irreversible action to occur. 
Finally, notice that Figure 2 shows that speculation can 
be cascading: speculation about one operator can drive 
the speculation of another operator, leading to greater 
degrees of parallelism and thus arbitrary speedups. 
As a result of the transformation shown in Figure 2, 
execution would then proceed as follows. Input data, 
such as (Sedan, $19000), would result in the retrieval of 
the initial search results in parallel with the predicted 
make and model - which would drive the predictions of 
summary and full-review URLs so that all four retrievals 
(three speculative) were executed at once. If all predictions 
are correct, the resulting execution time can be reduced 
to only 2s plus the overhead to speculate, a maximum 
speedup of about 4. However, the average speedup 
depends on the average accuracy of prediction: the 
greater this accuracy, the higher the average speedup. 

3 Value Prediction Strategies 

Caching can be used to implement value prediction when 
speculatively executing plans such as Carlnfo. Unfortunately, 
caching does not allow predictions to be issued 
for unseen hints. As a result, the average accuracy of 
prediction can be low when the domain of possible hints 
is large. Further, trying to achieve better accuracy under 
these conditions can require significant amounts of memory. 
In this section, we describe how a hybrid approach, 
consisting of classification and transduction, results in a 
more intelligent and space-efficient prediction strategy. 

3.1 Classification 
Classification involves extracting knowledge from a set 
of data (instances) that describes how the attributes of 
those instances are associated with a set of target classes. 
Given a set of instances, classification rules can be 
learned so that future instances can be classified cor-

Al AND DATA INTEGRATION 


rectly. Once learned, a classifier can also make reasonable 
predictions about new instances - a combination of 
attribute values that had not previously been seen. The 
ability for classification to accommodate new instances is 
intriguing for the speculative execution of information 
gathering plans because, unlike with caching, it is possible 
to make predictions about novel hints. 

For example, consider the prediction of the make and 
model of a car in the Carlnfo plan. It turns out that 
CarsDirect.com returns the same answer {Honda Accord) 
for "Sedan" as it does for other types (such as "All and 
"Coupe") in the same price range. The association of the 
same make and model to multiple criteria combinations 
occurs somewhat frequently on CarsDirect.com. 

To see why classification is a more effective technique 
than caching for the prediction of the make and model, 
consider prediction of a car based on type and price: 

type Price Car 
Sedan 18000 Saturn S Series 
Sedan 19000 Honda Accord 
Sedan 20000 VW Beetle 
Coupe 18000 Saturn S Series] 
Coupe 19000 Honda Accord 
Coupe 20000 VW Beetle 

The data above is what a cache would contain. In contrast, 
a classifier like ld3 (Quinlan 1986) would induce 
the following decision tree: 

pri <= 18000 : Saturn S Series (2.0) 


pri > 18000 : 


I pri <= 19000 : Honda Accord (2.0) 

I pri > 19000 : VW Beetle (2.0) 


When presented with an instance previously seen, such 
as (Sedan, 19000), both the cache and the classifier 
would result in the same prediction: (Honda Accord). 
However, when presented with a new instance, such as 
(Coupe, 18500), the cache would be unable to make a 
prediction. In contrast, the classifier would issue the correct 
prediction of (Honda Accord). Any errors made by 
classification would be caught automatically later in execution 
by the Confirm operator. 

The decision tree above is also more space efficient 
than a cache for the same data. The cache requires storing 
8 unique values. The decision tree above requires 
only storing 5 values (just those shown) plus the information 
required to describe tree structure and attribute value 
conditions (i.e.,pri <= 18000). 

In short, classifiers such as decision trees can function 
as better, more space-efficient predictors. And in the 
worst case, where each instance corresponds to a unique 
class, a classifier simply emulates a cache. 

finite sets corresponding to input and output alphabets, 8 
is the state-transition function that maps Q x I to Q, and 
a is the output function that maps Q x A to A*. We are 
interested in a particular type of sequential transducer 
called a P-subsequential transducer that allows at most p 
output symbols to be appended to the output (i.e., exist 
on the final state transition arc). 

Value prediction by transduction makes sense for Web 
information gathering plans primarily because of how 
Web sources organize information and how Web requests 
(i.e., HTTP queries) are standardized. In the case of the 
former, Web sources often use predictable hierarchies to 
catalog information. For example, in the Carlnfo example, 
the summary URL for the 1999 Honda Accord was 
http://cg.com/summ/2289.htm and the full review was 
http://cg.com/full/2289.htm. Notice that both URLs use 
the same piece of dynamic information (2289), but in 
different ways. By learning this transduction, we can then 
predict future full review URLs for corresponding summary 
URLs we have never previously seen. Transducers 
can also allow us to predict HTTP queries. For example, 
an HTTP GET query for the IBM stock chart is 
http.//finance.yahoo. com/q?s-ibm&d=c. By exploiting 
the regularity of this URL structure, the system can predict 
the URL for the Cisco Systems (CSCO) chart. 

In this paper, we define two new types of transducers 
that extend the traditional definition of P-subsequential 
transducers. The first is a high-level transducer, called a 
value transducer that describes how to construct the predicted 
value based on the regularity and transformations 
observed in a set of examples of past hints and values. 
Value transducers build the predicted value through substring-
level operations {Insert, Classify, and Transduce]. 
Insert constructs the static parts of predicted values. 
Classify categorizes hint information into part of a predicted 
value. Finally, Transduce transforms hint information 
into part of a predicted value. Transduce uses a 
second type of special transducer, a hint transducer, in 
which the operations {Accept, Copy, Replace, Upper, 
Lower] all function on individual characters of the hint 
and perform the same transformation as their name implies, 
with respect to the predicted value. 

To illustrate, consider the transducers shown in Figure 
3, for predicting the full-review URL in the Carlnfo example. 
Figure 3 shows the value transducer performs 
high-level operations - the insertion of substrings and the 
call to a lower-level transduction. The second transducer 
(in abbreviated form) uses the Accept and Copy opera-


Al AND DATA INTEGRATION 


tions to transform the part of the hint value into its proper 

point in the predicted value. Thus, the first step builds 

the "http://cg.com/fulir part, the second step copies the 

"2289" part and the third step appends the ".htm" part. 

In short, transducers lend themselves to value predictio 
n because of the way information is stored by and que


ried from Web sources. They are a natural fit because 

URLs are strings that are often the result of simple trans


formations on earlier input. Transducers are also space 

efficient: once learned, they occupy a finite size and can 

be applied to an endless number of inputs. Overall, for 

Web content that cannot be queried directly (instead re


quiring an initial query and then further navigation), 

transducers serve as compact predictors that capitalize on 

the regularity of Web queries and source structure. 

4 
A Unifying Learning Algorithm 

In this section, we present algorithms that induce value 
transducers (VTs), which combine classification and 
transduction, as predictors for the speculative execution 
of information gathering plans. The predictors learned 
represent not only a hybridization of classification and 
transduction, but also of caching, since classifiers default 
to caches when there is no feature of the key that is a 
particularly good indicator of the resulting value. 

To learn a VT, the general approach consists of: 

1. 
For each attribute of the answer tuple, identify an 
SD Template that distinguishes static from dynamic 
parts of the target string by analyzing the regularity 
between values of this attribute for all answers. 
2. 
For each static part, add an Insert arc to the VT. 
3. 
For each dynamic part, determine if transduction 
can be used; if so, add a Transduce arc to VT. 
4. 
If no transducer can be found, classify the dynamic 
part based on the relevant attributes of the hint and 
add a Classify arc to the VT. 
We implemented this in the algorithm LKARN-VALUETRANSDUCER, 
shown below. The algorithm takes a set of 
hints, a set of corresponding answers, and returns a VT 
that fits the data: 

1 Function LEARN-VALUE-TRANSDUCER 

returns ValueTransducer 
2 Input: the set of hints H, the set of answers A 
3 VT<-\A6\C8 

4 tmpl <- LEARN-SD-TEMPLATE (A)\
5 Foreach element e in tmpl6 If e is a static element 
7 Add Insert (e.value) arc to VT 
8 Else if e is a dynamic element 
9 DA <- the set of dynamic strings in A for this tmpl element 
10 HT<- LEARN-HINT-TRANSDUCER (//, DA)

11 If HT !=0 

12 Add Transduce (HT) arc to VT 

13 else 

14 C <- LEARN-CLASSIFIER (//, DA)
15 Add Classify (C) arc to VT 


16 Return VT 
17 End /* LEARN-VALUE-TRANSDUCER */ 


In this algorithm, learning a classifier can be achieved by 
decision tree induction algorithms such as ld3 (Quinlan 
1986). Learning the SD template and the hint transforming 
transducer, however, require unique algorithms. 

4.1 Learning templates of string sets 
Learning a VT requires first identifying a template for 
the target value that describes what parts are static and 
what parts are dynamic. Next, each static part is replaced 
with Insert operations and each dynamic part becomes a 
candidate for either transduction or classification. 

To identify an SD template, we use an approach based 
on the longest common subsequence (LCS) between a set 
of values. First, an LCS identification algorithm similar 
to the one described by (Hirschberg 1975) is applied to 
the set of answer values. We then iterate through the 
LCS on each answer value to determine the set of possible 
static/dynamic templates that fit that answer. Only 
those templates common to all are kept - from this, one 
of the set is returned (though all are valid). The algorithm 
that implements this, LEARN-SD-TEMPLATE, is 
shown below. 

1 Function LEARN-SD-TEMPLATE returns Template

2 Input: set of strings S 

3 tmpl <- 0 

4 les <- GET-LCS(S) 

5 If seq !=\A6\C8 

6 tmplSet <- 0 

7 Foreach string s in S 

8 curTmplSet <- EXTRACT-TEMPLATES (S, les)

9 tmplSet <-tmplSet n curTmplSet

10 If tmplSet !=0 

11 tmpl <\A1\AA choose any member of tmplSet /* all are valid */ 

12 Return tmpl 

13 
End /* LEARN-SD-TEMPLATE */ 

4.2 Learning hint transducers 
To learn a hint transducer (HT), we also make use of SD-
template identification. However, instead of identifying 
a template that fits all answers, we identify templates that 
fit all hints. Based on this template, we then construct an 
HT that accepts the static parts of the hint string and performs 
the character-level transformation on the dynamic 
parts. A sketch of the algorithm that implements this, 
LEARN-HINT-TRANSDUCKR, is shown below. 

1 Function LEARN-HINT-TRANSDUCER returns HintTransducer 
2 Input: the set of hints H, the set of resulting strings S 
3 Use LCS to identify static parts between all H 
4 Foreach H,S pair (h, s) 
5 h'<- extraction of h replacing static chars with token 'A' 
6 A <- Align (h' s) based on lowercased string edit distance 
7 Annotate A with character level operations 
8 End 
9 RE <- Build a reg expr that fits all annotations (using LCS) 
10 If RE !=0 
11 Return general predictive transducer based on RE 

that accepts static sequences of H where necessary 

and transduces dynamic sequences. 
12 Else 
13 Return 0 
14 End /* LEARN-HINT-TRANSDUCER */ 

Al AND DATA INTEGRATION 


For example, suppose prior hints {Dr. Joe Smith, Dr. Jane 
Thomas) corresponded to values {joe_s,jane_t). The algorithm 
would first identify the static part of the hints and rewrite 
the hints using the Accept operation, i.e., {AAAAJoe 
Smith, AAAAJane Thomas} where A refers to the operation 
Accept. It would then align each hint and value based on 
string edit distance and annotate with character operations 
required for transformation to the observed values, resulting 
in {AAAAALCCRLDDDD, AAAAALCCCRLDDDDD}. 
Next, it would use the LCS to build the regular expression 
{A*LC*RLD*} fitting these examples and, from this, the 
HT (partial form shown) in Figure 4: 


Figure 4: Partial form of the HT for the names example 

5 Experimental Results 

To evaluate our approach to value prediction, we compared 
it with caching on three sample plans that can 
benefit from speculative execution. These plans were 
chosen because all query multiple sources and/or multiple 
times within a source in order to retrieve information 
that is not possible to query directly. These plans normally 
require sequential execution; however, with speculative 
execution, significant speedup is possible. 

These plans included Carlnfo (which we have already 
described), Replnfo, and Phone Info. Replnfo, based on 
the plan described in (Barish and Knoblock 2002b), queries 
the site Vote-Smart.org for the issue positions of 

U.S. federal representatives for a particular nine-digit zip 
code. Its plan involves three queries: one for the list of 
representatives in the desired zip code, navigation to the 
profile page for each member, and navigation to their 
corresponding issue positions page. Phonelnfo is a similar 
plan that takes a U.S. phone number, does a reverse 
lookup of that number (on SuperPages.com) to find the 
state of origin and then queries the US Census (QuickFacts.
census.gov) about demographics for that state. 
When querying the Census, additional navigation is required 
to get from the initial summary page about the 
state to the corresponding demographics details page. 
All three were modified for speculative execution, with 
results similar to that shown in Figure 2. We then 
learned predictors for each, based on the data observed 
during execution. The learning was done offline, after 
each execution, so that future executions could benefit 
from the resulting predictors. The Carlnfo predictors, as 
described, involved classification (make/model/year to 
car summary page) and transduction (summary to full 
review page). Replnfo required classification (nine-digit 
zip to political district page) and transduction (district to 
issue positions page). Finally, Phonelnfo required classi


fication (phone number to state) and transduction (initial 
state facts page to detailed demographics page). 

When learning the predictors, instances were drawn 
from typical distributions for that domain; for example, 
instances for Replnfo were drawn from a list of addresses 
of individuals that contributed to presidential campaigns 
(obtained from the FEC) - a distribution that closely approximates 
the U.S. geographic population distribution. 
Similarly, the phone numbers used in Phonelnfo came 
from a distribution of numbers for common last names. 

We implemented speculative execution in Theseus, a 
streaming dataflow system for Web information gathering. 
Since our tests involved thousands of requests, we 
ran our plans using Theseus on local copies of the relevant 
data and simulated network latencies during retrieval. 
In doing so, we assumed each source had a latency 
of 2 seconds (note that the particular latency chosen 
does not matter - speedups will be the same). 

Our results focus on three measurements. The first, 
shown in Table 1, show the average number of examples 
required in order to learn the correct transducer required 
by each of the plans. Notice, that Replnfo required more 
than Phonelnfo or Carlnfo because there was a higher 
likelihood of the examples sharing some part of their dynamic 
data in common - and this was extracted by the 
LCS-based algorithm as a static element. 


Table 1: Learning the correct transducer 

The second set of results, shown in Figure 5, focuses 
on the accuracy of classification, as measured over a 10fold 
cross-validation sample of the data (where no data in 
the test fold was in any of the training folds). As expected, 
domains with larger sets of discrete target classes 
(such as Replnfo) required many more examples than 
those with smaller numbers of classes (like Phonelnfo). 

The final set of results show how the learning we have 
described resulted in better average plan execution 


Al AND DATA INTEGRATION 


speedups than did caching. Due to space constraints, we 
show results from only one of the plans -Carlnfo - in 
Figure 6. This figure shows that, while the benefit of 
caching degrades significantly as the composition of future 
requests contains a greater number of unseen examples, 
the learning allows accurate predictions to be made 

- even for a mix containing entirely new requests. Furthermore, 
as the number of examples increases, speedup 
from learning increases and the accuracy of speculation 
(even on 100% unseen instances) gradually increases. 
Though omitted here, these same general trends hold for 
both Rep Info and Phonelnfo as well (although to different 
degrees). 
6 Related Work 

Learning to speculatively execute programs has been 
well-studied in computer science. Historically, however, 
computer architecture research has focused mostly on 
predicting control (e.g., branch prediction), not data. 
While hardware-level value prediction has recently become 
an active area of research, the type of data being 
predicted (e.g., memory locations) is different. 

At a high-level, this work could be considered a form 
of speedup learning. In speedup learning, the goal is to 
improve problem solving performance through experience. 
Past research has focused on a number of areas, 
including learning sequences of operators (Fikes et al. 
1972) and learning control knowledge to aid in choosing 
what operators to execute next (Minton 1988). Although 
the use of learning in this work is for the purpose of 
value prediction (i.e., how to execute), the goal of using 
learned knowledge to improve performance is the same. 

There has not been any previous work on value prediction 
for information gathering systems. (Hull et al. 2000) 
proposed speculation in a decision flow framework, but 
one in which only control predictions were necessary. 
There has also been past work on information gathering 
with partial results (Hellerstein et al 1997; Shanmugasundaram 
et al. 2000), but these systems do not predict 
data values and instead use approximate values from intermediate 
aggregate operators in order to obtain approximate 
final results. 

Surprisingly, there has been little work on the learning 
of subsequential transducers. One existing algorithm is 


OSTIA (Oncina et al. 1993), which is able to induce traditional 
subsequential transducers capable of automating 
translations of decimal to Roman numbers or English 
word spellings of numbers to their decimal equivalents. 
Our work differs from OST1A mainly in that the transducers 
we learn capture the general process of a regular 
class of string transformations. After learning from only 
a few examples, our algorithm can achieve a high-degree 
of accuracy on such classes. In contrast, while OSTIA 
can learn more complex types of subsequential transducers, 
it can require a very large number of examples before 
it can learn the proper rule (Gildea and Jurafsky 1996). 

The transducer learning algorithm suggested by (Hsu 
and Chang 1999) viewed transduction as a means for information 
extraction. Our use is similar in that one part 
of our approach involves extracting dynamic values from 
hints. However, the transducers we describe go beyond 
extraction - they transform the source string so that it 
can be integrated into a predicted value. In doing so, our 
transduction process also makes use of classification. 

Finally, while our use of classification applies to predicting 
any type of data value in an information gathering 
plan, our typical use of transduction is for the prediction 
of URLs. Other approaches have explored point-based 
(Zukerman et al. 1999) or path-based (Su et al. 2000) 
methods of URL prediction, attempting to understand 
request models based on either time, the order of requests, 
or the associations between requests. However, 
unlike our approach, these techniques do not try to understand 
very general patterns in request content and 
thus cannot predict URLs that have not been previously 
requested. 

7 Conclusions 

Successful speculative execution of information gathering 
plans is fundamentally linked with the ability to make 
good predictions. In this paper, we have described how a 
hybrid approach based on two simple techniques - classification 
and transduction - can be combined and applied 
to the problem. The approach we describe represents a 
hybridization of not only classification and transduction, 
but also of caching, since classifiers effectively function 
as caches when no classification is possible. 

Our experimental results show that learning such predictors 
can lead to significant speedups when gathering 
information that must be queried indirectly. We believe 
that a bright future exists for data value prediction at the 
information gathering level, primarily because of the potential 
speedup enabled by speculative execution and 
because of the availability of resources (i.e., memory) 
that exist at higher levels of execution, enabling more 
sophisticated machine learning techniques to be applied. 

8 Acknowledgements 

We wish to thank Steven Minton and Cenk Gazen for 
very helpful and inspiring discussions related to techniques 
for template induction. 

Al AND DATA INTEGRATION 


This material is based upon work supported in part by 
the Defense Advanced Research Projects Agency 
(DARPA) and Air Force Research Laboratory under contract/
agreement numbers F30602-01-C-0197 and F3060200-
1-0504, in part by the Air Force Office of Scientific 
Research under grant numbers F49620-01-1-0053 and 
F49620-02-1-0270, in part by the United States Air Force 
under contract number F49620-02-C-0103, in part by 
gifts from the Intel and Microsoft Corporations. The 
U.S.Government is authorized to reproduce and distribute 
reports for Governmental purposes notwithstanding any 
copy right annotation thereon. The views and conclusions 
contained herein are those of the authors and should 
not be interpreted as necessarily representing the official 
policies or endorsements, either expressed or implied, of 
any of the above organizations or any person connected 
with them. 


Abstract 

We solve the problem of obtaining answers to 
queries posed to a mediated integration system under 
the local-as-view paradigm that are consistent 
wrt to certain global integrity constraints. For this, 
the query program is combined with logic programming 
specifications under the stable model semantics 
of the class of minimal global instances, and of 
the class of their repairs. 

 Introduction 
For several reasons a database may become inconsistent wrt 
certain integrity constraints (ICs). In such a situation, possibly 
most of the data is still consistent. In [Arenas et al., 1999] 
consistent data in a single relational database is characterized 
as the data that is invariant under all minimal restorations of 
consistency, i.e. true in all repaired versions of the original 
instance (the repairs). In that paper and others [Arenas et al, 
2000; Greco etal, 2001], some mechanisms have been developed 
for retrieving consistent answer when queries are posed 
to such an inconsistent database. 
When independent data sources are integrated, inconsistencies 
wrt to global ICs are likely to occur, specially when the 
sources are virtually integrated by means of a mediator, because 
the data sources are kept completely independent. The 
mediator provides a global schema as an interface, and is responsible 
for generating query plans to answer global queries 
by retrieving data sets from the sources and combining them 
into a final answer set for the user. 
"Local-as-view" (LAV) is a common paradigm for data integration 
that describes each data source as a set of views over 
the global schema. Another one, "global-as-view" (GAV), 
defines every global relation as a view of the set of relations 
in the sources ([Lenzerini, 2002] is a good survey). Query 
answering is harder in LAV [Abiteboul et al., 1998]. On the 
other side, the LAV approach allows more flexibility when 
new sources are integrated into an existing system. However, 
the flexibility to add new sources, without having to consider 
the other sources in the system, makes inconsistencies wrt 
global ICs more likely. 

Example 1 Consider the LAV based global integration system 
G1 with a global relation R(X, Y) and two source relations 
v1 = {Vi(a,6), V1(c,d)} and v2 = {V2{a,c),V2{d,e)} 

 Leopoldo Bertossi 
Carleton University 
School of Computer Science 
Ottawa, Canada 
bertossi .scs.carleton.ca 


that are described by the view definitions V\ (X, Y) <\A1\AA R(X, 
Y); V2{X,Y) -R{X,Y). The global functional dependency 
(FD) R: X \A1\AA> Y is violated through the pair of tuples 
{(a,b),(a,c)}. \A1\F5 

In a virtual integration system, the mediator should solve potential 
inconsistencies when the query plan is generated. In 
[Bertossi et a/., 2002], under the LAV approach, a methodology 
for generating query plans to compute answers to limited 
forms of queries that are consistent wrt an also restricted class 
of universal ICs was presented. The limitation comes from a 
first step where a query transformation for consistent query 
answering (CQA) is performed [Arenas et al, 1999]. Next, 
query plans are generated for the transformed query. However, 
[Bertossi et al, 2002] provides the right semantics for 
CQA in mediated integrated systems (see Section 2). 

Example 2 (example I continued) If we pose to the global 
system the query Q : Ans(X,Y) <\A1\AA R(X,Y), we obtain 
the answers {Ans(a,b),Ans(c,d),Ans(a, c),Ans(d,c)}. 
However, only the tuples Ans(c, d), Ans(d, e) should be returned 
as consistent answers wrt the FD R: X \A1\AA> Y. D 

In this paper, under the LAV approach and assuming that 
sources arc open (or incomplete) [Abiteboul et al, 1998], we 
solve the problem of retrieving consistent answers to global 
queries. We consider arbitrary universal ICs and referential 
ICs, in consequence all the ICs that are used in database 
praxis [Abiteboul et al, 1995]. View definitions are conjunctive 
queries, and global queries are expressed in Datalog and 
its extensions with negation. The methodology can be summarized 
as follows. First, in Section 3, the minimal legal 
global instances of a mediated system are specified by means 
of logic programs with a stable model, or answer sets, semantics. 
Next, in Section 4, the repairs of the minimal global instances 
are specified as the stable models of disjunctive logic 
programs. Those programs contain annotation constants, like 
those used to specify repairs of single relational databases for 
CQA [Barcelo et ai, 2002]. Finally, in Section 5, consistent 
answers to queries are obtained by running a query program 
in combination with the previous two specification programs. 

2 Preliminaries 

2.1 Global schemas and view definitions 
A global schema R is modeled by a finite set of relations 
{R1,R2, ...,Rn} over a fixed domain U. With these relation 


Specification of Repairs of a Global System 

In [Barcelo etal, 2002] repairs of single relational databases 
are specified using disjunctive logic programs with stable 
model semantics. The approach works for arbitrary universal 
and referential ICs in the sense that the repairs of the database 
correspond to the stable models of the program. We briefly 
explain these programs, because they will be used to specify 
repairs of integration systems. 

First, the database predicates in the program are expanded 
with an extra argument to be filled with one of a set of new annotation 
constants. An atom in (outside) the original database 
is annotated with ta (fd). Annotations ta and fa are considered 
advisory values, to solve conflicts between the database 
and the ICs. If an atom gets the derived annotation fa, it 
means an advise to make it false, i.e. to delete it from the 
database. Similarly, an atom that gets the annotation ta must 
be inserted into the database. 

Example 11 Consider the integrity constraint I \A1\AA> 
R(x)), and the inconsistent database instance r = {P(a)}. 
The logic program should have the effect of repairing the 
database. Single, local repair steps are obtained by deriving 
the annotations ta or fa. This is done when each IC is 
considered in isolation, but there may be interacting ICs, and 
the repair process may take several steps and should stabilize 
at some point. In order to achieve this, we use annotations t*, 
f *. The latter, for example, groups together the annotations 
fd and fa for the same atom (rules 1 and 4. below). These 
derived annotations are used to give a feedback to the bodies 
of the rules that produce the local, single repair steps, so that 

a propagation of changes is triggered (rule 2. below). The 
annotations t** and f** are just used to read off the literals 
that are inside (resp. outside) a repair. This is achieved by 
means of rules 6. below, that are used to interpret the models 
as database repairs. The following is the program: 

Only rules 2. depend on the ICs. They say how to repair them 
when violations are found. Rules 3. contain the database 
atoms. Rules 4. capture the CWA. Rules 5. are denial program 
constraints to discard models that contain an atom annotated 
with both ta and fa. The program has two stable 

The next definition combines into one program the specification 
of the minimal legal instances and their repairs. 

where X is the tuple of all variables appearing in database 
atoms in the rule. 

Rules 6. repair referential ICs by deletion of tuples or insertion 
of null values that are not propagated through other ICs 
(.Barcelo et al, 2003]. For this purpose, we consider that the 

Definition 11 The instance associated to a choice model M. 

 Consistent Answers 

ements of the global schema R. Each positive occurrence of 
those predicates, say P(t), is replaced by P(t, t**); and each 
negative occurrence, say not P(t), by P(i, f**). This program 
has a query predicate Ans that collects the answers to 

Q. In particular, first order queries can be expressed as strati-
then to the certain answers if Q is monotone. If we are interested 
in just the minimal answers to Q, without considering 
consistency issues, then they can be computed as above, but 

6 Conclusions 

We have presented the most general approach so far to specifying, 
by means of disjunctive logic programs with a stable 
model semantics, the database repairs of a mediated integration 
system with open sources under the LAV approach. 
Then, consistent answers to queries posed to such a system 
are computed by running a query program together with the 
specification of database repairs under the skeptical or cautious 
stable model semantics. The specification of the repairs 
is achieved by first specifying the class of minimal global legal 
instances of the integration system. To the best of our 
knowledge, this is also the first specification, under the LAV 
paradigm, of such global instances in a logic programming 
formalism. This specification is inspired by the inverse rules 
algorithms, where auxiliary functions are replaced by auxiliary 
predicates that are forced to be functional by means of 
the non deterministic choice operator. 

The methodology works for conjunctive view definitions, 
but can be extended to disjunctive views using the corresponding 
extension of the inverse rules algorithm [Duschka, 
1997]. Wrt the ICs and queries it can handle, the approach 
works for arbitrary universal and referential integrity constrains 
and queries expressed as Datalognot programs. In 
consequence, the current approach to consistent query answering 
(CQA) subsumes and extends the methodologies presented 
in [Bertossi et al, 2002] for integration systems, and 
the one in [Barcelo et al., 2002] for stand alone relational 
databases. 

For reasons of space, we just mention a few optimizations 
of the specification programs and their execution. The 
materialization of the CWA present can be avoided by program transformation. Classes of common ICs can be identified for which becomes head-cycle-free, and in consequence, can be transformed into a non disjunctive program [Ben-Eliyahu et al., 1994; 
Barcelo et al., 2003]. The program for CQA can be split [Lifschitz et al., 1994] into the program that specifies minimal 
legal instances, the program that specifies their repairs and 
the query program. The first is non stratified, but its models 
can be computed bottom-up as fixpoints of an iterative operator 
[Giannotti et al., 2001]; and the second one is locally 
stratified [Przymusinski, 1991]. Finally, if the query program 
is stratified, e.g. if the original query is first-order, then the 
consistent answers can be eventually computed by a bottom-
up evaluation mechanism. 

For CQA we have successfully experimented with DLV 
[Eiter et al., 2000]. The current implementations of the disjunctive 
stable models semantics would be much more effective 
in database applications if it were possible to evaluate 
open queries in a form that is guided by the query rather than 
based on, first, massive grounding of the whole program and, 
second, considering what can be found in every (completely 
constructed) stable model of the program. 


Wrt related papers, query answering in mediated integration 
systems under the assumption that certain global ICs 
hold has been treated in [Gryz, 1999; Duschka et al, 2000; 
Grant et al, 2002; Cali et al, 2002]. However, in CQA, 
we do not assume that global ICs hold. Logic programming 
specifications of repairs of single relational databases have 
been presented in [Arenas et al, 2000; Greco et al, 2001; 
Barcelo et al, 2002]. 

[Lembo et al, 2002] considers integration systems under 
the GAV approach that do not satisfy global key dependencies. 
There, legal instances are allowed to be more flexible, 
allowing their computed views to accommodate the satisfaction 
of the ICs. In this sense, the notion of repair is implicit; 
and the legal instances are the repairs we have considered 
here. View definitions are expressed as Datalog queries; and 
the queries to the global system are conjunctive. The "repairs" 
of the global system are specified by normal programs 
under stable model semantics. 

In [Bertossi et al, 2002], CQA in possibly inconsistent 
integration systems under the LAV approach is considered. 
There, the notion of repair of a minimal legal instance is introduced. 
The algorithm for CQA is based on a query transformation 
mechanism I Arenas et al, 1999] applied to first-
order queries. The resulting query may contain negation, and 
is run on top of an extension of the inverse algorithm to the 
case of stratified Datalognot queries. This approach is limited 
by the restrictions of the query transformation methodology. 


Abstract 

In this paper we address the problem of query answering 
and rewriting in global-as-view data integration 
systems, when key and inclusion dependencies 
are expressed on the global integration schema. 
In the case of sound views, we provide sound and 
complete rewriting techniques for a maximal class 
of constraints for which decidability holds. Then, 
we introduce a semantics which is able to cope with 
violations of constraints, and present a sound and 
complete rewriting technique for the same dec id-
able class of constraints. Finally, we consider the 
decision problem of query answering and give decidability 
and complexity results. 

1 Introduction 

The task of a data integration system is to combine data residing 
at different sources, providing the user with a unified view 
of them, called global schema. User queries are formulated 
over the global schema, and the system suitably queries the 
sources, providing an answer to the user, who is not obliged to 
have any information about the sources. The problem of data 
integration is a crucial issue in many application domains, 
e.g., re-engineering legacy systems, data warehousing, data 
mining, data exchange. 

A central aspect of query processing is the specification of 
the relationship between the global schema and the sources; 
such a specification is given in the form of a so-called mapping. 
There are basically two approaches for specifying the 
mapping. The first approach, called global-as-view (GAV), 
requires that a view over the sources is associated with every 
element of the global schema. Conversely, the second 
approach, called local-as-view (LAV), requires the sources to 
be defined as views over the global schema [Lenzerini, 2002; 
Duschka and Levy, 1997]. 

The global schema is a representation of the domain of interest 
of the data integration system: integrity constraints are 
expressed on such a schema to enhance its expressiveness, 
thus improving its capability of representing the real world. 

Since sources are in general autonomous, the data provided 
by the sources are likely not to satisfy the constraints 
on the global schema. Integrity constraints have to be taken 
into account during query processing; otherwise, the system 

may return incorrect answers to the user iFagin et al., 2003; 

Call et al, 2002]. 

Another significant issue is that the sources may not provide 
exactly the data that satisfy the corresponding portion of 
the global schema; in particular, they may provide either a 
subset or a superset of the data satisfying the mentioned portion, 
and the mapping is to be considered sound or completerespectively. Mappings that are both sound and complete are 
called exact. 

In this paper, we restrict our analysis to the GAV approach, 
which is the most used in the context of data integration. In 
particular, we study a relational data integration framework in 
which key dependencies (KDs) and inclusion dependencies 
(IDs) are expressed on the global schema, and the mapping 
is considered sound. The main contributions of this paper are 
the following: 

1. After showing that query answering in the general case 
in undecidable, we provide a sound and complete query 
rewriting technique first for the case of IDs alone, and 
then for the case of KDs together with the maximal class 
of IDs for which the problem is decidable, called nonkey-
conflicting IDs, or simply NKCIDs (Section 3). 
2. 
Since it is likely that data retrieved at different, autonomous 
sources violate the KDs, we introduce a novel 
semantics that is a "relaxation" of the sound semantics, 
and that allows minimal repairs of the data (Section 4). 
We then present a sound and complete query rewriting 
technique in the case where KDs and NKCIDs are expressed 
on the global schema (Section 5). 
3. Finally, we present decidability and complexity results 
of the (decision) problem of query answering in the different 
cases (Section 6). 
2 Formal framework for data integration 

In this section we define a logical framework for data integration, 
based on the relational model with integrity constraints. 

Syntax We consider to have an infinite, fixed alphabet T of 
constants (also called values) representing real world objects, 
and will take into account only databases having T as domain. 
We adopt the so-called unique name assumption, i.e., we assume 
that different constants denote different objects. 

Formally, a data integration system I is a triple (G, S, M), 
where: 

Al AND DATA INTEGRATION 


1. 
G is the global schema expressed in the relational mally, given a source database P for I = (G,S, M), the 
model with integrity constraints. 
In particular, G = semantics of I wrt P, denoted sern(l,D), is a set of global 
databases for I, where a global database B is in sem(l,V) 
if: 

assume, without loss of generality, that the attributes in 
A are the first n attributes of r. Moreover, we assume 
that at most one KD is specified for each relation. 

2. 
S is the source schema, constituted by the schemas of 
the various sources that are part of the data integration 
system. We assume that the sources are relational, and 
that integrity constraints expressed on S are satisfied 
data at the sources. Hence, we do not take such constraints 
into account in our framework. 
3. 
M is the mapping between the global and the source 
schema. In our framework the mapping is defined in the 
a view, i.e., a query, over the sources. We indicate the 
mapping as a set of assertions of the form (r, V), where r 
is a relation and V is the associated view over the source 
schema. We assume that the language used to express 
queries in the mapping is positive Datalog [Abiteboul et 
al, 1995], over the alphabet of the relation symbols in 

S. 
A Datalog query (or program) q of arity n is a col-
Finally, a query over the global schema q is a formula that 
is intended to extract a set of tuples of elements of T. The 
language used to express queries over G is union of conjunctive 
queries (UCQ) [Abiteboul et al, 1995], i.e., a Datalog 
program such that each rule head uses the same predicate of 
the same arity, and only relation symbols of G occur in each 
rule bodv. 

In order to specify the semantics of a data integration system 
I, we start by considering a source database for I, i.e., 
a database P for the source schema S. Based on P, we now 
specify which is the information content of the global schema 

G. We call global database for I any database for G. For-
By simply evaluating each view over the source database 
P, we obtain a global database, called retrieved global 
database ret (l,d), that actually satisfies the sound mapping 
(but that is not necessarily consistent with G). 

In this paper, we address the query answering problem, that 
is the problem of computing the set ans(q,l,D). To this 
aim, we make use of query rewriting techniques, i.e., we exploit 
the mapping M to reformulate the query q into another 

The source schema So consists of the schemas of three 
sources comprising the relation s1 of arity 4, and the relations 
s2 and ,s3, both of arity 3. Finally, the mapping M0 is 
defined by the two assertions 


Below we define the algorithm ID-rewrite to compute the 
perfect rewriting of a union of conjunctive queries Q. Informally, 
the algorithm computes the closure of the set of conjunctive 
queries Q with respect to the following two rules: 

3 Query rewriting 
In this section we present algorithms for computing the perfect 
rewriting of a UCQ query in GAV integration systems 
with KDs and IDs. We first study the case in which only IDs 
are expressed on the global schema, then we deal with the 
simultaneous presence of both IDs and KDs. 

Query rewriting under IDs only We start by studying 
query rewriting when only IDs are expressed on the global 
schema. To this aim, we need some preliminary definitions. 

Given a conjunctive query q, we say that a variable A" is 
unbound in q if it occurs only once in q, otherwise we say 
that X is bound in q. Notice that variables occurring in the 
head of the query are necessarily bound, since each of them 
must also occur in the query body. A bound term is either a 
bound variable or a constant. 

Roughly speaking, an inclusion / is applicable to an atom 
g if the relation symbol of g corresponds to the symbol in the 
right-hand side of / and if all the attributes for which bound 
terms appear in g are propagated by the inclusion 1. When I 
is applicable to g, gr(g, I) denotes the atom obtained from g 
by using / as a rewriting rule whose direction is right-to-left. 

Termination of the algorithm is immediately implied by the 
fact that the number of conjunctions that can be generated by 
the algorithm is finite, since the maximum length of a generated 
conjunction is equal to the maximum length of a conjunction 
in the body of the initial query (Q, and the number of 
different atoms that can be generated by the algorithm is finite, 
since the alphabet of relation symbols used is finite (and 
corresponds to the relation symbols occurring in Q and in 

Query rewriting under KDs and IDs Now we address the 
problem of query rewriting in the case where KDs and IDs are 
defined on the global schema. Unfortunately, KDs and IDs 
interact reciprocally so that the (decision) problem of query 
answering in this setting becomes undecidable. The following 
theorem is a consequence of a similar property proved in 
[Call et ai, 2003] in the context of a single database. 

Undecidability of calculating the certain answers to a query 
immediately implies undecidability of calculating the perfect 
rewriting [Call et a/., 20031. The problem of query answering 
becomes decidable if we restrict the IDs to be in a particular 
class, so that they do not interact with KDs. 

We now go back to query rewriting. In the case of a NKC 
data integration system, we can apply the same technique developed 
for IDs alone, provided that we take into account the 
KDs with suitable rules. Indeed, observe that if ret(I, V) vi-

with regard to this issue, we first introduce a unary global relation 
val: the idea is that val stores all values occurring in 

4 Semantics for inconsistent data sources 

In the sound semantics, violations of IDs are treated "automatically"
because of the nature of the semantics; instead, the 

We call maximal w.r.t. (J, V) a global database B for 
I consistent with G, such that there exists no global 
database B' consistent with G such that B' >>(I,D) 

B. Based on this notion, we define the loosely-
sound semantics sera/ as follows: semi(I,D) = 
{B | B is consistent with G and B is maximal w.r.t. (I , V)}. 
Finally, we denote with ans1{q,I,D) the set of answers to 
queries under the loosely-sound semantics. 

6 Summary of complexity results 

5 Query rewriting in loosely-sound semantics 
We now address the problem of computing answers to a query 
under the loosely-sound semantics. Specifically, we present 
a rewriting technique to compute answers to queries posed to 
NKC systems under the loosely-sound semantics. 

Strictly-sound semantics. Query answering is undecidable 
even if we allow a slightly more general class of IDs than the 
NKCIDs; let us define a 1-key-conflicting (1KC) data inte-

Theorem 6.1 The problem of query answering in 1KC integration 
systems, under the strictly-sound semantics, is undecidable. 


In the strictly-sound semantics, the complexity of the decision 
problem of query answering is immediately derived from 
the rewriting of Section 3. 

Theorem 6.2 The problem of query answering in NKC integration 
systems, under the strictly-sound semantics, is in 
PTIME in data complexity. 

incide, it is easy to see that the above properties of query 
answering under the strictly-sound semantics can be easily 
generalized. 

Theorem 6.3 The problem of query answering in 1KC integration 
systems, under the loosely-sound semantics, is undecidable. 


We now characterize the problem of query answering under 
the loosely-sound semantics in NKC systems. 

Theorem 6.4 The problem of query answering in NKC integration 
systems, under the loosely-sound semantics, is coNPcomplete 
in data complexity. 

Proof (sketch). Membership in coNP follows from Theorem 
5.1, and from the fact that query answering in Datalogis 
coNP-complete in data complexity, while coNP-hardness 
can be easily proved by a reduction of the 3-COLORABILITY 
problem to our problem. \A1\F5 

The summary of the results we have obtained is reported 
in the table in Figure 1, which presents the complexity of 
query answering for both the strictly-sound and the loosely-
sound semantics. Each row corresponds to a different class 
of dependencies (specified in the first two columns), while 
each cell of the table reports data complexity and combined 
complexity1 of query answering for UCQs: for each decidable 
case, the complexity of the problem is complete w.r.t. the 
class reported. In the second column of the table, FK stands 
for "foreign key dependencies" (a well-known class of IDs) 
while GEN stands for "general IDs". We have marked with 

results or to results immediately implied by known results. 

1The results for combined complexity, which we cannot present 
in detail due to space limitations, hold under the assumption that the 
mapping is expressed in terms of UCQs. 

7 Discussion and related work 

In this paper we have presented techniques for query rewriting 
in data integration systems with integrity constraints, and 
analyzed the complexity of query answering. To this aim, we 
have exploited formalisms and methods both from the traditional 
database theory and from computational logic. 

Several works in the literature address the problem of data 
integration under constraints on the global schema. In this 
respect, query rewriting under integrity constraints has been 
first studied in the LAV setting. In particular, [Duschka and 
Genesereth, 1997] presents a method for query rewriting under 
functional dependencies in LAV systems, which is able to 
compute the perfect rewriting in the case of queries and mapping 
expressed through conjunctive queries, and "maximally 
contained" rewritings in the case of recursive mappings. 

Then, [Gryz, 1999] analyzes query rewriting under inclusion 
dependencies in LAV systems, and presents a method 
which is able to deal simultaneously with acyclic IDs and 
functional dependencies, based on an algorithm for computing 
the rewriting of a conjunctive query in a database with 
inclusion dependencies. The algorithm is based on a very interesting 
idea: obtaining query rewriting by computing the 
rewriting of each atom in a way "almost independent" of the 
other atoms. This can be obtained if the body of the initial 
query q is preliminarly "minimized". However, we have 
found out that Gryz's algorithm does not actually compute 
the perfect rewriting, in the sense that some conjunctions of 
the perfect rewriting are missing. Our algorithm ID-rewrite 
presented in Section 3 is certainly inspired by Gryz's main intuitions, 
but overcomes the above mentioned incompleteness 
through a new technique for generating the rewriting. 

Complexity of query answering in GAV under IDs alone 
is immediately derived by the results in [Johnson and Klug, 
1984]; in this work, the same problem is solved for a restricted 
class of KDs and IDs, which, however, is significantly 
less general than the one treated in this paper. More recently, 
integration under constraints in GAV systems has been addressed 
in [Call et al, 20021, which presents a method for 
query rewriting in the presence of KDs and foreign key dependencies, 
under a semantics analogous to our strictly-sound 
semantics. Thus, the method does not deal with data inconsistencies 
w.r.t. KDs. Moreover, [Fagin et al, 2003] presents 
an approach for dealing with integrity constraints in a GLAV 
setting (a generalization of LAV and GAV). 

In single database settings, [Arenas et al, 1999; Greco et 
al., 2001] propose methods for consistent query answering in 
inconsistent databases, which are able to deal with universally 
quantified constraints. The semantics adopted in these works 

is different from the ones considered in the present paper. 

Acknowledgments This research has been supported by the 
Projects INFOMIX (IST-2001-33570) and SEWASIE (IST2001-
34825) funded by the EU, and by the Project D2I 
funded by MIUR (Ministero per I'lstruzione, I'Universita e 
la Ricerca). We thank Maurizio Lenzerini and Jarek Gryz for 
precious discussions about this material. 


Abstract 

Finding desired information on the Internet is becoming 
increasingly difficult. Internet directories 
such as Yahoo!, which organize web pages into hierarchical 
categories, provide one solution to this 
problem; however, such directories are of limited 
use because some bias is applied both in the collection 
and categorization of pages. We propose 
a method for integrating multiple Internet directories 
by instance-based learning. Our method provides 
the mapping of categories in order to transfer 
documents from one directory to another, instead 
of simply merging two directories into one. 
We present herein an effective algorithm for determining 
similar categories between two directories 
via a statistical method called the k-statistic. 
In order to evaluate the proposed method, we conducted 
experiments using two actual Internet directories, 
Yahoo! and Google. The results show that 
the proposed method achieves extensive improvements 
relative to both the Naive Bayes and Enhanced 
Naive Bayes approaches, without any text 
analysis on documents. 

1 Introduction 

The World-Wide Web (WWW) is now used not only by computer 
specialists, but by all sorts of people, from children 
to businessmen and businesswomen, which has resulted in 
an enormous quantity of web pages available on the Internet. 
This makes finding pages containing the desired information 
rather difficult. Search engines are requisite for finding 
the desired information. In general, current search engines 
perform their functions via one of two methods: a keyword 
search and a directory search. A keyword search engine 
performs searches by means of user-specified keywords. 
Keyword-based search engines like Google can reliably find 
pages containing the specified keywords. However, if the 
user does not have a thorough knowledge of his/her search 
domain and cannot choose the appropriate keywords, the 
search engine is useless. In such a situation, directory-based 
search engines do a good job. In a directory-based search 
engine, pages are evaluated and organized by humans before 

being registered in the search engine's archive. Directory-
based search engines provide knowledge of WWW navigation. 
Users reach the desired information by going up and 
down the directories. 

Although pages are carefully selected and well-organized, 
a single Internet directory is not sufficient because the Internet 
directory tends to have some bias in both collecting 
and categorizing pages. In order to solve these problems, 
we herein propose a method that coordinates multiple Internet 
directories by estimating directory similarities. The 
proposed method does not simply merge multiple directories 
into a larger directory, but instead determines the relationship 
between directories. Many public Internet directories exist. 
Some are designed to cover wide domains, while others focus 
on special domains. Such Internet directories are difficult 
to merge. Moreover, these directories should not be integrated, 
because the existing differences in concept hierarchies 
among the directories is important when selecting and using 
directory-based search engines. In this paper, we propose a 
method to solve this problem by determining the rules for 
mapping categories in a directory to those in another directory. 
Our solution can be applicable not only to the Internet 
directory problem, but also to the integration of web marketplace 
catalogs [Agrawal and Srikant, 2001]1 and ontology integration 
in general. 

The remainder of this paper is organized as follows. In 
Section 2, we define the problem of coordinating multiple Internet 
directories. In Section 3, we discuss related studies. In 
Section 4, we propose a new machine learning method for the 
above-mentioned problem. Next, in Section 5, we compare 
the performance of the proposed method to that of the Enhanced 
Naive Bayes approach [Agrawal and Srikant, 2001] 
for an integration problem using real Internet directories. Finally, 
in Section 6, we present our conclusions. 

2 Integration of Multiple Internet Directories 

In order to state the problem, we introduce a model for the 
Internet directories we intend to integrate. We assume there 
are two Internet directories: a source directory and a targetdirectory. The documents in the source Internet directory are 

'For example, the integration of a distributor's catalog and a web 
marketplace. 


expected to be assigned to categories in the target Internet directory. 
This produces a virtually integrated Internet directory 
in which the documents in the source directory are expected 
to be members of both the source and target directories. This 
integrated directory inherits the categorization hierarchy from 
the target Internet directory. 

The Internet directory model for the source and target is as 
follows: 

. 
The source Internet directory, Sp contains a set of categories, 
Cs1, CS2, . . ., Csn, that are organized into an "isa" 
hierarchy. Each category can also contain documents. 
. 
The target Internet directory Tp contains a set of categories, 
C11, C12, . . . , Ctm that are organized into an "isa" 
hierarchy. Each category can also contain documents. 
The proposed model permits documents to be assigned to 
intermediate categories. This model is similar to the catalog 
model in lAgrawal and Srikant, 2001], except that the categories 
are organized into a conceptual hierarchy. Since the 
catalog model ignores hierarchy structure and therefore cannot 
assign documents to an intermediate category, our Internet 
directory model is more general than the catalog model. 

The problem addressed in this paper is finding an appropriate 
category Ct in the target directory Tp for each document 
Ds1 in the source directory Sp. An example is shown 
as the mapping of a black box Dx in Figure 1, where the 
black circles indicate categories and the hollow boxes indicate 
documents. What we need to do is determine an appropriate 
category in Tp for a document which appears in Sp 
but not in TD, because mapping is not necessary if the document 
is included in both the source and the target directories. 
Documents D1 and D2 in Figure 1 are examples of such 
documents. This mapping can have several possibilities, e.g., 
Dx can be mapped to an upper left category or to a lower left 
category, etc. 


3 Related Work 
One popular approach to this kind of problem is to apply 
standard machine learning methods [Langley, 1996]. This 
requires a flattened class space having one class for every 
leaf node. The problem can then be considered to 
be a normal classification problem for documents. Naive 
Bayes(NB) [Mitchell, 1997] is an established method used for 
this type of document classification framework. A classifier 

is constructed using the words in the documents. However, 
this classification scheme ignores the hierarchical structure of 
classes and, moreover, cannot use the categorization information 
in the source directory. Enhanced Naive Bayes [Agrawal 
and Srikant, 2001], hereafter referred to as E-NB, is a method 
which does use this information. E-NB will be discussed 
in the next section. GLUE [Doan ct al., 2002] is another 
type of system employing NB. To improve accuracy, GLUE 
combines NB and a constraint optimization technique called 
relaxation labeling. However, the general performance of 
the system depends on that of NB. Unlike NB, the systems 
in [Koller and Sahami, 1997], [Wang et al., 1999] classify 
documents into hierarchical categories, and these systems use 
the words in the documents for classification rules. However, 
these systems cannot use the categorization information in the 
source directory. 

Another type of approach is ontology merging/alignment 
systems. These systems combine two ontologies, which 
are represented in a hierarchal categorization. Chimaera 
[McGuinness et al., 2000] and PROMPT [Noy and 
Musen, 2000] are examples of such systems and assist in the 
combination of different ontologies. However, such systems 
require human interaction for merging or alignment. In addition 
to this requirement, these systems are based on the 
similarity between words, which introduces instability. The 
dictionaries used for such systems often have word similarity 
bias. FCA-MERGE [Stumme and Madche, 2001] is another 
type of ontology merging method. It uses the attributes of 
concepts to merge different ontologies. As a result, it creates 
a new concept without regarding the original concepts in 
both ontologies. Calvanese et al. [Calvanese et al, 2001] also 
discussed an ontology integration framework with respect to 
global ontology and local ontology. 

As mentioned before, we also tackled the similar problem 
of catalog integration. An approach, besides E-NB, is to construct 
the abstract-level structure of two hierarchies [Omelayenko 
and Fensel, 2001]. This approach does not direct 
the transformation of source and target information, but transforms 
via the abstract-level structure. It is relatively easy to 
transfer information through many hierarchical structures, but 
it is hard to create a common structure for those hierarchies. 

The bookmark-sharing systems of Siteseer [Rucker and 
Polanco, 1997] and Blink [Blink, 2000] also deal with a 
similar problem. These systems attempt to share URL information 
which appears in the source bookmark but not in 
the target bookmark. These systems flatten the categorization 
of bookmarks in the same manner as NB and determine 
the mapping of categories in each bookmark, based on 
the shared URL information. Such systems are problematic 
when a given URL does not fit into an exact category. kMedia 
[Takeda et al, 2000] is another bookmark-sharing system 
that uses hierarchical structures explicitly, but is dependent 
on the similarity of the words within pages. 

In the context of information retrieval, Stuckenschmidt 
[Stuckenschmidt, 2002] has coordinated the formal 
model of hierarchies with multiple classification hierarchies. 
In our paper, since the upper and lower boundaries of concept 
mapping can be determined by subsumption relationships, 
we can infer the boundary of valid mapping and also check 

whether or not the given mapping is consistent. In our 
problem, since we have only partial evidence of classes and 
do not know the definitions of classes, we can not apply this 
method directly. 

4 Learning the Relationship between 

Categories of Two Internet Directories 
In this section, we explain our method to determine the relationship 
between categories in two Internet directories. One 
characteristic of the proposed method is to use the hierarchical 
structure "as is." We use all categories including intermediate 
categories and leaf categories, because information 
for categorization can also be obtained from these categories. 
Another characteristic of the proposed method is the exclusive 
reliance on the categorization structure of both directories, 
i.e., with no reliance on the semantic information of the 
documents. We can determine the relationship between categories 
of the two directories by statistically comparing the 
membership of the documents to the categories. 

4.1 Basic Concept 
Although E-NB was developed for a very similar problem, 

it is missing an important feature: categorization hierarchy. 

According to [Agrawal and Srikant, 2001], the initial defini


tion of the problem is identical to that of this paper. However, 

the previous paper assumes that "any documents assigned to 
an interior node really belong to a conceptual leaf node that 
is a child of that node," and concludes from this assumption 
that "we can flatten the hierarchy to a single level and treat 
it as a set of categories." However, the conclusion overlooks 
the categorization hierarchies. The categorizations are not in
dependent of each other. The categorization hierarchies are 
usually structured using an "is-a" or another relationship. If a document is categorized in a lower category in the categorization hierarchy, then the document should also be categorized in the upper categories 2. If we flatten the categories, such information regarding the relationships between categories will be lost. 

Another problem associated with NB is its sensitivity to 
the words in documents. For example, if a document contains 
the word bank, the category for the document could be 
a financial category; however, the category could also be a 
construction category. The quality of categorization with NB 
is thus not stable according to the contents of documents, due 
to the natural language techniques of processing words. It 
is critical to integrate Internet directories because mixing reliable 
Internet directories maintained by human editors with 
less reliable and less stable machine-generated classifiers will 
confuse users and decrease their confidence. In addition to 
the problem of sensitivity, analysis of a document to extract 
words is expensive. 

Our method focuses on the similarity of the way of categorization, 
not the similarity of documents. Then, how do we 
measure the similarity of categorization? We utilize shared 
documents in both the source and target Internet directories 

2Note that the reverse is not always true, because documents can 
be categorized into intermediate categories. 

as our measurement standard. If many documents in category 
Csi also appear in category Ctj at the same time, we consider 
these two categories to be similar, because the ways of categorization 
in Csi and Ctj are supposed to be similar, i.e., if 
another document D comes in Csi, it is likely that D will be 
also included in CtJ. This method avoids both the keyword 
extraction process and the handling of word meaning. The 
example shown in Figure 2 illustrates that documents in the 
bottom category in the source Internet directory can be transferred 
to a similar category in the target Internet directory. 


Figure 2: Document transfer from source Internet directory 
to target Internet directory. 

4.2 k-Statistic 
The remaining problem is how to determine the pairs of similar 
categories. We adopt a statistical method to determine the 
degree of similarity between two categorization criteria. The 
K-statistic method [Fleiss, 1973] is an established method for 
evaluating the similarity between two criteria. Suppose there 
are two categorization criteria, Csi in SD and Ctj in TD. We 
can determine whether or not a particular document belongs 
to a particular category3. Consequently, documents are divided 
into four classes, as shown in Table 1. The symbols 
N11, N12, -N21 and N22 denote the numbers of documents for 
these classes. For example, N11 denotes the number of documents 
which belong to both Csi and Ctj. We may logically 
assume that if categories Csi and Ctj have the same criterion 
of categorization, then N12 and N21 are nearly zero, and if 
the two categories have a different criterion of categorization, 
then N11 and N22 are nearly zero. The k-statistic method 
uses this principle to determine the similarity of categorization 
criteria. 
In the k-statistic method, we calculate the probability P, 

3 Remember that categorization in a directory is determined using 
the nodal structure of the categorization hierarchy. 

which denotes the percentage of coincidence of the conceptual 
criteria, and the probability P' which denotes the percentage 
of coincidence of the conceptual criteria by chance. 


As such, the value of the K-statistic is represented as the 
following equation: 


Next, we examine whether we can assume the K = 0, 
which indicates that the percentage of coincidence for the 
two conceptual criteria is zero. We therefore calculate the 
test statistic Z according to the following equation. 


The value Z follows a normal distribution. A null hypothesis 
is considered to occur when the percentage of coincidence 
of the concept criteria is zero. When we assume a significance 
level of 5% and the following equation is satisfied, we 
can dismiss the null hypothesis. 


When the null hypothesis can be dismissed, the criteria are 
determined to be the same. 

4.3 Determination of Pairs of Similar Categories 
The relationship between the two categorization criteria is examined 
from top to bottom. This algorithm is shown in Figure 
3. First, the most general categories in the two categorization 
hierarchies are compared using the K-statistic. If the 
comparison confirms that the two categories are similar, then 
the algorithm outputs this pair of categories. At the same 
time, the algorithm generates all possible pairs of their children 
categories. It generates two lists of categories each of 
which is a list of the confirmed category and its children categories. 
Then, the algorithm picks one category from each 
list and makes a pair, except for the original pair. This new 
pair is then evaluated recursively using the K-statistic method. 
When a similar pair is not generated, the algorithm outputs 
the pairs of similar concepts between the two categorization 
hierarchies. They are used as mapping rules from the source 
directory to the target directory4. 

This top-down comparison approach attempts to reduce the 
exploration space, using the structure of both directories as a 
guide. Assuming that similar categories include similar subcategories, 
we can skip the combination of categories which 

4The rules do not guarantee the consistency of the mappings, 
because category hierarchies have the possibility of inconsistency 
with regard to similar relationships. 


Figure 3: Algorithm to determine similar category pair. 

is likely unnecessary5. This is another benefit of using the ''as 
is" hierarchy, in contrast to the flattening approach. 

4.4 Application Policy of Rules 
Since the proposed method uses categorization similarity, if 
a category in the source Internet directory does not have a 
similar category in the target directory, then those documents 
cannot be categorized in the target Internet directory. In order 
to avoid this problem, we once again use the categorization 
hierarchy. If the system cannot find a similar category pair 
for the category containing the document, it applies the rule 
generated for the parent category instead. 

5This assumption may sometimes be too strong, e.g., a child category 
of a category is not so relevant, whereas its child category (a 
grandchild category of the original category) is relevant. In such 
cases, we should relax this constraint when exploring future candidates. 

Table 2 shows the numbers of categories, the links in each 
Internet directory and the links included in both Internet directories. 
Links are considered to be the same when the URLs 
in both directories are identical. 

5 Experiments Using Internet Directories 

5.1 Experimental Settings 
In order to evaluate the proposed algorithm, we conducted 
experiments using data collected from the Yahoo! [Yahoo!, 
2001] and Google [Google, 2001]6 Internet directories. The 
data was collected in the fall of 2001. In order to compare 
the proposed method to that in [Agrawal and Srikant, 2001], 
we selected the same locations in Yahoo! and Google for the 
experimental data. The locations are as follows: 

6Since the data in Google is constructed by the data in 
dmoz [dmoz, 2001], we collected data through dmoz. 

We conducted ten-fold cross validations for the shared 
links. The shared links were divided into ten data sets; nine 
of these sets were used to construct rules, and the remaining 
set was used for testing. Ten experiments were conducted for 
each data set, and the average accuracy is shown in the results. 
In order to compare our proposed method to the E-NB 
approach, the classifiers are assumed to correctly assign documents 
when the document is categorized cither in the same 
category as the test data or in the parent categories of the test 
data. E-NB constructs classifiers for only the first-level categories 
in the experimental domain, whereas the proposed 
method uses all of the categories from top to bottom. The 
significance level for the K-statistic was set at 5%. 

5.2 Experimental Results 
The experimental results are shown in Tables 3 and 4. Table 
3 shows the results obtained using Yahoo! as the source 
Internet directory and Google as the target Internet directory, 
and Table 4 shows the results obtained using Google as the 
source Internet directory and Yahoo! as the target Internet 
directory. For comparison, these tables also include the results 
of [Agrawal and Srikant, 2001]. E-NB denotes the 
method of [Agrawal and Srikant, 2001] and SBI denotes the 
similarity-based integration method that is proposed in this 
paper. The data obtained for the proposed method and that 
presented in [Agrawal and Srikant, 2001] are not truly comparable 
because the collection date is different7. 

The proposed algorithm did well compared to E-NB, performing 
more than 10% better in accuracy on the averages. In 
the Movies domain, the proposed algorithm performs much 
better in accuracy than E-NB. One reason for this is that the 
pages related to movies contain various words that indicate 
numerous movie settings, making classification using word-
based systems difficult. On the other hand, in the Outdoors 
domain, performance is similar for both methods because the 

7In addition, differences may have occurred due to data selection. 
For example, Yahoo! has links that use @(the at mark). The 
treatment of such links was not discussed in [Agrawal and Srikant, 
2001]. In the present study, we did not use such links. 

Table 3: Results of Yahoo! as the source Internet directory 
and Google as the target Internet directory. 


Table 4: Results of Google as the source Internet directory 
and Yahoo! as the target Internet directory. 

Outdoors domain treats pages using special outdoors-related 
words, and no classification method for the Outdoors domain 
has been established for human readers. 

As mentioned earlier, in the experiments described 
in [Agrawal and Srikant, 2001], the classifiers induced by the 
system have the ability to classify the documents into only the 
first-level categories of the test domain in the target Internet 
directory. The proposed classifiers can classify the documents 
into sub-categories as well. For example, we used 4,623 categories 
for document assignment in the Movies domain of 
Yahoo! as the source Internet directory and Google as the target 
Internet directory, whereas their system used only 40 categories. 
Therefore, the classifiers obtained by the proposed 
method are more reliable and useful than those obtained by 
other methods. 

Next, we compare our method with GLUE [Doan et al, 
2002]. The basic steps of GLUE are as follows. First, the user 
trains learners for each concept for target taxonomies. Learners 
can be a combination of different strategies like name- or 
contents-based learning. Then, they determine that the source 
concept and target concept are similar, if the application of 
learners for the source concept yields good enough. Finally, 
the system applies the relaxation labeling method for similar 
concept pairs. The method does not use the hierarchical 
structures in learning very well, and relies mainly on the performance 
of the learning method, namely NB. On the other 
hand, our method does use hierarchical structures in learning 
very well. So, we do not need a semantical analysis in learning. 
They report an accuracy of 65-95% which is equivalent 
to our results, but they use only a few categories, i.e., 40-363, 
while we use 170-4623 categories. We can say from both 
theoretical and practical viewpoints that our method is more 
appropriate in learning for large hierarchical structures. 

6 Conclusions 

In this paper, a statistical-based technique was proposed for 
integrating multiple Internet directories by determining the 
relationship between these directories. The proposed method 
uses the K-statistics to find similar category pairs, and transfers 
the document categorization from a category in the 
source Internet directory to a similar category in the target 
Internet directory. The proposed method has an advantage 
in document treatment in that it relies on the category structure 
only, and not on words or word similarity in a document. 
The performance of the proposed method was tested 
using actual Internet directories, and the results of these tests 
show that the performance of the proposed method was more 
than 10% better in accuracy than that of previous methods, 
although the number of categories were by far larger for our 
proposed method. In addition, our problem modeling is more 
general than the other models in terms of assignment documents 
on intermediate categories. From this, we can conclude 
our approach shows great promise in comparison to systems 
employing NB, such as E-NB [Agrawal and Srikant, 2001], 
GLUE [Doan et ai. , 2002], and so on. 

Although the present results are encouraging, much has 
yet to be done. The limitation of the proposed method is 
that this method is based on the existence of shared links. 
In other words, the proposed method is less reliable if there 
are fewer shared links. To improve our proposed method, 
various methods might be used to obtain semantic information 
in order to increase the number of shared links, e.g., to 
regard similar pages as the same links. Another option for 
this problem might be to combine semantic information or 
other information about the documents. If we could a establish 
good combination of our proposed method and such 
information, it is possible that we could greatly improve our 
method. Moreover, the present method can find only one-toone 
mapping rules. If the system is able to find one-to-many 
or many-to-many mappings, it would work more correctly in 
categorization. For the development of these mappings, we 
might invent probabilistic mapping representations and also 
introduce a subsumption technique [Stuckenschmidt, 2002]. 
Finally, the proposed method should be expanded so that it 
can apply to more than three concept hierarchies. In such 
a case, despite the conflict between several concept hierarchies, 
we would expect more information to be obtained. The 
aforementioned tasks for improvement will be investigated in 
future studies. 

Abstract 

With the proliferation of heterogeneous devices 
(desktop computers, personal digital assistants, 
phones), multimedia documents must be played under 
various constraints (small screens, low bandwidth). 
Taking these constraints into account with 
current document models is impossible. Hence, 
generic source documents must be transformed 
into documents compatible with the target contexts. 
Currently, the design of transformations is left to 
programmers. We propose here a semantic framework, 
which accounts for multimedia document 
adaptation in very general terms. A model of a multimedia 
document is a potential execution of this 
document and a context defines a particular class 
of models. The adaptation should then retain the 
source document models that belong to the class 
defined by the context if such models exist. Otherwise, 
the adaptation should produce a document 
whose models belong to this class and are "close" 
to those of the source documents. We focus on the 
temporal dimension of multimedia documents and 
show how adaptation can take advantage of temporal 
reasoning techniques. Several metrics are given 
for assessing the proximity of models. 

1 Introduction 

The multiplication of execution contexts for multimedia documents 
requires the adaptation of document specifications to 
the particularities of the contexts. Adaptation is not very precisely 
defined and it is currently specified through programming. 
We propose a semantic approach to multimedia documents 
(\A1\EC2). It does not deal with the semantics of document 
content, but with that of their composition. The approach 
allows the definition of adaptation in very general semantic 
terms independent from the multimedia objects (\A1\EC3). We then 
investigate the temporal dimension of multimedia documents 
specified qualitatively (\A1\EC4) and propose metrics for finding 
the "best" adaptations. We discuss then the limitations of 
current multimedia specifications that prohibit a better adaptation 
(\A1\EC5). 

We first introduce the characteristics of multimedia documents 
(\A1\EC1.1) and adaptation (\A1\EC1.2). 

1.1 Multimedia documents 
A multimedia document is a digital document composed of 
objects of different nature: text, sound, image, animation, 
etc. These objects and their compositions are called multimedia 
objects. Multimedia documents are traditionally analysed 
following four dimensions [Layaida, 1997]: 

. 
logical (organisation into chapters, shots, etc.), 
. 
spatial (graphic layout), 
. 
hypermedia (relations between documents and document 
fragments), 
. 
temporal (temporal ordering of the multimedia objects). 
These dimensions are not totally independent and require a 
combined processing. 

This paper primarily focuses on the adaptation of multimedia 
documents along their temporal dimension. In a temporal 
multimedia document, the presentation of the multimedia objects 
is scheduled over time. Such a document is presented 
in Figure 1. Time is displayed on the horizontal axis. The 
example presented is the introduction of a slideshow made 
of different panels composed of graphic objects that can be 
presented simultaneously. The first panel displays the title, 
authors and outline of the speech; each of these objects are 
represented by a segment whose begin and end points correspond 
to the beginning and ending of their presentation on 
screen. 


Figure 1: Temporal dimension of a multimedia document. 

The Title object starts at second 0 and ends at second 5, 
while the Author object starts at second 2 and ends at second 
5. Between seconds 5 and 8 the Outline object is presented, 
etc. Such a description is exact and quantitative since 
it defines exactly the beginning and ending instants of each 
multimedia object. This information is sufficient for playing 
the document: to one exact quantitative representation corresponds 
only one possible execution of the document (within 
a fixed temporal reference). 

Specifying a multimedia document in an exact manner is like writing a paper directly in PostScript instead of using 
LATEX. Multimedia documents are not often specified in an 
exact way because it is more convenient for the author to 
leave the interpretation of the specification to the machine as 
soon as the will of the author is clearly expressed. The author 
can concentrate on the creative part of is or her work instead 
of characterising the exact position of each object. 

Non-exact specifications can be achieved by expressing the 
qualitative relations between multimedia objects. There are 
several languages for specifying multimedia documents with 
different ways of expressing the temporal dimension: SMIL 
[W3C, 1998] expresses the positioning of multimedia objects 
with parallel and sequence operators on intervals; Magic 
I Dalai et al, 1996] and Madeus [Layaida, 1997] use a restriction 
of the Allen algebra of temporal intervals. 

The document of Figure 1 can be expressed qualitatively. 
For instance, the Authors object starts after and finishes with 
the Title object; the Authors object meets the Outline object. 
From such a specification, the multimedia presentation system 
(or the Player) computes a plan (called "scenario") that 
can be executed. This function is called temporal formatting. 

1.2 Adapting multimedia documents 
A server delivers a multimedia document to be played by a 
client. Clients and servers can be different machines with 
different capabilities. Different contexts of multimedia presentations 
introduce different constraints on the presentation 
itself. For instance, bandwidth limitations between the client 
and the server can result in preventing the client from playing 
two bandwidth-demanding videos at the same time. Display 
limitations can produce similar constraints. Other constraints 
may also be introduced by user preferences, content protection 
or terminal capabilities. The constraints imposed by a 
client are called a profile. 
Profiles can be expressed in terms of a restriction of the 
language used for specifying target documents or in terms of 
additional constraints imposed on the objects. For instance, 
if the device features only a screen with limited capabilities, 
it can be impossible to display two images simultaneously on 
the same screen. 
For satisfying these constraints, multimedia documents 
must be adapted before being played. From the profile and 
the source document, the adaptation must provide a document 
satisfying the constraints expressed in the profile. Qualitative 
specifications arc central to this process as they enable more 
efficient adaptation by providing more flexibility. This adaptation 
is usually performed by a program transforming the 
document IVillard, 2001; Lemlouma and Layaida, 2001]. 
For the purpose of characterising the adaptation process, 
we introduce a semantics of multimedia documents and illustrate 
it on the temporal dimension (\A1\EC2). The semantic definition 
of adaptation (\A1\EC3) leads to distinguish refining adaptation 
(in which the models of the adapted document are models 
of the source documents) from transgressive adaptation 
(in which the models are as close as possible to those of the 
source document). Section 4 illustrates the notion of closeness 
for the temporal dimension. The limits of our approach 
are then presented (\A1\EC5). 

2 A semantic approach to multimedia 
documents 


2.1 Specifications 
Table 1: The 13 relationships between temporal intervals. 

Definition 1 (Specification). A specification s \A1\AA (O, C) of a 
document is made of a set of objects O and a set of constraints 
C between these objects (i.e., a relation between several objects). 
The set of all specifications will be noted S. 

In the remainder, the constraints will be considered as binary. 
The temporal specification can then be represented as a 
relation graph [van Beek, 1992]. This representation will be 
used for describing models. 

Definition 3 (Resolved relation graph). A relation graph is 
resolved iff all the labels are singletons. 

2.2 Semantics of a specification 
The specification of a multimedia document is interpreted as 
the set of its potential executions. A model of a multimedia 
document (in the sense of model theory) is an execution of 
the document satisfying the specification. 

3.1 Adaptation constraints 
Definition 7 (Adaptation constraint). An adaptation constraint 
a determines a set of possible executions Ma- The set 
of adaptation constraints will be noted A 

The example above introduced a constraint prohibiting 
more than one image to be displayed at once on a screen. 
This can be expressed by a MSO constraint. 

Example 3 (Temporal model). The interpretation presented 
in Example 2 is a model of s1 but not of s2. 

In the following, we will always consider that there exists 
at least one model of the source specification (which is thus 
consistent). 

These models correspond faithfully to the execution of the 
multimedia documents. However, the formatter will consider 
executions as equal if they only differ by a translation factor 
and the adaptation will consider two executions as equal if 
they only differ in duration, preserving topology and ordering. 
We introduce qualitative representations of models as 
abstractions of models. 

Since the Allen relations are exclusive and exhaustive, 
qualitative representations of a model correspond to resolved 
relation graphs. 

 Semantics of adaptation 

The adaptation of a multimedia document is constrained by 
the profile. The profile defines constraints that must be satisfied 
by the document to be played. 

A profile p is a set of such constraints. It determines a class 
of qualitative models (those who satisfy the constraints). The 
role of adaptation is thus to determine if there exist models of 
the initial specification belonging to that class. Otherwise, it 
is convenient to alter the specification by finding, among the 
set of models satisfying the profile, those that are "semantically 
closer" to the source specification. 

3.2 Problems 
One of the benefits of the approach is to be able to clearly provide 
criteria that an adaptation function r must meet. These 
criteria are expressed here as a set of problems. 
The first one is that the adapted specification must satisfy 
the adaptation constraints. 

Moreover, if there exists a possible execution of the document 
satisfying the adaptation constraints, this execution 
must be preserved in the adapted specification. 

In such a case, the adaptation should not authorise models 
that were not models of the source specification. 

. Unfortunately, no guarantee is given that the languages 
used for expressing the specifications and the adaptation constraints 
allow the expression of a specification satisfying these 
requirements. 
Moreover, one constraint that should be achieved by a semantic 
approach is that the result of adaptation must not depend 
on the syntactic form of the specification. 

Taking the semantic approach to multimedia document 
adaptation allows the characterisation of adaptation in a very 
general way depending only on model theoretic considerations. 
In particular, these definitions are totally independent 
from the language used for expressing documents and profiles 
as well as the multimedia object and constraint types. 

This characterisation clearly emphasises the constraints 
that a refining adaptation must meet and that can be overlooked 
when programming the transformation. 

Transgressive adaptation is more difficult to characterise 
and this is considered in the next section. 

 Transgressive adaptation in the temporal 

dimension 
The goal of transgressive adaptation is to find a specification 
T(S) as close as possible to the source specification s. Semantically, 
this amounts to find the specifications whose models 
are the closest possible to those of the source specification. 
Figure 3 shows the set of models satisfying MSO(l). A dis-

Applying a semantic approach to transgressive adaptation 
can be compared to the use of the semantic approach for 
knowledge base revision [Dalai, 1988]. This will be the first 
step taken here. But we will show that the simple distance 
used for comparing models is not sufficient for adapting multimedia 
documents. Measures depending on the kind of multimedia 
objects are required. 

The usual way to compute the distance between sets of 
models is function of a distance d between two models and 
a method of aggregation F. 

Definition 9 (Distance between sets of models). 

The second element is the distance d that is considered 
hereafter1. 

4.1 Distance on the qualitative models 
The first distance that comes to mind consists of counting 
the relations between two objects that differ between models. 
It is comparable to the Hamming distance (i.e., the cardinal 
of the symmetric difference between two sets) counting the 
propositional atoms that do not have the same truth-value in 
propositional logic [Dalai, 1988]. Because qualitative models 
correspond to resolved relation graphs, the distance is easily 
computed by counting the labels which differ between each 
pair of objects. The distance is defined on graphs (and more 
precisely on their labelling functions). 
Definition 10 (Distance between resolved relation graphs). 


Example 7. Concerning s1, the four models are all at the 
same distance from the model of the source specification because 
they all differ by two relations (the one between A and 
B and its converse). 

In order to find more precise results that discriminate between 
the four models of the example, the relations between 
intervals can be transcribed into relations between their begin 
and end points and the same sort of distance can be used. 

Definition 11 (Distance between interval relations based 
on endpoints). 

The distance between models is the sum of the distance 
between each interval relation. 

Definition 12 (Distance between models based on endpoints). 


Table 2: Relations preserved by the linearization of an overlap 


adapting situation s\. Two models (before and meets) arc 
clearly preferred over the others (met by and after). But intuitively, 
it seems that meet is a belter solution because it 
reduces the distance between the two objects which where 
previously overlapping. We show that it is possible to find a 
distance conforming to this intuition. 

4.3 Conceptual distance in the interval algebra 
The problem with the former distance is that it does not take 
into account the topological structure of temporal relations 
(i.e., it only counts differing relations on endpoints or intervals 
without consideration for a proximity between the disagreeing 
relations). To take this proximity into account, we 
take advantage of the notion of conceptual neighbourhood 
[Freksa, 1996] and the shortest path distance in its graph (see 
Figure 4). 

Conceptual neighbourhood attempts at capturing the proximity 
between qualitative relations by observing the effects of 
transforming the related objects. 

The conceptual neighbourhood relation for the transformation 
that moves one endpoint without affecting the others is 
given in Figure 4. 

A distance between relations can be directly computed 
from the graph. 

Then the distance between models can be expressed by 
summing up the conceptual distances between the relationships 
used in both models. 

Definition 15 (Conceptual distance between models). 

Example 9. Concerning s1, the models satisfying the adaptation 
constraint MSO(l) are at different conceptual distances 
from the source specification: before is at a distance of 2, 
meet is at a distance of 1, met by is at a distance of 5 and after 
is at a distance of 6. So the closest solution to serializing the 
overlap relation is meet. This corresponds to the intuition. 

5 Limitations 

Extending the presented work to the spatial dimension does 
not look very difficult. The logical dimension is even easier 
because it provides a very structured organisation of the 
document that, we conjecture, can yield direct adaptation. So 
the proposed approach is able to cope with adaptation in each 
dimension of the document. 

Real difficulties arise when hypermedia and temporal and 
spatial dimensions are considered together. As a matter of 
fact, the presence of hypermedia links which, when triggered 
by the users, jump to other parts of the presentation, introduce 
non-determinism in the interpretation of documents [Dalai el 
al, 1996; Fargier el al, 1998]. This non determinism does 
not easily fit with the conceptual neighbourhood approach 
which favours continuity. 

A further analysis shows that the temporal information 
contained in specifications is not sufficient for a good adaptation. 
For instance, considering two panels composed of two 
pictures each (AB and then CD, like in Figure 1) and the 
MSO(l) constraint, the closest models linearizing the presentation 
are ABCD and ABDC with a conceptual distance of 

18. However, if both panels aim at comparing two objects 
01 (right) and 02 (left) on the basis of two features (one by 
panel), preserving the parallelism (which suggests the comparison) 
imposes the choice of ABCD. The absence of information 
about the comparison is missing from the specification 
resulting in lower quality adaptation. Some authors 
[Rutledge el al, 2000] have proposed to use rhetorical structures 
[Mann and Thompson, 19871 in order to choose a better 
presentation at the formatting stage. This could be useful for 
the adaptation stage as well. 

6 Related work 

The most related work is that of [Dalai et al, 1996], which 
describes the generation of multimedia presentations through 
the negotiation of the temporal constraints. Like the work 
presented here, the temporal specifications are expressed by 
Allen relations. The approach differs because we consider an 
existing specification to be adapted where the authors generate 
schedules and preferences among them on the fly. So 
there is no alteration of already existing constraints based on 
the semantic characterisation, but a satisfiability check and 
negotiation of constraints when inconsistency is detected. 

The transgressive adaptation can be compared with the revision 
in knowledge bases [Gardenfors, 1992]: the addition 
of a new (adaptation) constraint leads to inconsistency. It 
is necessary to find a new specification satisfying this constraint 
and not too different from the source specification. 
One difference is that adaptation constraints are not always 
formulas of the specification language. Having several constraints 
raises problems similar to incremental revision: since 
the constraints are not provided in a sequence but in a set, it is 
important that the adaptation does not depend on some order 
of presentation constraints. Although transgressive adaptation 
is neither revision (it does not correspond to some change 
in our knowledge) nor update (it cannot be compared to the 
acquisition of a new information), more generic techniques 
developed for revision could be used in the context of multimedia 
adaptation. 

7 Conclusion 

This paper applied a semantic approach to multimedia documents 
and their adaptation. This allows for a precise definition 
of what is expected from the adaptation of these documents 
and the comparison of the results given by handmade 
transformation with what was expected. It proposes a 
model-based distinction between compliant documents, refining 
adaptation and transgressive adaptation. This framework 
has been applied to the temporal dimension of the documents 
providing measures for sharply discriminating the possible 
transgressive adaptations. 

As discussed above, there remains more work to be carried 
out for covering all the aspects of multimedia documents and 
for deepening the specification of documents and adaptation 
constraint so that the adaptation produces quality results. 

Abstract 

In the Web, extractor agents process classes of 
pages (like 'call for papers' pages, researchers' 
pages, etc), neglecting the relevant fact that 
some of them are interrelated forming clusters 
(e.g., science). We propose here an architecture 
for cognitive multi-agent systems to retrieve and 
classify pages from these clusters, based on data 
extraction. To enable cooperation, two design 
requirements are crucial: (a) a Web vision 
coupling a vision for contents (classes and 
attributes to be extracted) to a functional vision 
(the role of pages in information presentation); 

(b) explicit representation of agents' knowledge 
and abilities in the form of ontologies, both 
about the cluster's domain and agents' tasks. 
Employing this Web vision and agents' 
cooperation can accelerate the retrieval of useful 
pages. We got encouraging results with two 
agents for the page classes of scientific events 
and articles. A comparison of results to similar 
systems comes up with two requirements for 
such systems: functional categorization and a 
thoroughly detailed ontology of the cluster. 
1 Introduction 

Although the terms "Web information agents" and 
"cooperative information gathering" (CIG) are in fashion, 
there arc scarce examples of Web systems endowed with 
some sort of cooperation or integration among tasks 
related to text processing, viz classification, retrieval, 
extraction and summarization. When defining the main 
aspects of CIG [Oates et al 1994], information extraction 
was mentioned as a key technology that addresses 
information acquisition in complex environments, 
suggesting implicitly task integration for CIG. 

Indeed, there is room for cooperation, in particular for 
extractor agents on the Web. These systems are being 
designed to process classes of pages with similar 
structuring and contents (e.g., call for papers, scientific 
articles, ads, etc). However, the relevant fact that many 
of these classes are interrelated forming clusters (e.g., 
Science) has been neglected so far. Cooperation among 

Guilherme Bittencourt 
Departamento de Automacao e Sistemas 
Universidade Federal de Santa Catarina 
Cx. Postal 476, 88.040-900, Florianopolis, Brazil 
gb@das.ufsc.br 

extractors could increase the retrieval of useful pages, 
taking advantage of these relations. 

Aiming at task integration and cooperation, we 
designed an architecture for cognitive multi-agent 
systems that retrieve and classify pages from clusters of 
information in the Web, extracting slots of data from it. 
Two design requirements are crucial. One is a Web 
vision coupling a vision for contents (classes and 
attributes to be extracted) to a functional vision (the role 
of pages in information presentation, i.e., whether a page 
is a list, message, class instance or garbage). The other, 
necessary to enable cooperation, consists of explicit 
knowledge representation in the form of ontologies, both 
about the cluster's domain and agents' tasks. 

Experiments held with two agents, that treated the 
classes of scientific events and articles, produced 
promising results, leading to two conclusions: (a) 
functional categorization can be profitable; (b) key 
requirements for success in Web CIG systems arc this 
categorization and a detailed ontology of the cluster. 

The article is organized as follows: Section 2 describes 
the proposed Web vision. Section 3 introduces the 
architecture, its components and many types of reuse 
enabled. Section 4 outlines a case study: MASTER-Web 
(Multi-Agent System for Text Extraction, classification 
and Retrieval over the Web), applied to the Science 
cluster, with the two agents cited above. The results on 
contents and functional classifications are also shown. 
Section 5 compares MASTER-Web to similar systems. 
Section 6 brings future work and conclusions. 

2 A Proposed Web Vision for CIG 

Web extractors seek pages that are instances of pages' 
classes (e.g., call for papers pages, researchers', etc). 
These pages share many common attributes (slots), like 
date and place of a conference. Pages' classes outline a 
Web division by contents. The slots usually found in a 
class are discriminant, once they help distinguish class 
members. This fact supports the use of extraction in class 
categorization. Researchers' pages, for instance, usually 
contain typical data, such as projects, interest areas, etc. 

Most links in classes' pages point to pages containing 
data from a few other classes. A set of interrelated 

classes and their relations gather a body of knowledge 
about a specific domain (e.g., science, tourism, etc). They 
form a cluster of classes. In researchers' pages, e.g., we 
often find links to papers, calls for papers and other 
classes from the scientific cluster. 

Another view of the Web, based on the work of Pirolli 
et alii [1996], focus on functionality, dividing pages by 
the role played in linkage and information storage. We 
separate them into five functional groups: content pages 
(class members), resource directories (lists of content 
pages, e.g., lists of researchers or scientific articles), 
messages about content pages, recommendations (pages 
that arc members of other classes) and garbage. 

Joining these two visions, we can accurately identify 
not only the information to be extracted from page 
classes, but also instances of relations among classes in a 
cluster. Thus, it can improve the search of useful pages. 

3 Proposed Architecture for CIG 

We propose an architecture for cognitive multi-agent 
system (MAS) to retrieve and extract data from Web 
pages belonging to classes of a cluster. The core idea of 
employing a MAS resides on taking advantage from the 
relations among classes through agents' cooperation. The 
architecture overview is illustrated in figure 1. 


Figure 1: Architecture overview. 

Each agent, represented as a circle in the figure, 
recognizes, filters and classifies pages, extracting 
attributes (slots) from them. The pages are supposed to 
belong to the class of pages that the agent processes. For 
instance, CFP pages are processed by the CFP agent, and 
papers pages by an articles' agent, in the Science cluster. 
The MAS is based on Distributed Problem Solving, 
where agents cooperate without overlapping functions. 

Each agent relies on a meta-robot that can be connected 
to multiple search engines - like AltaVista, Excite, etc. 
The meta-robot queries the search engines with keywords 
that assure recall for the agent's page class (e.g., the 
terms kcall for papers' and 'call for participation' for the 
CFP agent). Due to the lack of precision, the resultant 

URL set from the queries is characterized by a wide 
variation of functional groups, containing many lists, 
messages, content pages from its class and from others 
agents' classes, and garbage. The retrieved URLs feeds a 
low priority queue of URLs to be processed. 

An agent continuously accesses this queue and another 
one, assigned with high priority, which stores URLs sent 
as recommendations by other agents of the MAS or taken 
from pages considered as lists. These links are considered 
as "hot hints", because they were found under a safer 
context. Therefore, they arc expected to attain higher 
precision. Cooperation pays off if these suggestions 
contain less garbage than search engine results do. 

An agent processes URLs by extracting data and 
classifying the pages functionally and by contents. The 
extracted data and the results arc kept in a database 
together. A mediator facilitates database access to users 
or agents, from the system or external, offering simpler 
reduced non-normalized database views. 

When an agent joins the system, it announces itself and 
sends instances and rules to the other agents. This 
knowledge will be employed by them on the recognition 
of links or pages likely to belong to the page class 
processed by the sender agent. So, the agents update their 
knowledge bases and send to the new agent, in return, 
their own recognition instances and rules. When a link or 
page fires any other agent's recognition rule, the agent 
sends the link or page to that agent as a recommendation. 

3.1 Agents' tasks 
An agent performs the following steps for each URL. 
Figure 2 depicts an agent in detail. 


Figure 2: An agent and its processing steps. 

Validation. Non-html, non-http, inaccessible and pages 
already stored in the database arc ruled out in this step. 

Preprocessing. Representation elements - such as 
contents with and without html tags, centroid, title, links 
and e-mails, among other elements - are generated from 
each valid page applying IR techniques, like stop-lists, 
stemming and tagging. If necessary, shallow natural 

language representations can be used. The representation 
elements are passed to the agent's inference engine. 

Functional classification. An agent deduces to what 
functional group the page fits to, whether a list, message, 
garbage or class member dealt by the agent or by another. 

It is also here that links in a page arc suggested to 
other agents when any of their identification rules about 
the anchor or URL fires. For example, an anchor or URL 
with the word "conference" is useful for a CFP agent. 
Suggestions may trigger scanning on directory structure 
prefixes, like /staff / for a researchers' agent. These 
directories usually keeps plenty of content pages. 

Extraction and contents classification. These two 
tasks arc quite correlated; Riloff [1994] has already 
demonstrated the applicability of extraction for filtering. 
In our architecture, extraction assists classification, since 
each agent manages many subclasses, e.g., the CFP agent 
treats conferences, workshops and journals. After 
categorizing a page according to its semantic contents, 
the remaining slots, which weren't discovered yet, arc 
searched and extracted. In case contradictions or strange 
facts appear during extraction, the functional class 
assigned to the page can be modified. For instance, a 
supposed CFP page is retracted to be a list or garbage, if 
dates farther than a year are met in the top. 

Using as many representation elements as required, 
each slot of data is extracted by a union of templates, 
cases and rules. When a slot is part of a predefined list, it 
is gleaned by matching terms from the dictionaries (e.g. 
checking the presence of acronyms to find US states). 

3.2 Agents' knowledge 
Ontologies play a central role in the architecture, serving 
not only as vocabulary in agent messages, but also 
defining proper concepts, slots, instances, restrictions and 
relations. The architecture encompasses five ontologies: 

Cluster ontology. The main ontology ought to focus 
on the original domain of the cluster (e.g. Science), 
aiming at a rich and comprehensive representation of its 
classes, even if they don't appear in the cluster. There are 
some reasons for this. First, reuse possibilities by any 
application dealing with the domain are enhanced. 
Moreover, it provides a suitable structure to extract slots. 
To sum up, a detailed domain ontology assures 
performance in contents categorization (see section 5.1). 

Web ontology. Classes here contain representations of 
the pages, chosen according to their adequacy in the 
required tasks. They comprise HTTP protocol data, 
Information Retrieval (IR) page representations (such as 
terms and frequencies, contents with and without tags, 
links, e-mails, etc), and Natural Language Processing 
(NLP) representations, suitable for unstructured classes. 

CIG ontology. Operational knowledge classes (and 
instances) for the tasks of functional categorization, 
contents classification and extraction. It includes: 

. Templates to extract slots (e.g. deadline of a CFP), to 
recognize functional categories (such as garbage, messages, 
recommendations, etc), and to classify content pages (e.g. 
conference or workshop CFP); 
. Auxiliary classes like dictionaries concepts (with 
keywords and synonyms), agents and abilities, etc; 
. Functions to determine regions, extract, convert, format, 
check consistency and dismiss extracted data; 
. Complex and expressive cases specifying conditions, 
concepts and sets of slots whose presence or absence 
characterize a content page class. An example below: 
([Articles-with-author-org-and-place] of Case 

This case detects articles. If, in the beginning of a 
page, an author name, an organization and a country (or 
US state) were extracted, terms similar to "abstract" were 
found, but not terms related to the concept "thesis" (like 
"partial fulfillment"), a rule bound to this case is fired. 
The attribute All-of-Concepts-Keywords determines 
whether all the slots have to appear in the page; if it was 
false only one would be necessary. Next, a Class-
Recognizer instance (displayed below) and a rule 
creates an instance of the class associated with the case. 

( [Part-Publication] of Class-Recognizer 
(Cases [Articles-with-author-org-and-place]) 
(Class [Part-Publication])) 


The agents first categorize pages into abstract classes 
(which can't have instances, as, in the example, Part-
Publication), to assign later the exact class the pages 
fit to (e.g. Conference-Article). Cases, recognizers, 
concepts and rules are also applied on slot extraction and 
search of links and URLs to be suggested to other agents. 

Auxiliary ontologies. Comprise ontologies for NLP 
(like WordNet [Miller 1995]), time and places (with 
countries, states, etc), and other specific ontologies from 
other subject areas, which can be helpful for an agent or 
domain (like bibliographic data, for the "articles" agent). 

Indeed, a knowledge-based approach is adequate to 
permit not only high-level intentional cooperation, but 
also massive reuse of code, search engines' services, DB 
definitions - once agents' tables arc the same (pages not 
recognized, dictionaries, search engines and their 
queries), except the particular tablc where an agent stores 
its extracted data (for instance, CFPs to the CFP agent) and 
knowledge. This latter is the most important reuse 
and can happen in several ways. Ontologies can be 
downloaded from repositories, like Ontolingua. A cluster 
ontology serves to all agents in a cluster, as well as most 
knowledge from dictionaries and rules. Thus, a new agent 
is quickly built, needing only new rules and instances of 
concepts, cases and templates, given that knowledge 
acquisition has already been accomplished. 

4 Case Study: the Science domain 

A model of agent was developed in Java, and a mediator, 
in Delphi and HTML. We utilized KQML for agents' 
communication. Ontologies were written using Protege1. 
Rules were specified in the inference engine Jess2. The 
search engines AltaVista, NorthernLight and Goggle 
were queried for retrieval. NLP wasn't implemented yet. 
The data to be extracted was identified to support the 
classifications, but not extracted 

We reused an ontology of science from the project 
(KA)2 [Benjamins et al, 1998], introducing many 
refinements. A key one was the creation of abstract 
classes grouping classes with common features, in order 
to improve classification. For example, class Scientific-
Event derived abstract subclasses Live-Sc-Event - with 
concrete subclasses Conference and Workshop - and ScPublication-
Event, with subclasses Journal and Magazine 

Two agents from the Science cluster were developed: 

CFP Agent. Handles any CFP pages. Extraction 
templates for 21 slots were defined (e.g. Place, Deadline, 
Editor-in-Chief) from 8 scientific events subclasses (the 
ones already cited plus Generic-Live-Sc-Event, GenericSc-
Publication-Event and Special-Issues for Journal and 
Magazine), classified by 28 cases and templates. 

Scientific articles agent. The search engines were 
queried with terms that appear often in articles: 
"abstract", "introduction", "related work", "conclusion", 
"discussion", and others. The agent is responsible for 10 
classes: workshop, conference, journal and magazine 
articles, book chapters and generic articles, as well as 
thesis, dissertations, technical and project reports. 8 slot 
templates, 16 cases and 36 templates were created. 

Three tests were executed with each agent. The first 
two treated carpi obtained from the search engines' 
queries; one was applied for knowledge acquisition, to 
specify the cases, templates and rules, and the other, for 
blind test. The third test was made directly from the Web. 

4.1 Results 
Table 1 displays tests' results. In the table, recognition 
stands for an evaluation if the agent correctly identified 
member pages. Note that a fourth test was run with CFP 
agent, in which it benefited from lists of content pages. 
Lists of CFPs on a focused subject are usually maintained 
by researchers, departments and organizations. 

CFP Agent. More than 70% of content pages were 
conference CFPs, mostly recognized by the presence, in 
the top of the pages, of the concept CFP, and the slots 
initial date and location The few false content pages were 
very long and about communities of users (Linux, XML, 
etc), containing many of the slot triggering concepts. The 
agent failed to recognize CFPs with little data or not 
using the jargon ("deadline", etc). Lists were detected 



when many anchors contained terms related to events, or 
when lots of time intervals in a page (e.g., 1-4 December) 
were present. Special care should be taken to lists, once 
false positives can queue bad links as "hot hints". 


Tabic 1: Agents' performances. 

The lists increased substantially the retrieval of content 
pages. The set of pages retrieved in the fourth test was 
clearly more focused. Pages from other categories, like 
lists, messages and recommendation to the fictitious 
Organizations agent and Dividable-Publication agent 
(process Proceedings), were substituted by content pages 
in the form of frames. In the other tests, only one frame 
appeared. In this test, the number of contents pages rose 
between 13 and 22% (see Figure 3), due to frames pages. 


Even the garbage patterns changed: instead of false 
content pages, caused by search engines' lack of 
precision, garbage were graphical, initial pages of CFPs. 
It is also remarkable that some scientific events found in 
lists were not discovered in the other tests. These facts 
demonstrate that functional categorization, and, in 
particular, the use of lists, pays off. 

Scientific articles agent. Recognition errors 
happened in articles with few data, data in the end, or 
with affiliations in unknown companies. An article must 
have at least three of the slots Organization-Name 

Author-Name, Location, e-mail and Department. The 
agent missed articles containing only the authors' names, 
but this is normal in other similar systems like CiteSeer. 

Contents classification was based on information about 
articles' publication, usually put in the top of papers. 
More than half of the articles didn't bring this data. 

Cooperation. We executed one more test, in which the 
agents cooperated. The CFP agent asked the articles' 
agent for anchors in top of papers containing concepts 
like "conference", "journal", etc. The articles' agent 
transformed these requests into rules to suggest anchors 
to the CFP agent. Although it worked and no suggestion 
was wrong, only three anchors were sent. 

To prove the usefulness of cooperation, the CFP agent 
scanned anchors in Program Committees seeking for 
suggestions to a future "researchers" agent. No extraction 
technique (e.g. proper names) or dictionary was present, 
only simple heuristics. The agent recommended 30 
correct and 7 wrong anchors, a promising rate, given that 
researchers' pages are less structured and, therefore, 
harder to be retrieved by search engines queries. 

5 Related Work and Discussion 

Many subject areas are involved with this project: 
information, retrieval, extraction, classification, multi-
agents, information agents, NLP and ontologies, at least. 
Instead of trying to review these areas, we will focus on 
comparing and discussing solutions similar to ours. 

5.1 WebKB 
This system [Craven et al, 1999] integrates extraction and 
classification as separate tasks, supported by machine 
learning. Pages from computer science departments' sites 
in universities are classified against a domain ontology 
previously defined, with classes and relations. The pages 
were represented using IR techniques. Classification and 
extraction rules were learnt from annotated corpi. 
The decision of relying on machine learning depends 
upon some issues. The first one is a comparison between 
the costs of annotating corpi against inspecting them for 
knowledge acquisition, as we did. ML brings gains such 
as speed and adaptability. However, there are 
drawbacks, like readability and ontological engagement 
of the learned rules (which tends to be quite specific), 
difficulties to include a priori knowledge and to capture 
some rules without adding lots of features, and even to 
generalize through a huge number of features or classes. 
WebKB's ontology was composed of four basic 
classes: activities - with subclasses projects and courses persons 
- student, faculty and staff -, departments and 
other. Relations among classes were defined, as in our 
project: project members, advisors, instructors, etc. 

Discussion. The authors evaluated contents' 
classifications only by the false positives, reporting rates 
between 73 and 83 %, except for the classes "staff 
member" and "other". Nonetheless, if false negatives 
were computed, the class "other" reaches 93,6%, 

"student" 43% and the remaining 6 classes less than 27%, 
lowering the average rate to around 50%. 

Other drawbacks are the huge incidence in the class 
"other", almost three times the sum of the other classes 
together, 65% of them wrong. This class severely 
affected performance, since 90% of the mistakes of the 
tests came from it. Anyway, even if the classification was 
correct, the utility and costs of such system would be 
questionable, since more than 73% of the pages would 
have to be categorized as "other". 

This deluge of garbage pages seems to stem from two 
facts: (a) The absence of functional categorization, and 

(b) the role of ontologies was underestimated in WebKB, 
since its ontology doesn't seem to be comprehensive 
enough. On the contrary, MASTER-Wcb's ontology 
incorporates classes not useful yet, such as projects and 
products, because agents to process these classes can be 
available in the future. Moreover, current agents apply 
these definitions to manage their own classes. 
On the other hand, the learning process may face 
problems on generalization with an ontology with many 
classes. Much more annotated pages would probably be 
needed as well. Therefore, any solution shall rely on 
cooperation, with learning or inference agents. 

5.2 CiteSeer and DEADLINER 
These systems perform efficient retrieval, filtering and 
extraction over the Web, exploiting statistical and 
learning methods joined with a priori knowledge. 

CiteSeer. One of the most accessed software in the 
search of scientific articles [Bollacker et al, 1999]. They 
are found monitoring mailing lists, newsgroups and 
publishers, or querying search engines with keywords 
"papers", "publications" and "postscript". An article is 
recognized by the existence of a reference section. 

The system extracts bibliographic data from each 
article and from its reference section, which plays the 
role of the functional category of list, helping to locate 
other articles. The number of citations of an article by 
others represents a measure of its relevance. Databases of 
authors and journals, as well as complex techniques, are 
applied to identify co-references for authors and articles. 

DEADLINER. This system [Kruger et al 2000] 
searches the Web for scientific events. It scans 
newsgroups and broadcast e-mails, and has its own meta-
robot. From each CFP, it extracts initial and final date, 
deadline, program committee, and affiliation of each 
committee member, themes, events' name and country. 
Extraction is carried out by a sequence of positional 
filters integrated using automatic learning. Extraction 
performance achieves more than 70% for all the slots. 
For program committee extraction, the system benefits 
and updates CiteSeer's researchers' database. 

Recognition performance is similar to ours (more than 
95%), but its definition of event is more restrictive: all 
the slots are required, except country, plus some 
submission information. MASTER-Web's CFP agent 

offers more recall and flexibility, accepting 
announcements of book chapters, magazines, contests, 
workshops and journals. Moreover, the requirements are 
expressed in cases, which arc much more flexible. CFP 
agent was designed bearing in mind that users are 
interested in any CFPs for their research works. 

Discussion. Although developed by the same research 
group, CitcSccr and DEADLINER nowadays are not able 
to cooperate directly. They handle anchors and pages that 
interest each other, only cooperating in the construction 
of the researchers' database. 

This is a common problem for extractor agents on the 
Web: the simplicity, adaptability and speed of wrapping, 
learning and finite-state methods prevent these systems to 
cooperate in an intentional knowledge-based manner. 
Thus, they couldn't ask for data of a certain type, 
employing, as shared vocabulary for communication, 
ontologies about the domain, their abilities, desired 
pages, features among other pieces of knowledge. 

As the number of Web extractors grows, each with its 
robot collector, strain on bandwidth can be. Conversely, 
cooperation could turn out to be even more profitable 
than search engines results, constituting the only starting 
way to look for unstructured classes, like researchers' 
pages. Furthermore, cooperation can take place also at 
the level of data: the CFP agent can warn a researchers' 
agent that a certain researcher was chair of an event, a 
fact that could be missing in her homepage. 

This stems from the knowledge representation. In 
statistical and finite-state approaches, knowledge is 
hidden behind algorithms. It can be neither reasoned nor 
communicated at knowledge level, blocking cooperation 
among extractors and integrated extraction. On the other 
hand, declarative systems can change their behavior 
dynamically in favor of the cluster's processing, 
communicating in an elegant manner. Another advantage 
of these systems resides on the fact that, with an 
approach like CiteSeer and DEADLINER to process a 
new class, most of the new system has to be made from 
scratch. With our architecture, reuse is massive, and only 
part of the knowledge has to be acquired and specified. 

6 Future Work and Conclusions 

This project has a long road ahead. New agents for the 
Science cluster have to be deployed, like a researchers' 
agent, to make cooperation actually effective. We must 
extend our tests to other clusters formed by interrelated 
page classes, such as a tourism cluster - linking events, 
hotels, and transport pages. 

Learning and NLP techniques must be accomodated to 
deal with less structured page classes. The agents should 
also check data duplicity, and documents written in other 
formats (like .ps and XML) must be covered too. 

With this work, we tried to fertilize the field of CIG 
with some new issues and principles, with cooperation as 
the guiding idea: task integration, declarative inferential 
extractors capable of intentional cooperation, a Web 
vision coupling functional and contents aspects, mapping 
of semantic relations among page classes and identification of linkage patterns. A key requirement for CIG systems came up with the tests: a detailed ontology is needed to process the richness of a cluster's domain. On the practical side, we designed a fully reusable cognitive MAS to process data from whole regions of the Web, and got encouraging results from it. We claim that keyword-based search engines can be a basis for more accurate ontology-based domain-restricted cooperative 
information agents in a near future. 

Acknowledgments 

The authors thank Tercio Sampaio and Rafael Teske who 
helped with implementation, and the referees. 
Abstract 

Unlike conventional data or text, Web pages 
typically contain a large amount of information 
that is not part of the main contents of the pages, 
e.g., banner ads, navigation bars, and copyright 
notices. Such irrelevant information (which we 
call Web page noise) in Web pages can seriously 
harm Web mining, e.g., clustering and classification. 
In this paper, we propose a novel feature 
weighting technique to deal with Web page noise 
to enhance Web mining. This method first builds 
a compressed structure tree to capture the common 
structure and comparable blocks in a set of 
Web pages. It then uses an information based 
measure to evaluate the importance of each node 
in the compressed structure tree. Based on the tree 
and its node importance values, our method assigns 
a weight to each word feature in its content 
block. The resulting weights are used in Web 
mining. We evaluated the proposed technique 
with two Web mining tasks, Web page clustering 
and Web page classification. Experimental results 
show that our weighting method is able to dramatically 
improve the mining results. 

1 Introduction 

The rapid expansion of the Internet has made Web a 
popular place for disseminating and collecting information. 
However, useful information on the Web is often accompanied 
by a large amount of noise such as banner ads, 
navigation bars, copyright notices, etc. Although such 
information items are functionally useful for human 
viewers and necessary for Web site owners, they can seriously 
harm automated information collection and mining 
on the Web, e.g., Web page clustering, Web page 
classification, and information retrieval. Web noise can be 
grouped into two categories based on their granularities: 
Global noise: It refers to redundant objects with large 

granularities, which are no smaller than individual pages. 
Global noise includes mirror sites, duplicated Web pages 
and old versioned Web pages to be deleted, etc. 

Local (intra-page) noise: It refers to irrelevant items 
within a Web page. Local noise is usually incoherent 
with the main content of the page. Such noise includes 

banner ads, navigational guides, decoration pictures, etc. 
This work focuses on dealing with local noise in Web 
pages. We call it Web page cleaning. The work is challenging 
because it is a non-trivial task to decide which part 
of a Web page is meaningful and which is noisy. Despite 
its importance, relatively little research has been done on 
Web page cleaning so far (see Section 2). In this paper, we 
propose an effective feature weighting technique to clean 
Web pages with the purpose of improving Web mining. 
Our method does not decide and physically remove those 
noisy blocks of a page. Instead, it assigns low weights to 
the features in those possibly noisy blocks. The advantage 
of our method is that there is no need to use a threshold to 
determine whether a block is noisy or not. Setting a suitable 
threshold is very hard in practice. 

Our cleaning technique is based on the following observation. 
In a typical commercial Web site, Web pages 
tend to follow some fixed layouts and presentation styles 
as most pages are automatically generated. Those parts of 
a page that also appear in many other pages in the site are 
more likely to be the noise, and those parts of the page that 
are quite different from other pages are usually the main 
content of the page. To capture the common structure and 
comparable blocks among Web pages, we introduce the 
compressed structure tree to compress or to merge a set of 
Web pages from a Web site. We then propose a measure 
that is based on information theory [Shannon, 1948] to 
evaluate each element node in the compressed structure 
tree to determine the importance of the node. Based on the 
tree and node importance values, our method assigns a 
weight to each word feature in its content block. These 
feature weights are then directly used in Web mining. 

Our experimental results based on two popular Web 
mining tasks, i.e., Web page clustering and classification, 
demonstrate that the proposed cleaning technique is able 
to boost the mining accuracy significantly. For example, in 
classification, the average classification accuracy over all 
our datasets increases from 0.785 before cleaning to 0.951 
after cleaning. 

2 Related Work 
Although Web page cleaning is an important problem, 
relatively little work has been done to deal with it. In [Lin 
and Ho, 2002], a method is proposed to detect informative 
blocks in Web pages. Their concept of informative blocks 
is similar to our concept of main contents of a page. 
However, the work in [Lin and Ho, 2002] is limited by the 
following two assumptions: (1) the system knows a priori 
how a Web page can be partitioned into coherent content 
blocks; and (2) the system knows a priori which blocks are 
the same blocks in different Web pages. As we will see, 
partitioning a Web page and identifying corresponding 
blocks in different pages are actually two critical problems 
in Web page cleaning. Our system is able to perform these 
tasks automatically. Besides, their work views a Web page 
as a flat collection of blocks, and each block is viewed as a 
collection of words. These assumptions are often true in 
news Web pages, which is the domain of their applications. 
In general, these assumptions are too strong. 

In [Bar-Yossef and Rajagopalan, 2002], Web page 
cleaning is defined as a frequent template detection 
problem. They propose a frequency based algorithm to 
detect templates or patterns. Their work is not concerned 
with the context of a Web site, which can give useful clues 
for page cleaning. Moreover, in their work, the partitioning 
of a Web page is pre-fixed, which is not suitable for a 
Web site because a Web site typically has common layouts 
and presentation patterns. We can exploit these common 
patterns to partition Web pages and to clean them. 

Other related work includes data cleaning for data 
mining and data warehousing (e.g., [Lee, et al, 2000]). 
However, they are only concerned with structured data. 
Web page data are semi-structured, and thus require different 
cleaning techniques. 

Web page cleaning is also related to feature selection in 
traditional text learning (see [Yang and Pedersen, 1997] 
for a survey of feature selection techniques). In feature 
selection, features are individual words or attributes. 
However, items in Web pages have some structures, which 
are reflected by their nested HTML tags. Hence, different 
methods are needed in the context of the Web. 

[Kushmerick, 1999] proposes some learning mechanisms 
to recognize banner ads, redundant and irrelevant 
links of Web pages. However, these techniques are not 
automatic. They require a large set of manually labeled 
training data and also domain knowledge to generate 
classification rules. 

[Kao et al., 2002] enhances the HITS algorithm of 
[Kleinberg, 1998] by using the entropy of anchor text to 
evaluate the importance of links. It focuses on improving 
HITS in order to find informative structures in Web sites. 
Although it segments Web pages into content blocks to 
avoid unnecessary authority and hub propagations, it does 
not detect or eliminate noisy contents in Web pages. 

Our work is also related to segmentation of text documents, 
which has been studied extensively in information 
retrieval. Existing techniques roughly fall into two categories: 
lexical cohesion methods [Beeferman et a/., 1997; 
Eichman et al., 1999; Kaufmann, 1999; Reynar, 1999] and 
multi-source methods [Allan et al., 1998; Beeferman et al., 

1999]. The former identifies coherent blocks of text with 
similar vocabulary. The latter combines lexical cohesion 
with other indicators of topic shift, such as relative performance 
of two statistical language models and cue 
words. In [Hearst and Plaunt, 1993], Hearst and Plaunt 
discussed the merits of imposing structure on full-length 
text documents and reported good results of using local 
structures for information retrieval. Instead of using pure 
text, which is unstructured, we process semi-structured 
data. We can make use of the semi-structures present in 
Web pages to help our cleaning task. 

Finally, our feature weighting method is different from 
feature weighting methods used in information retrieval. 
One of the popular methods used in text information retrieval 
for feature weighting is the TFIDF scheme [Salton 
and McGill, 1983; Baeza-Yates and Bibeiro-Neto, 1999]. 
This scheme is based on individual word (feature) counts 
within a page and among all the pages. It is, however, not 
suitable for Web pages because it does not consider Web 
page structures in determining the importance of each 
content block and consequently the importance of each 
word feature in the block. For example, a word in a 
navigation bar is usually noisy, while the same word occurring 
in the main part of the page can be very important. 

3 The Proposed Technique 

The proposed cleaning technique is based on analysis of 
both layouts (structures) and contents of the Web pages in 
a given Web site. Thus, our first task is to find a suitable 
data structure to capture and to represent common layouts 
or presentation styles in a set of pages of the Web site. We 
propose the compressed structure tree (CST) for this purpose. 
Below, we start by giving an overview of the DOM 
(Document Object Model)1 tree and showing that it is 
insufficient for our task. We then present the compressed 
structure tree, which is followed by our entropy measures 
for evaluating the nodes in the compressed structure tree 
for noise detection. 

3.1 DOM tree 
Each HTML page corresponds to a DOM tree where tags 
are internal nodes and the actual text, images or hyperlinks 
are the leaf nodes. Figure 1 shows a segment of HTML 
codes and its corresponding DOM tree. In the DOM tree, 
each solid rectangle is a tag node. The shaded box is the 
actual content of the node, e.g., for the tag IMG, the actual 
content is "src=image.gif'' The order of child tag nodes is 
from left to right. Notice that our study of HTML Web 
pages begins from the BODY tag nodes since all the 
viewable parts are within the scope of BODY. Each tag 
node is also attached with the display properties of the tag. 
Display properties defined by CSS (Cascading Style Sheet) 
2 are treated as normal display properties if CSS is available 
in the page source (i.e., if CSS is not external). For 
convenience of analysis, we add a virtual root node without any attribute as the parent tag node of BODY tag . STYLEs is the set of presentation styles merged into 
node in each DOM tree. E. 

. 
CHILDs is the set of pointers pointing to the child 
element nodes of E in CST. 
Figure 1: A DOM tree example (lower level tags are omitted) 
Although a DOM tree is sufficient for representing the 
structure of one HTML page and it has been used in many 
applications (e.g., [Chakrabarti et al., 2001]), it cannot 
represent the common structure of a set of Web pages. We 
aim to compress individual DOM trees of Web pages into 
a single tree (compressed structure tree) which captures 
the commonalities of the pages in a Web site. Based on 
this tree, we can easily study some statistical properties of 
the structures and the contents of the Web pages. 

3.2 Compressed structure tree (CST) 
Before defining the compressed structure tree, we first 
define the presentation style of a tag node in a DOM tree. 

An element node (the basic information unit) of a 
compressed structure tree (CST) is defined as follows: 

Definition (element node): An element node E represents 
a set of merged tag nodes in the DOM tree. It has 5 
components, denoted by (Tag, Attr, TAGs, STYLEs, 
CHILDs), where 

. 
Tag is the tag name; 
. 
Attr is the set of display attributes of Tag. 
. 
TAGs is the set of actual tag nodes in the original 
DOM trees that are compressed (or merged) in E. 
Figure 2: DOM trees and the compressed structure tree (lower 
levels of trees are omitted) 

An example of compressed structure tree is given in Figure 
2, which compresses the DOM trees d1 and d2- We observe 
that, except for the tag node SPAN, all the tag nodes in d\ 
are merged with corresponding tag nodes in d2. So, an 
element node in CST actually denotes a set of merged tag 
nodes, which represents a set of logically comparable 
blocks in different DOM trees. 

Tag nodes in different DOM trees cannot be merged 
randomly. We need to ensure that the merged tag nodes are 
the same logical blocks in different Web pages. We build a 
CST of a set of Web pages from a Web site by merging 
their DOM trees from top to bottom in following steps: 

1. 
All root tag nodes of the DOM trees are merged to form 
the first (the top most) element node of the CST. We 
have Tag = root, Attr- {}, and TAGs being the set of all 
the root tag nodes in the DOM trees of the Web pages. 
2. 
We compute STYLEs of the element node E passed 
from the previous step. E.STYLEs is the set of presentation 
styles of all the tag nodes in the DOM trees 
covered by E. Note that common presentation styles are 
combined. 

4. If no child element node is created in step 3, stop; else 
for each child element node from step 3, goto step 2. 
After the CST is built, it is used to compute a weight for 
each word feature in each block of a Web page. 

3.3 Weighting policy 
Intuitively, if an element node contains many different 
presentation styles, it is more likely to be important and 
hence should be assigned a high weight. Otherwise, it will 
be assigned a low weight, i.e., it is more likely to be noisy. 
We use the entropy of presentation styles to encode the 
importance of an element node E in the CST. 

Definition: For an internal element node E in CST, let / = 
\E.STYLEs\ and m = IE.TAGs\. The importance of E, 
denoted by Nodelmp(E), is 

where p, is the probability that a tag node in E. TAGs uses 
the ith presentation style. 

Nodelmp(E) only evaluates local importance of E. In order 
to weigh a feature contained in a leaf node of a CST, we 
use the cumulative importance of the path from root to the 
node containing the feature. We call this cumulative importance 
of E the path importance, denoted by Pathlmp(E). 
PathImp(E) measures the importance of the structure from 
root to E in CST. 

Since the path importance is a cumulative importance 
value, it should meet the following requirement: 

. For any two element nodes E\ and E2 in CST, if E\ is an 
Definition: For an element node E in a CST, the path 
importance of E, denoted by Pathlmp(E), is: 

Our weighting policy considers both the structure and 
content context of the features. The weight of each feature 
in a particular block (or under a particular tag) of the Web 
page is computed as follow. 

In our implementation, we do not use the actual leaf tag 
nodes in a page as they overly fragment the page. Instead, 
we use the grandparent tag nodes of the actual leaf tag 
nodes in the DOM tree as the (virtual) leaf tag nodes in 
building CST, and in computing feature weights. 

After a weight to each feature in every block of a Web 
page is assigned, we add the weights of the same feature in 
the page. All the feature weights of the page together form 
a feature vector of the page, which is used as input to Web 
mining, e.g., clustering and classification. 

4 Empirical Evaluation 

This section evaluates the proposed cleaning technique. 
Since the purpose of our data cleaning is to improve Web 
mining, we performed two mining tasks, i.e., clustering 
and classification, to test our system. By comparing the 
mining results before cleaning and after cleaning, we show 
that our Web cleaning technique is able to boost mining 
accuracy dramatically. 

Below, we first describe the datasets used in our experiments 
and the evaluation measures. We then present 
our experimental results of clustering and classification. 

4.1 Datasets and evaluation measures 

Figure 3: The distribution 
suits (in both clustering and classification). 
All the five Web sites contain Web pages of many 
categories or classes of products. We choose the Web 
pages that focus on the following categories of products: 
Digital Camera, Notebook, TV, Printer and Mobile Phone. 
Table 1 lists the number of documents that we downloaded 


Table 1: Web pages and their classes from the 5 sites 

from each Web site, and their corresponding classes. 

Since we test our system using clustering and classification, 
we use the popular F score measure in information 
retrieval to evaluate the results before and after cleaning. F 
score is defined as follows: 


where P is the precision and r is the recall. F score measures 
the performance of a system on a particular class, and 
it reflects the average effect of both precision and recall. 
We will also include the accuracy results for classification. 

4.2 Experimental results 
We now report our experimental results before and after 
cleaning. Cleaning is done on all the Web pages from each 
Web site separately using the proposed technique. 

Clustering 
For clustering, we use the popular k-means algorithm 

of F scores of clustering 
[Anderberg, 1973]. We put all the 5 categories of Web 
pages into a big set, and use the K-means clustering algorithm 
to cluster them into 5 clusters. Since the K-means 
algorithm selects the initial seeds randomly, we performed 
a large number of experiments (800) to show the behaviors 
of K-means before and after page cleaning. The F score for 
each of the 800 runs is drawn in Figure 3, where X-axis 
shows the experiment number and Y-axis shows the F 
score. F score for each run is the average value of the 5 
classes, which is computed as follow: By comparing the 
pages' original classes and the clustering results, we find 
the optimal assignment of class to each cluster that gives 
the best average F score for the 5 classes. 
From Figure 3, we can clearly observe that after cleaning 
the results are drastically better. Over the 800 runs, the 
average F score for the noise case is 0.506, while the average 
F score for the clean case is 0.756, which is a 
remarkable improvement. 
After cleaning, 84.38% of the 800 results have F scores 
higher than 0.7, and only 4.63% lower than 0.5. Before 
cleaning, only 0.5% of the 800 results (4 out of 800) have 
F scores higher than 0.7, and 47.63% lower than 0.5. Thus, 
we can safely conclude that our feature weighting technique 
is highly effective for clustering. 

Classification 
For classification, we use Support Vector Machine 
(SVM), which has been shown to perform very well for 
text classification by many researchers (e.g., [Joachims 
1997]). We used the SVMlight package [Joachims, 1999] 
for all our experiments. 
In each experiment, we build classifiers based on two 
classes of pages from different Web sites. For example, the 
two classes can be camera pages from Amazon and 

notebook pages from CNet. Then we use the classifier to 
classify the combined set of camera and notebook pages 
from the other Web sites. We have experimented with all 
possible two class combinations of the 5 sites. Table 2 
gives the averaged F scores and accuracies before and after 
cleaning. In the table, F(N) and A(N) stand for F score and 
accuracy before cleaning respectively, while F(C) and 
A(C) stand for F score and accuracy after cleaning. From 
the results, we can see that after cleaning, the classifiers 
perform much better. On average over all the experiments, 
both the F score and accuracy improve by a large margin. 

5 Conclusions 

In this paper, we proposed an effective technique to clean 
Web pages for Web mining. Observing that Web pages in 
a Web site usually share some common layouts and presentation 
styles, we propose a new tree structure, called 
compressed structure tree to concisely capture the commonalities 
of a Web site. This tree provides us with rich 
information for analyzing both the structure and the contents 
of the Web pages. An information based measure is 
also proposed to evaluate each tree node to determine its 
importance, which is then used to assign weights to features 
in their corresponding Web pages. Using the resulting 
feature weights, we can perform Web mining tasks. 
Experimental results with two popular Web mining tasks 
show that the proposed technique is highly effective. 


Abstract 

This paper describes a jam session system that 
enables a human player to interplay with virtual 
players which can imitate the player personality 
models of various human players. Previous systems 
have parameters that allow some alteration 
in the way virtual players react, but these systems 
cannot imitate human personalities. Our system 
can obtain three kinds of player personality 
models from a MIDI recording of a session in 
which that player participated - a reaction model, 
a phrase model, and a groove model. The reaction 
model is the characteristic way that a player reacts 
to other players, and it can be statistically learned 
from the relationship between the MIDI data of 
music the player listens to and the MIDI data of 
music improvised by that player. The phrase 
model is a set of player's characteristic phrases; it 
can be acquired through musical segmentation of 
a MIDI session recording by using Voronoi diagrams 
on a piano-roll. The groove model is a 
model that generates onset time deviation; it can 
be acquired by using a hidden Markov model. 
Experimental results show that the personality 
models of any player participating in a guitar trio 
session can be derived from a MIDI recording of 
that session. 

1 Introduction 

Our goal is to create a jam session system in which virtual 
players react as if they were actual human players. We 
want to make it possible for a human player to interact, 
whenever they like, with a virtual player that can imitate 
whoever the human player wishes to perform, for example, 
with a familiar, professional, or deceased player, or even 
with themselves. What is most important in imitating 
players is to acquire the player's personality models of a 
target human player. 

Previous session systems have not been able to imitate a 
human player's personality. Some systems [Aono et al., 
1995] have been designed to follow the performance of a 
human soloist, but without considering the individual 
character of the virtual player. Although JASPER [Wake 

et al., 1994] has a set of rules that determine the system's 
reactions and VirJa Session [Goto et al., 1996] has parameters 
for altering how it reacts, these systems cannot 
develop player personality models of an actual human 
player. 

To realistically imitate a human player, a system must 
be able to acquire player personality models of that player. 
The imitating virtual player can then improvise according 
to the models. The Neuro-Musician [Nishijima and Kijima, 
1989; Nishijima and Watanabe, 1992] can learn the relationship 
between 30 sets of an 8-bar-length input pattern 
and an output pattern by using neural networks. However, 
it is only capable of dealing with the limited style of a jam 
session where a solo part must be changed in 8-bar rotation. 
In other words, a virtual player and a human player cannot 
both play a solo part in the same time. Moreover, the 
Neuro-Musician must prepare a training set of 
8-bar-length input-output data to enable neural network 
learning. In an actual jam session, a player does not always 
play an 8-bar solo to the 8-bar solo of the other players. 
Therefore, we cannot acquire the player models from a 
MIDI session recording by using the Neuro-Musician 
method. 

On the other hand, the Band-OUT-of-a-Box (BoB), 
which deals with a problem similar to ours [Thorn, 2001a; 
Thorn, 2001b], indicates that machine learning techniques 
provide a useful approach to acquire a player's models. 
However, BoB can only react to a human performance of 
an immediately previous four bars. It has a fixed relationship 
in which the human player is the leader and the 
virtual player is the follower. 

Our jam system allows us to acquire player personality 
models of a target human player from the MIDI recording 
of a session in which that player participated. The main 
advantage of our approach is that we do not have to directly 
evaluate the target player: all we need to build the 
models is session recording data. 

2 A Guitar Trio Session System 

Our system deals with constant-tempo 12-bar blues performed 
by a guitar trio. Figure 1 shows a jam session 

MIDI stands for Musical Instrument Digital Interface. 

model in which either a human or the computer can be 
selected to perform the part of each player. We can 
imagine sessions in which all players will be human 
players just as we can imagine sessions in which all 
players are computer players. The three players take the 
solo part one after another without a fixed leader-follower 
relationship. 

We obtain three kinds of player personality model from a 
MIDI session recording -a reaction model, a phrase 
model, and a groove model. 

The system has two modes, a learning mode and a session 
mode. In the learning mode (discussed in Sections 3,4, 
and 5), the system acquires player personality models in 
non-real time. These models are stored in a database and 
different personality models can be assigned to the two 
virtual players before session play (Figure 2). In the session 
mode (discussed in Section 6), a human player can 
interact with the virtual players in real time. 


3 Learning a reaction model 

A reaction model is the characteristic way that a player 
reacts to other players. Acquiring an actual player's individual 
reaction model is necessary to create a virtual 
player that reacts as that actual human player does. As 
shown in Figure 3, each virtual player listens to the performances 
of all the players (including its own) and uses 
the reaction model to determine what its next reaction 
(output performance) will be. The main issue in deriving 


the reaction model is to learn the relationship between the 
input and the output of the target player in MIDI recordings. 
This can be formulated as a problem of obtaining 
a mapping from the input to the target player's output. 
However the direct MIDI-level learning of this mapping is 
too difficult because the same MIDI-level sequence rarely 
occurs more than once and the mapping itself is too sparse. 
We therefore have introduced two intermediate subjective 
spaces: an impression space and an intention space (Figure 
4). 

3.1 Impression space 
The impression space represents the subjective impression 
derived from the MIDI input. By applying principal 
components analysis (PCA) to the results of subjective 
evaluations of various MIDI performances, we determined 
three coordinate axes for the impression space. PCA is a 
statistical method for reducing the number of dimensions 
while capturing the major variances within a large data set. 
While listening to a performance, a subject subjectively 
evaluated it by using ten impression words to rank the 
performance's impression on a scale of one to seven. The 
three selected axes of the impression space represent 
qualities that can be described as appealing, energetic, and 
heavy. To obtain a vector in this space, an impression 
vector corresponding to the MIDI input, we use canonical 
correlation analysis (CCA). This analysis maximizes the 
correlation between various low-level features of the 
MIDI input (such as pitch, note counts, tensions, and pitch 
bend) and the corresponding subjective evaluation. Since 
an impression vector is obtained from an individual 
player's performance, we have at every moment three 
impression vectors (Figure 4). 
The impression space is necessary for learning the relationship 
between various input performances and the 
corresponding output performances. If we represent the 
input performances as short MIDI segments without using 
the impression space, the same MIDI segments will not be 
repeated in different sessions. The impression space enables 
the abstracting of subjective impressions from input 
MIDI data and those impressions can be repeated. Even if 
two segments of the input MIDI data differ, they can be 
represented as a similar vector in the impression space as 
long as they give the same impression. 

Figure 4: Player architecture. 

Figure 5 shows the transition of the rated values for the 
impression word "appealing" The black line represents 
the value calculated by the system and the gray line 
represents the value as evaluated by a human listener. For 
92 percent of the performance, the calculated and subjectively 
evaluated values do not differ by more than 1. 


3.2 Intention space 
The intention space represents the intention of the player 
improvising the output. A vector in this space, an intention 
vector, determines the feeling of the next output. It is used 
to select short MIDI phrases from a phrase database, and 
connecting the selected phrases generates the output MIDI 
performance. 
Without the intention space, learning the relationship 
between impression vectors and output MIDI data is difficult 
because in actual MIDI recordings various outputs 
can occur when the input data gives the same impression. 
The intention space makes it easier to learn the player's 
reaction model. 
The intention space is constructed by using 
multidimensional scaling (MDS) [Kruskal and Wish, 
1978] such that intention vectors are distributed with 
proximities proportional to subjective similarities of 
short phrases corresponding to those vectors. Based on 
MDS results, we determined the three dimensions of this 

we determined the three dimensions of this space. Because 
the number of the short phrases is limited, those 
phrases are sparsely placed in the intention space. When 
generating the output, the system selects the output phrase 
close to the determined intention vector: an appropriate 
phrase can be selected even if the phrase database does not 
have a phrase that is exactly placed on the intention vector. 

3.3 Acquiring a reaction model 
We can regard the mapping from the impression space to 
the intention space as the reaction model. To derive this 
mapping function statistically, we obtain various training 
sets from the target session recordings. These sets are pairs 
of impression vectors obtained from the three players 
during a sequence of the past twelve bars and the corresponding 
next intention vector. For this learning we use 
Gaussian radial basis function (RBF) networks [Chen et 
al., 1991]. The RBF networks have one hidden layer with 
nonlinear inputs, and each node in the hidden layer computes 
the distance between the input vector and the center 
of the corresponding radial basis function. 
The RBF networks have good generalization ability and 
can learn whichever nonlinear mapping function we are 
dealing with. 

4 Learning a phrase model 

A phrase model is a set of player's characteristic phrases. 
To create a virtual player that performs using phrases as an 
actual human player does, acquiring the actual player's 
individual phrase model is necessary. This can be done 
through musical segmentation of a MIDI session recording 
(Figure 6). 


Two kinds of grouping appear in polyphony, one in the 
direction of pitch interval and the other in the direction of 
time. Grouping in the pitch interval direction divides polyphony 
into multiple homophony (Figure 7a). In the time 
direction, notes are grouped from time gap to time gap 
(Figure 7b). 

To segment a MIDI session recording into phrases, we 
need to automatically divide the polyphony notes into 
groups. The generative theory of tonal music (GTTM) 
[Lerdahl and Jackendoff, 1983] includes a grouping concept, 
and thus can be used to derive a set of rules for the 
division of notes into groups. We think that GTTM is the 
most promising theory of music in terms of computer 
implementation; however, no strict order exists for applying 
the rules of GTTM. This may lead to ambiguities in 
terms of analysis results. The implementation of GTTM as 
a computer system has been attempted [Ida et al., 2001], 
but the resulting system was only capable of dealing with a 
limited polyphony made up of two monophonies. 

In this paper, we propose a method of grouping based on 
applying the Voronoi diagram. We have developed a 
method of grouping rather than naively implementing 
GTTM so that a result obtained using our method is equifinal 
to one obtained with the GTTM approach. We 

a: Grouping in pilch interval direction. 
b: Grouping in time direction. 
Figure 7: Examples of grouping. 

compare the results of grouping by our method with the 
results of grouping by a human according to the GTTM. 

4.1 Generative Theory of Tonal Music 
The generative theory of tonal music is composed of four 
modules, each of which is assigned to a separate part of 
the structural description of a listener's understanding of 
music. The four GTTM modules are the grouping 
structure, the metrical structure, the time-span reduction, 
and the prolongational reduction. 
The grouping structure is intended to formalize the 
intuition that tonal music is organized into groups, which 
are in turn composed of subgroups. There are two kinds 
of rules for GTTM grouping: grouping well-formedness 
rules and grouping preference rules. Grouping 
well-formedness rules are necessary conditions for the 
assignment of a grouping structure and restrictions on the 
generated structures. When more than one structure may 
satisfy the grouping well-formedness rules, grouping 
preference rules only suggest the superiority of one 
structure over another; they do not represent a deterministic 
procedure. This can lead to the problem of ambiguity 
mentioned above. 

4.2 Use of Voronoi diagrams for grouping 
To overcome the ambiguity problem, we propose a 
method of grouping based on the use of Voronoi diagrams. 
The GTTM result is a binary tree that indicates the 
hierarchical structure of a piece of music. In our method, 
Voronoi diagrams on a piano-roll represent the hierarchical 
structure of a piece of music. 


We can form the Voronoi diagram for a given set of 
points in a plane as a connected set of segments of 
half-plane boundaries, where each of the half-planes is 
formed by partitioning the plane into two half-planes, one 
on either side of the bisector of the line between each 
adjacent pair/?, and Pj,. 

ART AND CREATIVITY 


Voronoi diagram for two notes 
Our method uses the piano-roll format as a score, and thus 
notes are expressed as horizontal line segments on a piano-
roll. To construct a Voronoi diagram on the score, we 
need to consider the Voronoi diagram for multiple horizontal 
line segments, which will be constructed of linear 
segments and quadratic segments. 
When two notes sound at the same time or no note 
sounds, the corresponding part of the Voronoi diagram is a 
linear segment (Figures. 9a and 9c). When a single note 
sounds, the Voronoi diagram becomes a quadratic segment 
(Figure 9b). 


Voronoi diagram for more than two notes 
To construct a Voronoi diagram for more than two notes, 
we construct the Voronoi diagrams for all note pairs and 
delete the irrelevant segments. For example, to construct a 
Voronoi diagram for notes a, b, and c, we construct three 
Voronoi diagrams (Figure 10). The boundaries in the three 
diagrams then intersect at a point that is equidistant from 
each note. The Voronoi diagram for notes a and b is divided 
into two half-lines at the intersection. We then delete 
the half-line that is closer to c than to a or b. 


4.3 Making groups 
Hierarchical groups of notes were constructed by converging 
adjacent notes iteratively. Here, we introduce the 
following principle for making groups; the smallest Voronoi 
cell is first merged to an adjacent group. 

We have implemented our grouping method (Figure 
1 la) and have compared the results with correct data obtained 
by a human (Figure 11b). We evaluated grouping 
performance in terms of a correctness rate defined as 

The number of notes grouped correctly 

Correctness rate = -\A1\AA \A1\AA (5) 

The number of notes 
When wc ran the program, we calculated that the correctness 
rate was 78.5 percent. The tune used as MIDI data 
in this experiment was the Turkish March. 

a. Result obtained using our method. 
b. Result obtained using GTTM. 
Figure 11: Results obtained using our method and GTTM. 

5 Learning a groove model 

A groove model is a model generating the deviation of on 
set times. Acquiring an actual player's individual groove 
model is necessary to create a virtual player that performs 
with musical expression as that human player does. A 
human player, even when repeating a given phrase on a 
MIDl-equipped instrument, rarely produces exactly the 
same sequence of onset notes because the onset times 
deviate according to performer's actions and expressions. 
We can model the process generating that deviation by 
using a probabilistic model. 

5.1 Formulation of the hidden Markov models 
Let a sequence of intended (normalized) onset times be 0 
and a sequence of performed onset times (with deviation) 
be y. Then, a model for generating the deviation of onset 
times can be expressed by a conditional probability 
P (y \0) (Figure 12). Using this conditional probability and 
the prior probability P (0) can be formulated as a hidden 
Markov model (HMM), which is a probabilistic model that 
generates a transition sequence of hidden states as a 
Markov chain. Each hidden state in the state transition 
sequence then generates an observation value according to 
an observation probability. 

Modeling of performance 

Target in modeling 

We modeled the onset time of a musical note (i.e. the start 

time of the note) and introduced a new model of distribution 
of onset times. While the duration- time-based model 

used in [Otsuki et al., 2002] is limited, our onset-
time-based model is suitable for treating polyphonic 
performances, such as those including two-hand piano 
voicing and guitar arpeggio. 

Unit in modeling 
We use a quarter note (beat) as the unit of each HMM: the 
temporal length corresponding to each HMM is a quarter 
note. The reason we use the quarter-note unit is to distinguish 
between eighth triplets and sixteenth notes within 
the scope of quarter notes. The three notes of eighth triplets 
are located on three equi-divided positions in a quarter 
note, while the four sixteenth notes are located on four 
equi-divided positions in a quarter note. An actual per


formance consisting of a sequence of quarter notes can be 
modeled by concatenating the quarter-note-length HMMs. 
This quarter-note modeling has the advantages of reducing 
calculation time and facilitating the preparation of the 
large data sets used for training the model. 

Unit of quantization 
We introduce two different discrete temporal indices, k 
and i. The unit of k is a quantization unit to describe performed 
onset time and is 1/480 of a quarter note, which is 
often used in commercial sequencing software. The i unit 
is a quantization unit to describe the intended onset time 
and is one-twelfth of a quarter note. It can describe both 
eighth triplets and sixteenth notes. 

Quarter-note hidden Markov model 
Figure 13 shows the HMM used in our study. We model a 
sequence of onset times within a quarter note (beat) by 
using the HMM. All the hidden states of the HMM correspond 
to possible positions of intended onset times, and 
an observed value that comes from a hidden state corresponds 
to a performed onset time with deviation. Onset 
times in a beat are quantized into 12 positions for hidden 
states and into 480 positions for observation values. That 
is, each component of the HMM is interpreted as follows. 
Hidden state /: intended onset time. (i= 1, ..., 12) 
Observation k: performed onset time. (k= 1, ..., 480) 
Transition probability aij: probability that intended onset 
time j follows intended onset time i. 
Observation probability b,(k)\ probability that performed 
onset time is k and intended onset time is i. 
A state-transition sequence begins with a dummy state 
"Start" and ends with a state "End" (Figure 14). 

Figure 14: Simple example of state sequences. 

5.2 Learning model parameters 
The HMM parameters aij and b,(k) were derived from a 
MIDI session recording by using the Baum-Welch algorithm. 
Figure 15 shows a b,(k) distribution obtained from a 
human performance in a MIDI session recording. 


6 Session mode 

Using the personality models acquired in the learning 
mode, each virtual player improvises while reacting to the 
human player and the other virtual player. The processing 
flow of each virtual player can be summarized as follows: 

1. The low-level features of the MIDI performances of all 
the players are calculated at every 1/48 bar. 
2. At every 1/12 bar, the three impression vectors are 
obtained from the low-level features. 
3. At the beginning of every bar, the intention vector is 
determined by feeding the reaction model the past impression 
vectors. 
4. The output performance is generated by connecting 
short phrases selected from a phrase-model database. Each 
phrase is selected, according to the determined intention 
vector, by considering its fitness for the chord progression. 
A virtual player can start a solo performance at any bar. 
5. The deviation of the onset times is generated according 
to the groove model. 
Note that the reaction model can predict the next intention 
vector from the impression vectors gathered during the 
past twelve bars in real time: a virtual player thus does not 
fall behind the other players. 
7 Experimental Results 

We have implemented the proposed system on a personal 
computer (with a Pentium IV 2.8GHz processor); Figure 

16 shows a screen snapshot of the system output. As 
shown, there are three columns (called player panels), 
each corresponding to a different player. The toggle switch 
on the top of each panel indicates whether the panel is for a 
virtual player or a human player, and each panel contains 
two boxes representing three-dimensional spaces: the 
upper box is the impression space and the lower box is the 
intention space. The sphere in each box indicates the 
current value of the impression or intention vector. 

In our experiments, after recording a session performance 
of three human guitarists playing MIDI guitars, we 
first made the system learn the reaction model, the phrase 
model and the groove model of each. We used a metronome 
sound to maintain the tempo (120 MM) when recording, 
and the total length of this recording session was 
144 bars. We then let five human guitarists individually 
use the system in session mode. The system indeed enabled 
each human guitarist to interact with two virtual 
guitarists, each with a different reaction model. 

To find out how well a virtual player could imitate a 
human player, we asked a human player to perform with 
virtual player A imitating him and with virtual player B 
imitating a different player. The human player and the 
virtual player imitating him tended to take a solo at almost 
the same time and to perform phrases that felt similar. 
Figure 17 shows the transition of intention vectors of three 
players during 48 bars where the intention vectors of the 
virtual player A and the human player are particularly 
similar. Examining all the values of the intention vectors 
during the session, we compared the distances between the 
intention vectors of the virtual players and the human 
player. Over 144 bars the average distance between the 
intention vectors of the human player and the virtual 
player imitating him was significantly smaller than that 
between the intention vectors of the human player and the 
virtual player imitating a different player. We think that 
this means the virtual player's RBF networks actually 
predicted the human player's intention. 

We also tested whether a virtual player could imitate the 
target human player by applying the Turing test format. As 
subjects, we used three guitarists (A, B, and C) who had 
played in the same band for more than a year and so un-


Figure 16: Screen snapshot of system output. 

derstood each other's musical personalities. We prepared a 
reaction model, a phrase model, and a groove model for 
each of these subjects. We then used different personality 
models to prepare 27 (= 3 players A 3 models) kinds of 
virtual player. The subjects evaluated each virtual player 
with regard to whose models it was based upon. Subjects 
were told in advance that the player was a virtual player 
imitating either player A, B, or C. We found that a virtual 
player having the three personality models acquired from 
the same human player was correctly identified as such. 
We calculated that the success rate was 100 percent. The 
subjects could thus distinguish when the virtual player was 
based on the personality models from one human player or 
from multiple players. 

Furthermore, five guitarists who performed with the 
system remarked that each virtual player performed 
characteristically. In particular, a human player who participated 
in a jam session with a virtual player that was 
imitating him remarked that he was uncomfortable playing 
with that virtual player because he felt that he was being 
mimicked. These results show that our system successfully 
derived and applied personality models from MIDI session 
recordings. 

8 Conclusion 

We have described a jam session system in which a human 
player and virtual players interact with each other. The 
system is based on the learning of three types of personality 
model - the reaction, the phrase, and the groove 
models. Our experimental results show that our system can 
imitate the musical personalities of human players. We 
plan to extend the system so that it can be applied to other 
musical instruments, such as piano and drums. 

Acknowledgments 

We thank Daisuke Hashimoto, Shinichi Chida, and Yasuhiro 
Saita, who cooperated with the experiment as 
players. 

Abstract 

Society needs humor, not just for entertainment. In 
the Web age, presentations become more and more 
flexible and personalized and they will require humor 
contributions for electronic commerce developments 
(e.g. product promotion, getting selective 
attention, help in memorizing names, etc...) more 
or less as it happened in the world of broadcasted 
advertisement. Even if deep modeling of humor in 
all of its facets is not something for the near future, 
there is something concrete that has been achieved 
and that can help in providing attention to the field. 
The paper refers to the results of HAHACRONYM, 
a project devoted to humorous acronym production, 
a circumscribed task that nonetheless requires various 
generic components. The project opens the way 
to developments for creative language, with applications 
in the world of advertisement. 

1 Introduction 

Future human-machine interaction, according to a 
widespread view, will emphasize naturalness and effectiveness 
and hence the incorporation of models of possibly 
all human cognitive capabilities, including the handling of 
humor. There are many practical settings where computational 
humor will add value. Among them: business world 
applications (e-commerce to name one), general computer-
mediated communication and human-computer interaction 
[Morkes et al, 1999], educational and edutainment systems. 
There are important prospects for humor also in automatic 
information presentation. In the Web age presentations 
become more and more flexible and personalized and they 
will require humor contributions for electronic commerce 
developments (e.g. product promotion, getting selective 
attention, help in memorizing names, etc...) more or less as 
it happened in the world of broadcasted advertisement. 

Yet deep modeling of humor in all of its facets is not something 
for the near future; the phenomena are too complex, 
humor is one of the most sophisticated forms of human intelligence. 
It is Al-complete: the problem of modeling it is 
as difficult to solve as the most difficult Artificial Intelligence 
problems. But some steps can be followed to achieve results. 
In the general case, in order to be successfully humorous, 

a computational system should be able to: recognize situations 
appropriate for humor; choose a suitable kind of humor 
for the situation; generate an appropriately humorous output; 
and, if there is some form of interaction or control, evaluate 
the feedback. 

We are concerned with systems that automatically produce 
humorous output (rather than systems that appreciate humor). 
Some of the fundamental competencies are within the range 
of the state of the art of natural language processing. In one 
form or in another humor is most often based on some form 
of incongruity. For verbal humor this means that different interpretations 
of utterances must be possible (and must not be 
detected before the culmination of the humorous process) or 
must cause perception of specific forms of opposition. Natural 
language processing research has often dealt with ambiguity 
in language. A common view is that ambiguity is an 
obstacle for deep comprehension. Most current text processing 
systems attempt to reduce the number of possible interpretations 
of the sentences, and a failure to do so is seen as 
a weakness of the system. From a different point of view, 
the potential for ambiguity can be seen as a positive feature 
of natural language. Metaphors, idioms, poetic language and 
humor use all the multiple senses of texts to suggest connections 
between concepts that cannot, or should not, be stated 
explicitly. 

The work presented here is based on various resources for 
natural language processing, adapted for humor. It is a small 
step, but aiming at an appreciable concrete result. It has 
been developed within HAHACRONYM1, a project devoted 
to computational humor. A visible and evaluable result was 
at the basis of the deal. We proposed a situation that is of 
practical interest, where there is no domain restriction and 
many components are present, but simpler than in more extended 
scenarios. The goal is a system that makes fun of existing 
acronyms, or, starting from concepts provided by the 
user, produces a new acronym, constrained to be a word of 
the given language. And, of course, it has to be funny. 

The project was meant to convince about the potential of 
computational humor, through the demonstration of a working 
prototype and an assessment of the state of the art and 

1European Project IST-2000-30039. HAHACRONYM has been 
the first EU project devoted to computational humor. The consortium 
included ITC-irst, as coordinator, and the University of Twente. 
See http://haha.itc.it. 

of scenarios where humor can add something to existing information 
technologies. The results of the project put us in 
a better position to move forwards in introducing computational 
humor in more complex scenarios. 

2 AI and Computational Humor 

So far only very limited effort has been put on building computational 
humor prototypes. Indeed very few working prototypes 
that process humorous text and/or simulate humor 
mechanisms exist, and mostly they worked in very restricted 
domains. 

There has been a considerable amount of research on linguistics of humor and on theories of semantics and pragmatics of humor [Attardo, 1994; Attardo and Raskin, 1991; Giora and Fein, 1999; Attardo, 2002]; however, most of the work has not been formal enough to be used directly for computational humor modeling. An effort toward formalization of forced reinterpretation jokes has been presented by Ritchie [2002]. 

Within the artificial intelligence community, most writing 
on humor has been speculative [Minsky, 1980; Hofstadter 
el al., 1989]. Minsky made some preliminary remarks 
about how humor could be viewed from the artificial intelligence/
cognitive science perspective, refining Freud's notion 
that humor is a way of bypassing our mental "censors" which 
control inappropriate thoughts and feelings. Utsumi [1996] 
outlines a logical analysis of irony, but this work has not been 
implemented. Among other works: De Palma and Weiner 
[ 1992] have worked on knowledge representation of riddles, 
Ephratt [1990] has constructed a program that parses a limited 
range of ambiguous sentences and detects alternative humorous 
readings. A formalization, based on a cognitive approach 
(the belief-desire-intention model), distinguishing between 
real and fictional humor has been provided by Mele 
[2002]. 

Probably the most important attempt to create a computational 
humor prototype is the work of Binsted and Ritchie 
[1997]. They have devised a formal model of the semantic 
and syntactic regularities underlying some of the simplest 
types of punning riddles. A punning riddle is a question-
answer riddle that uses phonological ambiguity. The three 
main strategies used to create phonological ambiguity are syllable 
substitution, word substitution and metathesis. 

Almost all the computational approaches try to deal with 
the incongruity theory at various level of refinement [Raskin, 
1985; Attardo, 1994]. The incongruity theory focuses on the 
element of surprise. It states that humor is created out of a 
conflict between what is expected and what actually occurs 
in the joke. This accounts for the most obvious features of a 
large part of humor phenomena: ambiguity or double meaning. 


Specific workshops concerned with Computational Humor 
have taken place in recent years and have drawn together most 
of the community active in the field. The proceedings of the 
most comprehensive events are [Hulstijn and Nijholt, 1996] 
and iStock et al, 2002]. Ritchie [2001] has published a survey 
of the state of the art in the field. 

3 HAHACRONYM 

3.1 Resources 
In order to realize the HAHACRONYM prototype, we have refined 
existing resources and we have developed general tools 
useful for humorous systems. As we will see, a fundamental 
tool is an incongruity detector/generator, that makes the system 
able to detect semantic mismatches between word meaning 
and sentence meaning (i.e. in our case the acronym and 
its context). For all tools, particular attention was put on 
reusability. 
The starting point for us consisted in making use of some 
"off-the-shelf resources, such as WORDNET DOMAINS 
[Magnini et al., 2002] (an extension of the well-known English 
WORDNET) and standard parsing techniques. The tools 
resulting from the adaptation will be reusable for other applications, 
and are portable straightforwardly to other languages 

(e.g. WORDNET DOMAINS is multilingual). 
Wordnet and Wordnet Domains 
WORDNET is a thesaurus for the English language inspired 
by psycholinguistics principles and developed at the Princeton 
University by George Miller [Fellbaum, 1998]. It has 
been conceived as a computational resource, therefore improving 
some of the drawbacks of traditional dictionaries, 
such as circularity of definitions and ambiguity of sense references. 
Lemmata (about 130,000 for version 1.6) are organized 
in synonym classes (about 100,000 synsets). WORDNET 
can be described as a "lexical matrix" with two dimensions: 
a dimension for lexical relations, that is relations 
holding among words, and therefore language specific, 
and a dimension for conceptual relations, which hold 
among senses (the synsets) and that, at least in part, we consider 
independent from a particular language. A synset contains 
all the words by means of which it is possible to express 
the synset meaning: for example the synset {horse, 
Equus-caballus} describes the sense of "horse" as an 
animal, while the synset {knight, horse} describes 
the sense of "horse" as a chessman. The main relations 
present in WORDNET are synonymy, antonymy, hyperonymyhypony 
my, meronymy-holonymy, entailment, troponymy. 

Augmenting WordNet with Domain information 
Domains have been used both in linguistics (i.e. Semantic 
Fields) and in lexicography (i.e. Subject Field Codes) to mark 
technical usages of words. Although this is useful information 
for sense discrimination, in dictionaries it is typically 
used for a small portion of the lexicon. WORDNET DOMAINS 
is an attempt to extend the coverage of domain labels 
within an already existing lexical database, WORDNET (version 
1.6). The synsets have been annotated with at least one 
domain label, selected from a set of about two hundred labels 
hierarchically organized. (Figure 1 shows a portion of 
the domain hierarchy.) 

We have organized about 250 domain labels in a hierarchy 
(exploiting Dewey Decimal Classification), where each 
level is made up of codes of the same degree of specificity: 
for example, the second level includes domain labels such as 
BOTANY, LINGUISTICS, HISTORY, SPORT and RELIGION, 
while at the third level we can find specialization such as 

Figure 1: A portion of the domain hierarchy 

AMERICAN_HISTORY, GRAMMAR, PHONETICS and TENNIS. 


Information brought by domains is complementary to what 
is already present in WORDNET . First of all a domain 
may include synsets of different syntactic categories: for 
instance MEDICIN E groups together senses from Nouns, 
such as doctor#l and hospital#l, and from Verbs 
such as operate#7. Second, a domain may include 
senses from different WORDNE T sub-hierarchies For example, 
SPORT contains senses such as athlete#l , deriving 
from life_form#l, game equipment # l, from 
physical_object#l, sport#l from act#2, and 
playing_f ield#l, from location#l. 

Opposition of semantic fields 

On the basis of well recognized properties of humor accounted 
for in many theories (e.g. incongruity, semantic field 
opposition, apparent contradiction, absurdity) we have modelled 
an independent structure of domain opposition, such as 
RELIGION VS. TECHNOLOGY, SEX VS. RELIGION, etc... We 
exploit these kind of opposition as a basic resource for the 
incongruity generator. 

Adjectives and Antonymy Relations 

Adjectives play an important role in modifying and generating 
funny acronyms. WORDNE T divides adjectives 
into two categories. Descriptive adjectives (e.g. big, 
beautiful, interesting, possible, married) 
constitute by far the largest category. The second category 
is called simply relational adjectives because they are related 
by derivation to nouns (i.e. electrical in electrical 
engineering is related to noun electricity). To relational 
adjectives, strictly dependent on noun meanings, it is 
often possible to apply similar strategies as those exploited 
for nouns. Their semantic organization, though, is entirely 
different from the one of the other major categories. In fact 
it is not clear what it would mean to say that one adjective 
"is a kind of' (ISA) some other adjective. The basic semantic 
relation among descriptive adjectives is antonymy. WORD NE 
T proposes also that this kind of adjectives is organized 
in clusters of synsets associated by semantic similarity to a 
focal adjective. Figure 2 shows clusters of adjectives around 
the direct antonyms fast I slow. 


Figure 2: An example of adjective clusters linked by 
antonymy relation 

Exploiting the hierarchy 
It is possible to exploit the network of lexical and semantic 
relations built in WORDNE T to make simple ontological 
reasoning. For example, if a noun or an adjective has a geographic 
location meaning, the pertaining country and continent 
can be inferred. 


Rhymes 
The HAHACRONYM prototype takes into account 
word rhymes and the rhythm of the acronym expansion. 
To cope with this aspect we got and 
reorganized the CMU pronouncing dictionary 
(http: //www. speech.es.cmu.edu/cgi-bin/cmudict) 
with a suitable indexing. The CMU Pronouncing Dictionary 
is a machine-readable pronunciation dictionary for North 
American English that contains over 125,000 words and their 
transcriptions. 
Its format is particularly useful for speech recognition and 
synthesis, as it has mappings from words to their pronunciations 
in the given phoneme set. The current phoneme set 
contains 39 phonemes; vowels may carry lexical stress. 

Parser, grammar and morphological analyzer 
Word sequences that are at the basis of acronyms are subject 
to a well-defined grammar, simpler than a complete noun 
phrase grammar, but complex enough to require a nontrivial 
analyzer. We have decided to use a well established nondeterministic 
parsing technique. As far as the dictionary is concerned, 
we use the full WORDNE T lexicon, integrated with 
an ad-hoc morphological analyzer. 
Also for the generation part we exploit the grammar as the 
source for syntactic constraints. 
All the components are implemented in Common Lisp augmented 
with nondeterministic constructs. 

Other resources 
An "a-semantic" dictionary is a collection of hyperbolic/
epistemic/deontic adjective/adverbs. This is a last resource, 
that some time can be useful in the generation 
of new acronyms. Some examples are: abnormally, abstrusely, 
adorably, exceptionally, exorbitantly, exponentially, 
extraordinarily, voraciously, weirdly, wonderfully. This resource 
is hand-made, using various dictionaries as information 
sources. 
Other lexical resources are: a euphemism dictionary, a 
proper noun dictionary, lists of typical foreign words commonly 
used in the language with some strong connotation. 

3.2 Architecture and Implementation 
To get an ironic or "profaning" re-analysis of a given 
acronym, the system follows various steps and strategies. The 
main elements of the algorithm can be schematized as follows: 


. 
acronym parsing and construction of a logical form 
. 
choice of what to keep unchanged (typically the head 
of the highest ranking NP) and what to modify (e.g. the 
adjectives) 
. 
look up for possible substitutions 
. 
exploitation of semantic field oppositions 
. 
granting phonological analogy: while keeping the constraint 
on the initial letters of the words, the overall 
rhyme and rhythm should be preserved (the modified 
acronym should sound similar to the original as much 
as possible) 
. 
exploitation of WORDNE T antonymy clustering for adjectives 
. use of the a-semantic dictionary as a last resource 
Figures 3 and 4 show a sketch of the system architecture. 
In our system, making fun of existing acronyms amounts to 
an ironical rewriting, desecrating them with some unexpectedly 
contrasting, but otherwise consistently sounding expansion. 


As far as acronym generation is concerned, the problem is 
more complex. To make the task more attractive - and difficult 
- we constrain resulting acronyms to be words of the 
dictionary (APPLE would be good, IBM would not). The 
system takes in input concepts (actually synsets, possibly the 
result of some other process, for instance sentence interpretation) 
and some minimal structural indication, such as the 
semantic head. The primary strategy of the system is to consider 
as potential acronyms words that are in ironic relation 
with the input concepts. By definition acronyms have to satisfy 
constraints - to include the initial letters of some lexical 
realization of the inputs words synsets, granting that the sequence 
of initials satisfy the overall acronym syntax. In this 
primary strategy, the ironic reasoning comes mainly at the 
level of acronym choice in the lexicon and in the selection of 
the fillers of the open slots in the acronym. 

For example, giving as input "fast" and "CPU", we 
get static, torpid, dormant. (Note that the complete 
synset for "CPU" is {processor#3, CPU#1, 
central..processing.unit#l, mainframe#2}. So 


we can use in the acronym expansion a synonym of "CPU". 
The same happens for "fast"). Once we have an acronym 
proposal, a syntactic skeleton has to be filled to get a correct 
noun phrase. For example given in input "fast" and "CPU", 
the system selects TORPID with the possible syntactic skeleton: 



where "rapid" and "processor" are synonyms respectively 
of "fast" and "CPU" and the notation 
<Part_o f _Speech>Le^tr means a word of that particular 
part_of.speech with Letter as initial. Then the system 
fills this syntactic skeleton with strategies similar to those 
described for re-analysis. 

The system is fully implemented in Common Lisp, exploiting 
CLOS and the Meta-Object protocol to import WORD NE 
T DOMAINS and to implement the navigation/reasoning 
algorithms. 

3.3 Examples 
Here below some examples of acronym re-analysis are reported. 
As far as semantic field opposition is concerned we 
have slightly tuned the system towards the domains FOOD, 
RELIGION and SEX. We report the original acronym, the reanalysis 
and some comments about the strategies followed by 
the system. 

ACM - Association for Computing Machinery 

-->Association for Confusing Machinery 
FBI - Federal Bureau of Investigation 

\A1\AA. Fantastic Bureau of Intimidation 

The system keeps all the main heads and works on the adjectives 
and the PP head, preserving the rhyme and/or using the 
a-semantic dictionary. 

CRT - Cathodic Ray Tube 
\A1\AA. Catholic Ray Tube 
ESA - European Space Agency 
\A1\AA> Epicurean Space Agency 
PDA - Personal Digital Assistant 
\A1\AA.Penitential Demoniacal Assistant 
\A1\AA> Prenuptial Devotional Assistant 
MIT - Massachusetts Institute of Technology 
\A1\AA. Mythical Institute of Theology 

Some re-analyses are RELIGION oriented. Note the rhymes. 

As far as generation from scratch is concerned, a main concept 
and some attributes (in terms of synsets) are given as input 
to the system. Here below we report some examples of 
acronym generation. 

Main concept: processor (in the sense of CPU); 
Attribute: fast 

OPEN - On-line Processor for Effervescent Net 

PIQUE - Processor for Immobile Quick Uncertain Experimentation 


TORPID - Traitorously Outstandingly Rusty Processor for 
Inadvertent Data_proeessing 

UTMOST - Unsettled Transcendental Mainframe for Off-line 
Secured Tcp/ip 

We note that the system tries to keep all the expansions 
of the acronym coherent in the same semantic field of the 
main concept (COMPUTER _SCIENCH). At the same time, 
whenever possible, it exploits some incongruity in the lexical 
choices. 

3.4 Evaluation 
Testing the humorous quality of texts is not an easy task. 
There have been relevant studies though, such as (Ruch, 
19961. For HAHAc RONYM, a simpler case, an evaluation 
was conducted under the supervision of Salvatore Attardo at 
Youngstown University, Ohio. Both reanalysis and generation 
have been tested according to criteria of success stated in 
advance and in agreement with the European Commission, at 
the beginning of the project. 

The participants in the evaluation were forty students. 
They were all native speakers of English. The students were 
not told that the acronyms had been computer generated. No 
record was kept of which student had given which set of 
answers (the answers were strictly anonymous). No demographic 
data were collected. However, generally speaking, 
the group was homogeneous for age (traditional students, between 
the ages of 19 and 24) and mixed for gender and race. 

The students were divided in two groups. The first group of 
twenty was presented the reanalysis and generation data. We 
tested about 80 reanalyzed and 80 generated acronyms (over 
twice as many as required by the agreement with the European 
Commission). Both the reanalysis module and the generation 
module were found to be successful according to the 
criteria spelled out in the assessment protocol. The acronyms 
reanalysis module showed roughly 70% of acronyms having 
a score of 55 or higher (out of a possible 100 points), 
while the acronym generation module showed roughly 53% 
of acronyms having a score of 55 or higher. The thresholds 
for success established in the protocol were 60% and 45%, 
respectively. 

A set of randomly generated acronyms were presented to a 
different group of twenty students. A special run of the system 
was performed in which the semantic filters and heuristics 
had been disabled, while only the syntactical constraints 
were operational. (If the syntactical rules had been disabled 
as well the output would have been gibberish and it would be 
difficult to compare with the regular system production). In 


Table 1: Evaluation Results 

A curiosity that may be worth mentioning: HAHACRONYM 
participated to a contest about (human) production 
of best acronyms, organized in December 2002 by RAI, 
the Italian National Broadcasting Service. The system won a 
jury's special prize. 

4 Prospects for advertisement 

Humor is the healthy way of creating 'distance* between 
one's self and the problem, a way of standing back and looking 
at the problem with perspective. Humor reveals new aspects, 
disarms and relaxes. It is also infectious and it is an 
important way to communicate ideas. On the cognitive side 
humor has two very important properties: 

- 
it helps getting and keeping people's attention. Type 
and rhythm of humor may vary and the time involved 
in building the effect may be different in different cases: 
some times there is a context - like joke telling - that 
from the beginning let you expect for the humorous climax, 
which may occur after a long while: other times the 
effect is obtained in almost no time, with one perceptive 
act - for instance in static visual humor, funny posters 
or in cases when some well established convention is reversed 
with an utterance; 
- 
it helps remembering. For instance it is a common experience 
to connect in our memory some knowledge we 
have acquired to a humorous remark or event. In a foreign 
language acquisition it may happen that an involuntary 
funny situation is created because of so called "false 
friends" - words that sound similar in two languages and 
may have the same origin but have a very different meaning. 
The humorous experience is conducive to remembering 
the correct use of the word. 
No wonder that humor has become one of the favorite 
strategies used by creative people involved in the advertising 
business. In fact among the various fields in which creative 
language is used, advertising is probably the area of activity 
in which creativity (and hence humor) is practiced with most 
precise objectives. 

From an applied AI point of view, we believe that an environment 
for proposing solutions to advertising professionals 
can be a realistic practical development of computational humor 
and one of the first attempts in dealing creative language. 

Some examples of the huge array of opportunities that language 
offers and that existent NLP techniques can cope with: 
rhymes, wordplays, popular sayings or proverbs, quotations, 
alliterations, triplets, chiasmus, neologism, non sequitur, 

adaptation of existing expressions, adaptation of proverbs, 

personification, synaesthesia (two or more senses combined). 

An intelligent system devoted to advertisement, based on 
an apparatus that extends the one we have described, can play 
with these expressions, alter them, change the context, use 
them as a platform for completely new ideas (e.g. create the 
slogan: Thirst come, thirst served for a soft drink). 

In the longer run the aim is to go for individual-oriented 
advertisement. Let us leave aside issues of privacy for the 
moment, though we know this is a critical theme. The availability 
of individual profiles, automatic inferencing of interest 
and behavior models, information about the location of the 
individual, combined with reasoning on the offer side (what 
is advisable to promote in a given context of business) will 
all provide the basic input for advertising things worth getting 
the interest of a specific individual in a given context. 
Commercial advertisement will be just one case along this 
line - one can think of influencing the individual behaviour on 
the basis of the availability of any possible resources such as, 
for instance, cultural goods. A prediction is that when such 
situation-oriented advertisements will become widespread, 
the same features that prevail now with broadcasted material 
will be sought after for the new case. So, forms of humorous 
advertisement for the individual (even playing on personal aspects) 
will become a plus. 

An important aspect to be taken into account is how humor 
is appreciated by different individuals. Personality studies regarding 
this specific theme give important indications [Ruch, 
20021. One option will also be to develop humor for conversational 
systems, based on embodied agents. The work of Nijholt 
[2002] Andre and Rist [2000] and Cassell [2001] could 
provide the starting point for introducing dynamic humor. 

For obtaining good results (there are not many things that 
are worse than bad humor!) for these longer term objectives, 
much work will be needed, especially further integration with 
ontologies, common sense reasoning and pragmatics. 



Abstract 

Hand-crafting effective visual presentations is 
time-consuming and requires design skills. Here we 
present a case-based graphic sketch generation 
algorithm, which uses a database of existing 
graphic examples (cases) to automatically create a 
sketch of a presentation for a new user request. As 
the first case-based learning approach to graphics 
generation, our work offers three unique contributions. 
First, we augment a similarity metric with a 
set of adequacy evaluation criteria to retrieve a case 
that is most similar to the request and is also usable 
in sketch synthesis. To facilitate the retrieval of case 
fragments, we develop a systematic approach to 
case/request decomposition when a usable case 
cannot be found. Second, we improve case retrieval 
speed by organizing cases into hierarchical clusters 
based on their similarity distances and by using 
dynamically selected cluster representatives. Third, 
we develop a general case composition method to 
synthesize a new sketch from multiple retrieved 
cases. Furthermore, we have implemented our case-
based sketch generation algorithm in a user-system 
cooperative graphics design system called IMPROVISE-!-, 
which helps users to generate creative and 
tailored presentations. 

1 Introduction 

Automated graphics generation systems promise to aid 
users in creating effective visual presentations (e.g., charts 
and diagrams) [Mackinlay, 1986; Zhou, 1999]. Upon a user 
request (e.g., displaying sales data), these systems directly 
provide users with the final presentation (e.g., a barchart). To 
better tailor a presentation to user preferences, we are building 
a user-system cooperative graphics generation system, 
called IMPROVISE*, IMPROVISE* generates a presentation in 
two steps: sketch generation and sketch refinement. Here a 
sketch is an outline of a presentation without all visual 
details (Figure 1). A generated sketch is first presented to 
users for their feedback. Depending on the feedback, IMPRO


VISE* may redesign the sketch or refine it to create a final 
illustration (e.g., setting the exact layout of Figure 1). By 
allowing users to critique a sketch first, IMPROVISE* can 
save the cost of fine-tuning an undesirable design. 

Our focus here is on a case-based learning approach to 
sketch generation. Given a user request, from a database of 
existing graphic examples (cases) our approach uses a similarity 
metric to retrieve the case that is most similar to the 
request. The retrieved case is then directly reused for or 
adapted to the new situation (e.g., new data). Instead of using 
a rule-based approach as most graphics generation systems 
do [Mackinlay, 1986; Zhou, 1999], our decision of using 
case-based learning is two-fold. First, it is difficult to hand-
extract a complete and consistent set of graphics design 
rules, while existing graphic examples are abundant [Zhou 
et al., 2002a]. Second, case-based learning is efficient for 
sketch generation, where we focus on learning overall visual 
structures instead of precise visual arrangements (e.g., exact 

positions and scales) '. 

Although case-based learning has been applied to various 
design problems [Borner, 2001], it has never been 
applied to graphics design. As the first case-based learning 

approach to graphics design, our work offers three unique 
contributions. 1) In case retrieval, we apply a set of adequacy 
evaluation criteria in addition to a similarity metric to ensure 
that the retrieved case is usable in sketch synthesis. To handle 
situations where a usable case may not be found, we 
decompose cases/requests into sub-cases/sub-rcquests to 
facilitate the retrieval of case fragments. 2) We improve case 
retrieval speed by organizing cases into hierarchical clusters 
based on their similarity distances and by using dynamically 
selected cluster representatives. 3) We synthesize a new 
sketch from multiple retrieved cases through case generalization 
and visual decoration inference (e.g., inferring decorations 
such as coordinate axes and legends). 

Starting with a brief discussion of related work, we then 
present our case-based learning algorithm, highlighting the 
three unique features mentioned above. Finally we use a 
concrete example to demonstrate how a new sketch is created. 


2 Related Work 

Unlike rule-based graphics systems [Mackinlay, 1986; 
Andre and Rist, 1993; Chuah ctal., 1997; Zhou, 1999], our 
work is the first to apply a general case-based learning technique 
to graphics design. Although one system, SAGE, has an 
example-based generation component, it only reuses examples 
created by its own rule engine [Chuah et al., 1997]. 
Compared to SAGE, IMPROVISE+ uses a much more fine-
grained representation to capture the semantic and syntactic-
features of each graphic example [Zhou et al., 2002b]. As a 
result, without using any rules IMPROVISE-*- can create new 
graphic sketches by directly learning from a wide variety of 
graphic examples. 

Differing from a programming by demonstration system 
[Myers et al., 1994], where users must supply the desired 
examples, our approach uses the graphic examples stored in 
a database. 

There are many case-based systems developed for other 
domains, e.g., engineering design [Sycara et al., 1992]. The 
closest to ours are known as case-based structure generation 
systems [Borner, 2001]. However we have gone beyond 
existing approaches to address specific challenges in graphics 
design. In particular, we support a systematic, multi-level 
case/request decomposition to achieve a more accurate case-
request matching. In contrast, existing systems either ignore 
case/request decomposition [Borner, 2001] or simplify it 
(e.g., only leaf-level decomposition [Michelena and 
Sycara, 1994]). We also develop a general case composition 

User request 

method to synthesize a new sketch from multiple cases, while 
existing systems only allow limited case composition (e.g., 
combining only highly similar cases [Borner, 2001]). 

3 Example-based Sketch Generation 

Figure 2 gives an overview of our case-based sketch 
generation algorithm. Our algorithm uses a database of existing 
information graphics (cases) to suggest a visual design 
for a new user request. Here each stored case is described by 
its data content D and visual representation v. Each request is 
presented by specifying the data D' and its desired visualization 
v, which may be partially or not specified at all. Given 
such a request, our algorithm first uses a similarity metric to 
retrieve the top-matched case by computing the similarity 
distances between the request and existing cases. A top-
matched case is the case that has the shortest similarity distance 
to the request. If the top-matched case fails our adequacy 
test, the current request is decomposed into a set of 
sub-requests. The whole process is recursively called to find 
the top-matched case for each sub-request. Depending on the 
retrieval result, a new sketch (a fully specified V) may be 
constructed from the visual encoding (V) of a single matched 
case or composed from multiple cases. Upon completion, the 
user is presented with a rendered graphic sketch (e.g., Figure 
1). We also involve users at different design stages (Figure 
2). For example, our studies have shown that users may 
express their preferences to retrieve more desired cases 
[Zhou et al., 2002b] or propose new visual compositions 
during sketch synthesis. 

Before discussing our algorithm, we first briefly 
describe the representations of our cases and user requests. 

3.1 Representation of Cases and User Requests 
We employ a labeled graph structure to represent our 
cases and requests. Since we have described in detail how to 
model and represent various semantic and syntactic features 
of an existing graphic example previously [Zhou 
et al., 2002b], we summarize the representation here. 

Visual Database of Cases 

Our case base contains an assortment of graphic designs 
collected from a wide variety of sources [Zhou et al., 2002a]. 
Using a labeled graph structure [Zhou et al., 2002b], each 
case is described in XML as a graph (Figure 3a), which 
expresses a complex mapping between a data hierarchy (DOD5) 
and a visual hierarchy (V0-V8). Within each hierarchy, a 
data/visual node is described by a set of features (e.g., D3 and 
V7). There are two types of links in the graph: intra-hicrarchy 
links for data/visual node relationships (e.g., D3 is indexed 
by D2 and all undirected edges implying parent-child relations), 
and inter-hierarchy links (red dotted lines) for data-
visual mappings (e.g., D5 to V7). To facilitate case retrieval, 
we also index each case using two graphs (Figure 3b): 
PDGraph organizes data-visual mapping pairs along the data 
hierarchy, while PVGraph arranges the mapping pairs by the 
visual hierarchy. Note that a data/visual node may be 
mapped to multiple visual/data nodes (e.g., D5 and V6) or 
none (denoted by "/"). Our bi-graph indexing not only preserves 
all the information captured in the original complex 
graph (Figure 3a), but also partitions one complex graph into 
two simpler sub-graphs (Figure 3b), which can greatly simplify 
the similarity measuring process. 

When loading all cases from the database to memory, 
IMPROVISE+ parses the XML document of each case and automatically 
builds all case indices (PDGraphs and PVGraphs). 

User Request 

A user request submitted in XML format is also described 
as a graph similar to the case representation. Figure 4 outlines 
a request for presenting the information of a city. Specifically, 
it asks to display the relevant county 
(CountyBoundary), basic information of the city (Name and 
Location), the city gazetteer (Population, Agelnfo for 6 age 
groups, and an arrow button indicating Morelnfo is available 
upon request), and 3 city amenities (Name, Location, and Type) 
such as community golf courses and swimming pools. Since 
users may not know every presentation aspect, the representation 
of a user request is often a partially specified graph 
with the majority of the visual elements left unspecified. For 
example, Figure 4 does not specify a visual encoding for any 
data nodes except Morelnfo. 

Based on the characteristics of data relations, our algorithm 
automatically assigns matching priorities to different 
data relations in a user request to indicate that matching certain 
data relations well (e.g., index in Figure 5) is more 
important than matching others (e.g., has-a). As described 
below, matching priorities aid us in evaluating the usability 
of a retrieved case and in selecting visual candidates. Currently, 
we assign priorities by relation type. For example, a 
higher priority is given to presentation relations like index 
than to semantic relations like has-a. 

3.2 Case Retrieval 
The success of an interactive case-based system like 
ours, depends largely on the quality and speed of the case 
retrieval process. To ensure retrieval quality, we augment 
similarity measuring with adequacy evaluation that tests 
whether a retrieved case is usable in sketch synthesis. If a 
usable case cannot be found, we decompose cases and 
requests into sub-cases and sub-requests to facilitate the 


Figure 4. A sample user request. 

retrieval of case fragments. In addition, we use a hierarchi 
cally structured case base to improve our retrieval speed. 

Adequacy-Guided Retrieval 

Using a quantitative similarity metric that we have 
developed [Zhou et al., 2002b], our algorithm first retrieves 
the top-matched case for a request. As a result, each data 
node and data relation in the request are associated with a 
computed similarity distance in the range of [0, 1]. Since our 
similarity model stresses the overall structural similarity 
between the request and the existing examples, a data node 
Dr in the request may or may not acquire a match (D, v) from 
the top-matched case. Here D is the data that matches Dr, and 
v(the visual mapping of D) is a potential visual candidate for 
encoding Dr in the new sketch. If no match is found for Dr, 
the recorded distance is 1.0. However, the top-matched case 
may be inadequate for creating an effective sketch for the 
following three reasons. Accordingly, we formulate three 
adequacy criteria to evaluate whether a retrieved case is 
usable in sketch synthesis. 

First, a top-matched case may produce a good overall 
match but poor or no matches for certain data nodes in the 
request. This implies that certain data or a sub-set of data 
expressed in the request may not be well visualized in the 
new sketch as they could be. To ensure a good match for 
every data node (i.e., a short similarity distance), our first 
criterion requires that the similarity distance for every data 
node of a request be below a threshold. After conducting a 
series of case retrieval experiments, we currently set the 
threshold to 0.3 on a [0, 1] scale, which proves to be a good 
indicator for creating a quality new sketch. 

Second, suppose that a top-matched case passes the 
above criterion. By the matching priorities set in the request, 
however the distances for more important data relations may 
be larger than those of less important ones. Since matching 
priorities are used to select visual candidates during case 
composition, using such a matching result may alter the original 
intention of the request and produce an undesired sketch. Therefore our second criterion states that for every data relation in a request, the higher its matching priority is, 
the shorter its associated similarity distance must be. 

Third, even though a top-matched case passes the above 
two criteria, it may still not be adequate for synthesizing a 
new sketch. Our synthesis starts with the visual candidates of 
data leaf nodes in a request and composes the higher-level 
visual nodes from bottom up using the lower-level visual 
nodes. However our top-down graph matching is a partial 
matching, which may not guarantee that every data leaf node 
in a request acquire a visual candidate [Zhou et al., 2002b]. 
Our synthesis would then fail due to a lack of basic building 
blocks. Thus our third criterion requires that every data leaf 
node in a request obtain a visual candidate. It is not required 
to find visual candidates for intermediate data nodes from 
case retrieval, since theirs can be composed from those of 
their children. 

Case and Request Decomposition 

It is rare to find an exact match for a user request from 
the case base, but it is quite common that a fragment of the 
user request matches well with a fragment of a case 
[Mitchell, 1997]. To facilitate the retrieval of case fragments, 
we support case/request decomposition. 

Case decomposition. To avoid creating a large search 
space, our challenge is to determine the granularity of subcases. 
Based on the data and visual characteristics of a 
graphic example [Zhou et al., 2002b], we develop four heuristics 
to guide our case decomposition. 

1). We extract independent and meaningful visual structures, 
which are schematic illustrations for conveying concepts 
and processes [Winn, 1987]. For example, the table 
showing the city information (Figure 6a) is considered as a 
sub-case. Within a case, such a structure can be easily identified 
by the value (VisualStructure) of its feature Category. 

2). We turn a case (e.g., Figure 6b) into a sub-case by 
trimming all its decorations (e.g., coordinate axes and legends). 
These simplified sub-cases are good matching candidates 
for most user requests, which normally do not specify 
the data for creating visual decorations. 

3). We extract all visual leaf objects along with their data 
mappings from cases (e.g., V7 and D5 in Figure 3b) to form a 
visual dictionary [Zhou and Feiner, 1997]. The dictionary is 
used to find matches for user requests that contain a single 
node. 

4). We decompose a case by data relations. Figure 5 
shows two data relations (has-a and index) encoded in Figure 
6(b), which lead to two sub-cases (Figure 5a-b). In Figure 
5(b), a Dummy node is also added to preserve a rooted graph 


structure for a fast graph matching (see below). 
For the sake of performance, we automatically extract 
all sub-cases during the case loading stage. 

Request decomposition. Unlike case decomposition, 
which is done once during the case loading stage, request 
decomposition occurs whenever a retrieved top-matched 
case fails our adequacy evaluation. To ensure the retrieval 
quality without incurring the expense of rematching, we 
extract only the failed fragments as sub-requests, while 
retaining the results for succeeded ones. Here a failed fragment 
is the biggest possible rooted sub-graph that contains 
failed nodes excluding the root of the current request. Suppose 
that every node in fragment A (Figure 4) passes our 
evaluation, but a node (e.g., AName) in fragment B fails. We 
then create a sub-request containing the whole fragment B 
with City as the root, while keeping the matching results of 
fragment A. Depending on the matching results, the decomposition 
may be repeated until sub-requests contain a single 
node. The matched cases for these single-node requests can 
be found from our visual dictionary. 

Since sub-requests break up the original structure of a 
request, it is always desirable not to produce too many sub-
requests. Request decomposition however enables us to find 
desired case fragments from a set of heterogeneous cases as 
we have, where finding a maximal common subgraph may 
fail [Borner, 2001]. Unlike a static request decomposition 
used by other researchers [Michelena and Sycara, 1994], our 
decomposition is dynamically performed based on our adequacy 
evaluation results. 

Improve Retrieval Speed 

As the number of cases grows, the cost of searching for a 
desired case increases. Moreover, finding a graphic example 
that matches a user request best is a computationally intensive 
graph-matching process itself. Specifically, we need to 
match two pairs of PDGraphs and PVGraphs between a case 
and a user request. By exploiting the rooted hierarchical 
structure of PD/PVGraph, we perform an ordered, top-town 
graph matching [Zhou et al., 2002b] to accelerate otherwise 
an arbitrary graph matching process (e.g., matching two 
complex graphs similar to Figure 3a). To further improve 
case retrieval speed, we reduce the search space by using a 
hierarchically structured case base. 

Instead of searching the case base linearly, we exploit 
the organization of the cases. We use a hierarchical clustering 
algorithm [Duda and Hart, 1973] to arrange all cases by 
their pair-wise distances computed using a similarity metric 
[Zhou et al., 2002b]. Figure 7 shows such a cluster hierarchy. 
Starting with the two outmost clusters in the hierarchy (e.g., 
clusters 1-2 in Figure 7), at each level we search only one 

cluster that is most likely to contain the top-matched case. 

To find a cluster to follow, we first select a representative 
for each cluster using a quick approximation. A representative 
is a case closest to the request by three meta 
properties of its PDGraph: the total number of nodes, the 
average number of levels, and the average number of nodes 
at each level. We then select the cluster whose representative 
produces the shortest similarity distance to the request by our 
similarity metric. The rationale here is that the top-matched 
case for a request is likely to be in the same cluster of cases 
that match the request well by both meta properties and our 
similarity metric. 

Following the selected cluster (e.g., cluster 2 in Figure 
7) down the hierarchy (e.g., clusters 3-4), our algorithm 
repeats the above process until it explores a leaf cluster. In 
our experiments, this cluster-guided search improves the performance 
over a linear search by a factor of 3. Due to the 
approximation used, our method however is not guaranteed 
to find the top-matched case. Unlike other structured case 
search, where cluster representatives are pre-selected in 
advance [Borner, 2001], we dynamically compute representatives 
for each request to achieve a more accurate retrieval. 

3.3 Case Composition: Sketch Synthesis 
As the result of a successful retrieval, each data leaf 
node of the request acquires at least one visual candidate. 
Starting with the visual candidates of the leaves, our algorithm 
synthesizes a sketch from bottom up by creating visual 
candidates for higher-level data nodes and finally for the 
root. In this section, we address three challenges arising in 
sketch synthesis. First, we resolve visual candidate conflicts 
when a data node in the request acquires multiple candidates. 
Second, to compose multiple retrieved cases, we use decision-
tree learning to generalize existing visual compositions 
and to verify new compositions. Third, we automatically 
infer visual decorations from existing cases, such as the 
coordinate axes and legends, to complete a visual sketch 
design. 

Conflict Resolution of Visual Candidates 

Within a user request, a data node may acquire multiple 
visual candidates from different case matching. For example, 
data nodes Name and Price appear in two sub-requests (Figure 
5a-b). It is most likely for both items to obtain multiple 
visual candidates as the two sub-requests are matched to different 
cases. To select a proper visual candidate, we currently 
use both the matching priority set in the user request 
and the distance calculated during similarity measuring. Specifically, 
we first retain candidates that are acquired through 
matching the data relations with a higher priority. In the 
above example, we will keep the candidates for Name and 
Price acquired through matching the sub-request in Figure 
5(b), since the priority for matching relation index precedes 
that of relation has-a (Figure 5a). If there are multiple candidates 
by the same matching priority, we then select candidates 
that have produced the shortest distance during the 
match. Should there still be multiple candidates left, our 
algorithm would choose a candidate randomly. 

Generalizing and Verifying Visual Composition 
Our sketch synthesis uses visual candidates acquired for 
the lower-level data nodes to create visual candidates for the 

higher-level nodes in the request. Since visual candidates 
may be retrieved from different cases, their compositions 
may never exist before and new compositions are needed to 
piece them together. Here we denote a visual composition 
(=>) of N visual elements as: 

Ej, ..., EN=>rC, 
where E, is the zth element, r is the composition relation, and 
C is the composed visual object. Below is an example of a 
cartogram (a map-based presentation) composition: 

Position, Map =s> overlay Cartogram. 
It composes a Cartogram by overlaying a Position element on 
top of a Map element. In our approach, a new composition for 
an intended data node (e.g., Amenity in Figure 4) is proposed 
using the composition information recorded in its children 
(e.g., AName) during the case retrieval. Specifically, in each 
data node that acquires a visual candidate v from the 
retrieved case (e.g., V7 from Figure 3a), we record the composition 
relation and category specified in vs parent (e.g., 
V4). As a result, for each retrieved visual candidate our algorithm 
records the possible compositions that the visual candidate 
has participated in. 

However a proposed composition may not always be 
valid. Here validity means that a visual composition must 
produce an effective visual design [Mackinlay, 1986]. For 
example, the above cartogram composition is a valid composition, 
but composing a visual object by juxtaposing two horizontal 
position elements is not. Without hand-crafting rules 
or pre-defining connectors [Michelena and Sycara, 1994], 
we use a decision-tree learning technique to automatically 
induce a set of classification rules from 200 visual composition 
samples extracted from our cases. By our visual composition 
definition, we describe each sample using N input 
features specifying N visual element categories (e.g., Position 
and Map) and one target specifying a combined composition 
relation and category (e.g., overiay_Cartogram). Currently N is 
set to 3 since most of our samples contain 3 elements. If a 
sample contains fewer than 3 elements, the remaining features 
are set to null. We train C4.5 [Quinlan, 1993], a decision-
tree learning algorithm, on the 200 samples using 5-fold 
cross-validation, a standard procedure when the amount of 
data is limited. We obtain 14 generalized composition rules 
with an overall classification error of 22%. We then use these 
rules to verify a new composition. 

To compensate for the situation where there is a lack of 
visual composition samples, we introduce negative samples 
that are known invalid compositions to help identify invalid 
new compositions. Similar to the above process, we use classification 
rules induced from the negative samples to verify 
invalid new compositions to be eliminated. After both positive 
and negative verifications, if there are still multiple proposed 
compositions we use the confidence factors generated 
in the decision-tree learning to select the most probably valid 
composition. Without a sufficient number of composition 
samples, our approach may not always verify a composition 
correctly. That is why we involve users in the design process 
to help IMPROVISE-*- in its decision-making (Figure 2). 

Inferring Visual Decoration 

A sketch is incomplete without the necessary visual decorations, 
such as coordinate axes and legends, which can 
guide users to comprehend the information encoded in the 

graphic [Wilkinson, 1999]. However in a request a user 
rarely specifies the data for creating such decorations, which 
our algorithm must infer. 

Currently our inference is based on an assumption that 
visual decorations can be created using only data leaves in a 
request. According to visual psychology studies, a person 
tends to first perceive the global structure of a graphic (e.g., 
a spatial map) then examine the details (a particular color or 
a position) [Goldsmith, 1984]. Only when a person attempts 
to interpret the meanings of visual primitives (e.g., the color 
code in Figure 6b), visual decorations are needed (e.g., the 
legend in Figure 6b). Hence visual decorations normally 
encode leaf data or their transformations. 

Specifically, for each leaf L and its match (D, V) from a 
retrieved case C, we trace the visual mappings of the 
matched data node D in C. Within case C, if data D is used to 
create visual decorations, our algorithm would create a 
visual decoration for L. in the user request. More precisely, if 
there exists a data-visual mapping pair (D', V) in case c, 
where D' is D or a transformation of D, and V is a visual decoration; 
v becomes a visual decoration candidate for the new 
sketch. However, L may need to be transformed for creating 
r, since D' is often a transformation of D. Suppose that a 
user asks to present the depth of different lakes (LakeDepth), 
which happens to acquire a match (Price, VertLength) from a 
retrieved case as in Figure 6(b). Here Price (a domain) has 
been transformed into a range [minPrice, maxPrice] in creating 
the Y axis. Similarly, in the new sketch a Y axis will be created 
for encoding the range of LakeDepth converted from the 
LakeDepth domain. 

4 An Example 

Here we demonstrate how a specific user request (Figure 
4) is fulfilled. For this request, our algorithm first retrieves 
the top-matched case, which fails our adequacy test, as the 
similarity distance (0.5803) for node Gazetteer in fragment B 
is above the required threshold (0.3). While keeping the 
matches for fragment A, we extract the whole fragment B as a 
sub-request with node City as its root, since we always 
attempt to find an overall good match for the bigger subgraph 
(e.g., the sub-graph covering B instead of B2). Next, 
the top-matched case for fragment B is retrieved, where 

nodes in B1 pass the evaluation, but those in B2-B3 do not. 
Subsequently, our algorithm treats fragments B2-B3 as two 
new sub-requests and finds matches for them. Except nodes 
under fragments B21 and B31, all other data nodes have now 
acquired acceptable matches. Eventually desired matches are 
found for the last two sub-requests generated for fragments 
B21 and B31. Since B31 contains only one node, its match is 
retrieved from our visual dictionary. 

Using the retrieved results, our algorithm synthesizes a 
new sketch from bottom up (Figure 8a). It proposes new 
compositions to create visual candidates for shaded data 
nodes, while reusing retrieved candidates for others. Specifically, 
a new composition is proposed for Gazetteer, since the 
visual candidates for its children Population, Agelnfo, and Morelnfo, 
are retrieved from different cases. In this case, a top-tobottom_
Table composition is recorded for Population during the 
retrieval. According to C4.5, this is a valid composition, thus 
it is used to create a Table for conveying Gazetteer with 3 
items arranged from top to bottom: a text, a pie chart, and an 
arrow button. Similarly, visual candidates are created for 
Amenity, then for City, at last for the root. As a result, a new 
sketch is created for this request with an inferred legend for 
interpreting the type of amenities (Figure 8b). 

If the user chooses to use a different set of cases, an 
alternative can be created for the same request (Figure 1). 
Once a new sketch is created, it can be directly stored back 
in our case base as a new addition, since the generated sketch 
has a representation similar to other existing cases. 

5 System Implementation and Performance 

IMPROVISE+ is implemented using Java and C++. We 
use Java to implement all design components, including our 
case-based sketch generation algorithm. Wc have implemented 
two rendering components, one in Java2D for 2D 
graphics and the other in C++/Open Inventor/OpenGL for 
3D graphics. IMPROVISR+ currently runs on Win32/Linux/ 
SGI. On a PC with a 1.13 GHz Pentium 111 processor, it takes 
about 5 seconds to create the sketch shown in Figure 8(b) 
from about 100 cases, each of which contains about 30 data 
nodes and 30 visual nodes on average. 

We have also conducted several experiments to test the 
effectiveness of our approach. According to our user feed-back, our case-based approach can provide more versatile 
and "creative" design suggestions (e.g., Figure 1) than a 
rule-based approach can [Zhou, 1999]. Moreover, involving 
users in proper design stages helps create a more tailored 
visual presentation. For example, by specifying different 
retrieval preferences [Zhou et al., 2002b], users can choose 
among different design alternatives (Figure 1 vs. Figure 8b). 

6 Conclusions and Future Work 

In this paper, we have presented a case-based graphic 
sketch generation algorithm with an emphasis on its three 
unique features and how they facilitate an efficient and 
effective sketch generation. First, we present an adequacy-
guided case retrieval method, which augments a similarity 
metric with a set of adequacy evaluation criteria to retrieve a 
top-matched case that is also usable in sketch synthesis. To 
facilitate the retrieval of case fragments, we also describe 
how to systematically decompose a case/request when a 
usable case cannot be found. Second, we explain how to 
enhance case retrieval speed by organizing cases into hierarchical 
clusters based on their similarity distances and by 
using dynamically selected cluster representatives. Third, we 
show how to construct a new sketch through case composition, 
including case generalization and visual decoration 
inference (e.g., inferring coordinate axes and legends). 

Currently, we arc working in two areas to improve 
IMPROVISE+. First, we are creating a GUI that allows users to 
easily specify a request by "drawing" a graph similar to Figure 
4 without writing an XML document. Second, we are 
developing more sophisticated interaction support, where 
IMPROVISE+ can automatically engage users in interaction 
based on context. For example, if IMPROVISE+ cannot find a 
valid visual composition using decision-tree learning, it may 
decide to solicit user inputs. 

Acknowledgments 

We would like to thank Keith Houck and Alison Lee for 
proofreading the earlier version of this paper. We would also 
like to thank Shimei Pan for useful discussions on case-
based learning. 


Abstract 

Filtering denotes any method whereby an agent updates 
its belief state\A1\AAits knowledge of the state of 
the world\A1\AAfrom a sequence of actions and observations. 
In logical filtering, the belief state is a logical 
formula describing possible world states and 
the agent has a (possibly nondeterministic) logical 
model of its environment and sensors. This 
paper presents efficient logical filtering algorithms 
that maintain a compact belief state representation 
indefinitely, for a broad range of environment 
classes including nondeterministic, partially observable 
STRIPS environments and environments 
in which actions permute the state space. Efficient 
filtering is also possible when the belief state is represented 
using prime implicates, or when it is approximated 
by a logically weaker formula. 

1 Introduction 

Any agent operating in a partially observable environment 
must perform computations that distinguish among the a priori 
possible current states of the world on the basis of past observations 
and actions. These computations may operate directly 
on a representation of the action observation sequence 
(e.g., [Winslett, 1990; Kautz et al, 1996]); they may reduce 
queries about the current state to queries about the initial state 
(e.g., [Reiter, 2001]); or, they may update the belief state (the 
agent's knowledge about the state of the world) after each action 
and observation. This latter approach, called filtering or 
recursive state estimation in the control theory literature, is 
particularly useful with unbounded sequences of actions and 
observations. 

The main computational difficulties are 1) the time needed 
to update the belief state, and 2) the space required to represent 
it. These depend on the nature of the transition model, 
which describes how the environment evolves over time, the 
observation model, which describes the way in which the environment 
generates observations, and the family of representations 
used to denote belief states. Early work, beginning 
with Gauss, assumed stochastic models. For example, 
the Kalman filter [Kalman, 1960] is a ubiquitous device that 
maintains a multivariate Gaussian belief state over n variables, 
assuming linear-Gaussian transition and observation 

model. Crucially, the 0(/n3) update cost and the 0(n2) space 
requirement do not depend on the length of the observation 
sequence; hence, a Kalman filter can run indefinitely. In this 
paper, we are interested in developing analogous results in the 
context of logical representations. 

We adopt a simple logical language (Section 2) for describing 
the transition and observation models; the observations 
and the belief state itself are also logical formulae. The initial 
state may be only partially known; the transition model, 
which allows for actions by the agent itself, may be nondeterministic; 
and the observation model may be nondeterministic 
and partial, in the sense that the agent may not be able to 
observe the actual state. 

Even when we restrict ourselves to propositional logic, it 
is clear that the general filtering problem is nontrivial (we 
prove it is computationally hard), because there are exponentially 
many possible states. We identify several classes of 
models that allow efficient filtering with respect to the belief-
state representation size. Our primary method is based on 
decomposition theorems showing that 1) filtering distributes 
over disjunction in the belief state formula, and 2) filtering 
distributes over conjunction and negation if the actions are 
permutations of the state space. Such actions serve as one-toone 
mappings between states, for those states in which they 
can be applied. We obtain efficient, exact algorithms for DNF 
belief states and for NNF (Negation Normal Form - all negations 
are in front of atoms) and CNF belief states with permuting 
actions. In other cases, we obtain efficient algorithms 
for approximate filtering. 

In another class of dynamic systems, we can filter efficiently 
if the belief state is represented in CNF that includes 
all its prime implicates. Finally, we show that STRIPS models 
(possibly with nondeterministic effects of actions) also admit 
efficient filtering. The STRIPS assumption, that every 
action has no conditional effects and that an effect's preconditions 
are the preconditions for the action's execution, is key 
to this efficiency. 

With respect to maintaining a compact representation, we 
show that properties similar to those mentioned above allow 
us to filter A-CNF formulae (CNF with clauses of at most K 
literals, when k is fixed) such that the result is represented in 
K-CNF (for the same fixed k). Thus, the belief state is maintained 
in 0(nk) space indefinitely. In particular, we show 
mild conditions under which a compact belief state can be 

maintained in nondeterministic STRIPS domains and in permutation 
domains. Finally, we show that DNF belief states 
remain compact if the effects of actions are deterministic and 
guaranteed to hold. These results are the first analogues, in 
the logical arena, of the desirable properties possessed by 
Kalman filters for continuous variables. 

Ours is by no means the first work on filtering in a logical 
context. Early on, it was pointed out that filtering is easy 
for deterministic systems with a known initial state [Fikes et 
al. , 1972; Lin and Reiter, 1997]. Filtering in nondeterministic 
domains is more difficult. In particular, the related problem of 
temporal projection is coNP-hard when the initial state is not 
fully known, or when actions have nondeterministic effects 
[Liberatore, 1997] (see also Section 3.3). 

Traditionally, computational approaches for filtering take 
one of three approaches: 1) enumerate the world states possible 
in every belief state and update each of those states separately, 
together generating the updated belief state [Ferraris 
and Giunchiglia, 2000; Cimatti and Roveri, 2000], 2) list the 
sequence of actions and observations and prove queries on 
the updated belief state [Reiter, 2001; Sandewall, 1994], or 
3) approximate the belief state representation [Son and Baral, 
2001; Williams and Nayak, 1996]. 

The first two approaches cannot be used when there are too 
many possible worlds (e.g., when the domain includes more 
than a few dozens of fluents and there are more than 240 possible 
states) or when the sequence of actions is long (e.g., 
more than 100 actions). Examples include robot localization, 
tracking of objects and their relationships, and data mining. 
The third approach gives rise to many mistakes that are sometimes 
dangerous, and requires an approximation that fits the 
given problem (if one exists). Many domains of 100 fluents 
or less are still computationally infeasible for it. 

 Logical Filtering 

In this section we define logical filtering using a transition 
model and action semantics that are compatible with the standard 
semantics belief update operator of [Winslett, 1990]. 
(To avoid confusion, this is different from another operator 
presented in the same publication, PMA, that applies a non-
monotonic approach to formalize minimal change.) This operator 
is simple and allows us to examine computational properties 
easily. It can represent any logical transition system, 
and specifications in other action languages can be compiled 
into it IWinslett, 1990; Doherty et a/., 1998]. 

propositions that are true in this world state), A is the set of 
actions in the system and Rl(s, a, s') means that state s' is a 
possible result of action a in state s. 

A logical nondeterministic domain description D is a finite 
set of statements of the following kinds: value propositions 
of the form 'Initially F" describe the initial state and 
effect rules of the form "a causes F if G" describe the effects 
of actions, for F and G being state formulae (propositional 
combinations of fluent names). We say that F is the head and 
G is the tail of those rules. 

When there is no confusion, we write R for RD. 
If action a has an effect of FALSE in s, then it cannot execute. 

In partially observable domains, we update our knowledge 
as a result of executing an action and collecting observations 
in the resulting state. The following definition of filtering assumes 
that a is a set of world states. We use our transition 
operator R to define the resulting belief state from each action. 
An observation o is a formula in our language. 


We call Step 2 progression with a and Step 3 filtering with o. 

For example, consider a robot that is in charge of cleaning 
a room. It can execute an action a = fetch(broom, closet) 
which has the single effect rule "a causes has(broom) A 

 Filtering Logical Formulae 

Approaches to filtering actions and observations that at any 
stage enumerate the states in a belief state do not scale to 
large domains. An alternative approach is to perform logical 
progression in a form similar to the one described by [Lin and 
Reiter, 1997; Mcllraith, 1998]. The difference is that now we 
wish to do so (efficiently) in the context of nondeterministic 
actions and observations. 

In this section we present a straightforward algorithm that 
filters belief state formulae directly, but does so in worst-case 
exponential time. This algorithm serves as a starting point 
for Section 4, where we propose efficient algorithms. We 
also present distribution properties of filtering over the log


3.2 Distribution Properties and Permutation 
Definition 3.3 (Permuting Actions). Action a is permuting, 
if for every state s' there is at most one s such that R(s, a, s'). 

the assertion that no precondition of a holds. This has a simDomains 
that include only permuting actions are called 
ilar effect to adding frame axioms to a set of effect axioms in permutation domain. 

For example, consider a \A1\AA fetch(broom, closet) from 
above, and assume that we add a second effect rule "a 
causes FALSE if -in{broom, closet)". Thus, a is not executable 
unless its first rule's precondition holds. Then, the action 
is a one-to-one mapping between states, when this mapping 
is defined (it is not defined when a state maps to no resulting 
state). If this second rule is not added, then the action 
is not one-to-one because it maps two different states (in the 
first we already have the broom and in the second the broom 
is in the closet) to the same state. 

In the same spirit, an action pickUp(A, D) that picks up 
block A from the top of block B is one-to-one when it is possible 
because we can find a single previous state for every 
resulting state. The same holds for putDown(A,C). Other 
natural examples include turning a row in a Rubik's cube, 
flipping a light switch, and buying a gallon of gas. In contrast, 
turning on the light, setting a Rubik's cube to a particular 
configuration, and filling up the gas tank are not permutation 
actions. Notice that we allow different actions to map 
different states to the same state (e.g., accelerating by 5MPH 
when driving 40MPH results in the same state as when decelerating 
by 5MPH when driving 50MPH). 

The proof of this theorem reduces the problem of representing 
the belief state after performing an action to that of 
representing a Craig Interpolant. It uses the following. 

4 Efficient and Indefinitely Compact Filtering 

In this section we present the main contribution of this paper, 
namely, a polynomial-time algorithm that computes logical 
filtering exactly for a significant class of transition systems. 
For some special cases we present simpler algorithms that 
are even more efficient. For systems that do not fall within 
this class our algorithm gives an approximation to the filtering. 
Also, we show that we can keep the representation of 
the filtered belief state compact indefinitely for a class of dynamic 
systems. This class includes nondeterministic STRIPS 
systems and some systems whose actions are permuting. 

3.3 Limitations for Compact Representation 
It may be argued that filtering may require only polynomial 
space, if we limit ourselves to initial belief states that are represented 
compactly and to actions whose effects and preconditions 
are represented compactly. In Theorem 3.5 we show 
the contrary. That is, for every general-purpose representation 
of belief states there is a dynamic system, an initial belief 
state, and a sequence of actions after which our belief 
state representation is exponential in the representation of the 
initial belief state. 

The conditions on action a in the last theorem hold, e.g., for 
actions whose every defining rule has a precondition that is a 
single clause (e.g., a literal). It also holds for actions which 
are defined by at most two rules, and actions that affect all the 
literals that appear in their preconditions. 

Theorem 3.5. There is dynamic system D with n fluents, belief 
state 0, and action sequence a\,..., an such that, for all 

Similarly, for in(broom, closet), one of our tests in step 1 
of NNF-ProgressStep finds that 

4.2 DNF and CNF Belief States 
Thus, when a is a deterministic action (every rule's effect 
is a conjunction of literals) with a single effect rule that is 
always guaranteed to succeed, then the number of disjuncts 
in the formula does not grow as the filtering progresses. 

For CNF formulae we can find a more significant class of 
actions that allow us to maintain compact representation. We 
show that under some conditions every fc-CNF formula is filtered 
into a fc-CNF formula (fixed fc). This implies that the 

belief state representation is no larger than (2n)k which is 
manageable for small fixed K's. 

The main observation that we use is that a clause of A: literals 
may give rise to a larger clause after filtering, only if one 
of the following holds: (a) the filtering of TRUE includes a 
clause of more than fc literals; or (b) the filtering of a literal 
includes a clause of 2 or more literals that is not subsumed by 
Filter[a](TRUE). The first case can occur when we do not 
know whether the action succeeded or not, and which rules 
applied if it did. In that case, we know that after the action 
one of the effects holds or no precondition holds (this yields a 
formula which may include many disjunctions). The second 
case can occur when the precondition of a rule includes a conjunction 
of literals. When we filter a single literal we may get 
a disjunction in the result (of the form the effect holds, or the 
rest of the precondition does not). When we filter a clause, 
this may cause the filtering to include a larger clause. 

The following theorem describes sufficient conditions for 
filtering a A:-CNF formula into A:-CNF, thus keeping the representation 
compact (fc is fixed). 

4.3 Prime-Implicate Belief States 
It turns out that a form of distribution over conjunction holds 
for all actions if the belief state is represented as the conjunction 
of all of its prime implicates (formulae we call prime implicate 
belief states (Pl-CNF)). In this form we can distribute 
the computation to the conjuncts and conjoin the result of filtering 
small subgroups of them separately. More precisely, 

for z a number that depends on the representation of the preconditions 
of a and on the number of rules defining a. 

Figure 3: Filtering a PI-CNF formula with STRIPS actions. 

We tested our STRIPS-filter algorithm in partially observable 
blocks-world domains. The implementation in LISP includes 
a random action-sequence and observation-sequence 
generator, and both the generator and filtering algorithm receive 
a description of the domain, actions and observations 
specified in PDDL (a plan-problem specification language). 

We ran the algorithm with blocks-world domains of increasing 
sizes (3 to 20 blocks), yielding domains that range 
from tens to over a thousand of propositional features. We 
collected the time taken per filtering step for each of the executions 
and the space taken overall at every iteration, starting 
with zero knowledge. The results are shown in Figure 4. 

Conclusions 

In this paper we presented the task of logical filtering and 
gave it a computational treatment. The results we obtained 
here have implications for monitoring and controlling dynamic 
systems. In many cases we present a closed-form computation 
of the filtering and in others show how to approximate 
this computation. In some cases we can guarantee that 
the size of the representation of the filtered formula can be 
bounded and kept small. In those cases, logical filtering can 
be used to control processes that run over very long periods 
of time. Examples of such systems are abundant and include 
robot motion control, natural language processing, and agents 
that explore partially observed worlds. 

We made use of several assumptions in this paper in different 
contexts and with different consequences. We presented 
permutation domains and the certainty of existence of an effect 
(D A a I= ->G) as characteristics of the domain that make 
filtering easier. We showed that the commonly used assumption 
that every action has a relatively small number of rules 
(at most polynomial in n), and that effects, preconditions and 
terms in the belief state typically use a small vocabulary, all 

have a drastic effect on the computational effort needed for 
filtering and on the size of the resulting belief state. 

The need to track the state of the world is a basic one, and 
many works have appealed to it implicitly in the past. However, 
the computational treatment of such tracking has been 
avoided so far, partially due to the absence of a developed 
theory of nondeterministic domains, and partially due to negative 
results about the general cases of this task. Nonetheless, 
this problem and methods for its solution have received 
much attention in control theory. The results we obtained here 
promise to find their application in this domain and may be 
combined with stochastic filtering techniques. 

Acknowledgments 

This research was supported by ONR MURI Fund N0001401-
1-0890, ONR MURI Fund N00014-00-1-0637, and NSF 
grant ECS-9873474. The first author thanks Xuanlong 
Nguyen for a stimulating discussion on Theorem 3.4. 


Abstract 

In previous work, Levesque proposed an extension 
to classical databases that would allow for a certain 
form of incomplete first-order knowledge. Since 
this extension was sufficient to make full logical deduction 
undecidable, he also proposed an alternative 
reasoning scheme with desirable logical properties. 
He also claimed (without proof) that this 
reasoning could be implemented efficiently using 
database techniques such as projections and joins. 
In this paper, we substantiate this claim and show 
how to adapt a bottom-up database query evaluation 
algorithm for this purpose, thus obtaining a 
tractability result comparable to those that exist for 
databases. 

1 Introduction 

As argued in [Levesque, 1998], there is only one deductive 
technique efficient enough to be feasible on knowledge bases 
(KBs) of the size seemingly required for common-sense reasoning: 
the deduction underlying classical database query 
evaluation. And yet, databases by themselves are too restricted 
to serve as the representational scheme for commonsense 
reasoning, since they require, among other things, complete 
knowledge of the domain. Levesque proposed a generalization 
of databases called proper knowledge bases, which 
allow for a limited form of incomplete knowledge. Despite 
the limitations, however, the deduction problem for proper 
KBs is no longer even decidablc. Levesque proposed an alternative 
reasoning procedure V for proper KBs that was logically 
sound and, when the query was in a certain normal form, 
logically complete. Moreover, he argued that it should be 
possible to implement V for very large KBs using database 
techniques. However, no proof was given. 

In this paper, we examine proper KBs and the V procedure 
more closely, and we prove a tractability result for this type 
of logical reasoning that are comparable to those that exist for 
classical database query evaluation. In particular, we adapt a 
bottom-up database retrieval algorithm to the case of proper 
KBs. Thus, what we show here is that in some cases it is 
indeed possible to reason efficiently with incomplete knowledge 
in a logically sound and complete way, or at least as 
efficiently as we can with a database. 

The rest of the paper is organized as follows. In the next 
section, we review proper KBs and V, prove a new property 
of V, i.e. locality, and define answers to open queries. In 
Section 3, we review the complexity of database query evaluation, 
and present a polynomial time algorithm for evaluating 
K-guarded formulas. In Section 4, we show how to use this algorithm 
to evaluate queries wrt proper KBs and hence obtain 
a tractability result. In Section 5, we illustrate this query evaluation 
method for proper KBs with some example queries. 
Finally in Section 6, we describe some future work. 

To illustrate the idea of a proper KB, imagine a scenario involving 
a robot security guard keeping track of the occupants 
of the rooms of a building. The robot can find out that someone 
has entered or left a room, and by going into a room, find 
out who the occupants are. We can express what the robot 
might know using a proper KB:1 

1We think of these as beliefs about some initial state of the world. 
It should be possible to generalize the notion of proper KB to deal 
with state change. See Section 6 for discussion. 

Note that this information cannot be expressed in a traditional 
database where, e.g. we cannot leave open the occupants of 
Room 2. On the other hand, facts requiring disjunction or existential 
quantification cannot be expressed as part of a proper 
KB, e.g. every room having at most three occupants. 

It is not hard to see that the problem of determining 
whether a sentence is logically entailed by a proper KB is 
undecidable, since when the KB is empty, this reduces to 
classical validity.2 Levesque [1998] proposed the reasoning 
procedure V instead. Given a proper KB and a query, V returns 
one of three values 0 (known false), 1 (known true), or 
1/2 (unknown) as follows: 

Proof: Follows from the theorem by taking * as the bijection 
that swaps c and d and leaves the rest constants unchanged. 
2.3 Locality 
2.2 Invariance under renaming 
The first difficulty that arises when attempting to reason with 
proper KBs is the fact that unlike with databases, we cannot 
fix the domain in advance, or even put an upper bound on its 

2.4 Answers to open queries 
other than a behave the same. This will allow us to define 
finite versions of answers to open queries, as shown below. 
Notation Let a be a formula and S a set of formulas. Let * 
be a bijection from C to C. We use a* to denote a with each 

2The classical validity problem is undecidable even when there 
are no function symbols. 

3 The complexity of database queries 

3.1 An overview 
The complexity of query evaluation has been one of the main 
pursuits of database theory. Traditionally, there are two complexity 
measures: combined complexity and data complexity 
[Vardi, 1982]. Combined complexity is measured in terms 
of the combined size of the database and the query. Chandra 
and Merlin [1977] proved that the combined complexity 
of conjunctive queries (queries expressed by first-order formulas that are of the form where a, are atoms) is NP-complete; Vardi L1982] proved that the combined 
complexity of first-order queries is PSPACE-complete. 
However, the main factor responsible for this high complexity 
is the size of the query and not the size of the database. This is 
contrary to the situation in practice where we normally evaluate 
small queries against large databases. Data complexity 
measures the complexity of query evaluation solely in terms 
of the size of the database and treats the size of the query as a 
constant. A folk result is that first-order queries can be evaluated 
in time no(1) where n is the size of the database and / is 
the size of the query. Thus the data complexity of first-order 
queries is in PT1ME. However, such a complexity can hardly 
qualify as tractable even if the exponent is small, say 5. 

Yannakakis [19951 was the first to suggest that parameterized 
complexity [Downey and Fellows, 1995] might be an 
appropriate complexity measure for database query evaluation. 
Query evaluation is fixed-parameter tractable if there is 
a computable function / : N \A1\AA. N and a constant c such that 
the problem can be solved in time /(/) . nc, where / and n are 
the size of the query and the database, respectively. However, 
Papadimitriou and Yannakakis [1999] proved that the parameterized 
complexity of conjunctive queries is W[l]-complete 
and thus most likely not fixed-parameter tractable. We refer 
the reader to [Grohe, 2002] for a survey on parameterized 
complexity in database theory. 

Therefore database query evaluation in general is hard with 
respect to both combined and parameterized complexity. And 
yet, database queries do work quite well in practice, even 
for very large databases. A careful examination of the hardness 
results show that they often depend on queries that are 
somewhat atypical in applications, e.g. databases representing 
graphs and queries asking for the existence of cliques. 

Naturally, many research efforts have gone into finding 
classes of queries that can be proven tractable, even in the 
worst cases. The earliest result of this form, due to Yannakakis 
[1981], showed that acyclic conjunctive queries can 
be evaluated in polynomial time. This result has been extended 
in several ways. The first extension, due to Chekuri 
and Rajaraman [1997], showed that conjunctive queries 
with bounded tree width are tractable. Later, Gottlob et al. 
[1999] introduced the notion of hypertree width and showed 
that conjunctive queries with bounded hypertree width are 
tractable; bounded hypertree width generalizes the notions 
of acyclicity and bounded treewidth. Recently, Flum et al 
[2001] generalized the notions of acyclicity and bounded 
treewidth from conjunctive queries to nonrecursive stratified 
Datalog (NRSD), which is known to have the same expressive 
power as all of first-order logic, and showed that acyclic 
and bounded treewidth NRSD are tractable. Inspired by their 
work, Gottlob et al [2001] extended the notion of hypertree 
width to NRSD, and obtained a nice logical characterization 
of hypertree width: they showed that the K-guarded fragment 
of first-order logic has the same expressive power as NRSD 
of hypertree width at most k. Thus K-guardcd first-order logic 
turns out to be the largest tractable class of queries so far. 

3.2 An evaluation algorithm 
In this section, we introduce the K-guarded fragment of \A1\EA, 
and explicitly present a polynomial algorithm for evaluating 
K-guarded formulas against databases and analyze its complexity. 
We will use this algorithm to evaluate K-guarded formulas 
with respect to proper KBs. 

Note that any /c-guarded sentence is strictly K-guarded. 
The evaluation algorithm below will take a strictly K-
guarded formula as the query. It turns out that any formula 

The evaluation algorithm will have worst-case time com4 
The complexity of V 

plexity that is exponential only in the k. For fixed k, the algorithm 
is polynomial, but perhaps impractical when k is large. 

Before presenting the evaluation algorithm, we first recall 
some basic notions of relational database theory. A database 
instance is simply a logical structure (or interpretation) with 
a finite domain. Let A be such a structure with domain A. 
Let X and Y be sets of variables. An X-relation R over A, is 

Here is the algorithm for computing answers: 

relation that is a subset of R. Moreover, the R is obtained 
from joins of at most A: guards, and so has size at most nk. 
Assuming all relations start in sorted form, the results of all 
operations can be kept in sorted form, and computed in time 
0(nk). Thus, the time overall is 0(lnk).

In this section, we consider how hard it is to compute V. Not 
surprisingly, V is no easier than database query evaluation. 
What is more significant is that under reasonable assumptions, 
it is not much harder either. 

4.1 Intractability results 
Theorem 4.1 The combined complexity of V is NP-hard 
for conjunctive queries, and PS PACE-hard for first-order 
queries. The parameterized complexity of V is W [1]-hard for 
conjunctive queries. 

Proof: The proof is essentially the same as that for database 
query evaluation. For conjunctive queries, the reduction is 
from the clique problem. For first-order queries, the reduction 
is from QBF (Quantified Boolean Formula). 

4.2 A tractability result 
We present examples of this transformation in the next section. 
Here we note the following correctness result: 

3 A positive (resp. negative) occurrence of P in a is one within 
the scope of an even (resp. odd) number of negations. 

This theorem says that the (finite version of) answers to 
open queries in C correspond exactly to the answers we get 
for the database constructed as above. This is a consequence 
of locality and the following lemma: 

To obtain a tractability result for open queries over proper 
KBs, we need only ensure that the database queries we construct 
are k-guarded then use the Eval procedure. 

Now we get our main complexity result: 

Proof: Follows from previous corollary and the fact that V is 
logically sound and complete for queries in NF. 
Note that these bounds are identical to their database counterparts 
modulo the w factor. In most cases of interest, the 
w will be small since even in a large KB, we only expect to 
see a large ep or e-p in database-like cases, where the e is 
relational or co-relational. In the case where none of the e are 
relational or co-relational, we get the following: 

5 An example 
To illustrate how query evaluation would work with proper 
KBs, we return to the example robotic scenario mentioned 
in the first section, involving the predicate ln (person, room). 
We can see from the example that e/n is given in relational 
form, while e-,In, is given in unrestricted form. To make 
things interesting, we assume two more predicates: One is 
Mgr (person1,person2) saying that the first person is a manager 
and that the second is one of his or her employees; we 
assume that Mgr is given as a closed database predicate. The 
other is Cmp(person1,person2) saying that the two persons 
are compatible. We assume that the robot knows that any two 
people are compatible except 77/(> 2) pairs; among these m 
pairs, the robot only knows that two pairs are not compatible. 

Here are some example queries,with the guards underlined. 

Observe that in all cases, the queries are strictly 2-guarded 
wrt the proper KB. Also, because no query contains a literal 
and a unifiable literal of opposite polarity, from results 
in [Levesque, 1998], all the queries are in NF. 

The corresponding database queries are as follows: 

6 Conclusions 

In this paper, we have shown how a bottom-up query evaluation 
procedure for databases can be used to answer queries for 
KBs with a certain form of incomplete knowledge. Although 
this procedure can be impractical for k-guarded queries 
where A: is large, they would be impractical for databases too. 

A number of questions remain to be addressed. First of all, 
Lakemeyer and Levesque [2002] have proposed an extension 
to proper KBs that allow disjunctions in the KB. It would be 
interesting to see how much of the database retrieval mechanism 
could be preserved in this case. We can also imagine 
other extensions to proper KBs, such as relaxing the unique 
name assumption over constants, or allowing a limited use 
of function symbols. Also, as suggested in the first section, 
we can imagine a dynamic scenario where at any given point 
what a robot or agent knows about the current situation is expressible 
as a proper KB. It would then be useful to amalgamate 
regression-based techniques for reasoning about change 
from [Reiter, 2001] with the database techniques considered 
here. Among other things, this would require determining 
those cases where the successor state axioms guarantee that 
a proper KB remains proper after an action has been performed, 
perhaps along the lines of [Petrick and Levesque, 
2002]. It would also be interesting to investigate the relationship 
between proper KBs and other subsets of logic to see if 
the complexity results presented here can be further generalized. 
Two immediate candidates are datalog programs and 
stratified logic programs that include some form of classical 
negation. Finally, we note that additional optimizations can 
be made to our query evaluation procedure that do not change 
the worst-case performance, but would improve its behaviour 
in practice. 

Acknowledgments 

We would like to thank Leonid Libkin for pointing us to the 
relevant database literature. Financial support was gratefully 
received from the Natural Sciences and Engineering Research 
Council of Canada. 



Abstract 

In previous work, Levesque proposed an extension 
to classical databases that would allow for a certain 
form of incomplete first-order knowledge. Since 
this extension was sufficient to make full logical deduction 
undecidable, he also proposed an alternative 
reasoning scheme with desirable logical properties. 
He also claimed (without proof) that this 
reasoning could be implemented efficiently using 
database techniques such as projections and joins. 
In this paper, we substantiate this claim and show 
how to adapt a bottom-up database query evaluation 
algorithm for this purpose, thus obtaining a 
tractability result comparable to those that exist for 
databases. 

1 Introduction 

As argued in [Levesque, 1998], there is only one deductive 
technique efficient enough to be feasible on knowledge bases 
(KBs) of the size seemingly required for common-sense reasoning: 
the deduction underlying classical database query 
evaluation. And yet, databases by themselves are too restricted 
to serve as the representational scheme for commonsense 
reasoning, since they require, among other things, complete 
knowledge of the domain. Levesque proposed a generalization 
of databases called proper knowledge bases, which 
allow for a limited form of incomplete knowledge. Despite 
the limitations, however, the deduction problem for proper 
KBs is no longer even decidablc. Levesque proposed an alternative 
reasoning procedure V for proper KBs that was logically 
sound and, when the query was in a certain normal form, 
logically complete. Moreover, he argued that it should be 
possible to implement V for very large KBs using database 
techniques. However, no proof was given. 

In this paper, we examine proper KBs and the V procedure 
more closely, and we prove a tractability result for this type 
of logical reasoning that are comparable to those that exist for 
classical database query evaluation. In particular, we adapt a 
bottom-up database retrieval algorithm to the case of proper 
KBs. Thus, what we show here is that in some cases it is 
indeed possible to reason efficiently with incomplete knowledge 
in a logically sound and complete way, or at least as 
efficiently as we can with a database. 

The rest of the paper is organized as follows. In the next 
section, we review proper KBs and V, prove a new property 
of V, i.e. locality, and define answers to open queries. In 
Section 3, we review the complexity of database query evaluation, 
and present a polynomial time algorithm for evaluating 
K-guarded formulas. In Section 4, we show how to use this algorithm 
to evaluate queries wrt proper KBs and hence obtain 
a tractability result. In Section 5, we illustrate this query evaluation 
method for proper KBs with some example queries. 
Finally in Section 6, we describe some future work. 

To illustrate the idea of a proper KB, imagine a scenario involving 
a robot security guard keeping track of the occupants 
of the rooms of a building. The robot can find out that someone 
has entered or left a room, and by going into a room, find 
out who the occupants are. We can express what the robot 
might know using a proper KB:1 

1We think of these as beliefs about some initial state of the world. 
It should be possible to generalize the notion of proper KB to deal 
with state change. See Section 6 for discussion. 

Note that this information cannot be expressed in a traditional 
database where, e.g. we cannot leave open the occupants of 
Room 2. On the other hand, facts requiring disjunction or existential 
quantification cannot be expressed as part of a proper 
KB, e.g. every room having at most three occupants. 

It is not hard to see that the problem of determining 
whether a sentence is logically entailed by a proper KB is 
undecidable, since when the KB is empty, this reduces to 
classical validity.2 Levesque [1998] proposed the reasoning 
procedure V instead. Given a proper KB and a query, V returns 
one of three values 0 (known false), 1 (known true), or 
1/2 (unknown) as follows: 

Proof: Follows from the theorem by taking * as the bijection 
that swaps c and d and leaves the rest constants unchanged.  

2.3 Locality 
2.2 Invariance under renaming 
The first difficulty that arises when attempting to reason with 
proper KBs is the fact that unlike with databases, we cannot 
fix the domain in advance, or even put an upper bound on its 

2.4 Answers to open queries 
other than a behave the same. This will allow us to define 
finite versions of answers to open queries, as shown below. 
Notation Let a be a formula and S a set of formulas. Let * 
be a bijection from C to C. We use a* to denote a with each 

2The classical validity problem is undecidable even when there 
are no function symbols. 

3 The complexity of database queries 

3.1 An overview 
The complexity of query evaluation has been one of the main 
pursuits of database theory. Traditionally, there are two complexity 
measures: combined complexity and data complexity 
[Vardi, 1982]. Combined complexity is measured in terms 
of the combined size of the database and the query. Chandra 
and Merlin [1977] proved that the combined complexity 
of conjunctive queries (queries expressed by first-order formulas 
that are of the form

 
where a, are 
atoms) is NP-complete; Vardi L1982] proved that the combined 
complexity of first-order queries is PSPACE-complete. 
However, the main factor responsible for this high complexity 
is the size of the query and not the size of the database. This is 
contrary to the situation in practice where we normally evaluate 
small queries against large databases. Data complexity 
measures the complexity of query evaluation solely in terms 
of the size of the database and treats the size of the query as a 
constant. A folk result is that first-order queries can be evaluated 
in time no(1) where n is the size of the database and / is 
the size of the query. Thus the data complexity of first-order 
queries is in PT1ME. However, such a complexity can hardly 
qualify as tractable even if the exponent is small, say 5. 

Yannakakis [19951 was the first to suggest that parameterized 
complexity [Downey and Fellows, 1995] might be an 
appropriate complexity measure for database query evaluation. 
Query evaluation is fixed-parameter tractable if there is 
a computable function / : N \A1\AA. N and a constant c such that 
the problem can be solved in time /(/) . nc, where / and n are 
the size of the query and the database, respectively. However, 
Papadimitriou and Yannakakis [1999] proved that the parameterized 
complexity of conjunctive queries is W[l]-complete 
and thus most likely not fixed-parameter tractable. We refer 
the reader to [Grohe, 2002] for a survey on parameterized 
complexity in database theory. 

Therefore database query evaluation in general is hard with 
respect to both combined and parameterized complexity. And 
yet, database queries do work quite well in practice, even 
for very large databases. A careful examination of the hardness 
results show that they often depend on queries that are 
somewhat atypical in applications, e.g. databases representing 
graphs and queries asking for the existence of cliques. 

Naturally, many research efforts have gone into finding 
classes of queries that can be proven tractable, even in the 
worst cases. The earliest result of this form, due to Yannakakis 
[1981], showed that acyclic conjunctive queries can 
be evaluated in polynomial time. This result has been extended 
in several ways. The first extension, due to Chekuri 
and Rajaraman [1997], showed that conjunctive queries 
with bounded tree width are tractable. Later, Gottlob et al. 
[1999] introduced the notion of hypertree width and showed 
that conjunctive queries with bounded hypertree width are 
tractable; bounded hypertree width generalizes the notions 
of acyclicity and bounded treewidth. Recently, Flum et al 
[2001] generalized the notions of acyclicity and bounded 
treewidth from conjunctive queries to nonrecursive stratified 
Datalog (NRSD), which is known to have the same expressive 
power as all of first-order logic, and showed that acyclic 
and bounded treewidth NRSD are tractable. Inspired by their 
work, Gottlob et al [2001] extended the notion of hypertree 
width to NRSD, and obtained a nice logical characterization 
of hypertree width: they showed that the K-guarded fragment 
of first-order logic has the same expressive power as NRSD 
of hypertree width at most k. Thus K-guardcd first-order logic 
turns out to be the largest tractable class of queries so far. 

3.2 An evaluation algorithm 
In this section, we introduce the K-guarded fragment of \A1\EA, 
and explicitly present a polynomial algorithm for evaluating 
K-guarded formulas against databases and analyze its complexity. 
We will use this algorithm to evaluate K-guarded formulas 
with respect to proper KBs. 

Note that any /c-guarded sentence is strictly K-guarded. 
The evaluation algorithm below will take a strictly K-
guarded formula as the query. It turns out that any formula 

The evaluation algorithm will have worst-case time com4 
The complexity of V 

plexity that is exponential only in the k. For fixed k, the algorithm 
is polynomial, but perhaps impractical when k is large. 

Before presenting the evaluation algorithm, we first recall 
some basic notions of relational database theory. A database 
instance is simply a logical structure (or interpretation) with 
a finite domain. Let A be such a structure with domain A. 
Let X and Y be sets of variables. An X-relation R over A, is 

Here is the algorithm for computing answers: 

relation that is a subset of R. Moreover, the R is obtained 
from joins of at most A: guards, and so has size at most nk. 
Assuming all relations start in sorted form, the results of all 
operations can be kept in sorted form, and computed in time 
0(nk). Thus, the time overall is 0(lnk). 

In this section, we consider how hard it is to compute V. Not 
surprisingly, V is no easier than database query evaluation. 
What is more significant is that under reasonable assumptions, 
it is not much harder either. 

4.1 Intractability results 
Theorem 4.1 The combined complexity of V is NP-hard 
for conjunctive queries, and PS PACE-hard for first-order 
queries. The parameterized complexity of V is W [1]-hard for 
conjunctive queries. 

Proof: The proof is essentially the same as that for database 
query evaluation. For conjunctive queries, the reduction is 
from the clique problem. For first-order queries, the reduction 
is from QBF (Quantified Boolean Formula).
4.2 A tractability result 
We present examples of this transformation in the next section. 
Here we note the following correctness result: 

3 A positive (resp. negative) occurrence of P in a is one within 
the scope of an even (resp. odd) number of negations. 

This theorem says that the (finite version of) answers to 
open queries in C correspond exactly to the answers we get 
for the database constructed as above. This is a consequence 
of locality and the following lemma: 

To obtain a tractability result for open queries over proper 
KBs, we need only ensure that the database queries we construct 
are k-guarded then use the Eval procedure. 

Now we get our main complexity result: 

Proof: Follows from previous corollary and the fact that V is 
logically sound and complete for queries in NF. 
Note that these bounds are identical to their database counterparts 
modulo the w factor. In most cases of interest, the 
w will be small since even in a large KB, we only expect to 
see a large ep or e-p in database-like cases, where the e is 
relational or co-relational. In the case where none of the e are 
relational or co-relational, we get the following: 

5 An example 
To illustrate how query evaluation would work with proper 
KBs, we return to the example robotic scenario mentioned 
in the first section, involving the predicate ln (person, room). 
We can see from the example that e/n is given in relational 
form, while e-,In, is given in unrestricted form. To make 
things interesting, we assume two more predicates: One is 
Mgr (person1,person2) saying that the first person is a manager 
and that the second is one of his or her employees; we 
assume that Mgr is given as a closed database predicate. The 
other is Cmp(person1,person2) saying that the two persons 
are compatible. We assume that the robot knows that any two 
people are compatible except 77/(> 2) pairs; among these m 
pairs, the robot only knows that two pairs are not compatible. 

Here are some example queries,with the guards underlined. 

Observe that in all cases, the queries are strictly 2-guarded 
wrt the proper KB. Also, because no query contains a literal 
and a unifiable literal of opposite polarity, from results 
in [Levesque, 1998], all the queries are in NF. 

The corresponding database queries are as follows: 

6 Conclusions 

In this paper, we have shown how a bottom-up query evaluation 
procedure for databases can be used to answer queries for 
KBs with a certain form of incomplete knowledge. Although 
this procedure can be impractical for k-guarded queries 
where A: is large, they would be impractical for databases too. 

A number of questions remain to be addressed. First of all, 
Lakemeyer and Levesque [2002] have proposed an extension 
to proper KBs that allow disjunctions in the KB. It would be 
interesting to see how much of the database retrieval mechanism 
could be preserved in this case. We can also imagine 
other extensions to proper KBs, such as relaxing the unique 
name assumption over constants, or allowing a limited use 
of function symbols. Also, as suggested in the first section, 
we can imagine a dynamic scenario where at any given point 
what a robot or agent knows about the current situation is expressible 
as a proper KB. It would then be useful to amalgamate 
regression-based techniques for reasoning about change 
from [Reiter, 2001] with the database techniques considered 
here. Among other things, this would require determining 
those cases where the successor state axioms guarantee that 
a proper KB remains proper after an action has been performed, 
perhaps along the lines of [Petrick and Levesque, 
2002]. It would also be interesting to investigate the relationship 
between proper KBs and other subsets of logic to see if 
the complexity results presented here can be further generalized. 
Two immediate candidates are datalog programs and 
stratified logic programs that include some form of classical 
negation. Finally, we note that additional optimizations can 
be made to our query evaluation procedure that do not change 
the worst-case performance, but would improve its behaviour 
in practice. 

Acknowledgments 

We would like to thank Leonid Libkin for pointing us to the 
relevant database literature. Financial support was gratefully 
received from the Natural Sciences and Engineering Research 
Council of Canada. 



Abstract 

We revisit the problem of revising probabilistic beliefs 
using uncertain evidence, and report results on 
four major issues relating to this problem: How 
to specify uncertain evidence? How to revise a 
distribution? Should, and do, iterated belief revisions 
commute? And how to provide guarantees on 
the amount of belief change induced by a revision? 
Our discussion is focused on two main methods for 
probabilistic revision: Jeffrey's rule of probability 
kinematics and Pearl's method of virtual evidence, 
where we analyze and unify these methods from the 
perspective of the questions posed above. 

1 Introduction 

We consider in this paper the problem of revising beliefs 
given uncertain evidence, where beliefs are represented using 
a probability distribution. There are two main methods for 
revising probabilistic beliefs in this case. The first method is 
known as Jeffrey's rule and is based on the principle of probability 
kinematics, which can be viewed as a principle for minimizing 
belief change [Jeffrey, 1965]. The second method is 
called virtual evidence and is proposed by Pearl in the context 
of belief networks\A1\AAeven though it can be easily generalized 
to arbitrary probability distributions\A1\AAand is based on reducing 
uncertain evidence into certain evidence on some virtual 
event [Pearl, 1988]. We analyze both of these methods in this 
paper with respect to the following four questions: 

1. How should one specify uncertain evidence? 
2. 
How should one revise a probability distribution? 
3. 
Should, and do, iterated belief revisions commute? 
4. 
What guarantees can be offered on the amount of belief 
change induced by a particular revision? 
Our main findings can be summarized as follows. First, we 
show that Jeffrey's rule and Pearl's method both revise beliefs 
using the principle of probability kinematics. Jeffrey's 
rule explicitly commits to this principle, while Pearl's method 
is based on a different principle. Yet, we show that Pearl's 
method implies the principle of probability kinematics, leading 
to the same revision method as that of Jeffrey's. The difference 
between Jeffrey's rule and Pearl's method is in the 

way uncertain evidence is specified. Jeffrey requires uncertain 
evidence to be specified in terms of the effect it has on 
beliefs once accepted, which is a function of both evidence 
strength and beliefs held before the evidence is obtained. 
Pearl, on the other hand, requires uncertain evidence to be 
specified in terms of its strength only. Despite this difference, 
we show that one can easily translate between the two 
methods of specifying evidence and provide the method for 
carrying out this translation. 

The multiplicity of methods for specifying evidence also 
raises an important question: how should informal statements 
about evidence be captured formally using available methods? 
For example, what should the following statement translate 
to: "Seeing these clouds, I believe there is an 80% chance 
that it will rain?" We will discuss this problem of interpreting 
informal evidential statements later, where we emphasize its 
subtlety. 

As to the question of iterated belief revision: It is well 
known that Jeffrey's rule does not commute; hence, the order 
in which evidence is incorporated matters [Diaconis & 
Zabell, 1982]. This has long been perceived as a problem, 
until clarified recently by the work of Wagner who observed 
that Jeffrey's method of specifying evidence is dependent on 
what is believed before the evidence is obtained and, hence, 
should not be commutative to start with [Wagner, 2002]. 
Wagner proposed a method for specifying evidence, based 
on the notion of Bayes factor, and argued that this method 
specifies only the strength of evidence, and is independent of 
the beliefs held when attaining evidence. Wagner argued that 
when evidence is specified in that particular way, iterated revisions 
should commute. He even showed that combining this 
method for specifying evidence with the principle of probability 
kinematics leads to a revision rule that commutes. We 
actually show that Pearl's method of virtual evidence is specifying 
evidence according to Bayes factor, exactly as proposed 
by Wagner and, hence, corresponds exactly to the proposal he 
calls for. Therefore, the results we discuss in this paper unify 
the two main methods for probabilistic belief revision proposed 
by Jeffrey and Pearl, and show that differences between 
them amount to a difference in the protocol for specifying uncertain 
evidence. 

Our last set of results relate to the problem of providing 
guarantees on the amount of belief change induced by a revision. 
We have recently proposed a distance measure for 

bounding belief changes, and showed how one can use it 
to provide such guarantees [Chan & Darwiche, 2002]. We 
show in this paper how this distance measure can be computed 
when one distribution is obtained from another using 
the principle of probability kinematics. We then show how 
the guarantees provided by this measure can be realized when 
applying either Jeffrey's rule or Pearl's method, since they 
both are performing revision based on the principle of probability 
kinematics. 

Probability Kinematics and Jeffrey's Rule 

Consider now the problem of revising a probability distribution 
Pr given uncertain evidence relating to a set of mutu


of specifying uncertain evidence is through the effect that it 
would have on beliefs once accepted. That is, we can say that 

the logical entailment relationship. This is exactly the distribution 
that Jeffrey suggests and, hence, this method of revision 
is known as Jeffrey's rule. We stress here that we 
are drawing a distinction between the principle of probability 
kinematics and Jeffrey's rule, which are often considered 
synonymous. Specifically, Jeffrey's rule arises from a combination 
of two proposals: (1) the principle of probability 
kinematics, and (2) the specification of uncertain evidence using 
a posterior distribution. It is possible for one to combine 
the principle of probability kinematics with other methods for 
specifying evidence as we discuss later. 

Therefore, for any event a, its probability under the new distribution 
Pr' is: 

which is the closed form for Jeffrey's rule. We now show an 
example of using Jeffrey's rule. 

Example 1 (Due to Jeffrey) Assume that we are given a 
piece of cloth, where its color can be one of: green (cg), blue 
(cb), or violet (cv). We want to know whether, in the next day, 
the cloth will be sold (s), or not sold (s). Our original state 
of belief is given by the distribution Pr: 

3 Virtual Evidence and Pearl's Method 

which is the closed form for Pearl 's method. 

The above revision method is a generalization of the 
method of virtual evidence proposed by Pearl [1988] in the 
context of belief networks. A belief network is a graphical 
probabilistic model, composed of two parts: a directed 
acyclic graph where nodes represent variables, and a set of 
conditional probability tables (CPTs), one for each variable 
[Pearl, 1988; Jensen, 2001]. The CPT for variable X with 
parents U defines a set of conditional probabilities of the 
form Pr(x | u), where x is a value of variable X', and u 
is an instantiation of parents U. Suppose now that we have 
some virtual evidence bearing on variable Y, which has val-

One day, Mr. Holmes' receives a call from his neighbor, 
Mrs. Gibbons, saying she may have heard the alarm of his 
house going off. Since Mrs. Gibbons suffers from a hearing 
problem, Mr. Holmes concludes that there is an 80% chance 
that Mrs. Gibbons did hear the alarm going off. According 
to the method of virtual evidence, this uncertain evidence can 

4 Comparing the Revision Methods 

From the illustrations of the two belief revision methods, Jeffrey's 
rule and Pearl's method of virtual evidence, we can see 
that a belief revision method can be broken into two parts: a 
formal method of specifying uncertain evidence, and a principle 
of belief revision that commits to a unique distribution 
among many which satisfy the uncertain evidence. 

4.1 Pearl's method and Probability Kinematics 
We now show that Pearl's method, like Jeffrey's rule, also 
obeys the principle of probability kinematics; what they differ 
in is how uncertain evidence is specified. 

4.3 From Jeffrey's Rule to Pearl's Method 
We can also easily translate from Jeffrey's rule to Pearl's 
method. Specifically, the new probabilities of events 

Therefore, both Jeffrey's rule and Pearl's method obey the 
principle of probability kinematics for belief revision. 

4.2 From Pearl's Method to Jeffrey's Rule 
With the previous result, we now show how we can easily 
translate between the two methods of specifying uncertain 
evidence. For example, to translate from Pearl's method to 
Jeffrey's rule, we note that the new probabilities of events 

Suppose instead that we want to evisc the distribution Pr 
using Jeffrey's rule, assuming that after accepting uncertain 

Substituting the above probability qt in Jeffrey's rule (Equation 
2), we get: 

which is exactly the distribution obtained by the method of 
virtual evidence (Equation 4). We now illustrate this translation 
by revisiting Example 2. 

5 Interpreting Evidential Statements 

The evidence specification protocols adopted by Jeffrey's rule 
and Pearl's method have been discussed by Pearl [2001], in 
relation to the problem of formally interpreting evidential 
statements. Consider the following statement as an example: 

Looking at this evidence, I am willing to bet 2:1 
that David is not the killer. 

This statement can be formally interpreted using either protocol. 
For example, if a denotes the event "David is not the 
killer," this statement can be interpreted in two ways: 

The first interpretation translates directly into a formal piece 
of evidence, Jeffrey's style, and can be characterized as an 
"All things considered" interpretation since it is a statement 
about the agent's final beliefs, which are a function of both 
his prior beliefs and the evidence [Pearl, 2001]. On the other 
hand, the second interpretation translates directly into a formal 
piece of evidence, Pearl's style, and can be characterized 
as a "Nothing else considered" interpretation since it is 
a statement about the evidence only [Pearl, 2001]. 

The two interpretations can lead to contradictory conclusions 
about the evidence. For example, if we use the "Nothing 
else considered" approach to interpret our statement, we will 
conclude that the evidence is against David being the killer. 
However, if we use the "All things considered" interpretation, 
it is not clear whether the evidence is for or against, unless 
we know the original probability that David is the killer. If, 
for example, David is one of four suspects who are equally 
likely to be the killer, we will originally have: Pr(a) = 3/4. 
Therefore, this evidence has actually increased the probability 
that David is the killer! Because of this, Pearl argues for 
the "Nothing else considered" interpretation, as it provides 
a summary of the evidence and the evidence alone, and discusses 
how people tend to use betting odds to quantify their 
beliefs even when they are based on the evidence only [Pearl, 
2001]. 

Example 2 provides another opportunity to illustrate the 
subtlety involved in interpreting evidential statements. The 
evidential statement in this case is "Mr. Holmes concludes 
that there is an 80% chance that Mrs. Gibbons did hear 
the alarm going off." Interpreting this statement using the 
"All things considered" approach gives us the conclusion that 
Pr\a) : Pr' (a) = 4:1, where a denotes the event that 
the alarm has gone off. This interpretation assumes that the 

4 : 1 ratio applies to the posterior beliefs in a and a, after Mr. 
Holmes has accommodated the evidence provided by Mrs. 
Gibson. However, in Example 2, this statement was given a 
"Nothing else considered" interpretation, as by Pearl [1988, 
Page 44-47], where the 4 : 1 ratio is taken as a quantification 
of the evidence strength. That is, the statement is interpreted 
as Pr(n | a) : Pr(n | a) = 4 : 1, where n stands for the 
evidence. In fact, the two interpretations will lead to two different 
probability distributions and, hence, give us different 
answers to further probabilistic queries. For example, if we 
use the "All things considered" approach in interpreting this 
evidential statement, the probability of having a burglary will 
be Pr'{b) = 7.53 x 10 -3, which is much larger than the probability 
we get using the "Nothing else considered" approach 
in Example 2, which is 3.85 x 10-4. 
From the discussions above, the formal interpretation of 
evidential statements appears to be a non-trivial task, which 
can be sensitive to context and communication protocols. Re


gardless of how this is accomplished though, we need to 
stress that the process of mapping an informal evidential 
statement into a revised probability distribution involves three 
distinct elements: 

1. One 
must adopt a formal method for specifying evidence. 
2. 
One must interpret the informal evidential statement, by 
translating it into a formal piece of evidence. 
3. One 
must apply a revision, by mapping the original 
probability distribution and formal piece of evidence 
into a new distribution. 
Our main point here is that Jeffrey's rule and Pearl's method 
employs the same belief revision principle, i.e. probability 
kinematics. Moreover, although they adopt different, formal 
methods for specifying evidence, one can translate between 
the two methods of specification. Finally, one may take the 
view that Jeffrey's rule and Pearl's method constitute commitments 
to how informal evidential statements should be interpreted, 
making the first and second elements the same, but 
wc do not take this view here. Instead, we regard the issue of 
interpretation as a third, orthogonal dimension which is best 
addressed independently. 

6 Commutativity of Iterated Revisions 

We now discuss the problem of commutativity of iterated revisions, 
that is, whether the order in which we incorporate 
uncertain evidence matters.2 

It is well known that iterated revisions by Jeffrey's rule are 
not commutative [Diaconis & Zabell, 1982]. As a simple example, 
assume that we are given a piece of uncertain evidence 
which suggests that the probability of event o is .7, followed 
by another piece of uncertain evidence which suggests that 
the probability of r* is .8. After incorporating both pieces 
in that order, we will believe that the probability of a is .8. 
However, if the reversed order of revision is employed, we 
will believe that the probability of a is .7. In general, even if 
we are given pieces of uncertain evidence on different events, 
iterated revisions by Jeffrey's rule are not commutative. 

This was viewed as a problematic aspect of Jeffrey's rule 
for a long time, until clarified recently by Wagner [2002]. 
First, Wagner observed and stressed that the evidence specification 
method adopted by Jeffrey is suitable for the "All 
things considered" interpretation of evidential statements. 
Moreover, he argued convincingly that when evidential statements 
carry this interpretation, they must not be commutative 
to start with. So the lack of commutativity is not a problem 
of the revision method, but a property of the method used to 
specify evidence. In fact, Wagner suggested a third method 
for specifying evidence based on Bayes factors [Good, 1950; 
1983; Jeffrey, 1992], which leads to commutativity. Specifically, 
if Pr and Pr' are two probability distributions, and 

2There is a key distinction between iterated revisions using certain 
evidence versus uncertain evidence. In the former case, pieces 
of evidence may be logically inconsistent, which adds another dimension 
of complexity to the problem [Darwiche & Pearl, 1997], 
leading to different properties and treatments. 

Interestingly enough, Wagner [2002] showed that when 
evidence is specified using Bayes factors and revisions are 
accomplished by probability kinematics, belief revision becomes 
commutative.4 This was an illustration that Jeffrey's 
rule is indeed composed of two independent elements: an evidence 
specification method, and a revision method. 

In fact, we now show that the evidence specification 
method adopted by Pearl's method corresponds to the method 
of specifying evidence by Bayes factors. This has a number 
of implications. First, it shows that revisions by the virtual 
evidence method are commutative. Second, it provides an 
alternate, more classical, semantics for the virtual evidence 
method. Finally, it shows again that Jeffrey's rule and Pearl's 
method both revise distributions using probability kinematics. 


 Bounding Belief Change Induced by 

Probability Kinematics 
One important question relating to belief revision is that of 
measuring the extent to which a revision disturbs existing 

beliefs. We have recently proposed a distance measure defined 
between two probability distributions which can be used 
to bound the amount of belief change induced by a revision 
[Chan & Darwiche, 2002]. We review this measure next and 
then use it to provide some guarantees on any revision which 
is based on probability kinematics.5 

This distance measure can also be expressed using the Bayes 
factor: 

This measure satisfies the three properties of distance: positiveness, 
symmetry, and the triangle inequality. It is useful to 
compute this distance measure between two probability distributions 
as it allows us to bound the difference in beliefs 
captured by them. 

According to Theorem 1, if we are able to compute the distance 
measure between the original and revised distributions, 
we can get a tight bound on the new belief in any conditional 
event given our original belief in that event. The following 
theorem computes this distance measure for belief revision 
methods based on probability kinematics. 

5The results in this section are reformulations of previous results 
[Chan & Darwiche, 2002], and are inspired by a new understanding 
of Jeffrey's rule and Pearl's method as two specific instances of 
revision based on probability kinematics, and the understanding of 
Pearl's method in terms of Bayes factors. 

Theorem 2 allows us to compute the distance measure for 
revisions based on Jeffrey's rule and virtual evidence. 

The significance of Corollaries l and 2 is that we can compute 
the distance measure easily in both cases. For Jeffrey's 
rule, we can compute the distance measure by knowing only 

We close this section by showing that the principle of probability 
kinematics is optimal in a very precise sense: it commits 
to a probability distribution that minimizes our distance 
measure. 

8 Conclusion 

In this paper, we analyzed two main methods for revising 
probability distributions given uncertain evidence: Jeffrey's 
rule and Pearl's method of virtual evidence. We showed that 
the two methods use the same belief revision principle, i.e. 
probability kinematics, with their difference being only in the 
manner in which they specify uncertain evidence, and showed 
how to translate between the two methods for specifying evidence. 
We also discussed the much debated problem of interpreting 
evidential statements. Moreover, we showed that the 
method of virtual evidence can be reformulated in terms of 
Bayes factors, which implies a number of results, including 
the commutativity of revisions based on this method. Finally, 
we showed that revisions based on probability kinematics are 
optimal in a very specific way, and pointed to a distance measure 
for bounding belief change triggered by any revision 
based on probability kinematics. Our bounds included Jeffrey's 
rule and Pearl's method as special cases. 

Acknowledgments 
This work has been partially supported by NSF grant IIS9988543, 
MURI grant N00014-00-1-0617, and DiMl grant 

00-10065. We also wish to thank Carl Wagner, Judea Pearl, 
and the late Richard Jeffrey, for their valuable comments and 
discussions. 



Abstract 

Degrees of information and of contradiction are investigated 
within a uniform propositional framework, 
based on test actions. We consider that the 
degree of information of a propositional formula 
is based on the cost of actions needed to identify 
the truth values of each atomic proposition, while 
the degree of contradiction of a formula is based 
on the cost of actions needed to make the formula 
classically consistent. Our definitions are to a large 
extent independent of the underlying propositional 
logic; this flexibility is of prime importance since 
there is no unique, fully accepted logic for reasoning 
under inconsistency. 

1 Introduction 

Information and contradiction are two fundamental aspects 
of knowledge processing. Quantifying them is an important 
issue when reasoning about beliefs (or preferences) stemming 
from one or different sources. Here are some contexts where 
quantifying information and contradiction is relevant: 

. diagnosis and testing. In model-based diagnosis, some 
initial assumptions that each component works correctly are 
made; these assumptions may conflict with actual observations. 
Measuring the conflict of the resulting base may be a 
good hint about how hard it will be to identify the faulty components. 
. preference elicitation. In the interactive process of elicitating 
the preference profile of an individual (user) about a set 
of possible alternatives, it is not unfrequent that contradictory 
preferences arise. In this situation, it is useful to quantify and 
localize the contradictions as well as the information about 
the user's preferences, so as to be in position to choose the 
next questions to ask. 
. belief merging. In this framework, degrees of information 
and contradiction can be the basis on which one can decide 
whether to take or not into account the data conveyed by an 
agent. If the degree of contradiction of the data given by an 
agent is high, it may be relevant to reject the information, 
since there is a significant evidence that the source is not reliable; 
however, this must be balanced by the quantity of information 
furnished by the agent, especially when she also gives 
some important and uncontroversial pieces of information. 
. group decision making. Contradictions arise frequently 
when trying to reach a compromise among several agents who 
have possibly conflictual preferences about a common decision 
(like voting, resource sharing, public goods buying). In 
this context, not only it is relevant to compute a global degree 
of conflict (among the set of agents) but also degrees of conflicts 
associated with small groups of agents (coalitions) so as 
to localize as precisely as possible where the conflicts are. 

Now, what do "degree of information" and "degree of contradiction" 
mean? There is no consensus about it. The main 
features shared by existing definitions (and there are not numerous, 
cf. Section 7) is that (1) such degrees are numerical 
values, and (2) they vary depending on the representation language. 
Thus, one may consider a as fully informative in the 
case where a is the single atomic proposition of the language 
but surely not fully informative when the vocabulary also contains 
b (provided that a and b are independent propositions). 

In this paper, our point of view is that it is inadequate 
to quantify the information/contradiction conveyed by some 
data without considering at the same time a set of available 
actions and a goal to be reached. Accordingly, our degrees 
of information and contradiction are defined in an "active" 
way. Acting so as to reduce inconsistency or to gain information 
often relies on performing knowledge-gathering actions 
(also called tests). We consider that the degree of information 
of an information base is based on the number (or 
the cost) of actions needed to identify the truth value of each 
atomic proposition (the lower the cost the more informative 
the base); and that the degree of contradiction of an information 
base is based on the number (or the cost) of actions 
needed to render the base classically consistent. Thus, both 
degrees are dependent on the language but also on the given 
set of tests and the way plans costs are computed. 

The rest of this paper is organized as follows. After some 
formal preliminaries in Section 2, we present our framework 
in Section 3. In order to show the generality of our framework, 
we instantiate it to three different propositional logics: 
classical logic (Section 4), the paraconsistent logic LPm 
(Section 5) and a syntax-based approach to inconsistency 
handling (Section 6). Related work is given in Section 7, and 
some conclusions in Section 8. 

2 Formal preliminaries and notations 

We consider a propositional language Lps based on a finite 
set of propositional symbols PS and a set of connectives that 
may vary depending on the logic used. Well-formed formulas 

a plan is defined as the maximum cost among its trajectories); 
this principle, consisting in assuming the worst outcome, 
is known in decision theory as Wald criterion. Other 
criteria could be used instead, such as the optimistic criterion 
obtained by replacing max by min. More interesting, 
the criterion obtained by first using max and then min for 
tie-breaking, or the leximax criterion, allow for a better discrimination 
than the pure pessimistic criterion. The choice 
of a criterion is fairly independent from the other issues discussed 
in this paper, which gives our framework a good level 
of flexibility and generality. Due to space limitations, however, 
we consider only the pessimistic criterion in the rest of 
the paper. 

This example also shows that mere expansion is not a very 
satisfying revision operator. Indeed, since it does not enable 
to purify any inconsistent base (whatever the test context), 
expansion does not enable as well to disambiguate any inconsistent 
base. Furthermore, it may lead to degrees of contradiction 
(or purification costs) that are not intuitively correct. 

son of this discrepancy between what is expected and what is 
achieved is that expanding an inconsistent information base 
always leads to an inconsistent base, while it would be necessary 
to restore consistency2 for achieving purification and disambiguation 
in classical logic. Note that using AGM revision 
instead of expansion would not help a lot since AGM operators 
do not behave well when applied to inconsistent bases. 

6 Case study 3: "syntax-based" information 
bases 
5 Case study 2: the paraconsistent logic LPm 
Paraconsistent logics have been introduced to avoid exfalso 
quodlibet sequitur of classical logic, hence handling inconsistent 
bases in a much more satisfying way. While many 
paraconsistent logics have been defined so far and could be 
used in our framework, we focus here on the LPm logic as 
defined in [Priest, 1991]. This choice is mainly motivated by 
the fact that this logic is simple enough and has an inference 
relation that coincides with classical entailment whenever the 
information base is classically consistent (this feature is not 
shared by many paraconsistent logics). 
6 Case study 3: "syntax-based" information 
bases 
5 Case study 2: the paraconsistent logic LPm 
Paraconsistent logics have been introduced to avoid exfalso 
quodlibet sequitur of classical logic, hence handling inconsistent 
bases in a much more satisfying way. While many 
paraconsistent logics have been defined so far and could be 
used in our framework, we focus here on the LPm logic as 
defined in [Priest, 1991]. This choice is mainly motivated by 
the fact that this logic is simple enough and has an inference 
relation that coincides with classical entailment whenever the 
information base is classically consistent (this feature is not 
shared by many paraconsistent logics). 

 Related work 

To the best of our knowledge, only few proposals for a notion 
of degree of information can be found in the literature, and 
things are even worse to what concerns the notion of degree 
of contradiction. All existing approaches are stuck to specific 
propositional logics with the corresponding consequence relations, 
which address only some aspects of the paraconsistency 
issue, if any (as evoked previously, there is no undebatable 
paraconsistent inference relation). 

Shannon's information theory [Shannon, 1948] provides 
the most famous approach on which notions of quantity of 
information can be defined, but it relies on the assumption 
that the available information is given under the form of a 
probability distribution; furthermore, it cannot directly address 
inconsistent data. Interestingly, our definition of degree 

of information is general enough to recover classical entropy, 
applied to classical logic4. 

Lozinskii f 1994a] gives a set of properties that a measure 
of quantity of information should satisfy. Our degree of ignorance 
is fully compatible with Lozinskii's requirements in 
several cases. The degree of information defined by Lozinskii 
corresponds to the notion in Shannon's theory, assuming 
a uniform distribution over the set of propositional interpretations5. 
It is thus required that the input information base 

[Knight, 2003] reports some other postulates for a measure 
of quantity of information. Our measure d1 does not 
satisfy all of them, even in simple cases (for space reasons, 
we cannot detail it here). This contrasts with the two measures 
introduced by Knight, which generalize in an elegant 
way Shannon's entropy-based measure to the case the information 
base is an inconsistent set of formulas. However, both 
measures trivialize when the information set is an inconsistent 
singleton. 

The only two approaches we are aware of, which consider 
(non-trivial) degrees of inconsistency defined for clas-

8 Conclusion 

The main contribution of the paper is a uniform action-based 
framework for quantifying both degrees of information and of 
contradiction. The framework is parameterized by a propositional 
logic (together with the corresponding notions of consequence, 
acceptance, contradiction and a revision operator), 
a test context and an aggregation criterion for computing plan 
costs. These parameters enable a great flexibility. 

There are many interesting notions that can be easily defined 
in our framework but that we cannot mention here for 
space reasons. Let us note that through the notion of purification 
plan, our approach for quantifying contradiction also 
allows to localize conflicts. Note also that notions of joint degrees 
and conditional degrees of information / contradiction 
can be easily defined. Another simple extension would consist 
in taking advantage of additional knowledge about the 
sources of information and the origin of conflicts (e.g., in a 
diagnosis setting, it can be the case that the failure of a component 
physically causes the failure of other components). 

Many other extensions of our approach can be envisioned. 
For instance, coping with preferences over the goal variables 
(determining whether a holds is more important than determining 
whether b holds). Another possible extension concerns 
the case where ontic actions arc available and the objective 
is to let the actual world as unchanged as possible (i.e., 
we can execute invasive actions but we prefer not to do it). 


Abstract 

We consider the problem of updating nonmonotonic 
knowledge bases represented by epistemic 
logic programs where disjunctive information and 
notions of knowledge and beliefs can be explicitly 
expressed. We propose a formulation for epistemic 
logic program updates based on a principle 
called minimal change and maximal coherence. 
The central feature of our approach is that during 
an update procedure, contradictory information is 
removed on a basis of minimal change under the 
semantics of epistemic logic programs and then coherent 
information is maximally retained in the update 
result. By using our approach, we can characterize 
an update result in both semantic and syntactic 
forms. We show that our approach handles 
update sequences and satisfies the consistency requirement. 
We also investigate important semantic 
properties of our update approach such as reduction, 
persistence and preservation. 

1 Introduction 

Logic programming has been proved to be one of the most 
promising logic based formulations for problem solving, 
knowledge representation and reasoning, and reasoning about 
actions and plans. Recent research on logic program updates 
further shows that logic programming also provides a feasible 
framework for modeling agents' activities in dynamic environments 
[Alferes and et al, 2000; Eiter and et al, 2002; 
Sakama and Inoue, 1999; Zhang and Foo, 1998]. 

While all current approaches for logic program updates focus 
on the problem of updating extended logic programs or 
their variations, updating epistemic logic programs, however, 
has yet to be explored in the research. By combining knowledge 
and belief operators into logic rules, epistemic logic 
programming [Gelfond, 1994] is a powerful representation 
formalism in logic programming paradigm. It can deal with 
more difficult problems in reasoning with disjunctive information 
while traditional disjunctive extended logic programs 
fail to handle. Furthermore, epistemic logic programs seem 
more feasible for knowledge reasoning than many other autoepistemic 
logics [Gelfond, 1994] and has been used as a 
formal basis for modeling knowledge in action theories, e.g. 

[Lobo et al, 2001]. When we use an epistemic logic program 
to represent an agent's knowledge base, it is a nontrivial 
question how the agent's knowledge base (an epistemic logic 
program) can be updated when new information is received. 

In this paper, we propose an approach for epistemic logic 
program updates. Contrary to other logic programs, notions 
of knowledge and beliefs in epistemic logic programs have 
strong semantic connections to the standard Kripke structures 
of modal logics. On the other hand, epistemic logic programs 
are also sensitive with various syntactic forms. Hence, we believe 
that a pure model-based or syntax-based approach will 
not be appropriate to handle epistemic logic program updates. 
Instead, we require our update formulation to meet three major 
criteria: (1) an update should be performed on a basis 
of minimal change semantics to remove contradictory information; 
(2) based on the minimal change semantics, the update 
result should have a clear syntactic representation and 
contain maximal consistent information from previous pro-
grants); and (3) the underlying update procedure should be 
consistent, that is, updating a consistent program by another 
consistent program (or a sequence of consistent programs) 
should generate a consistent result. Our main idea to accomplish 
these criteria is so called minimal change and maximal 
coherence which presents both semantic and syntactic features 
in an update procedure. 

The paper is organized as follows. Section 2 presents a 
brief overview on epistemic logic programs. Section 3 develops 
a formulation for epistemic logic program updates, 
while section 4 extends this formulation to handle update sequences. 
Section 5 investigates important semantic properties 
for our update approach. Finally, section 6 concludes the paper 
with discussions on related work and future research. 

2 Epistemic Logic Programs: An Overview 

In this section, we present a general overview on epistemic 
logic programs. Gelfond extended the syntax and semantics 
of disjunctive logic programs to allow the correct representation 
of incomplete information (knowledge) in the presence 
of multiple extensions. Consider the following disjunctive 
program about the policy of offering scholarships in some 
university [Gelfond, 1994]: 

Rule r4 can be viewed as a formalization of the statement: 
"the sutdents whose eligibility is not decided by rules r1, 
r2 and r3 should be interviewed by the committee". It is 
easy to see that V has two answer sets {highGPA(mike), 
eligible (mike)} and {fair GPA(mike), interview(mike)}. 
Therefore the answer to query interview (mike) is unknown, 
which seems too weak from our intuition. Epistemic logic 
programs will overcome this kind of difficulties in reasoning 
with incomplete information. 

Step 1. Let V be an epistemic logic program not containing 
modal operators K and M and negation as failure not. A 
set W of ground literals is called a belief set of V iff W 

Now we define that a collection A of sets of ground literals 
is a world view of V if A is the collection of all belief sets of 
PA* Consider program V about the eligibility of scholarship 
discussed earlier, if we replace rule r4 with the following 
rule: 

3 Formalizing Epistemic Logic Program 

Updates 
From this section, we start to develop a formulation for epistemic 
logic program updates. Consider the update of an epistemic 
logic program V\ by another epistemic logic program 
P2- Our approach consists of two stages: firstly, we update 
each world view of "P1 by P2 - this will remove contradictory 
information between P1 and P2 and ensure a minimal change 
for the underlying update semantics; and secondly, from the 
first stage result, we will derive a resulting program which retains 
the maximal consistent information represented by V\. 

3.3 Maximal Coherence and Resulting Programs 
A collection of belief sets is consistent if each of its belief 
sets is consistent. An epistemic logic program is consistent if 
it has a world view and all of its world views are consistent. 
To simplify our presentation, in the rest of this paper, we will 
simply call an epistemic logic program program. By V(P) 
we denote the set of all collections of belief sets in which V 
is satisfied. We also denote the set of all world views of V as 

3.2 Minimal Change on World View Updates 
As discussed earlier, during the second stage of an update 
procedure, we need to derive a resulting program which 
should contain the maximal consistent information represented 
by the initial program in syntactic forms. This is 
achieved by introducing the concept of coherence. Let 

Now the resulting program can be specified by the following 
definition. 

 Handling Update Sequences 

where the following conditions hold: 

decreasing priorities. Similarly to our previous update formulation, 
we will formalize this principle from both semantic 
and syntactic considerations. We first illustrate our idea by 
the following example. 

Example 4 Consider an irrigation system which has the following 
general rules to decide whether the plants should be 
watered: 

If there is no evidence showing that it will not be 

raining next day, then we do not need to water the 

plants; and 

If there is no evidence showing that the soil is dry, 

then we do not need to water the plants. 

It is also assumed that the soil is dry or it will not be raining 
next day. This scenario can be represented by the following 
program P1: 

from which it is concluded that we do not need to water the 
plants. However, from a conservative viewpoint for plants' 
growth, this result is rather optimistic because r3 does not 
represent an exclusive disjunctive information. Therefore, we 
consider to update P1 by P2 

which says that if it is not known that the soil is dry and it 
may be believed that it will not be raining next day, then we 
water the plants; and we do not need to water the plants if the 
soil is not dry or it will be raining next day. After a period 
of time, suppose new information is further received that is 
represented by P3 as follows: 

 Semantic Characterizations 

Theorem 1 Let P be an update sequence with a length of k 
andV a program. Then the following properties hold: 


Now we investigate two specific properties called persistence 
and preservation for epistemic logic program updates. 
Informally, the persistence property ensures that if a literal is 
derivable from a program, then after updating this program, 
this literal is still derivable from the resulting program. The 
preservation property, on the other hand, says that if a literal 
is derivable from a program, then updating other program by 
this program will still preserve this literal's derivability from 
the resulting program. While the persistence property is usually 
not valid for classical belief revision and update due to 
their nonmonotonicity, the preservation property, nevertheless, 
indeed holds for classical belief revision and update. It is 
not difficult to observe that generally none of these two properties 
holds for extended logic program updates or epistemic 
logic program updates. However, it is always worthwhile 
to explore their restricted forms because under certain conditions, 
these properties may significantly simplify the computation 
of a query to the update result. We first present the 
following lemma. 

4One referee pointed that as a general extension of Lifschitz and 
Turner's result, Watson also proposed a splitting set theorem for 
epistemic logic programs [Watson, 2000]. It appears that our splitting 
theorem has a different feature from Watson's though the later 
may be also used for this proof. 

Concluding Remarks 

In this paper, we proposed a formulation for epistemic logic 
program updates. Our update approach was developed based 
on the principle of minimal change and maximal coherence. 
By using our approach, not only a minimal change semantics 
is embedded into the underlying update procedure, but also a 
maximal syntactic coherence is achieved after the update. We 
also investigated important semantic properties of our update 
approach. This work can be viewed as a further development 
on knowledge update [Baral and Zhang, 2001]. Although all 
current approaches of logic program updates have their own 
features, it is not clear yet whether they are suitable to handle 
epistemic logic program updates. For instance, in [Zhang, 
2003] we demonstrated that a straightforward extension of 
Alferes et al's approach [Alferes and el a/, 2000J or Eiter et 
ai.'s approach [Liter and ct al, 2002] to epistemic logic program 
updates may generate incorrect solutions, while the proposed 
generic framework in [Eiter and et a/, 2001] seems not 
applicable to our case either. On the other hand, a syntax-
based approach, e.g. [Sakama and Inoue, 1999], cannot characterize 
the semantics of epistemic logic program updates, 
and some approaches, e.g. [Eiter and ct al, 20021, do not obey 
the consistency requirement that is believed to be one of the 
essential requirements for any revision and update systems 
[Katsuno and Mendelzon, 1991]. 

Several issues remain open for our future research. First, in 
our update approach, we did not consider the issue of preference 
over different resulting programs. In practice, it is possible 
that one resulting program is more preferred than the other 
in terms of the domain semantics. This problem involves conflict 
resolution which is a difficult issue in logic program updates 
[Zhang and Foo, 1998]. Second, as world view se


mantics can be viewed as a generalization of answer set semantics, 
our update approach is also applicable for extended 
logic program updates. Therefore, it is important to characterize 
similarities and differences between our approach and 
other logic program update approaches from a semantic viewpoint. 
Finally, since epistemic logic programs have been used 
as a main component in knowledge based action theories, e.g. 
[Lobo et a/., 2001], we expect that these theories may be significantly 
enhanced for representing interactions between actions 
and knowledge by applying our update approach. 


Abstract 
Increasing dialogue efficiency in case-based 
reasoning (CBR) must be balanced against the risk 
of commitment to a sub-optimal solution. Focusing 
on incremental query elicitation in recommender 
systems, we examine the limitations of naive 
strategies such as terminating the dialogue when 
the similarity of any case reaches a predefined 
threshold. We also identify necessary and sufficient 
conditions for recommendation dialogues to be 
terminated without loss of solution quality. Finally, 
we evaluate a number of attribute-selection 
strategies in terms of dialogue efficiency given the 
requirement that there must be no loss of solution 
quality. 

1 Introduction 

In conversational case-based reasoning (CCBR), a query is 
incrementally elicited in an interactive dialogue with the 
user [Aha et al, 2001]. Attribute-selection strategies that 
aim to minimize the length of such dialogues have recently 
attracted much research interest [Doyle and Cunningham, 
2000; Kohlmaier et al, 2001; McSherry, 2001; Schmitt et 
al, 2002]. Potential benefits include avoiding frustration for 
the user, reducing network traffic in e-commerce domains, 
and simplifying explanations of how conclusions were 
reached [Breslow and Aha, 1997; Doyle and Cunningham, 
2000]. While our focus in this paper is on incremental query 
elicitation in recommender systems, it is worth noting that 
the ability to solve problems by asking a small number of 
questions has been a major factor in the success of help-
desk applications of CBR [Aha et al, 2001; Watson, 1997]. 

An advantage of information gain [Quinlan, 1986] as a 
basis for attribute selection is that it tends to produce small 
decision trees, thus helping to reduce the length of problem-
solving dialogues [Doyle and Cunningham, 2000; 
McSherry, 2001]. However, concerns about its suitability in 
e-commerce domains include the fact that no use is made of 
the system's similarity knowledge [Kohlmaier et al, 2001; 

Schmitt et al, 2002]. Any importance weights associated 
with the case attributes are also ignored. As a result, it is 
possible for a case to be recommended simply because no 
other case has the preferred value for an attribute of low 
importance, even if its values for other attributes are 
unacceptable to the user. 

Kohlmaier et al [2001] propose a similarity-based 
approach to attribute selection in which the best attribute is 
the one that maximizes the expected variance of the 
similarities of candidate cases. In domains in which cases 
are indexed by different features, another alternative to 
information gain is to rank questions in decreasing order of 
their frequency in the most similar cases [Aha et al, 2001]. 

To address the trade-off between dialogue efficiency and 
solution quality, a CBR system must also be capable of 
recognizing when the dialogue can be terminated while 
minimizing the risk of commitment to a sub-optimal 
solution. Existing approaches include terminating the 
dialogue when the similarity of any case reaches a 
predefined threshold, or the achievable information gain is 
less than a predefined level, or the set of candidate cases has 
been reduced to a manageable size [Aha et al, 2001; Doyle 
and Cunningham, 2000; Kohlmaier et al, 2001]. 

However, a limitation of these approaches is that there is 
no guarantee that a better solution would not be found if the 
dialogue were allowed to continue. Whether it is possible to 
identify more reliable criteria for termination of problem-
solving dialogues is an issue that has received little 
attention, if any, in CBR research. 

In Section 2, we examine the trade-off between dialogue 
efficiency and solution quality in recommender systems. In 
Section 3, we present empirical techniques for identifying 
cases that can never emerge as the "best" case and can thus 
be eliminated. In Section 4, we identify necessary and 
sufficient conditions for the dialogue to be terminated 
without loss of solution quality. In Section 5, we evaluate a 
number of attribute-selection strategies in terms of dialogue 
efficiency given the requirement that there must be no loss 
of solution quality. Our conclusions are presented in 
Section 6. 

2 CCBR in Product Recommendation 

A generic algorithm for CCBR in product recommendation 
is shown in Figure 1. In this context, the elicited query, or 
partial query, represents the preferences of the user with 
respect to the attributes of the available products. At each 
stage of the recommendation dialogue, the system selects 
the next most useful attribute, asks the user for the preferred 
value of this attribute, and retrieves the case (or product) 
that is most similar to the query thus far elicited. The 
dialogue continues until the termination criteria are 
satisfied, or until no further attributes remain. At this point, 
the case that is most similar to the current query is presented 
to the user as the recommended case. 

Figure 1. CCBR in product recommendation. 

2.1 Similarity Measures 
The similarity of a given case C to a query Q over a set of 
case attributes A is typically defined as: 

In practice, the full-length query Q* that represents the 
preferences of the user with respect to all the case attributes 
may never be known, and is only one of the many possible 
completions of Q in the sense of the following definition. 

2.2 Measures of Retrieval Performance 
Often in recommender systems, cases other than those that 
are maximally similar to a target query are presented as 
alternatives that the user may wish to consider [e.g. 
McGinty and Smyth, 2002]. However, in measuring 
retrieval performance, we assume that the retrieval set for a 
given query Q is the set of cases C for which Sim(C, Q) is 
maximal; that is, no case is more similar to Q. 

Often in practice, rs(0 contains a single case; if not, we 
assume that the user is shown all cases that are maximally 
similar to her final query. A simple measure of dialogue 
efficiency is the number of questions, on average, that the 
user is asked before a recommendation is made. We 
measure precision and recall for an incomplete query Q 
relative to the full-length query Q* that represents the 
preferences of the user with respect to all the case attributes. 

Definition 3 Given an incomplete query Q, we define: 

2.3 Similarity Thresholds 
We now use an example recommender system in the PC 
domain to illustrate the trade-off between dialogue 
efficiency and solution quality in CCBR with termination 
based on similarity thresholds. The case library contains the 
descriptions of 120 personal computers [McGinty and 
Smyth, 2002], The attributes in the case library and weights 
assigned to them in our experiments are type (8), price (7), 
manufacturer (6), processor (5), speed (4), monitor size (3), 
memory (2), and hard disk capacity (1). In this initial 
experiment, attributes are selected in decreasing order of 
their importance weights and the dialogue is terminated 

when the similarity of any case reaches a predefined 
threshold. 

We use a leave-one-out approach in which each case is 
temporarily removed from the case library and used to 
represent the preferences of the user in a simulated 
recommendation dialogue. We measure dialogue length as 
the percentage of the 8 possible questions the user is asked 
before the dialogue is terminated. Average dialogue length, 
precision and recall over all simulated dialogues are shown 
in Figure 2 for similarity thresholds in the range from 0.4 to 

1. In this case library, there is never more than a single case 
in the retrieval set for the full-length query that provides the 
baseline for our evaluation of retrieval performance. It 
follows that for each threshold, recall is the percentage of 
dialogues in which this "best" case is recommended. 
Figure 2. Trade-off between dialogue efficiency and solution 
quality with termination based on a predefined threshold. 

The similarity threshold of 0.7 can be seen to have 
reduced average dialogue length by almost 50%. This 
equates to about 4 out of 8 questions, on average, being 
asked before a recommendation is made. However, the 
trade-off is a reduction of more than 20% in both precision 
and recall. This means that the best case is recommended in 
less than 80% of dialogues. As precision is less than recall 
for the 0.7 threshold, there arc also occasions when the 
system recommends the best case along with one or more 
other cases that are equally similar to the final query. It is 
also worth noting that even a threshold of 0.9, though 
providing a reduction in dialogue length of 17%, docs not 
ensure that the best case is always recommended. 

2.4 When Can the Dialogue be Safely Terminated? 
The potentially damaging effects of similarity thresholds on 
solution quality highlight the need for more reliable criteria 
for terminating CCBR dialogues. An incomplete query Q 
gives perfect precision and recall if and only if rs(Q) = 
rs(Q*), but the problem is that Q* is unknown. One can 
imagine an approach to CCBR that relics on exhaustive 
search to determine when the dialogue can be safely 
terminated; that is, in the certain knowledge that there can 

be no loss of precision or recall. The dialogue would be 
terminated only if all possible completions Q* of the current 
query Q yielded the same retrieval set as Q. However, this 
approach is unfeasible in practice as the number of possible 
completions of a given query is often very large. 

In Section 4, we identify criteria for safely terminating 
the dialogue that require minimal computational effort in 
comparison with exhaustive search. The approach is based 
on the concept of case dominance that we now introduce. 

The importance of case dominance can easily be seen. 
Any case that is dominated with respect to the current query 
can be eliminated as it can never emerge as the best case 
regardless of the preferences of the user with respect to the 
remaining attributes. In the following section, we present 
empirical techniques for identifying dominated cases that 
can easily be applied in practice. 

3 Identifying Dominated Cases 

We now present 3 alternative criteria for identifying cases 
that are dominated with respect to an incomplete query. We 
will refer to the dominance criteria identified in Theorems 1, 
2 and 3 as DC1, DC2 and DC3 respectively. DC1 and DC2 
are sufficient but not necessary conditions for a given case 
to be dominated by another case, while DC3 is both a 
necessary and a sufficient condition. DC1 and DC2 have the 
advantage of not relying on the triangle inequality, but only 
DC3 is guaranteed to detect all dominance relationships. 

A limitation of DC1 is that it fails to recognize that the 
similarity of the more similar case C\ is a moving target for 
the less similar case C2, and that the latter may be 
dominated even if it can equal or exceed the current 
similarity of C1. For example, if C\ and C2 have the same 
values for one of the remaining attributes, then any increase 
in similarity gained by C2 with respect to this attribute is 
also gained by C\. Our second dominance criterion, DC2, 
ignores attributes for which C1 and C2 have the same values, 
thus making it less susceptible to this problem. 

However, the moving target problem is only partially 
addressed by disregarding attributes for which C1 and C2 
have the same values. Any increase in similarity gained by 
C2 with respect to an attribute for which they have different 
values may also be gained in equal or greater measure by 
C\. Our third dominance criterion addresses this issue by 
taking account of the similarity between C\ and C2 with 
respect to each of the remaining attributes. 

Theorem 3 If Sim is a regular similarity measure, then a 
given case C2 is dominated by another case C\ with respect 
to an incomplete query Q if and only if 

So C2 is dominated by C\ as required. It remains to show 
that if: 

Figure 3 shows the numbers of dominated cases, on 
average, according to DCl , DC2 and DC3 after each 
question in simulated dialogues based on the PC case library 
[McGinty and Smyth, 2002]. The experimental setup is the 
same here as in Section 2.3, except that the dialogue is 
allowed to continue until no further attributes remain. 

Figure 3. Numbers of dominated cases in the PC case library 
according to DCl, DC2, and DC3. 

The results clearly show the inferiority of DCl as a basis 
for detecting dominance relationships. It fails to detect any 
dominance relationships until three questions have been 
asked, while it can be seen from the results for DC3 that 
more than 70% of cases, on average, are in fact dominated 
after the second question. The results also show DC2 to be 
much more effective in detecting dominance relationships 
than DCl, though unable to compete with DC3. 

4 Safely Terminating the Dialogue 

We now identify conditions in which a recommender 
system dialogue can be terminated without loss of precision 
or recall. We also show that these conditions must be 
satisfied in order for the dialogue to be safely terminated. 
That is, termination on the basis of any other criterion runs 
the risk of some loss of solution quality. 

The cost of testing condition (a) of Theorem 4 increases 
only linearly with the size of the retrieval set. At first sight, 
condition (b) may seem expensive to test, particularly in the 
early stages of query elicitation when |rs(Q| may be large. 
However, it can be seen from Lemma 2 that if (a) is true and 
C1 is any case selected from rs(Q), then (b) is true if and 

It is worth noting that failure of the underlying similarity 
measure to respect the triangle inequality does not affect the 
ability of a CCBR algorithm that uses the termination 
criteria presented in Theorem 4 to provide perfect precision 
and recall. However, the use of DC2 (which does not rely on 
the triangle inequality) as the dominance criterion is likely 
to affect retrieval performance in terms of dialogue 
efficiency. In the rest of this paper, we assume that the 
underlying similarity measure is regular, thus permitting the 
use of DC3 in the identification of dominance relationships. 

5 Attribute-Selection Strategies 

We now examine the effects on dialogue efficiency of four 
approaches to attribute selection in CCBR algorithms that 
use the termination criteria we have shown to be essential to 
ensure perfect precision and recall. Two of our algorithms 
are goal driven in that attribute selection is based on the 

CCBR1: Select attributes in random order 
CCBR2: Select attributes in order of decreasing importance 
CCBR3: Select the attribute that maximizes the similarity variance 

of cases not currently dominated by the target case 
CCBR4: Select the attribute that maximizes the number of cases 
dominated by the target case 

Attribute selection in CCBR3 is adapted from the 
approach proposed by Kohlmaier et al. [2001]. However, an 
expected similarity variance over all values of each attribute 
is not computed in our approach. Instead, the impact on 
similarity variance of each attribute is evaluated only for its 
value in the target case; this greatly reduces the 
computational effort involved in attribute selection. 

Our experimental method is designed to compare 
average dialogue length for each attribute-selection strategy 
with the optimal dialogue length that can be achieved by 
any CCBR algorithm that always gives perfect precision and 
recall. Any such algorithm, like the algorithms in our 
evaluation, must use the termination criteria identified in 
Theorem 4. As in our previous experiments, each case is 
temporarily removed from the case library and used to 
represent the preferences of the user. To determine the 
optimal dialogue length for a left-out case, we simulate all 
possible dialogues based on that case; that is, with the 
available attributes selected in every possible order. The 
optimal dialogue length is the minimum number of 
questions asked over all such dialogues. For each left-out 
case, we also record the dialogue length for each of our 
attribute-selection strategies. 

For each strategy, Figure 4 shows the maximum, 
minimum, and average number of questions asked over all 
simulated dialogues. Similar statistics are shown for the 
optimal dialogues determined as described above. CCBR4 
gave the best performance, reducing the number of 
questions asked by up to 63% and by 35% on average 
relative to a full-length query. Its average dialogue length of 

5.2 is only 4% higher than the lowest possible average that 
can be achieved by any CCBR algorithm that guarantees 
perfect precision and recall. 
Attribute selection based on similarity variance (CCBR3) 
also performed well on this case library, with an average 
dialogue length of 5.4 compared with 7.4 for the random 
strategy (CCBR1). With an average dialogue length of 5.8, 
selecting attributes in order of decreasing importance 
(CCBR2) was also more effective in reducing average 
dialogue length than the random strategy. 

The case library used in our final experiment (www.aicbr.
org) contains over 1,000 holidays and their descriptions 
in terms of 8 attributes such as price, region, duration, and 
season. The experimental setup is the same as in our 
previous experiment except that we do not attempt to 
determine the optimal length of each dialogue. The results 
are shown in Figure 5. Once again, CCBR4 gave the best 
performance, reducing dialog length by up to 63% and by 
25% on average. On this occasion CCBR3, though reducing 
average dialogue length more effectively than CCBR1, was 
outperformed by CCBR2. 


6 Conclusions 

Recognizing when problem-solving dialogues can be 
terminated while minimizing the impact on solution quality 
is an important issue that has received little attention in 
CBR research. Focusing on incremental query elicitation in 
recommender systems, we have identified necessary and 
sufficient conditions for the dialogue to be terminated 
without loss of solution quality. We have also evaluated 
several attribute-selection strategies in terms of dialogue 
efficiency given the requirement that there must be no loss 
of solution quality. The best results were obtained with a 
goal-driven strategy in which the selected attribute is the 
one that maximizes the number of cases dominated by the 

target case. In spite of its low computational cost (linear in 
the size of the case library) this strategy gave close to 
optimal performance on the PC case library. It was also 
more effective in reducing average dialogue length than the 
other strategies evaluated on the Travel case library, a 
standard benchmark containing over 1,000 cases. 

A feature of CBR recommender systems on which the 
techniques presented in this paper depend is that each 
outcome class (a unique product or service) is represented 
by a single case [McSherry, 2001]. Investigation of criteria 
for safe termination of problem-solving dialogues in other 
areas of CBR is an important objective for further research. 

Abstract 

User feedback is vital in many recommender systems 
to help guide the search for good recommendations. 
Preference-based feedback (e.g. "Show 
me more like item A ") is an inherently ambiguous 
form of feedback with a limited ability to guide the 
recommendation process, and for this reason it is 
usually avoided. Nevertheless we believe that certain 
domains demand the use of preference-based 
feedback. As such, we describe and evaluate a flexible 
recommendation strategy that has the potential 
to improve the performance of case-based recommenders 
that rely on preference-based feedback. 

1 Introduction 

At last year's European Case-Based Reasoning conference 
(ECCBR 2002) in Scotland, delegates were treated to a 
whiskey tasting. A range of different whiskeys were sampled, 
each accompanied by a feature-based description (e.g. 

distillery, age, alcohol content, cask, peatiness, sweetness, 
palette, etc.). However, the value of these descriptions was 
limited as most of us had little or no understanding of the 
features, nor could we reliably reconcile individual features 
with the taste of a particular whiskey. Nevertheless virtually 
everyone could chose a favourite whiskey by the end of the 
session (after a lot of sampling it has to be said!). This example 
corresponds to a challenging problem for developers of 
recommender systems, for reasons that will become clear. 

Normally the success of case-based recommenders depends 
on two important domain properties: (1) the items 
need to be described using well-defined features; and (2) 
users must have some understanding of these features and 
how they relate to their requirements. For example, restaurants 
are readily described using features such as cuisine, location 
and price, and most people have a reasonable understanding 
of their needs in relation to these features. When the 
above characteristics are present content-based recommendation 
strategies can be combined with different forms of user 
feedback, such as value elicitation (e.g. "I want an expensive, 
Asian restaurant") [Bridge, 20011, or critiquing (e.g. "Show 

"The support of the Informatics Research Initiative of Enterprise 
Ireland is gratefully acknowledged 

me more like Holly's Bistro but less formal''), to good effect 
[Burke, 20001. However, if these properties are not present 
then other strategies are needed. For example, if item descriptions 
are not available, collaborative filtering can be used 
to recommend items that have been rated positively by similar 
users [Burke, 2000; Shardanand and Maes, 1995]. The 
whiskey recommendation scenario above is different. The 
availability of item descriptions suggests case-based recommendation, 
but the lack of user expertise limits the use of 
feedback strategies such as value elicitation or critiquing. 

Indeed the whiskey domain is not unique in this respect. In 
many domains users are able to express a preference without 
necessarily understanding individual item properties. This is 
especially true in domains where a specialised v :abulary already 
exists to describe items (e.g. fashion, jewellery, art, music, 
etc.). Consider a recommender system designed to help 
a bride-to-be to choose her ideal dress, individual dresses are 
described using specific features (e.g. coul-back, scoop-cut, 
georgian-style, etc.) but most of these terms are meaningless 
to the average bride, and yet the typical bride-to-be is capable 
of recognising what she likes when she sees it. This problem 
is more than the user being unaware of the correct vocabulary 
to use during recommendation. The point is that even if they 
were presented with the appropriate vocabulary terms, they 
would not be in a position to use or appreciate these terms. 

One form of feedback that could be used is preference-
based feedback; the user simply expresses a preference for 
an item (e.g. "Show me more like dress 2"). This has 
been largely avoided by researchers - it is assumed to be 
too limited to efficiently guide the recommendation process. 
Nevertheless in our research we arc interested in looking at 
how to make this a practical form of feedback in these challenging 
domains. Incidentally, unlike other feedback types, 
preference-based feedback also benefits from minimal interface 
needs, and so is particularly relevant for recommenders 
designed for use with the current generation of mobile devices, 
such as WAP phones and PDAs. 

In this work we describe a way to improve the performance 
of recommenders that use preference-based feedback. 
Our novel adaptive selection method mirrors how real-life 
sales assistants adapt their recommendation strategies in response 
to user feedback by balancing the importance of similarity 
and diversity during each recommendation cycle. Dramatic 
improvements in recommendation performance have 
been observed (e.g. reductions in dialog length of up to 80%). 

2 Comparison-Based Recommendation 
Comparison-based recommendation is a content-based 
(or case-based) recommendation strategy [Burke, 2000; 
McGinty and Smyth, 2002], as opposed to a collaborative 
recommendation strategy [Burke, 2000; Konstan etal., 1997; 
Shardanand and Maes, 1995]. The notion of comparison-
based recommendation was introduced by [McGinty and 
Smyth, 2002] to emphasise the role of feedback in content-
based recommenders, and to allow for an analysis of 
preference-based feedback in particular. The focus was 
on how query modification methods might be used with 
preference-based feedback to produce efficient recommendation 
dialogs without richer forms of feedback like value elicitation 
or critiquing. We briefly review this earlier work, describing 
the basic comparison-based recommendation framework 
and some successful query modification strategies. 

2.1 The Basic Algorithm 
Comparison-based recommendation (Figure 1) is an iterative 
recommendation algorithm that presents the user with a selection 
of k items as recommendations during each of a sequence 
of n recommendation cycles. During each cycle the 
user expresses a preference for one of the suggestions and this 
feedback is used to revise the query for the next cycle; in the 
vocabulary of Shimazu, it is a type of navigation by proposing 
[Shimazu, 2001]. The recommendation dialog terminates 
when the user is presented with an acceptable suggestion. 

2.2 Simple Selection: More Like This 
The simplest approach to comparison-based recommendation 
is to recommend the k most similar items to the current query, 
and use the newly preferred item as the query for the recommendation 
next cycle. This more like this (MLT) method 
is a facility often provided by Web search engines; although 
it should be noted that search engines rarely rely upon this 
method, primarily focusing instead on query elicitation and 
term-based search. The MLT method is flawed by a tendancy 
to follow so-called false-leads as it overfits to the current 
preference, and this results in protracted recommendation 
dialogs. Using each preferred item as the next query is 
problematic if some of its features are not relevant to the user. 
For example, a user may indicate a preference for a whiskey 
that is oak-aged, sweet, and peaty. But if the preference was 
largely a result of the peatiness and independent of, or even at 
odds with, the sweetness or the aging, then blindly focusing 
the next selection on sweet and oak-aged whiskeys, as well 
as peaty ones, may result in poor recommendations. 
Figure 1 shows the scope of this problem by graphing the 
similarity of the preference to the target in a sample recommendation 
dialog. Instead of an increasing similarity profile, 
we find sustained decreases in similarity as MLT follows 
false-leads. Between cycle 3 and 11, target similarity falls 
from 0.46 to as low as 0.26 as the user is forced to accept 
poor recommendations, and again between cycle 11 and 40 
there is a noticeable similarity trough. Indeed it is only after 
cycle 38 that MLT finally begins to focus on the target region 
and similarity rises. 


Figure 1: Profiling the similarity of the preference case to the 
target case in a typical recommendation session 

2.3 Query Modification Strategies 
Query modification techniques can relieve the false-lead 
problem, and have been shown to improve recommendation 
efficiency, reducing MLT dialogs by 30% on average 
[McGinty and Smyth, 2002]. The central idea is to identify 
features in the preference case that are likely to be the reason 
for the user's choice, and those features that are likely 
to be irrelevant. Only relevant features are transferred to the 
new query. Returning to the whiskey example above, if the 
rejected cases are also very sweet then sweetness may irrelevant 
to the user. Hence, one query modification strategy, 
partial MLT (pMLT) only includes preference features in the 
query that are not found in any of the rejected cases. 

[McGinty and Smyth, 2002] also propose more sophisticated 
query modification strategies that weight preference 
features in the query according to how likely they are to be 
reliable. One strategy computes a weight for a preference 
feature based on the proportion of rejected items that also 
contain that feature. If 3 cases are recommended to the user, 
and if the preference and just one of the rejected items are 
oak-aged and then this feature is transferred to the new query 
with a weight of 0.5 (50% of the rejected items are oak-aged). 

3 Adaptive Case Selection 

So far we have assumed the use of similarity-based selection 
during each cycle. However, in the real-world two selection 
strategies can be observed in the dialogs that take place between 
customers and sales assistants. If a customer is unsure 
of their exact needs the sales assistant will tend to present a 
diverse range of alternatives, based on a preliminary set of 
requirements, to focus in on a promising region of a product 
space. A good sales assistant can recognise when the customer 
sees something that they genuinely like and uses this 
as a cue to switch their strategy to one that tries to refine subsequent 
recommendations in the region of the preference by 
selecting similar items. 

In this paper, instead of using query modification methods 
to improve recommendation efficiency, we propose an alternative 
called adaptive selection. It adapts the way that new 
items are selected for recommendation and is motivated by 
the above observation that different selection strategies are 
appropriate at different times in the recommendation process. 

3.1 Similarity vs Diversity 
Our observation about real-life recommendation scenarios is 
partly echoed by recent research that has questioned the apparent 
over-emphasis on similarity in content-based recommenders, 
with diversity forwarded as an important additional 
selection constraint. A number of strategies for improving 
recommendation diversity have been suggested. [Shimazu, 
2001] proposes the recommendation of three items or cases, 
i1, in and 73, per recommendation cycle: i1 is the most similar 
case to the query; i2 is maximally dissimilar from i1; and 
i13 is maximally dissimilar from i1 and i2. By design these 
items are maximally diverse, but similarity to the query may 
be compromised as there are no guarantees that in and i3 will 
be very similar to the query. 

Other researchers have proposed alternative diversity enhancing 
mechanisms with a more manageable balance between 
item diversity and query similarity [Bridge, 2001; 
McSherry, 2002; Smyth and McClave, 20011. Although all 
of these techniques introduce diversity into selection in a uniform 
way, they do not fully reflect the strategy switching seen 
in real-world scenarios. Moreover, while increasing diversity 
may improve the efficiency of of a recommender by allowing 
it to cover more of the item-space per recommendation cycle, 
it may also lead to new types of inefficiencies. Diversity 
enhancing methods pass over items that might otherwise be 
selected and the target item may be one of these. 

A more flexible mechanism for introducing diversity must 
be adopted based on whether or not the recommender has focussed 
on the correct region of the item space. If there is evidence 
that it is correctly focused then a pure similarity-based 
selection method can be used to refine the recommendations 
and safe-guard against passing over the target item. If the 
recommender is not correctly focused, then diversity is introduced 
in order to maximise the coverage of the item space 
in the next cycle, and so help to refocus the recommender by 
offering the user contrasting recommendations. 

3.2 Preference Carrying 
Unfortunately it is not immediately obvious how to judge 
whether the recommender is correctly focused; the target item 
is unknown and user requirements are typically vague. Adaptive 
selection solves this by evaluating whether the recommendations 
made in the iih cycle are an improvement on 
those in the i ~ Ith cycle before choosing a selection strategy 
for the i + \th cycle. 

This is achieved by carrying the preference item from the 
previous cycle to the current cycle. If the user selects the carried 
preference in the current cycle it must mean that they are 
unhappy with the k - I new alternatives in that cycle. These 
new items must have failed to improve upon the recommendations 
made during the previous cycle, and so the recommender 
must not be correctly focussed. If the user ignores the 
carried preference and selects one of the newly recommended 
items then the recommender must be correctly focused. Thus, 
by carrying the preference item, and monitoring whether or 
not the user reselects it, we can implement a switching mechanism 
between two alternative selection strategies: a refine 
strategy that emphasises similarity; and a refocus strategy that 

balances similarity and diversity for improved recommendation 
coverage. 

In theory carrying the preference may lead to new inefficiencies 
because the carried preference takes up a valuable 
slot in each cycle, thus limiting recommendation coverage. 
However, this is easily compensated for because carrying the 
preference helps protect against against false-leads. If none 
of the k: - 1 new cases are relevant then by reselecting the 
carried preference the user is at least maintaining the previous 
best recommendation rather than being forced to accept a 
lower quality false-lead. In practice, this on its own can offer 
a substantial improvement to recommendation efficiency. 

3.3 Refine & Refocus 
As mentioned above, the refine strategy makes use of a standard 
similarity-based selection method, picking k \A1\AA 1 new 
items that are maximally similar to the current query. The 
refocus strategy uses the bounded-greedy diversity technique 
proposed by [Smyth and McClave, 2001]; see Figure 2. 
Very briefly, the bounded-greedy technique involves two 
basic phases. First, the bk most similar items to the query 
are selected (where b is typically an integer between 2 and 5). 
During the second phase, the set (R) of selected items is built 
incrementally. During each step of this build the remainder 
of the bk items are ordered according to their quality and the 
highest quality item added to R. The quality of a item i is proportional 
to the similarity between i and the current query q, 
and to the diversity of i relative to those items so far selected, 

The first case to be selected is always the most similar to the 
query and in subsequent iterations, the chosen case has the 
highest quality relative to the query and diversity and cases 
selected so far. Note, we set b = 3 and a = 0.5 to balance 
similarity and diversity during refocusing. 

4 Evaluation 

In this section we evaluate the performance of adaptive selection 
focusing on the average number of cycles and unique 
items that must be presented to a user in a typical recommendation 
dialog. The shorter the dialog, the more successful the 
recommender is likely to be, and we demonstrate that adaptive 
selection delivers dramatic reductions in dialog length. 

4.1 Setup 
A case-base of 585 unique Scotish whiskey cases (Figure 3) 
is used to compare two comparison-based recommenders using 
preference-based feedback: MLT uses the standard MLT 
strategy; MLT+AS implements adaptive selection. We use 
a leave-one-out methodology for testing using 200 randomly 
selected cases as test cases. Each test case (base) is removed 
from the case-base and used in two ways. 

First it serves as the basis for a set of queries made by taking 
random subsets of features. Second, it is used to identify 
a maximally similar (target) case. Thus, the base is the ideal 
query for a user, the generated query is the initial test query 
given to the recommender, and the target is the best case for 
the user given their ideal. During each recommendation cycle 
3 cases are recommended to the user, and the most similar 
to the target is chosen as a preference, thus simulating a 
user that is capable of correctly selecting the best preference 
item in each cycle; we relax this assumption in Section 4.3. 
A query is satisfied when the target is returned in a cycle, 
thereby simulating the situation where the user is seeking a 
specific case; once again, we relax this condition in Section 

4.4. In total 1250 queries are generated and divided into three 
groups based on their difficulty; where difficulty is based on 
the number of recommendation cycles required by MLT. 
4.2 Recommendation Efficiency 
Basic recommendation efficiency can be measured in terms of 
the average number of cycles (or unique cases or items) that 
a user must work through before being presented with their 
ideal target. The leave-one-out method outlined above is used 
by the two recommenders for each of the 1250 queries and 
and the mean number of cycles and unique items presented 
to the user are measured. The results are shown in Figure 
4(a-b) as graphs of mean cycles and items for each algorithm 
and query group. The benefits of MLT+AS should be clear 
for both measures. For example, we see that MLT requires 

17.5 cycles (and 52.5 items) for simple queries, but MLT-AS 
needs only 9.5 cycles (and 20 items), a relative reduction of 
45% in terms of cycles, and 62% in terms of items. 
We see similar results for the other query sets. In fact, the 
relative reductions enjoyed by MLT-AS increase with query 
difficulty. The recommendation dialogs associated with more 
difficult queries include more false-leads than the dialogs 
for simpler queries, and so offer adaptive selection an even 
greater opportunity for dialog reduction. This trend is clearly 
seen in Figure 4(c), which graphs the percentage reduction in 
cycles and items due to MLT-AS, relative to MLT. For cycles, 
the relative reduction grows from 45% (simple) to 68% (difficult) 
and for unique items it grows from 62% to 78%. These 
results clearly show a dramatic benefit for adaptive selection. 
Indeed this benefit is so great that MLT-AS is capable of satisfying 
the most difficult queries far more efficiently than MLT 
takes to satisfy even the simplest queries. 

4.3 Preference Noise 
In this experiment we re-evaluate our recommenders by relaxing 
the assumption that the user always prefers the most 
similar item to the target to test whether benefits are found 
with sub-optimal preferences. The above experiment is repeated 
except that noise is introduced into the preference selection 
by perturbing the similarities between each recommended 
item and the target by some random amount within a 
set noise limit. A 10% noise means that each similarity value 
can change by up to +/-10% of its actual value. This will potentially 
change the internal ordering of recommended items 
during each cycle resulting in the selection of a preference 
that may not be the most similar to the target. This approach 

mimics the situation where users are likely to make preference 
mistakes more frequently if there is little difference between 
the target similarities of the recommended items. 

Figure 4(d&e) graph the mean number of cycles and items 
presented to the user versus the noise limit for moderate 
queries. As expected, introducing preference noise has a negative 
impact on the ability of each recommender to locate the 
target item. MLT dialogs increase from 24 cycles at 0% noise 
to 39 cycles at 40% noise, and MLT-AS dialogs increase from 
11 cycles to 37 cycles. The number of items required is also 
seen to increase in a similar manner. However, the benefits of 
MLT-AS remain across all levels of noise (see Figure 4(f)) although 
the magnitude of these benefits is seen to fall as noise 
increases. For example, the MLT-AS benefit, in terms of 
unique items, falls from 68% at the 0% noise level to 36% at 
the 40% level. Nevertheless, adaptive selection once again offers 
significant improvements in recommendation efficiency 
even when users make imperfect preference selections. While 
the results presented here focus only on moderate queries, for 
reasons of brevity, it is worth noting that qualitatively similar 
results are found for simple and difficult queries. Once 
again the MLT-AS benefits increase with query difficulty; for 
example, in terms of unique items, at the 40% noise level, a 
27% MLT-AS benefit is observed for simple queries, and a 
43% MLT-AS benefits for difficult queries. 

4.4 Target Noise 
Our second key assumption is that users are interested in a 
single target item during recommendation, and that the dialog 
only terminates when this item is returned. It is perhaps 
more realistic to assume that the user will be satisfied by any 
one of a group of items that are similar to the optimal target. 
We re-evaluate our recommenders under this more relaxed 
termination condition by repeating the basic efficiency experiment 
except that we terminate each dialog once an item has 
been recommended that is within some pre-defined similarity 
of the target. A similarity threshold of 70% means that the 
dialog terminates when an item that is at least 70% similar to 
the targer has been recommended. 

Figure 4(g-i) presents the results in a number of ways. First 
Figures 4(g&h) graph the mean number of cycles and items 
presented to the user versus the similarity threshold for moderate 
queries. As expected, relaxing the termination condition 
results in shorter recommendation dialogs for MLT and 
MLT-AS. For example, MLT dialogs reduce from just under 
26 cycles at the 100% similarity threshold (where the optimal 
item must be recommended) to just over 17 cycles at 60% 
similarity. MLT-AS dialogs reduce from 11 cycles to just under 
9 cycles across the same similarity range. The number 
of items required is also seen to reduce in a similar manner. 
However, positive MLT-AS benefits are maintained across all 
similarity thresholds (see Figure 4(i)). In fact, as the threshold 
increases, and the termination condition becomes more rigid, 
we find increasing benefits for MLT-AS in terms of cycles and 
items. For example, the MLT-AS benefit, in terms of unique 
items, grows from 54%; at the 60% similarity threshold to just 
under 70% at the 100% threshold. Once again, adaptive selection 
delivers significant improvements in recommendation 
efficiency across a variety of termination conditions. 

Figure 5: Preference-Based Feedback vs Critiquing. 

4.5 Preference-Based vs Critiquing 
Finally, it is worth briefly considering the impressive dialog 
reductions observed for adaptive selection with preference-
based feedback in the context of a much richer form of 
feedback, namely critiquing [Burke, 2000; McGinty and 
Smyth, 2003]. Figure 5 presents an efficiency comparison 
(in terms of average recommendation cycles) between our 
two preference-based recommenders and an equivalent recommender 
that implements critiquing; briefly, during each 
cycle feedback corresponds to a preference case plus a critique 
that constrains a selected feature value. 
On the face of it we expect critiquing to produce more efficient 
dialogs than preference-based feedback. And so it does 
for all query types when compared to standard preference-
based feedback. However, the surprise is that combining 
preference-based feedback and adaptive selection produces 
recommendation dialogs that are consistently and significantly 
shorter than even those produced by critiquing. For 
example, relative to MLT-AS, critiquing requires 37% more 
cycles for simple queries and 52% more cycles for the difficult 
queries. 

5 Conclusions 
To date preference-based feedback has been largely ignored 
by recommender systems. It is an inherently ambiguous form 
of feedback with a limited ability to efficiently guide the 
recommendation process. Nevertheless, we believe that this 
form of feedback is useful, and perhaps even vital, in certain 
recommendation domains where other forms of feedback 
cannot be used, perhaps because of limited user expertise or 
even basic device restrictions. 
We have described how preference-based feedback can be 
made more efficient using the adaptive selection technique, 
which modifies its recommendation strategy depending on 
whether or not the recommender is correctly focused on the 
right region of the recommendation space. Using this method 
we have demonstrated dramatic performance improvements 
over standard preference-based feedback, across a variety of 
experimental conditions, with reductions in dialog length of 
up to nearly 80%. Indeed, adaptive selection is so effective 
that it is capable of using simple and cheap preference-based 
feedback to produce recommendation dialogs that are even 
more efficient than those available with richer forms of feedback 
such as critiquing. 
In this paper we have presented evaluation results from one 
recommendation domain, Whiskey. This domain is especially 

interesting because of its dependency on preference-based 
feedback (as discussed in Section 1). We have also tested our 
technique on more traditional domain datasets (i.e. Travel, 
PC) and have found similar results iMcGinty and Smyth, 
2003]. We believe that these results have the potential to 
change the perception of simple preference-based feedback, 
facilitating its practical use in a variety of recommendation 
scenarios. Indeed adaptive selection is in no way limited to 
preference-based feedback. In our future research we intend 
to investigate if similar performance benefits can be achieved 
by integrating this technique in recommenders that use other 
forms of feedback (e.g. critiquing and rating-based). 



Abstract 

We introduce a new kernel for Support Vector Machine 
learning in a natural language setting. As a 
case study to incorporate domain knowledge into 
a kernel, we consider the problem of resolving 
Prepositional Phrase attachment ambiguities. The 
new kernel is derived from a distance function 
that proved to be succesful in memory-based learning. 
We start with the Simple Overlap Metric from 
which we derive a Simple Overlap Kernel and extend 
it with Information Gain Weighting. Finally, 
we combine it with a polynomial kernel to increase 
the dimensionality of the feature space. The closure 
properties of kernels guarantee that the result 
is again a kernel. This kernel achieves high classification 
accuracy and is efficient in both time and 
space usage. We compare our results with those obtained 
by memory-based and other learning methods. 
They make clear that the proposed kernel 
achieves a higher classification accuracy. 

1 Introduction 

An important issue in natural language analysis is the resolution 
of structural ambiguity. A sentence is said to be structurally 
ambiguous when it can be assigned to more than one 
syntactic structure [Zavrel et al. , 1997]. In Prepositional 
Phrase (PP) attachment one wants to disambiguate between 
cases where it is uncertain whether the PP attaches to the verb 
or to the noun. 

Example 1 Consider the following two sentences: 

1. I bought the shirt with pockets. 
2. 1 washed the shirt with soap. 
In sentence 1, with modifies the noun shirt because with 
pockets (PP) describes the shirt. In sentence 2 however, with 
modifies the verb washed because with soap (PP) describes 
how the shirt is washed [Ratnaparkhi, 1998]. 

This type of attachment ambiguity is easy for people to 
resolve because they can use their world knowledge [Stetina 

* Author funded by a doctoral grant of the institute for advancement 
of scientific technological research in Flanders (1WT). 
and Nagao, 1997]. A computer program usually cannot rely 
on that kind of knowledge. 

This problem has already been tackled using memory-
based learning like for example K-nearest neighbours. Here, 
the training examples are first stored in memory and classification 
of a new example is done based on the closest example 
stored in memory. Therefore, one needs a function 
that expresses the distance or similarity between examples. 
There already exist several dedicated distance functions to 
solve all kind of natural language problems using memory-
based learning [Veenstra et al, 2000; Zavrel et al, 1997; 
Daelemans et al, 2002]. 

We will use a Support Vector Machine (SVM) to tackle the 
problem of PP attachment disambiguation. Central to SVM 
learning is the kernel function K : .X x X -> R where X 
contains the examples and the kernel K calculates an inner 
product in a second space, the feature space F. This product 
expresses how similar examples are. 

Our goal is to combine the power of SVMs with the distance 
functions that arc well-suited for the probem for which 
they were designed. Deriving a distance from a kernel is 
straightforward, see Section 2.1. However, deriving a kernel 
from a distance is not trivial since kernels must satisfy some 
extra conditions, i.e. being a kernel is a much stronger condition 
than being a distance. In this paper we will describe 
a method that shows how such dedicated distance functions 
can be used as a basis for designing kernels that sequentially 
can be used in SVM learning. 

We use the PP attachment problem as a case study to illustrate 
our approach. As a starting point we take the Overlap 
Metric that has been succesfully used in memory-based learning 
for the same problem [Zavrel et al, 1997]. 

Section 2 will give a short overview of the theory of SVMs 
together with some theorems and definitions that are needed 
in Section 4. Based on [Zavrel et al, 1997], section 3 gives 
an overview of metrics developed for memory-based learning 
applied to the PP attachment problem. In Section 4 the new 
kernels will be introduced. Finally Sections 5 and 6 give some 
experimental results and a conclusion of this work. 

2 Support Vector Machines 
For simplicity, in our explanation we will consider the case 
of binary classification only, i.e. we consider an input space 
X with input vectors x and a target space D = {1, -1}. The goal of the SVM is to assign every x to one of two classes 
D = {1,-1} . The decision boundary that separates the input 
vectors belonging to different classes is usually an arbitrary 
n \A1\AA 1-dimensional manifold if the input space X is n-
dimensional. 

SVMs sidestep both difficulties [Vapnik, 1998]. First, 
overfitting is avoided by choosing the unique maximum margin 
hyperplane among all possible hyperplanes that can separate 
the data in F. This hyperplane maximizes the distance 
to the closest data points. 

7b be more precise, once we have chosen a kernel K we 
can represent the maximal margin hyperplane (or decision 
boundary) by a linear equation in x 

convex quadratic objective function with linear constraints. 
Moreover, most of the cti prove to be zero. By definition the 
vectors xi corresponding with non-zero a, are called the support 
vectors SV and this set consists of those data points that 
lie closest to the hyperplane and thus are the most difficult to 
classify. 

In order to classify a new point xnew, one determines the 
sign of 

If this sign is positive xnew, belongs to class 1, if negative to 
class -1, if zero xnew lies on the decision boundary. Note 
that we have now restricted the summation to the set SV of 
support vectors because the other a, are zero anyway. 

2.1 Some Properties of Kernels 
even if we don't know the exact form of the features that are 
used in F. Moreover, the kernel expresses prior knowledge 
about the patterns being modelled, encoded as a similarity 
measure between two vectors [Brown et al, 1999]. 

But not all maps over X x X are kernels. Since a kernel K 
is related to an inner product, cfr. the definition above, it has 
to satisfy some conditions that arise naturally from the definition 
of an inner product and are given by Mercer's theorem: 
the map must be continuous and positive definite I Vapnik, 
1998]. 

In this paper we will use following methods to construct 
kernels iCristianini and Shawe-Taylor, 2000]: 

M1 Making kernels from kernels: Based on the fact that kernels 
satisfy a number of closure properties. In this case, 
the Mercer conditions follow naturally from the closure 
properties of kernels. 

M2 Making kernels from features: Start from the features 
of the input vectors and obtain a kernel by working out 
their inner product. A feature is a component of the input 
vector. In this case, the Mercer conditions follow 
naturally from the definition of an inner product. 

3 Metrics for Memory-based Learning 

In this section, we will focus on the distance functions [Zavrel 
et al, 1997; Cost and Salzberg, 1993] used for memory-based 
learning with symbolic values. First, we will have a look at 
the Simple Overlap Metric (SOM) and next we will discuss 
Information Gain Weighting (1GW). Memory-based learning 
is a class of machine learning techniques where training instances 
are stored in memory first and classification of new 
instances later on is based on the distance (or similarity) between 
the new instance and the closest training instances that 
have already been stored in memory. A well-known example 
of memory-based learning is k-nearest neighbours classification. 
We will not go into further detail, the literature offers many books and articles that provide a comprehensive introduction 
to memory-based learning, e.g. [Mitchell, 1997]. For 
our purpose, the important thing to remember is that we will 
be working with symbolic values like strings over an alphabet 
of characters. Instances are n-dimensional vectors a and b in 

A Kernel for Natural Language settings 

3.1 Simple Overlap Metric 
The most basic metric for vectors with symbolic values is the 
Simple Overlap Metric(SOM) [Zavrel et a/., 1997]: 

Here d SOM (a,b) is the distance between the vectors a and 
b, represented by n features and S is the distance per feature. 
The k;-ncarest neighbour algorithm equipped with this metric 
is called IB1 [Aha et al, 1991]. The IB1 algorithm simply 
counts the number of (mis)matching feature values in both 
vectors. This is a reasonable choice if we have no information 
about the importance of the different features. But if we do 
have information about feature importance then we can add 
linguistic bias to weight the different features. 

3.2 Information Gain Weighting 
Information Gain Weighting (1GW) measures for every feature 
i separately how much information it contributes to our 
knowledge of the correct class label. The Information Gain 
(IG) of a feature i is measured by calculating the entropy between 
the cases with and without knowledge of the value of 
that feature: 

weights Wi can be used to extend Equation 2 with weights 
(see Equation 6). The k-nearest neigbour algorithm equipped 
with this metric is called 1B1-1G [Zavrel et al, 1997] and the 
corresponding distance called dIG: 

We don't have to proof that the kernel ksok satisfies the Mercer 
conditions because this follows naturally from the definition 
of the inner product. We started from the features, de


inner product between them, see M2 from Section 2.1. However, 
to show that the kernel really corresponds to the distance 
function given in Equations 3 and 4 we have to verify the distance 
formula for kernels given in Section 2.1: 

4.2 Extending the SOK to n Dimensions 
We don't have to proof that the kernel KSOK is a valid 
kernel as this follows naturally from the definition of 
the inner product. However, we again have to show that 
it is a kernel that corresponds to the distance function 
dSOM from Equation 2. In the following we will show 

However, this does not impose any problems for the kernel 
we are aiming to develop. 

4.3 Adding Information Gain to the SOK 
4.4 A Polynomial Information Gain Kernel 
In this section, we will increase the dimensionality of the feature 
space F by making use of a polynomial kernel Kpoly. 
We will start this section by giving an example [Cristianini 
and Shawe-Taylor, 2000]: 

From the closure properties of kernels, it follows naturally 
that KPIG indeed is a valid kernel which calculates the inner 
product of two vectors transformed by a feature mapping 

5 Experimental Results 

5.1 PP attachment disambiguation problem 
If it is uncertain in a sentence whether the preposition attaches 
to the verb or to the noun then we have a prepositional phrase 
(PP) attachment problem. For example, in sentence 1 of Example 
1, with modifies the noun shirt because with pockets 
(PP) describes the shirt. In contrast, in sentence 2, with modifies 
the verb washed because with soap (PP) describes how 
the shirt is washed [Ratnaparkhi, 1998]. 

In fact, we only keep those words that are of 
any importance to the PP attachment problem, i.e. 
(V(erb),N(oun),P(reposition),N(oun)). 

In the case where sentences are reduced to quadruples as 
illustrated in Example 3, the human performance is approximately 
88.2% [Ratnaparkhi et al, 1994]. This performance 
rate gives us an acceptable upper limit for the maximum 
performance of a computer because it seems unreasonable 
to expect an algorithm to perform much better than a human. 
As we will show in our experimental results the kernel 
KIG achieves a classification accuracy up to 82,9%, see Section 
5.3. However, in [Zavrel et al. , 1997] the 1B1-1G attains 
a maximum classification accuracy of 84.1%, this is a good 
indication of the classification accuracy that should be possible 
to obtain with a kernel based on the distance defined in 
Equation 6. 

5.2 Experimental Setup 
The experiments have been done with LIBSVM, a C/C++ and 
Java library for SVMs [Chih-Chung and Chi-Jen, 2002]. The 
machine we have used is a Pentium III with 256MB RAM 
memory, running Windows XP. We choose to implement the 
kernels KIG and KPIG in Java. 
The type of SVM learning we have used is C-SVM [Chih-
Chung and Chi-Jen, 2002]. The parameter C controls the 
model complexity versus the model accuracy and has to be 
chosen based on the complexity of the problem. 
The experiments in this section are conducted on a simplified 
version of the full PP attachment problem (see Example 
3). The data consists of four-tuples of words, extracted from 
the Wall Street Journal Treebank [Marcus et al., 1993] by a 
group at IBM [Ratnaparkhi et al. , 1994]1. 
The data set contains 20801 training patterns, 3097 test 
patterns and an independant validation set of 4093 patterns 
for parameter optimization. All of the models described below 
were trained on all training examples and the results are 
given for the 3097 test patterns. For the benchmark comparison 
with other models from the literature, we use only results 
for which all parameters have been optimized on the validation 
set. For more details concerning the data set we refer 
to [Zavrel et al. , 1997]. 

5.3 Results 
The fact that the kernel KIG performs worse than IB1-IG, 
although it is equipped with the very same distance metric, 
may seem somehow surprising. We believe this is because 
SVMs perform linear separation in the feature space F. The 
decision boundary of IB1-IG on the other hand is non-linear. 
Due to the linearity of the decision boundary of the SVM, 
some points get misclassified. The number of misclassifications 
is controlled by the parameter C. Choosing a larger value for C forces the SVM to avoid classification errors. 
However, choosing a value for C that is too high will result 
in overfitting. Increasing the dimensionality of F makes linear 
separation easier (Cover's theorem). This can be seen by 
comparing the classification accuracies between the kernels 
KIG and KPIG- The results also show that our kernel KPIG 
outperforms all other methods in the comparison. 

 Conclusion 

In this paper, we have proposed a method for designing kernels 
for SVM learning in natural language settings. As a starting 
point we took distance functions that have been used in 
memory-based learning. The resulting kernel KPIG achieves 
high classification accuracy compared to other methods that 
have been applied on the same data set. In our approach we 
started from the vectors of the input space, defined a mapping 
on those vectors and worked out their inner product in 
the feature space. All necesary kernel conditions follow naturally 
from the definition of the inner product. We used the 
distance formula for kernels to show that the kernels are actually 
based on the distance functions from Section 3. The 
experimental results of the kernel KPIG show that increasing 
the dimensionality in the feature space yields better results. 
We increased the dimensionality of the feature space by making 
combinations of the features ,. through a polynomial 
kernel. In this way, we do not only take into account the similarity 
between corresponding features of vectors, but we also 
take into account the similarity between non-corresponding 
features of the vectors. In fact this comes down to taking into 
account, to a greater extend, the context in wich a word occurs. 


We used the resolvement of PP attachment ambiguities as a 
case-study to validate our findings, but it is our belief that the 
kernel KPIG can be used for a wide range of natural language 
problems. In the future we will apply our kernels in more 
complex natural language settings and compare the results to 
other methods that have been applied on the same problems. 
At the moment we are already running experiments on language 
independent named-entity recognition [Sang, 2002]. 
The first results look promising, but it is too early to draw 
any significant conclusions. We will also try to further extend 
the kernels proposed in this paper to achieve even higher 
accuracies. Moreover, there are still many distance functions 
reported in the literature that have not yet been investigated to 
see whether they are applicable in SVM learning, we intend 
to investigate such distance functions and if possible derive a 
kernel from them. 


Abstract 
The importance of determinate theories lies in the fact that 

We introduce a logical formalism of irreflexivc 
causal production relations that possesses both a 
standard monotonic semantics, and a natural nonmonotonic 
semantics. The formalism is shown to 
provide a complete characterization for the causal 
reasoning behind causal theories from [McCain and 
Turner, 1997]. It is shown also that any causal relation 
is reducible to its Horn sub-relation with respect 
to the nonmonotonic semantics. We describe 
also a general correspondence between causal relations 
and abductive systems, which shows, in effect, 
that causal relations allow to express abductive 
reasoning. The results of the study seem to suggest 
causal production relations as a viable general 
framework for nonmonotonic reasoning. 

1 Introduction 

Causal theories have been introduced in [McCain and Turner, 
1997] as a nonmonotonic formalism that provides a natural 
solution for both the frame and ramification problem in reasoning 
about actions (see [Giunchiglia el al, 2001] for a detailed 
exposition). A causal theory is a set of causal rules 
A->B that express a kind of a causal relation among propositions. 
The nonmonotonic semantics of such theories is determined 
by causally explained interpretations, namely the interpretations 
that arc both closed with respect to the causal 
rules and such that every fact holding in them is caused. 

The above fonnalism has been defined semantically, and 
the main aim of our study consists in laying down its logical 
foundations. As we will show, such foundations can 
be built in the framework of an inference system for causal 
rules that we will call causal production relations. The logical 
origins of the latter are in input/output logics [Makinson 
and der Torre, 2000], but we will supply them with a natural 
nonmonotonic semantics allowing to represent significant 
parts of nonmonotonic reasoning. Thus, the main result 
of the present study is that causal production relations completely 
characterize the inference rules for causal conditionals 
that preserve the nonmonotonic semantics of causal theories. 
It will be shown also that any causal theory is reducible 
with respect to this semantics to a determinate causal theory 
that contain only Horn causal rules with literals in heads. 

(modulo some mild finiteness restrictions) the explained interpretations 
of such a theory are precisely the interpretations 
of its classical completion (see [McCain and Turner, 1997]). 
Consequently, the nonmonotonic consequences of such theories 
are obtainable by the use of classical inference tools 
(such as the Causal Calculator, described in [Giunchiglia et 
al.,2001]). 

Finally, we will describe a relationship between causal relations 
and abductive reasoning. As we will see, the latter 
will be representable via a special kind of causal relations 
called abductive causal relations. This will serve as yet another 
justification for our general claim that causal production 
relations could be used as a general-purpose formalism 
for nonmonotonic reasoning, a viable alternative to other nonmonotonic 
formalisms (such as default logic). 

2 Causal production relations 

We will assume that our language is an ordinary propositional 
language with the classical connectives and constants 
{A, V, ->, ->, t, f}. In addition, t= and Th will denote, respectively, 
the classical entailment and the associated logical closure 
operator. 

A causal rule is a rule of the form A => B, where A and 
B arc classical propositions. Informally, such a rule says 
that, whenever A holds, B is caused. The following definition 
describes an inference system for causal rules that will be 
shown to be adequate (and complete) for causal theories. Actually, 
many of the postulates below (e.g., And and Or) have 
already been suggested in the literature (see, Lifschitz, 1997; 
Schwind, 1999]). 

Definition 2.1. A causal production relation (or simply a 
causal relation) is a relation => on the set of propositions satisfying 
the following conditions: 

(Strengthening) If A I= B and B => C, then A => C; 

(Weakening) If A => B and B I= (7, then A => C; 

(And) If A => B and A =. C, then A=>B/\C\ 

(Or) If A => C and B => C, then AvB=>C; 

(Cut) If A => B and A A B => C, then A => C; 

(Truth) t => t; 

(Falsity) f=>f. 
Though causal relations satisfy most of the rules for classical 
entailment, their distinctive feature is that they are irreflexive, 
that is, they do not satisfy the postulate A=>A. Actually, 
such relations correspond to a strongest kind of input-
output logics from [Makinson and der Torre, 2000], basic 
input-output logics with reusable output, with the only distinction 
that causal relations satisfy also Falsity. The latter 
restricts the universe of discourse to consistent theories1. 

As a consequence of Cut, any causal relation will already 
be transitive, that is, it will satisfy 


However, transitivity is a weaker property than Cut, since 
it does not imply the latter (see [Makinson and der Torre, 
2000]). In addition, the material implications corresponding 
to the causal rules can be used as auxiliary assumptions in 
making derivations. This is a consequence of the following 
property that holds for causal relations: 

(AS) If A=> D and C/\(A-+B) => D, then C => D. 

Remark. The notion of a causal production sanctioned by 

the above postulates is atemporal. For example, the rule 

cannot be understood as saying that P and q jointly 
cause -q (afterwards) in a temporal sense; instead, by Cut 
and Falsity it implies , which means, in effect, that 
p Aq cannot hold. Speaking generally, 'causally consistent' 
propositions cannot cause an effect incompatible with them. 
A representation of temporal and action domains in this formalism 
can be achieved, however, by adding explicit temporal 
arguments to propositions, just as in the classical logic 
(see [Giunchiglia et ai, 2001]). 

A constraint is a causal rule of the form

 
Such 
constraints correspond to state constraints in action theories. 
Now, any causal rule implies the corresponding constraint: 

(Reduction) 


Note, however, that the reverse entailment does not hold. 
Actually, given the rest of the postulates, Reduction can replace 
the Cut postulate: 

Lemma 2.1. Cut is equivalent to Reduction. 
Another important fact about causal relations is the following 
'decomposition' of causal rules: 


Causal rules of the form are ' classically trivial', 
but they play an important explanatory role in non-reflexive 
causal reasoning. Namely, they say that, in any causally explained 
interpretation in which A holds, we can freely accept 
B, since it is self-explanatory in this context. Accordingly, 
such rules could be called conditional abducibles. We will 
discuss the role of abducibles later as part of the general correspondence 
between causal and abductive reasoning2. 

1 In fact, it has been shown in [Bochman, 2002] that other, weaker 
input-output logics can also be given a semantics of the kind described 
in what follows. 

2Note also that, under the general correspondence between 
causal inference and default logic (see [Turner, 1999]), such rules 
correspond to normal defaults in default logic. 

Now the above lemma says that any causal rule can be decomposed 
into a (non-causal) constraint and a conditional abducible. 
This decomposition neatly delineates two kinds of 
information conveyed by causal rules. One is a logical information 
that constraints the set of admissible models, while the 
other is an explanatory information describing what propositions 
are caused (explainable) in such models . 

For a finite set u of propositions, will denote their conjunction. 
We will extend causal rules to rules having arbitrary 
sets of propositions as premises using the following 'compactness' 
recipe: for any set u, 


For a given causal relation =., we will denote by C(u) the 
set of propositions caused by u, that is 


The operator C will play much the same role as the usual 
derivability operator for consequence relations. It satisfies the 
following familiar properties: 


_ 

3 Monotonic semantics of causal inference 

We will describe now a monotonic semantics for causal relations. 
Actually, it will be just a slight modification of the 
semantics given in [Turner, 1999]. 

A fully formal semantic interpretation of causal relations 
could be given in terms of possible worlds frames of the form 
(i,W), where i is a propositional interpretation, while W 
is a set of propositional interpretations that contains i (see 
[Turner, 1999]). In order to make our descriptions more transparent, 
however, we will identify such frames with pairs of 
theories of the form

 , where a is a world (maximal deductively 
closed set), while u is a deductively closed set included 
in a. Such pairs will be called bitheories. Clearly, 
any possible worlds frame (i, W) determines a certain bitheory, 
and vice versa, so all our definitions will admit a purely 
semantic reformulation. 

By a causal semantics we will mean a set of bitheories. 
The corresponding notion of validity for causal rules is given 
in the next definition. 


. 
A => B is valid with respect to a causal semantics B if it 
holds in all bitheories from B. 
We will denote by =B the set of all causal rules that are 
valid in a causal semantics B. It can be easily verified that this 
set is closed with respect to the postulates for causal relations, 
and hence we have 

3Cf. a similar decomposition of causal rules in [Thielschcr, 
1997]. 
the following theorem shows that this semantics determines 
precisely the source causal relation, including the causal rules 
with arbitrary sets of premises. In other words, the source 
causal relation is strongly complete for this semantics. 

Thus, any causal semantics determines a causal relation 
and vice versa, any causal relation is generated by some 
causal semantics. Hence we conclude with the following general 
completeness result: 

Corollary 3.3. A relation on propositions is a causal relation 
iff it is generated by a causal semantics. 

3.1 Possible worlds semantics 
Causal relations can also be given a semantic interpretation in 
terms of standard possible worlds models. 

The following definition provides the notion of validity for 
causal rules in such models: 

Theorem 3.4. A relation is causal if and only if it is determined 
by a quasi-reflexive possible worlds model. 

4 The nonmonotonic semantics 
In addition to the monotonic semantics, a causal relation determines 
also a natural nonmonotonic semantics. 


fact is causally explained. It is these worlds that determine 
the nonmonotonic semantics, defined below. 


Propositions that hold in all causally explained worlds are 
considered as the nonmonotonic consequences determined by 
the causal relation. This semantics is indeed nonmonotonic, 
since the set of such consequences changes nonmonotonically 
with changes in the underlying set of causal rules. 

Now we will establish a correspondence between the above 
semantics and the nonmonotonic semantics of causal theories 
from [McCain and Turner, 1997]. 

Definition 4.2. Two causal theories will be called (nonmonotonically) 
equivalent if they determine the same set of 
causally explained worlds. 

As a first part of our general result, the next theorem shows 
that the postulates of causal inference preserve the above nonmonotonic 
semantics. 

The above theorem shows, in effect, that the logic of 
causal relations is adequate for the nonmonotonic reasoning 
in causal theories. 

Definition 4.3. . A causal theory is determinate if it contains 
only Horn rules. A causal relation will be called 
determinate if it is generated by a determinate causal 
theory. 

. 
A causal theory will be called definite if it is determinate 
and locally finite. 
Clearly, any finite causal theory will be locally finite, 
though not vice versa. Similarly, any finite determinate theory 
will be definite. As can be easily verified, a determinate 

4Thus, any constraint A => f will be a Horn rule in this sense. 

causal theory is definite if and only if, for any literal /, there 
is no more than a finite number of rules with the head /. Consequently, 
our definition of definite theories turns out to be 
equivalent to that given in [McCain and Turner, 1997]. 

By a determinate subrelation of a causal relation => we 
will mean the least causal relation generated by the set of all 
Horn rules from =.. Then we have 

Theorem 4.2. Any causal relation is nonmonotonically 
equivalent to its determinate subrelation. 

By Theorem 4.1 we can conclude now that any causal theory 
is equivalent to some determinate theory. 
Example. The causal theory 

Then the following result can be proved: 

Unfortunately, the above algorithm is not modular. Moreover, 
it does not preserve local finiteness: there are locally 
finite causal theories such that their determinate counterparts 
are not locally finite. Still, in many simple cases it gives a 
convenient recipe for transforming an arbitrary causal theory 
into an equivalent determinate theory. 

Now we arc going to show the second part of our main 
result, namely that causal relations constitute a maximal logic 
suitable for reasoning in causal theories. To begin with, we 
introduce 

is a monotonic (logical) notion, and hence, unlike the nonmonotonic 
equivalence, it is preserved under addition of new 
causal rules. For example, though any causal theory is equivalent 
to a determinate one, they may give different results if 
we add some causal rules to them. 

Strongly equivalent theories are 'equivalent forever', that 
is, they are interchangeable in any larger causal theory without 
changing the nonmonotonic semantics. Consequently, 
strong equivalence can be seen as an equivalence with respect 
to the monotonic logic of causal theories. And the next result 
shows that this logic is precisely the logic of causal relations. 

Theorem 4.5. Two causal theories are strongly equivalent if 
and only if they are causally equivalent. 

The above result (and its proof) has the same pattern as 
the corresponding results about strong equivalence of logic 
programs and default theories (see [Lifsehitz et al., 2001; 
Turner, 2001]). 

The above result implies, in effect, that causal relations are 
maximal inference relations that are adequate for reasoning 
with causal theories: any postulate that is not valid for causal 
relations can be 'falsified' by finding a suitable extension of 
two causal theories that would determine different causally 
explained interpretations, and hence would produce different 
nonmonotonic conclusions. 

5 Causation versus abduction 

Causal inference has numerous connections with other nonmonotonic 
formalisms. Thus, [Turner, 1999] describes the relations 
with circumscription, autoepistemic and default logic. 
In this section we will describe a correspondence between 
causal inference and abductive reasoning. In addition to specific 
results, this will give us a broader perspective on the 
representation capabilities of causal relations. 

Corollary 4.4. Causally equivalent theories are nonmonotonically 
equivalent. 

A general correspondence between abductive frameworks 
The reverse implication in the above corollary does not and input-output logics has been established in [Bochman, 
hold, and a deep reason for this is that the causal equivalence 2002], based on the identification between the above notion 

of explainability and input-output derivability. By this correspondence, 
abducibles are representable by 'reflexive' propositions 
satisfying input-output rules A=> A, while abductive 
frameworks themselves correspond exactly to input-output 
logics satisfying an additional postulate of Abduction (see 
below). By these results, input-output logics allow to give 
a syntax-independent representation of abductive reasoning. 

5.1 The abductive subrelation 
We will begin with the following definitions: 
Causal inference in abductive causal relations is always 
mediated by abducibles. Consequently, propositions caused 
by worlds are caused, in effect, by the abducibles that hold in 
these worlds: 

The abductive subrelation of a causal relation preserves 
many properties of the latter. For example, both have the 
same constraints and abducibles. 

Recall that in causally explained interpretations any proposition 
is caused by other propositions, the latter are also 
caused by accepted propositions, and so on. Clearly, if our 
'causal resources' are limited, such a causal chain should stop 
somewhere. More exactly, it should reach abducible (selfexplanatory) 
propositions. This indicates that in many cases 
the nonmonotonic semantics associated with a causal theory 
should be determined by the corresponding abductive subrelation. 
Below we will make this claim precise. 

Definition 5.2. A causal relation will be called weakly abductive 
if it is nonmonotonically equivalent to its abductive 
subrelation. 

The next definition will give us an important sufficient condition 
for weak abductivity. 

The above definition describes a variant of a standard 
notion of well-foundedness with respect to the (transitive) 
causal order. It should be clear that any causal relation in 

a finite language should be well-founded. Moreover, let us 
say that a causal relation is finitary if it is generated by some 
finite set of causal rules. Then we have 

Lemma 5.2. Any finitary causal relation is well-founded. 

Finally, the next result shows that all such causal relations 
will be weakly abductive. 
Theorem 5.3. Any well-founded causal relation is weakly 

abductive. 

It turns out that well-foundedness is not the only condition 
that is sufficient for weak abductivity. Thus, adequately 
acyclic causal theories (see [McCain and Turner, 1998] for 
a definition) are not in general well-founded, but they also 
satisfy this property. 

Theorem 5.4. Any adequately acyclic causal relation is 
weakly abductive. 

The above results show that in many cases of interest, the 
nonmonotonic semantics of causal theories can also be computed 
using abduction. Still, as a general conclusion, we can 
say that causal inference constitutes a proper generalization 
of abductive reasoning beyond the situations where facts are 
explainable by reference to a fixed set of self-explanatory abducibles. 


6 Conclusions and perspectives 

Summing up the main results, we can argue that causal relations 
constitute a powerful formalism for nonmonotonic reasoning, 
especially suitable for representing action domains. It 
has been shown, in particular, that it gives an exact description 
of the logic underlying causal theories. We have seen 
also that it allows to give a syntax-independent representation 
of abductive reasoning. If we add to this also the natural 
correspondences with other nonmonotonic formalisms, such 
as default logic and circumscription (see [Turner, 1999]), we 
can safely conclude that causal inference covers a significant 
part of nonmonotonic reasoning. 

Viewed from this perspective, there is still much to be done 
in order to realize the opportunities created by the formalism. 
There are two kinds of problems that need to resolved in this 
respect. 

The nonmonotonic semantics for causal theories is based 
on the principle of universal causation which is obviously 
very strong. The principle implies, for example, that if we 
have no causal rules for a certain proposition, it should be 
false in all explainable interpretations. As a result, adding 
a new propositional atom to the language makes any causal 
theory inconsistent, since we have no causal rules for it. This 
makes causal theories extremely sensitive to the underlying 
language in which they are formulated. One way out has 
been suggested in [Lifschitz, 1997]; it amounts to restricting 
the principle of universal causation to a particular subset of 
explainable propositions. This approach, however, is purely 
syntactical and hence retains language dependence. 

More subtle, yet perceptible difficulties arise also in representing 
indeterminate situations in causal theories. Thus, 
since any causal theory is reducible to a determinate theory, 
causal rules with disjunctive heads are ignored in the nonmonotonic 
semantics; more exactly, they are informative only 

to the extent that they imply corresponding non-causal constraints 
or Horn rules. This does not mean, however, that 
we cannot represent indeterminate information in causal theories. 
Actually, one of the main contributions of [McCain 
and Turner, 1997] consisted in showing how we can do this 
quite naturally in common cases (see also [Lin, 1996]). Still, 
there is yet no systematic understanding whether and how an 
indeterminate information can be represented by Horn causal 
rules. 

A more general problem concerns the role and place of 
causal inference in general nonmonotonic reasoning. Though 
the former covers many areas of nonmonotonic reasoning, 
it does not cover them all. Thus, it does not seem suitable 
for solving the qualification problem in representing actions. 
Speaking generally, causal reasoning appears to be independent 
of the kind of nonmonotonicity described by preferential 
inference from [Kraus et ai, 1990]. This naturally suggests 
that the two kinds of nonmonotonic reasoning with conditionals 
could be combined into a single formalism, a grand uniform 
theory of nonmonotonic reasoning. Actually, this idea 
is not new; it has been explored more than ten years ago in 
[GefTner, 1992]. It remains to be seen whether current studies 
of causal reasoning can contribute to viability of such a 
general theory. 

Acknowledgments. The author is grateful to Vladimir Lifschitz, 
Hudson Turner, David Makinson and Leendert van der 
Torre, as well as to the anonymous referees, for their instructive 
comments and suggestions. 


Abstract 

Finding desired information on the Internet is becoming 
increasingly difficult. Internet directories 
such as Yahoo!, which organize web pages into hierarchical 
categories, provide one solution to this 
problem; however, such directories are of limited 
use because some bias is applied both in the collection 
and categorization of pages. We propose 
a method for integrating multiple Internet directories 
by instance-based learning. Our method provides 
the mapping of categories in order to transfer 
documents from one directory to another, instead 
of simply merging two directories into one. 
We present herein an effective algorithm for determining 
similar categories between two directories 
via a statistical method called the k-statistic. 
In order to evaluate the proposed method, we conducted 
experiments using two actual Internet directories, 
Yahoo! and Google. The results show that 
the proposed method achieves extensive improvements 
relative to both the Naive Bayes and Enhanced 
Naive Bayes approaches, without any text 
analysis on documents. 

1 Introduction 

The World-Wide Web (WWW) is now used not only by computer 
specialists, but by all sorts of people, from children 
to businessmen and businesswomen, which has resulted in 
an enormous quantity of web pages available on the Internet. 
This makes finding pages containing the desired information 
rather difficult. Search engines are requisite for finding 
the desired information. In general, current search engines 
perform their functions via one of two methods: a keyword 
search and a directory search. A keyword search engine 
performs searches by means of user-specified keywords. 
Keyword-based search engines like Google can reliably find 
pages containing the specified keywords. However, if the 
user does not have a thorough knowledge of his/her search 
domain and cannot choose the appropriate keywords, the 
search engine is useless. In such a situation, directory-based 
search engines do a good job. In a directory-based search 
engine, pages are evaluated and organized by humans before 

being registered in the search engine's archive. Directory-
based search engines provide knowledge of WWW navigation. 
Users reach the desired information by going up and 
down the directories. 

Although pages are carefully selected and well-organized, 
a single Internet directory is not sufficient because the Internet 
directory tends to have some bias in both collecting 
and categorizing pages. In order to solve these problems, 
we herein propose a method that coordinates multiple Internet 
directories by estimating directory similarities. The 
proposed method does not simply merge multiple directories 
into a larger directory, but instead determines the relationship 
between directories. Many public Internet directories exist. 
Some are designed to cover wide domains, while others focus 
on special domains. Such Internet directories are difficult 
to merge. Moreover, these directories should not be integrated, 
because the existing differences in concept hierarchies 
among the directories is important when selecting and using 
directory-based search engines. In this paper, we propose a 
method to solve this problem by determining the rules for 
mapping categories in a directory to those in another directory. 
Our solution can be applicable not only to the Internet 
directory problem, but also to the integration of web marketplace 
catalogs [Agrawal and Srikant, 2001]1 and ontology integration 
in general. 

The remainder of this paper is organized as follows. In 
Section 2, we define the problem of coordinating multiple Internet 
directories. In Section 3, we discuss related studies. In 
Section 4, we propose a new machine learning method for the 
above-mentioned problem. Next, in Section 5, we compare 
the performance of the proposed method to that of the Enhanced 
Naive Bayes approach [Agrawal and Srikant, 2001] 
for an integration problem using real Internet directories. Finally, 
in Section 6, we present our conclusions. 

2 Integration of Multiple Internet Directories 

In order to state the problem, we introduce a model for the 
Internet directories we intend to integrate. We assume there 
are two Internet directories: a source directory and a targetdirectory. The documents in the source Internet directory are 

'For example, the integration of a distributor's catalog and a web 
marketplace. 
expected to be assigned to categories in the target Internet directory. 
This produces a virtually integrated Internet directory 
in which the documents in the source directory are expected 
to be members of both the source and target directories. This 
integrated directory inherits the categorization hierarchy from 
the target Internet directory. 

The Internet directory model for the source and target is as 
follows: 

. 
The source Internet directory, Sp contains a set of categories, 
Cs1, CS2, . . ., Csn, that are organized into an "isa" 
hierarchy. Each category can also contain documents. 
. 
The target Internet directory Tp contains a set of categories, 
C11, C12, . . . , Ctm that are organized into an "isa" 
hierarchy. Each category can also contain documents. 
The proposed model permits documents to be assigned to 
intermediate categories. This model is similar to the catalog 
model in lAgrawal and Srikant, 2001], except that the categories 
are organized into a conceptual hierarchy. Since the 
catalog model ignores hierarchy structure and therefore cannot 
assign documents to an intermediate category, our Internet 
directory model is more general than the catalog model. 

The problem addressed in this paper is finding an appropriate 
category Ct in the target directory Tp for each document 
Ds1 in the source directory Sp. An example is shown 
as the mapping of a black box Dx in Figure 1, where the 
black circles indicate categories and the hollow boxes indicate 
documents. What we need to do is determine an appropriate 
category in Tp for a document which appears in Sp 
but not in TD, because mapping is not necessary if the document 
is included in both the source and the target directories. 
Documents D1 and D2 in Figure 1 are examples of such 
documents. This mapping can have several possibilities, e.g., 
Dx can be mapped to an upper left category or to a lower left 
category, etc. 


3 Related Work 
One popular approach to this kind of problem is to apply 
standard machine learning methods [Langley, 1996]. This 
requires a flattened class space having one class for every 
leaf node. The problem can then be considered to 
be a normal classification problem for documents. Naive 
Bayes(NB) [Mitchell, 1997] is an established method used for 
this type of document classification framework. A classifier 

is constructed using the words in the documents. However, 
this classification scheme ignores the hierarchical structure of 
classes and, moreover, cannot use the categorization information 
in the source directory. Enhanced Naive Bayes [Agrawal 
and Srikant, 2001], hereafter referred to as E-NB, is a method 
which does use this information. E-NB will be discussed 
in the next section. GLUE [Doan ct al., 2002] is another 
type of system employing NB. To improve accuracy, GLUE 
combines NB and a constraint optimization technique called 
relaxation labeling. However, the general performance of 
the system depends on that of NB. Unlike NB, the systems 
in [Koller and Sahami, 1997], [Wang et al., 1999] classify 
documents into hierarchical categories, and these systems use 
the words in the documents for classification rules. However, 
these systems cannot use the categorization information in the 
source directory. 

Another type of approach is ontology merging/alignment 
systems. These systems combine two ontologies, which 
are represented in a hierarchal categorization. Chimaera 
[McGuinness et al., 2000] and PROMPT [Noy and 
Musen, 2000] are examples of such systems and assist in the 
combination of different ontologies. However, such systems 
require human interaction for merging or alignment. In addition 
to this requirement, these systems are based on the 
similarity between words, which introduces instability. The 
dictionaries used for such systems often have word similarity 
bias. FCA-MERGE [Stumme and Madche, 2001] is another 
type of ontology merging method. It uses the attributes of 
concepts to merge different ontologies. As a result, it creates 
a new concept without regarding the original concepts in 
both ontologies. Calvanese et al. [Calvanese et al, 2001] also 
discussed an ontology integration framework with respect to 
global ontology and local ontology. 

As mentioned before, we also tackled the similar problem 
of catalog integration. An approach, besides E-NB, is to construct 
the abstract-level structure of two hierarchies [Omelayenko 
and Fensel, 2001]. This approach does not direct 
the transformation of source and target information, but transforms 
via the abstract-level structure. It is relatively easy to 
transfer information through many hierarchical structures, but 
it is hard to create a common structure for those hierarchies. 

The bookmark-sharing systems of Siteseer [Rucker and 
Polanco, 1997] and Blink [Blink, 2000] also deal with a 
similar problem. These systems attempt to share URL information 
which appears in the source bookmark but not in 
the target bookmark. These systems flatten the categorization 
of bookmarks in the same manner as NB and determine 
the mapping of categories in each bookmark, based on 
the shared URL information. Such systems are problematic 
when a given URL does not fit into an exact category. kMedia 
[Takeda et al, 2000] is another bookmark-sharing system 
that uses hierarchical structures explicitly, but is dependent 
on the similarity of the words within pages. 

In the context of information retrieval, Stuckenschmidt 
[Stuckenschmidt, 2002] has coordinated the formal 
model of hierarchies with multiple classification hierarchies. 
In our paper, since the upper and lower boundaries of concept 
mapping can be determined by subsumption relationships, 
we can infer the boundary of valid mapping and also check 
whether or not the given mapping is consistent. In our 
problem, since we have only partial evidence of classes and 
do not know the definitions of classes, we can not apply this 
method directly. 

4 Learning the Relationship between 

Categories of Two Internet Directories 
In this section, we explain our method to determine the relationship 
between categories in two Internet directories. One 
characteristic of the proposed method is to use the hierarchical 
structure "as is." We use all categories including intermediate 
categories and leaf categories, because information 
for categorization can also be obtained from these categories. 
Another characteristic of the proposed method is the exclusive 
reliance on the categorization structure of both directories, 
i.e., with no reliance on the semantic information of the 
documents. We can determine the relationship between categories 
of the two directories by statistically comparing the 
membership of the documents to the categories. 

4.1 Basic Concept 
Although E-NB was developed for a very similar problem, 
it is missing an important feature: categorization hierarchy. 
According to [Agrawal and Srikant, 2001], the initial definition of the problem is identical to that of this paper. However, the previous paper assumes that "any documents assigned to an interior node really belong to a conceptual leaf node that is a child of that node," and concludes from this assumption that "we can flatten the hierarchy to a single level and treat it as a set of categories." However, the conclusion overlooks the categorization hierarchies. The categorizations are not in
dependent of each other. The categorization hierarchies are 
usually structured using an "is-a" or another relationship. If a 
document is categorized in a lower category in the categorization 
hierarchy, then the document should also be categorized 
in the upper categories 2. If we flatten the categories, such information 
regarding the relationships between categories will 
be lost. 

Another problem associated with NB is its sensitivity to 
the words in documents. For example, if a document contains 
the word bank, the category for the document could be 
a financial category; however, the category could also be a 
construction category. The quality of categorization with NB 
is thus not stable according to the contents of documents, due 
to the natural language techniques of processing words. It 
is critical to integrate Internet directories because mixing reliable 
Internet directories maintained by human editors with 
less reliable and less stable machine-generated classifiers will 
confuse users and decrease their confidence. In addition to 
the problem of sensitivity, analysis of a document to extract 
words is expensive. 

Our method focuses on the similarity of the way of categorization, 
not the similarity of documents. Then, how do we 
measure the similarity of categorization? We utilize shared 
documents in both the source and target Internet directories 

2Note that the reverse is not always true, because documents can 
be categorized into intermediate categories. 

as our measurement standard. If many documents in category 
Csi also appear in category Ctj at the same time, we consider 
these two categories to be similar, because the ways of categorization 
in Csi and Ctj are supposed to be similar, i.e., if 
another document D comes in Csi, it is likely that D will be 
also included in CtJ. This method avoids both the keyword 
extraction process and the handling of word meaning. The 
example shown in Figure 2 illustrates that documents in the 
bottom category in the source Internet directory can be transferred 
to a similar category in the target Internet directory. 


Figure 2: Document transfer from source Internet directory 
to target Internet directory. 

4.2 k-Statistic 
The remaining problem is how to determine the pairs of similar 
categories. We adopt a statistical method to determine the 
degree of similarity between two categorization criteria. The 
K-statistic method [Fleiss, 1973] is an established method for 
evaluating the similarity between two criteria. Suppose there 
are two categorization criteria, Csi in SD and Ctj in TD. We 
can determine whether or not a particular document belongs 
to a particular category3. Consequently, documents are divided 
into four classes, as shown in Table 1. The symbols 
N11, N12, -N21 and N22 denote the numbers of documents for 
these classes. For example, N11 denotes the number of documents 
which belong to both Csi and Ctj. We may logically 
assume that if categories Csi and Ctj have the same criterion 
of categorization, then N12 and N21 are nearly zero, and if 
the two categories have a different criterion of categorization, 
then N11 and N22 are nearly zero. The k-statistic method 
uses this principle to determine the similarity of categorization 
criteria. 
In the k-statistic method, we calculate the probability P, 

3 Remember that categorization in a directory is determined using 
the nodal structure of the categorization hierarchy. 

which denotes the percentage of coincidence of the conceptual 
criteria, and the probability P' which denotes the percentage 
of coincidence of the conceptual criteria by chance. 


As such, the value of the K-statistic is represented as the 
following equation: 


Next, we examine whether we can assume the K = 0, 
which indicates that the percentage of coincidence for the 
two conceptual criteria is zero. We therefore calculate the 
test statistic Z according to the following equation. 


The value Z follows a normal distribution. A null hypothesis 
is considered to occur when the percentage of coincidence 
of the concept criteria is zero. When we assume a significance 
level of 5% and the following equation is satisfied, we 
can dismiss the null hypothesis. 


When the null hypothesis can be dismissed, the criteria are 
determined to be the same. 

4.3 Determination of Pairs of Similar Categories 
The relationship between the two categorization criteria is examined 
from top to bottom. This algorithm is shown in Figure 
3. First, the most general categories in the two categorization 
hierarchies are compared using the K-statistic. If the 
comparison confirms that the two categories are similar, then 
the algorithm outputs this pair of categories. At the same 
time, the algorithm generates all possible pairs of their children 
categories. It generates two lists of categories each of 
which is a list of the confirmed category and its children categories. 
Then, the algorithm picks one category from each 
list and makes a pair, except for the original pair. This new 
pair is then evaluated recursively using the K-statistic method. 
When a similar pair is not generated, the algorithm outputs 
the pairs of similar concepts between the two categorization 
hierarchies. They are used as mapping rules from the source 
directory to the target directory4. 

This top-down comparison approach attempts to reduce the 
exploration space, using the structure of both directories as a 
guide. Assuming that similar categories include similar subcategories, 
we can skip the combination of categories which 

4The rules do not guarantee the consistency of the mappings, 
because category hierarchies have the possibility of inconsistency 
with regard to similar relationships. 


Figure 3: Algorithm to determine similar category pair. 

is likely unnecessary5. This is another benefit of using the ''as 
is" hierarchy, in contrast to the flattening approach. 

4.4 Application Policy of Rules 
Since the proposed method uses categorization similarity, if 
a category in the source Internet directory does not have a 
similar category in the target directory, then those documents 
cannot be categorized in the target Internet directory. In order 
to avoid this problem, we once again use the categorization 
hierarchy. If the system cannot find a similar category pair 
for the category containing the document, it applies the rule 
generated for the parent category instead. 

5This assumption may sometimes be too strong, e.g., a child category 
of a category is not so relevant, whereas its child category (a 
grandchild category of the original category) is relevant. In such 
cases, we should relax this constraint when exploring future candidates. 

4.5 Computational Complexity 

Table 2 shows the numbers of categories, the links in each 
Internet directory and the links included in both Internet directories. 
Links are considered to be the same when the URLs 
in both directories are identical. 

5 Experiments Using Internet Directories 

5.1 Experimental Settings 
In order to evaluate the proposed algorithm, we conducted 
experiments using data collected from the Yahoo! [Yahoo!, 
2001] and Google [Google, 2001]6 Internet directories. The 
data was collected in the fall of 2001. In order to compare 
the proposed method to that in [Agrawal and Srikant, 2001], 
we selected the same locations in Yahoo! and Google for the 
experimental data. The locations are as follows: 

6Since the data in Google is constructed by the data in 
dmoz [dmoz, 2001], we collected data through dmoz. 

We conducted ten-fold cross validations for the shared 
links. The shared links were divided into ten data sets; nine 
of these sets were used to construct rules, and the remaining 
set was used for testing. Ten experiments were conducted for 
each data set, and the average accuracy is shown in the results. 
In order to compare our proposed method to the E-NB 
approach, the classifiers are assumed to correctly assign documents 
when the document is categorized cither in the same 
category as the test data or in the parent categories of the test 
data. E-NB constructs classifiers for only the first-level categories 
in the experimental domain, whereas the proposed 
method uses all of the categories from top to bottom. The 
significance level for the K-statistic was set at 5%. 

5.2 Experimental Results 
The experimental results are shown in Tables 3 and 4. Table 
3 shows the results obtained using Yahoo! as the source 
Internet directory and Google as the target Internet directory, 
and Table 4 shows the results obtained using Google as the 
source Internet directory and Yahoo! as the target Internet 
directory. For comparison, these tables also include the results 
of [Agrawal and Srikant, 2001]. E-NB denotes the 
method of [Agrawal and Srikant, 2001] and SBI denotes the 
similarity-based integration method that is proposed in this 
paper. The data obtained for the proposed method and that 
presented in [Agrawal and Srikant, 2001] are not truly comparable 
because the collection date is different7. 

The proposed algorithm did well compared to E-NB, performing 
more than 10% better in accuracy on the averages. In 
the Movies domain, the proposed algorithm performs much 
better in accuracy than E-NB. One reason for this is that the 
pages related to movies contain various words that indicate 
numerous movie settings, making classification using word-
based systems difficult. On the other hand, in the Outdoors 
domain, performance is similar for both methods because the 

7In addition, differences may have occurred due to data selection. 
For example, Yahoo! has links that use @(the at mark). The 
treatment of such links was not discussed in [Agrawal and Srikant, 
2001]. In the present study, we did not use such links. 

Table 3: Results of Yahoo! as the source Internet directory 
and Google as the target Internet directory. 


Table 4: Results of Google as the source Internet directory 
and Yahoo! as the target Internet directory. 

Outdoors domain treats pages using special outdoors-related 
words, and no classification method for the Outdoors domain 
has been established for human readers. 

As mentioned earlier, in the experiments described 
in [Agrawal and Srikant, 2001], the classifiers induced by the 
system have the ability to classify the documents into only the 
first-level categories of the test domain in the target Internet 
directory. The proposed classifiers can classify the documents 
into sub-categories as well. For example, we used 4,623 categories 
for document assignment in the Movies domain of 
Yahoo! as the source Internet directory and Google as the target 
Internet directory, whereas their system used only 40 categories. 
Therefore, the classifiers obtained by the proposed 
method are more reliable and useful than those obtained by 
other methods. 

Next, we compare our method with GLUE [Doan et al, 
2002]. The basic steps of GLUE are as follows. First, the user 
trains learners for each concept for target taxonomies. Learners 
can be a combination of different strategies like name- or 
contents-based learning. Then, they determine that the source 
concept and target concept are similar, if the application of 
learners for the source concept yields good enough. Finally, 
the system applies the relaxation labeling method for similar 
concept pairs. The method does not use the hierarchical 
structures in learning very well, and relies mainly on the performance 
of the learning method, namely NB. On the other 
hand, our method does use hierarchical structures in learning 
very well. So, we do not need a semantical analysis in learning. 
They report an accuracy of 65-95% which is equivalent 
to our results, but they use only a few categories, i.e., 40-363, 
while we use 170-4623 categories. We can say from both 
theoretical and practical viewpoints that our method is more 
appropriate in learning for large hierarchical structures. 

6 Conclusions 

In this paper, a statistical-based technique was proposed for 
integrating multiple Internet directories by determining the 
relationship between these directories. The proposed method 
uses the K-statistics to find similar category pairs, and transfers 
the document categorization from a category in the 
source Internet directory to a similar category in the target 
Internet directory. The proposed method has an advantage 
in document treatment in that it relies on the category structure 
only, and not on words or word similarity in a document. 
The performance of the proposed method was tested 
using actual Internet directories, and the results of these tests 
show that the performance of the proposed method was more 
than 10% better in accuracy than that of previous methods, 
although the number of categories were by far larger for our 
proposed method. In addition, our problem modeling is more 
general than the other models in terms of assignment documents 
on intermediate categories. From this, we can conclude 
our approach shows great promise in comparison to systems 
employing NB, such as E-NB [Agrawal and Srikant, 2001], 
GLUE [Doan et ai. , 2002], and so on. 

Although the present results are encouraging, much has 
yet to be done. The limitation of the proposed method is 
that this method is based on the existence of shared links. 
In other words, the proposed method is less reliable if there 
are fewer shared links. To improve our proposed method, 
various methods might be used to obtain semantic information 
in order to increase the number of shared links, e.g., to 
regard similar pages as the same links. Another option for 
this problem might be to combine semantic information or 
other information about the documents. If we could a establish 
good combination of our proposed method and such 
information, it is possible that we could greatly improve our 
method. Moreover, the present method can find only one-toone 
mapping rules. If the system is able to find one-to-many 
or many-to-many mappings, it would work more correctly in 
categorization. For the development of these mappings, we 
might invent probabilistic mapping representations and also 
introduce a subsumption technique [Stuckenschmidt, 2002]. 
Finally, the proposed method should be expanded so that it 
can apply to more than three concept hierarchies. In such 
a case, despite the conflict between several concept hierarchies, 
we would expect more information to be obtained. The 
aforementioned tasks for improvement will be investigated in 
future studies. 

Abstract have been previous attempts to use functional causal models 

This paper reconsiders the notions of actual cause 
and explanation in functional causal models. We 
demonstrate that isomorphic causal models can 
generate intuitively different causal pronouncements. 
This occurs because psychological factors 
not represented in the model determine what criteria 
we use to determine causation. This partially 
explains the difficulty encountered in previous 
attempts to define actual cause. Freed from 
trying fit all examples to match intuition directly 
(which is not possible using only the information 
in causal models), we provide definitions for causation 
matching the different causal criteria we intuitively 
apply. This formulation avoids difficulties 
associated with previous definitions, and allows a 
more refined discussion of what constitutes a cause 
in a given situation. The definitions of actual cause 
also allow for more refined formulations of explanation. 


1 Introduction 

Identifying an actual cause in a specific situation is a normal 
part of every day human reasoning. From the specifics of the 
situation, we sift through the events, identifying those that 
actually caused the event in question. This is different than 
general claims about causal tendencies. In fact, the causal 
nature of events can actually run counter to the general causal 
tendency of the variables. For example, buying a lottery ticket 
has the general tendency to decrease wealth. However, in 
certain circumstances (the numbers on the ticket correspond 
to the winning numbers), it can cause a significant increase 
wealth. People are able to make these situation dependent 
pronouncements fairly easily. 

Automating causal determinations has a number of applications, 
including natural language processing, diagnosis, 
and planning. Unfortunately, formulating a successful notion 
of actual cause has proved elusive. Previous attempts 
were often hindered by trying to embed them in formalisms 
that are not appropriate for modeling causality (See [Pearl, 
2000, Chapter 10] for a discussion of these). Recently, functional 
causal models [Pearl, 2000] have proven to be an effective 
formalism for modeling causal relationships. There 

as a basis for defining actual cause, but they have been shown 
to suffer from serious defects of their own [Hopkins & Pearl, 
2003]. In this paper, we identify (at least partially) why previous 
methods have failed, and provide new definitions that 
avoid many of their problems. These definitions allow for a 
more refined determination of actual causation. Additionally, 
they provide a useful basis for producing explanations. 

2 Review of causal models 

In this section we review the basic definitions from [Pearl, 
2000] that we will use. First we need some notation. Upper 
case letters (e.g. X, Y, Z) represent variables. Lower case 
letters (e.g. x, y, z) represent a particular value of the corresponding 
variable. Bold face upper case letters (e.g. X, Y, Z) 
represent sets of variables, and bold face lower case letters 

(e.g. x, y, z) an instantiation of the corresponding variables. 
fx maps fro , Together, for any setting 
of the background variables, T determines the values of 
all variables in V. The parents of a variable X are the variables 
that directly influence the value of X (i.e. the set of 
variables that is non-trivial in fx). The causal diagram of a 
model is a directed graph with vertices U U V where there 
is an edge from vertex X to vertex Y iff X is a parent of Y 
in the model. We will restrict our attention to models with 
acyclic causal diagrams.
der interventio 
A submodel of causal model M unwhere 
. This allows 
us to talk about what would have happened if X had been x. 
We will write Y x(u) to mean the value that Y takes on in 
the submodel Mx in context U = u. For a propositional formula 
\A6\D7, defined over primitives of the form 
represents the value of the formula when the primitives are 
evaluated in model Mx in context u. 

A causal model is a triple (U, V, F) where U is the set 
of background (or exogenous) variables (i.e. their values are 
determined outside the model), and V is the set of endogenous 
variables (i.e. those whose value is determined by model 
variables).i set of functions where 

3 Previous work 

Formulating the notion of cause has been a topic of philosophic 
debate for many years. Unfortunately, all of the philo

sophical works of which we are aware either suffer from serious 
problems, or are not precise enough to be falsifiable (See 
[Halpem & Pearl, 2001a] for a review of some of these problems). 


There have been at least two previous attempts to formulate 
actual cause in functional causal models. The first was the 
causal beam approach [Pearl, 2000]. This notion of actual 
cause seemed to be too restrictive, classifying certain events 
as noncauses that intuitively clearly should have been causes 
(See for example the critique in [Halpern & Pearl, 2001a]). 
The other [Halpem & Pearl, 2001a] (which we will call ASC 
for Alternate situation Strong Cause) tried to remedy those 
problems, but introduced several others. [Hopkins & Pearl, 
2003] provides a thorough analysis of its problems. 

4 Why so hard? 

The many formulations encountered, and the problems they 
exhibit beg the question: why is ascribing cause so difficult to 
formalize? Consider the following example from [Halpem & 
Pearl, 2001a]: Two arsonists light matches at opposite ends 
of the forrest. Both start fires, and the forrest burns down. 
Either arsonist alone was sufficient to burn down the forrest. 
Now, we ask whether arsonist 1 lighting his match caused the 
forrest to burn down. Clearly it did. The reason being that it 
was sufficient to burn it down, and participated in the actual 
causal process that brought it about. 

Consider another example: A prisoner is scheduled for execution. 
The execution proceeds as follows: The executioner 
selects a gun from among several, which may or may not have 
a bullet in the chamber. The prisoner is given the option of 
death by lethal injection (which is always fatal), or face execution 
by gunshot. If the prisoner selects gunshot, the executioner 
fires the weapon. If the weapon he selected had a 
bullet, it kills the prisoner. If not, the execution fails and the 
prisoner goes free. In the situation we consider, the prisoner 
chooses execution by lethal injection. As it turns out, the selected 
gun was loaded so the prisoner would have died even 
if he had he chosen gunshot. 

Now, we ask if the prisoner's choice of execution method 
caused his death. Obviously it did not. The reason being that 
in this circumstance, regardless of his choice, he would still 
die. That is, his choice had no impact on the outcome. 

Surprisingly, the obvious causal models in these two examples 
are in fact isomorphic as are the specific situations considered. 
The effect (forrest burns down <-> death) occurs in 
all but one possible instantiation of its direct causes (neither 
arsonist lights their match <-> execution by gunshot is selected 
and the gun is not loaded) and both sufficient conditions occurred 
in the situation under consideration. In other words, 
the information that rendered one a cause and the other a non-
cause is not contained in the causal model, or the specific situation. 
We achieved intuitively different results because we 
tested different notions of cause. In the forest fire example, 
we tested the ability to produce the fire, i.e. whether it was 
capable of bringing about the result. In the second example, 
we tested the consequentiality of the choice of execution 
methods, i.e. whether an alternate decision would produce a 
different result. The choice of what test to apply seems to be 
based on psychological factors outside of the causal model. 
We believe that the difficulties associated with previous definitions are at least in part due to the fact that they failed 
to distinguish between different notions of cause.1 In fact, 
we would argue that like many problems in philosophy, one 
reason cause eludes satisfactory definition is because the intuitive concept is fundamentally imprecise. What criteria to apply is often a subjective decision, witnessed by the ease with 
which one can often convince others that an alternate criteria is the appropriate one. For example, one may argue that it is inappropriate to say that arsonist 1 caused the fire since 
regardless of his action, the fire would still have occurred. 
While the choice of criteria for determining cause is imprecise and subject to factors not representable in a functional causal model, the same is not true once a criteria has been chosen. Each criteria is precise and completely decidable based on the causal model and the state of its variables. 
Formulating and illustrating appropriate definitions of these 

criteria is the focus of the next section. 

5 A new proposal 

In this section, we introduce basic definitions which we then 
compose to form criteria for evaluating causal relationships. 
As there are several ways in which an event may be considered a cause, we provide definitions for the different aspects. 
We believe that all causal tests are based on two basic ideas, 
which are related to , though distinct from, necessity and sufficiency. By combining these two criteria we can produce the 
different criteria used for making causal judgments. 
First, we need some background and auxiliary definitions. 
Foremost among them is what we allow to be causes and effects. 

Definition 1 An instantiation x, of endogenous variables X 
is an event in context u if X(u) = x. A propositional formula 
\A6\D7 defined over the endogenous variables of the model is an 
effect in context u if \A6\D7(u). 

Any potential cause of an effect must be an event. Note that we force events and effects to be true in the situation being 
considered. This is merely a convenience meant to simplify 
subsequent definitions. 

Since we are concerned with determining cause in a particular 
context, the causal diagram of a model is not very informative. 
A variable X that in general is necessary to determine 
the value of another variable Y (and hence has an edge from 
X to Y in the diagram), may become irrelevant in a particular 
context. That is, a particular value of the background variables 
u may make the value of Y unresponsive to the value 
of A". This observation motivates the following definition: 

Definition 2 The context diagram of causal world M(u) is 
the directed graph whose vertices are the endogenous variables 
of M, and includes each edge (X, Y) where the function 
fy is nontrivial in X when the background variables are 

fixed to u. 

The context diagram simplifies reasoning about causal relationships in a specific context. 

1 [Hall, 2002] makes a similar argument, although the details differ 
from ours. 
Another preliminary notion we will need is sufficiency.2 

Definition 3 An event x is sufficient to cause effect \A6\D7 in context 
u if for all instantiations w of the nondescendants W of 
X in the context diagram of 


Note that it must hold only when the nondescendants are manipulated. 
Descendants are excluded from manipulation since 
we believe that the values they take on, including interplay 
between them, are an integral component of determining causation. 
Allowing them to be manipulated would upset and 
skew that process. 

Sufficiency is not really strong enough for our purposes. 
For example, consider this example from [Hall, 2002]: 

Suzy and Billy both pick up rocks and throw them 
at a bottle. Suzy's rock gets there first, shattering 
the bottle. Since both throws are perfectly accurate, 
Billy's would have shattered the bottle had it not 
been preempted by Suzy's throw 


Figure 1: Rock throwing example 

Figure 1 shows the causal diagram of a model of the story 
(the background variables, as well as the functional relationship 
between variables should be obvious in this story, as well 
as the others, so will not be explicated). BT (ST) represents 
Billy (Suzy) throwing BH (SH) represents Billy's (Suzy's) 
rock hitting the bottle, and BS stands for the bottle shattering. 
Billy throwing is sufficient to cause the bottle to shatter, but 
it doesn't actually participate in the causal process that made 
the bottle shatter. Suzy's action preempted Billy's. We expect 
not only sufficiency of the cause, but that in some sense, 
the event actually participates in the causal process. Defining 
this notion has been one of the primary difficulties in talking 
about actual cause. The following two definitions formalize 
these notions in a way that avoids the problems associated 
with previous approaches. 

Definition 4 Let Z be the children of X in the context diagram 
of u, and let Y be some non-empty subset of Z. Let W 
be the intersection of the descendants of X (including X) and 
the ancestors of Y (excluding Y). Then event xy is a child 
of event x in context 


A child event can be thought of as a next step in an unfolding 
causal process. The criteria that

 is merely to enforce 
the intuition that before a variable takes on a value, its direct 

2Note that this definition differs from the definition for sufficiency 
presented in [Pearl, 2000]. That definition was used for a 
different purpose, and is not appropriate in this context. It is more 
closely related to, although distinct from the notion of sustenance 
[Pearl, 2000]. 

causes must be fixed. That is, a new value can not be added, 
if the relevant parents have not already been included. 

Definition 5 An event x is a strongly sufficient cause of effect 
\A6\D7 in context u if either of the following conditions hold: 

. For all instantiations 
. 
x is a sufficient cause of \A6\D7 and there exists some child 
event yof x which is a strongly sufficient cause of \A6\D7 in 
context u 
Effectively, what we require is that the event be sufficient 
for the effect, and that as the causal process progresses from 
the event, the sufficiency is maintained. This prevents events 
that arc preempted from being strongly sufficient. 
whilei 
For example, 
sufficient for the only child 
event,

 
is not, so the event is not strongly sufficient. 
Suzy throwing, on the other hand, is strongly suf


 witnessed by the sequence 
Strong sufficiency is the first of the two primary notions 
used to evaluate causality in specific situations. In fact, strong 
sufficiency by itself corresponds to one of the primary notions 
of causation. Strong sufficiency corresponds to our intuitions 
about physical causality. The cause sets in motion a chain of 
events capable of bringing about the effect. Intuitively, we 
expect causation to be transitive. If x causes y and y causes 
z, we expect x to cause z.3 It is not hard to see that strong 
sufficiency meets that expectation. In fact, strong sufficiency 
turns out to be the only transitive relation among our definitions 
of cause. 

The other primary consideration when testing actual 
causality is the notion of consequentiality. In addition to participating 
in the causal process, we often require that an event 
be in some sense consequential in order to be considered a 
cause. That consideration motivates the following definition. 

Definition 6 Let Z be the nondescendants of X in the context 
diagram of u. Then, an event x is potentially consequential 
with respect to effect \A6\D7 in context u if for some instantiations 
). Event x 
is actually consequential with respect to effect \A6\D7 if for some 
instantiation 


Like strong sufficiency, actual conscquentiality alone is a 
common criteria for causation. For example, in the execution 
example from Section 4 the choice of lethal injection was not 
considered a cause of his death precisely because it was not 
consequential. If instead the gun had not been loaded, which 
would make the decision consequential, we would consider 
it a cause. Note that a consequential event necessarily participates 
in the actual causal process. That is, for every actually 
consequential event, there is a minimal strongly sufficient 
event that contains some member of the consequential event. 

Potential consequentiality on the other hand is not of itself 
enough for an event to be considered a cause. However, 
combining it with strong sufficiency produces a very common 
causal criteria. 

3 In fact, this intuition is so strong that some theories of causation 
start with the transitivity of cause as a starting point [Lewis, 1986]. 

Test Strong 
Sufficiency 
Test Consequentiality 
Actual | Potential | No 
Yes Strong Sustaining Strongly 
Sufficient 
No Consequential --

Table 1: Different types of actual cause can be tested by combining 
tests for strong sufficiency and consequentiality. 

Definition 7 An event x sustains effect \A6\D7 in context u if 'x is 
strongly sufficient and potentially consequential for \A6\D7. 

The notion of sustenance defined here is a modified version of 
the one that appeared in [Pearl, 2000]. It differs in two ways. 
First, it requires strong sufficiency as opposed to merely sufficiency. 
Second, it fixes the variables W, against which x 
must be tested, while the previous definition has W as one of 
the parameters. 

Sustenance is a very commonly applied criteria for causation. 
Most of the examples in [Halpern & Pearl, 2001a] 
which were deemed causes correspond to our notion of sustaining 
cause. In the forrest fire example in Section 4, arsonist 
1 was considered a cause of the fire because it sustained the 
forrest fire. 

A more restrictive causal criteria is also sometimes useful. 

Definition 8 An event x is a strong cause of \A6\D7 in context u if 
x is a strongly sufficient and actually consequential cause of 

A strong cause can be thought of as a complete explication of 
why the effect occurred. 

Often we talk about an event helping to cause an event even 
though it is neither consequential nor sufficient. It is instead 
part of some cause. That consideration motivates the following 
definition. 

Definition 9 An event xy (y may be empty) is a contributory 
cause of effect \A6\D7 in context u if x is potentially consequential, 
and for some event z, xz is a minimal sustaining cause of \A6\D7. 

Contributory causation is the closest in spirit to the ASC definition. 
If we insist on minimality, it reduces to singletons just 
as that definition does. Additionally, in all but one example 
included in [Halpern & Pearl, 2001a], contributory causation 
matches the pronouncements of the ASC definition. We consider 
that example among others in the next section. 

6 Examples 

The single example presented in [Halpern & Pearl, 2001a] 
where the ASC definition differs from our definition of contributory 
causation comes from the following story (along 
with some commentary), quoted from [Hall, 2002]: 

You are standing at a switch in the railroad tracks. 
Here comes the train: If you flip the switch, you'll 
send the train down the left-hand track; if you 
leave where it is, the train will follow the right-
hand track. Either way, the train will arrive at the 
same point, since the tracks reconverge up ahead. 
Your action is not among the causes of this arrival; 
it merely helps to determine how the arrival 

is brought about (namely, via the left-hand track, or 

via the right-hand track). 

Two models which produced different causal pronouncements 
were given, demonstrating the impact that model 
choice has on the determination of causation. The model that 
classified it as a cause is the one we arc interested in. Figure 2 
shows the model. F represents the state of the switch, LT (RT) 
whether the train goes down the left (right) track, and A represents 
whether the train arrives. The justification given was 


Figure 2: A possible model for the train story 

that under normal circumstances, we may not call it a cause, 
but if something abnormal were to happen (for example, one 
of the tracks malfunctions, or is destroyed) then the action 
would be a cause. Our new definitions say that flipping the 
switch is a strongly sufficient cause ( In the words of Hall it 
''helps to determine how the arrival is brought about"), but it 
is not even potentially consequential, so is not a contributory 
cause. This seems a more satisfactory result. 

Unlike the ASC definition, these definitions do not always 
reduce to singleton causes even if minimality is enforced. 
Additionally, the different criteria produce causes that correspond 
to different ways that people think about cause. 

For example, consider a situation where 5 people vote regarding 
a ballot measure. The votes are tallied, and the measure 
passes if 3 or more vote for it. The associated diagram 
appears in Figure 3. Suppose that 4 vote for the measure, and 
1 votes against. According to the ASC definition, each in-


Figure 3: A voting example. The V1 correspond to the votes 
cast, T the vote tally, and P whether the measure passed. 

dividual who votes for the measure is an actual cause of the 
measure passing. In the causal beam approach, nothing qualifies 
as a cause of the measure passing. In this approach, it 
depends what type of cause we arc considering. 

Three votes for the measure constitute a strong cause of the 
measure passing, while two do not. This corresponds to our 
notion that a majority of the votes is required to cause it to 
pass. 

Two votes for the measure constitute a consequential cause. 
Changing those votes to vote against the measure would make 

the measure fail. This corresponds to the way we often talk 
about causes in elections. Often election outcomes are attributed 
to the voting behavior of some minority of the electorate 
(in U.S. politics these include "Reagan Democrats", 
"soccer moms", "angry white men" , "African Americans", 
etc). While they are far from sufficient to win an election, 
we say that they caused the election result because if they 
had voted significantly differently the outcome of the election 
would have been different. On the other hand, most people 
understand that in the vast majority of elections their single 
vote did not cause the outcome. This is reflected in our definitions 
by the fact that none of the criteria yield a single voter 
as a cause in this situation. A single vote for the measure is a 
contributory cause however, as one would expect. 

[Halpern & Pearl, 2001a] provides a number of examples 
that have proved problematic for other methods. As most of 
the examples correspond either to issues of sustenance or consequentially4 
(and with the exception of the train example 
considered above, all of the positive examples are contributory 
causes, and the negative examples are not), we will not 
review them all here. We would like to revisit one example 
that was considered troublesome. The first version of the 
story is as follows: A machine severed Fred's finger while he 
was at work. He was rushed to the hospital and it was reattached. 
A month later his finger worked just as before. We 
would of course not consider getting his finger cut off a cause 
of it being fully function later (because it is not consequential). 
Now, suppose we add another component to the story: It 
is possible that Larry the loanshark may be waiting outside for 
Fred, so that he can cut off his finger to make an example of 
him for not paying his loan promptly. If Larry cuts off the finger, 
he will destroy it so that it can not be reattached. If Fred 
severs his finger at the factory, he will be rushed to the hospital, 
so Larry will not have an opportunity to do it himself. 
The causal diagram appears in Figure 4. Now, consider the 


Figure 4: Finger severing example. FS represents severing his 
finger. LL represents whether Larry is lurking. LC represents 
whether Larry cuts off his finger and FF represents whether 
his finger is fully functional 

situation in which Larry turns out not to be waiting outside, 
and Fred does sever his finger. Was Fred cutting his finger off 
a cause of his finger being fully functional the next month? 
According to the ASC definition it is. This was considered 
disturbing because adding a provision for a contingency that 
did not occur changed the the event from noncause to cause. 

4ln fact, most of the difficult examples play off of the conflicting 
interpretation of cause between sustenance and consequential ity. 
That is, the event in question is typically either consequential or sustaining, 
but not both. 

In light of our new definitions, we do not see that particular 
consideration as a problem. According to our definition, it 
again depends on what you are looking for. It is sustaining, 
but it is not consequential. Typically, when we are testing if 
an event caused something to remain unchanged, we are looking 
for a consequential cause. That is clearly the case in this 
example. However, it is still reasonable to say that cutting off 
his finger at the factory was a sustaining cause, because that 
event makes certain the effect, regardless of what Larry does 
or does not do. 

Our definitions also handle correctly the examples in [Hopkins 
& Pearl, 2003] avoiding the problems of ASC, but because 
of space limitations we will not review them all here. 
We will consider a single example of the problem to demonstrate 
how these definitions avoid it. Consider the following 
story: Before an execution, the captain loads one of the guns 
of the two members of his firing squad, putting a blank in 
the other gun. Both gunmen then fire simultaneously and 
the prisoner dies. In the current situation, the captain put 
the bullet in the gun of gunman 1, and both men shoot. The 
model consists of the loader, and the two gunmen as direct 
causes of the the variable representing whether the prisoner 
is alive. Now, we ask if gunman 2 shooting caused the prisoner's 
death. According to the ASC definition, it did. Gunman 
2 did not cause the prisoner's death according to any of 
our definitions of cause. It is clearly not consequential, nor 
sufficient, so it can not be a cause. It is not a contributory 
cause either because it is not a member of any minimal sufficient 
cause. According to these definitions, gunman 1 shooting 
is a consequential cause, gunman 1 shooting and the captain 
loading his gun is a strong cause, and gunman 1 shooting 
and the captain loading his gun are both contributory causes. 

7 Explanations 

If we have a probability distribution quantifying our uncertainty 
about what context will actually occur, we can use that 
distribution, combined with our model to produce explanations 
for an observed effect. 

Causation forms an integral part of evaluating explanations 
(see [Chajewska & Halpern, 1997; Pearl, 2000; Halpern & 
Pearl, 2001b] for critiques of methods that ignore causal considerations 
when producing causes). In spirit, we agree with 
the intuitive notion of explanation put forth in [Halpern & 
Pearl, 2001b]. That is, an explanation is fundamentally just a 
hypothesized event which if true would be a possible cause of 
the observed phenomenon. Our new view of actual causation, 
however, makes the particulars somewhat different. Our definitions 
of actual cause form the basis for evaluating possible 
explanations. We allow any instantiation of endogenous variables 
to be an explanation and compare them based on how 
well they explain the effect. According to our view, there are 
different types of explanation, just as there are different types 
of causes. Which type of explanation to choose depends on 
its intended purpose. This is in contrast to the approach using 
ASC as the basis for causal explanation. We believe that this 
approach provides more flexibility and is more general than 
ASC based explanations. 

By way of example, consider the following situation: We have a circuit consisting of a single 3 input parity gate, with 
inputs X1, X2 and X3, and output Y (i.e. Y \A1\AA 1 when an 
odd number of the inputs arc 1). Each input has an associated 
background variable, independent of the others, where 
So, typically 
we observe that 

 we want an explanation in terms of the A" variables. 
The first question is what constitutes an explanation 
(or even a partial explanation in the ASC definition). According 
to the ASC based definition, the only possible explanations 
(or partial explanations) are singletons.5 Now, how 
can we compare between them? The ASC approach provides 
three measures of the quality of an instantiation x as an explanation 
of effect \A6\D7 in light of evidence e (which must entail 
\A6\D7). The first, named goodness is Pr(x causessecond, the posterio 
\A6\D7|xe).'' The 
The third, explanatory 
power is , The goodness of all 
explanations in this example are 1. Also, the explanatory 
power,

 since every instantiation 
is a weak cause according to the ASC definition. Table 
2 shows the explanatory power and posterior probability 
of the different possible explanations. It is not clear how to 
choose between them in this case. One thing to notice about 
this example is that every variable is consequential in every 
context, so notions of consequential causation (or by extension, 
contributory causation) become less interesting. We 
would argue that in this case, what we are really interested 
in is sustenance. Our definitions allow that to be considered 
easily. For a particular instantiation we can calculate the 
probabilities Pr(x sustains , Pr(x strongly causes \A6\D7|e), 
Pr(x is a consequential cause of i from the model and the 
probability distribution over the background variables. For a 
particular causal criteria

 the probability that x is a 
cause of \A6\D7 given e using criteria c is just 


The probability of causation can then be used as the measure 
of explanation quality. In our circuit example, for example, 
.444, which is the maximal probability for sustenance. This 
explanation (which is not allowed in the ASC version) seems 
more satisfying than any of the single variable explanations. 


5Notc that this is not always the case. This example was specifically 
crafted to exhibit such behavior. 

6 In the ASC definition of explanation, it "causes" would mean is 
a weak cause. We write it in a general form, because these measures 
are applicable to the new method as well 

This is not to say that the measures used in [Halpern & 
Pearl, 2001 b] are not useful. In fact they can be very useful. 
We simply note that our definitions allow a further analysis 
that is useful in other situations. They prove especially useful 
when we are looking for a sufficient or sustaining explanations. 
Because our definitions can handle different notions of 
cause, the explanations they produce can be more informative 
than the explanations formed when using the methods in 
[Halpern & Pearl, 2001b], where they are restricted to one 
weak form of cause. 

8 Conclusion 

We demonstrated that the intuitive choice of causal criteria is 
subjective, and is influenced psychological factors not rcpresentable 
in a causal model. The criteria, once chosen however 
can be evaluated from the information contained in the causal 
model. We presented new criteria covering the different notions 
of causation. Fundamental among these is the definition 
of strong sufficiency, which provides a simple and intuitively 
satisfying definition for actually participating in the 
active causal process that generated the effect. These definitions 
avoid the problems of previous attempts, and allow for 
a more refined discussion of actual causality. Additionally, 
they provide a foundation for evaluating explanations, which 
proves to be more satisfactory than previous attempts to use 
functional causal models for explanation. 

Acknowledgment 

This work has been partially supported by MURI grant 
N00014-00-1-0617 

Abstract 

An approach for designing countermeasures to cure 
conflict in aircraft pilots' activities is presented, 
both based on Artificial Intelligence and Human 
Factors concepts. 
The first step is to track the pilot's activity, i.e. to 
reconstruct what he has actually done thanks to the 
flight parameters and reference models describing 
the mission and procedures. The second step is 
to detect conflict in the pilot's activity, and this is 
linked to what really matters to the achievement 
of the mission. The third step is to design accurate 
countermeasures which are likely to do better 
than the existing onboard devices. The three 
steps are presented and supported by experimental 
results obtained from private and professional pilots. 


1 Introduction 

The review of civilian and military reports reveals that a conflictual 
situation is a precursor to the loss of aircrews' situation 
awareness and is a major cause of air accidents. Conflict 
may occur through different forms: open conflict between the 
different operators (Air Philippines crash, April 1999), representation 
conflict about the aircraft position (Korean air, August 
1997), resource conflict (Streamline, May 2000), knowledge 
conflict (Crossair, January 2000), or more recently, a 
conflict between automated systems (TCAS) and human operators 
(air collision between a Tupolev 154 and a Boeing 757 
over Switzerland, July 2002). Therefore, the idea is to detect 
conflict so as to propose accurate on-line countermeasures to 
pilots. 
The first step is to track the pilot's activity, i.e. to reconstruct 
what he has actually done thanks to the flight parameters and 
reference models describing the mission and procedures. The 
second step is to detect conflict in the pilot's activity, and this 
is linked to what really matters to the achievement of the mission. 
The third step is to design accurate countermeasures 
which are likely to do better than the existing onboard devices. 
The three steps are presented and supported by experimental 
results obtained both from private and military pilots. 

2 Track the pilot's activity 

2.1 Reference model 
The approach to capture the pilot's activity is based on situation 
tracking classically used for system diagnosis, monitoring 
and intelligence [Ghallab, 1996; Chaudron et al, 1997; 
Rota and Thonnat, 2000]. Whatever the formalism used to 
represent knowledge, situation tracking rests on a matching 
between observed facts and known situations (reference models), 
and its object is to carry out postdictions, predictions or 
to diagnose the current state of the system. 
To represent knowledge, we use the concept of propositional 
attitude, "which covers the notions of belief, desire, intention, 
[...] which have in common the fact of being identified 
by their pwpositional contents " [Tessier et al, 2000]. 
Definition: a propositional attitude (PA) is a pair (K.T) 
where K is a logical cube, i.e. a conjunction of first order 
literals [Chaudron et al, 2003], and T is the time interval 
within which the properties captured by K are true. 
The pilot's knowledge is divided into three categories of PAs, 
which constitute the reference model: 

. Propositional attitude goal describes a goal of the mission. 
Ex: pa-goal(goal(5) ,<Landing(francazal)>,T) 
means that the fifth goal of the mission is to land at Francazal 
during time interval T. 
. Propositional attitude plan describes the properties the pilot 
is committed to satisfy to reach a PA-goal as long as he carries 
out this goal. 
Ex: pa.plan(goal(5) , <VOR-frequency (116) , 
VOR-heading.def lection (true) , gear_down (true) , 
flaps-down (2) , visibility (true) >,T) 
means that to satisfy goal 5, the pilot has to select the accurate 
radio frequency, fly a correct route, lower gears and flap 
deflection 2 and have a good visibility during time interval T. 
. A crucial propositional attitude describes the hard constraints 
associated to one or several goals. 
Ex: pa-crucial(goal(5),<visibility(true)>,T) 
means that to succeed in landing at Francazal, the pilot must 
have a correct visibility during T. 
Remark: the model does not implement deontic logic [Traum 
and Allen, 1994] to capture the pilot's obligations; at each 
time step, the system will check whether the pilot meets the 
different constraints on the PA-goals. 

2.2 Observed activity 
The pilot's activity is observed and analyzed through his actions on the airplane interface: during the experiments, the flight parameters are recorded every second. The parameters are then translated into symbolic knowledge and aggregated to build observation prepositional attitudes. 
The numerical thresholds that are used for numerical to symbolic translation are adjusted during interviews with the pilots 
to built an accurate model for each pilot. For example, if the 
pilot has to fly 2500 ft, he defines his own flight envelope 
where he feels secure [Amalberti and Wioland, 1997]. After 
numerical to symbolic translation, parameter altitude is characterized as altitud e (false) (i.e. out of flight envelope) 
or allitude (true) (i.e. within flight envelope). 
The same kind of analysis is performed with parameters such as the route, the fuel level, etc. 

Therefore, propositional attitude observation describes perceived 
and deduced facts from the pilot's activity: 
Ex: pa.obs (<VOR_f requency (116) , 
VOR_heading.def lection (true) , gear_down (true) , 
flaps-down (3) , visibility (false) >,<1500,15 00>) 
means that at time 1500, the pilot has selected the accurate 
radio frequency, is flying a correct route, has lowered gears 
and flap deflection 3 and has a bad visibility. 


3 Detect conflict 

3.1 Conflict 
Considering research in the field of A.I. and particularly 
on multi-agent systems, the focus has been 
brought much on how to avoid, solve or get rid of 
conflicts, especially via negotiation [Sycara, 1989; 
Rosenschein and Zlotkin, 1994], argumentation [Jung and 
Tambe, 2000] or constraint satisfaction [Conry et al., 1991; 
Khedro and Genesereth, 1994; Hannebauer, 2000]. Such 
approaches, where (i) conflict is likened to formal inconsistency 
and (ii) the internal knowledge of the agents is clearly 
identified, are not suited to our purpose. Indeed in aeronautics, 
uncertainty on data is high and the human behavior is 
extremely variable and unpredictable. Moreover, while in 
air, aircrews frequently face incoherent situations due to the 
uncertain environment (e.g. weather, failures) or because 
of their own errors. Nevertheless, these situations are not 
necessarily conflictual, and pilots are used to going on with 
their missions with such inconsistencies. An inconsistency 
becomes conflictual only if it leads to a critical flight phase 
and matters to the mission or to the aircrew's safety. 
Therefore we are very close to Eastcrbrook's definition of 
conflict [Easterbrook, 1991]: "[...] viewpoints are free to 
differ, and only conflict when that difference matters for 
some reason, leading to interference. [...] A conflict, then, 
is simply a difference that matters." This point of view 
has been emphasized in social sciences [Festinger, 1957; 
Lewin et al.f 1939] and in A.l. applied to social modeling 
[Castelfranchi, 2000], where human conflict is seen as the 
result of an impossibility to succeed in a goal that matters. 
Definition: a conflict is a set of propositional attitudes that is 

not coherent1 and the fact that it is not coherent matters. 

The inconsistency of a set of PAs matters either because 

there is a need: 

. to satisfy the rationality2 of the agent(s) involved (pilot, 
copilot...); 
. or to build a coherent set of knowledge; 
. or to decide on further goals. 
All these different reasons are implemented in the reference 
model as crucial propositional attitudes (see section 2.1). 
Therefore a conflict is an incoherent set of knowledge that 
matters through one or several crucial propositional attitudes. 
3.2 Conflict detection 
Conflict detection is a tracking process that monitors the 
pilot's PAs at each time step t. It is a two-step process: (1) 
detect the sets of PAs that are not coherent at time t and (2) 
detect the conflicts given the crucial PAs that concern time 

t. One could have designed a single-step process, however 
it is of great interest to track PAs that are not coherent and 
do not conflict: indeed, contrary to the minimal inconsistent 
sets approach [De Kleer, 1986], we aim at capturing all 
the inconsistencies and analyzing how they are managed 
by the pilots for "lessons learned" and further man-artefact 
interaction model tuning. 
(1) Detect the sets of PAs that are not coherent 
Predicate not -hold-together detects incoherent sets of 
PAs Inct at time t,. 
Ex: given the set of PAs 
.pa_goal(goal(5) , <Landing(francazal)>,<1000,tl>) 
. pa_plan(goal (5) , <VOR_f requency (116) , 
VOR_heading.deflection (true) , gear_down (true) , 
flaps-down(2),visibility(true)>,<l000,tl>) 
. pa.obs (<VOR.frequency (116) , 
VOR_heading_deflection (true) , gear_down (true) , 
f laps_down(3),visibility(false)>/<1500/1500>), 
two incoherent sets of PAs arc detected: 
\A1\AA> not_hold altogether (pa_plan (5, flaps-down (2) ) , 
pa.obs (flaps_down (3) , <1500,1500>) ; 
\A1\AA. not_hold_together (pa.plan (5 , visibilit y (true) , 
pa.obs(visibility(false)),<1500,1500>). 
(2) Detect the conficts 
The crucial PAs that concern time t are then considered. 
Definition: let Inct an incoherent set of PAs at time /. If 
Inct includes PAs or properties that matter through one or 
several crucial PAs, then Inct is a conflict. 
Predicate matters screens sets Inct thanks to the crucial 
PAs, so as to determine whether they are conflicts or not. 
Ex: for the previous example, the crucial PA that holds at 
time 1500 is: 
. pa.crucial(goal(5),<visibility(true)>, 
<1000,tl>) 
Predicate matters detects that one of the two incoherent 
1 in relation to the formalism used to represent the properties in 
the PAs. 

2 According to Festinger [Festinger, 1957] who postulates that an 
individual always aims at being coherent when interacting with his 
environment. 

sets of PAs is conflictual: 

matters (pa_plan(goal (5) , visibility (true) ) , 
pa_obs(visibility(false),<1500,1500>) 


The other set of PAs relative to flaps deflection is not a 
conflict because a landing with flap deflection 3 is possible 
even though less recommended. 

3.3 Preliminary experiment and results 
A preliminary experiment designed to test conflict tracking 
[Dehais, 2002] was conducted with ten pilots on a research 
flight simulator. The pilots were proposed a flight scenario 
whose aim was to follow a flight plan at a constant altitude 
and then land with instruments. The difficult point in this 
scenario was to manage the fuel consumption correctly considering 
refuelling areas. 
This experiment has validated conflict detection and interestingly 
enough has shown that a psychological perseveration 
phenomenon appeared with conflict : the pilots who faced 
conflictual situations did not come to the right decision to 
solve their conflict, on the contrary they got themselves tangled 
up in it, and all the more so since the temporal pressure 
was increasing. The most surprising thing was that all the 
necessary data to come to the accurate decision were available 
and displayed on the onboard equipments. 
Ex: one of the pilots flew toward waypoint 5 instead of flying 
first over waypoint 6 for refuelling, and started circling 
around, until he ran out of fuel (see figure 1). 

Figure 1: Conflict and perseveration 


vinced that waypoint 5 was the refuelling area (in spite of 
the map displayed in the cockpit); he reached it and noticed 
that the fuel level did not increase. Thinking he had missed 
waypoint 5, he decided to fly back to waypoint 5. He did 
this manoeuvre again and again till he ran out of fuel, getting 
more and more stressed. 

4 Send accurate countermeasures 

4.1 Guidelines 
The findings of the preliminary experiments are akin 
to a recently published report of the BEA3 (the French 
national institute for air accident analysis) that reveals 
that pilots' erroneous attitudes of perseveration 
have been responsible for more than 40 percent 
of casualties in air crash (in civilian aeronautics). 
This behavior, called the "perseveration syndrome", 
is studied in neuropsychology [Vand Der Kolk, 1994; 
Pastor, 2000] and psychology [Beauvois and Joule, 1999]: 
it is known to summon up all the pilot's mental efforts 
toward a unique objective (excessive focus on a single 
display or focus of the pilot's reasoning on a single task). 
Once entangled in perseveration, the pilot does anything to 
succeed in his objective even if it is dangerous in terms of 
security, and worse, he neglects any kind of information that 
could question his reasoning (like alarms or data on displays). 

Therefore the idea is to design countermeasures in order to 
break this mechanism and get the pilot out of the conflict. The 
design of the countermeasures is grounded on the following 
theoretical and empirical results: 

. conflictual situations lead pilots to persevere; 
. additional information (alarms, data on displays) designed 
to warn pilots arc often unnoticed when pilots persevere. 
Our conjectures are then: 
\A1\AA. with an accurate on-line conflict detection, it is possible to 
analyze and predict the subject of the pilot's perseveration; 
\A1\AA> instead of adding information (classical alarms), it is more 
efficient to remove the information on which the pilot is focused 
and which makes him persevere, and to display a message 
to explain his error instead. 
Ex: in the experiment described afterwards, pilots persevere 
in trying a dangerous landing at Francazal despite the bad 
visibility, with a particular focus on an instrument called the 
H.S.I.4 to locate Francazal airport. The countermeasures will 
consist in removing the H.S.I, during a few seconds, to display 
two short messages instead, one after the other ("Landing 
is dangerous"... "look at central display") and next to 
send an explanation of the conflict on the central display 
("Low visibility on Francazal, fly back to Blagnac"). 

The idea here is to shock the pilot's attentional mechanisms 
with the short disappearance of the H.S.I., to introduce a cognitive 
conflict ("iff land, I crash") to affect the pilot's reasoning, 
and to propose a solution on the central display. 

3 http://www.bea-fr.org 

4The H.S.I is a display that gives the route to follow and indicates 
any discrepancy from the selected route. 

During the debriefing, the pilot explained that he was con-

4.2 GHOST 
Ghost is an experimental environment designed to test countermeasures 
to cure the pilot's perseveration. It is composed 
of; 

. Flightgear5, an open source flight simulator, which means 
that many modifications can be made, e.g. implementing new 
displays. To fly the airplane, pilots have a stick, rudder and 
a keyboard. Almost all the airports and beacons (NDB and 
VOR6) of the world are modeled, and the air traffic control is 
simulated. During the experiment, the flight simulator interface 
is displayed on a giant screen; 
. Atlas7, a freeware designed to follow the pilot's route. The 
airplane trajectory, cities, airports and beacons arc displayed 
with an adjustable focus; 
. a wizard of Oz interface (see figure2) we have designed 
and implemented, which allows a human operator to trigger 
events (failures, weather alteration and the countermeasures) 
from an external computer via a local connection (TCP/IP 
protocol, sockets communication). 
As far as the countermeasures are concerned, several actions 
are available to the wizard of Oz: 
. replace a display on which the pilot is focused by a black 
one (time of reappearance is adjustable); 
. blink a display (frequency and time of blinking is adjustable); 
. fade a display and brighten it again; 
. send a message to a blinked, removed or faded display; 
. send a message to the central display for explicit conflict 
solving. 
Figure 2: The wizard of Oz interface - countermeasures 

4.3 Experimental scenarios 
As conflict appears when a desired goal cannot be reached, 
we have designed three experimental scenarios where a crucial goal of the mission cannot be achieved: 

. scenario 1 is a navigation task from Toulouse-Blagnac airport 
to Francazal airport including three waypoints (VOR 
117.70, NDB 331 and NDB 423). An atmospheric depression 
is positioned in such a way that the pilot cannot see the 
landing ground but at the last moment when it is too late to 
land on it. 
. scenario 2 is a navigation task from Toulouse-Blagnac airport 
to Francazal airport including three waypoints (VOR 
117.70, NDB 415 and NDB 423). The visibility is decreased 
from the runway threshold: from far away, the landing ground 
is visible but as the pilot gets closer to it, it disappears totally 
in a thick fog. 
. scenario 3 is a navigation from Toulouse-Blagnac back to 
Toulouse-Blagnac including three waypoints (VOR 117.70, 
NDB 331 and NDB 423). An atmospheric depression is positioned 
over waypoint 2 (NDB 331), and as the pilot flies over 
this waypoint, the left engine fails. 
In scenarios 1 and 2 the pilot's conflict can be summarized as 
"should I try a dangerous landing at Francazal or should I 
fly back to Blagnac for a safer landing?". If our conjectures 
hold, pilots will persevere and try to land at Francazal. 
In scenario 3, the pilot's conflict can be summarized as 

"should J go on with the mission despite the failure or should 
I land at Francazal and therefore abort the mission?" 

4.4 Experimental Results 
21 experiments were conducted with Ghost in December 
2002 (see figure 3). The pilots' experiences ranged from 
novice (5 hours, small aircraft) to very experienced (3500 
hours, military aircraft). Conflicts where detected with the 
conflict tracker (described above) and were inputs for the wizard 
of Oz to elaborate the countermeasures. 

Results for scenarios 1 and 2: "impossible landing" 
In these scenarios, the pilots faced the decision of mission 
abortion, i.e. not to land at Francazal and fly back to Blagnac. 
Both scenarios were tested within two different contexts: 
without countermeasures and with countermeasures. 


. Context 1: no countermeasures 
7 pilots tested scenario 1 or 2 without any countermeasures 
(see next table). "Circuits" corresponds to the number of circuits 
performed by the pilot round Francazal before crashing 
or landing. 
The results suggest that without any countermeasures, 
none of the pilots came to the right decision (fly back to 
Blagnac): they all persevered at trying to land at Francazal. 
Four of them hit the ground, and the other three had a "chance 
landing", which means that while they were flying round 
Francazal, the runway appeared between two fog banks and 

they succeeded in achieving a quick landing. During the debriefing, 
all of them admitted they had come to an erroneous 
and dangerous decision. 

. Context 2: with countermeasures 
12 pilots tested scenario 1 or 2 with countermeasures (see 
next table). "Circuits" corresponds to the number of circuits 
performed by the pilot round Francazal before a countermeasures 
is triggered by the wizard of Oz. 
The results show the efficiency of the countermeasures 
to cure perseveration: 9 pilots out of 12 changed their 
minds thanks to the countermeasures, and flew back safely 
to Blagnac. During the debriefing, all the pilots confirmed 
that the countermeasures were immediately responsible for 
their change of mind. Moreover, the short disappearance of 
the data due to the countermeasures did not cause any stress 
to them. 4 military pilots found that the solutions proposed 
by the countermeasures were close to what a human co-pilot 
would have proposed. 
The results with Pilot 19 suggest that the more a pilot perseveres, 
the more difficult it is to get him out of perseveration. 
During the debriefing, Pilot 19 told us that he was obsessed 
by the idea of landing, and that he became more and more 
stressed as long as he was flying round Francazal. He then 
declared that he did not notice any countermeasures. Pilot 16 
and Pilot8 also persevered despite the countermeasures: they 
declared they knew they were not flying properly and that 
they had done that on purpose because they wanted to test the 
flight simulator. 

Results for scenario 3: "failure" 
Only 2 pilots tested scenario 3. One pilot experimented this 
scenario without any countermeasures: he did not notice the 
failure and hit the ground. The second pilot was warned 
through the countermeasures that he had a failure and that 
there was a landing ground at vector 80: the pilot immediately 
performed an emergency landing on this landing ground. 
Other experiments are currently being conducted with this 
scenario. 

Other results 
During the experiments, 9 pilots made errors, e.g. selection 
of a wrong radio frequency, erroneous altitude, omission to 
retract flaps and landing gear. To warn them, the wizard of 
Oz blinked the display on which they were focusing and 
displayed the error (e.g.: "gear still down"). In each case, 

the countermeasure was successful: the pilots performed 
the correct action at once. During the debriefing, the pilots 
declared that this kind of alarm was very interesting, and 
much more efficient and stressless than a classical audio or 
visual alarm because they could identify at once what the 
problem was. 

These experiments have shown also the significance of the 
message contents on the blinked display. For example, as far 
as the erroneous landing at Francazal is concerned, the initial 
message was "Don't land'', but the pilots did not take it 
into account, thinking it was a bug of the simulator. When 
the message was changed to "Fly back to Blagnac" or "Immediate 
overshoot" it was understood and taken into account 
at once. 


Figure 3: GHOST 

5 Conclusion and perspectives 

We have presented an approach to detect and cure conflicts in 
aircraft pilots' activities. A first experiment has validated the 
conflict detection tool and has shown that pilots facing conflicts 
have a trend to persevere in erroneous behaviors. This 
agrees with the observations made by experts of the pilots' 
behaviors in real air accidents. These first results have led us 
to design GHOST, an experimental environment dedicated to 
test countermeasures designed to get pilots out of perseveration. 
A second experiment was conducted with twenty-one 
pilots in this new experimental environment: it has proved 
the efficiency of the countermeasures to cure perseveration 
and has also confirmed the relevance of conflict detection. 
Further experiments are to be conducted to go on tuning the 
countermeasures according to the pilots' feedback. 

The next step is to design and implement the whole countermeasures 
closed-loop, i.e to remove the wizard of Oz and 
perform an automated conflict and perseveration management. 
This is currently done thanks to (see figure 4): 

. the conflict detection tool (see section 3). 
. Kalmansymbo(aero), a situation tracker and predictor 
[Tessier, 2003] which is based on predicate/transition timed 
Petri nets for procedure and pilot's activity modeling. 
Kalmansymbo assesses the current situation thanks to the pa-

rameters received in real-time from Flightgear and predicts 
what is likely to happen. Both the current situation and the 
predictions are inputs for the conflict detection tool, which 
allows possible conflicts to be better anticipated. 

. a tool for building and sending accurate countermeasures 
back to the pilot via the Flightgear cockpit. 
Figure 4: The countermeasures closed-loop 

Acknowledgments 

Many thanks to Olivier Grisel and Fabrice Schwach for their wonderful 
work on Flightgear and the wizard of Oz. 

Abstract 

Conventional methods used for the interpretation 
of activation data provided by functional neuroimaging 
techniques provide useful insights on 
what the networks of cerebral structures are, and 
when and how much they activate. However, 
they do not explain how the activation of these 
large-scale networks derives from the cerebral 
information processing mechanisms involved in 
cognitive functions. At this global level of representation, 
the human brain can be considered as 
a dynamic biological system. Dynamic Bayesian 
networks seem currently the most promising 
modeling paradigm. Our modeling approach is 
based on the anatomical connectivity of cerebral 
regions, the information processing within cerebral 
areas and the causal influences that connected 
regions exert on each other. The capabilities 
of the formalism's current version are illustrated 
by the modeling of a phonemic categorization 
process, explaining the different cerebral activations 
in normal and dyslexic subjects. The 
simulation data are compared to experimental results 
[Ruff et al, 2001]. 

1 Introduction 

In Neurology and Neuropsychology, the diagnosis of the 
neurological causes of cognitive disorders, as well as the 
understanding and the prediction of the clinical outcomes 
of focal or degenerative cerebral lesions, necessitate 
knowing the link between brain and mind, that is what 
the cerebral substratum of a cognitive or a sensorimotor 
function is and how the substratum's activity can be interpreted 
in cognitive terms, i.e. in terms of information 
processing. 

Studies in humans and animals [Bressler, 1995; Demonet 
et al, 1994] have shown that sensorimotor or cognitive 
functions are the offspring of the activity of oriented 
large-scale networks of anatomically connected 
cerebral regions (Figure 1). In humans, functional neuroimaging 
techniques provide activation data, which are 
indirect measures of the brain's electrical or metabolic 

activity during a task performance. Statistical analyses of 
the activation data allow determining where [Fox and 
Raichle, 1985], i.e. in which areas, and/or when [Giard et 
al., 1995] during the task performance, the activation 
reaches local extrema. Through the study of covariation 
between local activations, they give a sketch of what the 
network of cerebral areas involved in the cognitive function 
is [Herbster et al. ,, 1996]. A known oriented anatomical 
link between 2 areas allows determining why the 
activation of one area can affect the other one [Buchel 
and Friston, 1997]. Above methods allow identifying the 
substratum of a cognitive function and the activation 
level and dynamics of the substratum during the function 
performance. They do not give any clue of how the cognitive 
processes participating in the function are implemented 
by the substratum and how the activation derives 
from the processing. That is, they do not allow interpreting 
neuroimaging data as the result of information processing 
at the integrated level of large-scale networks. 

Figure 1: Large-scale network involved in phoneme monitoring, 
according to results from [Demonet, et al., 1994] 

Interpretative models, linking a networked structure 
activity to the realization of a function, are at the core of 
Computational Neurosciences. Most existing works in the 
domain are based on formal neural networks, with varying 
levels of biological plausibility, from physiology 
[Wang and Buzsaki, 1996], hardly interpretable in terms 
of information processing, to more or less biologically 

plausible models of how basic cognitive functions 
emerge from neuronal activation [Grossberg et al, 2002], 
and to purely functional models [Cohen et al., 1990], not 
concerned with cerebral plausibility. Although these 
models answer the how, they do not meet two major requirements 
for an interpretative approach of functional 
neuroimaging data. The models are not explicit enough to 
be directly used for clinical purpose, and they cannot 
evolve quickly and easily with new findings in neuroscience, 
such as the integration of more detailed knowledge 
on the substratum, which often necessitates a complete 
rebuilding of the formal network. 

The causal connectivity approach [Pastor et al. , 2000] 
aims at answering the how and satisfying the constraints. 
However, the underlying formalism, causal qualitative 
networks based on interval calculus, limits severely the 
biological plausibility of the models, since it cannot represent 
major cerebral features, such as learning or the 
non-linearity and the uncertainty of cerebral processes. 
Dynamic Bayesian networks only meet the three major 
constraints: temporal evolution, uncertainty and nonlinearity 
[Labatut and Pastor, 2001]. The utility of graphical 
probabilistic formalisms for cognitive modeling has also 
been demonstrated in the representation of visuomotor 
mechanisms with Bayesian networks [Ghahramani and 
Wolpert, 1997]. 

Hereafter, we describe how the interpretation of functional 
images for a clinical purpose can be tackled. Section 
2 presents our viewpoint on large-scale cerebral 
networks. After a short introduction to dynamic Bayesian 
networks, section 3 describes the characteristics of our 
formalism. Section 4 illustrates the formalism's capabilities 
by an example. We conclude with some perspectives. 

2 Representation of Large-Scale Cerebral 
Networks 

2.1 Structural and Functional Nodes 
The function implemented by a large-scale network depends 
on three properties: the network's structure 
[Goldman-Rakic, 1988], the functional role of each node 

(E.g. Wernicke's area (Figure 1), which is supposed to 
realize the early stages of phoneme processing), and the 
properties of the links (length, role: inhibitory or excitatory, 
...). In each network, regions, which are the stridetural 
nodes, are information processors and connecting 
oriented fibers are information transmitters [Leiner and 
Leiner, 1997]. 
All neurons in a region do not have the same structure 
or the same role. Similar neurons constitute generally 
local populations that realize a specific function. For example, 
the inhibitory role of GABAergic neurons on 
other neuronal populations may explain the fact that 
every visual stimulus is not perceived in high frequency 
stimulation [Pastor, et al, 2000]. Therefore, each region 
is itself a network of smaller neuronal populations {functional 
nodes), connected through neuronal fibers. These 

nodes are information processors that implement functional 
primitives, which may all be different. 

A large-scale network has therefore neurophysiologically 
constrained, oriented edges and possibly differentiated 
nodes. The explicit representation of the nodes' 
function allows the direct expression of hypotheses on 
cerebral processing, and their easy modification in order 
to follow the evolution of knowledge in neurosciences. 
This cannot be dealt with by formal neural networks' 
implicit modeling that requires modifying the whole network 
architecture to implement functional changes. 
Hereafter, a structural or a functional structure will be 
indifferently named a cerebral zone. 

2.2 Information Representation and 
Processing 
The cerebral information processed by a neuronal population 
can be seen as the abstraction of the number and the 
pattern of the neurons firing for this information. It can be 
represented both by an energy level and by a category. Energy 
is indirectly represented by the imprecise activation 
data provided by neuroimaging techniques. The category 
representation is in agreement with the "topical" organization 
of the brain, which reflects category maps of the input 
stimuli, and can persist from primary cortices to nonprimary 
cortices and subcortical structures [Alexander et al, 1992], 
through transmission fibers [Leiner and Leiner, 1997]. The 
energy and the category of a stimulus can also be easily 
extracted from its psychophysical properties. 

Modeling cerebral processes necessitates an explicit and 
discrete representation of time, both for taking into account 
the dynamics of cerebral mechanisms (transmission delays, 
response times...), and for complying with sampled functional 
neuroimaging data. 

According to a definition of causality inspired by Hume 
[Hume, 1740] and consistent with Pearl's probabilistic causality 
[Pearl, 2001], information processing in a large-scale 
network can be considered as mediated through causal 
mechanisms. Causality is defined by three properties: spatial 
and temporal contiguity, temporal consistency, and statistical 
regularity [Labatut and Pastor, 2001]. In other words, 
two entities A and B are causally linked if they are contiguous 
relatively to the system they belong to, if the beginning 
of A precedes temporally the beginning of B, and if most of 
the times, A provokes B. In the brain, oriented anatomical 
links provide spatial and temporal contiguity between cerebral 
nodes, cerebral events are temporally consistent (a firing 
zone provokes the activation of downstream zones), and 
there is a statistical regularity in the response of a specific 
neuronal population to a given stimulus. 

3 Description of the Formalism 

3.1 Dynamic Bayesian Networks 
In summary, the brain can be viewed as a network whose 
nodes are differentiated dynamic and adaptive informa-

tion processors and oriented edges convey causality. 
Moreover, cerebral mechanisms, which are the abstraction, 
at the level of a neuronal population, of the chemical 
and electrical mechanisms at the cell levels, are often 
nonlinear. Causal dynamic Bayesian networks are the 
paradigm that meets best the constraints derived from 
these properties [Labatut and Pastor, 2001]. 

A causal Bayesian network consists of a directed 
acyclic graph where nodes represent random variables 
and edges represent causal relationships between the 
variables [Pearl, 1988]. A conditional probability is associated 
with each relationship between a node and its parents. 
If the node is a root, the probability distribution is a 
prior. When some nodes' values are observed, posterior 
probabilities for the hidden nodes can be computed 
thanks to inference algorithms such as the junction tree 
algorithm [Jensen, 1996]. 

In a dynamic Bayesian network (DBN), the evolution 
of random variables through time is considered. Time is 
seen as a series of intervals called time slices [Dean and 
Kanazawa, 1988]. For each slice, a submodel represents 
the state of the modeled system. DBNs are used to model 
Markovian processes, i.e. processes where a temporally 
limited knowledge of the past is sufficient to predict the 
future. The choice of the inference algorithm, generally 
an extension of the junction tree algorithm [Murphy, 
1999], depends on the DBN's structure, the nature of its 
variables (discrete or continuous), and relationships (linear 
or nonlinear). 

Activation data and/or the subject's responses to the 
stimuli are the only observable variables we have. Therefore, 
they must be integrated in our models. One may 
reasonably consider that the hidden variables, describing 
the successive states of the cerebral network, constitute a 
Markov chain, and that observable variables depend only 
on them. Moreover, the variables are continuous and their 
relationships may be nonlinear. This is typically the description 
of a type of DBNs called fully nonlinear state 
space models. Specific and recent algorithms allowing 
dealing with nonlinearity exist for this type of structures. 
Their general principle is to linearize the model in order 
to apply the classic Kalman filter. These algorithms differ 
on the used linearization method: first-order Taylor approximations 
for the extended Kalman filter [Julier and 
Uhlmann, 1997; Norgaard et al, 2000] or polynomial 
approximations for the unscented Kalman filter [Julicr 
and Uhlmann, 1997], the divided difference filter (DDF) 
[Norgaard, et al, 2000], and others [Van Der Merwe and 
Wan, 2001]. The algorithms based on polynomial approximations 
seem to give more reliable results 
[Norgaard, et al., 2000]. Their computational complexity 
is 0(L3), where L is the state dimension [Van Der Merwe 
and Wan, 2001]. They offer equivalent qualities, but 
those of the DDF are more accurate according to its author 
[Norgaard, et al, 2000]. 

3.2 Formal definition 
Static and Dynamic Networks 
A static network is the graphical representation of a 
large-scale network, whose nodes are cerebral zones and 
edges are the oriented axon bundles connecting zones. 
Due to anatomical loops, it is often cyclic. The DBN is 
the acyclic temporal expansion of the static network. 
Each node of the DBN is the processing entity related to 
a cerebral zone, i.e. the mathematical expression, at a 
given time slice, of information processing in the zone. 
Each edge is the propagation entity, whose orientation is 
its corresponding axon bundle's orientation. When deriving 
the DBN from the static network, values are given to 
the temporal parameters, according to known physiology 
results (e.g. the transmission speed in some neural fibers). 
That is, the length of the time slices is fixed, and a 
delay representing the average propagation time in the 
bundle's fibers is associated to the propagation entity. 

Information Representation 
Cerebral information is the flowing entity that is computed 
at each spatial (cerebral zone) and temporal (time 
slice) step, by a processing entity. It is a two-
dimensioned data. The first part, the magnitude, stands 
for the cerebral energy needed to process the information 
in the zone. It is represented by a real random variable in 
the DBN. For the second part, the type, which represents 
the cerebral category the zone attributes to the information, 
the representation is based on the symbol and categorical 
field concepts. 
A symbol represents a "pure" (i.e. not blurred with 
noise or another symbol) category of information. For 
example, when the information represents a linguistic 
stimulus, a symbol may refer to a non ambiguous phoneme. 
For cerebral information, the symbol represents, in 
each zone, the neuronal subpopulation being sensitive to 

(i.e. that fires for) the corresponding category. It may be, 
in the primary auditory cortex, the subpopulation sensitive 
to a specific frequency interval. A categorical field 
is a set of symbols describing stimuli of the same semantic 
class. The "color" categorical field contains all the 
color symbols, but it cannot contain phonemes. 
A type concerns several symbols, due to the presence 
of noise or because of some compound information. Let 
S be the set of all existing symbols. We assume that a 
type T is defined for only one categorical field. Let S1 

be the subset of S, corresponding to this categorical 
field. The type T is an application from ST to [0,1], with 

the property , i.e. it describes a symbol repartition 
for a specific categorical field. In a stimulus, this 
repartition corresponds to the relative importance of each 
symbol compounding the information carried by the 
stimulus. Inside the model, T(s) stands for the proportion 
of s-sensitive neurons in the population that fired for the 
information whose type is T. Unlike the magnitude, the 

type is not represented by a random variable. Indeed, it is 
not necessary to represent its uncertainty (and hence to 
make the computational complexity harder) since we 
cannot compare it to neuroimaging data. 

At time / and node X, the information is represented by 
the type

 
and the magnitude 
at the output of X. 
Propagation and Processing 
For a zone X, both the cerebral propagation mechanisms 
(i.e. the relationships towards the zone) and the 
processing (spatial and temporal integration of the inputs, 
and processing as such) are described by a pair of functions, 
the type and the magnitude functions fM . In the general case where n zones are inputs to X, let be the corresponding delays of these relationships. In the DBN, the general form of the magnitude functions is: wherabl 
The random varimodels uncertainty in the cerebral processing. 

The type function is any combination of the incoming 
types and of the previous type that respects our type definition. 
If all types are defined on the same categorical 
field 5, the type function can be the linear combination: 


The functions' definition, as well as the setting of the 
parameters1 values (e.g. the value of a firing threshold), 
utilize mostly results in neuropsychology or in neurophysiology. 
The existence of generic models, that is, non 
instantiated, reusable, models of functional networks, is 
assumed. For example, primary cortices may implement 
the same mechanisms, although they arc parameterized so 
that they can process different types of stimuli [Pastor, et 
al., 2000]. 

4 Example 

The model, presented hereafter, is based on an experimental 
study [Ruff, et al, 2001] that focused on the differences 
between normal and dyslexic subjects during a 
passive phonemic categorization process. 

Six patients and six controls were submitted to a passive 
hearing of stimuli that are mixes of the two phonetically 
close syllables /pa/ and /ta/. The pivot is noted devO 
and the deviants are 4 different mixes of /pa/ and /ta/, 
noted dev2M, dev1M, dev1P, dev2P (Table 1). The measurements 
were made with fMRI. An experiment is constituted 
of 5 blocks, corresponding to the pivot and the de


viants. Each block contains 6 sequences of 4 sounds, 3 
pivots and the block's deviant, in a random order. 

We focus on a single region, a part of the right temporal 
superior gyrus involved in the early processing of 
auditory stimuli and activated differently in controls and 
dyslexic subjects. Phylogeny is in favor of the existence 
of specialized phonemic processors in this area (Figure 
2). Since their location is unknown, they cannot constitute 
separate structural nodes. They are supposed to have 
the same building functional nodes. According to our 
genericity hypothesis, the processors' structure and parameters 
are based on a previously released visual cortex 
model [Pastor, et al., 2000]. The Input Gating Nodes 
(IGN.) express the phoneme processors' sensitivity to the 
stimulus. The Output Gating Nodes (OGN.) send information 
to the downstream areas. Intra and inter (lateral) 
inhibitions (/TV. and LIN.) are assumed between the /pa/ 
and /ta/ processors. LIN. make the activation of an IGN. 
cause an inhibition in the opposite IGN.. Each Firing 
Threshold Node (FTN.) is modulated by an OGN. that 
can lower it. Since only one activation measure is provided 
by fMRI for the area, it is represented by the sole 
AN node in the static model. Stim stands for the stimulus. 


Figure 2: Static network used to model the cerebral phonemic 
categorization process. 

Except for the parameterization of the IGN. nodes, 
which reflects the specialization of each phonemic processor 
to the phoneme category (/pa/ or /ta/), the functions 
for both the /pa/ and /ta/ parts share exactly the same 
structure and parameters. Thus, only the pa part will be 
presented. In the following equations, the ith parameter of 

 (1) 
the function of a node X is noted aThe refractory period of the processor's neurons is 
modeled in that makes the node sensitive to the incoming stimulus only if the magnitude of the output is already close to zero: 


The categorical field contains two symbols (pa and ta). 
The type of a stimulus represents the proportions of the 
two symbols (Table 1). 
 The sigmoid magnitude function allows it to fire only if 
the magnitude coming from the is greater than the firin threshold's one: 
AN consists in the sum of the successive IGNs' activations 
during one experimental block: 


(8) 
This is a gross approximation of the fMRI data, which 
models only the part of the information processing mechanisms 
in the activation building and neglects metabolic 
processes at the level of the cerebral blood flow. Since, except 
the Stim and the AN nodes, all nodes represent neuronal 
activities, the time unit is set to 1 ms. We used the 
DD2 algorithm [Norgaard, ei al., 2000] to perform the simulations. 


The hypothesis is that the difference of processing between 
normal and dyslexic subjects is caused by a disorder 
in the inhibitory mechanisms. Thus, the two models, one for 
the average patient and the other for the average control, use 
the same functions and share the same parameters, except 
for the inhibition nodes (IN. and LIN.). There are no lateral 
inhibitions in the dyslexic model. It can be interpreted in 
cognitive terms as the fact that all the processors compete 
for each stimulus and that no clear category can be built. 
Also, the dyslexic model's internal inhibitions are slightly 
stronger than in the normal one, leading to a slowing in the 
stimulus perception. These two tentative interpretations are 
good starting points for new experiments. 

The differences in the inhibition parameters are sufficient 
to obtain very different activation data. For con


trols, the more distant (from the pivotal stimulus, categorically 
speaking) the deviant is, the stronger the activation 
is (Figure 3). This is supposed to be caused by a habituation 
mechanism that lowers the activation, followed 
by an activation the force of which depends on the "surprise" 
caused by the deviant. Dyslexic subjects do not 
correctly categorize the different phonemes, both the pa 
and the ta parts of the gyrus activate for each block. This 
illustrates how activation data can be explained thanks to 
the understanding of the cerebral information processing 
mechanisms expressed in the models. 


Figure 3: Compared results between simulated data (\A1\C0 2 standard 
deviations) and experimental measures. 

5 Conclusion 

Instead of building a specialized model, designed for a 
specific function or cerebral network, we have presented 
a general framework, allowing the interpretation of functional 
neuroimaging data. This framework has been designed 
to be open to evolutions of the knowledge in neuropsychology 
and neurophysiology. Using DBNs allows 
modeling the brain as a dynamic causal probabilistic 
network with nonlinear relationships. We have illustrated 
this with an example concerning a language-related process. 
Currently, our framework is adapted to automatic 
processing, which is dominant in cerebral functioning. In 
function of the stimulus type, nodes can react differently 
and different networks may be activated, thus implementing 
different functions. Our future work will focus on the 
integration of more biological plausibility in the framework. 
The representation of complex relationships between 
and inside the zones will allow the representation 
of controlled processes and contextual modulation of the 
cerebral activity. The combination of types from different 
categorical domains and the search for regularities in the 
combinations will allow the implementation of learning 
mechanisms. Another essential topic is to make our models independent of the used data acquisition technique, 
thanks to interface models, able to translate cerebral information 
processing variables into neuroimaging results. 
Our long-term goal is to progressively include in our 
framework various validated models and to build a consistent 
and general brain theory based on large-scale networks. 

Abstract 

This paper presents a method for analyzing human-
robot interaetion by body movements. Future 
intelligent robots will communicate with 
humans and perform physical and communicative 
tasks to participate in daily life. A human-like 
body will provide an abundance of non-verbal 
information and enable us to smoothly communicate 
with the robot. To achieve this, we have 
developed a humanoid robot that autonomously 
interacts with humans by speaking and making 
gestures. It is used as a testbed for studying embodied 
communication. Our strategy is to analyze 
human-robot interaction in terms of body movements 
using a motion capturing system, which 
allows us to measure the body movements in detail. 
We have performed experiments to compare 
the body movements with subjective impressions 
of the robot. The results reveal the importance of 
well-coordinated behaviors and suggest a new 
analytical approach to human-robot interaction. 

1 Introduction 

Over the past several years, many humanoid robots such as 
Honda's [Hirai et aL, 1998] have been developed. We 
believe that in the not-too-distant future humanoid robots 
will interact with humans in our daily life. Their human-
like bodies enable humans to intuitively understand 
their gestures and cause people to unconsciously behave as 
if they were communicating with humans [Kanda et al, 
2002a]. That is, if a humanoid robot effectively uses its 
body, people will naturally communicate with it. This 
could allow robots to perform communicative tasks in 
human society such as route guides. 

Several researchers have investigated about social relationships 
between humans and robots. For example, 
Kismet was developed for studying early caregiver-infant 
interaction [Breazeal, 2001]. Also, a robot that stands in a 
line [Nakauchi et al., 2002] and a robot that talks with 
multiple persons [Nakadai et al. , 2001] have been devel


* This research was supported in part by the Telecommunications 
Advancement Organization of Japan. 
oped. Furthermore, various communicative behaviors 
using a robot's body have been discovered, such as a 
joint-attention mechanism [Scassellati et al., 2000]. 

On the other hand, methods of analyzing social robots, 
especially with respect to human-robot interaction, are 
still lacking. To effectively develop any systems in general, 
it is essential to measure the systems' performance. For 
example, algorithms arc compared with respect to time 
and memory, and mechanical systems are evaluated by 
speed and accuracy. Without analyzing current performance, 
we cannot argue advantages and problems. For social 
robots, no analysis method has yet been established, 
thus, it is vital to determine what types of measurements 
we can apply. Although questionnaire-based methods have 
been often used, they are rather subjective, static and 
obtrusive (that is, we would interrupt the interaction when 
we apply a questionnaire). Less commonly, human behaviors 
are employed for this purpose, such as distance 
[Hall, 1966], attitude [Reeves and Nass, 1996], eye gaze 
(often used in psychology), and synchronized behaviors 
[Ono et aL, 2001]. Although those methods are more difficult 
to apply, the results are more objective and dynamic. 
Sometimes, interactive systems observe human behaviors 
for synthesizing behaviors [Jebara and Pentland, 1999]. 
However, they are still fragments rather than a systematic 
analysis method applicable for human-robot interaction. 

In this paper, we present our exploratory analysis of 
human-robot interaction. Our approach is to measure the 
body movement interaction between a humanoid robot and 
humans, and compare the results with traditional subjective 
evaluation. We have developed an interactive humanoid 
robot that has a human-like body as the testbed of this 
embodied communication. Furthermore, many interactive 
behaviors have been implemented. It encourages people to 
treat the robot as a human child. We employ a motion capturing 
system for measuring time and space accurately. 

2 An Interactive Humanoid Robot 

2.1 Hardware 
Figures 1 and 2 display an interactive humanoid robot 
"Robovie," which is characterized by its human-like body 
expression and various sensors. The human-like body consists of eyes, a head and arms, which generate the 
complex body movements required for communication. 
The various sensors, such as auditory, tactile, ultrasonic, 
and visual, enable it to behave autonomously and to interact 
with humans. Furthermore, the robot satisfies mechanical 
requirements of autonomy. It includes all computational 
resources needed for processing the sensory 
data and for generating behaviors. It can continually operate 
for four hours with its battery power supply. 

2.2 Software 
Using its body and sensors, the robot performs diverse 
interactive behaviors with humans. Each behavior is gen-
crated by a situated module, each of which consists of 
communicative units. This implementation is based on a 
constructive approach [Kanda et al, 2002b]: "combining 
as many simple behavior modules (situated modules) as 
possible." We believe that the complexity of the relations 
among appropriate behaviors enriches the interaction and 
creates perceived intelligence of the robot. 

Communicative Unit 
Previous works in cognitive science and psychology have 
highlighted the importance of eye contact and arm 
movement in communication. Communicative units are 
designed based on such knowledge to effectively use the 
robot's body, and each unit is a sensory-motor unit that 
realizes certain communicative behavior. For example, we 
have implemented "eye contact," "nod," "positional relationship," 
"joint attention (gaze and point at object)." 
When developers create a situated module, they combine 
the communicative units at first. Then, they supplement it 
with other sensory-motor units such as utterances and 
positional movements for particular interactive behaviors. 

Situated Modules 
In linguistics, an adjacency pair is a well-known term for a 
unit of conversation where the first expression of a pair 
requires the second expression to be of a certain type. For 
example, "greeting and response" and "question and answer" 
arc considered pairs. We assume that embodied 
communication is materialized with a similar principle: 
the action-reaction pair. This involves certain pairs of 
actions and reactions that also include non-verbal expressions. 
The continuation of the pairs forms the communication 
between humans and a robot. 
Although the action and reaction happen equally, the 
recognition ability provided by current computer science is 
not as powerful as that of humans. Thus, the robot takes the 
initiative and acts rather than reacting to humans actions. 
This allows the flow of communication to be maintained. 
Each situated module is designed to realize a certain action-
reaction pair in a particular situation (Fig. 3), where a 
robot mainly takes an action and recognizes the humans' 
reaction. Since it produces a particular situation by itself, it 
can recognize humans' complex reactions under limited 
conditions; that is, it expects the human's reaction. This policy 
enables developers to easily implement many situated 
modules. On the other hand, when a human takes an action 


toward the robot, it recognizes the human's action and reacts 
by using reactive transition and reactive modules; that is, 
some of the situated modules can catch the human's initiating 
behaviors and interrupt its operations to react to them (as 
shown in Fig. 4: the second module TURN). 

A situated module consists of precondition, indication, 
and recognition parts (Fig. 3). By executing the precondition, 
the robot checks whether the situated module is in 
an executable situation. For example, the situated module 
that performs a handshake is executable when a human is 
in front of the robot. By executing the indication part, the 
robot interacts with humans. With the handshake module, the 
robot says "Let's shake hands," and offers its hand. This behavior 
is implemented by combining communicative units of 
eye contact and positional relationships (it orients its body toward 
the human), and by supplementing a particular utterance 
("Let's shake hands") and a particular body movement (offering 
its hand). The recognition part is designed to recognize 
several expected human reactions toward the robot's action. 
As for the handshake module, it can detect human handshake 
behavior if a human touches its offered hand. 

The robot system sequentially executes situated modules 
(Fig. 4). At the end of the current situated module 
execution, it records the recognition result obtained by the 
recognition part, and progresses to the next executable 
situated module. The next module is determined by the 
results and the execution history of previous situated 
modules, which is similar to a state transition model. 
2.3 Realized Interactive Behaviors 
We installed this mechanism on "Robovic." The robot's 
task is to perform daily communication as children do. The 
number of developed situated modules has reached a 
hundred: 70 of which arc interactive behaviors such as 
handshake (Fig. 2, upper-left), hugging (Fig. 2, upper-
right), playing paper-scissors-rock (Fig. 2, lower-left), 
exercising (Fig. 2, lower-right), greeting, kissing, singing 
a song, short conversation, and pointing to an object in the 
surroundings; 20 are idling behaviors such as scratching 
its head, and folding its arms; and 10 are moving-around 
behaviors, such as pretending to patrol an area and going 
to watch an object in the surroundings. 
Basically, the transition among the situated modules is 
implemented as follows: it sometimes asks humans for 
interaction by saying "Let's play, touch me," and exhibits 
idling and moving-around behaviors until a human acts in 
response; once a human reacts to the robot (touches or 
speaks), it starts and continues the friendly behaviors 
while the human reacts to these; when the human stops 
reacting, it stops the friendly behaviors, says "good bye" 
and re-starts its idling or moving-around behaviors. 

3 Body Movement Analysis 

3.1 Experiment Settings 
We performed an experiment to investigate the interaction 
of body movements between the developed robot and a 
human. We used 26 university students (19 men and 7 
women) as our subjects. Their average age was 19.9. First, 
they were shown examples how to use the robot, then they 
freely observed the robot for ten minutes in a rectangular 
room 7.5 m by 10 m. As described in section 2.3, the robot 
autonomously tries to interact with subjects. At the beginning 
of the free observation, the robot asks subjects to 
talk and play together, and then subjects usually start 
touching and talking. 
After the experiment, subjects answered a questionnaire 
about their subjective evaluations of the robot with five 
adjective pairs shown in Table 1, which was compared 
with the body movements. We chose these adjective pairs 
because they had high loadings as evaluation factors for an 
interactive robot in a previous study [Kanda et al., 2002a]. 

3.2 Measurement of Body Movements 
We employed an optical motion capturing system to 
measure the body movements. The motion capturing sys


tem consisted of 12 pairs of infrared cameras and infrared 
lights and markers that reflect infrared signals. These 
cameras were set around the room. The system calculates 
each marker's 3-D position from all camera images. The 
system has high resolution in both time (120 Hz) and space 
(accuracy is 1 mm in the room) 

As shown in Fig. 5, we attached ten markers to the heads 
(subjects wore a cap attached with markers), shoulders, 
necks, elbows, and wrists of both the robot and the subjects. 
By attaching markers to corresponding places on the 
robot and subjects, we could analyze the interaction of 
body movements. The three markers on the subjects' head 
detect the individual height, facing direction, and potential 
eye contact with the robot. The markers on the shoulders 
and neck are used to calculate the distance between the 
robot and subjects, and distance moved by them. The 
markers on the arms provide hand movement information 
(the relative positions of hands from the body) and the 
duration of synchronized movements (the period where 
the movements of hands of the subject and robot highly 
correlate). We also analyzed touching behaviors via an 
internal log of the robot's touch sensors. 

3.3 Results 
Comparison between the body movements and the subjective 
evaluations indicates meaningful correlation. From 
the experimental results, well-coordinated behaviors such 
as eye contact and synchronized arm movements proved to 
be important. This suggests that humans make evaluations 
based on their body movements. 

Subjective Evaluation: "Evaluation Score" 
The semantic differential method is applied to obtain subjective evaluations with a l-to-7 scale, where 7 denotes the most positive point on the scale. Since we chose the adjective pairs that had high loadings as evaluation factors for an interactive robot, the results of all adjective pairs 
represent subjective evaluation of the robot. Thus, we 
calculated the evaluation score as the average of all adjective-
pairs' scores. Table 1 indicates the adjective pairs 
used, the averages, and standard deviations. 

Correlation between Body Movements and Sub


jective Impressions 
Table 2 displays the measured body movements. Regarding 
eye contact, the average time was 328 seconds, 
which is more than half of the experiment time. Since the 
robot's eye height was 1.13 m and the average of subject 

In the left figure, white circles indicate the attached markers, and the circles 
in the right figure indicate the observed position of the markers 

eye height was 1.55 m, which was less than their average 
standing eye height of 1.64 m, several subjects sat down or 
stooped to bring their eyes to the same height as the robot's. 
The distance moved was farther than what we expected, 
and it seemed that subjects were always moving littlc-
by-little. For example, when the robot turned, the 
subjects would then correspondingly turn around the robot. 
Some subjects performed arm movements synchronized 
with the robot's behaviors, such as exercising. 

Next, we calculated the correlation between the 
evaluation score and the body movements (Table 3). Since 
the number of subjects is 26, each correlation value whose 
absolute value is larger than 0.3297 is significant. We 
highlight these significant values with bold face in the 
table. From the calculated results, we found that eye 
contact and synchronized movements indicated higher 
significant correlations with the evaluation score. 

According to the correlations among body movements, 
the following items showed significant correlations: eye 
contact - distance, eye contact - distance moved, synchronized 
behaviors - distance moved by hands, and 
synchronized behaviors - touch. However, these items 
(distance, distance moved, distance moved by hands, and 
touch) do not significantly correlate with the evaluation 
score. That is, only the well-coordinated behaviors correlate 
with the subjective evaluation. Isolated active body 
movements of subjects, such as standing near the robot, 
moving their hands energetically, and touching the robot 
repetitively, do not correlate to the subjective evaluation. 

Estimation of Momentary Evaluation: ''En


trainment Score" 
The results indicate that there are correlations between 
subjective evaluation and body movements. We performed 
multiple linear regression analysis to estimate the evaluation 
score from the body movements, which confirms the 
above analysis and reveals how much each body movement 
affects the evaluation. We then applied the relations 
among body movements to estimate a momentary evaluation 
score called the entrainment score. 


As a result of the multiple linear regression analysis, 
standardized partial regression coefficients were obtained, 

where DIST, EC, EH, DM, DMH, SM, and TOUCH are the 
standardized values of the experimental results for the 
body movements. Since the evaluation was scored on a 
l-to-7 scale, evaluation score E is between 1 and 7. The 
multiple correlation coefficient is 0.77, thus 59% of the 
evaluation score is explained by the regression. The validity 
of the regression is proved by analysis of variance 

(F(7,18) = 3.71, P<0.05). 

The coefficients (Table 4) also indicate the importance 
of well-coordinated behaviors. Eye contact and synchronized 
movements positively affected the evaluation score; 
on the contrary, distance, distance moved and touch seem 
to have negatively affected the evaluation score. In other 
words, the subjects who just actively did something 
(standing near the robot, moved around, and touched repeatedly), 
especially without cooperative behaviors, did 
not evaluate the robot highly. 

Because we can momentarily observe all terms involved 
in the body movements of the regression (l), we can estimate 
a momentary evaluation score by using the same 
relations among body movements as follows: 


where designations such as DIST(t) are the momentary 
values of the body movements at time /. We named this 
momentary evaluation score the "entrainment score " with 
the idea that the robot entrains humans into interaction 
through its body movements and humans move their body 
according to their current evaluation of the robot. The 

evaluation score and entrainment score satisfy the following 
equation, which represents our hypothesis that the 
evaluation forms during the interaction occurring through 
the exchange of body movements: 


(3) 
Let us show the validity of the estimation by examining 
the obtained entrainment score. Figure 6 shows the entrainmcnt 
scores of two subjects. The horizontal axis indicates 
the time from start to end (600 seconds) of the 
experiments. The solid line indicates the entrainment score 
E(t), while the colored region indicates the average of the 
entrainment score Eft) from the start to time t (this integration 
value grows the estimation of E at the end time). 

The upper graph shows the score of the subject who 
interacted with the robot very well. She reported after the 
experiment that, "It seems that the robot really looked at 
me because of its eye motion. I nearly regard the robot as a 
human child that has an innocent personality." This entrainment-
score graph hovers around 5 and sometimes 
goes higher. This is because she talked to the robot while 
maintaining eye contact. She performed synchronized 
movements corresponding to the robot's exercising behaviors, 
which caused the high value around 200 sec. 

At the other extreme, the lower graph is for the subject 
who became embarrassed and had difficulty in interacting 
with the robot. The graph sometimes falls below 0. In 
particular, at the end of the experiment, it became unstable 
and even lower. He covered the robot's eye camera, 
touched it like he was irritated, and went away from the 
robot. We consider that those two examples suggest the 
validity of the entrainment score estimation. 

Evaluation of the implemented behaviors 
In the sections above, we explained the analysis of body 
movement interaction. Here, we evaluate the implemented 
behaviors. Although the application of this result is limited 
to our approach, our findings also prove the validity and 
applicability of the entrainment score. 
We calculated the evaluation score of each situated 
module based on the average of the entrainment score 
while each module was being executed. Tables 5 and 6 
indicate the worst and best five modules, respectively, and 
their scores. The worst modules were not so interactive. 
SLEEP_POSE and FULLY_FED do not respond to human 


Figure 6: Illustration of entrainment score 

(upper: subject who treated the robot as if it were a humans child, lower: 

subject who was embarrassed by interacting with it) 

action and exhibit behavior similar to the sleeping pose. 
NOTTUR N is the behavior for brushing off a human's 
hand while saying "I'm busy" when someone touches on 
its shoulder. The best modules were rather interactive 
modules that entrain humans into the interaction. 
EXERCISE and CONDUCTOR produce the exercising 
and imitating of musical conductor behaviors, which induced 
human synchronized body movements. Other 
highly rated modules also produce attractive behaviors, 
such asking and calling, which induce human reactions. 
We believe that the entrainment scores provide plenty of 
information for developing interactive behaviors of robots 
that communicate with humans. 

4 Discussions 

The experiment reveals the correlation between humans' 
subjective evaluations and body movements. If a human 

evaluates the robot highly, then the human behaves cooperatively 
with it, which will further improve its evaluation. 
That is, once they establish cooperative relationships 
with the robot, they interact well with the robot and 
evaluate the robot highly. Regarding evaluation of the 
implemented behaviors, the modules that entrain humans 
into interaction were highly evaluated, such as asking 
something that induces human's answer and producing 
cheerful body movements like exercising to let humans 
join and mimic the movements. We believe that the entraiment 
can help us to establish cooperative relationships 
between humans and robots. 

Meanwhile, the multiple linear regression explains 59% 
of the subjective evaluation. This is remarkable because it 
is performed without regard to the contents or context of 
language communication. With speech recognition, the 
robot can talk with humans, although its ability is similar 
to that of a little child. Some of the subjects spoke to the 
robot. Often, there were requests for the robot to present 
particular behaviors (especially behaviors it had performed 
just previously), to which it sometimes responded 
correctly and sometimes incorrectly. To analyze this, we 
could use several analytical methods such as conversation 
analysis, however, these methods are rather subjective. On 
the other hand, our evaluation employed objective measures 
only: numerically obtained body movements without 
context, which means there could be a lot of potential 
usages. For example, an interactive robot could learn and 
adjust its behavior by using this method. It would be applicable 
to different subjects (age, culture, etc.), different 
agents (physical-virtual, body shape, behaviors, etc.), and 
inter-human communication. 

5 Conclusions 

This paper reported a new approach to analyzing embodied 
communication between humans and a robot. Our 
interactive humanoid robot is able to autonomously interact 
with humans. This complexity and autonomy is 
achieved by many simple behaviors. We measured humans' 
body movements while they observed and interacted 
with the robot, and the result of the analysis indicates 
positive correlations between cooperative body movements 
and subjective evaluations. Furthermore, the multiple 
linear regression explains 59% of the subjective 
evaluation without regard to language communication. We 
consider our approach of body movement analysis to be 
widely applicable in embodied communication. 

Abstract 

This paper proposes a unique map learning method 
for mobile robots based on the co-visibility information 
of objects i.e., the information on whether 
two objects are visible at the same time or not 
from the current position. This method first estimates 
empirical distances among the objects using 
a simple heuristics - "a pair of objects observed at 
the same time more frequently is likely to be located 
more closely together". Then it computes all 
the coordinates of the objects by multidimensional 
scaling (MDS) technique. In the latter part of this 
paper, it is shown that the proposed method is able 
to learn qualitatively very accurate maps though it 
uses only such primitive information, and that it is 
robust against some kinds of object recognition errors. 


1 Introduction 

Map learning problem of autonomous mobile robots has 
been a central issue in the field of artificial intelligence as 
well as robotics, because it contains several important aspects 
of intelligence such as recognition of environment and 
acquisition of internal representations. In fact, a variety 
of map building methods have been developed for a wide 
range of robots, tasks and environments so far. These methods 
are traditionally classified [Kortenkamp et a/., 1998; 
Murphy, 2000] in terms of the way of map representation 
into metric map building methods[Moravec and Elfes, 1985; 
Uhlmann et al, 1997], topological methods[Mataric, 1992; 
Zimmer, 1996; Shatkay and Kaelbling, 1997], and hybrids of 
them[Thrun et al., 1998]. 

On the other hand, when we turn our attention to a new 
trend of robot navigation called qualitative navigation[Levitt 
and Lawton, 1990; Schlieder, 1993], we can see there is another 
way of qualitative map representation that is different 
from both the metric and topological representations. A most 
important point of the maps used in this qualitative navigation 
is that they are not required to be accurate in a metric sense as 
long as they correctly preserve the qualitative spatial relationships 
(such as circular ordering) of the objects or landmarks 
in the actual environment. 

A challenging problem in this paradigm is to construct autonomously 
such qualitative maps from qualitative observation 
information robots obtain. A representative approach to 
this qualitative map learning problem is [Sogo et al, 2001], 
in which qualitative information of "how object positions are 
classified to two sets with respect to arbitrary straight lines" 
is used to construct a map by propagating "three point" constraints. 


In this paper, we also propose a map learning method for 
mobile robots based on qualitative observation information. 
It uses the information of "co-visibility" or whether two objects 
are visible or not at the same time from robot's positions, 
which is much more primitive than the information used in 
[Sogo et al, 2001] and the ordering information[Schlieder, 
1993]. 

In this method, co-visibility of two objects is translated into 
an empirical distance based on a simple heuristics "closely located 
objects are likely to be seen simultaneously more often 
than distant objects and vice versa" or "temporal and spatial 
proximities are approximately equivalent". Then, positions 
of all objects are calculated by well-known multidimensional 
scaling (MDS) technique. 

A noteworthy feature of this method from a practical viewpoint 
is that it does not require robots localize their own positions 
while mapping. Moreover, it is shown that it can learn 
qualitatively accurate maps without higher level infonmation 
such as ordering, and is robust against observation errors. We 
also discuss the validity of the heuristics above from several 
viewpoints. 

2 Problem Definition 

2.1 Assumptions on Environment and Robot 
we consider a map building task by a mobile robot, in which 
the robot estimates the positions of objects in the environment 
by repeated explorations and observations (Figure 1). 
More specifically, we make the following assumptions about 
the environment and robot. 

Environment 
The environment is a closed area containing a finite number 
of objects. Each object is assigned a unique ID. In addition, 
it is assumed that all objects are about the same size. 

Environment 

Figure 1: Assumed map building task of a mobile robot (exploration, 
observation and map estimation) 


Observation 
The robot obtains a panoramic camera image at each observation 
point, and extracts a list of recognized or visible objects 
by some image processing and recognition technique. These 
lists of visible objects are accumulated over time and used to 
build a map later. 


Exploration 
The robot explores the environment with some randomness, 
avoiding collisions with objects. 


2.2 Evaluation Criterion for Qualitative Maps 
Though a constructed map is represented in the form of numeric 
coordinates of the objects on 2-D plane as ordinary 
metric maps, its goodness is measured by the correctness of 
the qualitative spatial relationships rather than by the metric 
accuracy. 

To evaluate the qualitative correctness of the obtained 
maps, we employ the notion of triangle 
orientation[Schlieder, 1993; Sogo et al., 2001] or counterclockwise 
order of three points. In short, triangle orientation 
of three points

 in 2-D plane is defined as + when 
the order

 
yields a counter-clockwise turn, and \A1\AA 
otherwise (Figure 2). 
When there are N objects in the environment, the number 
of all possible triangles formed by them becomes 

So we define the orientation error of a constructed map 
as the percentage of triangles with wrong orientations, com-


We use this as an evaluation criterion of constructed 
maps in the later simulation study. 

2.3 Co-visibility and Distance Between Objects 
As previously mentioned, we use a heuristics "a pair of objects 
observed simultaneously more frequently is likely to be 
located more closely together" to estimate the distance between 
them. Though it is hard to prove the validity of this 
heuristics strictly in general cases, we consider it is approximately 
appropriate for the following reason. 
First, we assume that an object becomes difficult to identify 
as the distance from the robot increases, because the image 
size of the object becomes smaller and the chance of occlusion 
increases. 
Given this assumption, the total area size of the region 
where the robot can observe two objects simultaneously decreases 
monotonically according to the distance between the 
objects. For example, consider the case the probability that 
an object can be observed from the robot at a distance of 
and 
r is 
(Figure 3). If two object A and B are located 
respectively, the probability that both 
objects are visible from an arbitrary robot position X (x,y) 
becomes 


Then, the'expectcd size of area where both objects are visible 
simultaneously is 


In the simplest case that the robot is located at any place in 
the environment with equal probability, the probability that 
the two objects are co-visible is expected be proportional to 
Scovis{l), which is a monotonic decreasing function of/. 

In section 4, we will discuss this "co-visibility and distance" 
heuristics in a more general form: "temporal and spatial 
proximities are approximately equivalent". 

3 Proposed Method 

3.1 Outline of CoviMap 
Based on the assumptions above, we propose CoviMap -a 
map learning method based on the co-visibility of objects. 
In this method, the mobile robot obtains approximate positions 
of objects based on the co-visibility frequencies of them, 
which are updated repeatedly by explorations and observations. 
The outline of CoviMap is described as below: 


1. 
The robot repeats the following behavior steps and updates 
the number of observations of each object (nt), and 
the number of simultaneous observations of every pair of 
objects 
(a) The robot moves to the next observation position, 
avoiding collisions with the objects. Figure 3: Relationship between co-visibility and distance of 
two objects in a simplified environment 

(b) It obtains 
a list of visible objects Lo from the 
panoramic image captured at the current position, 
and updates ni and nly) as bellow: 
2. 
After a specified number of steps, co-visibility frequencies 
is computed for each pair of objects based on 
Then empirical distance 
of each 
pair is computed from 

3. The robot obtains the estimated positions of all objects 
by applying Multi-Dimensional Scaling 

(MDS) to the distance matrix D whose (i, j) element is In the remaining of this section, we explain the second and 
third parts in the above procedure. 

3.2 Computation of Co-visibility Frequency and 
Empirical Distance 
Co-visibility frequency

 
between two objects is defined as 
follows: 
fi,j stands for the conditional probability that two objects are 
visible at the same time, given that at least one of them is 
visible. It takes a value between 0 and 1. This definition of 


is also known as Jaccard's coefficient. 

With the definition of co-visibility frequency 

the 
heuristics introduced in 2.3 can be rewritten as "distance between 
two objects

 
monotonically decreases as 
increases". 
Figure 4 (scattered points) illustrates the actual relationship 
between the real (squared) distance and co-visibility 
frequency

 
in the simulation environment in section 4. The 
result indicates that the heuristics is approximately valid. 
Therefore, we introduce a notion of empirical (squared) 
distance

 between an arbitrary pair of objects, which is 
defined by some monotonic decreasing non-negative function 
i.e., 


Figure 4: Relationship between squared distance and co-visibility frequency fij of objects in the simulation environment 


We also define an empirical distance matrix D whose (i, j) 
element equals to D is a symmetric matrix and its diagonal components equal to zero. 
A possible solution to the problem of deciding a suitable 
combination of function and parameter values for is to select 
the one which minimizes the stress value of the MDS results 
described later. Furthermore, if no appropriate empirical 
distance functions are provided, non-metric MDS (or ordinal 
MDS) can be used instead. 

Another problem is that cannot be computed properly 
for the pairs whose co-visibility frequencies are zero, because 
becomes some constant value. In this case, CoviMap utilizes 
the triangular inequality constraint (TIC) to correct the 
value of as follows: 


3.3 Map Construction Based on 
Multi-Dimensional Scaling (MDS) 
Multi-dimensional scaling (MDS)[Young and Householder, 
1938] is a multivariate data analysis technique used to visualize 
a potentially high-dimensional data structure by mapping 
it into a relatively low dimensional space[Cox and Cox, 
2001]. The purpose of MDS is to find an optimal configuration 
of objects in a low-dimensional space, when dissimilarities 
or distances among them are given. 

While there are several kinds of MDS methods[Cox and 
Cox, 2001], CoviMap employs classical scaling[Young and 
Householder, 1938] method that is a kind of metric MDS to 
reproduce a map of objects in 2-D plane from the set of empirical 
distances. In brief, it obtains a set of 2-D coordinates of 
N objects xt (i = 1, . ., N) by applying Young-Householder 
transformation and spectral decomposition to the empirical 
distance matrix D. 

A non-trivial problem of using MDS in CoviMap is that 
there is a fifty percent chance of obtaining the mirror image 
of the expected map. However, if any correct triangle orientation 
of three objects is given, CoviMap can detect the mirror 
map and get the right map by turning it over. 

Figure 5: Empirical distance functions used in the simulation 


4 A Simulation Study 
We conducted a simulation study to examine the effectiveness 
of our method with Cyberbotics1 WEBOTS simulator 
(ver.2.0). 


4.1 Settings 
Environment and Objects 
The environment is a square field of 1.5[m] x 1.5[mJ containing 
5 - 30 objects. Each object is cylinder-shaped (height: 
160[mm], diameter: 48[mm]), and is given a unique ID number 
like obO,obJ, 


Observation and Object Recognition 
The robot has a camera with a resolution of 160 x 120 pixels, 
and obtains a panoramic image by rotating and capturing 
several images at each observation position. We make a simplified 
assumption that the robot can recognize an object if its 
image is wider than 10 pixels. 


Exploration Strategy 
At each observation position, the robot chooses its next moving 
direction randomly within the range of 
and proceeds


 
in the direction. In addition, 
the robot has 8 proximity sensors to detect nearby obstacles 
(objects and walls) around it, and avoids collisions with them. 


Empirical Distance Function 
We chose a logarithmic function as the empirical distance 


functio 
where are parameters which take positive values. In 
this simulation, we set them as: 
because the stress values of MDS were relatively small with 
this combination compared with others. Figure 5 shows 
the shape of this empirical distance function. In addition, 
triangular inequality constraint (TIC) mentioned in 3.2 was 
used to correct the empirical distances for the object pairs 
whose co-visibility frequencies were zero. 

4.2 Experiment 1 : Varying Number of Objects 
First, we examined the basic map building capabilities of 
CoviMap with varying number of objects in the environment 


Numbers of Steps 

Figure 6: (Experiment 1) Change of average map errors 
comparison among various numbers of objects 


Table 1: Change of average map errors [%] with 6 different 
numbers of objects 

from 5 to 30. In each case, the robot repeated the observation 
and exploration steps for 1000 times, and built a map 
eachtime the number of steps reached one of specified numbers 
(20,50,100,...,1000). For each case, we prepared 5 layouts 
of objects which arc randomly generated and averaged 
the orientation errors. 

Figure 6 and Table 1 show how the average orientation error


 
changes according to the number of steps for each 
number of objects. As can be seen from this, 

converges 
to a narrow range (approximately between 2.5[%] and 
3.0[%]) after several hundreds of steps, when the number of 
objects varies from 10 to 30. This means that CoviMap is not 
affected drastically by the increase in the number of objects. 

Figure 7 illustrates an example of real configuration which 
contains 30 objects, while Figure 8 illustrates a map constructed 
by CoviMap after 500 steps. In this case, out of total 
4060 triangles, the number of triangles whose orientations are 
inconsistent between the real map and the constructed map is 
98

 Interestingly, if we consider only the 
triangles whose largest angles are smaller than 170[deg] in 
the real map (the number of such triangles is 3835), the number 
of inconsistent triangles falls to 27 
This means our method is able to acquire very accurate qualitative 
layouts of the objects, unless the objects are almost on 
a line. Dotted lines in the figures represent Delaunay graphs 
of the configurations. A comparison of them also tells that 
the constructed map is qualitatively very accurate. 

Figure 7: (Experiment 1) An example of object configuration 
with 30 objects, dotted lines are edges of Delaunay graph 


Figure 8: (Experiment 1) Constructed map after 500 steps 
for all triangles) 

4.3 Experiment 2 : Robustness Against Object 
Recognition Errors 
In the previous experiment, it was assumed that there are no 
errors in the object recognition process. In the real environment, 
however, it is almost inevitable to fail in recognizing 
objects occasionally, due to various uncertainties. Therefore, 
we examined how the recognition errors affect the quality of 
maps constructed by CoviMap. 

More specifically, we considered two kinds of recognition 
errors -non-recognition and mis-recognition. These errors 
were artificially generated as follows: 

Non-recognition : Randomly chosen elements in Lo are removed 


Mis-recognition : Randomly chosen elements in L0 are replaced 
with other object IDs. 

In this experiment, we fixed the number of objects to 15 
and conducted 5 trials for each of 5 different object configurations. 


Result with Non-recognition Error 
Figure 9 shows the average orientation errors of the constructed 
maps when the percentage of non-recognition error 
is set to 0.0 (no errors), 3.0, 10, 20[%] respectively. In 
an early stage of learning, i.e, with fewer steps, the map 
error

 becomes larger according to the magnitude of 
the non-recognition error. However, the difference becomes 


Figure 9: (Experiment 2) Change of average map errors 
comparison among various non-recognition rates 


Figure 10: (Experiment 2) Change of average map errors 
comparison of various mis-recognition rates 

smaller and almost negligible as the number of steps increases. 


Result with Mis-recognition Error 
Next, Figure 10 shows the results when mis-recognition error 
is set to 0.0, 0.5, 2, 10[%] respectively. Not surprisingly, 
the influence of this kind of error on the map error Errori is 
relatively larger than the non-recognition error. 
In both cases, however, we can conclude CoviMap is robust 
against those recognition errors, in that the map error steadily 
decreases according to the increase of the number of steps. 

5 Related Works 

As mentioned before, the central idea of our map building 
method is a simple heuristics - "a pair of objects observed 
simultaneously more frequently is likely to be located more 
closely", which is used to estimate the distances and qualitative 
configuration of objects from the co-visibility information 
of them. In this section, we discuss the meaning of this 
heuristics from other viewpoints, especially, considering several 
related works in other fields. 

First, as co-visibility of objects can be regarded as cooccurrence 
of events that objects are visible, a lot of studies 
on learning of behavior and models based on the cooccurrence 
information have been made. The most basic 
principle related to this issue is Hebb's rule which postulates 
that if two events co-occur often, the connection of 
them should be strengthened. Many recently developed rein


forcement learning methods such as Q-learning[Watkins and 
Dayan, 1992] are also considered to be based on the cooccurrence 
or temporal proximity between behavior and reward 
events. Interestingly, there is another similarity between 
reinforcement learning and our CoviMap. That is to say, the 
former learns a complicated behavior system from discrete 
events of rewards, while the latter learns a qualitatively accurate 
map from discrete events of co-visibility among objects. 

The heuristics above can also be regarded as a special 
case of more general one - "temporal and spatial proximities 
are approximately equivalent in many environments". 
In the research of human spatial memory, it has been reported 
that humans often rely on the temporal proximity 
rather than the spatial proximity to memorize spatial structures 
of environments[Curiel and Radvansky, 1998]. This 
might suggest that the principles and results of CoviMap presented 
in this paper are helpful to understand some aspects 
of human spatial memory and cognitive mapping process, although 
we doubt that an MDS-like algorithm exists in human 
brain. 

6 Conclusion 

In this paper, we proposed a unique map building method 
for mobile robots named CoviMap which is based on the co-
visibility information of objects in the environment. It can 
learn qualitatively accurate maps from such primitive information 
using a simple heuristics "temporal and spatial proximities 
are approximately equivalent" and multidimensional 
scaling (MDS) technique. It was also shown by the simulation 
results that CoviMap is applicable to large environments 
containing dozens of objects, and robust against observation 
errors such as non-recognition and mis-recognition. 

A most important theoretical issue to be studied is to validate 
and generalize the heuristics on the equivalence of temporal 
and spatial proximities. Especially, we need to investigate 
how our method will be affected if the camera image is 
not panoramic but more restricted, or the object recognition 
rate is not isotropic but dependent on the direction. On the 
other hand, future works from a practical point of view will 
include experiment with real robots, integration of CoviMap 
with qualitative navigation[Levitt and Lawton, 1990], and extension 
or replaccment [Faloutsos and Lin, 1995] of MDS. 

Abstract 

We present an algorithm Pref-AC that limits arc 
consistency (AC) to the preferred choices of a tree 
search procedure and that makes constraint solving 
more efficient without changing the pruning 
and shape of the search tree. Arc consistency thus 
becomes more scalable and usable for many real-
world constraint satisfaction problems such as configuration 
and scheduling. Moreover, Pref-AC directly 
computes a preferred solution for tree-like 
constraint satisfaction problems. 

1 Introduction 
In the last two decades, considerable research effort in Al 
has been spent on studying the complexity of reasoning and 
problem solving, and the identification of tractable cases has 
become an important goal of this research. However, even 
if a problem can be solved by a polynomial algorithm, or if 
search effort can be reduced by polynomial algorithms, this 
might not be sufficient for real-world AI systems that face 
challenges such as interactivity and real-time. We give some 
examples from constraint satisfaction, where achieving arc 
consistency might be too costly, although it is tractable: 

1. Web-based configuration systems must provide a response 
in a few seconds while serving multiple users. 
Although arc consistency (AC) is an appropriate technique 
to handle compatibility constraints, it will be too 
expensive in presence of large product catalogs. 
2. 
Constraint-based approaches to problems such as 
scheduling, vehicle routing, and personnel planning often 
only maintain bound consistency in constraints involving 
time or other variables with large domains. As 
a consequence, complex rules on breaks, rest-times, and 
days-off provide only a poor propagation, which could 
be improved by arc consistency. 
These examples illustrate that domain reduction does not develop 
its full power in many real-world applications of constraint 
programming. Pruning power and, indirectly, solution 
quality are traded against short response times. In certain circumstances, 
such a trade-off can be avoided if the system focuses 
on the right deductions and computations. In this paper, 


we argue that preferences are one possible way for achieving 
this. As stated by Jon Doyle [Doyle, 2002], preferences 
can play different roles and need not only represent desiderata 
or user preferences. Preferences can also control reasoning 
meaning that only preferred inferences and domain reductions 
are made. Non-interesting deductions are left apart, but 
may become preferred if additional information is added. 

We can easily apply this idea to typical backtrack tree 
search algorithms that maintain arc consistency when solving 
a constraint satisfaction problem. The crucial question is 
whether such an algorithm needs to achieve full arc consistency 
and to remove all values that are not in the maximal 
arc-consistent domains. In [Schiex el al, 1996], it has been 
shown that the fails of the algorithm can be preserved if it 
simply constructs some arc-consistent domains, thus reducing 
the computation effort in average. In this paper, we show 
that the search algorithm can preserve its search strategy and 
the overall pruning if 1. the search algorithm uses a variable 
and value ordering heuristics that can be described by 
(static) preference ordering between variables and values and 

2. we construct some arc-consistent domains that contain the 
preferred values. The first condition is, for example, met by 
configuration problems [Junker and Mailharro, 2003]. The 
second condition needs some careful adaption of the definition 
of arc consistency, which will be elaborated in this paper. 
We can thus cut down the propagation effort in each search 
node without changing the search tree, which leads to an overall 
gain if the search tree will not be explored completely. 
This happens if we are interested in one solution, the best solution 
in a given time frame, or preferred solutions as defined 
in [Junker, 2002]. Arc consistency thus becomes more scalable 
and less dependent on the size of domains, which opens 
exciting perspectives for large-scale constraint programming. 

We first introduce preferences (Section 2), use them to define 
a directed constraint graph (Section 3), and discuss arc 
consistency for preferred values (Sections 4 and 5). After 
introducing preferred supports (Section 6), we present the algorithm 
Pref-AC (Section 7). 

2 Preferred Solutions 
In this paper, we consider constraint satisfaction problems of 
the following form. Let X be a set of variables and V be a set 
of values. A constraint c of arity kc has a sequence of variables 
X(c) = {x1 (r),..., Xkt (c)} from X and a relation Rc 
that is a set of tuples from

 
. Let C be a set of constraints. 
The triple (X',D, C) is then called a constraint network. 

A solution of a constraint network is a mapping v of the 
variables X to the values in V such that each constraint is satisfied, 
i.e.,for all constraints c. The constraint satisfaction problem (CSP) involves finding a solution of a constraint network. 
Alternatively, we can represent the tuples by sets of assignments 
of the form x = v between a variable :/: and a value v: 
if t is a tuple of the then let f be the 
set of assignments. Furthermore, let Rc be the set of all /. such that t is in Rc. This representation will facilitate definitions and permits a straightforward logical characterization of a constraint c: 

We further suppose that the following kinds of preferences 
are given. For each variable x, we introduce a strict partial order among the possible values for x. This order 
can represent user preferences as occurring in configuration 
problems (cf. [Junker and Mailharro, 2003]). For example, 
the user might prefer a red car to a white car. Projecting preferences 
on criteria to decision variables [Junker, 2002] also 
produces such an order (e.g. if price is minimized then try 
cheaper choices first). Furthermore, we consider a strict partial 
order

 
between variables. For example, we 
may state that choosing a color is more important than choosing 
a seat material and should therefore be done first. In the 
sequel, we suppose that the search procedure follows these 
preferences as described in [Junker and Mailharro, 2003] and 
always chooses a variable and value for x. These preferences are given statically and thus correspond to a static variable and value ordering heuristics, which are usually sufficient for configuration problems. 

We now use the preferences to define a preferred solution. 
First of all we choose a linearization of the orders 
and in form of total orders that are super that corresponds to the sets of the strict partial orders. Then we consider a ranking 
We then consider two solutions v1 and v2 and compare them lexicographically 

(2) 
Definition 1 A solution v of a constraint network P := 
(X,D,C) is a preferred solution ofV iff there exist linearizations is the best solution of P w.r.t. the lexicographical order defined by 


These preferred solutions correspond to the extreme solutions 
in [Junker, 20021 that are obtained if all variables are 
criteria. If the search procedure follows the preferences then 
its first solution is a preferred solution. Hence, the notion of 
a preferred solution helps us to forecast the first solution in 
certain cases. 


Figure 1: Directed constraint graphs. 

3 Preference-based Constraint Graph 

In order to find a preferred solution, the search procedure 
chooses values for more important variables first. We will 
see later that, in some cases, we can anticipate a preferred solution 
and construct it by a constraint propagation procedure 
if this procedure does not only follow the constraints, but also 
the order among the variables used by the search procedure. 

In order to illustrate this, consider the bipartite constraint 
graph that connects the constraints with their variables. Suppose 
that a variable x is connected via a constraint c to a less 
important variable y. Search will then assign a value to x 
and cause a domain reduction of y. We can thus say that the 
preferences among variables impose an order on the constraint 
graph. For example, we will say that the arc between 
x and c leads from x to c and the arc between y and c leads 
from c to y. 

We generalize this idea for arbitrary constraints and partial 
orders. For each constraint r, we consider the -<.v-best 
variables in X(c) and call them input variables. The other 
variables are called output variables. Arcs lead from input 
variables to a constraint and from there to the output variables. 


Definition 2 Let (X,D,C) be a constraint network and 
be a strict partial order between the variables X. A variable 
x of a constraint c is called input variable of c iff there is 
no other variable y of c s.t.x. A variable x of c is 
called output variable of c iff it is not an input variable of c. 
The directed constraint graph of the CSP is a bipartite graph 
having the nodes and the set of arcs (x, c) s.t. x is an 
input variable of c and (c, y) s.t. y is an output variable of c. 
Each constraint can have several input and several output 
variables (cf. figure 1). If its variables are not comparable at 
all, then the constraint has no output variable, and it is a sink 
of the directed graph. However, each constraint has at least 
one input variable. As a consequence, no constraint can be a 
source of the directed graph. In general, the graph is not guaranteed 
to be acyclic, but it satisfies the following properties 
which are needed for our proofs: 1. All variables of the set X are sources of the constraint graph since they cannot 
be an output variable of any constraint. However there can be 
sources that are not variables. 2. If x is a ranking of 
the variables X that is a linearization of then each output variable of each constraint is preceded by at least one input 
variable of the same constraint in the ranking. 

4 Arc Consistency 

A typical systematic search procedure for solving CSPs proceeds 
as follows. In each search node, it picks an assignment 
x \A1\AA v for such a variable and creates two successor nodes, 
one for x = v and one for

 
Throughout the paper, 
we assume that a variable x is eliminated from the CSP of a 
successor node by a preprocessing step iff it has only a single 
possible value. Usually, the search procedure will not select 
an arbitrary assignment from the set A of all assignments, 
but tries to anticipate certain cases where the sub-tree of an 
assignment does not contain a solution. If such an assignment 
is detected it is eliminated from A. Hence, the search 
procedure maintains a set

 
of possible assignments and 
considers only elements of A. Each solution can be characterized 
by a subset of A that assigns exactly one value to each 
variable and that satisfies all constraints. As a consequence, 
if a variable x has no assignment in A then no solution exists 
and the search will backtrack. In the sequel, we write Ax for 
the set

 of possible values for x in A. 
Assignments can be eliminated from A if they lack a support 
on a constraint: 


Let be the set of supports for x = v in c. In this paper, 
we are interested in search procedures that maintain arc 
consistency, i.e. that work with a set A where all assignments 
have supports S that are subsets of A and this on all relevant 
constraints: 

Definition 4 Let A be a set of assignments. A subset A of A 
is an arc-consistent set of A iff I. Ad is non empty for any and all constraints c of x, there exists a support S for (x = v) in c such that S is a subset of A. 

If an arc-consistent set A contains a single assignment for 
each variable then A corresponds to a single solution. If no 
arc-consistent set exists then the CSP has no solution. Otherwise, 
there is a unique maximal arc-consistent set, which is a 
superset of all arc-consistent sets, including the solutions. 

Alternatively, we can consider a set of some assignments 
for which we know that they do not belong to any solution: 
(3) 
This can easily be translated to a reflexive and monotonic operator that eliminates additional assignments: 


Fixed-point theory then tells us that the transitive closure of 
this operator is the unique minimal set satisfying equation 
3. As a consequence, the set A* of all assignments that are 
not eliminated (i.e., 
is the unique maximal set 
satisfying item 2 of def. 4. If each variable has an assignment 
in A* then A* is the maximal arc-consistent assignment. If 
some variable has no assignment in A* no arc-consistent assignment 
exists. In this case, no solution exists. 


Figure 2: Activating best values and their supports. 

5 Arc Consistency for Preferred Values 

Maintaining arc consistency prunes the search tree as follows: 

PI if the current CSP is not arc-consistent then a fail occurs. 

P2 if a value v is not arc-consistent for a variable x then the 
search procedure will not assign v to x inside the current 
subtree. 

In general, it is desirable to eliminate as many values as possible 
and to determine A* by applying an algorithm of the 
AC-family. The worst-case complexity is

 for 
binary constraints, if domains contain thousands or more elements 
as is the case for typical scheduling problems and for 
configuration problems with large catalogs, then maintaining 
arc consistency is no longer feasible in practice. 
Interestingly, it is not necessary to eliminate all assignments 
of A - A*. [Schiex et al., 1996] try to construct a 
small arc-consistent set A. If this succeeds then prunning rule 
PI cannot be triggered. This can lead to saving time in search 
nodes that are arc-consistent. We now want to extend this 
policy to pruning rule P2 in the case that the original search 
procedure uses the given preferences between variables and 
values in order to select an assignment. In each search state, 
the search procedure will pick variable and try out 
its possible values starting with the best value 
is not in the maximal arc-consistent set A* then we must ensure 
that it will be eliminated when constructing the subset A. 
However, if

 
then we must ensure that it will 
be in the subset A. 

Instead of doing a pure elimination process, we interleave 
construction and elimination steps and determine a set F of 
activated assignments, called focus, and a set

 
of eliminated 
assignments. Both sets will grow during the process and our 
purpose is to obtain 


as arc-consistent set. This set 
is sufficient for our search procedure if some properties are 
satisfied: 1. If x is a 


variable and 
among the non-eliminated values of x then the search procedure 
might pick x \A1\AA v as next assignment. Hence, we have to 
activate it. 2. If an activated assignment does not have a support 
on a constraint c then it has to be eliminated. 3. The supports 
for the activated, but non-eliminated assignments must 
only contain activated and non-eliminated elements. 4. We 
need to guarantee that

 
contains at least one value per 
variable if an arc-consistent set exists. Since all variables can 
be reached from the sources via constraints, we activate best 
values for all sources of the directed constraint graph. 

Figure 4: A preferred solution as stable activation. 

Given a preferred solution, we cannot guarantee that each 
assignment has a preferred support in all constraints. However, 
if certain assignments in a solution S have preferred 
supports then S is preferred solution: 

Proposition 5 Let S be a solution of V. Suppose that 
and all

 
are strict total orders. If each assignment x \A1\AA v 
to a source x of the direct constraint graph is a best assignment 
for x in the unique maximal arc-consistent set A* and 
all activated assignments to input variables of a constraint 
c have a preferred support on c in S then S is a preferred 
solution of P. 
When constructing a stable activation, we choose preferred 
supports since this may result in a preferred solution: 

Proposition 6 Suppose that and all are strict total 
orders. Let S1,..., Sm be an activation sequence and let 
be defined as in def 6. If J. each S1, is a preferred 
support for some x = v on some constraint c s.t. x is an input 
variableand 2. 
preferred solution of P. 

The first condition in proposition 6 can be satisfied by always 
activating preferred supports. The second condition can 
be met by CSPs of a special structure. For example, consider 
a binary constraint network such that its directed constraint 
graph forms a tree (cf. figure 4). In this case, we only activate 
supports for assignments to input variables, but not for 
assignments to output variables, since each variable can only 
be an output variable of a single constraint. This result is, 
for example, interesting for certain car configuration problems 
and permits us to efficiently take into account certain 
user preferences. 

Preferred solutions are also obtained in trivial cases: 

Proposition 7 Suppose that and all are strict total 
orders. If there is a preferred solution containing all best 
values of all variables in A then it is equal to all activations 
produced by preferred supports. 
Algorithm Pref-AC 

In this Section, we present Pref-AC, an algorithm for computing 
an arc-consistent set A from a binary constraint network 
[X,D,C). This algorithm activates supports for unsupported 
elements and meets the requirements of definitions 5 and 6, 
and proposition 6. An algorithm for non-binary constraints 
can be derived from this one. Pref-AC follows the principles 
of lazy arc consistency [Schiex et al, 1996] in order to 

= 
build an arc-consistent set of assignments that is not necessarily 
maximal. We base Pref-AC on AC-6 [Bessiere, 1994] 
for keeping the discussion simple, although we could exploit 
the bi-directionality of supports [Bessiere et al., 1999]. 

AC-6 assigns an ordering of the values in the domain of 
every variable x1, checks one support (the first one or smallest 
one with respect to the ordering) for each assignment 
(xi = a) on each constraint

 
to prove that (x1 = a) 
is currently viable. When (xj = b) is found as the smallest 
support of (xi \A1\AA a) on c{xiXj), (xi = a) is added to 

the list of assignments currently having (xj = b) 
as smallest support. If b is removed from the domain of Xj 
then AC-6 looks for the next support in the domain of xj for 
each assignment 

Pref-AC uses the domain order

 
, a chosen linearization 
of 
when looking for supports in the domain of a variable 
xi. Furthermore, Pref-AC seeks supports only for elements 
of A, the current set of activated and non-eliminated values 
(i.e., 
uses following data structure: 

. 
Each variable x, has a fixed initial domain Dx, ordered 
by 
containing all values from V except those 
that violate unary constraints on xi. The set 
contains the values of that have not yet 
been removed by arc consistency. 
A is the arc-consistent set built by the algorithm. 
. 
For each contains the assignments that are currently supported by (xi = a). 

The set Pending contains all the 4-tuples (xia,Xj,b) 
such that a support for (xt = a) has to be sought on 
r(xi,xj). 
this means that all values of Xj 
better than b have already been removed. 

Pref-AC uses two subprocedures (Algorithm 1). Procedure 
Activate(xia) adds an assignment (xi = a) to A, the 
set of current activated values. It initializes its data structure 
CS, and puts in Pending all the information needed to look 
for supports for (x, = a) on the constraints involving xt. 
Function BestSup(x,, a, xj , b, B) looks for the best support 
for (xi = a) in D which is less preferred than b (we know 
there is no support in B better than or equal to b). B is 
or AXj depending on the status of xi (input variable or not). 

Pref-AC works as follows. We start by initializing to 
DX, for each x1 (line 1 of Algorithm 2). We activate the best 
value of the best variable (line 2). Then, 4-tuples (x1, a, xj, b) 
are taken from the Pending set (line 4), and if (x1 = a) is still 
active (line 5), we must seek a (new) support c for it. If x1 is 
an input variable of

 
we seek the support in 
because 
we have to ensure that it will be a preferred support (lines 6 to 
7). Otherwise, we first seek a support c among the activated, 
but non-eliminated elements (line 8). If none exists, we seek 
a new best support c (line 10) following def. 6. If c exists, we 
activate it if not yet done, and store the fact that it supports 
(x1 = a) (lines 11 to 13). If no support exists for (xt \A1\AA a), 
we remove it from and A (line 14). If

 is empty, a 
wipe out stops the procedure (line 15). If xi is a source of the 
directed constraint graph and the best value of is not in was the best of AX, and , the best value is the preferred support for this assignment on this constraint 
if the variable was an input variable for the constraint 
(because of line 7 and the way BestSup works). We know 
A contains at least an assignment per variable because of the 
activation of supports. Therefore, A is an arc-consistent set. 
And because of lines 16-18, we know that for each Xi which is a source of The space complexity of Pref-AC is the same as AC-6, 
namely since in the worst case, all the values 
are activated and have a support stored on each constraint. 
The time complexity is also bounded above by that of AC-6, 
namelysinc e for each value in each domain, we 
perform at most constraint checks on each constraint. 

8 Conclusion 

We have shown that it is sufficient to maintain an arc-
consistent set for the preferred choices of a search procedure. 
This can significantly speed up constraint solving if the variable 
and value ordering heuristics of the search procedure can 
be described by preferences as is the case for typical configuration 
problems. We developed an algorithm called Pref-AC 
for achieving this reduction and are currently testing it for 
configuration problems with large domains. 

Many real-world applications of constraint programming 
suffer from insufficient propagation, since most approaches 
support only bound consistency for crucial constraints such 
as precedences of activities, resources, and rest-time rules. 
Adapting algorithm Pref-AC to those constraints provides an 
interesting future perspective for improving constraint solving 
in areas such as scheduling, personnel planning, and other 
logistics applications. Future work will be devoted to improve 
Pref-AC such that it directly finds a preferred solution 
if the problem structure permits this (e.g. by activating preferred 
supports that are common to several variables). 

Acknowledgements 

We would like to thank Jean-Charles Regin and Olivier 
Lhomme for very helpful discussions. 

Abstract 

In [Jegou, 1993], a decomposition method has been 
introduced for improving search efficiency in the 
area of Constraint Satisfaction Problems. This 
method is based on properties of micro-structure of 
CSPs related to properties of triangulated graphs. 
This decomposition allows to transform an instance 
of CSP in a collection of sub-problems easier to 
solve, and then gives a natural and efficient way for 
a parallel implementation [Habbas et al, 2000]. 
In this paper, we present a generalization of this approach, 
which is based on a generalization of triangulated 
graphs. This generalization allows to define 
the level of decomposition which can be fixed 
by a graph parameter. The larger this parameter is, 
the more level of decomposition that is the number 
of sub-problems is. As a consequence, we can 
then define the level of decomposition, with respect 
to the nature of the parallel configuration used (the 
number of processors). 

First experiments reported here show that this extension 
increases significantly the advantage of the 
basic decomposition, already shown in [Habbas et 
al, 2000]. 

1 Introduction 

Constraint-satisfaction problems (CSPs) involve the assignment 
of values to variables which are subject to a set of constraints. 
Examples of CSPs are map coloring, conjunctive 
queries in a relational databases, line drawings understanding, 
pattern matching in production rules systems, combinatorial 
puzzles... CSP is known to be a NP-complete problem. 
So, during last twenty years, many works have been proposed 
to optimize the classical Backtrack procedure, as constraint 
propagation, intelligent backtracking or network decompositions. 


In [Jegou, 1993], another approach has been proposed 
which is based on the decomposition of the "micro-structure" 
of the CSP. The micro-structure of a CSP is the graph defined by the compatible relations between variable-value pairs: vertices 
are these pairs, and edges arc defined by pairs of compatible 
vertices. Given the micro-structure of a CSP and using 
graph properties, the method realizes a pre-processing to 
simplify the problem with a decomposition of the domains of 
variables. For a CSP V, the decomposition generates a collection 
of subproblems P1, P2, .. . Pk, which is equivalent to 
the initial problem V. Each subproblem Pi has the size of 
its domains less or equal to the size of domains of P, so the 
time complexity of each Pi is necessarily less than for the 
initial CSP. Moreover, given this collection of subproblems, 
we can separately solve them, or recursively apply again the 
decomposition. 

This decomposition allows to transform an instance of CSP 
in a collection of subproblems the domains size of which is 
less than domains size of the CSP. So, these sub-problems are 
theoretically easier to solve. Moreover, this decomposition 
defines and then gives a natural and efficient way for a parallel 
implementation to solve CSP. In [Habbas et al, 2000], an 
experimental analysis of this approach is presented, realized 
on a parallel configuration. These experiments show clearly 
the efficiency of the approach: the decomposition generally 
offers better results than the results obtained to solving the 
problems without decomposition. This advantage is related 
to the natural parallelization of the approach since each subproblem 
is solved independently. This fact is clear for consistent 
CSPs. Moreover, if we consider inconsistent CSPs, the 
results are more insteresting: without parallelization, solving 
the problem after its decomposition is clearly better as solving 
it without decomposition. 

In this paper, we present a generalization of this approach. 
This approach is based on a generalization of triangulated 
graphs. This generalization allows to define the level of decomposition 
which can be fixed by a graph parameter. The 
larger this parameter is, the more level of decomposition (that 
is the number of sub-problems) is. As a consequence, we can 
then define the level of decomposition, with respect to the 
power of the parallel configuration used. 

To define this generalization, we exploit a generalization of 
triangulated graphs called CSGk graphs which has been introduced 
in [Chmeiss and Jegou, 1997]. This class of graphs 
possesses the same kind of properties as triangulated graphs 
do. The most important here is the fact that the CLIQUE 
problem has a polynomial time complexity on these graphs. 

First experiments reported here show that this extension 
increases significantly the advantage of the basic decomposition, 
already shown in [Habbas et al, 2000]. Indeed, we 
present experiments with two levels of decomposition. The 
basic decomposition, and the second level (CSG2 graphs). 
The results show that for the second level of decomposition 
we obtain better results. 

The section 2 recalls the decomposition method while section 
3 presents the generalization of the decomposition. Experimental 
results and a discussion about these result are presented 
in the section 4. 

2 Preliminaries 

2.1 Constraint Satisfaction Problems 
A finite CSP (Constraint Satisfaction Problem) is defined as 
four tuple P = {X,D,C,R). X is a set of n variables 
x1,X2,--.xn. D is a set of finite domains D1,D2, ..... Dn, 
and C is a set of rn constraints. Here we only consider binary 
CSPs, that is the constraints are defined on pairs of variables 
{xi, Xj } and will be denoted Ci,j. To each constraint , we 
associate a subset of the cartesian product (Di x Dj) which 
is denoted Rij and specifies which values of the variables are 
compatible with each other. R is the set of all RlJ. A solution 
is an assignment of values to variables which satisfies all the 
constraints. For a CSP P, the pair (X',C) is a graph called 
the constraint graph. Given a CSP, the problem is either to 
know if there exists any solution, to find solution, or to find 
all solutions. The first problem is known be to NP-complete. 

CSPs are normally solved by different versions of backtrack 
search. In this case, if d is the size of domains (maximum 
number of values in domains A) , the theoretical time 
complexity of search is then bounded by the size of the search 
space, that is dn. Consequently, many works tried to improve 
the search efficiency. At present time, it seems that the most 
efficient algorithm is the procedure MAC which has been introduced 
in [Sabin and Freuder, 1994]. Other approaches use 
decomposition techniques based on structural properties of 
the CSP. These methods exploit the fact that the tractability 
of CSPs is intimately connected to the topological structure 
of their underlying constraint graph. Moreover, these methods 
give an upper bound to the complexity of the problem. 
As example of such decompositions, we have tree-clustering 
scheme [Dechter and Pearl, 1989]. The time complexity of 
this method is bounded by dk where k is induced by the topological 
features of the constraint graph. Intuitively, more the 
constraint graph is dense, more the value k is large. For example, 
if the constraint network is a complete graph, then 
k = n. So, the complexity of these decomposition methods 
is the same as for classical backtracking, that is dn. So, in 
[Jegou, 1993], Jegou proposed an alternative way to decompose 
CSPs which is not based on the constraint graph but on 
the micro-structure of a CSP. In the sequel, we called this decomposition 
scheme TR-Decomposition. 

2.2 TR-Decomposition 
Theorem 1 
In other words, transforming a CSP as its micro-structure 
is clearly a polynomial reduction from CSP to CLIQUE, 
which is the problem of finding a clique of a given size 
belongs to a graph. As a consequence of this fact, as CSP, 
the CLIQUE problem is NP-Complete. Since the problem 
of finding a n-cliquc is NP-hard, TR-Decomposition exploits 
the fact that triangulated graphs constitute a polynomial 
class of instances for this problem. We recall briefly basic 
properties of these graphs. Given a graph G \A1\AA (V, E) and an 
ordering 
simplicial in is a clique. The ordering 
of V, the successors of a vertex u2 
are the 
w 
isof V is a perfect elimination ordering if, 
, the vertex v1 is simplicial in the ordering 
A graph G = (V,E) is triangulated if and only if V 
admits a perfect elimination ordering. Triangulated graphs 
satisfies desirable properties. A triangulated graph on n 
vertices has at most n. maximal cliques (a clique is maximal 
iff it is not included in an other clique) [Fulkerson and 
Gross, 1965]. Moreover, the problem of finding all maximal 
cliques in a triangulated graph is in 0(n + m) if n is the 
number of vertices and m the number of egdes [Gavril, 1972]. 

Given the micro-structure of any CSP, it is not possible to 
immediately use these properties because any micro-structure 
is not necessary a triangulated graph. So, in adding edges 
in E, we can build a new graph T(G) = (V, Ef) which is 
triangulated. This addition of edges is called triangulation, 
and can be realized in a linear time in the size of the graph. 
So, after the triangulation, all maximal cliques of G can be 
found in linear time. Applied to a the triangulation of its micro-structure produces.

The steps 1, 2 and 3 can be realized in a linear time w.r.t. 
the size of the problem V. The procedure Triangulation returns 
a perfect elimination ordering a which allows to the 
procedure Maximal Cliques to find sets Yi. The step 4, that 
is the iteration for, can be replaced by a parallel execution 
on independent subproblems P(Yl). In this case, if the CSP 
V is consistent, the cost is related to the cost of solving the 
easiest subproblem, while it is the cost of the hardest subproblem 
for inconsistent CSPs. A such implementation has 
been realized in [Habbas et al, 2000]. In step 5, a set Yi is 
not a covering of all the domains in D if there is a subdomain 
Dy.j induced by Yi which is the empty set. This subproblem 
is then trivially inconsistent. In step 6, to solve V(Yi), we can 
use any classical search method such that standard backtracking, 
Forward-Checking or MAC [Sabin and Freuder, 1994]. 
Finally, note that the quality of decomposition depends on the 
quality of the triangulation step. 

The last point is the most important. Nevertheless, it's well 
known that the problem of finding an optimal (w.r.t. the size 
of the largest induced clique) triangulation is NP-Hard (see 
[Arnborg et al, 1987]). Several algorithms have been proposed 
for triangulation. In all cases, the aim is to minimize 
either the number of added edges or the size of cliques in 
G'. Three classes of approaches are usable. The first one 
consists in finding an optimal triangulation which produces a 
graph the maximum size clique of which has minimum size 
over all triangulations. [Shoikhet and Geiger, 1997] propose 
a such algorithm the time complexity of which is exponential. 
The second is to find a minimal triangulation, that is 
a triangulation computing a set such as if for any other E\
triangulationis not triangulated. Note that a triangulation can be minimal and not optimal. See [Berry, 1999] who presents an algorithm in 0(n(n + m)). The last one consist in using a triangulation consuming a linear time which does not gurantee the 
minimality. Nevertheless, the purpose of this paper is not to 
propose a good triangulation, but to exploit a generalization 
of triangulated graphs to generalize this decomposition. One 
of the aims of this generalization is to avoid the problem of 
finding optimal triangulations. 

3 Generalizing TR-Decomposition 

3.1 A first basic property 
To generalize TR-Decomposition, we can extend theand 
its 
a graph 
Consider 

 the set of maximal cliques of 
Every Zi is a set of values, and then, as in section 2.2, defines 
a subproblem of P which is the CSP induced by Z i, on V (it 
is denoted V(Zi)). 

Theorem 3 Given a binary CSP P, its micro-structure 
{Z 1,... Zj), the set of the maximal cliques of 
then Solutions 

Proof. Given a binary CSP V, its micro-structure 
whereeacmaxima 
clique 
cliqueand by theorem 1, we know that each solution of 
of is included in a 
V is a 
clique of size n. If we consider 

 
Therefore, each solution of 
V appears in the associated subproblem 

While TR-Decomposition is limited to triangulated graphs, 
theorem 3 allows to define a larger class of decomposition 
of domains. Nevertheless, because the number of maximal 
cliques can be exponential in an arbitrary graph, and then, the 
number of induced subproblems, we must consider graphs 

which possess a limited number of maximal cliques 
as triangulated graphs do. So, we exploit here a class of 
graphs which is a generalization of triangulated graphs. 

3.2 A generalization of triangulated graphs 
In [Chmeiss and Jegou, 1997], a new class of graphs, called 
CSGk graphs has been introduced. These graphs which 
generalize triangulated graphs, possess the same kind of 
properties as triangulated graphs, e.g. hereditary property of 
subgraphs, existence of an elimination scheme, polynomial 
time recognizing algorithm, and polynomial class of graph 
for CLIQUE problem. Informally, a CSG0 graph is a 
complete graph, a CSG1 graph is a triangulated graph, and a 
CSG2 graph is a graph allowing an elimination vertex scheme 
where the successors of every vertex induce triangulated 
graphs. More generally, CSGk graphs are defined inductively. 

Definition [Chmeiss and Jegou, 1997] The class of CSG0 
Graphs is the class of complete graphs. Given k > 0, the 
class of CSGk graphs is the class of graphs G = (V, E) such 
that there exists an ordering of V such that for i = l,2, ...n, the graph . 
The ordering a is then called a CSGk scheme. 

it's easy to see that CSG1 Graphs is the class of triangulated 
graphs. As for triangulated graphs, there exists a 
polynomial time algorithm for recognizing CSGk 
graphs; its time complexity is 
In the context of decomposition of micro-structure, the number 
of maximal cliques is an important criteria. CSGk graphs 
inherit of a generalization of the property of Fulkerson and 
Gross [Fulkerson and Gross, 1965] related to this number 
since, a Graphs with n vertices have less than nk 
maximal cliques. Moreover, a graph with n vertices 
and m edges has at most n + m maximal cliques. Finding 
these cliques is easy, since, given a constant value k the 
CLIQUE problem is polynomial on this class of graphs since 
the time complexity of finding the clique of maximum size in 
an undirected graph is bounded by 

3.3 TRk-Decomposition 
CSGk graphs can be exploited as a way to generalizing of TR-
Decomposition, defining TRk-Decomposition as decompositions 
corresponding to the class of associated graphs. For K= 
1, TRk-Decomposition is exactly TR-Decomposition, while 
for k= 2, TRk-Decomposition, that is TR2-Decomposition, 
it's the decomposition induced by CSG2 graphs. More generally, 
TRk-Decomposition is summarized as: 

Algorithm: TRk-Decomposition 
Begin 
1 
2 
3 
4 
5 
6 
7 

End; 

Note that: 

.Th e procedure k-Triangulation produces a graph 
which is a CSGk graph and returns an ordering 
a which allows to the procedure k-MaximalCliques 
to find all the maximal cliques 

. 
As for TR-Decomposition the step 4, that is the iteration 
for, can be replaced by a parallel execution on independent 
subproblems 
In this case, if the CSP V is 
consistent, the cost is related to the cost of solving the 
easiest subproblem, while it is the cost of the hardest 
subproblem for inconsistent CSPs. 

. 
For steps 5 and 6, remarks given for TR-Decomposition 
hold. 
. 
Finally, the quality of decomposition depends on the 
quality of the k-triangulation step. 
Because of the time complexity of managing graphs, 
that is it seems reasonable to focus our 
attention on small values of k;. So, in the sequel we limit our 
study to CSG2 graphs. Therefore, the number of maximal 
cliques will be bounded now by 

Nevertheless, 
two problems must be adressed here. Finding an 
efficient algorithm for 2-triangulation, and an operational algorithm  to find maximal cliques of a graph. 

For 2-triangulation, we can imagine an algorithm the time 
complexity of which is theoreticaly bounded by 0(nm(n. + 
m)). This algorithm consider an ordering on vertices which 
is obtained by the Maximum Cardinality Search of Tarjan and 
Yannakakis. Given this ordering, we consider vertices vi and 
then, we run a triangulation on . This first step is 
in 0(n(n + m)) but this approach does not guarantee to get 
a 2-triangulation. 
the triangulatio 
Indeed, given two vertices vl 
not triangulated. So, a second 
and Vj, with 
i < j, which is realized after 
the triangulatio add edges that restore 
the new subgraphstep of verification and repairing the subgraphs 
in adding new edges must be realized. This additionnal work 
induces a theoretical complexity of 0(nm(n + 777,)). Nevertheless, 
expermients show that this additionnal work will be 
rarely reached in practical cases (see section 4). 

4 Experiments 
Our aim in this section is to show the usefulness of the generalized 
decomposition method comparing to the MAC (Maintaining 
Arc Consistency) algorithm and the decomposition 
method by triangulation of the micro-structure and to give 
an idea about the efficiency of the proposed method. For the 
generalized decomposition method we will consider the TR2Decomposition, 
i.e. k = 2. 
As mentionned in section 3, we will not focus on the 
triangulation and 2-triangulation algorithms because it is 
not the principal objective of this paper. In experiments, we 
will use the Tarjan and Yannakakis's triangulation algorithm 
[Tarjan et al, 1984]. This algorithm requires a precomputed 
elimination ordering on the vertices. Then, it adds edges, 
with respect to the ordering, which maintain the chordality 
property (i.e. when a vertex is treated, its successors must 
form a clique). Also, for the 2-triangulation, we will use an 
algorithm based on the Tarjan and Yannakakis's triangulation 
one as described in section 3.3. In practice, we have observed 
that the additionnal step is rarely realized and then the 
observed time of the 2-triagnluation is in n(n +m) . So, for 
CSPs of great size, the CPU time used to decompose the 
problem is generally not significant with respect to the time 
ofsolving the CSP. 

We have expermiented these methods on random CSPs 
generated at the phase transition. These CSPs are generated 
according to the model B generator [Prosser, 1996]. Such a 
generator admits four parameters: 

. the number N of variables 
. the common size of the initial domains D 
. 
the proportion p1 of constraints in the network (or the 
number C = p1 * N . (N - l)/ 2 of constraints) 

Table 1: experimental results 

. the proportion p2 of forbidden pairs of values in a constraint 
(or the number T = p2 * D * D of forbidden 
pairs) 
In the following, we will use C and T instead of p1 and 
p2. C is called the density of the constraint graph and T the 
tightness of the constraints. We have analysed decomposition 
using the procedure MAC as basic search algorithm (the 
used heuristic is dom/deg [Bessiere and Regin, 1996]). That 
is MAC has been used before decomposition on the initial 
problem and after decomposition on each induced subproblem. 


In order to have a credible idea on the performance of these 
methods, we consider several classes of problems. We vary 
two parameters (the number of variables N and the domains 
size D). In order to deal with classes near the phase transition, 
we search the appropriate values of the tightness (T) and the 
density (C). 

For each value of N, we vary D from small domains size 
to larger ones. So, we can observe the behavior of the decomposition 
when D grows. Also, we have remarked that the 
performance of the decomposition method was not the same 
for satisfiable and unsatisfiablc problems. So, we have chosen 
to present the results seperately. 

Results are reported in three tables. Each table represents 
results for a given number of variables TV. As measures 
of comparison, we used the number of consistency checks 
(#checks) performed by each of the methods and the number 
of visited nodes (#nodes). We mention that, the execution 
time is proportional to the number of checks. 

In the tables, results are organized as following : the first 
column contains the calsses of problems represented by the 
four parameters < N,D,T, C >. For each class, we give 
results of the differents methods for satisfiable (Sat) and unsatisfiable 
(Unsat) problems seperately. So, we have two 
lines per class. The second and the third columns give the 
number of subproblems induced by the TR-Decomposition 
and the TR2-Decomposition respectively. The fourth column 
indicates the number of satisfiable and unsatisfiable problems 
over the 50 randomly generated CSPs. The next three 
columns give the number of consistency checks for MAC, 
TR-Decomposition and the TR2-Decomposition respectively. 
The last three columns present the number of visited nodes by 
the different methods. 

As the subproblems induced by the decomposition are independent, 
we can envisage a parallel implementation. In 
other words, we can imagine one process by subproblem. 
So, for saisfiable problems we consider the faster subproblem 
(the firstly solved subproblem since we have the response the 
it is satisfiable) while we consider the hardest subproblem for 
unsatisfiable ones. 

Tables show, that decomposition improve the MAC algorithm 
on all classes. The improvement is more significant 
for larger domains. Also, the performance of MAC is improved 
more significately for denser constraint graphs. If 
we look at table 1 where N = 20, we remark that the 
TR2-Decomposition is five times faster than MAC for the 
< 20,10,30,130 >. For classes with D = 20 the improvement 
is more significant. This remark is confirmed when 


the constraint graphe is denser. For example, for the class 

< 20,20,130,150 > and for satisfiable problems, the perfor


mance ratio between TR2-Decomposition and MAC is about 

1 tp,22. In other words, for each consistency check performed 

by TR2-Decomposition l'algorithm MAC performs 22 checks 

for satisfiable problems. For unsatisfiable problems, the per


formance ratio is about 1 to 12. 

This overall view on experiments shows that the generalized 
decomposition method is promising and results are encouraging. 
Finally, we would like to mention the importance 
of the triangulation (2-triangulation) phase. We think that the 
efficieny of the decomposition method is related to the quality 
of the triangulation (2-triangulation). We believe that other 
triangulation algorithms might offer a better decomposition. 

 Conclusion 

In this paper, we have presented a generalization of the decomposition 
of micro-structure, introduced in [Jegou, 1993]. 
This decomposition allows to transform an instance of CSP in 
a collection of sub-problems easier to solve, and then gives a 
natural and efficient way for a parallel implementation [Habbas 
et al, 2000]. While the original method is based on triangulated 
graphs, our generalization is based on a generalization 
of triangulated graphs called CSGk graphs. For a given 
value A:, we get a special sub-class of graphs. E.g. CSG\A1\E3 
graphs are complete graphs and CSG1 graphs are triangulated 
graphs. The value k allows to define a particular level of 
decomposition. So, we have introduced TRk-Decomposition 
as the generalized decomposition based on CSGk graphs. 

There are two motivations for this generalization of the decomposition: 


. to extend the level of decomposition, 
. 
to be able to fix this level to give an optimal exploitation 
for the parallel configuration used. 
Indeed, it has been experimentally observed in [Habbas et 
al., 2000] that the first level of decomposition, that is TR1Decomposition, 
outperforms classical algorithms, and that 
the more the number of sub-problems is, the more the efficiency 
of TR1-Decomposition is. 

Our experimental results confirm these facts. We have 
studied two levels of decomposition, for k = 1 and k = 2 and 
the experiments show that TR2-Decomposition outperforms 
TR1-Decomposition. Moreover, the number of sub-problems 
is greater for k \A1\AA 2 than for k = 1. 


Abstract 

Many real life optimization problems contain both 
hard and soft constraints, as well as qualitative conditional 
preferences. However, there is no single 
formalism to specify all three kinds of information. 
We therefore propose a framework, based 
on both CP-nets and soft constraints, that handles 
both hard and soft constraints as well as conditional 
preferences efficiently and uniformly. We study 
the complexity of testing the consistency of preference 
statements, and show how soft constraints 
can faithfully approximate the semantics of conditional 
preference statements whilst improving the 
computational complexity. 

 Introduction and Motivation 

Representing and reasoning about preferences is an area of 
increasing interest in theoretical and applied Al. In many 
real life problems, we have both hard and soft constraints, 
as well as qualitative conditional preferences. For example, 
in a product configuration problem, the producer may have 
hard and soft constraints, while the user has a set of conditional 
preferences. Until now, there has been no single 
formalism which allows all these different kinds of information 
to be specified efficiently and reasoned with effectively. 
For example, soft constraint solvers [Bistarelli et al, 1997; 
Schiex et al, 1995] are most suited for reasoning about the 
hard and soft constraints, while CP-nets [Boutilicr et al, 
1999] are most suited for representing qualitative conditional 
preference statements. In this paper, we exploit a connection 
between these two approaches, and define a framework 
based on both CP-nets and soft constraints which can efficiently 
handle both constraints and preferences. 

*This research was partially funded by AFOSR, grant F4962001-
1-0076 (Intelligent Information Systems Institute, Cornell University) 
and F49620-01-1-0361 (MURI grant on Cooperative Control 
of Distributed Autonomous Vehicles in Adversarial Environments), 
DARPA, F30602-00-2-0530 (Controlling Computational 
Cost: Structure, Phase Transitions and Randomization) and F3060200-
2-0558 (Confi guring Wireless Transmission and Decentralized 
Data Processing for Generic Sensor Networks), the Italian MIUR 
projects NAPOLI and COVER, the ASI (Italian Space Agency) 
project ARISCOM and Science Foundation Ireland. 

Soft constraints [Bistarelli et al, 1997; Schiex et al, 1995] 
are one of the main methods for dealing with preferences in 
constraint optimization. Each assignment to the variables of a 
constraint is annotated with a level of its desirability, and the 
desirability of a complete assignment is computed by a combination 
operator applied to the "local" preference values. 
Whilst soft constraints are very expressive, and have a powerful 
computational machinery, they are not good at modeling 
and solving the sort of conditional preference statements that 
occur in the real world. Moreover, soft constraints are based 
on quantitative measures of preference, which tends to make 
preference elicitation more difficult. 

Qualitative user preferences have been widely studied in 
decision-theoretic Al [Doyle and Thomason, 1999]. Of particular 
interest are CP-nets [Boutilier et al, 1999]. These 
model statements of qualitative and conditional preference 
such as "I prefer a red dress to a yellow dress", or "If the 
car is convertible, I prefer a soft top to a hard top". These 
are interpreted under the ceteris paribus (that is, "all else 
being equal") assumption. Preference elicitation in such 
a framework is intuitive, independent of the problem constraints, 
and suitable for naive users. However, the Achilles 
heel of CP-nets and other sophisticated qualitative preference 
models [Lang, 2002] is the complexity of reasoning with 
them [Domshlak and Brafman, 2002; Boutilier et al, 2002]. 

Motivated by a product configuration application [Sabin 
and Weigel, 1998], we have developed a framework to reason 
simultaneously about qualitative conditional preference statements 
and hard and soft constraints. In product configuration, 
the producer has hard (e.g., component compatibility) and 
soft (e.g., supply time) constraints, while the customer has 
preferences over the product features. We first investigate the 
complexity of reasoning about qualitative preference statements, 
addressing in particular preferential consistency. To 
tackle the complexity of preference reasoning, we then introduce 
two approximation schemes based on soft constraints. 

To the best of our knowledge, this work provides the first 
connection between the CP-nets and soft constraints machinery. 
In addition, for product configuration problems or any 
problem with both hard and soft quantitative constraints as 
well as qualitative conditional preferences, this framework 
lets us treat the three kinds of information in a unifying environment. 
Finally, we compare the two approximations in 
terms of both expressivity and complexity. 

2 Formalisms for Describing Preferences 

2.1 Soft constraints 
There are many formalisms for describing soft constraints. 
We use the c-semi-ring formalism [Bistarelli et al, 1997], 
which is equivalent to the valued-CSP formalism when total 
orders are used [Bistarelli et ai, 1996], as this generalizes 
many of the others. In brief, a soft constraint associates each 
instantiation of its variables with a value from a partially ordered 
set. We also supply operations for combining (x) and 

tion to an SCSP is a complete assignment to its variables. 
The preference value associated with a solution is obtained 
by multiplying the preference values of the projections of 
the solution to each constraint. One solution is better than 
another if its preference value is higher in the partial order. 
Finding an optimal solution for an SCSP is an NP-complete 
problem. On the other hand, given two solutions, checking 
whether one is preferable is easy: we compute the semi-ring 
values of the two solutions and compare the resulting values. 

2.2 CP-nets 
Soft constraints are the main tool for representing and reasoning 
about preferences in constraint satisfaction problems. 
However, they require the choice of a semi-ring value for each 
variable assignment in each constraint. They are therefore a 
quantitative method for expressing preferences. In many applications, 
it is more natural for users to express preferences 
via generic qualitative (usually partial) preference relations 
over variable assignments. For example, it is often more intuitive 
for the user to say "I prefer red wine to white wine", 
rather than "Red wine has preference 0.7 and white wine has 


Figure 1: The CP-net graph for the example. 

preference 0.4". Of course, the former statement provides 
less information, but it does not require careful selection of 
preference values to maintain consistency. Moreover, soft 
constraints do not naturally represent conditional preferences, 
as in "If they serve meat, then 1 prefer red wine to white 
wine". It is easy to see that both qualitative statements and 
conditions are essential ingredients in many applications. 

CP-nets [Boutilier et al, 1999] are a graphical model for 
compactly representing conditional and qualitative preference 
relations. They exploit conditional preferential independence 
by structuring a user's preferences under the ceteris 
paribus assumption. Informally, CP-nets are sets of conditional 
ceteris paribus (CP) preference statements. For instance, 
the statement "I prefer red wine to white wine if meat 
is served." asserts that, given two meals that differ only in 
the kind of wine served and both containing meat, the meal 
with a red wine is preferable to the meal with a white wine. 
Many philosophers (see [Hansson, 2001] for an overview) 
and AI researchers [Doyle and Wellman, 1994], have argued 
that most of our preferences are of this type. 

 Consistency and Satisfiability 

Given a set of preference statements

 
extracted from a user, 
we might be interested in testing consistency of the induced 
preference relation. In general, there is no single notion of 
preferential consistency [Hansson, 2001]. In [Boutilier et al.9 
1999], a CP-net N was considered consistent iff the partial ordering 
induced by N is asymmetric, i.e. there exist at least 
one total ordering of the outcomes consistent with 
. However, 
in many situations, we can ignore cycles in the preference 
relation, as long as these do not prevent a rational choice, 
i.e. there exist an outcome that is not dominated by any other 
outcome. In what follows, we refer to this as satisfiability^. It 
is easy to see that satisfiability is strictly weaker than asymmetry, 
and that asymmetry implies satisfiability. We will consider 
two cases: When the set 
of preference statements induces 
a CP-net and, more generally, when preferences can 
take any form (and may not induce a CP-net). 
When

 
defines an acyclic CP-net, the partial order induced 
by 
is asymmetric [Boutilier et al, 1999]. However, 
for cyclic CP-nets, asymmetry is not guaranteed. In 
the more general case, we are given a set 
of conditional 
preference statements without any guarantee that they define 
a CP-net. Let the dependence graph of

 be defined similarly 
to the graphs of CP-nets: the nodes stand for problem 
features, and a directed arc goes from

 iff 
contains a statement expressing preference on the values of 
Xj conditioned on the value of Xi. For example, the set 


does not induce a CP-
net (the two conditionals are not mutually exclusive), and the 
preference relation induced by

 is not asymmetric, despite 
the fact that the dependence graph of 
is acyclic. 
Note that while asymmetry implies satisfiability, the reverse 
does not hold in general. For example, the set

 above 
is not asymmetric, but it is satisfiable (the assignment acb is 
undominated). Given such a satisfiable set of statements, we 
can prompt the user with one of the undominated assignments 
without further refinement of its preference relation. Theorem 
1 shows that, in general, determining satisfiability of a 
set of statements is NP-complete. On the other hand, even 
for CP-nets, determining asymmetry is not known to be in 
NP [Domshlak and Brafman, 2002]. 

Theorem 1 SATISFIABILITY 
statements 
of a set of conditional preference 
is NP-complete. 

Proof: Membership in NP is straightforward, as an assignment 
is a polynomial-size witness that can be checked for 
non-dominance in time linear in the size of . To show hardness, 
we reduce 3-SAT to our problem: Given a 3-cnf formula 
tional preference statement 
construct the for each clausew 
_
F , condiThis 
set of 
conditional preferences is satisfiable iff the original original 
formula F is satisfiable. D 

'in preference logic [Hansson, 2001], these notions of Consistency 
as satisfi ability" and Consistency as asymmetry" correspond 
to the notions of eligibility and restrictable eligibility, respectively. 
However, we will use the former terms as they seem more intuitive. 

While testing satisfiability is hard in general, Theorem 2 
presents a wide class of statement sets that can be tested for 
satisfiability in polynomial time. 

Theorem 2 A set of conditional preference statements Vt, 
whose dependency graph is acyclic and has bounded node 
in-degree can be tested for satisfiability in polynomial time. 


clauses of the formula the proof of Theorem 1). However, 
when at most one condition is allowed in each preference 
statement, and the features are boolean, then SATISFIABILITY 
can be reduced to 2-SAT, and thus tested in polynomial 
time. Further study of additional tractable cases is clearly of 
both theoretical and practical interest. 

4 Approximating CP-nets with Soft 

Constraints 
In addition to testing consistency and determining preferentially 
optimal outcomes, we can be interested in the preferential 
comparison of two outcomes. Unfortunately, determining 
dominance between a pair of outcomes with respect to a set 
of qualitative preferential statements under the ceteris paribus 

assumption is PSPACE-complete in general [Lang, 2002], and 
is NP-hard even for acyclic CP-nets [Domshlak and Brafman, 
2002]. However, given a set

 of preference statements, instead 
of using a preference relation 
induced by , one can 
use an approximation 
achieving tractability while 
sacrificing precision to some degree. Clearly, different approximations


 are not equally good, as they can be 
characterized by the precision with respect to

 
time complexity 
of generating ., and time complexity of comparing 
outcomes with respect to ^>. In addition, it is vital that >> 
faithfully extends (i.e. should entail We call this information preserving. Another desirable property of approximations is that of preserving the ceteris paribus property (we call this the cp-condition for short). 

For acyclic CP-nets, two approximations that are information 
preserving have been introduced, both comparing outcomes 
in time linear in the number of features. The first 
is based on the relative position of the features in the CP-
net graph [Boutilier et al, 2002]. This approximation does 
not require any preprocessing of the CP-net. However, it 
is problematic when there are hard constraints. The second, 
based on UCP-nets [Boutilier et al 2001], can be used 
as a quantitative approximation of acyclic CP-nets. UCP-
nets resemble weighted CSPs, and thus they can be used 
in constraint optimization using the soft constraints machinery. 
However, generating UCP-nets is exponential in the size 
of CP-net node's Markov family2, and thus in the CP-net 
node out-degree. An additional related work is described in 
LMcGeachie and Doyle, 2002], where a numerical value function 
is constructed using graph-theoretic techniques by examining 
the graph of the preference relation induced by a set of 
preference statements. Note that this framework is also computationally 
hard, except for some special cases. 

Here we study approximating CP-nets via soft constraints 
(SCSPs). This allows us to use the rich machinery underlying 
SCSPs to answer comparison queries in linear time. 
Moreover, this provides us a uniform framework to combine 
user preferences with both hard and soft constraints. Given 
an acyclic CP-net, we construct a corresponding SCSP in 
two steps. First, we build a constraint graph, which we call 
SC-net. Second, we compute the preferences and weights 
for the constraints in the SC-net, and this computation depends 
on the actual semi-ring framework being used. Here 
we present and discuss two alternative semi-ring frameworks, 
based on min+ and SLO (Soft constraint Lexicographic Ordering) 
semi-rings, respectively. In both cases, our computation 
of preferences and weights ensures information preserving 
and satisfies the cp-condition. We illustrate the construction 
of the SCSP using the example in Figure 2, which 
continues our running example from Figure 1. 

Given a CP-net N, the corresponding SC-net Nc has two 
types of nodes: First, each feature

 is represented 
r each featuresuch 
in Nc by a node Vx that stands for a SCSP variable with 
nodewitah 
Edge in correspond to 


2 Markov family of a node X contains X, its parents and children, 
and the parents of its children. 

We now prove that our algorithm for weight computation 
ensures the cp-condition on the resulting set of soft constraints, 
and this also implies preserving the ordering information 
with respect to the original CP-net. 

Theorem 4 (Complexity) Given an acyclic CP-net N with 
the node in-degree bounded by a constant, the construction of 
the corresponding SC-net based weighted SCSP Nc is polynomial 
in the size ofN. 

Proof: If the CP-net has n nodes then the number of vertices 
V of the derived SC-net is at most 2n. In fact, in the SC-net 
a node representing a feature appears at most once and there 
is at most one node representing its parents. If the number of 
edges of the CP-net is e, then the number of edges E in the 
SC-net (including hard and soft edges) is at most e + n, since 
each edge in the CP-net corresponds to at most one constraint, 
and each feature in the CP-net generates at most one new soft 

Since the min+ approximation is a total ordering, it is a linearization 
of the original partial ordering. In compensation, 
however, preferential comparison is now linear time. 

4.2 SLO soft constraints 

Note that the SLO model both preserves information and 
ensures the cp-condition. The proof of this is straightforward 
and is omitted due to lack of space. The SLO model, like the 
weighted model, is very useful to answer dominance queries 
as it inherits the linear complexity from its semi-ring structure. 
In addition, the sequences of integers show directly the 
"goodness" of an assignment, i.e. where it actually satisfies 
the preference and where it violates it. 

4.3 Comparing the two approximations 
is to be preferred to the SLO model, as far as approximation 
is concerned. However, maximizing the minimum reward, as 
in any fuzzy framework [Schiex, 1992], has proved its usefulness 
in problem representation. The user may therefore need 
to balance the linearization of the order and the suitability of 
the representation provided. 

 Future Work 

We plan to use our approach in a preference elicitation system 
in which we guarantee the consistency of the user prefer


ences, and guide the user to a consistent scenario. Morover, 
we also plan to exploit the use of partially ordered preferences, 
as allowed in soft constraints, to better approximate 
CP nets. Finally, we intend to use machine learning techniques 
to learn conditional preferences from comparisons of 
complete assignments. 

Abstract 

We identify a new and important global (or non-
binary) constraint which ensures that the values 
taken by two vectors of variables, when viewed 
as multisets, are ordered. This constraint is useful 
for a number of different applications including 
breaking symmetry and fuzzy constraint satisfaction. 
We propose and implement a linear time algorithm 
for enforcing generalised arc-consistency 
on such a multiset ordering constraint. Experimental 
results show considerable promise. 

1 Introduction 

Global (or non-binary) constraints are one of the factors central 
to the success of constraint programming [Regin, 1994; 
1996; Beldiceanu, 2000]. Global constraints specify patterns 
that occur in many problems, and call efficient and effective 
constraint propagation algorithms. In this paper, we identify 
a new and important global constraint. This constraint ensures 
that the values taken by two vectors of variables, when 
viewed as multisets, are ordered. Such a constraint is useful 
in a number of domains. For example, in the progressive 
party problem (probO13 in csplib.org), we wish to assign a 
host for each guest and period. We can model this with a vector 
of variables for each period. Each variable is assigned the 
host for a particular guest. This model has unnecessary symmetry 
as the periods can be freely permuted. We can break 
this symmetry by considering the multiset of values associated 
with each vector and ordering these multisets.The aim of 
this paper is to study such multiset ordering constraints and to 
develop efficient and effective techniques for enforcing them. 

2 Formal Background 

A constraint satisfaction problem (CSP) consists of a set of 
variables, each with a finite domain of values, and a set of 
constraints that specify allowed values for subsets of variables. 
A solution is an assignment of values to the variables 
satisfying the constraints. To find such solutions, we explore 
partial assignments enforcing a local consistency like generalized 
arc-consistency (GAC). A constraint is GAC iff, when 

* Support received by EPSRC under GR/R30792 and by the Science 
Foundation Ireland. We thank Chris Beck and Chris Jefferson. 
Vectors of variables are indexed from 0. The minimum 
element in the domain of x1 is

 
, and the maximum 
is max(xl). The function floor(x) assigns all variables in f 
to their minimum values, whilst ceil(x) assigns all to their 
maximums. The vector

 is identical to x except v now 
has the domain . An occurrence vector occ(x) associated 
with x is indexed in decreasing order from the maximum 
max(x) to the minimum rnin(x) value from the domains in 

x. The ith element of occ(x) is the number of occurrences of 
max(x) -i in x. When comparing two occurrence vectors, 
we assume they start and end with the occurrence of the same 
value, adding leading/ trailing zeroes as necessary. Finally, 
iff x is lexicographically less than or equal to y. 

3 Motivating applications 

3.1 Matrix symmetry 
Many constraints programs contain matrices of decision variables 
(so called "matrix models"), and the rows and/or 

columns of these matrices are symmetric and can be permuted 
[Flener et al,, 2002]. Such symmetries are very difficult to 
deal with as there are a super-exponential number of permutations 
of the rows or columns to consider. There are several 
ways to break symmetry in a CSP, such as SBDS [Gent and 
Smith, 2000] or SBDD [Fahle et al, 2001]. One of the most 
effective, and the one which we will concentrate on as a major 
application for a multiset ordering constraint, is adding extra 
symmetry-breaking constraints to an initial model. Existing 
techniques for dealing with such symmetries typically eliminate 
only some of the symmetry. Additional techniques, like 
those proposed here, are therefore of considerable value. 

The progressive party problem mentioned earlier has a 2d 
matrix of decision variables with matrix symmetry. The rows 
of the matrix are the guests, the columns are the periods. Each 
variable gives the host assigned to a given guest in a given period. 
As periods are indistinguishable, the columns of the matrix 
are symmetric. One way to break such column symmetry 
is to lex order the columns [Frisch et al., 2002]. Similarly, 
as guests can be indistinguishable, (some of) the rows may 
be symmetric and can be lex ordered. Alternatively, we can 
treat each row and/or column as a multiset and break such 
symmetry by multiset ordering the rows and/or columns. 

Unlike lex ordering, multiset ordering the rows of a matrix 
may not eliminate all row symmetry. For example, consider 
the symmetric matrices: 


Both satisfy the constraint that the first row is multiset less 
than the second. It is therefore a little surprising to discover 
that multiset ordering (which does not break all row 
symmetry) is not dominated by lex ordering (which does) 
but is incomparable. For example,When we have both row and column symmetry, we can multiset order both rows and columns. Like lex ordering both rows and columns, this may not eliminate all row and column 

symmetry. Consider the symmetric matrices: 
Both have multiset ordered rows and columns. Unsurprisingly, 
multiset ordering rows and columns is incomparable 
to lex ordering rows and columns. Consider the symmetric 
matrices: 


The first has lex ordered rows and columns, but the columns 
are not multiset ordered. The second has rows and columns 
that are multiset ordered but the columns are not lex ordered. 

An alternative way to deal with row and column symmetry 
is to multiset order in one dimension and apply the symmetry 
breaking method of our choice in the other dimension. 
This is one of the best features of using multiset ordering to 
break symmetry. It is compatible with any other method in 
the other dimension. For instance, we can multiset order the 
rows and lex order the columns. Preliminary results in [Kiziltan 
and Smith, 2002] suggest that such a combined method 
is very promising. This combined method does not eliminate 

all symmetry (but it is unlikely that any polynomial set of 

constraints does). Consider the symmetric matrices: 
Both have rows that are multiset ordered, and rows and 
columns that are lex ordered. Multiset ordering the rows and 
lex ordering the columns is again incomparable to lex order


ing rows and columns. Consider the symmetric matrices: 
The first matrix has rows that are multiset ordered and 
columns that are lex ordered. However, its rows are not lex 
ordered. The second matrix has rows and columns that are 
lex ordered but does not have rows that are multiset ordered. 
Whilst the two orderings are theoretically incomparable, our 
experimental results (see later) show that multiset ordering 
the rows and lex ordering the columns is often the most effective 
symmetry breaking constraint currently known. 

3.2 Fuzzy constraints 
Another application for multiset ordering is to fuzzy CSPs. A 
fuzzy constraint associates a degree of satisfaction to an assignment 
tuple for the variables it constrains. To combine degrees 
of satisfaction, we can use a combination operator like 
the minimum function. Unfortunately, the minimum function 
may cause a drowning effect when one poorly satisfied constraint 
'drowns' many highly satisfied constraints. One solution 
is to collect a vector of degrees of satisfaction, sort these 
values in ascending order and compare them lexicographically. 
This leximin combination operator identifies the assignment 
that violates the fewest constraints [Fargier, 1994]. 
This induces an ordering identical to the multiset ordering except 
that the lower elements of the satisfaction scale are the 
more significant. It is simple to modify a multiset ordering 
constraint to consider the values in a reverse order. To solve 
such leximin fuzzy CSPs, we can then use branch and bound, 
adding an ordering constraint when we find a solution to ensure 
that future solutions are greater in the leximin ordering. 

4 GAC algorithm for multiset ordering 
The last section motivated why we want multiset ordering 
constraints. We need, however, to be able to propagate such 
constraints efficiently. We therefore developed an efficient 
GAC algorithm for such constraints. 

4.1 Background 
The algorithm exploits two theoretical results. The first reduces 
the problem to testing support for upper bounds of x 
and lower bounds of y on suitable ground vectors. The second 
reduces these tests to lex ordering suitable occurrence 
vectors. Identical results hold for the strict multiset ordering 
constraint but for reasons of space we omit them here. 
Lemma 1 Given two disjoint and non-repeating vectors of 
variables, x and y, with non-empty domains, GAC(x <my) 

Proof: (=>) As the constraint is GAC, all values have support. 
In particular,

 
has support. The best 
support comes if all the other variables in x take their minimums, 
and all the variables in y take their maximums. Hence, 


Similarly, for y1. 

(.=) The first constraint ensures that rnax(x1) is supported. 
The values which support max(xi) also support all smaller 
values. Hence, all the values in the domain of Xi are supported. 
By an analogous argument, all the values in the domain 
of yi are supported. Hence the constraint is GAC. QED. 

The next lemma reduces these tests for support to lex ordering 
suitable occurrence vectors. 
Lemma 2 Given two multisets of values, M and N, M <m 


Proof: See [Kiziltan and Walsh, 2002]. 

4.2 A worked example 
Based on these lemmas, we have designed an efficient algorithm 
for enforcing GAC on a multiset ordering constraint. 
The algorithm goes through the xt and yj checking for support 
in the appropriate occurrence vectors. Incremental computation 
of the lex ordering test avoids repetition of work. 
Consider the multiset ordering constraint


 where: 


We construct occurrence vectors for floor(x) and ceil(y), indexed 
from 5 to 0: 


Recall that indexdenote s the number of 
occurrences of the

 For example, index 4 is 
2 as the value 4 occurs twice. 
We first check if

 
If so, we 
can fail immediately because no value for any variable can 
have support. Here, . In fact, 
we record (in a pointer, .) that the two occurrence vectors 
are lex ordered by index 4 of occ(floor{x)), which is strictly 
smaller than index 4 of occ(ceil(y)). This means that we will 
fail to find support in the yj if any of the xi is assigned a new 
value greater than 4. We now go through the xi checking for 
support for their maximum values, and then the yi checking 
for support for their minimum values. 

Consider X0. AS it has a singleton domain, and 


its only value must 
have support so we skip it. Now consider x\. Do 
its values have support? Changing occ(floor{x)) to 


decreases the 
number of occurrences ofmm(ii ) = 4 by 1, and increases 
the number of occurrences of

 
by 1. As 


this upsets the lex ordering of the two occurrence 
vectors. We therefore prune all values in the domain 
of x1 larger than .. This leaves a single supported value, 4. 

Now consider x2. Changing 


to 


decreases 
the number of occurrences of min (x2) = 3 by 1, and 
increases the number of occurrences of 
by 1. As with x1 any value of x2 larger than 
upsets the lex ordering. We therefore prune 5 from the 

 
to 

decreases 

 number of occurrences of by 1, and 
increases the number of occurrences of max(x3) = 4 by 1. 
The occurrence vectors beneath a would now be lex ordered 
the wrong way. We therefore also prune the value

 
= 4, 
leaving a single supported value 2 in the domain of x3. As 
x4 and x5 have singleton domains, their values have support. 
Similarly, we check the minimums of the

 for support. 
However, rather than prune values above (and in some cases 
equal to) , there is now a dual pointer . and we prune values 
in the domains of 
up to (and in some cases equal to) 
The 
pointer

 is the largest index such that the occurrence vectors 
beneath it are lex ordered the wrong way. Any value less than 
cannot hope to change the lex ordering 
will still order the vectors the wrong way. Such values can 
therefore be pruned. Once we have considered each of the yj, 
we have the following generalized arc-consistent vectors: 
4.3 Algorithm details 
The algorithm uses two pointers 
and 
and two flags 
and S to avoid traversing the occurrence vectors each time we 
look for support. The pointer 
is set to to the most significant 
index above which all occurrences are pair-wise equal and at 
. they are strictly ordered. If the vectors are equal then . is 
set to - The pointer 
is set to the most significant index 
below . such that the occurrence vectors are lex ordered the 
wrong way. If no such index exists, we set 
to \A1\AA 00. The 
flag 
is set to true if all the indices between . and (3 are 
pair-wise equal and the flag 
is set to true if the sub-vectors 
below

 
are lex ordered the wrong way. For example, given 
the occurrence vectors in section 4.2, . is set to 4, 
to 2, and 
the flags and 
are set to true. 
We summarise the major steps the algorithm performs: 

A. 
B. 
according to their definitions 

C. 
For each xi If its maximum disturbs the lex ordering 
on the occurrence vectors, tighten its upper-bound to . 
when the occurrence vectors are lex ordered below ., 
otherwise to 
D. 
For each yi If its minimum disturbs the lex ordering on 
the occurrence vectors, then tighten its lower-bound to 
when the occurrence vectors are lex ordered below a, 
otherwise to 
+ 1. 
When we prune a value, we do not need to check recursively 
for previous support. Pruning changes neither the 
lower bounds of x nor the upper bounds of y. These values 
continue to provide support. The exception is when a domain 
is a singleton, and pruning causes a domain wipe-out. 
We now give pseudo-code for an algorithm that maintains 
GAC on a multiset ordering constraint between vectors x and 
y which are of length n and m respectively. As the algorithm 
reasons about occurrences vectors, the original vectors need 
not be identical in length (though they often are). 
The algorithm is called whenever lower bounds of x1 or 
upper bounds of yj change. Lines Al to A3 build the occurrence 
vectors ox and dy. Line Bl calls the procedure to 

is equivalent to GAC on the original multiset ordering constraint. 
However, such an arithmetic constraint is only feasible 
for small n. Further, most existing solvers will not enforce 
BC on such an arithmetic constraint, but will delay it until all 
but one of the variables are instantiated. 

5.2 Decomposition 
Multiset ordering is equivalent to the lex ordering the associated 
occurrence vectors. As we have efficient algorithms 
for constructing occurrence vectors (via the global cardinality 
constraint [Regin, 1996]) and for lex ordering [Frisch et 
al., 2002], this might be an alternative approach. However, 
as the following theorem shows, such a decomposition hinders 
constraint propagation. Also, the two global cardinality 
constraints in such a decomposition are more expensive to enforce 
than the algorithm presented here. We write gcc(x, ox) 
for the global cardinality constraint that channels from a vector 
of variables x to the associated occurrence vector ox. 



Another approach is to use the sorted constraint in the 
Eclipse solver. This ensures that the values taken by one vector 
of variables are identical but in sorted order to the values 
taken by a second vector of variables. To post a multiset ordering 
constraint on two vectors, we can channel each into a 
sorted vector and lex order these. The above example demonstrates 
that such a decomposition again hinders propagation. 
The sorting constraint is also more expensive to enforce. 

 multiset 
ordering o , (strict) multiset ordering 
on the columns and combinations of these 
constraints. Such constraints are posted between adjacent 
rows/columns. The results of the experiments are shown in 
tables where a "-" means no result is obtained in 1 hour (3600 
secs). The experiments are done using 1LOG Solver 5.2 on a 
1000MHz pentium III with 256 Mb RAM using Windows XP. 

6 Experimental results 
We designed some experiments to test three goals. First, is 
multiset ordering an effective method for dealing with row 
and/or column symmetry? Second, how does multiset ordering 
compare to lex ordering? Which one breaks more symmetry? 
Is a combined method, which multiset orders one dimension 
and lex orders the other one of the matrix, superior? 
Third, does our GAC algorithm do more inference in practice 
than the decomposition? Similarly, is the algorithm more 
efficient in practice than its decomposition? 
The symmetry breaking constraints we used are strict lex 
ordering on the on the rows

6.1 Progressive Party Problem 
There are a set of host boats, each with a capacity, and a set of 
guest boats, each with a crew size. We wish to assign a host 


Table 1: 5-13-29 progressive party problem using row-byrow 
labelling. 

for each guest and period, such that a guest crew never visits 
the same host twice, no two guest crews meet more than once, 
and the spare capacity of each host boat, after accommodating 
its own crew, i s not exceeded (probO13 i n csplib.org) . 

A matrix model of this problem [Smith et al., 1995] is a 
2-d matrix of guests x periods where each variable is assigned 
a host representing that a host is accommodating a 
particular guest in a given time period. The rows are the 
guests, the columns are the periods. This model has column 
and partial row symmetry: any two periods, and any 
two guests with the same crew size are indistinguishable. We 
consider the 13-hosts and 29 guests problem with 5 and 6 
time periods, referred as 5-13-29 and 6-13-29. These problems 
have ;P!14!2!4!5!7! row and column symmetries where 
p is the number of time periods. The actual data can be 
found in csplib.org. Due to the problem constraints, no pair 
of rows/columns can be equal, hence we can safely pose strict 
lex ordering. However, any two distinct rows/columns might 
be equal when viewed as multisets. 

As in [Smith et al., 1995], the guest boats are ordered in 
descending order of their size. We order the host boats in 
descending order of spare capacity to choose a value in a 
succeed-first manner. Results obtained by row-by-row, and 
column-by-column labelling strategies are given in Tables 1 
and 2. With row-by-row labelling, we cannot solve 6-13-29 
with or without symmetry breaking. For the 5-13-29 problem, 
breaks a lot more row symmetry thanHowever, <ieXKthe reverse is true for the columns. Here, ' does not 
break any symmetry but

 
does. Multiset ordering one 
dimension of a matrix therefore does not necessarily break 
less symmetry than lex ordering the same dimension. Such 
phenomena occur through interactions with the search strategy: 
a search strategy might already lex order, so multiset 
ordering constraints break additional symmetry. The smallest 
search tree and also the least solving time is obtained by 
<lexR. This supports our conjecture that lex ordering one dimension 
combined with multiset ordering the other can break 
more symmetry than lex/multiset ordering both dimensions. 

With column-by-column labelling, we are able to solve the 
6-13-29 problem. Neither of break 
any symmetry. The smallest search tree is obtained by 
This supports our conjecture that multiset ordering one dimension 
can break more symmetry than lex ordering the same 
or both dimensions. If the search strategy already orders both 
dimensions lexicographically, imposing a constraint like multiset 
ordering in one dimension breaks additional symmetry. 

Table 2: 6-13-29 progressive party problem using columnby-
column labelling. 

6.2 Sports Scheduling with Odd Teams 
This is a modified version of prob026 in csplib.org. We have 
n teams (n is odd), playing over n weeks. Each week is 
divided into (n \A1\AA l)/ 2 periods, and each period is divided 
into 2 slots, home and away. We wish to find a schedule 
so that every team plays at most once a week, every team 
plays twice in the same period over the tournament and every 
team plays every other team. We slightly modify the model 
in [Van Hentenryck et al., 1999], where teams is a 3-d matrix 
of periods x weeks x slots. Each element of teams is the 
team playing in a given period, week and slot. We treat this 
matrix as 2-d where the rows are the periods and columns are 
the weeks, and each entry is a list of variables giving the slots. 

As the periods and the weeks are indistinguishable, this 
problem has

 
row and column symmetries. We 
pose strict ordering constraints on the rows and columns of 
teams as the periods and weeks cannot be equal. Due to the 
constraints on the periods, posing multiset ordering on the 
rows is not effective. 

Results obtained by column-by-column labelling of the 
teams are given in Table 3. For one column, we first label 
the first slots; for the other, we first label the second slots. 
With this strategy,

 
does not break any symmetry, so 
we omit it in the table. Posing multiset ordering by our algorithm 
is much more effective and efficient than by gec and lex 
ordering constraints. This holds for many other search strategies. 
In Table 3, we note that

 
gives a smaller search tree 
than

 However, for other search strategies the reverse 
is true. This supports the theoretical result that lex ordering 
and multiset ordering are incomparable. 

7 Conclusions 

We have identified a new and important global (non-binary) 
constraint. This constraint ensures that the values taken by 
two vectors of variables, when viewed as multisets, are ordered. 
We have developed an efficient linear time algorithm 
for enforcing generalised arc-consistency on such a multiset 
ordering constraint. We have proposed a number of applications 
for this new constraint including breaking symmetry 
in matrix models, and fuzzy constraint satisfaction. We have 
shown that alternative methods for posting a multiset ordering 
constraint like an arithmetic constraint or decomposition are 
inferior. Finally, we have implemented this generalized arc-
consistency algorithm in ILOG Solver. Experimental results 
on a number of problem domains show considerable promise. 




# 人工智能报告
## Paper
## 实验

# Paper
### Deep Residual Learning for Image Recognition
![](https://github.com/chynphh/2014201882/blob/master/2014201882/Paper/1/1.png)
![](https://github.com/chynphh/2014201882/blob/master/2014201882/Paper/1/2.png)
![](https://github.com/chynphh/2014201882/blob/master/2014201882/Paper/1/3.png)

### Convolutional Neural Networks for Sentence Classification
#### Abstract
    作者将卷积神经网络CNN引用到了NLP的文本分类任务中，模型比较简单，性能却很好。作者做了不同输入情况的模型实验，并对结果进行了对比。
#### Model
    第一层（输入层）：
    输入层相当于表示一个句子的矩阵，每行是一个词向量；假设每个词向量是k维，该句子有n个词，则这个矩阵就是n×k 的。同时这个矩阵的类型可以是静态的，也可以是动态的；静态就是词向量是不变的，而动态则是在模型训练过程中，词向量被当做是一种参数，同样可被调优。
    第二层（卷积层）：
    卷积窗口的大小为 h×k ，其中h 表示纵向词语的个数，而 k 表示词向量的维数。多个滤波器作用于词向量层，不同滤波器生成不同的Feature Map，得到若干个Feature Map。
    第三层（池化层）：
    作者采用了一种简单的方法：从之前一维的Feature Map中提出最大的值，最大值代表着最重要的特征。这样操作可以处理变长句子输入问题，因为第三层只提取了最大值，输出只依赖于滤波器的个数。
    最终池化层的输出是一个一维向量。
    第四层（全连接的softmax层）：
    第三层的输出通过全连接连接一个Softmax层，Softmax层通常反映最终类别上的概率分布；
![](https://github.com/chynphh/2014201882/blob/master/2014201882/Paper/2/6.png)

# 实验
## BOT图像分类
### 实验内容
    在BOT数据集上做图像分类任务
### 实验环境
    Ubuntu16.04LTS
    Python
    theano,cudn,cudnn,kares
### 实现
#### 数据预处理
##### 划分训练集和测试集
    将bot的数据集按照4:1分为train集和val集,以便之后训练,防止过拟合.
   ![](https://github.com/chynphh/2014201882/blob/master/2014201882/bot/pictures/1.png)
   ![](https://github.com/chynphh/2014201882/blob/master/2014201882/bot/pictures/2.png)
##### 数据转换与扩充
    在做训练之前,需要对数据进行归一化处理,在通过一系列随机变换,如旋转,翻转,剪切等等提升数据量,我们可以利用keras中的函数来完成上述操作
    keras.preprocessing.image.ImageGenerator
    利用有限的训练数据，将通过一系列随机变换堆数据进行提升
    rescale:将在执行其他处理前乘到整个图像上，图像在RGB通道都是0\~255的整数，这样的操作可能使图像的值过高或过低，所以我们将这个值定为0\~1之间的数
    shear_range:用来进行剪切变换的程度
    horizontal_flip随机的对图片进行水平翻转
    zoom_range用来进行随机的放大
![](https://github.com/chynphh/2014201882/blob/master/2014201882/bot/pictures/5.png)
#### 模型搭建
    在模型上选择CNN模型,一共六层,前三层均为卷积池化层,接下来的两层为全连接层,最后一层为softmax分类层
    此处贴出代码和可视化展示
#### 实验结果
    在迭代100次之后,loss收敛,在训练集上的准确率达到了0.928,在测试集上的准确率达到了0.935
![](https://github.com/chynphh/2014201882/blob/master/2014201882/bot/pictures/Screenshot%20from%202016-11-05%2021_20_32.png)
#### 实验结果分析
    可以看到,实验结果已经达到了比较好的效果,而且网络结构也十分简单,对比其他同学复杂的深层网络,可能是由于本方法数据处理部分做的十分细致,导致训练结果有所差距


## 真假论文判别
### 实验内容
    判断论文的真假,辨别由scigen生成的假论文
### 实验环境
    Ubuntu16.04LTS
    Python
    libsvm
    theano,cudn,cudnn,kares,
### 实验数据
    真论文:老师给的3000多篇论文
    假论文:通过爬虫从scigen爬取的论文,大概由1000多篇
### 实现
#### 数据预处理
##### 格式转换
    利用软件,将作文pdf格式,转为txt格式.
##### 数据清洗
    将论文转换为txt格式之后,虽然去除了图表公式,但是还是存在一些数字,停用词等等的无效信息,所以在进行训练之前,需要将数据进行清洗.
   ![](https://github.com/chynphh/2014201882/blob/master/2014201882/Scigen/pictures/1.png)
##### 统计词频
    对清洗完毕的数据进行统计,以便训练.
    考虑到只需要分辨真假论文,无关文章的具体内容,变抽取每篇文章词频top300的词,构成300维的一个向量,每一维的值便是词频.
   ![](https://github.com/chynphh/2014201882/blob/master/2014201882/Scigen/pictures/2.png)
##### 归一化
    一个300维的向量代表一篇文章,但是每一位都是一个整数,在训练之前,便需要将数据处理成-1~1之间的值.
   ![](https://github.com/chynphh/2014201882/blob/master/2014201882/Scigen/pictures/3.png)
#### 模型
    采用的模型为SVM,直接利用libsvm工具即可.
    参数设置:
        -t 0 (线性核)
        -v 10 (10折交叉验证)
#### 实验结果
    准确率达到了0.995,且训练速度十分的快.
   ![](https://github.com/chynphh/2014201882/blob/master/2014201882/Scigen/pictures/4.png)
#### 代码使用
    python spider_neg.py
    python stop.py
    python svm.py
    ./svm-train -t 0 -v 10 data_train_scale.txt model.txt
    
    

## 文章分类
### 实验内容
    通过不同算法实现作文分类,并对比实验效果.
### 实验环境
    Ubuntu16.04LTS
    Python
    libsvm
    theano,cudn,cudnn,kares,
### 实验数据
    网上的数据,数据总量在530,000篇左右,其中有标记的有7000左右,分为了7大类.
### 实现
#### 数据预处理
##### 分词
    将一个汉字序列切分成一个一个单独的词。我们知道，在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是字、句和段能通过明显的分界符来简单划界，唯独词没有一个形式上的分界符.所以在做之后的步骤之前,需要将整篇文章切分成一个一个的词语.
##### 去停用词
    为节省存储空间和提高效率，在处理数据之前需要过滤掉某些无用或者信息量特别小的字或词.
##### 训练词向量
    利用gensim工具,对53万篇文章进行训练,生成每个单词的词向量.和每句话的句子向量
##### 统计词频
    对于所有文章,生成一个词典,统计每个词在文章中出现的频率,以此作为每一篇的文章的向量.

#### 模型
##### CNN - word
    每篇文章由一系列单词的词向量构成
    模型共四层:
        第一层:卷积池化层,卷积核的高度分别为3,4,5
        第二层:全连接层,150维
        第三层:全连接层,100维
        第四层:softmax分类层,7维
    参数设置:
        dropout = 0.5
        optimizer = 'Adagrad',
        loss = 'categorical_crossentropy'
        nb_epoch = 2
        batch_size = 50
    
##### CNN - sentence
    每篇文章由一系列句子的句子向量构成
    模型共四层:
        第一层:卷积池化层,卷积核的高度分别为3,4,5
        第二层:全连接层,150维
        第三层:全连接层,100维
        第四层:softmax分类层,7维
    参数设置:
        dropout = 0.5
        optimizer = 'Adagrad',
        loss = 'categorical_crossentropy'
        nb_epoch = 3
        batch_size = 50
##### CNN - merge
    将CNN - word 和 CNN - sentence的模型结合起来,在卷积赤化层之后,对输出结果进行reshape,再讲两部分连接起来.再接两个全连接层和分类层.
    参数设置:
        dropout = 0.5
        optimizer = 'Adagrad',
        loss = 'categorical_crossentropy'
        nb_epoch = 2
        batch_size = 50
##### SVM
    采用的模型为SVM,直接利用libsvm工具即可.
#### 结果

| Algorithm          | Acc           | 
|:------------------:|:-------------:|
|word2vec+SVM        |0.600371       |
|word2vec+CNN-word   |0.661428       |
|word2vec+CNN-sen    |0.638571       |
|word2vec+CNN-merge  |0.695945       |
#### 结果分析
        可以看到，CNN模型总体上来说要比SVM效果好很多，sentence-level准确率提升了0.038，word-level提升了了0.06，merge-level效果提升了将近0.1

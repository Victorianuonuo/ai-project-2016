Networked Distributed POMDPs: A Synergy of Distributed

Constraint Optimization and POMDPs

Ranjit Nair

Pradeep Varakantham, Milind Tambe

Makoto Yokoo

Knowledge Services Group

Computer Science Dept.

Dept. of Intelligent Systems

Honeywell Laboratories

University of Southern California

Kyushu University

ranjit.nair@honeywell.com

{varakant,tambe}@usc.edu

yokoo@is.kyushu-u.ac.jp

Abstract

In many real-world multiagent applications
such as distributed sensor nets, a network of
agents is formed based on each agent’s limited
interactions with a small number of neighbors.
While distributed POMDPs capture the real-
world uncertainty in multiagent domains, they
fail to exploit such locality of interaction. Dis-
tributed constraint optimization (DCOP) cap-
tures the locality of interaction but fails to cap-
ture planning under uncertainty. This paper
present a new model synthesized from distrib-
uted POMDPs and DCOPs, called Networked
Distributed POMDPs (ND-POMDPs). Ex-
ploiting network structure enables us to present
a distributed policy generation algorithm that
performs local search.

1 Introduction

In many real-world multiagent applications such as dis-
tributed sensor nets, a network of agents is formed based
on each agent’s limited interactions with a small num-
ber of neighbors. For instance, in distributed sensor nets,
multiple sensor agents must coordinate with their neigh-
boring agents in order to track individual targets mov-
ing through an area. In particular, we consider in this
paper a problem motivated by the real-world challenge
in [Lesser et al., 2003]. Here, each sensor node can scan
in one of four directions — North, South, East or West
(see Figure 1), and to track a target, two sensors with
overlapping scanning areas must coordinate by scanning
the same area simultaneously. We assume that there are
two independent targets and that each target’s move-
ment is uncertain but unaﬀected by the actions of the
sensor agents. Additionally, each sensor receives obser-
vations only from the area it is scanning and this obser-
vation can have both false positives and false negatives.
Each agent pays a cost for scanning whether the target
is present or not but no cost if turned oﬀ. Existing dis-
tributed POMDP algorithms [Montemerlo et al., 2004;
Nair et al., 2003; Becker et al., 2004; Hansen et al., 2004]
although rich enough to capture the uncertainties in this

N

Loc1-1

Loc2-1

EW
1

S

2
Loc1-2

5

Loc2-2

Loc1-3

3

4

Figure 1: Sensor net scenario: If present, target1 is in
Loc1-1, Loc1-2 or Loc1-3, and target2 is in Loc2-1 or
Loc2-2.

domain, are unlikely to work well for such a domain be-
cause they are not geared to take advantage of the local-
ity of interaction. As a result they will have to consider
all possible action choices of even non-interacting agents,
in trying to solve the distributed POMDP. Distributed
constraint satisfaction and distributed constraint opti-
mization (DCOP) have been applied to sensor nets but
these approaches cannot capture the uncertainty in the
domain. Hence, we introduce the networked distributed
POMDP (ND-POMDP) model, a hybrid of POMDP and
DCOP, that can handle the uncertainties in the domain
as well as take advantage of locality of interaction. Ex-
ploiting network structure enables us to present a novel
algorithm for ND-POMDPs – a distributed policy gen-
eration algorithm that performs local search.

2 ND-POMDPs

We deﬁne an ND-POMDP for a group Ag of n agents
as a tuple hS, A, P, Ω, O, R, bi, where S = ×1≤i≤nSi ×
Su is the set of world states. Si refers to the set of
local states of agent i and Su is the set of unaﬀectable
states. Unaﬀectable state refers to that part of the world
state that cannot be aﬀected by the agents’ actions, e.g.
environmental factors like target locations that no agent
can control. A = ×1≤i≤nAi is the set of joint actions,
where Ai is the set of action for agent i.

We assume a transition independent distributed
POMDP model, where the transition function is de-
ﬁned as P (s, a, s′) = Pu(su, s′
i),
where a=ha1, . . . , ani is the joint action performed in
state s=hs1, . . . , sn, sui and s′ =hs′
uiis the re-

u) ·Q1≤i≤n Pi(si, su, ai, s′

1, . . . , s′

n, s′

i) = Pr(s′

sulting state. Agent i’s transition function is deﬁned as
Pi(si, su, ai, s′
i|si, su, ai) and the unaﬀectable
transition function is deﬁned as Pu(su, s′
u|su).
Becker et al. [2004] also relied on transition indepen-
dence, and Goldman and Zilberstein [2004] introduced
the possibility of uncontrollable state features. In both
works, the authors assumed that the state is collectively
observable, an assumption that does not hold for our do-
mains of interest.

u) = Pr(s′

Ω = ×1≤i≤nΩi is the set of joint observations where
Ωi is the set of observations for agents i. We make
an assumption of observational independence,
i.e., we
deﬁne the joint observation function as O(s, a, ω) =
Q1≤i≤nOi(si, su, ai, ωi), where s = hs1, . . . , sn, sui, a =
ha1, . . . , ani, ω = hω1, . . . , ωni, and Oi(si, su, ai, ωi) =
Pr(ωi|s1, su, ai).

The reward function, R,

is deﬁned as R(s, a) =
Pl Rl(sl1, . . . , slk, su, hal1, . . . , alki), where each l could
refer to any sub-group of agents and k = |l|.
In the
sensor grid example, the reward function is expressed
as the sum of rewards between sensor agents that have
overlapping areas (k = 2) and the reward functions for
an individual agent’s cost for sensing (k = 1). Based on
the reward function, we construct an interaction hyper-
graph where a hyper-link, l, exists between a subset of
agents for all Rl that comprise R. Interaction hypergraph
is deﬁned as G = (Ag, E), where the agents, Ag, are the
vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R}
are the edges. Neighborhood of i is deﬁned as Ni = {j ∈
Ag|j 6= i ∧ (∃l ∈ E, i ∈ l ∧ j ∈ l)}. SNi = ×j∈NiSj refers
to the states of i’s neighborhood. Similarly we deﬁne
ANi, ΩNi, PNi and ONi .

b, the distribution over the initial state, is deﬁned as
b(s) = bu(su) ·Q1≤i≤n bi(si) where bu and bi refer to the
distributions over initial unaﬀectable state and over i’s
initial state, respectively. We deﬁne bNi = Qj∈Ni
bj(sj ).
We assume that b is available to all agents (although it is
possible to reﬁne our model to make available to agent
i only bu, bi and bNi). The goal in ND-POMDP is to
compute joint policy π = hπi, . . . , πni that maximizes the
team’s expected reward over a ﬁnite horizon T starting
from b. πi refers to the individual policy of agent i and
is a mapping from the set of observation histories of i to
Ai. πNi and πl refer to the joint policies of the agents in
Ni and hyper-link l respectively.

ND-POMDP can be thought of as an n-ary DCOP
where the variable at each node is an individual agent’s
policy. The reward component Rl where |l| = 1 can be
thought of as a local constraint while the reward com-
ponent Rl where l > 1 corresponds to a non-local con-
straint in the constraint graph. In the next section, we
push this analogy further by taking inspiration from the
DBA algorithm [Yokoo and Hirayama, 1996], an algo-
rithm for distributed constraint satisfaction, to develop
an algorithm for solving ND-POMDPs.

We deﬁne Local neighborhood utility of agent i as the
expected reward accruing due to the hyper-links that

contain agent i:
Vπ[Ni]=X

πl(sl1, . . . ,slk, su, hi)

bu(su)bNi(sNi)bi(si)X

V 0
l∈E s.t. i∈l

si,sNi ,su

(1)

While trying to ﬁnd best policy for agent i given its
neighbors’ policies, we do not need to consider non-
neighbors’ policies. This is the property of locality of
interaction that is used in the following section.

3 Locally optimal policy generation
The locally optimal policy generation algorithm called
LID-JESP (Locally interacting distributed joint equi-
librium search for policies) is based on the DBA algo-
rithm [Yokoo and Hirayama, 1996] and JESP [Nair et
al., 2003].
In this algorithm (see Algorithm 1), each
agent tries to improve its policy with respect to its neigh-
bors’ policies in a distributed manner similar to DBA.
Initially each agent i starts with a random policy and
exchanges its policies with its neighbors. It then evalu-
ates its contribution to the global value function, from
its initial belief state b with respect to its current policy
and its neighbors’ policy (function Evaluate()). Agent
i then tries to improve upon its current policy by calling
function GetValue, which returns the value of agent
i’s best response to its neighbors’ policies. Agent i then
computes the gain that it can make to its local neigh-
borhood utility, and exchanges its gain its neighbors. If
i’s gain is greater than that one any of its neighbors,
i changes its policy and sends its new policy to all its
neighbors. This process of trying to improve the local
policy is continued until termination, which is based on
maintaining and exchanging a counter that counts the
number of cycles where gaini = 0. This is omitted from
this article in order to simplify the presentation.

u, st

i, st

Ni , ~ωt

The algorithm for computing the best response is a
dynamic-programming approach similar to that used in
JESP. Here, we deﬁne an episode of agent i at time t as
et
i = (cid:10)st
Ni(cid:11). Given that the neighbors’ poli-
cies are ﬁxed, treating episode as the state, results in a
single agent POMDP, where the transition function and
observation function can be deﬁned as follows:
u, ai, st+1
P ′(et
, st+1

i, et+1
PNi (st

)=Pu(st
Ni , st

i
u , aNi , ωNi)

u, st+1

u ) · Pi(st

u, aNi , st+1

Ni ) · ONi (st+1

Ni

i, st

i, at

i

) ·

O′(et+1

i

, at

i, ωt+1

i

) =Oi(st+1

i

, st+1

u , ai, ωi)

The function GetValue() returns the optimal policy
for agent i given the above deﬁnitions of transition and
observation functions and the policies of Ni. In this pa-
per, GetValue() was implemented via value iteration
but any other single agent POMDP algorithm could have
been used.

4 Experimental Results
For our experiments, we ran the LID-JESP algorithm on
the sensor domain (see Figure 1). The ﬁrst benchmark

bNi (sNi )
prevV al
Evaluate(Agent i, si, su, sNi , πi, πNi , hi , hi , 0, T )
gaini ← GetValue(Agent i, b, πNi , 0, T ) − prevV al
Exchange gaini with Ni

bi(si)

·

·

Algorithm 1 LID-JESP(Agent i)
1: πi ← randomly selected policy, prevV al ← 0
2: Exchange πi with Ni
3: while termination not detected do
4:
5:

for all si, sNi , su do

+← bu(su)

·

winner ← argmaxj∈Ni∪{i}gainj
if maxGain > 0 and i = winner then

6:
7:
8: maxGain ← maxj∈Ni∪{i}gainj
9:
10:
11:
12:
13:
14:
15:
16: return πi

initialize πi
FindPolicy(Agent i, b, hi , πNi , 0, T )
Communicate πi with Ni
else if maxGain > 0 then

Receive πwinner from winner and update πNi

(JESP) is Nair et al.’s JESP algorithm [2003], which uses
a centralized processor to ﬁnd a locally optimal joint pol-
icy and does not consider the interaction graph. The sec-
ond benchmark (LID-JESP-no-nw) is LID-JESP with a
fully connected interaction graph. Figure 2(a) shows the
run time in seconds on a logscale on the Y-axis for in-
creasing ﬁnite horizon T on the X-axis, while Figure 2(b)
shows the value of policy found on the Y-axis and in-
creasing ﬁnite horizon T on the X-axis. All run times
and values were obtained by averaging 5 runs, each with
diﬀerent randomly chosen starting policies . As seen in
Figure 2(b), the values obtained for LID-JESP, JESP
and LID-JESP-no-nw are quite similar, although LID-
JESP and LID-JESP-no-nw often converged on a higher
local optima than JESP. In comparing the run times, it
should be noted that LID-JESP outperforms LID-JESP-
no-nw and JESP (which could not be run for T > 4
within 10000 secs).

5 Acknowledgments

This material
is based upon work supported by the
DARPA/IPTO COORDINATORs program and the Air
Force Research Laboratoryunder Contract No. FA8750–
05–C–0030.The views and conclusions contained in this
document are those of the authors, and should not be in-
terpreted as representing the oﬃcial policies, either ex-
pressed or implied, of the Defense Advanced Research
Projects Agency or the U.S. Government.

References

[Becker et al., 2004] R. Becker, S. Zilberstein, V. Lesser,
Solving transition indepen-
and C.V. Goldman.
dent decentralized Markov decision processes. JAIR,
22:423–455, 2004.

[Goldman and Zilberstein, 2004] C.V. Goldman and
S. Zilberstein. Decentralized control of cooperative
systems: Categorization and complexity analysis.
JAIR, 22:143–174, 2004.

10000

)
s
c
e
s
(
 
e
m

i
t
 

n
u
R

1000

100

10

1

0.1

0.01

0.001

300

250

200

150

100

50

0

l

e
u
a
V

LID-JESP

LID-JESP-no-n/w

JESP

1

2

3

Horizon

4

5

LID-JESP

LID-JESP-no-n/w

JESP

1

2

3

Horizon

4

5

Figure 2: 5 agent F-conﬁg.
Value

(a) Run time (secs), (b)

[Hansen et al., 2004] E.A. Hansen, D.S. Bernstein, and
S. Zilberstein. Dynamic Programming for Partially
Observable Stochastic Games. In AAAI, 2004.

[Lesser et al., 2003] V. Lesser, C. Ortiz, and M. Tambe.
Distributed sensor nets: A multiagent perspective.
Kluwer academic publishers, 2003.

[Montemerlo et al., 2004] R. E. Montemerlo, G. Gor-
don, J. Schneider, and S. Thrun. Approximate so-
lutions for partially observable stochastic games with
common payoﬀs. In AAMAS, 2004.

[Nair et al., 2003] R. Nair, D. Pynadath, M. Yokoo,
M. Tambe, and S. Marsella. Taming decentralized
POMDPs: Towards eﬃcient policy computation for
multiagent settings. In IJCAI, 2003.

[Yokoo and Hirayama, 1996] M. Yokoo and K. Hi-
rayama. Distributed breakout algorithm for solving
distributed constraint satisfaction problems.
In IC-
MAS, 1996.


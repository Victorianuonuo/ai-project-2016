Continuous Time Particle Filtering

Brenda Ng

Harvard University

Cambridge, MA 02138
bmng@eecs.harvard.edu

Avi Pfeffer

Harvard University

Cambridge, MA 02138
avi@eecs.harvard.edu

Richard Dearden

University of Birmingham
Birmingham B15 2TT, UK

R.W.Dearden@cs.bham.ac.uk

Abstract

We present
the continuous-time particle ﬁlter
(CTPF) – an extension of the discrete-time particle
ﬁlter for monitoring continuous-time dynamic sys-
tems. Our methods apply to hybrid systems con-
taining both discrete and continuous variables. The
dynamics of the discrete state system are governed
by a Markov jump process. Observations of the dis-
crete process are intermittent and irregular. When-
ever the discrete process is observed, CTPF sam-
ples a trajectory of the underlying Markov jump
process. This trajectory is then used to estimate
the continuous variables using the system dynam-
ics determined by the discrete state in the trajectory.
We use the unscented Kalman-Bucy ﬁlter to han-
dle nonlinearities and continuous time. We present
results showing that CTPF is more stable in its per-
formance than discrete-time particle ﬁltering, even
when the discrete-time algorithm is allowed to up-
date many more times than CTPF. We also present
a method for online learning of the Markov jump
process model that governs the discrete states.

1 Introduction
The work described here is directly motivated by problems
encountered in creating a state estimation system for the K-
9 experimental Mars rover at NASA Ames Research Cen-
ter [Willeke and Dearden, 2004]. As with many robotic sys-
tems, K-9 contains numerous sensors that report on its per-
formance. These sensors produce telemetry at different rates,
and frequently at rates that vary over time. As a result, using
a ﬁxed time-step state estimator, for example a Kalman ﬁlter,
is extremely difﬁcult. In this paper we describe a continuous-
time state estimation algorithm based on particle ﬁltering that
can handle telemetry arriving at different rates and changes in
system behavior between observations. The system can also
update its model of system behavior to improve its perfor-
mance and to detect gradual degradation of the system.

State estimation is typically done either using discrete di-
agnosis algorithms such as Livingstone [Williams and Nayak,
1996], or more recently, using hybrid system representations.
The discrete approach is well suited for domains where there
are occasional transitions between relatively stable states, and
where sensor readings tend to be very reliable. Unfortunately,

this is not the case for K-9 and discrete approaches have
largely proven unhelpful [Washington et al., 2000]. For this
reason, we use a hybrid system model for state representation
and particle ﬁltering for state estimation.

Probabilistic hybrid automata (PHAs)

[Hofbaur and
Williams, 2002] are commonly used for representing hybrid
systems in diagnosis and state estimation (other representa-
tions have similar expressive power). In the PHA model, a
system consists of a set of discrete modes and a set of contin-
uous variables. For each mode, a set of differential equations
describes the continuous behavior in that mode. Transitions
between modes are stochastic; a transition matrix represents
the probability of being in state s(cid:1)
at time t + 1 given that
the system is in state s at time t. Transition probabilities may
depend on the values of the continuous variables.

The problem with applying the PHA model (and other sim-
ilar representations) to systems such as K-9 is the assump-
tion that a transition matrix applied at ﬁxed time intervals is
sufﬁcient to describe the evolution of the system. This im-
plies that only one discrete transition can occur per interval,
and also that it doesn’t matter when in the interval the transi-
tion occurs. These assumptions lead to two problems. First,
they make the system model unwieldy as they force a single
transition matrix to represent the effects of multiple discrete
transitions. Second, they reduce efﬁciency by forcing model
steps to be small enough for the assumptions to be reason-
able. The continuous time model avoids these assumptions,
allowing state updates to be performed only when a new ob-
servation arrives or when a signiﬁcant event occurs.

The approach we take is to build a continuous-time model
of the system (see Section 2). As before, there is a differen-
tial equation model governing the continuous system behav-
ior for every discrete system mode, but now the discrete state
transitions are represented as a continuous-time Markov jump
process. That is, we keep a set of transition probabilities for
each discrete mode, as well as the distribution over how long
the system will remain in the mode before transitioning out.
As we said above, we use particle ﬁltering to track the sys-
tem state (see Section 4). For efﬁciency, and to make it possi-
ble to track large systems, we use Rao-Blackwellized particle
ﬁlters [Doucet et al., 2000a], in which the discrete mode is
sampled, just as in a standard particle ﬁlter, but the continu-
ous state is tracked using an unscented Kalman-Bucy ﬁlter.

This work addresses the key challenge that many mode
changes on the rover, as with most systems, are not observ-
able directly. Commanded changes such as moving from

stopped to driving are observable, but the occurrence of
faults, and other mode transitions triggered by the environ-
ment or the continuous state, do not produce telemetry di-
rectly, and can only be inferred from their effects on the con-
tinuous system behavior. The algorithm we have developed to
address these difﬁculties, the continuous-time particle ﬁlter,
is described in Section 5.

One major advantage of the continuous-time approach is
that it allows resource-bounded computation. Although the
cost of a single update may be higher than that of a discrete
update (because of the Kalman-Bucy ﬁlter equations), com-
putation need only be performed when new observations ar-
rive. Indeed, if computation is limited, observations can even
be ignored until sufﬁcient computation is available to update
the estimate. For discrete approaches, an update must be per-
formed for each time step, even if no telemetry has arrived.

In Section 6, we discuss how the parameters of the
continuous-time Markov jump process can be learned by ob-
serving the system. Section 7 applies the approach to the sim-
ulated rover model presented in Section 3.

2 Continuous-time hybrid-state processes
A continuous-time hybrid-state process consists of interact-
ing discrete-state and continuous-state random variables.

The discrete-state variables Zt evolve according to dynam-
ics described by continuous-time Markov jump processes.
A Markov jump process is a random variable Z t that is
parametrized by time t ∈ [0,∞). Zt starts in an initial state
z and remains in this state for a random amount of time be-
fore it makes a transition to a different state. The time that Z t
stays in a particular state is exponentially distributed, due to
the Markovian property of the process.

Mathematically, a Markov jump process Z t is character-

ized by its intensity matrix
2
64

Q =

−q1
q12
q12 −q2
...
...

3
75

. . .
. . .
.. .

where
qij = lim
∆t→0

P{Zt+∆t = j|Zt = i}

∆t

, i (cid:3)= j and qi =

(cid:1)

j(cid:3)=i

qij

in which qij is the transition rate that deﬁnes the probability
per time unit that the system makes a transition from state
i to state j and qi is the total transition rate out of state i.
The amount of time that the process stays in state i is dis-
tributed according to the exponential distribution f i(t) =
qi exp(−qit). When the process leaves state i, it enters the
j qij if i (cid:3)= j and 0 oth-
next state j with probability Pij = qijP
erwise. In this paper, we assume that the process is stationary,
i.e. the intensity matrix is the same at all times.

In recent work,

[Nodelman et al., 2002] introduced
continuous-time Bayesian networks (CTBNs) as a way to fac-
torize Markov jump processes over discrete-valued variables.
Our work extends the inference to hybrid-state processes.

The instantiation of Zt corresponds to a unique set of equa-
tions that govern the dynamics of the continuous variables
Xt. Let τ1, τ2, . . . denote the times at which Zt changes

Stuck

Terrain

Left front wheel

Left middle wheel

Left bogey angle

Stuck

Left rocker angle

Solar energy level

Left back wheel

Sunny

Rainy

Speed

Ground stickiness

Stuck

Stuck

Right back wheel

Rover roll angle

Stuck

Right rocker angle

Right middle wheel

Right bogey angle

Rockiness

Right front wheel

Stuck

Figure 1: Rover CTBN. Note that a CTBN may contain cycles.

value. Let zi denote the value of Zt in the interval [τi, τi+1].
During the time span of [τi, τi+1] the variables Xt follow
nonlinear dynamics given by

d
dt

Xt = Fzi(Xt, Ut) + Wt

(1)

and the observations Yt are given by

Yt = Hzi(Xt, Ut) + Vt

(2)
where Wt ∼ N (0, Φt) and Vt ∼ N (0, Ψt) are the respective
process and measurement noise, and U t is the control input.
In this work, we assume that some variables in Z t may be
observed, but only at random discrete points in time. On the
other hand, Yt are observed more frequently, but as noisy
measurements of the true continuous state X t.

3 A continuous-time rover model
As a testbed, we use a continuous-time rover model. The
rover employs a rocker-bogey suspension system for its six
wheels. On each side, the front two wheels are attached to
the bogey, which pivots around its center. The center of the
bogey is attached to the rocker, which is attached to the main
rover chassis and the rear wheels. Motion from the bogey
can cause the rocker to pivot around the differential axle, a
mechanism that centers the rover chassis between the left and
right sets of wheels. From the rocker angles and the bogey
angles, we can infer the rover orientation and thus the wheel
heights relative to the rover chassis.

We are interested in reasoning about the rover wheel dy-
namics under the effects of weather, terrain, ground condi-
tions and faulty wheel behavior. The model consists of 20
state variables and 4 transient variables. There are two groups
of state variables: 5 that model background conditions and
15 that directly model the rover. The background variables
model environmental factors such as weather, terrain, ground
rockiness and ground stickiness, while the rover variables
model the solar energy level available to the rover, the rover
speed, the wheel heights, the wheel stuck conditions, and the
rover roll angle. The CTBN structure is shown in Figure 1.

The model comprises 11 binary and ternary discrete vari-
ables and 9 continuous variables. The only observable dis-
crete variables are Sunny, Rainy, T errain and Rockiness.
The observation of these variables at sparse time points cor-
responds to situations when the rover takes measurements of
its surroundings, to plan navigation or to manage power con-
sumption. The wheel stuck states are never observed and are

inferred from observations of the continuous wheel variables.
The wheel stuck condition is more frequently induced when
the rover travels at high speed and when the ground is rocky.
The rover dynamics are adapted from the kinematic anal-
ysis in [Hacot, 1998]. The front wheels receive an input
proportional to some height perturbation induced by the ter-
rain. The change in the middle wheel height is proportional
to the difference between the front and middle wheel height,
taken relative to the rover body which experiences roll (tor-
sional rotation) on uneven terrain. To illustrate the system’s
nonlinearity, the equation for the middle wheel height is:
(cid:3)
t + β)

t ) − l2 sin(θB
where Θt is the roll angle, θB
is the bogey angle, and l1, l2
and β are constant parameters of the physical rover model.
The change in the back wheel is deﬁned in a similar manner.
For each wheel, the proportionality constant κ t is dependent
on the speed and whether the wheel is stuck. The speed is
affected by the availability of solar energy and by ground sur-
face characteristics such as stickiness and rockiness.

(cid:2)
l1 sin(θB

t = κt cos(Θt)
zM

d
dt

(3)

t

Noisy measurements of the continuous variables that
model the bogey and rocker angles are observed more fre-
quently than the discrete observations. The bogey angle is
determined from the front and middle wheel heights. The
rocker angle depends on the middle and back wheel heights
as well as the bogey angle.

4 Preliminaries
Let the hybrid system state be represented as S t = {Zt, Xt}.
We denote y1:t as the history of observations from time 1 to
time t. The aim is to track the probability p(S t|y1:t) that any
given state may be the true system state, conditioned upon the
sequence of past observations y1:t. This probability distribu-
tion, also referred to as the belief state, can be recursively
estimated given the previous belief state p(St−1|y1:t−1) and
the current observation yt:

p(St|y1:t) = p(yt|St) p(St|y1:t−1)
p(yt|y1:t−1)

(4)
p(St|St−1)p(St−1|y1:t−1)dSt−1.
where p(St|y1:t−1) =
However, the computation of this integral leads to intractabil-
ity in all but the smallest conditional linear Gaussian models.
As a result, one must resort to approximate inference methods
to compute the updated belief state.

(cid:4)

4.1 Particle ﬁltering
Particle ﬁltering (PF) [Doucet et al., 2000b] approximates (4)
by a discrete sum of particles or samples of possible states
drawn from that distribution:

p(St|y1:t) ≈ N(cid:1)

1
N

δ(s(i)
t )

i=1

(5)
where δ(·) denotes the Dirac delta function. Since it is difﬁ-
cult to sample from p(St|y1:t) directly, importance sampling
is used, in which particles are drawn from a more tractable
proposal distribution, then each particle is weighted to ac-
count for this bias. Since variance increases as the state space

increases, the number of particles required to achieve decent
accuracy increases as well, thus making PF an expensive so-
lution for tracking complex, high-dimensional systems.

Rao-Blackwellization [Doucet et al., 2000a] is a technique
that improves PF by analytically marginalizing out some of
the variables. This method is especially applicable to our do-
main since the structure of the rover model can be factorized:

(cid:4)

p(St|y1:t−1) =

p(Xt|Zt, Xt−1)p(Zt|St−1)p(St−1|y1:t−1)dSt−1 (6)
Hence, Rao-Blackwellized Particle Filtering (RBPF) can be
used to sample only from the (lower-dimensional) discrete
distribution and the continuous distribution can subsequently
be computed using the Kalman ﬁlter.

4.2 The unscented Kalman-Bucy ﬁlter
The Kalman ﬁlter [Grewal and Andrews, 2001] is an efﬁcient,
recursive method that ﬁnds the least-square estimate of the
state of a discrete-time, linear stochastic process with Gaus-
sian noise. The Kalman-Bucy ﬁlter is the continuous-time
counterpart to the discrete-time Kalman ﬁlter. It uses a differ-
ential equation to generate an estimate ˆxt of the continuous
state, and generates from that an estimate ˆyt of the observa-
tion. It also generates Pt, the covariance of the state estimate
error xt − ˆxt, and Rt, the covariance of yt − ˆyt. The update
of Pt also uses a differential equation. Given these estimates,
the probability of the observation y t is given by a normal dis-
tribution with mean ˆyt and covariance Rt.

The Kalman-Bucy ﬁlter assumes a linear transition model
and observation model. To handle nonlinear models, we
adopt an approach similar to the discrete-time unscented
Kalman ﬁlter (UKF) [Wan and van der Merwe, 2000] and
extend the Kalman-Bucy ﬁlter by applying the unscented
transformation [Julier and Uhlman, 2002] to the state esti-
mation. Thus, the unscented Kalman-Bucy ﬁlter (UKB) is a
continuous-time ﬁlter that allows for nonlinear models. UKB
is used in RBPF as an approximation to the Kalman-Bucy ﬁl-
ter. This approximation introduces bias and does not provide
the theoretical guarantee that variance is reduced in RBPF.

5 The continuous-time particle ﬁlter
In the continuous-time particle ﬁlter (CTPF), we perform an
update each time a discrete observation is received.
If no
discrete-state process is observed, updates can be triggered
by an asynchronous request for update. Technically, this is
treated the same way as observing the discrete-state process
with a vacuous observation. If the new observation is received
at time tn and the previous observation was at time t n−1, an
update means generating particles of the state at t n, using the
particles generated at tn−1. We assume that observations of
the discrete-state processes are sparse and intermittent. We
make no assumption that observations come at regular inter-
vals or that the observations coincide with any discrete tran-
sitions. Let ζn be the observation at time tn. We may observe
ζn as the entirety of Ztn, or ζn may be an assignment of val-
ues to a subset of the variables in Ztn, or ζn may be vacuous,
assigning values to none of the variables. In any case, we will

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

23:
24:
25:

26:
27:
28:

say that a value z agrees with the observation ζ n if it assigns
the same value to all variables assigned by ζn.

At a high level, the CTPF works as follows. When up-
dating from tn−1 to tn, we sample a trajectory from the
Markov jump process for the time interval [t n−1, tn]. This
results in a set of time points τ0 = tn−1, τ1, . . . , τk = tn
at which Z changes value. Let zi denote the value of Z in
interval [τi, τi+1]. We then perform Rao-Blackwellization to
obtain updates for the continuous variables. We go through
the sequence of intervals, starting at [τ0, τ1] and ending at
[τk−1, τk], propagating our estimate of the mean and variance
of the continuous variables X using UKB.
In the time pe-
riod [τi, τi+1], the continuous dynamics are governed by the
value zi. As we propagate the continuous variables, we also
compute the probability of the trajectory, which is used as
the particle weight. We repeat this process to obtain a set of
weighted particles at time tn, and resample from these to get
a new set of unweighted particles.

The behavior of the Markov jump process governing Z is
characterized by the intensity matrix Q Z. The process of gen-
erating a trajectory of the Markov jump process from time
tn−1 to tn is as follows. For each particle at time tn−1, let z0
be the value of Z in that particle. Set τ0 = tn−1 and i = 0.
We use the intensity matrix to sample a time from the ex-
ponential distribution fzi(t) = qzi exp(−qzi t) to obtain the
length of time γ until Z next changes value. If τ i+γ < tn, we
sample zi+1 from the intensity matrix, and continue to loop,
setting τi+1 = τi + γ. Otherwise, we know that Z = zi at tn.
If this value agrees with the evidence ζn, the particle is kept,
otherwise it is rejected.

Once we have the jump process, we proceed to prop-
agate the continuous variables X through the time points
τ0, . . . , τk. Since there may be many observations of the con-
tinuous variables in the interval [τi, τi+1], when we propagate
them from τi to τi+1, we need to consider all the observations
between those times. Let δ0 be τi, δ(cid:1) be τi+1 and δ1, . . . , δ(cid:1)−1
be the time points at which continuous observations were
received. We loop through the intervals [δ j, δj+1] in turn.
For each interval, we propagate the mean and variance of
the continuous variables forward using UKB. We also obtain
the probability of the observation at time δ j+1, given by the
predictive density p(yδj+1|yδ1:δj ) = N (yδj+1; ˆyδj+1 , Rδj+1)
where ˆyδj+1 and Rδj+1 are the estimate and covariance of
yδj+1 under UKB. We then multiply the probability of all the
observations together over all time points δ j in [τi, τi+1] and
over all τi to obtain the probability of the sequence of obser-
vations. This becomes the weight of the particle.

We repeat this procedure for each of the M particles from
time tn−1. Each particle consists of a candidate Markov jump
process for each discrete variable in Zt, and the mean and
covariance for the continuous variables in X t. The Markov
jump process candidates that are far from the ground truth
will lead to poor tracking of the continuous variables due to
incorrect parametrization of the continuous-state dynamics.
Particles associated with the ill-ﬁt Markov jump processes
will have low weights and will be resampled out.
The pseudocode for CTPF is shown in Figure 2.

let M be the number of particles
let N be the number of discrete observations
for n = 1 to N do

Dynamics propagation
for m = 1 to M do

let ζn be the nth observation
let tn be the observed time of ζn
Simulate a Markov jump process
let i = 0, τ0 = tn−1
let z0 be the value of Z in the mth particle at time tn−1
while τi < tn do

Sample γ from the PDF f(t) = qzi exp(−qzi t)
if τi + γ < tn then
let τi+1 = τi + γ
Sample zi+1 using the intensity matrix QZ

else
i ← i + 1

let τi+1 = tn; let z(m) = zi

if z(m) disagrees with ζn, reject the particle
let k = i
Propagate continuous variables
let ˆxtn−1 and Ptn−1 be the mean and covariance of
the continuous variables in the mth particle at time tn−1
let w(m) = 1
for i = 0 to k − 1 do

let δ1, . . . , δ(cid:1)−1 be the times of
continuous observations in [τi, τi+1]
let δ0 = τi, δ(cid:1) = τi+1
for j = 0 to (cid:8) − 1 do

Compute ˆxδj+1, Pδj+1 , ˆyδj+1 and Rδj+1
using UKB beginning with ˆxδj and Pδj ,
under the dynamics determined by z i
w(m) ← w(m) × N (yδj+1 ; ˆyδj+1 , Rδj+1)

let ˆx(m) and P (m) be ˆxtn and Ptn

29:
30:
31: Resampling
32: Normalize w(m) ← w(m)
33:

P

w(m)

Select M new particles where the probability of
(z(m), ˆx(m), P (m)) is w(m)

Figure 2: Continuous-time particle ﬁlter

6 Learning
In real-world applications, we may not know the exact
parametrization of Zt. Instead, we may have a prior distri-
bution over the intensity matrices Q Z and need to ﬁne tune
our estimate of QZ as observations about Zt are made.

In this case, we model QZ as a random variable and we
attribute a probability distribution φ(Q Z) over the space of
admissible intensity matrices, QZ. Since the observations are
exponentially distributed, we model φ(Q Z) as a Gamma dis-
tribution since Gamma and exponential are conjugates.
to qij ∼ Γ

Assume that the elements of QZ are distributed according
. The prior distribution QZ is given by:

, αij

(cid:5)

(cid:6)

1
βi

φ(QZ) ∝ m(cid:7)

(cid:7)

i=1

j(cid:3)=i

qαij−1

ij

e−qij βi

(7)

Upon the arrival of a discrete-state observation ζ n, we
simulate a Markov jump process J over the time interval
[tn−1, tn]. From J, we can compute the likelihood of Q Z:

L(QZ) =

ij e−qij Ri
qNij

(8)

m(cid:7)

(cid:7)

i=1

j(cid:3)=i

m(cid:7)

(cid:7)

i=1

j(cid:3)=i

where Nij is the number of transitions from state i to j in the
process J and Ri is the time spent in state i during process J.
From this, we update the QZ-distribution

ψ(QZ) = L(Q)φ(Q) =

q[Nij +αij ]−1

ij

e−qij [Ri+βi]

1

(cid:6)

(cid:5)

Ri+βi

, Nij + αij

(9)
Upon the arrival of the next discrete-state observation, we
sample qij ∼ Γ
to get an instantiation
for QZ that reﬂects the updated distribution.
The learning procedure is integrated into algorithm 2 as
follows: we sample QZ ∼ φ(QZ) right before line (8) and
perform the QZ-distribution update, as prescribed in Equa-
tions (7)-(9), after line (20) of the pseudocode.

This procedure may seem like magic. After all, we are
using statistics generated from simulating from Q Z itself to
obtain an updated estimate of QZ. The reason it works is that
not all QZs from the previous set of particles have the same
chance of being accepted into the next set of particles. First of
all, if the trajectory disagrees with the discrete observation ζ n,
it will be rejected. Second, the trajectories will be weighted
by the probability of the continuous observations, and these
weighted particles will be resampled. A trajectory, and thus a
new intensity matrix, that agrees well with the observations,
is more likely to be kept.
In this way, the set of intensity
matrices that are kept will be inﬂuenced by the observations,
both discrete and continuous.

Other learning approaches have been explored in [Nodel-
man et al., 2003] for parameter learning of CTBNs, and
in [Bladt and Sørensen, 2003] for statistical inference of dis-
cretely observed Markov jump processes. However, these ap-
proaches are ofﬂine learning approaches where the sequence
of discrete-time observations are assumed known a priori to
the learning process. Our approach integrates online learn-
ing into the particle ﬁltering framework, where we also take
advantage of the continuous observations in eliminating poor
candidates for QZ. This advantage allows for relatively quick
convergence to the neighborhood of the true Q Z value.

7 Experimental results
We evaluate the performance of the CTPF algorithm on a sim-
ulated small model and the rover model. The small model
consists of one discrete ternary variable, two continuous state
variables, and two continuous observation variables. Depend-
ing on the state of the discrete-valued process, the continuous
behavior can be either linear, polynomial or sinusoidal. The
low dimensionality of the small model makes it feasible to
compare against the discrete-time particle ﬁlter and to evalu-
ate the performance of the learning procedure.

Figure 3 shows an experiment comparing CTPF to the
discrete-time particle ﬁlter (DTPF). The results shown are av-
eraged over 50 runs. The ﬁrst graph shows the performance
of CTPF with 10 particles and 55 updates, comparing the es-
timate of one of the continuous state variables produced by
CTPF with the actual value of that variable. While CTPF up-
dates the discrete variables at infrequent intervals, DTPF per-
forms an update at equally-spaced intervals ﬁxed by a preset
time granularity. Both algorithms perform continuous infer-
ence, via UKB or UKF, at many intermediate time points, cor-
responding to when the continuous variables are observed.

The plotted points are the estimates at these intermediate
points. From the graph, we see that CTPF is consistently able
to track the variable through the different mode changes. The
second graph (DTPF1) shows DTPF’s performance using the
same number of particles and the same number of updates as
CTPF. We see that DTPF1 has worse tracking throughout, and
does particularly poorly from time 75 to time 100. CTPF out-
performs DTPF because it is able to perform updates precisely
when they are needed.

DTPF is considerably faster than CTPF because it does not
require solving differential equations. This was exacerbated
by a slow Matlab implementation of the Kalman-Bucy ﬁlter,
compared to a well-optimized implementation of the Kalman
ﬁlter. We expect this difference to lessen with a more efﬁcient
implementation of CTPF. Nonetheless, we ran experiments
that allowed DTPF the same running time as CTPF.

We ﬁrst allowed DTPF to use more particles than CTPF,
while maintaining the same number of updates. The results of
DTPF, with 58 particles and 55 updates, are shown as DTPF2,
in the third graph of Figure 3. When the number of updates
is not sufﬁcient for DTPF to track the variables, increasing the
number of particles does not particularly help. We then al-
lowed DTPF to update more frequently than CTPF, but use the
same number of particles. The results, with 10 particles and
584 updates, are shown in the fourth graph and are labeled
DTPF3. The DTPF3 results show a signiﬁcant improvement
in the performance of DTPF, although DTPF still does not per-
form as well as CTPF when the continuous variables are os-
cillating rapidly. Furthermore, we found the variance of the
estimates in DTPF3 to be higher than that produced by CTPF.
Figure 4 shows the results of learning the Q matrix on the
small model. The results were quite promising. The ﬁrst
graph compares the estimation error achieved by CTPF with
learning the Q matrix and CTPF with using the true Q ma-
trix. Both sets of CTPF experiments used 10 particles. We
see that the error of CTPF with learning comes close to that
of CTPF without learning after 1000 time steps. The second
graph shows the distance of the learned Q matrix from the
true Q matrix over time, computed as the sum of the element-
wise differences between the two matrices. The graph shows
the learned Q matrix converging towards the true Q matrix.

Lastly, we tested the CTPF algorithm on our main applica-
tion, the continuous-time rover model. Despite the nonlin-
earity of the model, CTPF is able to track the wheel heights
relatively well with 10 particles. The results of the CTPF in-
ference, averaged over 20 runs, are presented in Figure 5 (one
graph per rover wheel). The tracking error is mainly due to
incorrect estimation of the wheel stuck variable. Whenever

10

0

−10

10

0

−10

10

0

−10

10

0

−10

ctpf

0

10

20

dtpf1

0

10

20

dtpf2

0

10

20

dtpf3

0

10

20

truth

30

truth

30

truth

30

truth

30

40

50

60

70

80

90

100

40

50

60

70

80

90

100

40

50

60

70

80

90

100

40

50
Time

60

70

80

90

100

 

r
o
r
r
e
d
e
g
a
r
e
v
a
−
e
m
T

i

 

r
o
r
r
e
d
e
g
a
r
e
v
a
−
e
m
T

i

40

30

20

10

0

0

4

3.5

3

2.5

2

1.5

1

0

Error from learned Q
Error from true Q

1000

2000

3000

4000

5000
Time

6000

7000

8000

9000

10000

Q error

1000

2000

3000

4000

5000
Time

6000

7000

8000

9000

10000

20
0
−20

20
0
−20

20
0
−20

20
0
−20

20
0
−20

20
0
−20

0

0

0

0

0

0

wheelLF

20

10

truth
30

40

50

60

70

80

90

100

wheelLM
20

10

truth

30

40

50

60

70

80

90

100

wheelLB
20

10

truth
30

40

50

60

70

80

90

100

wheelRF
20

10

truth

30

40

50

60

70

80

90

100

wheelRM
20

10

truth

30

40

50

60

70

80

90

100

wheelRB
20

10

truth

30

40

50
Time

60

70

80

90

100

Figure 3: Estimation results

Figure 4: Learning results

Figure 5: CTPF results on the rover

the CTPF estimate drifts from the ground truth, it is able to
quickly recover from the error and continue to closely track
the wheel heights. The empirical results conﬁrm CTPF’s ap-
plicability for monitoring complex continuous-time systems.

8 Conclusion and future work
Since most real-world systems evolve in continuous time,
CTPF is a natural way to address the time granularity problem
that is often associated with systems that have components
that evolve at different rates. By performing state estimation
in continuous time, CTPF is also better suited for resource-
bounded computation since inference is no longer performed
at every ﬁxed time step (as in the discrete-time particle ﬁlter).
Our results show that CTPF can effectively track the state of
complex systems such as the rover. Moreover, when compar-
ison is possible with discrete-time approaches, CTPF tracks
the state more accurately. We believe that the approach is an
important step in making recent advances in particle ﬁltering
applicable to a much larger set of continuous-time problems.
An important aspect in hybrid system modelling is the is-
sue of autonomous transitions, in which the continuous state
triggers a discrete transition. We model this in our approach
by making the intensity matrices dependent on the contin-
uous variables. This leads to difﬁculties in that while the
system is in a state, the distribution over the time the sys-
tem remains there may vary. At present we approximate this
by only computing the intensity matrix at updates or when
the discrete state changes. This is consistent with our aim of
resource-bounded computation. We plan to investigate other
alternatives such as resampling from an updated intensity ma-
trix when the current distribution deviates drastically from the
assumed distribution used for sampling.

Other areas of future work include applying this approach
with factored state spaces [Ng et al., 2002] to reduce the com-
plexity of the Kalman-Bucy updates, and implementating this
approach onboard an actual robot.

References
[Bladt and Sørensen, 2003] M. Bladt and M. Sørensen. Sta-
tistical
inference for discretely observed markov pro-
cesses. Technical report, University of Copenhagen, 2003.

[Doucet et al., 2000a] A. Doucet, N. de Freitas, K. Murphy,
and S. Russell. Rao-blackwellised particle ﬁltering for dy-
namic bayesian networks. In Proc. of the 16th UAI, 2000.
[Doucet et al., 2000b] A. Doucet, S. Godsill, and C. An-
drieu. On sequential monte carlo sampling methods for
bayesian ﬁltering. Statistics and Computing, 2000.

[Grewal and Andrews, 2001] M. S. Grewal and A. P. An-
drews. Kalman Filtering: Theory and Practice Using
MATLAB. Wiley-Interscience, 2001.

[Hacot, 1998] H. Hacot. Analysis and traction control of a
rocker-bogie planetary rover. Master’s thesis, MIT, 1998.
[Hofbaur and Williams, 2002] M. W. Hofbaur and B. C.
Williams. Hybrid diagnosis with unknown behaviour
modes. In Proc. of the 13th Intl. Workshop on Principles
of Diagnosis, 2002.

[Julier and Uhlman, 2002] S. Julier and J. Uhlman. The
In Proc. of the IEEE

scaled unscented transformation.
American Control Conference, 2002.

[Ng et al., 2002] B. Ng, L. Peshkin, and A. Pfeffer. Factored
particles for scalable monitoring. In Proc. of the 18th UAI,
2002.

[Nodelman et al., 2002] U. Nodelman, C. R. Shelton, and
D. Koller. Continuous time bayesian networks. In Proc. of
the 18th UAI, 2002.

[Nodelman et al., 2003] U. Nodelman, C. R. Shelton, and
D. Koller. Learning continuous time Bayesian networks.
In Proc. of the 19th UAI, 2003.

[Wan and van der Merwe, 2000] E. Wan and R. van der
Merwe. The unscented kalman ﬁlter for nonlinear esti-
mation. In Proc. of IEEE Symposium, 2000.

[Washington et al., 2000] R. Washington, K. Golden, and
J. L. Bresina. Plan execution, monitoring, and adaptation
for planetary rovers. Electron. Trans. Artif. Intell., 2000.

[Willeke and Dearden, 2004] T. Willeke and R. Dearden.
Building hybrid rover models: Lessons learned. In Proc. of
the 15th Intl. Workshop on Principles of Diagnosis, 2004.
[Williams and Nayak, 1996] B. C. Williams and P. P. Nayak.
A model-based approach to reactive self-conﬁguring sys-
tems. In Proc. of the 13th AAAI and the 8th IAAI, 1996.


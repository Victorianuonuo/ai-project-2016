Stepwise Nearest Neighbor Discriminant Analysis∗

Xipeng Qiu and Lide Wu

Media Computing & Web Intelligence Lab

Department of Computer Science and Engineering

Fudan University, Shanghai, China

xpqiu,ldwu@fudan.edu.cn

Abstract

Linear Discriminant Analysis (LDA) is a popu-
lar feature extraction technique in statistical pat-
tern recognition. However, it often suffers from
the small sample size problem when dealing with
the high dimensional data. Moreover, while LDA
is guaranteed to ﬁnd the best directions when each
class has a Gaussian density with a common co-
variance matrix, it can fail if the class densities
are more general.
In this paper, a new nonpara-
metric feature extraction method, stepwise nearest
neighbor discriminant analysis(SNNDA), is pro-
posed from the point of view of the nearest neigh-
bor classiﬁcation. SNNDA ﬁnds the important
discriminant directions without assuming the class
densities belong to any particular parametric fam-
ily. It does not depend on the nonsingularity of the
within-class scatter matrix either. Our experimental
results demonstrate that SNNDA outperforms the
existing variant LDA methods and the other state-
of-art face recognition approaches on three datasets
from ATT and FERET face databases.

Introduction

1
The curse of high-dimensionality is a major cause of the
practical limitations of many pattern recognition technolo-
gies, such as text classiﬁcation and object recognition.
In
the past several decades, many dimensionality reduction tech-
niques have been proposed. Linear discriminant analysis
(LDA) [Fukunaga, 1990] is one of the most popular super-
vised methods for linear dimensionality reduction. In many
applications, LDA has been proven to be very powerful.

The purpose of LDA is to maximize the between-class scat-
ter while simultaneously minimizing the within-class scatter.
It can be formulated by Fisher Criterion:
JF (W ) = W T SbW
W T SwW

(1)

,

where W is a linear transformation matrix, Sb is the between-
class scatter matrix and Sw is the within-class scatter matrix.
∗The support of NSF of China (69935010) and (60435020) is

acknowledged.

A major drawback of LDA is that it often suffers from the
small sample size problem when dealing with the high dimen-
sional data. When there are not enough training samples, Sw
may become singular, and it is difﬁcult to compute the LDA
vectors. For example, a 100 × 100 image in a face recog-
nition system has 10000 dimensions, which requires more
than 10000 training data to ensure that Sw is nonsingular.
Several approaches[Liu et al., 1992; Belhumeur et al., 1997;
Chen et al., 2000; Yu and Yang, 2001] have been proposed
to address this problem. A common problem with all these
proposed variant LDA approaches is that they all lose some
discriminative information in the high dimensional space.

Another disadvantage of LDA is that it assumes each class
has a Gaussian density with a common covariance matrix.
LDA guaranteed to ﬁnd the best directions when the distri-
butions are unimodal and separated by the scatter of class
means. However, if the class distributions are multimodal
and share the same mean, it fails to ﬁnd the discriminant
direction[Fukunaga, 1990]. Besides, the rank of Sb is c − 1,
where c is the number of classes. So the number of extracted
features is, at most, c − 1. However, unless a posteriori prob-
ability function are selected, c − 1 features are suboptimal in
Bayes sense, although they are optimal with regard to Fisher
criterion [Fukunaga, 1990].

In this paper, a new feature extraction method, step-
wise nearest neighbor discriminant analysis(SNNDA), is pro-
posed. SNNDA is a linear feature extraction method in or-
der to optimize nearest neighbor classiﬁcation (NN). Near-
est neighbor classiﬁcation [Duda et al., 2001] is an efﬁcient
method for performing nonparametric classiﬁcation and of-
ten used in the pattern classiﬁcation ﬁeld, especially in ob-
ject recognition. Moreover, the NN classiﬁer has a close
relation with the Bayes classiﬁer. However, when nearest
neighbor classiﬁcation is carried out in a high-dimensional
feature space, the nearest neighbors of a point can be very
far away, causing bias and degrading the performance of the
rule [Hastie et al., 2001]. Hastie and Tibshirani [Hastie and
Tibshirani, 1996] proposed a discriminant adaptive nearest
neighbor (DANN) metric to stretch the neighborhood in the
directions in which the class probabilities don’t change much,
but their method also suffers from the small sample size prob-
lem.

SNNDA can be regarded as an extension of nonparametric
discriminant analysis[Fukunaga and Mantock, 1983], but it

doesn’t depend on the nonsingularity of the within-class scat-
ter matrix. Moreover, SNNDA ﬁnds the important discrimi-
nant directions without assuming the class densities belong to
any particular parametric family.

The rest of the paper is organized as follows: Section 2
gives the review and analysis of the current existing variant
LDA methods. Then we describe stepwise nearest neighbor
discriminant analysis in Section 3. Experimental evaluations
of our method, existing variant LDA methods and the other
state-of-art face recognition approaches are presented in Sec-
tion 4. Finally, we give the conclusions in Section 5.

2 Review and Analysis of Variant LDA

Methods

The purpose of LDA is to maximize the between-class scatter
while simultaneously minimizing the within-class scatter.

The between-class scatter matrix Sb and the within-class

scatter matrix Sw are deﬁned as

Sb =

Sw =

pi(mi − m)(mi − m)T

piSi,

(2)

(3)

c(cid:88)
c(cid:88)

i=1

i=1

(cid:80)c

where c is the number of classes; mi and pi are the mean
vector and a priori probability of class i, respectively; m =
i=1 pimi is the total mean vector; Si is the covariance ma-
LDA method tries to ﬁnd a set of projection vectors W ∈

trix of class i.
RD×d maximizing the ratio of determinant of Sb to Sw,

W = arg max
W

|W T SbW|
|W T SwW| ,

(4)

From Eq.(4), the transformation matrix W must be consti-
w Sb corresponding to its ﬁrst

where D and d are the dimensionalities of the data before and
after the transformation respectively.
tuted by the d eigenvectors of S−1
d largest eigenvalues [Fukunaga, 1990].
However, when the small sample size problem occurs, Sw
becomes singular and S−1
w does not exist. Moreover, if the
class distributions are multimodal or share the same mean (for
example, the samples in (b),(c) and (d) of Figure 2), it can
fail to ﬁnd the discriminant direction[Fukunaga, 1990]. Many
methods have been proposed for solving the above problems.
In following subsections, we give more detailed review and
analysis of these methods.

2.1 Methods Aimed at Singularity of Sw
In recent years, many researchers have noticed the problem
about singularity of Sw and tried to overcome the computa-
tional difﬁculty with LDA.

To avoid the singularity of Sw, a two-stage PCA+LDA ap-
proach is used in [Belhumeur et al., 1997]. PCA is ﬁrst used
to project the high dimensional face data into a low dimen-
sional feature space. Then LDA is performed in the reduced
PCA subspace, in which Sw is non-singular. But this method

is obviously suboptimal due to discarding much discrimina-
tive information.

Liu et al. [Liu et al., 1992] modiﬁed Fisher’s criterion by
using the total scatter matrix St = Sb + Sw as the denom-
inator instead of Sw.
It has been proven that the modiﬁed
criterion is exactly equivalent to Fisher criterion. However,
when Sw is singular, the modiﬁed criterion reaches the max-
imum value, namely 1, for any transformation W in the null
space of Sw. Thus the transformation W cannot guarantee the
maximum class separability |W T SbW| is maximized. Be-
sides, this method still needs to calculate an inverse matrix,
which is time consuming. Chen et al. [Chen et al., 2000]
suggested that the null space spanned by the eigenvectors of
Sw with zero eigenvalues contains the most discriminative in-
formation. A LDA method (called NLDA)in the null space of
Sw was proposed. It chooses the projection vectors maximiz-
ing Sb with the constraint that Sw is zero. But this approach
discards the discriminative information outside the null space
of Sw. Figure 1(a) shows that the null space of Sw probably
contains no discriminant information. Thus, it is obviously
suboptimal because it maximizes the between-class scatter in
the null space of Sw instead of the original input space. Be-
sides, the performance of the NLDA drops signiﬁcantly when
N − c is close to the dimension D, where N is the number
of samples and c is the number of classes. The reason is that
the dimensionality of the null space is too small in this situ-
ation and too much information is lost [Li et al., 2003]. Yu
et al. [Yu and Yang, 2001]proposed a direct LDA (DLDA)
algorithm, which ﬁrst removes the null space of Sb. They as-
sume that no discriminative information exists in this space.
Unfortunately, it be shown that this assumption is incorrect.
Fig.1(b) demonstrates that the optimal discriminant vectors
do not necessarily lie in the subspace spanned by the class
centers.

Figure 1: (a) shows that the discriminant vector (dashed line)
of NLDA contains no discriminant information. (b) shows
that the discriminant vector (dashed line) of DLDA is con-
strained to pass through the two class centers m1 and m2.
But according to the Fisher criteria, the optimal discriminant
projection should be solid line (both in (a) and (b)).

2.2 Methods Aimed at Limitations of Sb
When the class conditional densities are multimodal, the class
separability represented by Sb is poor. Especially in the case
that each class shares the same mean, it fails to ﬁnd the dis-
criminant direction because there is no scatter of the class
means[Fukunaga, 1990].

−10−8−6−4−20246810−0.2−0.15−0.1−0.0500.050.10.150.2(a)Class 1 Class 2 NLDA LDA −1.5−1−0.500.511.5−0.2−0.15−0.1−0.0500.050.10.150.2(b)Class 1 Class 2 m1 m2 DLDA LDA Notice the rank of Sb is c − 1, so the number of extracted
features is, at most, c − 1. However, unless a posteriori prob-
ability function are selected, c − 1 features are suboptimal in
Bayes sense, although they are optimal with regard to Fisher
criterion [Fukunaga, 1990].

In fact,

if classiﬁcation is the ultimate goal, we need
only estimate the class density well near the decision
boundary[Hastie et al., 2001].

Fukunaga and Mantock [Fukunaga and Mantock, 1983]
presented a nonparametric discriminant analysis (NDA) in an
attempt to overcome these limitations presented in LDA. In
nonparametric discriminant analysis the between-class scat-
ter Sb is of nonparametric nature. This scatter matrix is gen-
erally full rank, thus loosening the bound on extracted fea-
ture dimensionality. Also, the nonparametric structure of this
matrix inherently leads to the extracted features that preserve
relevant structures for classiﬁcation. Bressan et al. [Bressan
and Vitri`a, 2003] explored the nexus between nonparametric
discriminant analysis (NDA) and the nearest neighbors (NN)
classiﬁer and gave a slight modiﬁcation of NDA which ex-
tends the two-class NDA to a multi-class version.

Although these nonparametric methods overcomes the lim-
itations of Sb, they still depend on the singularity of Sw(or
ˆSw). The rank of ˆSw must be no more than N − c.
3 Stepwise Nearest Neighbor Discriminant

Analysis

In this section, we propose a new feature extraction method,
stepwise nearest neighbor discriminant analysis(SNNDA).
SNNDA also uses nonparametric between-class and within-
class scatter matrix. But it does not depend on singularity of
within-class scatter matrix and improves the performance of
NN classiﬁer.
3.1 Nearest Neighbor Discriminant Analysis

Criterion

Assuming a multi-class problem with classes ωi(i =
1, . . . , c), we deﬁne the extra-class nearest neighbor for a
sample x ∈ ωi as

xE = {x0 /∈ ωi| ||x0 − x|| ≤ ||z − x||,∀z /∈ ωi}.

(5)
In the same fashion, the set of intra-class nearest neighbors

are deﬁned as

are deﬁned as

xI = {x0 ∈ ωi| ||x0 − x|| ≤ ||z − x||,∀z ∈ ωi}.

(6)
The nonparametric extra-class and intra-class differences

where wn is the sample weight deﬁned as

wn =

n||α

||∆I
n||α + ||∆E

n ||α ,

||∆I

(11)

where α is a control parameter between zero and inﬁnity. This
sample weight is introduced to deemphasize the samples in
the class center and give emphases to the samples near to the
other class. The sample that has a larger ratio between the
nonparametric extra-class and intra-class differences is given
an undesirable inﬂuence on the scatter matrix. The sample
weights in Eq.(11) take values close to 0.5 near the classiﬁca-
tion boundaries and drop to zero as we move to class center.
The control parameter α adjusts how fast this happens. In this
paper, we set α = 6.
n || represents
the distance between the sample xn and its nearest neighbor
in the different classes, and ||∆I
n|| represents the distance be-
tween the sample xn and its nearest neighbor in the same
class. Given a training sample xn, the accuracy of the nearest
neighbor classiﬁcation can be directly computed by examin-
ing the difference

From the Eq.(7) and (8), we can see that ||∆E

Θn = ||∆E

n ||2 − ||∆I

n||2,

(12)
where ∆E and ∆I are nonparametric extra-class and intra-
class differences and deﬁned in Eq.(7) and (8).

If the difference Θn is more than zero, xn will be correctly
classiﬁed. Otherwise, xn will be classiﬁed to the false class.
The larger the difference Θn is, the more accurately the sam-
ple xn is classiﬁed.
Assuming that we extract features by the D × d linear pro-
jection matrix W with a constraint that W T W is an identity
matrix, the projected sample xnew = W T x. The projected
nonparametric extra-class and intra-class differences can be
written as δE = W T ∆E and δI = W T ∆I. So we expect to
n||2 in
ﬁnd the optimal W to make the difference ||δE
the projected subspace as large as possible.

n ||2 −||δI

(cid:99)W = arg max

N(cid:88)

W

n=1

wn(||δE

n ||2 − ||δI

n||2).

(13)

This optimization problem can be interpreted as: ﬁnd the
linear transform that maximizes the distance between classes,
while minimizing the expected distance among the samples
of a single class.

Considering that,

∆E = x − xE,
∆I = x − xI .

(7)
(8)

=

.

matrix are deﬁned as

The nonparametric between-class and within-class scatter

N(cid:88)
N(cid:88)

n=1

ˆSb =

ˆSw =

wn(∆E

n )(∆E

n )T ,

wn(∆I

n)(∆I

n)T ,

(9)

(10)

n=1

n=1

= tr(W T (

wn∆E

n (∆E

n )T )W )

n ||2 − ||δI

n||2)

n ) − NX

n=1

wn(W T ∆E

n )T (W T ∆E

wn(W T ∆I

n)T (W T ∆I
n)

= tr(

wn(W T ∆E

n )(W T ∆E

n )T )

wn(W T ∆I

n)(W T ∆I

n)T )

n=1

wn(||δE

NX
NX
NX
NX
NX

−tr(

n=1

n=1

n=1

and intra-class differences in its current dimensionality. Thus,
we keep the consistency of the nonparametric extra-class and
intra-class differences in the process of dimensionality reduc-
tion.

Figure 3 gives the algorithm of stepwise nearest neighbor

discriminant analysis.

• Give D dimensional samples {x1,··· , xN}, we expect

• Suppose that we ﬁnd the projection matrix (cid:99)W via T

to ﬁnd d dimensional discriminant subspace.

steps, we reduce the dimensionality of samples to dt in
step t, and dt meet the conditions: dt−1 > dt > dt+1,
d0 = D and dT = d.

• For t = 1,··· , T

1. Calculate the nonparametric between-class ˆSt

b and
w in the current dt−1

within-class scatter matrix ˆSt
dimensionality,

2. Calculate the projection matrix(cid:99)Wt,(cid:99)Wt is dt−1×dt
3. Project the samples by the projection matrix (cid:99)Wt,
x =(cid:99)W T
• The ﬁnal transformation matrix(cid:99)W =

(cid:81)T

t × x.

(cid:99)Wt.

matrix.

t=1

Figure 3: Stepwise Nearest Neighbor Discriminant Analysis

3.3 Discussions
SNNDA has an advantage that there is no need to calculate
the inverse matrix, so it is a more efﬁcient and stable method.
Moreover, though SNNDA optimizes the 1-NN classiﬁcation,
it is easy to extend it to the case of k-NN.

However, a drawback of SNNDA is the computational in-
efﬁciency in ﬁnding the neighbors when the original data
space is high dimensionality. A improved method is that PCA
is ﬁrst used to reduce the dimension of data to N −1 (the rank
of the total scatter matrix) through removing the null space of
the total scatter matrix. Then, SNNDA is performed in the
transformed space. Yang et al. [Yang and Yang, 2003] shows
that no discriminant information is lost in this transformed
space.

4 Experiments
In this section, we apply our method to face recognition
and compare it with the existing variant LDA methods and
the other state-of-art face recognition approaches, such as
PCA [Turk and Pentland, 1991], PCA+LDA [Belhumeur et
al., 1997], NLDA [Chen et al., 2000], NDA [Bressan and
Vitri`a, 2003] and Bayesian [Moghaddam et al., 2000] ap-
proaches. All the experiments are repeated 5 times indepen-
dently and the average results are calculated.

4.1 Datasets
To evaluate the robustness of SNNDA, we perform the ex-
periments on three datasets from the popular ATT face

NX

Figure 2: First projected directions of NNDA (solid) and
LDA (dashed) projections, for four artiﬁcial datasets.

−tr(W T (

n=1

n(∆I

wn∆I

n)T )W )
= tr(W T ˆSbW ) − tr(W T ˆSwW )
= tr(W T ( ˆSb − ˆSw)W ),
(14)
where tr(·) is the trace of matrix, ˆSb and ˆSw are the non-
parametric between-class and within-class scatter matrix, as
deﬁned in Eq.(9) and (10).

So Eq.(13) is equivalent to

(cid:99)W = arg max

W

tr(W T ( ˆSb − ˆSw)W ).

(15)

We call Eq.(15) the nearest neighbor discriminant analysis

The projection matrix (cid:99)W must be constituted by the d

criterion(NNDA).
eigenvectors of ( ˆSb − ˆSw) corresponding to its ﬁrst d largest
eigenvalues.

Figure 2 gives comparisons between NNDA and LDA.
When the class density is unimodal ((a)), NNDA is approxi-
mately equivalent to LDA. But in the cases that the class den-
sity is multimodal or that all the classes share the same mean
((b),(c) and (d)), NNDA outperforms LDA greatly.

3.2 Stepwise Dimensionality Reduction
In the analysis of the nearest neighbor discriminant analysis
criterion, notice that we calculate nonparametric extra-class
and intra-class differences (∆E and ∆I) in original high di-
mensional space, then project them to the low dimensional
space (δE = W T ∆E and δI = W T ∆I), which does not ex-
actly agree with the nonparametric extra-class and intra-class
differences in projection subspace except for the orthonor-
mal transformation case, so we have no warranty on distance
preservation. A solution for this problem is to ﬁnd the projec-

tion matrix(cid:99)W by stepwise dimensionality reduction method.

In each step, we re-calculate the nonparametric extra-class

−5−4−3−2−1012345−0.2−0.15−0.1−0.0500.050.10.15(a)Class 1Class 2LDANNDA−8−6−4−202468−1−0.8−0.6−0.4−0.200.20.40.60.81(b)Class 1Class 2LDANNDA−15−10−5051015−15−10−5051015(c)Class 1Class 2LDANNDA−15−10−5051015−5−4−3−2−1012345(d)Class 1Class 2LDANNDAdatabase [Samaria and Harter, 1994] and FERET face
database [Phillips et al., 1998]. The descriptions of the three
datasets are below:
ATT Dataset This dataset is the ATT face database (for-
merly ‘The ORL Database of Faces’), which con-
tains 400 images (112 × 92) of 40 persons, 10 im-
The images are taken at differ-
ages per person.
ent
facial expres-
sions (open/closed eyes, smiling/non-smiling) and fa-
cial details (glasses/no-glasses). Each image is linearly
stretched to the full range of pixel values of [0,255].
Fig.4 shows some face examples in this database. The
set of the 10 images for each person is randomly parti-
tioned into a training subset of 5 images and a test set of
the other 5. The training set is then used to learn basis
components, and the test set for evaluate.

times, varying lighting slightly,

Figure 4: Face examples from ATT database

FERET Dataset 1 This dataset is a subset of the FERET
database with 194 subjects only. Each subject has 3
images: (a) one taken under controlled lighting condi-
tion with a neutral expression; (b) one taken under the
same lighting condition as above but with different fa-
cial expressions (mostly smiling); and (c) one taken un-
der different lighting condition and mostly with a neutral
expression. All images are pre-processed using zero-
mean-unit-variance operation and manually registered
using the eye positions. All the images are normal-
ized by the eye locations and are cropped to the size of
75 × 65. A mask template is used to remove the back-
ground and the hair. Histogram equalization is applied
to the face images for photometric normalization. Two
images for each person is randomly selected for training
and the rest one is used for test.

FERET Dataset 2 This dataset is a different subset of the
FERET database. All the 1195 people from the FERET
Fa/Fb data set are used in the experiment. There are
two face images for each person. This dataset has no
overlap between the training set and the galley/probe set
according to the FERET protocol [Phillips et al., 1998].
500 people are randomly selected for training, and the
remaining 695 people are used for testing. For each test-
ing people, one face image is in the gallery and the other
is for probe. All images are pre-processed by using the
same method in FERET Dataset 1.

4.2 Experimental Results
Fig. 5 shows the rank-1 recognition rates with the different
number of features on the three different datasets. It is shown
that SNNDA outperforms the other methods. The recogni-
tion rate of SNNDA can reach almost 100% on ATT dataset.

The recognition rate of SNNDA have reached 100% on two
FERET dataset surprisedly when the dimensionality of sam-
ples is about 20, while the other methods have poor perfor-
mances in the same dimensionality. Moreover, SNNDA does
not suffer from overﬁtting. Except SNNDA and PCA, the
rank-1 recognition rates of the other methods have a descent
when the dimensionality increases continuously.

Fig. 6 shows cumulative recognition rates on the three dif-
ferent datasets. From it, we can see that none of the cumula-
tive recognition rates can reach 100% except SNNDA.

When dataset contains the changes of lighting condition
(such as FERET Dataset 1), SNNDA also has obviously bet-
ter performance than the others.

Different from ATT dataset and FERET dataset 1, where
the class labels involved in training and testing are the same,
the FERER dataset 2 has no overlap between the training
set and the galley/probe set according to the FERET proto-
col [Phillips et al., 1998]. The ability of generalization from
known subjects in the training set to unknown subjects in the
gallery/probe set is needed for each method. Thus, the result
on FERET dataset 2 is more convincing to evaluate the robust
of each method. We can see that SNNDA also gives the best
performance than the other methods on FERET dataset 2.

A major character, displayed by the experimental results,
is that SNNDA always has a stable and high recognition rates
on the three different datasets, while the other methods have
unstable performances.

5 Conclusion
In this paper, we proposed a new feature extraction method,
stepwise nearest neighbor discriminant analysis(SNNDA),
which ﬁnds the important discriminant directions without as-
suming the class densities belong to any particular paramet-
ric family. It does not depend on the nonsingularity of the
within-class scatter matrix either. Our experimental results
on the three datasets from ATT and FERET face databases
demonstrate that SNNDA outperforms the existing variant
LDA methods and the other state-of-art face recognition ap-
proaches greatly. Moreover, SNNDA is very efﬁcient, accu-
rate and robust. In the further works, we will extend SNNDA
to non-linear discriminant analysis with the kernel method.
Another attempt is to extend SNNDA to the k-NN case.

References
[Belhumeur et al., 1997] P.N. Belhumeur, J. Hespanda, and
D. Kiregeman. Eigenfaces vs. Fisherfaces: Recognition
using class speciﬁc linear projection. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 19(7):711–
720, 1997.

[Bressan and Vitri`a, 2003] M. Bressan

and

Nonparametric
neighbor classiﬁcation.
24:2743C2749, 2003.

discriminant

J. Vitri`a.
nearest
Pattern Recognition Letters,

analysis

and

[Chen et al., 2000] L. Chen, H. Liao, M. Ko, J. Lin, and
G. Yu. A new LDA-based face recognition system which
can solve the small sample size problem. Pattern Recog-
nition, 33(10):1713–1726, 2000.

Figure 5: Rank-1 recognition rates with the different number of features on the three different datasets. (Left: ATT dataset;
Middle: FERET dataset 1; Right: FERET dataset 2)

Figure 6: Cumulative recognition rates on the three different datasets. Left:ATT dataset(the number of features is 39; Mid-
dle:FERET dataset 1 (the number of features is 60); Right: FERET dataset 2 (the number of features is 60)

[Duda et al., 2001] R. O. Duda, P. E. Hart, and D. G. Stork.
Pattern classiﬁcation. Wiley, New York, 2nd edition,
2001.

[Fukunaga and Mantock, 1983] K. Fukunaga and J. Man-
tock. Nonparametric discriminant analysis. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
5:671C678, 1983.

[Fukunaga, 1990] K. Fukunaga.

Introduction to statistical
pattern recognition. Academic Press, Boston, 2nd edition,
1990.

[Hastie and Tibshirani, 1996] T. Hastie and R. Tibshirani.
Discriminant adaptive nearest neighbor classiﬁcation.
IEEE Transactions on Pattern Analysis and Machine In-
telligence, 18:607C616, 1996.

[Hastie et al., 2001] T. Hastie, R. Tibshirani, and J. Fried-
man. The Elements of Statistical Learning. Springer, New
York, 2001.

[Li et al., 2003] H.F. Li, T. Jiang, and K.S. Zhang. Efﬁcient
and robust feature extraction by maximum margin crite-
rion. In Proc. of Neural Information Processing Systems,
2003.

[Liu et al., 1992] K. Liu, Y. Cheng, and J. Yang. A general-
ized optimal set of discriminant vectors. Pattern Recogni-
tion, 25(7):731C739, 1992.

[Moghaddam et al., 2000] B. Moghaddam, T. Jebara, and
A. Pentland. Bayesian face recognition. Pattern Recog-
nition, 33:1771–1782, 2000.

[Phillips et al., 1998] P.J. Phillips, H. Wechsler, J. Huang,
and P. Rauss. The feret database and evaluation proce-
dure for face recognition algorithms.
Image and Vision
Computing, 16(5):295–306, 1998.

[Samaria and Harter, 1994] Ferdinando Samaria and Andy
Harter. Parameterisation of a stochastic model for human
face identiﬁcation. In Proc. of 2nd IEEE Workshop on Ap-
plications of Computer Vision, 1994.

[Turk and Pentland, 1991] M. Turk and A. Pentland. Eigen-
faces for recognition. Journal of Cognitive Neuroscience,
3(1):71–86, 1991.

[Yang and Yang, 2003] J. Yang and J.Y. Yang. Why can
LDA be performed in PCA transformed space? Pattern
Recognition, 36:563–566, 2003.

[Yu and Yang, 2001] H. Yu and J. Yang. A direct LDA al-
gorithm for high-dimensional data with application to face
recognition. Pattern Recognition, 34:2067–2070, 2001.

0510152025303540455000.10.20.30.40.50.60.70.80.91Number of featuresRecognition ratesPCAPCA+LDANLDABayes MLNDASNNDA02040608010012014016018020000.20.40.60.81Number of featuresRecognition ratesPCAPCA+LDANLDABayes MLNDASNNDA02040608010012014016018020000.20.40.60.81Number of featuresRecognition ratesPCAPCA+LDANLDABayes MLNDASNNDA123456789100.20.30.40.50.60.70.80.911.1RankRecognition ratesPCAPCA+LDANLDABayes MLNDASNNDA123456789100.10.20.30.40.50.60.70.80.91RankRecognition ratesPCAPCA+LDANLDABayes MLNDASNNDA123456789100.70.750.80.850.90.9511.05RankRecognition ratesPCAPCA+LDANLDABayes MLNDASNNDA
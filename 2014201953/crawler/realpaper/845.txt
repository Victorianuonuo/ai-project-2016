Boosting Kernel Discriminant Analysis and Its Application to Tissue Classiﬁcation

of Gene Expression Data

Guang Dai & Dit-Yan Yeung

Department of Computer Science and Engineering
Hong Kong University of Science and Technology

Clear Water Bay, Kowloon, Hong Kong

{daiguang,dyyeung}@cse.ust.hk

Abstract

Kernel discriminant analysis (KDA) is one of the
most effective nonlinear techniques for dimension-
ality reduction and feature extraction. It can be ap-
plied to a wide range of applications involving high-
dimensional data, including images, gene expres-
sions, and text data. This paper develops a new al-
gorithm to further improve the overall performance
of KDA by effectively integrating the boosting and
KDA techniques. The proposed method, called
boosting kernel discriminant analysis (BKDA), pos-
sesses several appealing properties. First, like all
kernel methods, it handles nonlinearity in a disci-
plined manner that is also computationally attrac-
tive; second, by introducing pairwise class discrimi-
nant information into the discriminant criterion and
simultaneously employing boosting to robustly ad-
just the information, it further improves the classi-
ﬁcation accuracy; third, by calculating the signiﬁ-
cant discriminant information in the null space of the
within-class scatter operator, it also effectively deals
with the small sample size problem which is widely
encountered in real-world applications for KDA;
fourth, by taking advantage of the boosting and
KDA techniques, it constitutes a strong ensemble-
based KDA framework. Experimental results on
gene expression data demonstrate the promising per-
formance of the proposed methodology.

1 Introduction

Principal component analysis (PCA) and linear discriminant
analysis (LDA) are two classical feature extraction and dimen-
sionality reduction techniques broadly used in many tasks in-
volving high-dimensional data.
It is generally believed that
for pattern classiﬁcation problems such as face recognition
and tissue classiﬁcation of gene expression data, LDA-based
algorithms usually outperform PCA-based ones. The reason
is that the former optimizes the low-dimensional representa-
tion of the objects for classiﬁcation by maximizing the ratio
of the between-class scatter to the within-class scatter, while
the latter simply optimizes object reconstruction without tak-
ing class information into consideration. Many LDA-based
algorithms have been proposed. Among them, as an attrac-
tive approach, some new LDA algorithms [Lu et al., 2003a;

Masip and Vitri`a, 2006] which effectively integrate the boost-
ing and LDA techniques have been developed recently to fur-
ther enhance the classiﬁcation performance.

On the other hand, a major problem of linear subspace
methods such as PCA and LDA is that they fail to extract
nonlinear features representing higher-order statistics. In or-
der to overcome this limitation, kernel dimensionality reduc-
tion techniques such as kernel principal component analy-
sis (KPCA) [Sch¨olkopf et al., 1999] and kernel discriminant
analysis (KDA) [Baudat and Anouar, 2000; Lu et al., 2003b;
Dai and Qian, 2004; Xiong et al., 2005; Dai and Yeung, 2005;
Zheng et al., 2005] have been proposed recently to extend lin-
ear dimensionality reduction techniques to nonlinear versions
by using the kernel trick, demonstrating better performance in
many applications than that obtained using their linear coun-
terparts. The basic idea is to ﬁrst map each input data point
x ∈ Rn into the feature space F via a nonlinear mapping φ
and then apply the corresponding linear dimensionality reduc-
tion algorithm in F . Moreover, similar to their linear counter-
parts, KDA-based methods are generally better than KPCA-
based methods for classiﬁcation tasks. However, KDA-based
algorithms usually suffer from the small sample size problem,
because the number of training examples available is usually
smaller than the dimensionality of the feature space F espe-
cially for high-dimensional data.
In order to overcome this
problem, many approaches have been developed based on dif-
ferent criteria. Recently, more effective solutions [Dai and
Qian, 2004; Zheng et al., 2005] have been proposed to cal-
culate the optimal discriminant vectors in the null space of
the within-class scatter operator where the signiﬁcant discrim-
inant information exists. On the other hand, similar to con-
ventional LDA-based techniques [Lotlikar and Kothari, 2000;
Tang et al., 2005; Dai and Yeung, 2005], the performance of
KDA degrades further due to the following deﬁciencies which
are referred to as non-balanced problems in this paper:

1. For multi-class pattern classiﬁcation problems, the op-
timal criterion based on the conventional between-class
scatter is not directly related to classiﬁcation accuracy.
In particular, the corresponding dimensionality reduc-
tion procedure tends to overemphasize the inter-class dis-
tances of well-separated outlier classes in the input space
at the expense of classes that are close to each other, lead-
ing to signiﬁcant overlap between them.

2. The expression of the average within-class scatter has an
implicit assumption that all classes have the same weight

IJCAI-07

744

for the covariances. In fact, if the class with dominant
covariance is simultaneously an outlier class in the input
space, the within-class scatter will fail to estimate the cor-
rect value for improved classiﬁcation, due to minimizing
the spread of the outlier classes while neglecting the min-
imization of other covariances.

In this paper, we further improve the overall performance
of KDA by proposing a novel KDA algorithm called boosting
KDA (BKDA). The proposed approach effectively integrates
the boosting technique with the most recently developed KDA
algorithms based on the pairwise class discriminant informa-
tion. The BKDA approach employs the boosting technique to
robustly calculate the pairwise class discriminant information
that is integrated into the scatter operators in order to solve the
non-balanced problems of KDA. Although boosting has been
applied to LDA, so far it has not been studied much for KDA.
It is worthwhile to mention here several appealing properties
of the proposed BKDA method:

1. It handles nonlinearity in a disciplined manner that is also

computationally attractive like all kernel methods.

2. By introducing the pairwise class discriminant informa-
tion into the discriminant criterion and simultaneously
employing boosting to robustly adjust the information,
it effectively overcomes the non-balanced problems in
KDA and further increases the classiﬁcation accuracy.

3. It effectively boosts the signiﬁcant discriminant informa-
tion contained in the null space of the within-class scatter
operator and simultaneously deals with the small sam-
ple size problem. However, the algorithm and analysis
presented here can also be applied easily to boost other
discriminant information such as that included in the or-
thogonal complement of the null space of the within-class
scatter operator.

4. It constitutes a strong ensemble-based kernel KDA
framework, taking advantage of both the boosting and
KDA techniques.

To demonstrate the effectiveness of the BKDA method, we
apply the proposed method to effectively extract discriminant
features for the tissue classiﬁcation of gene expression data.
Experimental results conﬁrm that our BKDA method is supe-
rior to several existing dimensionality reduction methods in
terms of classiﬁcation accuracy.

2 Nonlinear Feature Extraction via Kernel

Discriminant Analysis

As a nonlinear extension of LDA, KDA essentially performs
LDA in the feature space F . Note that F may be inﬁnite-
dimensional and it should be regarded as a Hilbert space. As
such, the scatter matrices in the input space for LDA corre-
spond to operators in F for KDA. For any operator A in F , we
let A(0) denote the null space of A, i.e., A(0) = {x | Ax =
0}, and A(0) the orthogonal complement of A(0). Thus,
A(0) ⊕ A(0) = F .

Suppose X denotes a training set of N examples belonging
to C classes, with each example being a vector in Rn. Let
Xi ⊂ X be the ith class containing Ni examples, with xio
i
denoting the ioth example in it.
In addition, each example

∈ X has the corresponding label i ∈ {1, . . . , C}. By
xio
i
the implicit nonlinear mapping φ : Rn → F , the N images
in F can be represented by the set {φ(xio
i )}. The conven-
tional between-class scatter operator Sφ
b , within-class scatter
w, and population scatter operator Sφ
operator Sφ
t can be ex-
(cid:2)
(cid:2)
b = 1
−mφ)(mφ
pressed as: Sφ
w =
i
(cid:2)
(cid:2)
1
C
Ni
i ) − mφ
i )(φ(xio
io=1(φ(xio
b +
i=1
(cid:2)
(cid:2)
N
w = 1
C
i ) − mφ)T , where
i ) − mφ)(φ(xio
Sφ
i=1
i = 1
C
Ni
mφ
io=1 φ(xio
i ).
i=1
We maximize the Fisher criterion below to obtain the optimal
projection directions w in F :

(cid:2)
i ) − mφ
io=1(φ(xio

−mφ)T , Sφ
i )T , Sφ
t = Sφ
(cid:2)

i ) and mφ = 1

i=1 Ni(mφ

Ni

io=1 φ(xio

N

Ni

N

Ni

C

i

N

J φ(w) =

w

wT Sφ
b
wT Sφ
ww

.

(1)

However, in many practical applications, one of the major
problems with KDA is the so-called small sample size problem
with respect to (1), because of the degeneracy of the within-
class scatter operator in F . In general, this problem is solved
by applying techniques such as pseudoinverse, kernel PCA,
and QP decomposition. Recently, some more effective meth-
ods [Dai and Qian, 2004; Zheng et al., 2005] have been de-
veloped to explore the signiﬁcant discriminant information in
the null space of the within-class scatter operator. In general,
a simple and efﬁcient approach of ﬁnding this signiﬁcant dis-
criminant information is to calculate the optimal discriminant
vectors in the intersection subspace Sφ
w(0) with re-
spect to the following modiﬁed criterion:

t (0)

(cid:3)

Sφ

J φ
f (w) = wT Sφ

b

w ((cid:6)w(cid:6) = 1).

(2)

Furthermore, in many situations, nonlinear feature extraction
based on the above criterion (2) indeed further enhances the
overall performance in terms of classiﬁcation accuracy and nu-
merical stability. In addition, it should be pointed out that, ac-
cording to [Chen et al., 2000], for conventional LDA, when the
dimensionality n of the training examples is less than the total
number of examples N , some useful discriminant information
in the null space of the within-class scatter operator will be
lost. This is especially the case when n is less than N − C
because all this discriminant information will be fully dis-
carded. In KDA described above, this situation can be avoided
by choosing the kernel function appropriately, with which the
dimensionality of F nonlinearly mapped from the input space
can become larger than the number of training examples.

3 Boosting Kernel Discriminant Analysis

Learner

3.1 Boosting Kernel Discriminant Analysis
Similar to LDA, KDA also suffers from the non-balanced
problems described in Section 1. Here, we will present the
BKDA algorithm that combines the strengths of the boosting
and KDA techniques and effectively solves the non-balanced
problems of KDA at the same time.

Boosting is a general machine learning meta-algorithm for
improving the accuracy of any given learning algorithm. One
of the most effective boosting algorithms, referred to as Ad-
aBoost, can be used in conjunction with many learning al-
gorithms to improve their performance. Boosting algorithms

IJCAI-07

745

are adaptive in the sense that the classiﬁers built are tweaked
in favor of those instances misclassiﬁed by previous classi-
ﬁers [Freund and Schapire, 1997]. More speciﬁcally, the un-
derlying idea of AdaBoost is based on the sample distribu-
tion, which, in essence, is a measure of how hard to classify
an example. Moreover, it should be notable that for multi-
class problems, a version of AdaBoost called AdaBoost.M2
outperforms AdaBoost.M1 for many real-world applications.
Thus we prefer using AdaBoost.M2 in this paper.
In order
to effectively overcome the non-balanced problems of KDA
and simultaneously form a strong connection between KDA
and AdaBoost.M2, following [Freund and Schapire, 1997;
Lu et al., 2003a], the pairwise class discriminant distribution
is introduced on the basis of the mislabel distribution from Ad-
aBoost.M2. Further, at the tth iteration in AdaBoost.M2, any
pairwise class discriminant distribution dt
i,j between classes
Xi and Xj can be calculated as follows:

j

`PNi

PNj
jo =1 Γt(xjo

´

,

Γt(xio

io=1

1
2
0,

j , i)

dt
i,j =

i , j) +

if i (cid:2)= j;
otherwise.
(3)
Here, the mislabel distribution Γt((cid:3), ∗) measures the extent
of the difﬁculty of discriminating the example (cid:3) from the im-
proper label ∗ on the basis of the previous boosting results.
Obviously, a larger value of dt
i,j intuitively indicates worse
separability between two classes Xi and Xj, further embody-
ing also that both are closer together in F .

To address the ﬁrst non-balanced problem in KDA, we re-
b by a

place the ordinary between-class scatter operator Sφ
weighted between-class scatter operator Sφ
B :

NiNj
N 2

w(dt

i,j)(mφ

i − mφ

j )(mφ

i − mφ

j )T ,

(4)

c−1X

cX

i=1

j=i+1

Sφ

B =

where the weighting function w((cid:3)) is generally chosen to be a
monotonically increasing function of (cid:3). For simplicity, we let
w((cid:3)) = (cid:3) in this paper. Obviously, based on the deﬁnitions of
the pairwise class discriminant distribution and the weighting
function w((cid:3)), it can be seen that classes that are not well sep-
arated in F and thus can potentially impair the classiﬁcation
performance should be more heavily weighted in F .

In addition, a weighting scheme should also be employed to
alleviate the second non-balanced problem, i.e., the negative
effect of outlier classes when estimating the within-class scat-
ter operator. Further, at the tth iteration, we replace the ordi-
nary within-class scatter operator Sφ
w by the weighted within-
class scatter operator Sφ
W as follows:

NiX
CX
(cid:2)
where rt
i,j ) is the relevance-based weight for
class Xi and qt
j(cid:2)=i Γt(xio
i . Let us
highlight some characteristics of Sφ

i , j)) is that for xio
W here:

(cid:2)
j(cid:2)=i w(dt

i ) − mφ

i ) − mφ

iio = w(

i )(φ(xio

rt
iqt
iio

(φ(xio

W =

i )T ,

i =

1
N

io =1

Sφ

(5)

i=1

− By incorporating rt

i in (5), it ensures that the estimated
Sφ
W is only inﬂuenced slightly if class Xi is an outlier
class. This is reasonable since if one class is well sep-
arated from the other classes in F , then whether the
within-class covariance operator of this class in the new
space is compact or not will not have much effect on clas-
siﬁcation [Tang et al., 2005].

− Generally, a larger value of qt
culty of classifying example xio
ing results. As such, qt
ples in the within-class covariance of class Xi.

iio indicates a greater difﬁ-
i w.r.t. the previous boost-
iio emphasizes the difﬁcult exam-

B and Sφ

Based on the AdaBoost.M2 algorithm and the deﬁnitions
of Sφ
W , we propose the BKDA algorithm as detailed
in Fig.1.1 It should be pointed out that the KDA involved in
the BKDA algorithm is to calculate the optimal discriminant
vectors in the null space of the new Sφ
W in order to take ad-
vantage of the signiﬁcant discriminant information in Sφ
W (0),
and the detailed calculation procedure can be found in sub-
section 3.2 below. In addition, discriminant analysis is quite
a strong feature extraction technique for classiﬁcation. As a
result, the boosting process cannot go forward due to the very
small pseudo-loss t. In general, some sampling procedures
are employed to artiﬁcially weaken the corresponding discrim-
inant technique, and, in BKDA, we choose some examples in
each class based on qt
iio to focus on the hardest examples in
each class. For the features extracted by discriminant analy-
sis, a simple nearest neighbor classiﬁer is generally employed
for classiﬁcation. In order to be consistent with the AdaBoost
algorithm, in BKDA, the hypothesis ht((cid:3), ∗) between exam-
ple (cid:3) and class ∗ can be built easily based on the normalized
nearest neighbor classiﬁer, while it gives results identical to
the classical nearest neighbor classiﬁer.

3.2 How to Calculate the Optimal Discriminant

Vectors in the Feature Space F

In what follows, we will describe how to efﬁciently calculate
the signiﬁcant discriminant information in the null space of
the variant Sφ
W in (5). Furthermore, on the basis of the vari-
ants Sφ
B and Sφ
W , we also represent the corresponding popu-
lation scatter operator as Sφ
W . As described in
(cid:3)
Section 2, the optimal discriminant vectors can be obtained in
Sφ

W (0) with respect to the following criterion:

T = Sφ

B + Sφ

T (0)

Sφ

J φ
F (w) = wT Sφ
(cid:3)

B

w ((cid:6)w(cid:6) = 1).

(6)

T (0)

T , since Sφ

t , it is intractable to directly compute Sφ

Sφ
W (0), which requires calculating Sφ
T (0) or Sφ

Furthermore, based on the above criterion, we have to cal-
culate Sφ
T (0) or
Sφ
W (0) ﬁrst. However, the computation of Sφ
W (0) is
quite intractable to some extent, due to the following reasons:
1) unlike Sφ
T (0) by the
eigenanalysis of Sφ
T cannot be explicitly expressed
as Sφ
T = AAT , where A is an operator with the explicit for-
mulation; 2) the direct computation of Sφ
W (0) is very infeasi-
ble, since Sφ
W (0) is in general very large in F ; 3) according
to [Cevikalp et al., 2005], Sφ
W (0) can be indirectly calculated
by F − Sφ
W (0), while,
in F , high computational complexity is involved at the same
time. In order to efﬁciently solve this problem, we provide the
following theorem.

W (0) on the basis of ﬁrst calculating Sφ

1For simplicity, KDA in subsection 3.2 is described on X but it is

essentially very similar to that based on St.

IJCAI-07

746

Input: A set of training examples X = {xio
∈
i
Rn, i = 1, . . . , C, io = 1, . . . , Ni}; a set of all mis-
labels M = {[(i, io), j] | i, j ∈ {1, . . . , C}, io ∈
{1, . . . , Ni}, i (cid:8)= j}; the initial mislabel distribution on M:
Γt(xio
for t = 1, . . . , Tmax do

N (C−1) ; a small constant ε.

i , j) = 1|M| =

| xio
i

1

− Calculate the terms dt
− Select s hardest examples per class based on qt

i,j by (3), and rt

i and qt

iio in (5).

iio to

form a training subset St ⊂ X .

− Apply KDA in subsection 3.2 on St, and constitute
the KDA-based feature extraction technique, denoted
by KDA-t; apply KDA-t on X to obtain {yio,t
∈ Rr};
build the hypothesis ht((cid:3), ∗) ∈ [0, 1] on the subset Y t
of {yio,t
(cid:2)

− Calculate the pseudo-loss based on ht:

∈ Rr}, corresponding to St.

t =

i

i

,j)−ht(y

io ,t
i

,i))

Γt(x

io
i ,j)(1+ht(y

[(i,io),j]∈M

io ,t
i
2

.

− Set βt = t/(1 − t). If βt ≤ ε, then Tmax = t − 1

and break.

− Update the mislabel distribution Γt: Γt+1(xio

i , j) =

Γt(xio

i , j)β

(1+ht(y
t

,i)−ht(y

io ,t
i

io ,t
i

,j))/2

.

i , j) =

Γt+1(xio

(cid:2)
− Normalize Γt+1
: Γt+1(xio
[(l,lo),g]∈M Γt+1(xlo
i , j)/(
(cid:2)

end for
Output: The ﬁnal hypothesis: hf (x) =
supi∈{1,...,C}{−
log(βt)ht(yt, i)}, where, for any
example x, yt ∈ Rr is the corresponding nonlinear feature
vector extracted by KDA (KDA-t) in subsection 3.2 over
St.

l , g)).

Tmax
t=1

Figure 1: Summary of BKDA algorithm.

Theorem 1. The subspace Sφ
F } is equivalent to the subspace Sφ
0, x ∈ F }, where Sφ
T = Sφ
population scatter operator.

B + Sφ

W and Sφ

t (0) = {x | (cid:10)Sφ

T , x(cid:11) (cid:8)= 0, x ∈
t , x(cid:11) (cid:8)=
t is the conventional

T (0) = {x | (cid:10)Sφ

Sφ

(cid:3)

W (0) in Sφ

Based on Theorem 1, we thus propose to use the fol-
lowing steps to calculate the optimal discriminant vectors
for (6): 1) Calculate the orthonormal basis of Sφ
t (0); 2)
Calculate the orthonormal basis of Sφ
t (0) to con-
struct Sφ
W (0); 3) Calculate the optimal discrimi-
T (0)
(cid:3)
nant vectors with respect to the discriminant criterion (6) in
Sφ

T (0)
In what follows, we will present the detailed computation
procedure. From the discussions above, we ﬁrst need to calcu-
late the orthonormal basis of Sφ
t (0) by applying KPCA on Sφ
t .
Moreover, Sφ
C(cid:4)

t can be rewritten as:

W (0).

Sφ

ΦiΦT

i = ΞΞT ,

(7)

Sφ

t =

1
N

i=1

where Φi = [φ(x1

i ) − mφ, . . . , φ(xNi

i ) − mφ] and Ξ =

1√
N

[Φ1, . . . , ΦC ]. Since the dimensionality of Sφ

t in F is usu-
ally very high or even inﬁnite-dimensional, KPCA can be car-
ried out by the eigenanalysis of ΞT Ξ instead, with size N ×N .
From the training set {φ(xio
i )}, an N ×N matrix K can be de-
ﬁned as K = (Kij)i,j=1,...,C where Kij = (kio jo )jo=1,...,Nj
io=1,...,Ni
and kiojo = (cid:10)φ(xio
j )(cid:11). By the kernel trick, ΞT Ξ can
(cid:5)
be expressed as
1
N

K − 1
N

(K1 + 1K) +

i ), φ(xjo

ΞT Ξ =

1
N 2

1K1

(cid:6)

(8)

,

where 1 is an N × N matrix with all terms being one. Let λl
and el (l = 1, . . . , m) be the ith positive eigenvalue and the
corresponding eigenvector of ΞT Ξ, respectively. Then θl =
−1/2
Ξelλ
(l = 1, . . . , m) constitute the orthonormal basis of
l
t (0) (or Sφ
Sφ
T (0)) [Dai and Qian, 2004; Zheng et al., 2005].
Hence, any input x ∈ Rn can be transformed into r in the
low-dimensional space Rm via KPCA as follows:

(cid:7)
1/N ET (I − 1/N )T ΥT ,

r = PT φ(x) =

, . . . , emλ

2 , . . . , k1

the identity matrix,
C , . . . , kNC

and Υ = (k1
i = (cid:10)φ(xio
} in Rn (or {φ(xio

(9)
−1/2
−1/2
where P = [θ1, . . . , θm], E = [e1λ
m ],
1
1, . . . , kN1
I is
1 ,
k1
i ), φ(x)(cid:11).
C ) with kio
2, . . . , kN2
Then, by (9), the training examples {xio
i )}
i
} in
in F ) can be mapped to the corresponding points {rio
i
the low-dimensional space Rm. The scatter matrices SB and
SW in the low-dimensional space Rm corresponding to Sφ
B
and Sφ
W can be computed by either of the following two ap-
proaches (for simplicity, this paper adopts the ﬁrst approach):
W , SB and
SW can be reconstructed based on the training examples
{rio
i

− By the corresponding deﬁnitions of Sφ

B and Sφ

} in Rm;

− By the kernel trick, the scatter matrices can be directly
P,

P and SW = PT Sφ
W

computed using SB = PT Sφ
B
similar to KPCA.

Sφ

(cid:3)

(cid:3)

T (0)

T (0)

Furthermore, we can directly calculate the null space of Sφ
W
in Sφ
T (0) by the eigenanalysis of SW , since SW is only an
m × m matrix. More speciﬁcally, let V = [γ 1, . . . , γ
u]
be the eigenvectors corresponding to the zero eigenvalues
of SW , and then Sφ
Sφ
W (0) can be spanned by PV.
the discriminant criterion (6) can be further
As a result,
transformed into the projection space of Sφ
W (0) by
PVz = zT VT SBVz ((cid:6)z(cid:6) = 1). Let
JF (z) = zT VT PT Sφ
B
zl (l = 1, . . . , r) be the eigenvectors of VT SBV, sorted in
descending order of the corresponding eigenvalues. Accord-
ing to [Dai and Qian, 2004; Zheng et al., 2005], it is clear
that wl = PVzl (l = 1, . . . , r) constitute the optimal dis-
criminant vectors with respect to the corresponding criterion
(6) in Sφ
Sφ
W (0). For an input pattern x, its correspond-
ing nonlinear feature vector extracted by the KDA procedure
described above can be computed as y = (cid:10)W, φ(x)(cid:11) ∈ Rr,
where W = (w1, . . . , wr). This expression can be rewritten
via the kernel trick as follows:

1/N (z1, . . . , zr)T VT ET (I − 1/N )T kx.

y = (cid:3)W, φ(x)(cid:4) =
Here, 1 is an N × N matrix with all terms being one, I is the
identity matrix, and kx = (k(x, x1

1), . . . , k(x, xNC

T (0)

p

(cid:3)

C ))T .

IJCAI-07

747

is capable of improving the overall performance for both ker-
nel functions on all three data sets due to its advantages as
discussed above; specially, BKDA can effectively improve the
performance of the corresponding KDA-r on the same num-
ber of features. In addition, we ﬁnd that when s is ﬁxed at
very small values, BKDA fails to show its effectiveness when
compared with KDA* since the ensemble-based learner is too
weak.

4 Experimental Results

Gene expression data are usually high-dimensional involving
a large number of genes but a small sample size. Thus, how to
effectively extract discriminant features plays a very important
role in gene expression data classiﬁcation [Ye et al., 2004]. To
evaluate the performance of the proposed BKDA algorithm,
we conduct some gene expression classiﬁcation experiments
to compare BKDA with some other dimensionality reduction
methods.

Since the non-balanced problems mentioned above only ex-
ist in multi-class classiﬁcation problems, our experiments are
performed on three different data sets involving more than two
classes each in order to demonstrate the behavior of BKDA:2

1. 11 Tumors: 174 human tumor examples corresponding

to 11 different cancer types;

100

95

90

85

80

)

%

(
 

t

e
a
r
 
y
c
a
r
u
c
c
a
n
o

 

i
t

a
c
i
f
i
s
s
a
C

l

2. A subset A of 14 Tumors: contains all 218 human tumor

examples corresponding to 14 different cancer types;

75

0

20

the T

 values in BKDA

60

40

max

BKDA: Training
BKDA: Test
KDA−r
KDA*

)

%

(
 
e

t

a
r
 
y
c
a
r
u
c
c
a
n
o

 

i
t

a
c
i
f
i
s
s
a
C

l

100

95

90

85

80

80

100

75

0

20

the T

 values in BKDA

60

40

max

100

95

90

85

80

75

70

65

)

%

(
 
e
t
a
r
 
y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
C

l

60

0

20

100

95

90

85

80

75

70

65

)

%

(
 
e
t
a
r
 
y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
C

l

60

0

20

(a)

(b)

BKDA: Training
BKDA: Test
KDA−r
KDA*

100

95

90

85

80

75

70

65

)

%

(
 

t

e
a
r
 
y
c
a
r
u
c
c
a

 

n
o

i
t

a
c
i
f
i
s
s
a
C

l

60

 values in BKDA

80

100

60

0

20

the T

40

max

the T

 values in BKDA

60

40

max

(c)

(d)

BKDA: Training
BKDA: Test
KDA−r
KDA*

)

%

(
 

t

e
a
r
 
y
c
a
r
u
c
c
a
n
o

 

i
t

a
c
i
f
i
s
s
a
C

l

100

95

90

85

80

75

70

65

the T

40

max

60

 values in BKDA
(e)

80

100

60

0

20

the T

 values in BKDA

60

40

max

(f)

BKDA: Training
BKDA: Test
KDA−r
KDA*

80

100

BKDA: Training
BKDA: Test
KDA−r
KDA*

80

100

BKDA: Training
BKDA: Test
KDA−r
KDA*

80

100

Figure 2: Comparative performance of BKDA and KDA under dif-
ferent Tmax values for BKDA. BKDA:Training and BKDA:Test de-
note the results of BKDA on the training set X and test set, re-
(a) Polynomial kernel on 11 Tumors; (b) RBF kernel
spectively.
on 11 Tumors; (c) Polynomial kernel on subset A of 14 Tumors;
(d) RBF kernel on subset A of 14 Tumors; (e) Polynomial kernel
on subset B of 14 Tumors; (f) RBF kernel on subset B of 14 Tumors.

The second experiment compares BKDA with several ef-
fective linear dimensionality reduction methods,
including
BDLDA [Lu et al., 2003a], LPP [He and Niyogi, 2004], and
NPE [He et al., 2005], and several other effective kernel-
based nonlinear dimensionality reduction methods, including
KPCA [Sch¨olkopf et al., 1999], GDA [Baudat and Anouar,
2000], KDDA [Lu et al., 2003b], WKDA [Dai and Yeung,
2005], and KDA/QR [Xiong et al., 2005]. Table 1 reports the
maximum classiﬁcation rates of different methods on the three
data sets. We can see that BKDA is generally more effective
than all other methods compared here.

Notice that there has been extensive research based on clas-
siﬁers for tissue classiﬁcation of gene expression data [Stat-
nikov et al., 2005], including k-nearest neighbor classiﬁer

3. A subset B of 14 Tumors: contains all examples cor-
responding to various human tumor and normal tissue
types, each of which contains at least eight examples.

Each data set is randomly partitioned into disjoint training
and test sets. For the training set X , each class Xi contains
L examples. Since the ﬁrst data set is less challenging than
the second and third data sets, we set L = 5 for the ﬁrst
data set and L = 7 for the other two. Moreover, in our ex-
periments, no preprocessing such as gene selection is applied
on the data sets. For each feature extraction method, we use
a simple and efﬁcient minimum mean distance rule with Eu-
clidean distance measure to assess the classiﬁcation accuracy,
and simultaneously build the corresponding normalized ver-
sion for the hypothesis in BKDA based on [Lu et al., 2003a].
Each experiment is repeated 10 times and the average classiﬁ-
cation rate is reported. For the kernel methods, we use the RBF
kernel k(z1, z2) = exp((cid:6)z1 − z2(cid:6)2/σ) and polynomial kernel
k(z1, z2) = (zT
. In addition, it
should be noted that BKDA can effectively boost the discrim-
ination ability of the different features extracted by KDA in
BKDA. To reduce the computational cost and simultaneously
consider the space limitation in this paper, we simply ﬁx the
number of features r in BKDA to C − 3 for each data set,
where C is the number of classes.

where σ = 1012

1 z2/σ + 1)2

For comparison, the ﬁrst set of experiments implements
BKDA and KDA [Zheng et al., 2005] using both the polyno-
mial and RBF kernels above, since KDA [Zheng et al., 2005]
can effectively calculate the signiﬁcant discriminant informa-
tion in the null space of the within-class scatter operator and
the proposed method [Ye et al., 2004] for gene expression
data classiﬁcation is in essence a special case of KDA in high-
dimensional spaces. Furthermore, to explicitly show the effec-
tiveness of BKDA, in each comparison, KDA offers two base-
lines: KDA* denotes the maximum classiﬁcation rate over the
variant features; KDA-r denotes the classiﬁcation rate based
on r ﬁxed features. In addition, in BKDA, the number of cho-
sen examples per class is set to s = L−1, where L is the num-
ber of examples for each class in the training set. The experi-
mental results shown in Fig. 2 reveal that: as expected, BKDA

2All data sets are available at http://discover1.mc.vanderbilt.edu

/discover/public/mc-svm/.

IJCAI-07

748

Table 1: Maximum classiﬁcation rates (%) of different meth-
ods on three data sets.

subset A

subset B

11 Tumors
Algorithm Poly. RBF

Poly. RBF

Poly. RBF
78.74 78.82 52.92 52.83 49.63 49.49
KPCA
80.84 80.67 63.33 64.08 59.71 60.15
GDA
84.29 84.12 63.75 64.17 60.07 60.37
KDDA
89.08 89.08 66.50 67.17 63.97 64.34
WKDA
KDA/QR
84.87 84.12 63.67 64.25 60.06 61.69
Our BKDA 90.84 90.76 68.33 68.42 65.37 65.44

BDLDA

LPP
NPE

86.55
83.78
84.79

65.17
57.58
61.42

63.90
55.81
58.01

(kNN), decision tree, naive Bayes classiﬁer, bagging, boost-
ing, support vector machine (SVM), etc. However, our com-
parative experiments have not included these methods since
we focus on the dimensionality reduction and feature extrac-
tion aspect in our study.
In addition, over the past decade
or so, dimensionality reduction techniques have been playing
a very important role in face recognition. We have also ap-
plied BKDA to face recognition based on the extended YaleB
and PIE databases, showing again that BKDA outperforms all
other dimensionality reduction methods compared. The re-
sults are not included in this paper due to space limitation.

5 Conclusion

In this paper, a novel KDA algorithm has been presented by
incorporating the boosting technique into KDA to give the
BKDA algorithm. This new algorithm effectively integrates
the strengths of the boosting and KDA techniques to give an
ensemble-based KDA framework with strong nonlinear fea-
ture extraction capability, and simultaneously overcomes the
non-balanced and small sample size problems commonly en-
countered by KDA-based methods. Extensive empirical com-
parison of BKDA with many other linear and nonlinear dimen-
sionality reduction methods on gene expression data classiﬁ-
cation shows that BKDA is a very promising method.

6 Acknowledgments

This research has been supported by Competitive Earmarked
Research Grant 621305 from the Research Grants Council of
the Hong Kong Special Administrative Region, China.

References
[Baudat and Anouar, 2000] G. Baudat and F. Anouar. Generalized
discriminant analysis using a kernel approach. Neural Computa-
tion, 12:2385–2404, 2000.

[Cevikalp et al., 2005] H. Cevikalp, M. Neamtu, M. Wilkes, and
A. Barkana. Discriminative common vectors for face recognition.
IEEE Transactions on Pattern Analysis and Machine Intelligence,
27(1):4–13, January 2005.

[Chen et al., 2000] L.F. Chen, H.Y.M. Liao, M.T. Ko, J.C. Lin, and
G.J. Yu. A new LDA-based face recognition system which
can solve the small sample size problem. Pattern Recognition,
33(10):1713–1726, 2000.

[Dai and Qian, 2004] G. Dai and Y.T. Qian. Kernel generalized non-
linear discriminant analysis algorithm for pattern recognition. In

Proceedings of the IEEE International Conference on Image Pro-
cessing, pages 2697–2700, 2004.

[Dai and Yeung, 2005] G. Dai and D.Y. Yeung. Nonlinear dimen-
sionality reduction for classiﬁcation using kernel weighted sub-
space method. In Proceedings of the IEEE International Confer-
ence on Image Processing, pages 838–841, September 2005.

[Freund and Schapire, 1997] Y. Freund and R.E. Schapire.

A
decision-theoretic generalization of on-line learning and an appli-
cation to boosting. Journal of Computer and System Sciences,
55(1):119–139, 1997.

[He and Niyogi, 2004] X.F. He and P. Niyogi. Locality preserving
projections. In Advances in Neural Information Processing Sys-
tems 16. MIT Press, Cambridge, MA, USA, 2004.

[He et al., 2005] X.F. He, D. Cai, S.C. Yan, and H.J. Zhang. Neigh-
borhood preserving embedding. In Proceedings of the Tenth IEEE
International Conference on Computer Vision, pages 1208–1213,
October 2005.

[Lotlikar and Kothari, 2000] R. Lotlikar and R. Kothari. Fractional-
IEEE Transactions on Pattern

step dimensionality reduction.
Analysis and Machine Intelligence, 22(6):623–627, June 2000.

[Lu et al., 2003a] J.W. Lu, K.N. Plataniotis, and A.N. Venet-
sanopoulos. Boosting linear discriminant analysis for face recog-
nition. In Proceedings of the IEEE International Conference on
Image Processing, pages 657–660, 2003.

[Lu et al., 2003b] J.W. Lu, K.N. Plataniotis, and A.N. Venet-
sanopoulos. Face recognition using kernel direct discriminant
analysis algorithms.
IEEE Transactions on Neural Networks,
14(1):117–126, January 2003.

[Masip and Vitri`a, 2006] D. Masip and J. Vitri`a. Boosted discrim-
Pattern

inant projections for nearest neighbor classiﬁcation.
Recognition, 39(2):164–170, 2006.

[Sch¨olkopf et al., 1999] B. Sch¨olkopf, A. Smola, and K.R. M¨uller.
Nonlinear component analysis as a kernel eigenvalue problem.
Neural Computation, 10:1299–1319, 1999.

[Statnikov et al., 2005] A. Statnikov, C.F. Aliferis, I. Tsamardinos,
D. Hardin, and S. Levy. A comprehensive evaluation of multicate-
gory classiﬁcation methods for microarray gene expression cancer
diagnosis. Bioinformatics, 21(1):631–643, 2005.

[Tang et al., 2005] E.K. Tang, P.N. Suganthan, X. Yao, and A.K.
Qin. Linear dimensionality reduction using relevance weighted
lda. Pattern Recognition, 38(4):485–493, 2005.

[Xiong et al., 2005] T. Xiong, J.P. Ye, Q. Li, V. Cherkassky, and
R. Janardan. Efﬁcient kernel discriminant analysis via QR de-
composition. In Advances in Neural Information Processing Sys-
tems 17. MIT Press, Cambridge, MA, USA, 2005.

[Ye et al., 2004] J.P Ye, T. Li, T. Xiong, and R. Janardan. Using un-
correlated discriminant analysis for tissue classiﬁcation with gene
expression data. IEEE/ACM Transactions on Computational Biol-
ogy and Bioinformatics, 1(4):181–190, 2004.

[Zheng et al., 2005] W. Zheng, L. Zhao, and C. Zou. Foley-Sammon
optimal discriminant vectors using kernel approach. IEEE Trans-
actions on Neural Networks, 16(1):1–9, January 2005.

IJCAI-07

749


InterActive Feature Selection

Hema Raghavan(cid:3)

University of Massachusetts

140 Governor’s Drive,

Amherst, MA-01002, USA

hema@cs.umass.edu

Omid Madani

Yahoo! Inc.

Rosie Jones
Yahoo! Inc.

74 N Pasadena Ave, Pasadena,

74 N Pasadena Ave, Pasadena,

CA 91103 USA

madani@yahoo-inc.com

CA 91103 USA

jonesr@yahoo-inc.com

Abstract

We study the effects of feature selection and hu-
man feedback on features in active learning set-
tings. Our experiments on a variety of text catego-
rization tasks indicate that there is signiﬁcant po-
tential in improving classiﬁer performance by fea-
ture reweighting, beyond that achieved via selec-
tive sampling alone (standard active learning) if we
have access to an oracle that can point to the impor-
tant (most predictive) features. Consistent with pre-
vious ﬁndings, we ﬁnd that feature selection based
on the labeled training set has little effect. But our
experiments on human subjects indicate that human
feedback on feature relevance can identify a suf-
ﬁcient proportion (65%) of the most relevant fea-
tures. Furthermore, these experiments show that
feature labeling takes much less (about 1/5th) time
than document labeling. We propose an algorithm
that interleaves labeling features and documents
which signiﬁcantly accelerates active learning.

1 Introduction
A major bottleneck in machine learning applications is the
lack of sufﬁcient labeled data for adequate classiﬁer perfor-
mance, as manual labeling is often tedious and costly. Tech-
niques such as active learning, semi-supervised learning, and
transduction have been pursued with considerable success in
reducing labeling requirements. In the standard active learn-
ing paradigm, learning proceeds sequentially, with the learn-
ing algorithm actively asking for the labels of instances from
a teacher. The objective is to ask the teacher to label the most
informative instances in order to reduce labeling costs and
accelerate the learning. There has been very little work in
supervised learning in which the user (teacher) is queried on
aspects other than class assignment of instances. In exper-
iments in this paper we study the beneﬁts and costs of fea-
ture feedback via humans on active learning. To this end we
pick document classiﬁcation [Sebastiani, 2002] as the learn-
ing problem of choice because it represents a case of super-
vised learning which traditionally relies on example docu-
ments as input for training and where users have sufﬁcient

(cid:3)This work was done in part when the author was at Yahoo! Inc.

prior knowledge on features which can be used to accelerate
learning. For example, to ﬁnd documents on the topic cars in
traditional supervised learning the user would be required to
provide sufﬁcient examples of cars and non-cars documents.
However, this is not the only way in which the information
need of a user looking for documents on cars can be satisﬁed.
In the information retrieval setting the user would be asked to
issue a query, that is, state a few words (features) indicating
her information need. Thereafter, feedback which may be at
a term or at a document level may be incorporated. In fact,
even in document classiﬁcation, a user may use a keyword
based search to locate the initial training examples. How-
ever, traditional supervised learning tends to ignore the prior
knowledge that the user has, once a set of training examples
have been obtained. In this work we try to ﬁnd a marriage
between approaches to incorporating user feedback from ma-
chine learning and information retrieval and show that active
learning should be a dual process – at the term and at the
document-level. This has applications in email ﬁltering and
news ﬁltering where the user has some prior knowledge and
a willingness to label some (as few as possible) documents in
order to build a system that suits her needs. We show that hu-
mans have good intuition for important features in text clas-
siﬁcation tasks since features are typically words that are per-
ceptible to the human and that this human prior knowledge
can indeed accelerate learning.

In summary, our contributions are: (1) We demonstrate
that access to a feature importance oracle can improve perfor-
mance (F 1) signiﬁcantly over uncertainty sampling. (2) We
show that even naive users can provide feedback on features
with about 60% accuracy of the oracle. (3) We show that the
relative manual costs of labeling features is about 1/5th that
of document feedback. (4) We show a method of simulta-
neously soliciting class labels and feature feedback that im-
proves classiﬁer performance signiﬁcantly.

We describe the experimental setup in Sec. 2 and show
how feature selection using an oracle is useful to active learn-
ing in Sec. 3. In Sec. 4 we show that humans can indeed iden-
tify useful features and show how human-chosen features can
be used to accelerate learning in Sec. 5. We relate our work
to past work in Sec. 6 and outline directions for the future in
section Sec. 7.

2 Experimental setup
Our test bed for this paper comes from three domains:
(1) The 10 most frequent classes from the Reuters-21578
corpus (12902 documents). (2) The 20-Newsgroups corpus
(20000 documents from 20 Usenet newsgroups). (3) The ﬁrst
10 topics from the TDT-2001 corpus (67111 documents in 3
languages from broadcast and news-wire sources).
For all three corpora we consider each topic as a one versus
all classiﬁcation problem. We also pick two binary classi-
ﬁcation problems viz., Baseball vs Hockey and Automobiles
vs Motorcycles from the 20-Newsgroups corpus.
In all we
have 42 classiﬁcation problems.1 All the non-english stories
in the TDT corpus were machine translated into English. As
features we use words, bigrams and trigrams obtained after
stopping and stemming with the Porter stemmer in the Rain-
bow Toolkit [McCallum, 1996]

We use linear support vector machines (SVMs) and un-
certainty sampling for active learning [Scholkopf and Smola,
2002; Lewis and Catlett, 1994]. SVMs are the state of art
in text categorization, and have been found to be fairly ro-
bust even in the presence of many redundant and irrelevant
features [Brank et al., 2002; Rose et al., 2002]. Uncertainty
sampling [Lewis and Catlett, 1994] is a type of active learn-
ing in which the example that the user (teacher) is queried on
is the unlabeled instance that the classiﬁer is most uncertain
about. When the classiﬁer is an SVM, unlabeled instances
closest to the margin are chosen as queries [Tong and Koller,
2002]. The active learner may have access to all or a sub-
set of the unlabeled instances. This subset is called the pool
and we use a pool size of 500 in this paper. The newly la-
beled instance is added to the set of labeled instances and the
classiﬁer is retrained. The user is queried a total of T times.
The Deﬁciency metric [Baram et al., 2003] quantiﬁes the
performance of the querying function for a given active learn-
ing algorithm. Originally deﬁciency was deﬁned in terms of
accuracy. Accuracy is a reasonable measure of performance
when the positive class is a sizeable portion of the total. Since
this is not the case for all the classiﬁcation problems we have
chosen, we modify the deﬁnition of deﬁciency, and deﬁne it
in terms of the F 1 measure (harmonic mean of precision and
recall [Rose et al., 2002]). Using notation similar to the orig-
inal paper [Baram et al., 2003], let U be a random set of P
labeled instances, F 1t(RAN D) be the average F 1 achieved
by an algorithm when it is trained on t randomly picked ex-
amples and F 1t(ACT ) be the average F 1 obtained using t
actively picked examples. Deﬁciency D is deﬁned as:

DT =

PT
t=init(F 1M (RAN D) (cid:0) F 1t(ACT ))
PT
t=init(F 1M (RAN D) (cid:0) F 1t(RAN D))

(1)

F 1M (RAN D) is the F 1 obtained with a large number (M)
of randomly picked examples.
For this paper we take
M = 1000 and t = 2; 7:::42. When t = 2 we have one
positive and one negative example. F 1t((cid:15)) is the average
F 1 computed over 10 trials. In addition to deﬁciency we re-
port F 1t for some values of t. Intuitively, if Cact is the curve

1

http://www.daviddlewis.com/resources/testcollections/reuters21578/, http://kdd.ics.uci.edu/da -

tabases/20newsgroups/20newsgroups.html, http://www.ldc.upenn.edu/Projects/TDT3/

obtained by plotting F 1t(ACT ), Crand is the correspond-
ing curve using random sampling and CM is the straight line
F 1t = F 1M then deﬁciency is the ratio of the area between
Cact and CM and the area between Crand and CM . The lower
the deﬁciency the better the active learning algorithm. We
aim to minimize deﬁciency and maximize F 1.

3 Oracle Feature Selection Experiments
The oracle in our experiments has access to the labels of all
P documents in U and uses this information to return a list of
the k most important features. We assume that the parameter
k is input to the oracle. The oracle orders the k features in
decreasing information gain order. Given a set of k features
we can perform active learning as discussed in the previous
section and plot Cact for each value of k.

Figure 1: Average F 1t(ACT ) for different values of k. k is
the number of features and t is the number of documents.

Figure 1 shows a plot of F 1t(ACT ) against number of
features k and number of labeled training examples t, for the
Earnings category in Reuters. The x, y and z axes denote k,
t and F 1 respectively. The number of labeled training exam-
ples t ranges from 2...42 in increments of 5. The number of
features used for classiﬁcation k has values from 32 to 33718
(all features). The dark dots represent the maximum Ft for
each value of t, while the dark band represents the case when
all features are used. This method of learning in one dimen-
sion is representative of traditional active learning. Clearly
when the number of documents is few, performance is bet-
ter when there is a smaller number of features. As the num-
ber of documents increases the number of features needed to
achieve best accuracy increases. From the ﬁgure it is obvious
that we can get a big boost in accuracy by starting with fewer
features and then increasing the complexity of the model (the
number of relevent features) as the number of labeled docu-
ments increase.

All 42 of our classiﬁcation problems exhibit behavior like
that in Figure 1[Raghavan et al., 2005]. We report the aver-
age deﬁciency, F 17 (F1 score with 7 labeled examples) and
F 122 in Fig. 2 to illustrate this point. The column labeled
Act shows performance using traditional active learning and
all the features. The column labeled Ora shows performance
obtained using a reduced subset of features using the Oracle.
Intuitively, with limited labeled data, there is little evi-
dence to prefer one feature over another. Feature/dimension
reduction (by the oracle) allows the learner to “focus” on di-
mensions that matter, rather than being “overwhelmed” with

Class #

Reuters
20-News.
TDT
Bas. vs Hock
Auto vs Mot.

D42

F 17

F 122

Act
0.421
0.602
0.735
0.710
0.676

Oracle Act
0.319
0.344
0.656
0.447
0.321

0.345
0.072
0.186
0.587
0.431

Ora
0.446
0.222
0.290
0.701
0.724

Act
0.569
0.21
0.282
0.785
0.758

Ora
0.621
0.29
0.407
0.828
0.860

F 11000
Act
0.727
0.446
0.751
0.963
0.899

Figure 2: Improvements in deﬁciency, F 17 and F 122 using an oracle to select the most important features. Remember that the
objective is to minimize deﬁciency and maximize F 1. For each of the three metrics, ﬁgures in bold are statistically signiﬁcant
improvements over Uncertainty sampling using all features (the corresponding columns denoted by Act). When 1000 documents
are labeled (F 11000) using the entire feature set leads to better F 1 scores.

numerous dimensions right at the outset of learning.
As
the number of labeled examples increases, feature selection
becomes less important, as the learning algorithm becomes
more capable of ﬁnding the discriminating hyperplane ( fea-
ture weights). We experimented with ﬁlter based methods for
feature selection, which did not work very well (i.e., tiny or no
improvements). This is expected given such limited training
set sizes (see Fig. 3), and is consistent with most previous
ﬁndings [Sebastiani, 2002]. Next we determine if humans
can identify these important features.

4 Human Labeling
Consider our introductory example of a user who wants to
ﬁnd all documents that discuss cars. From a human perspec-
tive the words ‘car’, ‘auto’ etc may be important features in
documents discussing this topic. Given a large number of
documents labeled as on-topic and off-topic, and given a clas-
siﬁer trained on these documents, the classiﬁer may also ﬁnd
these features to be most relevant. With little labeled data
(say 2 labeled examples) the classiﬁer may not be able to
determine the discriminating features. While in general in
machine learning the source of labels is not important to us,
in active learning scenarios in which we expect the labels to
come from humans we have valid questions to pose: (1) Can
humans label features as well as documents? (2) If the labels
people provide are noisy through being inconsistent, can we
learn well enough? (3) Are features that are important to the
classiﬁer perceptible to a human?

Our concern in this paper is asking people to give feedback
on features, or word n-grams, as well as entire documents.
We may expect this to be more efﬁcient, since documents
contain redundancy, and results from our oracle experiments
indicate great potential. On the other hand, we also know
that synthetic examples composed of a combination of real
features can be difﬁcult to label [Baum and Lang, 1992].

4.1 Experiments and Results
In order to answer the above questions we conducted the
following experiment. We picked 5 classiﬁcation problems
which we thought were perceptible to the average person on
the street and also represented the broad spectrum of prob-
lems from our set of 42 classiﬁcation problems. We took the
two binary classiﬁcation problems and from the remaining 40
one-versus-all problems we chose three (earnings, hurricane
Mitch and talk.politics.mideast). For a given classiﬁcation

problem we took the top 20 features as ranked by informa-
tion gain on the entire labeled set. In this case we did not stem
the data so that features remain as legitimate English words.
We randomly mix these with features which are much lower
in the ranked list. We show each user one feature at a time
and give them two options – relevant and not-relevant/don’t
know. A feature is relevant if it helps discriminate the pos-
itive or the negative class. We measure the time it takes the
user to label each feature. We do not show the user all the
features as a list, though this may be easier, as lists provide
some context and serve as a summary. Hence our method
provides an upper bound on the time it takes a user to judge
a feature. We compare this with the time it takes a user to
judge a document. We measure the precision and recall of the
user’s ability to label features. We ask the user to ﬁrst label
the features and then documents, so that the feature labeling
process receives no beneﬁt due to the fact that the user has
viewed relevant documents. In the learning process we have
proposed, though, the user would be labeling documents and
features simultaneously, so the user would indeed be inﬂu-
enced by the documents he reads. Hence our method is more
stringent than the real case. We could in practice ask users to
highlight terms as they read documents. Experiments in this
direction have been conducted in information retrieval [Croft
and Das, 1990].

Our users were six graduate students and two employees
of a company, none of whom were authors of this paper. Of
the graduate students, ﬁve were in computer science and one
from public health. All our users were familiar with the use of
computers. Five users understood the problem of document
classiﬁcation but none had worked with these corpora. One of
our users was not a native speaker of English. The topics were
distributed randomly, and without considering user expertise,
so that each user got an average of 2-3 topics. There were
overlapping topics between users such that each topic was
labeled by 2-3 users on average. A feedback form asking
the users some questions about the difﬁculty of the task was
handed out at the end.

We evaluated user feature labeling by calculating their av-
erage precision and recall at identifying the top 20 features as
ranked by an oracle using information gain on the entire la-
beled set. Fig. 3 shows these results. For comparison we have
also provided the precision and recall (against the same ora-
cle ranking of top 20 features) obtained using 50 labeled ex-
amples (picked using uncertainty sampling) denoted by @50.

Class
Problem
Baseball..
Auto vs ...
Earnings
...mideast
...Mitch
Average

Prec.

Rec.

Hum. @50 Hum. @50
0.42
0.54
0.53
0.68
0.716
0.580

0.7
0.81
0.66
0.55
0.56
0.65

0.3
0.25
0.2
0.35
0.65
0.35

Avg. Time (secs)
Feat. Docs
2.83
12.6
19.84
3.56
13
2.97
12.93
2.38
13.19
2.38
2.82
14.31

0.3
0.25
0.25
0.35
0.65
0.38

Figure 3: Ability of users to identify important features. Pre-
cision and Recall against an oracle, of users (Hum.) and an
active learner which has seen 50 documents(@50). Average
labeling times for features and documents are also shown. All
numbers are averaged over users.

Precision and Recall of the humans is high, supporting our
hypothesis that features that a classiﬁer ﬁnds to be relevant af-
ter seeing a large number of labeled instances are obvious to a
human after seeing little or no labeled data (the latter case be-
ing true of our experiments). Additionally the Precision and
Recall @50 is signiﬁcantly lower than that of humans, indi-
cating that a classiﬁer like an SVM needs to see much more
data before it can ﬁnd the discriminatory features.

The last column of Fig. 3 shows time taken for labeling
features and documents. On average humans require about 5
times longer to label documents than to label features. Note
that features may be even easier to label if they are shown in
context – as lists, with relevant passages etc. There are sev-
eral other metrics and points of discussion such as user ex-
pertise, time taken to label relevant and non-relevant features
and so on, which we reserve for future work. One impor-
tant consideration though, is that document length inﬂuences
document labeling time. We found the two to be correlated
by r = 0:289 which indicates a small increase in time for a
large increase in length. The standard deviations for precision
and recall are 0.14 and 0.15 respectively. Different users vary
signiﬁcantly in precision, recall and the total number of fea-
tures labeled relevant. Based on feedback on a post-labeling
survey we are inclined to believe that this is due to individual
caution exercised during the labeling process.

Some of the highlights of the post-labeling survey are as
follows. On average users found the ease of labeling features
to be 3.8 (where 0 is most difﬁcult and 5 is very easy) and
documents 4.2. In general users with poor prior knowledge
found the feature labeling process very hard, although as we
will show, their labels were extremely useful to the classi-
ﬁer. The average expertise (5=expert) was 2.4, indicating that
most users felt they had little domain knowledge for the tasks
they were assigned. We now proceed to see how to use fea-
tures labeled as relevant by our naive users in active learning.

5 A Human in the Loop
We saw in Sec. 3 that feature selection coupled with uncer-
tainty sampling gives us big gains in performance when there
are few labeled examples. In Sec. 4 we saw that humans can
discern discriminative features with reasonable accuracy. We
now describe our approach of applying term and document
level feedback simultaneously in active learning.

InterActive Learning Algorithm

5.1
Let documents be represented as vectors Xi = xi1:::xijF j,
where jF j is the total number of features. At each iteration
the active learner not only queries the user on an uncertain
document, but also presents a list of f features and asks the
user to label features which she considers relevant. The fea-
tures to be displayed to the user are the top f features ob-
tained by ordering the features by information gain. To ob-
tain the information gain values with t labeled instances we
trained a classiﬁer on these t labeled instances. Then to com-
pute information gain, we used the 5 top ranked (farthest from
the margin) documents from the unlabeled set in addition to
the t labeled documents. Using the unlabeled data for term
level feedback is very common in information retrieval and is
called pseudo-relevance feedback [Salton, 1968].

The user labels some of the f features which he considers
discriminative features. Let ~s = s1:::sjF j be a vector con-
taining weights of relevant features.
If a feature number i
that is presented to the user is labeled as relevant then we
set si = a, otherwise si = b, where a and b are parameters
of the system. The vector ~s is noisier than the real case be-
cause in addition to mistakes made by the user we lose out on
those features that the user might have considered relevant,
had he been presented that feature when we were collecting
relevance judgments for features. In a real life scenario this
might correspond to the lazy user who labels few features
as relevant and leaves some features unlabeled in addition to
making mistakes. If a user had labeled a feature as relevant in
some past iteration we don’t show the user that feature again.
We incorporate the vector ~s as follows. For each Xi in
the labeled and unlabeled sets we multiply xij by sj to get
ij. In other words we scale all relevant features by a and
X 0
non-relevant features by b. We set a = 10 and b = 1. 2

By scaling the important features by a we are forcing the
classiﬁer to assign higher weights to these features. We
demonstrate this with the following example. Consider a lin-
ear SVM, jF j = 2 and 2 data points X1 = (1; 2) and X2 =
(2; 1) with labels +1 and (cid:0)1 respectively. An SVM trained
on this input learns a classiﬁer with w = ((cid:0)0:599; +0:599).
Thus both features are equally discriminative. If feature 1 is
considered more discriminative by a user, then by our method
2 = (20; 1) and w0 = (0:043; (cid:0)0:0043),
X 0
thus assigning higher weight to f1. Now, this is a “soft” ver-
sion of the feature selection mechanism of Sec. 3. But in that
case the Oracle knew the ideal set of features; we can view
those set of experiments as a special case where b = 0. We
expect that human labels are noisy and we do not want to
zero-out potentially relevant features.

1 = (10; 2) and X 0

5.2 Experiments and Results
To make our experiments repeatable (to compute average per-
formance and for convenience) we simulate user interaction
as follows. For each classiﬁcation problem we maintain a list
of features that a user might have considered relevant had he

2We picked our algorithm’s parameters based on a preliminary
test on 3 topics (baseball, earnings, and acquisitions) using the oracle
features of Sec. 3.

been presented that feature. For these lists we used the judg-
ments obtained in Sec. 4. Thus for each of the 5 classiﬁcation
problems we had 2-3 such lists, one per user who judged that
topic. For the 10 TDT topics we have topic descriptions as
provided by the LDC, and we simulated explicit human feed-
back on feature relevance as follows: The topic descriptions
contain names of people, places and organizations that are
key players in this topic in addition to other keywords. We
used the words in these topic descriptions as the list of rele-
vant features. Now, given these lists we can perform the sim-
ulated HIL (Human in the Loop) experiments for 15 classiﬁ-
cation problems. At each iteration f features are shown to the
user. If the feature exists in the list of relevant features, we set
the corresponding bit in ~s and proceed with the active learn-
ing as in Sec. 5.1. Fig. 4 shows the performance of the HIL
experiments. As before we report deﬁciency, F 17 and F 122.
As a baseline we also report results for the case when the top
20 features as obtained by the information gain oracle are in-
put to the simulated HIL experiments (this represents what
a user with 100% precision and recall would obtain by our
method). The Oracle is (as expected) much better than plain
uncertainty sampling, on all 3 measures, reinforcing our faith
in the algorithm of Sec. 5.1. The performance of the HIL ex-
periments is almost as good as the Oracle, indicating that user
input (although noisy) can help improve performance signiﬁ-
cantly. The only relative poor performance for the HIL simu-
lation is on the average D42 measure for the TDT categories,
where we used all the words from the topic descriptions as a
proxy for explicit human feedback on features. The plot on
the right is of F 1t(HIL) for hurricane Mitch. As a compar-
ison F 1t(ACT ) is shown. The HIL values are much higher
than for plain uncertainty sampling.

We also observed that relevant features were usually spot-
ted in very early iterations. For the Auto vs Motorcycles prob-
lem, the user has been asked to label 75% (averaged over mul-
tiple iterations and multiple users) of the oracle features at
some point or the other. The most informative words (as de-
termined by the Oracle) – car and bike are asked of the user
in very early iterations. The label for car is always (100%
of the times) asked, and 70% of the time the label for this
word is asked of the user in the ﬁrst iteration itself. This is
closely followed by the word bike which the user is queried
on within the ﬁrst 5 iterations 80% of the time. Most relevant
features are queried within 10 iterations which makes us be-
lieve that we can stop feature level feedback in 10 iterations
or so. When to stop asking questions on both features and
documents and switch entirely to documents remains an area
for future work.

6 Related Work
Our work is related to a number of areas including query
learning, active learning, use of (prior) knowledge and feature
selection in machine learning, term-relevance feedback in in-
formation retrieval, and human-computer interaction, from
which we can cite only a few.

Our proposed method is an instance of query-based learn-
ing and an extension of standard (“pool-based”) active learn-
ing which focuses on selective sampling of instances (from

a pool of unlabeled data) alone [Cohn et al., 1994]. Al-
though query-based learning can be very powerful in theory
[Angluin, 1992], arbitrary queries may be difﬁcult to answer
in practice [Baum and Lang, 1992], hence the popularity of
pool-based methods, and the motivation for studying the ef-
fectiveness and ease of predictive feature identiﬁcation by
humans in our application area. That human prior knowl-
edge can accelerate learning has been investigated by [Paz-
zani and Kibler, 1992], but our work differs in techniques
(they use prior knowledge to generate horn-clause rules) and
applications. [Beineke et al., 2004] uses human prior knowl-
edge of co-occurrence of words to improve classiﬁcation of
product reviews. None of this work, however, considers the
use of prior knowledge in the active learning setting. Our
work is unique in the ﬁeld of active learning as we extend
the query model to include feature as well as document level
feedback. Our study of the human factors (such as quality
of feedback and costs) is also a major differentiating theme
between our work and previous work in incorporating prior
knowledge which did not address this issue, or might have
assumed experts in machine learning taking a role in train-
ing the system [Schapire et al., 2002; Wu and Srihari, 2004;
Godbole et al., 2004]. We only assume knowledge about the
topic of interest. Our algorithmic techniques and the studied
modes of interaction differ and are worth further comparison.
In both [Wu and Srihari, 2004; Schapire et al., 2002], prior
knowledge is given at the outset which leads to a “soft” label-
ing of the labeled or unlabeled data that is incorporated into
training via modiﬁed boosting or SVM training. However,
in our scheme the user is labeling documents and features si-
multaneously. We expect that our proposed interactive mode
has an advantage over requesting prior knowledge from the
outset, as it may be easier for the user to identify/recall rele-
vant features while labeling documents in the collection and
being presented with candidate features. The work of [God-
bole et al., 2004] puts more emphasis on system issues and
focuses on multi-class training rather than a careful analy-
sis of effects of feature selection and human efﬁcacy. Their
proposed method is attractive in that it treats features as sin-
gle term documents that can be labeled by humans, but they
also study labeling features before documents (and only in
an “oracle” setting, i.e., not using actual human annotators),
and do not observe much improvements using their particu-
lar method over standard active learning in the single domain
(Reuters) they test on.

7 Conclusions and Future Work
We showed experimentally that for learning with few labeled
examples, good feature selection is extremely useful. As the
number of examples increases, the vocabulary (feature set
size) of the system also needs to increase. A teacher, who is
not knowledgeable in machine learning, can help accelerate
training the system in this early stage, by pointing out poten-
tially important words. We also conducted a user study to see
how well naive users performed as compared to a feature ora-
cle. We used our users’ outputs in realistic human in the loop
experiments and found signiﬁcant increase in performance.

This paper raises the question of what questions (other than

Dataset

Baseball
Earnings
Auto vs Motor
Hurr. Mitch
talk.politics.mideast
Avg TDT performance

Act
0.71
0.90
0.82
0.89
0.49
0.86

D42
Oracle HIL Act
0.49
0.41
0.61
0.64
0.35
0.33
0.04
0.38
0.28
0.14
0.09
0.77

0.46
0.64
0.60
0.38
0.28
0.89

F 17
Oracle HIL Act
0.63
0.63
0.80
0.79
0.71
0.62
0.08
0.46
0.28
0.32
0.18
0.21

0.60
0.73
0.60
0.60
0.29
0.24

F 122
Oracle HIL
0.70
0.79
0.86
0.85
0.73
0.83
0.58
0.63
0.49
0.49
0.22
0.32

Figure 4: Improvement in deﬁciency due to human feature selection. Numbers for HIL are averaged over users. The graph
shows Human Feature Selection for Hurricane Mitch with the x-axis being the number of labeled documents and y-axis
F 1(HIL); the difference between these two curves is summarized by the deﬁciency score. The F 17 and F 122 scores show the
points on the two curves where 7 and 22 documents have been labeled with active learning. The difference between no feature
feedback and human-labeled features is greatest with few documents labeled, but persists up to 42 documents labeled.

questions about the labels of instances) an active learner can
ask a user about the domain in order to learn as quickly as
possible. In our case, the learner asked the teacher queries on
the relevancy of words in addition to the labels of documents.
Both types of questions were easy for the teacher to under-
stand. Our subjects did indeed ﬁnd marking words without
context a little hard, and suggested that context might have
helped. We intend to conduct a user study, to see what users
can perceive easily, and to incorporate these into learning al-
gorithms.

Acknowledgments
This work was supported in part by the Center for Intelligent Infor-
mation Retrieval and in part by SPAWARSYSCEN-SD grant num-
ber N66001-02-1-8903. Any opinions, ﬁndings and conclusions or
recommendations expressed in this material are the author(s) and do
not necessarily reﬂectthose of the sponsor. We would also like to
thank our users who voluntarily labeled data.
References
[Angluin, 1992] D. Angluin. Comp. learning theory: survey
and selected bibl. In Proc. 24th Annu. ACM Sympos. The-
ory Comput., pages 351–369, 1992.

[Baram et al., 2003] Y. Baram, R. El-Yaniv, and K. Luz. On-
In Proc. of

line choice of active learning algorithms.
ICML-2003, pages 19–26, 2003.

[Baum and Lang, 1992] E. B. Baum and K. Lang. Query
learning can work poorly when human oracle is used. In
Intern Joint Conf in Neural Netwroks, 1992.

[Beineke et al., 2004] Philip Beineke, Trevor Hastie, and
Shivakumar Vaithyanathan. The sentimental factor: Im-
proving review classiﬁcation via human-provided infor-
mation. In Proc. of ACL’04, Main Volume, pages 263–270,
Barcelona, Spain, July 2004.

[Brank et al., 2002] J. Brank, M. Grobelnik, N. Milic-
Frayling, and D. Mladenic. Feature selection using lin-
ear support vector machines. Technical report, Microsoft
Research, 2002.

[Cohn et al., 1994] D. A. Cohn, L. Atlas, and R. E. Ladner.
Improving generalization with active learning. Machine
Learning, 15(2):201–221, 1994.

[Croft and Das, 1990] W. B. Croft and R. Das. Experiments
with query acquisition and use in document retrieval sys-
tems. In SIGIR ’90, 1990.

[Godbole et al., 2004] S. Godbole, A. Harpale, S. Sarawagi,
and S. Chakrabarti. Document classiﬁcation through inter-
active supervision of document and term labels. In PKDD,
pages 185–196, 2004.

[Lewis and Catlett, 1994] D. D. Lewis and J. Catlett. Het-
erogeneous uncertainty sampling for supervised learning.
In Proc. of ICML-94, pages 148–156, 1994.

[McCallum, 1996] A. K. McCallum. Bow: A toolkit for sta-
tistical language modeling, text retrieval, classiﬁcation and
clustering. http://www.cs.cmu.edu/ mccallum/bow, 1996.
[Pazzani and Kibler, 1992] M. J. Pazzani and D. Kibler. The
role of prior knowledge in inductive learning. Machine
Learning, 9 , 54-97., 9, 1992.

[Raghavan et al., 2005] Hema Raghavan, Omid Madani, and
Rosie Jones. Interactive feature selection. Technical re-
port, University of Massachusetts, Amherst, 2005.

[Rose et al., 2002] T. G. Rose, M. Stevenson, and M. White-
head. The reuters corpus vol. 1 - from yesterday’s news to
tomorrow’s language resources. In Inter. Conf. on Lang.
Resources and Evaluation, 2002.

[Salton, 1968] G. Salton. Automatic Information Organiza-

tion and Retrieval. McGraw Hill, 1968.

[Schapire et al., 2002] R. Schapire, M. Rochery, M. Rahim,
and N. Gupta. Incorporating prior knowledge into boost-
ing. In ICML, 2002.

[Scholkopf and Smola, 2002] B. Scholkopf and A. Smola.
Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[Sebastiani, 2002] F. Sebastiani. Machine learning in au-
tomated text categorization. ACM Computing Surverys,
2002.

[Tong and Koller, 2002] S. Tong and D. Koller. Support vec-
tor machine active learning with applications to text clas-
siﬁcation. J. Mach. Learn. Res., 2:45–66, 2002.

[Wu and Srihari, 2004] X. Wu and R. Srihari. Incorporating
prior knowledge with weighted margin support vector ma-
chines. In Proc. of KDD, 2004.


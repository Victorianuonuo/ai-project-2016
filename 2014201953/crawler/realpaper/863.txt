Improving Anytime Point-Based Value Iteration Using Principled Point Selections

Michael R. James, Michael E. Samples, and Dmitri A. Dolgov

AI and Robotics Group

Technical Research, Toyota Technical Center USA

{michael.james, michael.samples, dmitri.dolgov}@tema.toyota.com

Abstract

Planning in partially-observable dynamical sys-
tems (such as POMDPs and PSRs) is a com-
putationally challenging task.
Popular approx-
imation techniques that have proven successful
are point-based planning methods including point-
based value iteration (PBVI), which works by ap-
proximating the solution at a ﬁnite set of points.
These point-based methods typically are anytime
algorithms, whereby an initial solution is obtained
using a small set of points, and the solution may
be incrementally improved by including additional
points. We introduce a family of anytime PBVI al-
gorithms that use the information present in the cur-
rent solution for identifying and adding new points
that have the potential to best improve the next so-
lution. We motivate and present two different meth-
ods for choosing points and evaluate their perfor-
mance empirically, demonstrating that high-quality
solutions can be obtained with signiﬁcantly fewer
points than previous PBVI approaches.

1 Introduction
Point-based planning algorithms [Pineau et al., 2003; Spaan
and Vlassis, 2005] for partially-observable dynamical sys-
tems have become popular due to their relatively good per-
formance and because they can be implemented as anytime
It has been suggested (e.g., in [Pineau et al.,
algorithms.
2005]), that anytime point-based planning algorithms could
beneﬁt signiﬁcantly from the principled selection of points to
add.

We provide several methods for using information col-
lected during value iteration (speciﬁcally, characteristics of
the value function) to choose new additional points. We show
that properties of the value function allow the computation of
an upper bound on the potential improvement (gain) resulting
from the addition of a single point. We argue that the addition
of points with maximal gain produces good approximations
of the value function, and this argument is empirically ver-
iﬁed. Further, we deﬁne an optimization problem that ﬁnds
the point with the maximal gain for a given region.

These new approaches are empirically compared to tradi-
tional anytime point-based planning. The results show that

our algorithms choose additional points that signiﬁcantly im-
prove the resulting approximation of the value function. Such
improvement is potentially beneﬁcial in two ways. First,
computation time can be reduced because there are many
fewer points for which computation must be performed.
However, this beneﬁt is sometimes (but not always) offset by
the additional time required to select new points. Another po-
tential beneﬁt is reduced memory storage for smaller sets of
points. Currently, the solution of most systems do not require
large numbers of points, but a storage beneﬁt may become
more important as the size of problems increases such that
more points are required.

2 Background
The planning methods developed here are for partially-
observable, discrete-time, ﬁnite-space dynamical systems in
which actions are chosen from a set A, observations from a
set O, and rewards from a set R. There is exactly one action,
one observation, and one reward per time-step—planning al-
gorithms attempt to ﬁnd a policy that maximizes the long-
term discounted reward with discount factor γ ∈ [0, 1).

These point-based planning methods are suitable for at
least two classes of models that can represent such dynamical
systems, the well-known partially observable Markov deci-
sion processes (POMDP), and the newer predictive state rep-
resentation (PSR) [Singh et al., 2004]. For the sake of famil-
iarity, we use POMDPs here, but it should be noted that the
methods presented here (as well as other planning methods
for such dynamical systems) can just as easily be applied to
PSRs (see [James et al., 2004] for details).

2.1 POMDPs
POMDPs are models of dynamical systems based on under-
lying latent variables called nominal states s ∈ S. At any
time step, the agent is in one nominal state s. Upon taking
an action a ∈ A the agent receives a reward r(s, a) that is
a function of the state and action, transitions to a new state
(cid:2)|s, a), and
(cid:2) according to a stochastic transition model pr(s
s
receives an observation o ∈ O according to a stochastic ob-
servation model pr(o|s, a). For compactness, we deﬁne the
vector ra as ra(s) = r(s, a). 1

1The notation v(e) for vector v refers the value of entry e in v.

IJCAI-07

865

The agent does not directly observe the nominal state s,
and so, must maintain some sort of memory to fully charac-
terize its current state.
In POMDPs, this is typically done
using the belief state: a probability distribution over nomi-
nal states. The belief state b summarizes the entire history
by computing the probability of being in each nominal state
b(s). This computation of the new belief state bo
a reached af-
ter taking action a and getting observation o from belief state
(cid:2)
b is given in the following update:

(cid:2)

a =
bo

s

(cid:2) pr(o|s
(cid:2)

, a)
s pr(s
pr(o|a, b)

(cid:2)|s, a)b(s)

.

This equation is used as the agent interacts with the dynami-
cal system to maintain the agent’s current belief state.
2.2 Point-based Value Iteration Algorithms
Point-based planning algorithms, such as PBVI and Perseus,
for partially observable dynamical systems perform planning
by constructing an approximation of the value function. By
updating the value function for one point, the value-function
approximation for nearby points is also improved. The ba-
sic idea is to calculate the value function (and its gradient)
only at a small number of points, and the value function for
the other points will be “carried along”. This point-based ap-
proach uses only a fraction of the computational resources
to construct the approximate value function, as compared to
exact methods.

There are two main sets of vectors used: the set P of points,
each of which is a belief state vector; and the set Sn which
represents the approximate value function Vn at step n. The
value function is a piecewise linear convex function over be-
lief states, deﬁned as the upper surface of the vectors α ∈ Sn.
The value for belief state b is

Vn(b) = max
α∈Sn

bT α.

The Bellman equation is used to deﬁne the update for the

(cid:4)

pr(o|a, b)Vn(bo
a)

o

pr(o|a, b) max
αi∈Sn

(bo

a)T αi

(cid:5)

(cid:5)

(cid:3)
next value function:
Vn+1(b) = max

a

(cid:3)

bT ra + γ
(cid:4)

= max

bT ra + γ

bT ra +
(cid:4)

a

(cid:3)
= max
(cid:4)
max
αi∈Sn
(cid:3)

γ

a

o
= max

(cid:2)

s
bT ra + γ

a

o

o

(cid:4)

s

, a)

(cid:2)|s, a)b(s)αi(s
(cid:2)

pr(s
(cid:5)

(cid:5)

)

pr(o|s
(cid:2)
(cid:4)

max{gi
ao} bT gi

ao

where

(cid:4)

pr(o|s
(cid:2)

(cid:2)|s, a)αi(s
(cid:2)

(cid:2)

, a)pr(s

ao(s) =
gi

(1)
for αi ∈ Sn. These g vectors are deﬁned as such because they
lead to computational savings. The computation of α vectors
for Vn+1(b) uses the so-called backup operator.

),

s

backup(b) = argmax
a}a∈A

{gb

bT gb
a,

(2)

(cid:2)

ao}i bT gi

a = ra + γ

o argmax{gi

ao. In PBVI, the

where gb
backup is applied to each point in P , as follows.
function oneStepPBVIbackup(P , Sold)
1. Snew = ∅
2. For each b ∈ P compute α = backup(b) and

set Snew = Snew ∪ α

3. return Snew

For Perseus, in each iteration the current set of α vectors
is transformed to a new set of α vectors by randomly choos-
ing points from the current set P and applying the backup
operator to them. This process is:
function oneStepPerseusBackup(P , Sold)
1. Snew = ∅, P = P , where P lists the

non-improved points

add α

(cid:2) = argmaxα

compute α = backup(b)

2. Sample b uniformly at random from P ,
3. If bT α ≥ Vn(b) then add α to Snew, else
4. Compute P = {b ∈ P : Vn+1(b) −  < Vn(b)}
5. If P is empty return Snew, otherwise
Given this, the basic (non-anytime) algorithms are, for * ∈

go to step 2

(cid:2)(cid:2)∈Sold bT α

to Snew

(cid:2)(cid:2)

[PBVI, Perseus]:
function basic*()
1. P = pickInitialPoints(), S0 =

minimalAlpha(), n = 0

2. Sn+1 = oneStep*Backup(P , Sn)
3. increment n, goto 2

where pickInitialPoints() typically uses a random walk with
distance-based metrics to choose an initial set of points, and
the function minimalAlpha() returns the α vector with all en-
tries equal to minr∈R r/(1 − γ). The algorithm will stop on
either a condition on the value function or on the number of
iterations. This algorithm is easily expanded to be an any-
time algorithm by modifying pickInitialPoints() to start with
a small set of initial points and including a step that iteratively
expands the set of current points P . Typically, having a cur-
rent set with more points will result in a better approximation
of the value function, but will require more computation to
update. The anytime versions of the algorithms are:
function anytime*()
1. P = pickInitialPoints(), S0 =
2. if(readyToAddPoints()) then P = P ∪

minimalAlpha(), n = 0

bestPointsToAdd(findCandidatePoints())

3. Sn+1 = oneStep*Backup(P , Sn)
4. increment n, goto 2

IJCAI-07

866

There are three new functions introduced in this anytime
algorithm: readyToAddPoints() which determines when the
algorithm adds new points to S; ﬁndCandidatePoints() which
is the ﬁrst step in determining the points to add; and best-
PointsToAdd() which takes the set of candidate points and
determines a set of points to add. In Section 4, we present
variations of these functions, including our main contribu-
tions, methods for determining the best points to add based
on examination of the value function.

3 Incremental PBVI and Perseus
In this section we present methods for ﬁnding the best points
to add, given a set of candidate points. In some cases, the
best points will be a subset of the candidates, and in other
cases new points that are outside the set of original candidates
will be identiﬁed as best. In all cases, we will make use of
a scalar measure called the gain of a point, which is a way
to evaluate how useful that point will be. We show how the
current (approximate) value function can be used to compute
useful measures of gain that can then be used in informed
methods of point selection.

For a point b in the belief space, the gain g(b) estimates
how much the value function will improve. The problem,
then, is to construct a measure of gain that identiﬁes the points
that will most improve the approximation of the value func-
tion. This is a complex problem, primarily because changing
the value function at one point can affect the value of many
(or all) other points, as can be seen by examining the backup
operator (Equation 2). Changes may also propagate over an
arbitrary number of time steps, as the changes to other points
propagate to yet other points. Therefore, exactly identifying
the points with best gain is much too computationally expen-
sive, and other measures of gain must be used.

Here, we present two different measures of gain: one based
on examining the backup operator, written gB, and the other
is derived from ﬁnding an upper bound on the amount of gain
that a point may have. We use linear programming to com-
pute this gain, written gLP .

Given these deﬁnitions of gain, we present two differ-
ent methods for identifying points to add. The simplest
method (detailed in Section 4) takes the candidate points, or-
ders them according to their gains, and then selects the top
few. A more sophisticated method leverages some informa-
tion present when computing gLP to identify points that have
even higher gain than the candidate points, and so returns dif-
ferent points than the original candidates. We describe how
these selection methods can be implemented in Section 4, fol-
lowing a detailed discussion (Sections 3.1 and 3.2) of the two
measures of gain.

3.1 Gain Based on One-Step Backup
The gain gB measures the difference between the current
value of point b and its value according to an α vector com-
puted by a one-step backup of b via Equation 2. This mea-
sure does not have strong theoretical support, but intuitively,
if the one-step difference for a point is large, then it will
also improve the approximations of nearby points signiﬁ-
cantly. These improvements will then have positive effects

on points that lead to these points (although the effect will
be discounted), improving many parts of the value function.
This gain is deﬁned as:

(cid:3)

gB(b) = max

a

r(b, a) + γ

= bT α − V (b),

(cid:5)

pr(o|b, a)V (bo
a)

− V (b)

(cid:4)

o

where α = backup (b). Note that this measure includes
the immediate expected reward at b, a quantity that was not
previously included in the computation of any α vector. This
measure can make use of cached ga vectors (Equation 1), and
so it is inexpensive from the computational standpoint. In the
following section, we describe a different measure of gain
that has a stronger theoretical justiﬁcation, but also comes at
the expensive of higher computational cost.
3.2 Gain Based on Value-Function Bounds
This second measure of gain that we introduce uses the fol-
lowing insight, which we ﬁrst illustrate on a system with two
nominal states, then extend the intuition to a three-state sys-
tem, and then describe the general calculation for any num-
ber of states. The value function is a piecewise linear convex
function consisting of (for two nominal states) the upper sur-
face of a set of lines deﬁned by α vectors (see Figure 1a for a
running example). The vectors which deﬁne these lines corre-
spond to points interior to the regions where the correspond-
ing lines are maximal (points A, B, and C). Intuitively, the
value function is most relevant at these points, and becomes
less relevant with increasing distance from the point. Here,
we assume that the value given by the α vector is correct at
the corresponding point. Now consider the inclusion of a new
α vector for a new point (point X in Figure 1a). If the value
function for point A is correct, then the new α vector cannot
change the value of A to be greater than its current value; the
same holds for B. Therefore, the new α vector for point X
is upper-bounded by the line between the value for A and the
value for B. Thus, the gain for X is the difference between
this upper bound and the current value of X.

For the case with three nominal states, the analogous pla-
nar solution to the upper bound is not as simple to compute
(see Figure 1b and 1c). To visualize the problem, consider
a mobile plane being pushed upward from point X, which
is constrained only to not move above any of the existing
points that deﬁne the value-function surface (A, B, C, and
D in Figure 1b). In the example depicted in Figure 1b, this
plane will come to rest on the points deﬁned by the loca-
tions of A, B, C in the belief space and their values. The
value of that plane at X then gives the tightest upper bound
for its value if we were to add a new α vector at X. The
gain is then computed in the same manner as in the two
state case above, but ﬁnding the above-described plane is not
straightforward (and becomes even more difﬁcult in higher-
dimensional spaces). The difﬁculty is that in higher dimen-
sional spaces — in contrast to the two state case where the
minimal upper bound is always deﬁned by the points imme-
diately to the left and right of the new point — selecting the
points that deﬁne that minimal upper-bounding surface is a
non-trivial task. However, this problem can be formulated as

IJCAI-07

867

e
u
l
a
V

gLP (X)

A

X B
p(s1)

C

(a)

(b)

Figure 1: (a): Value function for a dynamical system with two nominal states (note that p(s2) = 1− p(s1), so an axis for p(s2)
is unnecessary). The value function is deﬁned by the α vectors corresponding to points A, B, and C. The gain gLP (X) for point
X is the difference between the upper bound deﬁned by the line (cid:6)V (A), V (B)(cid:7) and the current value of X. (b,c): Value function
for a system with three nominal states. The value function is deﬁned by the α vectors corresponding to points A, B, C, and D,
and the gain gLP (X) for point X is the difference between the tightest upper bound (deﬁned by plane (cid:6)V (A), V (B), V (C)(cid:7))
(cid:2) is
and the current value for X. That tightest upper-bound plane is produced by the ﬁrst LP (FindGainAndRegionForPoint). X
the best point produced by the second LP (FindBestPointForRegion) for the bounding region {A, B, C}.

(c)

(cid:2)

a linear program (LP). We describe this LP in Table 1 and
refer to it as FindGainAndRegionForPoint(b0).
Given a candidate belief point b0 its current scalar value v0,
the current points {b1...bm}, the current scalar values v1...vm
of these points, and the value vectors α1...αn, this minimiza-
tion LP ﬁnds the tightest upper bound Vu(b0) =
i wivi,
which can then be used to compute the gain gLP (b0) =
Vu(b0) − V (b0). In words, this minimization LP works by
ﬁnding a set of points (these are the set of points with non-
zero weights wi) such that: i) b0 can be expressed a convex
linear combination of these points, and ii) the value func-
tion deﬁned by the plane that rests on their value-function
points is the minimal surface above v0. We call the set
of such points with non-zero weights the bounding region
Γ(b0) = {bi|wi > 0} for point b0; it has the property that
the point b0 must be interior to it, and these bounding regions
will play an important role in the next section.

There is one subtle issue regarding the above LP, which is
that not all points of potential interest are interior to the set of
current points for which α vectors are deﬁned (in fact early
in the process when there are few points, most of the useful
points will be outside the convex hull of current points). This
is problematic because we want to consider these points (they
often correspond to relatively poorly approximated areas of
the value function), but they are never interior to any subset
of the current points.

To resolve this issue, we introduce the set U of unit basis
points: the points with all components but one equal to 0, and
the remaining entry equal to 1 (the standard orthonormal basis
in the belief space). We then augment the set of current belief
points in our LP to include the unit basis points: {b1...bm} =
{P ∪ U}. The values vj of the points in U \ P are computed
as vj = backup (bj) ∀bj ∈ U \ P , since these points were
not updated in oneStep*Backup() as were the points in P .

As before, the variables in this augmented LP are the scalar
weights w1...wm that deﬁne which points from {P ∪U} form
the bounding region Γ(b0)
3.3 Finding the Best Point in a Bounding Region
Given the gains gLP for the candidate points, it is possible
to choose points to add based on this criteria, but one draw-
back to this is that we might add multiple points from the
same bounding region Γ, and thus waste resources by ap-
proximating the value function for multiple points when just
one would work almost as well. Further, it may be that none
of these points is the point in the region with the highest
gain. To address these issues, we introduce another LP called
FindBestPointForRegion(Γ) (Table 1) that takes a bounding
region and ﬁnds the point interior to that region with maximal
gLP . Note that this region may contain unit basis points, al-
lowing the new point to be exterior to the current set of points.
This LP takes as input a bounding region Γ(b0) =
{b1...bl}, scalar values v1...vl of these points, and the value
vectors α1...αn. The optimization variables are:
scalar
weights w1...wl, vector bnew , which is the point to be found,
and the scalar value vnew, which is the value of bnew. The
process of ﬁnding the best points to add will call this LP on all
unique regions found by running the ﬁrst LP (FindGainAn-
dRegionForPoint) on all candidate points.

4 Using Gains in PBVI and Perseus
We now deﬁne two incremental algorithms, PBVI-I and
Perseus-I, using the gains gB and gLP , and the linear
programs above. Referencing the anytime*() algorithm,
we explored various possibilities for readyToAddPoints(),
bestPointsToAdd(C), and ﬁndCandidatePoints(). While
many variations exist, this paper presents only a few that seem
most promising.

IJCAI-07

868

Table 1: The linear programs used to i) compute the gain and region Γ(b0) for point b0 given a value function, and ii) ﬁnd the
best point within a given region Γ given the value function.

FindGainAndRegionForPoint(b0):

(cid:4)

Minimize:

Given:
Variables:
Constraints:

wivi

i

b0, v0, b1...bm, v1...vm, α1...αn
(cid:4)
w1...wm
wivi ≥ v0
b0 =
(cid:4)

wibi;

(cid:4)

wi = 1; wi ≥ 0 ∀i ∈ [1..m]

i

i

FindBestPointForRegion(Γ)

(cid:6)(cid:4)

(cid:7)

− vnew

wivi

i

Maximize:

Given:
Variables:
Constraints:

(cid:4)

b1...bl, v1...vl, α1...αn
w1...wl, vnew, bnew
bnew =
(cid:4)

i
wi = 1;

wibi; wi ≥ 0 ∀i ∈ [1..l]

vnew ≥ αjbnew ∀j ∈ [1..l]

i

i

a

Two computationally cheap possibilities for readyToAdd-
Points() are to add points: i) after a ﬁxed number of iterations,
and ii) when the update process has approximately converged,
i.e., the maximal one-step difference in value over all points is
below some threshold (maxb∈P (Vn+1(b) − Vn(b)) ≤ ). We
have also explored two methods for ﬁndCandidatePoints().
The ﬁrst is to keep a list of points that are one-step successors
to the points in P , the set {bo
|b ∈ P}. The second method is
to do a random walk and use a distance threshold to choose
points, just as in pickInitialPoints().

The gains discussed in the previous section can then be
used for bestPointsToAdd(C). Gain gB orders the candi-
date points C and the top k are chosen. For gain gLP ,
the candidate points deﬁne a set of unique bounding regions
{Γ(b)|b ∈ C}. For each region, the LP FindBestPointFor-
(cid:2). These candidate
Region() returns the best candidate point b
(cid:2)) and the top k are chosen. To
points are ordered by gLP (b
compare our methods against a baseline anytime algorithm,
we used the standard random walk method, choosing the ﬁrst
k points that exceeded a distance threshold.

5 Experiments
To evaluate our principled point selection algorithms, we con-
ducted testing on a set of standard POMDP domains. Three of
the problems are small-to-mid sized, and the fourth (Hallway)
is larger. Domain deﬁnitions may be obtained at [Cassan-
dra, 1999]. We tested three algorithms: i) a baseline, anytime
Perseus using the distance-based metric; ii) one-step backup
Perseus-I using gB; and iii) LP-based Perseus-I using gLP
and the two LPs for ﬁnding the best points.
All three algorithms used the convergence condition with
 = 1E − 3 for readyToAddPoints(). The algorithms for Net-
work, Shuttle, and 4x3 added up to 3 points at a time, while
Hallway added up to 30 points at a time. However, choos-
ing points using LPs often did not add all 30 because fewer
unique regions were found. In the one-step backup Perseus-
I, we used the successor metric for ﬁndCandidatePoints() as
it was easily integrated with little overhead, while in the LP-
based Perseus-I we used a random walk with a distance met-
ric as it introduced the least overhead in this case. The results
were averaged over 30 runs, and the performance metric is
the average discounted reward, for a given initial belief state.
The results are presented in the graphs in Figure 2, which are

of three types. The ﬁrst type (performance vs. number of
points) is a measure of the effectiveness of point selection.
The second type (performance vs. number iterations) and the
third type (performance vs. cpu time) both measure how well
the use of principled point selection affects the overall per-
formance, but the third type shows the impact of additional
time incurred by the point-selection process. These graphs
show performance on the x-axis, and the amount of resource
(points, iterations, or time) used to achieve that performance
on the y-axis. When comparing two algorithms with respect
to a given resource, the better algorithm will be lower.

Examination of the ﬁrst-type graphs shows that the LP-
based Perseus-I did a good job of identifying the best points to
add. The resulting policies produced better performance with
fewer points than either of the other two methods. Further-
more, on three of the problems, the one-step backup Perseus-
I also did signiﬁcantly better than the baseline for choosing
points. Thus, for this metric, our goal of ﬁnding useful points
has been achieved. Most promisingly, on the largest problem
(Hallway) our algorithms used signiﬁcantly fewer points to
achieve good performance. However, the other graphs show
that choosing good points does not always translate into a
faster algorithm. On one problem (Network) both versions
of Perseus-I were faster than the baseline, but on the other
problems, the performance was either closely competitive or
slightly worse due to the overhead incurred by identifying
good points. While this result was not optimal, the fact that
there were signiﬁcant speedups on one problem combined
with the point selection results leads us to draw the conclusion
that this method is promising and needs further investigation
and development.

References
[Cassandra, 1999] A. Cassandra.

page.
http://www.cs.brown.edu/research/ai/ pomdp/index.html, 1999.
[Izadi et al., 2005] Masoumeh T. Izadi, Ajit V. Rajwade, and Doina
Precup. Using core beliefs for point-based value iteration. In 19th
International Joint Conference on Artiﬁcial Intelligence, 2005.

pomdp

Tony’s

[James et al., 2004] Michael R.

and
Michael L. Littman. Planning with predictive state representa-
tions. In The 2004 International Conference on Machine Learn-
ing and Applications, 2004.

James, Satinder Singh,

IJCAI-07

869

Points vs. Performance

Iterations vs. Performance

Time vs. Performance

160

140

120

100

80

60

40

20

0

−200

20

15

10

5
0

70

60

50

40

30

20

10

0
−1

1400

1200

1000

800

600

400

200

0
0

LP
Distance
One Step

−100

0

100

200

300

5

10

15

20

25

30

−0.5

0

0.5

1

1.5

2

2.5

0.2

0.4

0.6

0.8

1

500

450

400

350

300

250

200

150

100

50

0

−200

150

100

50

0
0

400

350

300

250

200

150

100

50

0
−1

250

200

150

100

50

0
0

0.5

0.4

0.3

0.2

0.1

0

−200

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0
0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
−1

1800

1600

1400

1200

1000

800

600

400

200

0
0

k
r
o
w
t
e
N

e
l
t
t
u
h
S

3

x

4

y
a
w

l
l
a
H

−100

0

100

200

300

5

10

15

20

25

30

−0.5

0

0.5

1

1.5

2

2.5

0.2

0.4

0.6

0.8

1

−100

0

100

200

300

5

10

15

20

25

30

−0.5

0

0.5

1

1.5

2

2.5

0.2

0.4

0.6

0.8

1

Figure 2: Empirical comparisons of three methods for selecting new points during value iteration. Each graph evaluates
performance against one of three metrics—number of points, total number of iterations used in constructing a policy, and total
CPU time. The y-axis of each graph represents the average value of the column’s metric required to earn a particular reward
(x-axis) during empirical evaluation. Experiments were conducted on four dynamical systems (one per row): Network, Shuttle,
4x3, and Hallway. Lower values are indicative of less resource consumption, and so are more desirable.

[Pineau et al., 2003] J. Pineau, G. Gordon, and S. Thrun. Point-
based value iteration: An anytime algorithm for POMDPs.
In
Proc. 18th International Joint Conference on Artiﬁcial Intelli-
gence, Acapulco, Mexico, 2003.

[Pineau et al., 2005] J. Pineau, G. Gordon, and S. Thrun. Point-
based approximations for fast pomdp solving. Technical Report
SOCS-TR-2005.4, McGill University, 2005.

[Shani et al., 2005] Guy Shani, Ronen Brafman, and Solomon Shi-
mony. Model-based online learning of POMDPs. In 16th Euro-

pean Conference on Machine Learning, 2005.

[Singh et al., 2004] Satinder Singh, Michael R.

and
Matthew R. Rudary. Predictive state representations, a new the-
ory for modeling dynamical systems. In 20th Conference on Un-
certainty in Artiﬁcial Intelligence, 2004.

James,

[Spaan and Vlassis, 2005] M.T.J Spaan and N. Vlassis. Perseus:
Randomized point-based value iteration for pomdps. Journal of
Artiﬁcial Intelligence Research, 24:195–220, 2005.

IJCAI-07

870


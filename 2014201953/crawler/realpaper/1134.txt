Efﬁciently Exploiting Symmetries in Real Time Dynamic Programming

Shravan Matthur Narayanamurthy and Balaraman Ravindran

Department of Computer Science and Engineering

Indian Institute of Technology Madras

shravmn@cse.iitm.ac.in and ravi@cse.iitm.ac.in

Abstract

Current approaches to solving Markov Decision
Processes (MDPs) are sensitive to the size of the
MDP. When applied to real world problems though,
MDPs exhibit considerable implicit redundancy,
especially in the form of symmetries. Existing
model minimization methods do not exploit this re-
dundancy due to symmetries well.
In this work,
given such symmetries, we present a time-efﬁcient
algorithm to construct a functionally equivalent re-
duced model of the MDP. Further, we present a
Real Time Dynamic Programming (RTDP) algo-
rithm which obviates an explicit construction of the
reduced model by integrating the given symmetries
into it. The RTDP algorithm solves the reduced
model, while working with parameters of the origi-
nal model and the given symmetries. As RTDP uses
its experience to determine which states to backup,
it focuses on parts of the reduced state set that are
most relevant. This results in signiﬁcantly faster
learning and a reduced overall execution time. The
algorithms proposed are particularly effective in the
case of structured automorphisms even when the
reduced model does not have fewer features. We
demonstrate the results empirically on several do-
mains.

1 Introduction

Markov Decision Processes (MDPs) are a popular way to
model stochastic sequential decision problems. But most
modeling and solution approaches to MDPs scale poorly with
the size of the problem. Real world problems often tend to be
very large and hence do not yield readily to the current so-
lution techniques. However, models of real world problems
exhibit redundancy that can be eliminated, reducing the size
of the problem.
One way of handling redundancy is to form abstractions, as
we humans do, by ignoring details not needed for performing
the immediate task at hand. Researchers in artiﬁcial intelli-
gence and machine learning have long recognized the impor-
tance of abstracting away redundancy for operating in com-
plex and real-world domains [Amarel, 1968]. Given a model,

ﬁnding a functionally equivalent smaller model using this ap-
proach forms the crux of the model minimization paradigm.
Identifying symmetrically equivalent situations frequently re-
sults in useful abstraction. Informally, a symmetric system
is one which is invariant under certain transformations onto
itself. An obvious class of symmetries is based on geomet-
ric transformations such as, rotations, reﬂections and transla-
tions. Existing work on model minimization of MDPs such as
[Givan et al., 2003] and [Zinkevich and Balch, 2001] do not
handle symmetries well. They either fail to consider state-
action equivalence or do not provide speciﬁc algorithms to
minimize an MDP, considering state-action equivalence.

In this article we consider a notion of symmetries, in the
form of symmetry groups, as formalized in [Ravindran and
Barto, 2002]. Our objective here is to present algorithms that
provide a way of using the symmetry information to solve
MDPs, thereby achieving enormous gains over normal solu-
tion approaches. First, we present a time-efﬁcient algorithm
(G-reduced Image Algorithm) to construct a reduced model
given the symmetry group. The reduced model obtained is
functionally equivalent to the original model in that, it pre-
serves the dynamics of the original model. Hence a solu-
tion in the reduced model will lead to a solution in the origi-
nal model. However, the reduced model can be signiﬁcantly
smaller when compared to the original model depending on
the amount of symmetry information supplied. Thus, solv-
ing the reduced model can be a lot easier and faster. Fur-
ther, we identify that an explicit construction of a reduced
model is not essential for the use of symmetry information
in solving the MDP. We use the G-reduced Image Algorithm
as a basis to present the Reduced Real Time Dynamic Pro-
gramming(RTDP) algorithm that integrates the symmetry in-
formation into the RTDP algorithm [Barto et al., 1995] used
for solving MDPs. Though the algorithm works directly with
the original model it considers only a portion of the original
model that does not exhibit redundancy and also is relevant
in achieving its goals. This focus on the relevance of states
results in signiﬁcantly faster learning leading to huge savings
in overall execution time. To make the algorithms more ef-
fective, especially in terms of space, we advocate the use of
certain structural assumptions about MDPs. We use several
domains to demonstrate the improvement obtained by using
the reduced RTDP algorithm.

After introducing some notation and background informa-

IJCAI-07

2556

tion in Sec. 2, we present the G-reduced Image Algorithm
in Sec. 3. We then present the reduced RTDP algorithm in
Sec. 4. The experiments done and results achieved are pre-
sented in Sec. 5. Finally we conclude the article by giving
some directions for future work in Sec. 6.

2 Notation and Background

2.1 Markov Decision Processes
A Markov Decision Process is a tuple (cid:2)S, A, Ψ, P, R(cid:3), where
S = {1, 2, . . . , n} is a set of states, A is a ﬁnite set of actions,
Ψ ⊆ S × A is the set of admissible state-action pairs, P : Ψ ×
(cid:6))
S → [0, 1] is the transition probability function with P(s, a, s
(cid:6)
under
being the probability of transition from state s to state s
action a, and R : Ψ → R is the expected reward function,
with R(s, a) being the expected reward for performing action
= {a|(s, a) ∈ Ψ} ⊆ A denote the set
a in state s. Let As
actions admissible in state s. We assume that ∀s ∈ S, As is
non-empty.
(cid:2)
A stochastic policy π is a mapping Ψ → [0, 1], s.t.,
π(s, a) = 1, ∀s ∈ S. The value of a state s under pol-
a∈As
icy π is the expected value of the discounted sum of future
rewards starting from state s and following policy π there-
corresponding to a policy π is
after. The value function V
the mapping from states to their values under π. It can be
(cid:3)
shown that V

satisﬁes the bellman equation:

(cid:3)

(cid:5)

π

π

(cid:4)
R(s, a) + γ

π(s) =

V

π(s, a)

P(s, a, s

(cid:6))V

π(s
(cid:6))

(1)

a∈As

s(cid:6)∈S
where 0 ≤ γ < 1 is a discount factor.

The solution of an MDP is an optimal policy π∗

that uni-
formly dominates all other policies for that MDP. In other
words V

π(s) for all s ∈ S and for all π.

π∗ (s) ≥ V

2.2 Factored Markov Decision Processes
Factored MDPs are a popular way to model structure in
MDPs. A factored MDP is deﬁned as a tuple (cid:2)S, A, Ψ, P, R(cid:3).
M
The state set, given by M features or variable, S ⊆
i=1 Si,
where Si is the set of permissible values for the feature i, A
is a ﬁnite set of actions, Ψ ⊆ S × A is the set of admissible
state-action pairs. The transition probabilities P are often de-
scribed by a two-slice Temporal Bayesian Network (2-TBN).
The state transition probabilities can be factored as:

(cid:6)

M(cid:7)

i=1

P(s, a, s

(cid:6)) =

(cid:6)
Prob(s
i

(cid:6)
|Pre(s
i

, a))

(2)

(cid:6)
, a) denotes the parents of node s
i

(cid:6)
where Pre(s
in the 2-
i
TBN corresponding to action a and each of the probabilities
(cid:6)
(cid:6)
, a)) is given by a conditional probabilitiy table
|Pre(s
Prob(s
i
i
(cid:6)
. The reward function may be simi-
associated with node s
i
larly represented.

2.3 Homomorphisms and Symmetry Groups
This section has been adapted from [Ravindran and Barto,
2002].

Let B be a partition of a set X. For any x ∈ X, [x]

B denotes
the block of B to which x belongs. Any function f from a set

f

(cid:6)

(cid:6)

(cid:6)]

f x

= [x

(cid:6)) and x, x

f if and only if f (x) = f (x

X to a set Y induces a partition (or equivalence relation) on X,
with [x]
are f -
. Let B be a partition of Z ⊆ X × Y,
equivalent written x ≡
where X and Y are arbitrary sets. The projection of B onto X
is the partition B|X of X such that for any x, x
=
[x
B|X if and only if every block containing a pair in which x
is a component
is a component also contains a pair in which x
(cid:6)
is a component
or every block containing a pair in which x
also contains a pair in which x is a component.

(cid:6) ∈ X, [x]

B|X

(cid:6)]

(cid:6)

Deﬁnition 1. An MDP homomorphism h from an MDP
M = (cid:2)S, A, Ψ, P, R(cid:3) to an MDP M(cid:6) = (cid:2)S
(cid:6)(cid:3)
is a surjection from Ψ to Ψ(cid:6)
, deﬁned by a tuple of surjections
(cid:6)
|s ∈ S}(cid:3), with h((s, a)) = ( f (s), gs
(cid:2) f, {gs
(a)), where f : S → S
: As
and gs
(cid:3)

f (s) for s ∈ S, such that: ∀s, s

(cid:6) ∈ S, a ∈ As

(cid:6), Ψ(cid:6), P

→ A

(cid:6), A

(cid:6), R

(cid:6)

(cid:6)( f (s), gs

P

(a), f (s

(cid:6))) =

P(s, a, s

(cid:6)(cid:6))

(cid:6)( f (s), gs
R

s(cid:6)(cid:6)∈[s(cid:6)]
(a)) = R(s, a)

f

(3)

(4)

We use the shorthand h(s, a) for h((s, a)).

(cid:6), A

(cid:6), Ψ(cid:6), P

Deﬁnition 2. An MDP homomorphism h = (cid:2) f, {gs
|s ∈
S}(cid:3) from MDP M = (cid:2)S, A, Ψ, P, R(cid:3) to MDP M(cid:6)
=
(cid:6)(cid:3) is an MDP isomorphism from M to M(cid:6)
(cid:2)S
if and only if f and gs, are bijective. M is said to be isomor-
phic to M(cid:6)
and vice versa. An MDP isomorphism from MDP
M to itself is call an automorphism of M.

(cid:6), R

Deﬁnition 3. The set of all automorphisms of an MDP M,
denoted by AutM, forms a group under composition of ho-
momorphisms. This group is the symmetry group of M.

BG

= [(s2, a2)]

Let G be a subgroup of AutM. The subgroup G in-
duces a partition BG of Ψ: [(s1, a1)]
BG if and
only if there exists h ∈ G such that h(s1, a1) = (s2, a2) and
(s1, a1), (s2, a2) are said to be G equivalent written (s1, a1) ≡G
(s2, a2). Further if s1 ≡
BG|S s2 then we write as shorthand
s1 ≡G|S s2. It can be proved that there exists a homomorphism
, such that the partition induced by
h
G
is called
h
the G-reduced image of M.

from M to some M(cid:6)
, BhG , is the same BG. The image of M under h

G

G

Adding structure to the state space representation allows
us to consider morphisms that are structured, e.g., Projection
homomorphisms (see sec. 5 of [Ravindran and Barto, 2003]).
It can be shown that symmetry groups do not result in projec-
tion homomorphisms, except in a few degenerate cases.
Another simple class of structured morphisms that do lead to
(cid:2)
useful symmetry groups are those generated by permutations
M be the set of all possible permuta-
of feature values. Let
(cid:2)
tions of {1, . . . , M}. Given a structured set X ⊆
M
i=1 Xi and
a permutation σ ∈
M, we can deﬁne a permutation on X by
(cid:3)) = (cid:2)xσ(1), . . . , xσ(M)(cid:3), and it is a valid permu-
σ((cid:2)x1, . . . , xM
tation on X if xσ(i) ∈ Xi for all i and for all (cid:2)x1, . . . , xM
(cid:3) ∈ X.

(cid:6)

Deﬁnition 4. A permutation automorphism h on a structured
MDP M = (cid:2)S, A, Ψ, P, R(cid:3) is a bijection on Ψ , deﬁned by a
tuple of bijections (cid:2) f (s), gs
(a)),
: S → S is a valid permutation on S and
where f ∈

(a)(cid:3), with h((s, a)) = ( f (s), gs

(cid:2)

M

IJCAI-07

2557

gs

: As

→ A

(cid:6)( f (s), gs

P

(cid:6)

f (s) for s ∈ S, such that: ∀s, s
(a), f (s

(cid:6))
(cid:6))) = P(s, a, s

(cid:6) ∈ S, a ∈ As

M(cid:7)

=

i=1

(cid:6)( f (s), gs
R
Here f (Pre f (s)(s

(a)) = R(s, a)

(cid:6)
f (i)

, a)) = {s f (j)|sj

assigned according to f (s).

Prob(s

(cid:6)
f (i)

| f (Pre f (s)(s

(cid:6)
f (i)

, a)))

(5)

(6)
, a)} with s f (j)

∈ Pre(s

(cid:6)
f (i)

3 G-reduced Image Algorithm

3.1 Motivation

In a large family of tasks, the symmetry groups are known
beforehand or can be speciﬁed by the designer through a su-
perﬁcial examination of the problem. A straight forward ap-
proach to minimization using symmetry groups would require
us to enumerate all the state-action pairs of the MDP. Even
when the symmetry group, G, is given, constructing the re-
duced MDP by explicit enumeration takes time proportional
to |Ψ|.|G|.

We present in Fig. 1, an efﬁcient incremental algorithm for
building the reduced MDP given a symmetry group or sub-
group. This is an adaptation of an algorithm proposed by
[Emerson and Sistla, 1996] for constructing reduced models
for concurrent systems.

(cid:6), A

(cid:6), a

(cid:6)) for some (s

(cid:6), a

If (s, a) (cid:2)G (s

(cid:6), R
(cid:6)(cid:3).
(cid:6) ← {s0}

s = dequeue{Q}
For all a ∈ As

01 Given M = (cid:2)S, A, Ψ, P, R(cid:3) and G ≤ AutM,
(cid:6), Ψ(cid:6), P
02 Construct M/BG = (cid:2)S
03 Set Q to some initial state {s0}, S
04 While Q is non-empty
05
06
07
08
09
10
11
12
13
14
15
16
17

Ψ(cid:6) ← Ψ(cid:6) ∪ (s, a)
(cid:6) ← A
A
(cid:6)(s, a) = R(s, a)
R
For all t ∈ S such that P(s, a, t) > 0

P
else
(cid:6) ← S
S
(cid:6)(s, a, t) = P(s, a, t)
P
add t to Q.

If t ≡G|S s
(cid:6)(s, a, s

(cid:6), for some s
(cid:6)) ← P

(cid:6)(s, a, s

(cid:6),
(cid:6) ∈ S

(cid:6) ∪ a

(cid:6) ∪ t

(cid:6)) ∈ Ψ(cid:6), then

(cid:6)) + P(s, a, t)

Figure 1:
Incremental algorithm for constructing the G-
reduced image given MDP M and some G ≤AutM. Q is
the queue of states to be examined. Terminates when at least
one representative from each equivalence class of G has been
examined.

3.2 Comments

The algorithm does a breadth-ﬁrst enumeration of states skip-
ping states and state-action pairs that are equivalent to those
already visited. On encountering a state-action pair not equiv-
alent to one already visited, it examines the states reachable

from it to compute the image MDP parameters. The algo-
rithm terminates when at least one representative from each
equivalence class of G has been examined. For a proof that
the transition probabilities actually represent those for the re-
duced image, see App. A. The algorithm as presented as-
sumes that all states are reachable from the initial state. It is
easy, however, to modify the algorithm suitably. Assuming an
explicit representation for the symmetry group and that table
look-up takes constant time, the algorithm will run in time
proportional to |Ψ|(cid:6).|G|. However an explicit representation
of G demands exorbitant memory of the order of |G|.|Ψ|.

As discussed in Sec. 2.3, structured morphisms can be used
advantageously to reduce the state space. The advantage here
is that the morphisms forming the symmetry group need not
be stored explicitly as they are deﬁned on the features instead
of states.

For example, let us consider the case of permutation auto-
(cid:6)), we need to
morphisms. To check whether (s, a) ≡G (s
(cid:6)) by applying
generate |G| states that are equivalent to (s
each h ∈ G. Each application of h incurs a time linear in the
number of features. Thus in this case the time complexity of
the algorithm presented is of the order of |Ψ|(cid:6).|G|.M, where
M is the number of features whereas no space is needed for
storing the G explicitly.

(cid:6), a
(cid:6), a

Thus by restricting the class of automorphisms to func-
tions that are deﬁned on features instead of states, we only
incur additional time which is a function of the number of
features (signiﬁcantly less than the number of states) along
with a drastic decrease in the space complexity. The use of
factored representations leads to further reduction in space
needed for storing the transition probabilities and the reward
function, thereby making the algorithm presented more effec-
tive than its use in the generic case. Also as G is just a sub-
group, the algorithm can work with whatever little symmetry
information the designer might have.

4 Reduced RTDP Algorithm

4.1 Motivation

Given a real world problem modeled as an MDP, the state
space invariably consists of vast regions which are not rele-
vant in achieving the goals. The minimization approach leads
to a certain degree of abstraction which reduces the extent of
such regions. Nonetheless the reduced image still contains
regions which are not relevant in achieving the goals even in
the reduced model. Since our goal here is only to ﬁnd a pol-
icy for acting in the original model, we can forgo the explicit
construction of the reduced model by integrating the informa-
tion in the symmetry group into the algorithm which solves
the original model. Though there are a variety of ways to
solve an MDP, we choose to take up RTDP as it uses the ex-
perience of the agent to focus on the relevant sections of the
state space. This saves the time spent on explicit construction
of the reduced model.

Also the G-reduced Image algorithm as presented doesn’t
preserve any structure in the transition probabilities or the
reward function that might have existed because of the use
of factored representations. Consequently the reduced image
might take considerably more space than the original model.

IJCAI-07

2558

01 Given M = (cid:2)S, A, Ψ, P, R(cid:3) and G ≤ AutM,
02 Hashtable Q ← Nil is the action value function.
03 Repeat (for each episode)
(cid:6) ← {s}
04
05

Initialize s and S
Choose a from s using policy derived from Q (e.g.
-greedy policy)
Repeat (for each step in the episode)
(cid:6)(cid:6), a

(cid:6)(cid:6)) for some (s

(cid:6)(cid:6)) ∈ Q

(cid:6)(cid:6), a
(cid:6)(cid:6)) (cid:3) (s, a)

if (s, a) ≡G (s
(cid:6)(cid:6), a
where (s
(cid:6)(cid:6); a ← a
s ← s
continue.

(cid:6)(cid:6)

(cid:6)
from s

Take action a and observe reward r and
(cid:6)
next state s
(cid:6)
Choose a
(e.g.-greedy policy)
For all t such that P(s, a, t) > 0
(cid:6),
(cid:6)(cid:6) ∈ S

(cid:6)(cid:6), for some s

If t ≡G|S s

using policy derived from Q

(cid:6)(s, a, s

(cid:6)(cid:6)) + P(s, a, t)

(cid:6)(s, a, s

(cid:6)(cid:6)) ← P

P
else
(cid:6) ← S
S
(cid:6)(s, a, t) = P(s, a, t)
P

(cid:6) ∪ t

if (s, a) (cid:4) Q

add (s, a) to Q.
Q(s, a) ← 0
(cid:3)
Q(s, a) ← R(s, a)

+

γ.P

(cid:6)(s, a, s

s(cid:6)(cid:6)∈S(cid:6)
(cid:6); a ← a

s ← s

(cid:6)

(cid:6)(cid:6)). max
a(cid:6)(cid:6)∈As

(cid:6)(cid:6)

Q(s

(cid:6)(cid:6), a

(cid:6)(cid:6))

06
07

08
09
10

11

12
13
14
15
16
17
18
19
20
21

22

the reduced image. As normal RTDP converges to an optimal
action value function [Barto et al., 1995], the reduced RTDP
also converges, as long as it continues to back up all states in
the reduced image.

The complete construction of the reduced image can take
up a considerable amount of time mapping all the irrelevant
states into the reduced model whereas with the use of this
algorithm one can get near optimal policies even before the
construction of a reduced image is complete. It is also faster
than the normal RTDP algorithm as its state space is reduced
by the use of the symmetry group information.

5 Experiments and Results

Experiments were done on three domains that are explained
below. To show the effect of the degree of symmetry consid-
ered in the domain we consider a 2-fold symmetry for which
G is a strict subgroup of AutM, that is, G < AutM and full
symmetry G = AutM. We compare the reduced RTDP al-
gorithm using these 2 degrees of symmetry with the normal
RTDP algorithm. We present learning curves representing the
decrease in the number of steps taken to ﬁnish each episode.
To show the time efﬁciency of the reduced RTDP algorithm
we present a bar chart of times taken by the reduced RTDP al-
gorithm using 2 degrees of symmetry and the normal RTDP
algorithm for completing 200 episodes of each domain. All
the algorithms used a discount factor, γ = 0.9. An epsilon
greedy policy with  = 0.1 was used to choose the actions
at each step. Due to lack of space we present one graph per
domain though experiments were done with different sizes of
each domain. The results are similar in other cases. We note
exceptions if any as is relevant.

5.1 Deterministic Grid-World(DGW)

Two Grid-Worlds of sizes 10x10 and 25x25 with four de-
terministic actions of going UP, DOWN, RIGHT and LEFT
were implemented. The initial state was (0,0) and the goal
states were {(0,9),(9,0)} and {(0,24),(24,0)} respectively. For
the 2-fold symmetry, states about NE-SW diagonal, i.e., (x,y)
and (y,x) were equivalent. If the grid is of size M × N then
let maxX = M − 1 and maxY = N − 1. For the full sym-
metry case, states (x,y), (y,x), (maxX-x,maxY-y) and (maxY-
y,maxX-x) were equivalent. State action equivalence was de-
ﬁned accordingly.

5.2 Probabilistic Grid-World(PGW)

Two Grid-Worlds of sizes 10x10 and 25x25 with four actions
of going UP, DOWN, RIGHT and LEFT were implemented.
Unlike the deterministic domain, here actions led to the rele-
vant grid only with a probability of 0.9 and left the state un-
changed with a probability of 0.1. The initial state was (0,0)
and the goal states were {(0,9),(9,0)} and {(0,24),(24,0)} re-
spectively. For the 2-fold symmetry, states about NE-SW di-
agonal, i.e., (x,y) and (y,x) were equivalent. For the full sym-
metry case, states (x,y), (y,x), (maxX-x,maxY-y) and (maxY-
y,maxX-x) were equivalent. State action equivalence was de-
ﬁned accordingly.

Figure 2: RTDP algorithm with integrated symmetries which
computes the Action Value function for the reduced MDP
without explicitly constructing it.

The algorithm we present in Fig. 2 tries to achieve the best
of both worlds as it not only works with the original model
but also includes the state space reduction by integrating the
symmetry group information into the RTDP algorithm.

4.2 Convergence of Reduced RTDP

The algorithm is a modiﬁcation of the RTDP algorithm with
steps from the previous algorithm integrated into lines 7 to
(cid:6)
, then
17. If we assume that we have the reduced MDP M
leaving out lines 7 to 9 and lines 12 to 17 leaves us with
the normal RTDP algorithm being run on the reduced image
since as explained below, for all (s, a) ∈ Ψ(cid:6), R
(cid:6)(s, a) = R(s, a).
Due to the equivalence tests done at lines 7 and 13, the algo-
rithm maintains a policy for and considers only the reduced
state space. From App. A, lines 12 to 17 compute the transi-
tion probabilities for the reduced image. From Eqn. 6, R(s, a)
is the expected reward under the reduced image. So for all
(s, a) ∈ Ψ(cid:6), R
(cid:6)(s, a) = R(s, a). Thus the update equation in
line 21 can be rewritten as,

(cid:3)

Q(s, a) = R

(cid:6)(s, a) +

γ.P

s(cid:6)(cid:6)∈S(cid:6)

(cid:6)(s, a, s

(cid:6)(cid:6)). max
a(cid:6)(cid:6)∈As

(cid:6)(cid:6)

Q(s

(cid:6)(cid:6), a

(cid:6)(cid:6))

(7)

which is nothing but the update equation for the reduced im-
age. Thus it is exactly similar to running normal RTDP on

IJCAI-07

2559

2500

2000

1500

1000

500

e
d
o
s
i
p
E
 
r
e
p

 
s
p
e
t

S
#

 

0

0

Normal RTDP

Reduced RTDP
2-fold symmetry

Reduced RTDP
full symmetry

50

100

# Episodes

150

200

5000

4000

3000

2000

1000

e
d
o
s
i
p
E
 
r
e
p
 
s
p
e
t

S
#

 

0

0

Normal RTDP

Reduced RTDP
2-fold symmetry

Reduced RTDP
full symmetry

50

100

# Episodes

150

200

Figure 3: Learning curves for the Deterministic Grid
World(25x25 grid).

Figure 5: Learning curves for the Probabilistic Towers of
Hanoi(5 disks).

e
d
o
s
i
p
E
 
r
e
p
 
s
p
e
t
S
 
#

3000

2500

2000

1500

1000

500

0

0

Normal RTDP

Reduced RTDP
2-fold symmetry

Reduced RTDP
full symmetry

50

100

# Episodes

150

200

Figure 4:
World(25x25 grid).

Learning curves for

the Probabilistic Grid

5.3 Probabilistic Towers of Hanoi(PTOH)

The towers of Hanoi domain as implemented had 3 pegs. Two
domains, one with 3 and the other with 5 disks were imple-
mented. Actions that allowed the transfer of a smaller disk
onto a larger disk or to an empty peg were permitted. The
actions did the transfer of disk with a probability of 0.9 and
left the state unchanged with a probability of 0.1. The initial
state in the case of 3 disks was {(1,3), (2), ()} and {(4), (1,2),
(3,5)} in the 5 disk case. The goal states were designed to
allow various degrees of symmetry. For a 2-fold symmetry,
the goal states considered were states where, all disks were
either on peg 1 or 2. Equivalent states were those that have
disk positions of pegs 1 and 2 interchanged. For the full sym-
metry case, the goal states considered were states where, all
disks were on any one peg. Equivalent states were those that
have disk positions interchanged by any possible permutation
of the pegs. State action equivalence was deﬁned accordingly.

5.4 Time efﬁciency

The bar-graph in Fig. 6 shows the running times (scaled to
even the graph) of the normal RTDP, reduced RTDP with a
2-fold symmetry and reduced RTDP with full symmetry. The
ﬁrst cluster is on the Deterministic Grid-World domain with a
25x25 grid, the second cluster is on Probabilistic Grid-World
with a 25x25 grid and the third on Probabilistic Towers of

Hanoi with 5 disks.1

s
e
d
o
s
i
p
e
 
0
0
2

 
r
o
f
 
n
e
k
a
t
 
)
d
e
l
a
c
s
(
e
m
T

i

140

120

100

80

60

40

20

0

1

2

3

Different Domains

Figure 6: Comparison of running times(scaled).

5.5 Discussion

The results are as expected. The comparisons show that the
reduced RTDP algorithm learns faster than the normal RTDP
both in the full symmetry case as well as in the 2-fold sym-
metry case. Further among the reduced RTDP algorithms, the
one using more symmetry is better than the one with lesser
symmetry. The same is reﬂected in the running times of algo-
rithms. The full symmetry case is at least 5 times faster than
the normal RTDP. The 2-fold symmetry is also faster than the
normal RTDP.

One observation contrary to the graph shown in the bar
graph of Fig. 6 is that when reduced RTDP algorithms are
used for very small domains like 3-disk Towers of Hanoi,
the overhead involved in checking equivalence of states out-
weighs the beneﬁt from the reduction due to symmetry.
Though we have not been able to quantify the exact extent
of the trade-offs involved, we feel that when the expected
length of a trajectory to the goal state from the initial state
is small in comparison with the state space, the beneﬁts ob-
tained by using the symmetry group information are masked

1Running times for domains of lesser size do not follow the pat-

tern indicated by the graphs. See Sec. 5.5

IJCAI-07

2560

by the overhead involved in doing the equivalence compar-
isons. However, this is true only in case of very small do-
mains. In any domain of reasonable size the agent implement-
ing normal RTDP has to explore vast spaces before arriving
at the correct trajectory. But, for an agent implementing re-
duced RTDP, the symmetry restricts exploration to a smaller
space. Also, greater the symmetry used, lesser the space that
has to be explored. This explains the better performance of
the reduced RTDP algorithm.

6 Conclusions and Future Work

The algorithms presented in this article provide an efﬁcient
way of exploiting varying amounts of symmetry present in
a domain resulting in faster learning and reduced execution
times. With the use of structured morphisms on factored
representations the algorithms are even more effective, espe-
cially in terms of space.

The notion of equivalence used here is very strict. One
direction for future work, that we perceive, is to include no-
tions of approximate equivalence. Another possibility will be
to quantify the exact trade-offs involved due to overheads of
checking equivalence and the performance gained by the use
of symmetries. As we assume that the symmetry group infor-
mation is input to the algorithm, another direction to proceed
will be to attempt symmetry detection, which has been dis-
cussed in [Puget, 2005a] and [Puget, 2005b].

A Transition probabilities computed for the

reduced model

(cid:6), P

(cid:6), R

(cid:6), Ψ

(cid:6), A

Let M = (cid:2)S, A, Ψ, P, R(cid:3) be an MDP and G the given sym-
metry group. Let BG be the partition induced by G. Let
(cid:6)(cid:3) be the reduced image. Let
M/BG = (cid:2)S
ρ(s, a) denote the set of states reachable from state s by doing
∩ ρ(s, a) = ∅}. When
action a. Let BG|S = {[s]
(s, a) is used with P
, they represent blocks whereas when
used with P,
From Def. 1 the transition probabilities, P
(cid:3)

(s, a) ∈ S and are representatives for [s, a]

∀(s, a) ∈ Ψ

(cid:6)]
(cid:6), ∀[s

∈ BG|S

, satisfy,

| [s]

BG|S

BG|S

BG

(cid:6)

(cid:6)

BG|S
(cid:6)]
(cid:6)(s, a, [s

P

) =

BG|S

P(s, a, s

(cid:6)(cid:6))

(8)

s(cid:6)(cid:6)∈[s(cid:6)]

BG |S

By the deﬁnition of BG|S,

∀(s, a) ∈ Ψ
(cid:2)

(cid:6)]
(cid:6), ∀[s

BG|S

∈ BG|S

(cid:6)]
(cid:6)(s, a, [s

P

BG|S

) = 0 (9)

As

s(cid:6)(cid:6)∈([s(cid:6)]

BG |S

−ρ(s,a)) P(s, a, s

(cid:6)(cid:6)) = 0

Hence Eqn. 10 can be rewritten as

∀(s, a) ∈ Ψ

(cid:6), ∀t ∈ ρ(s, a)
(cid:6)(s, a, [t]
) =

BG|S

P

(cid:3)

P(s, a, s

(cid:6)(cid:6))

(11)

s(cid:6)(cid:6)∈(ρ(s,a)∩[s(cid:6)]

BG |S

)

It is evident that lines 11 to 17 of Fig. 1 implement Eqn. 9 and
Eqn. 11.

References
[Amarel, 1968] Saul Amarel. On representations of prob-
lems of reasoning about actions.
In Donald Michie, ed-
itor, Machine Intelligence 3, volume 3, pages 131–171.
Elsevier/North-Holland, Amsterdam, London, New York,
1968.

[Barto et al., 1995] A. G. Barto, S. J. Bradtke, and S. P.
Singh. Learning to act using real-time dynamic program-
ming. Artiﬁcial Intelligence, 72:81–138, 1995.

[Emerson and Sistla, 1996] F. Allen Emerson and A. Prasad
Sistla. Symmetry and model checking. Formal Methods in
System Design: An International Journal, 9(1/2):105–131,
August 1996.

[Givan et al., 2003] R. Givan, T. Dean, and M. Greig. Equiv-
alence notions and model minimization in markov deci-
sion processes. Artiﬁcial Intelligence, 147(1-2):163–223,
2003.

[Puget, 2005a] Jean-Francois Puget. Automatic detection of
In CP, pages 475–489,

variable and value symmetries.
2005.

[Puget, 2005b] Jean-Francois Puget. Breaking all value sym-
In CP, pages 490–504,

metries in surjection problems.
2005.

[Ravindran and Barto, 2002] Balaraman Ravindran and An-
drew G. Barto. Model minimization in hierarchical rein-
forcement learning. Lecture Notes on Computer Science,
2371:196–211, 2002.

[Ravindran and Barto, 2003] Balaraman Ravindran and An-
drew G. Barto. Smdp homomorphisms: An algebraic ap-
proach to abstraction in semi markov decision processes.
In Proceedings of IJCAI-03, pages 1011–1016. American
Association for Artiﬁcial Intelligence, August 2003.

[Zinkevich and Balch, 2001] M. Zinkevich and T. Balch.
Symmetry in markov decision processes and its implica-
tions for single agent and multiagent learning. In Proceed-
ings of the ICML-01, pages 632–640. Morgan Kaufmann,
2001.

∀(s, a) ∈ Ψ

(cid:6)]
(cid:6), ∀[s
BG|S
(cid:6)]
(cid:6)(s, a, [s

BG|S

P

(cid:3)

∈ BG|S − BG|S,
) =

s(cid:6)(cid:6)∈([s(cid:6)]

BG |S

∩ρ(s,a))

P(s, a, s

(cid:6)(cid:6))

(10)

As BG|S is a partition of S, ∀t ∈ ρ(s, a), there exists exactly
(cid:6)]
one [s

(cid:6)]
such that t ∈ [s

(cid:8)
(cid:9)
BG|S − BG|S

BG|S.

BG|S

∈

IJCAI-07

2561


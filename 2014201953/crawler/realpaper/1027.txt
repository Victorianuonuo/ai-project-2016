A Decision-Theoretic Model of Assistance

Alan Fern and Sriraam Natarajan and Kshitij Judah and Prasad Tadepalli

School of EECS, Oregon State University

Corvallis, OR 97331 USA

{afern,natarasr,judahk,tadepall}@eecs.oregonstate.edu

Abstract

There is a growing interest in intelligent assis-
tants for a variety of applications from organiz-
ing tasks for knowledge workers to helping peo-
ple with dementia. In this paper, we present and
evaluate a decision-theoretic framework that cap-
tures the general notion of assistance. The objec-
tive is to observe a goal-directed agent and to se-
lect assistive actions in order to minimize the over-
all cost. We model the problem as an assistant
POMDP where the hidden state corresponds to the
agent’s unobserved goals. This formulation allows
us to exploit domain models for both estimating
the agent’s goals and selecting assistive action. In
addition, the formulation naturally handles uncer-
tainty, varying action costs, and customization to
speciﬁc agents via learning. We argue that in many
domains myopic heuristics will be adequate for se-
lecting actions in the assistant POMDP and present
two such heuristics. We evaluate our approach in
two domains where human subjects perform tasks
in game-like computer environments. The results
show that the assistant substantially reduces user
effort with only a modest computational effort.

1 Introduction

The development of intelligent computer assistants has
tremendous impact potential in many domains. A variety of
AI techniques have been used for this purpose in domains
such as assistive technologies for the disabled [Boger et al.,
2005] and desktop work management [CALO, 2003]. How-
ever, most of this work has been ﬁne-tuned to the particular
application domains. In this paper, we describe and evaluate
a more comprehensive framework for intelligent assistants.

We consider a model where the assistant observes a goal-
oriented agent and must select assistive actions in order to
best help the agent achieve its goals. To perform well the as-
sistant must be able to accurately and quickly infer the goals
of the agent and reason about the utility of various assis-
tive actions toward achieving the goals. In real applications,
this requires that the assistant be able to handle uncertainty
about the environment and agent, to reason about varying ac-
tion costs, to handle unforeseen situations, and to adapt to
the agent over time. Here we consider a decision-theoretic
model, based on partially observable Markov decision pro-
cesses (POMDPs), which naturally handles these features,

providing a formal basis for designing intelligent assistants.

The ﬁrst contribution of this work is to formulate the prob-
lem of selecting assistive actions as an assistant POMDP,
which jointly models the application environment and the
agent’s hidden goals. A key feature of this approach is that
it explicitly reasons about the environment and agent, which
provides the potential ﬂexibility for assisting in ways unfore-
seen by the developer. However, solving for such policies is
typically intractable and we must rely on approximations. A
second contribution is to suggest an approximate solution ap-
proach that we argue is well suited to the assistant POMDP
in many application domains. The approach is based on ex-
plicit goal estimation and myopic heuristics for online action
selection. For goal estimation, we propose a model-based
bootstrapping mechanism that is important for the usability
of an assistant early in its lifetime. For action selection, we
propose two myopic heuristics, one based on solving a set of
derived assistant MDPs, and another based on the simulation
technique of policy rollout. A third contribution is to evalu-
ate our framework in two novel game-like computer environ-
ments using 12 human subjects. The results indicate that the
proposed assistant framework is able to signiﬁcantly decrease
user effort and that myopic heuristics perform as well as the
more computationally intensive method of sparse sampling.

The remainder of this paper is organized as follows.

In
the next section, we introduce our formal problem setup,
followed by a deﬁnition of the assistant POMDP. Next, we
present our approximate solution technique based on goal es-
timation and online action selection. Finally we give an em-
pirical evaluation of the approach in two domains and con-
clude with a discussion of related and future work.

2 Problem Setup

We refer to the entity that we are attempting to assist
as the agent. We model the agent’s environment as a
Markov decision process (MDP) described by the tuple
(cid:2)W, A, A(cid:2), T, C, I(cid:3), where W is a ﬁnite set of world states,
A is a ﬁnite set of agent actions, A(cid:2) is a ﬁnite set of assistant
actions, and T (w, a, w(cid:2)) is a transition distributions that rep-
resents the probability of transitioning to state w(cid:2) given that
action a ∈ A ∪ A(cid:2) is taken in state w. We will sometimes use
T (w, a) to denote a random variable distributed as T (w, a, ·).
We assume that the assistant action set always contains the
action noop which leaves the state unchanged. The compo-
nent C is an action-cost function that maps W × (A ∪ A(cid:2)) to
real-numbers, and I is an initial state distribution over W .

We consider an episodic setting where at the beginning of

IJCAI07

1879

each episode the agent begins in some state drawn from I
and selects a goal from a ﬁnite set of possible goals G. The
goal set, for example, might contain all possible dishes that
the agent might prepare. If left unassisted the agent will exe-
cute actions from A until it arrives at a goal state upon which
the episode ends. When the assistant is present it is able
to observe the changing state and the agent’s actions, but is
unable to observe the agent’s goal. At any point along the
agent’s state trajectory the assistant is allowed to execute a
sequence of one or more actions from A(cid:2) ending in noop, af-
ter which the agent may again perform an action. The episode
ends when either an agent or assistant action leads to a goal
state. The cost of an episode is equal to the sum of the costs
of the actions executed by the agent and assistant during the
episode. Note that the available actions for the agent and as-
sistant need not be the same and may have varying costs. Our
objective is to minimize the expected total cost of an episode.
Formally, we will model the agent as an unknown stochas-
tic policy π(a|w, g) that gives the probability of selecting ac-
tion a ∈ A given that the agent has goal g and is in state
w. The assistant is a history-dependent stochastic policy
π(cid:2)(a|w, t) that gives the probability of selecting action a ∈ A(cid:2)
given world state w and the state-action trajectory t observed
starting from the beginning of the trajectory. It is critical that
the assistant policy depend on t, since the prior states and
actions serve as a source of evidence about the agent’s goal,
which is critical to selecting good assistive actions. Given an
initial state w, an agent policy π, and assistant policy π(cid:2) we
let C(w, g, π, π(cid:2)) denote the expected cost of episodes that
begin at state w with goal g and evolve according to the fol-
lowing process: 1) execute assistant actions according to π(cid:2)
until noop is selected, 2) execute an agent action according
to π, 3) if g is achieved then terminate, else go to step 1.

In this work, we assume that we have at our disposal the
environment MDP and the set of possible goals G. Our ob-
jective is to select an assistant policy π(cid:2) that minimizes the
expected cost given by E[C(I, G0, π, π(cid:2))], where G0 is an
unknown distribution over agent goals and π is the unknown
agent policy. For simplicity we have assumed a fully observ-
able environment and episodic setting, however, these choices
are not fundamental to our framework.

3 The Assistant POMDP

POMDPs provide a decision-theoretic framework for deci-
sion making in partially observable stochastic environments.
A POMDP is deﬁned by a tuple (cid:2)S, A, T, C, I, O, μ(cid:3), where
S is a ﬁnite set of states, A is a ﬁnite set of actions, T (s, a, s(cid:2))
is a transition distribution, C is an action cost function, I is an
initial state distribution, O is a ﬁnite set of observations, and
μ(o|s) is a distribution over observations o ∈ O given the
current state s. A POMDP policy assigns a distribution over
actions given the sequence of preceding observations. It is of-
ten useful to view a POMDP as an MDP over an inﬁnite set of
belief states, where a belief state is simply a distribution over
S. In this case, a POMDP policy can be viewed as a map-
ping from belief states to actions. Actions can serve to both
decrease the uncertainty about the state via the observations
they produce and/or make direct progress toward goals.

We will use a POMDP model to address the two main chal-

lenges in selecting assistive actions. The ﬁrst challenge is to
infer the agent’s goals, which is critical to provide good as-
sistance. We will capture goal uncertainty by including the
agent’s goal as a hidden component of the POMDP state. In
effect, the belief state will correspond to a distribution over
possible agent goals. The second challenge is that even if we
know the agent’s goals, we must reason about the uncertain
environment, agent policy, and action costs in order to select
the best course of action. Our POMDP will capture this infor-
mation in the transition function and cost model, providing a
decision-theoretic basis for such reasoning.

Given an environment MDP (cid:2)W, A, A(cid:2), T, C, I(cid:3), a goal
distribution G0, and an agent policy π we now deﬁne the cor-
responding assistant POMDP.

The state space is W × G so that each state is a pair (w, g)
of a world state and agent goal. The initial state distribu-
tion I (cid:2) assigns the state (w, g) probability I(w)G0(g), which
models the process of selecting an initial state and goal for
the agent at the beginning of each episode. The action set
is equal to the assistant actions A(cid:2), reﬂecting that assistant
POMDP will be used to select actions.

The transition function T (cid:2) assigns zero probability to any
transition that changes the goal component of the state, i.e.,
the assistant cannot change the agent’s goal. Otherwise, for
any action a except for noop, the state transitions from (w, g)
to (w(cid:2), g) with probability T (w, a, w(cid:2)). For the noop action,
T (cid:2) simulates the effect of executing an agent action selected
according to π. That is, T (cid:2)((w, g), noop, (w(cid:2), g)) is equal to
the probability that T (w, π(w, g)) = w(cid:2).

The cost model C (cid:2) reﬂects the costs of agent and assis-
tant actions in the MDP. For all actions a except for noop we
have that C (cid:2)((w, g), a) = C(w, a). Otherwise we have that
C (cid:2)((w, g), noop) = E[C(w, a)], where a is a random vari-
able distributed according to π(·|w, g). That is, the cost of a
noop action is the expected cost of the ensuing agent action.
The observation distribution μ(cid:2) is deterministic and re-
ﬂects the fact that the assistant can only directly observe the
world state and actions of the agent. For the noop action in
state (w, g) leading to state (w(cid:2), g), the observation is (w(cid:2), a)
where a is the action executed by the agent immediately af-
ter the noop. For all other actions the observation is equal to
the W component of the state, i.e. μ(cid:2)(w(cid:2), g) = (w(cid:2), a). For
simplicity of notation, it is assumed that the W component of
the state encodes the preceding agent or assistant action a and
thus the observations reﬂect both states and actions.

We assume an episodic objective where episodes begin by
drawing initial POMDP states and end when arriving in a
state (w, g) where w satisﬁes goal g. A policy π(cid:2) for the assis-
tant POMDP maps state-action sequences to assistant actions.
The expected cost of a trajectory under π(cid:2) is equal to our ob-
jective function E[C(I, G0, π, π(cid:2))] from the previous section.
Thus, solving for the optimal assistant POMDP policy yields
an optimal assistant. However, in our problem setup the as-
sistant POMDP is not directly at our disposal as we are not
given π or G0. Rather we are only given the environment
MDP and the set of possible goals. As described in the next
section our approach will approximate the assistant POMDP
by estimating π and G0 based on observations and select as-
sistive actions based on this model.

IJCAI07

1880

4 Selecting Assistive Actions

Approximating the Assistant POMDP. One approach to ap-
proximating the assistant POMDP is to observe the agent act-
ing in the environment, possibly while being assisted, and to
learn the goal distribution G0 and policy π. This can be done
by storing the goal achieved at the end of each episode along
with the set of world state-action pairs observed for the agent
during the episode. The estimate of G0 can then be based on
observed frequency of each goal (perhaps with Laplace cor-
rection). Likewise, the estimate of π(a|w, g) is simply the
frequency for which action a was taken by the agent when
in state w and having goal g. While in the limit these esti-
mates will converge and yield the true assistant POMDP, in
practice convergence can be slow. This slow convergence can
lead to poor performance in the early stages of the assistant’s
lifetime. To alleviate this problem we propose an approach to
bootstrap the learning of π.

In particular, we assume that the agent is reasonably close
to being optimal. This is not unrealistic in many applica-
tion domains that might beneﬁt from intelligent assistants.
There are many tasks, that are conceptually simple for hu-
mans, yet they require substantial effort to complete. Given
this assumption, we initialize the estimate of the agent’s pol-
icy to a prior that is biased toward more optimal agent ac-
tions. To do this we will consider the environment MDP with
the assistant actions removed and solve for the Q-function
Q(a, w, g). The Q-function gives the expected cost of exe-
cuting agent action a in world state w and then acting opti-
mally to achieve goal g using only agent actions. We then
deﬁne the prior over agent actions via the Boltzmann dis-
tribution. In our experiments, we found that this prior pro-
vides a good initial proxy for the actual agent policy, al-
lowing for the assistant to be immediately useful. We up-
date this prior based on observations to better reﬂect the pe-
culiarities of a given agent. Computationally the main ob-
stacle to this approach is computing the Q-function, which
need only be done once for a given application domain.
A number of algorithms exist to accomplish this includ-
ing the use of factored MDP algorithms [Boutilier et al.,
1999], approximate solution methods [Boutilier et al., 1999;
Guestrin et al., 2003], or developing specialized solutions.

Action Selection Overview. Let Ot = o1, ..., ot be the ob-
servation sequence from the beginning of the current trajec-
tory until time t. Each observation provides the world state
and the previously selected action (by either the assistant or
agent). Given Ot and an assistant POMDP the goal is to se-
lect the best assistive action according to the policy π(cid:2)(Ot).
Unfortunately, exactly solving the assistant POMDP will
be intractable for all but the simplest of domains. This has led
us to take a heuristic action selection approach. To motivate
the approach, it is useful to consider some special characteris-
tics of the assistant POMDP. Most importantly, the belief state
corresponds to a distribution over the agent’s goal. Since the
agent is assumed to be goal directed, the observed agent ac-
tions provide substantial evidence about what the goal might
and might not be. In fact, even if the assistant does nothing
the agent’s goals will often be rapidly revealed. This sug-
gests that the state/goal estimation problem for the assistant

POMDP may be solved quite effectively by just observing
how the agent’s actions relate to the various possible goals.
This also suggests that in many domains there will be little
value in selecting assistive actions for the purpose of gath-
ering information about the agent’s goal. This suggests the
effectiveness of myopic action selection strategies that avoid
explicit reasoning about information gathering, which is one
of the key POMDP complexities compared to MDPs. We note
that in some cases, the assistant will have pure information-
gathering actions at its disposal, e.g. asking the agent a ques-
tion. While we do not consider such actions in our experi-
ments, as mentioned below, we believe that such actions can
be handled via shallow search in belief space in conjunction
with myopic heuristics. With the above motivation, our action
selection approach alternates between two operations.

Goal Estimation. Given an assistant POMDP with agent
policy π and initial goal distribution GO, our objective is
to maintain the posterior goal distribution P (g|Ot), which
gives the probability of the agent having goal g conditioned
on observation sequence Ot. Note that since the assistant
cannot affect the agent’s goal, only observations related to
the agent’s actions are relevant to the posterior. Given the
agent policy π, it is straightforward to incrementally update
the posterior P (g|Ot) upon each of the agent’s actions. At
the beginning of each episode we initialize the goal distribu-
tion P (g|O0) to G0. On timestep t of the episode, if ot does
not involve an agent action, then we leave the distribution un-
changed. Otherwise, if ot indicates that the agent selected
action a in state w, then we update the distribution according
to P (g|Ot) = (1/Z) · P (g|Ot−1) · π(a|w, g), where Z is a
normalizing constant. That is, the distribution is adjusted to
place more weight on goals that are more likely to cause the
agent to execute action a in w.

The accuracy of goal estimation relies on how well the pol-
icy π learned by the assistant reﬂects the true agent. As de-
scribed above, we use a model-based bootstrapping approach
for estimating π and update this estimate at the end of each
episode. Provided that the agent is close to optimal, as in our
experimental domains, this approach can lead to rapid goal
estimation, even early in the lifetime of the assistant.

We have assumed for simplicity that the actions of the
agent are directly observable. In some domains, it is more
natural to assume that only the state of the world is observ-
able, rather than the actual action identities. In these cases,
after observing the agent transitioning from w to w(cid:2) we can
use the MDP transition function T to marginalize over possi-
ble agent actions yielding the update,
(cid:2)

P (g|Ot) = (1/Z) · P (g|Ot−1) ·

π(a|w, g)T (w, a, w(cid:3)).

a∈A

Action Selection. Given the assistant POMDP M and a
distribution over goals P (g|Ot), we now address the problem
of selecting an assistive action. For this we introduce the idea
of an assistantMDPrelative to a goal g and M , which we will
denote by M (g). Each episode in M (g) evolves by drawing
an initial world state and then selecting assistant actions until
a noop, upon which the agent executes an action drawn from
its policy for achieving goal g. An optimal policy for M (g)
gives the optimal assistive action assuming that the agent is
acting to achieve goal g. We will denote the Q-function of

IJCAI07

1881

M (g) by Qg(w, a), which is the expected cost of executing
action a and then following the optimal policy.

Our ﬁrst myopic heuristic is simply the expected Q-value
of an action over assistant MDPs. The heuristic value for
assistant action a in state w given observations Ot is,

(cid:2)

H(w, a, Ot) =

Qg(w, a) · P (g|Ot)

g

and we select actions greedily according to H. Intuitively
H(w, a, Ot) measures the utility of taking an action under
the assumption that all goal ambiguity is resolved in one step.
Thus, this heuristic will not select actions for purposes of
information gathering. This heuristic will lead the assistant
to select actions that make progress toward goals with high
probability, while avoiding moving away from goals with
high probability. When the goal posterior is highly ambigu-
ous this will often lead the assistant to select noop.

The primary computational complexity of computing H is
to solve the assistant MDPs for each goal. Technically, since
the transition functions of the assistant MDPs depend on the
approximate agent policy π, we must re-solve each MDP af-
ter updating the π estimate at the end of each episode. How-
ever, using incremental dynamic programming methods such
as prioritized sweeping [Moore and Atkeson, 1993] can al-
leviate much of the computational cost. In particular, before
deploying the assistant we can solve each MDP ofﬂine based
on the default agent policy given by the Boltzmann bootstrap-
ping distribution described earlier. After deployment, prior-
itized sweeping can be used to incrementally update the Q-
values based on the learned reﬁnements we make to π.

When it is not practical to solve the assistant MDPs, we
may resort to various approximations. We consider two ap-
proximations in our experiments. One is to replace the user’s
policy to be used in computing the assistant MDP with a ﬁxed
default user’s policy, eliminating the need to compute the as-
sistant MDP at every step. We denote this approximation by
Hd. Another approximation uses the simulation technique
of policy rollout [Bertsekas and Tsitsiklis, 1996] to approx-
imate Qg(w, a) in the expression for H. This is done by
ﬁrst simulating the effect of taking action a in state w and
then using π to estimate the expected cost for the agent to
achieve g from the resulting state. That is, we approximate
Qg(w, a) by assuming that the assistant will only select a
single initial action followed by only agent actions. More
¯Cn(π, w, g) be a function that simulates n tra-
formally, let
jectories of π achieving the goal from state w and then av-
eraging the trajectory costs. The heuristic Hr is identical to
H(w, a, Ot) except that we replace Qg(w, a) with the expec-
w(cid:2)∈W T (w, a, w(cid:2)) · ¯C(π, w(cid:2), g). We can also com-
tation
bine both of these heuristics, which we denote by Hd,r. Fi-
nally, in cases where it is beneﬁcial to explicitly reason about
information gathering actions, one can combine these myopic
heuristics with shallow search in belief space of the assistant
MDP. One approach along these lines is to use sparse sam-
pling trees [Kearns et al., 1999] where myopic heuristics are
used to evaluate the leaf nodes.

(cid:3)

5 Experimental Results
We conducted user studies and simulations in two domains
and present the results in this section.

5.1 Doorman Domain

In the doorman domain, there is an agent and a set of possible
goals such as collect wood, food and gold. Some of the grid
cells are blocked. Each cell has four doors and the agent has
to open the door to move to the next cell (see Figure 1). The
door closes after one time-step so that at any time only one
door is open. The goal of the assistant is to help the user reach
his goal faster by opening the correct doors.

A state is a tuple (cid:2)s, d(cid:3), where s stands for the the agent’s
cell and d is the door that is open. The actions of the agent
are to open door and to move in each of the 4 directions or
to pickup whatever is in the cell, for a total of 9 actions. The
assistant can open the doors or perform a noop (5 actions).
Since the assistant is not allowed to push the agent through
the door, the agent’s and the assistant’s actions strictly alter-
nate in this domain. There is a cost of −1 if the user has to
open the door and no cost to the assistant’s action. The trial
ends when the agent picks up the desired object.

In this experiment, we evaluated the heuristics Hd and Hr.
In each trial, the system chooses a goal and one of the two
heuristics at random. The user is shown the goal and he tries
to achieve it, always starting from the center square. After
every user’s action, the assistant opens a door or does nothing.
The user may pass through the door or open a different door.
After the user achieves the goal, the trial ends, and a new one
begins. The assistant then uses the user’s trajectory to update
the agent’s policy.

The results of the user studies for the doorman domain are
presented in Figure 2. The ﬁrst two rows give cummulative
results for the user study when actions are selected according
to Hr and Hd respectively. The table presents the total op-
timal costs (number of actions) for all trials across all users
without the assistant N, and the costs with the assistant U,
and the average of percentage cost savings (1-(U/N)) over all
trials and over all the users1. As can be seen, Hr appears to
have a slight edge over Hd.

Figure 1: Doorman Domain.

To compare our results with a more sophisticated solution
technique, we selected actions using sparse sampling [Kearns
et al., 1999] for depths d =2 and 3 while using b=1 or 2 sam-
ples at every step. The leaves of the sparse sampling tree are

1This gives a pessimistic estimate of the usefulness of the assis-
tant assuming an optimal user and is a measure of utility normalized
by the optimal utility without the aid of the assistant.

IJCAI07

1882

Total

User

Heuristic

Actions Actions

Hr
Hd
Hr

d = 2, b = 1
d = 2, b = 2
d = 3, b = 1
d = 3, b = 2

N
750
882

1550
1337
1304
1167
1113

U
339
435

751
570
521
467
422

Average

1 − (U/N )

0.55± 0.055
0.51 ± 0.05
0.543 ± 0.17
0.588 ± 0.17
0.597 ± 0.17
0.6 ± 0.15
0.623 ± 0.15

Time
per

action
0.0562
0.0021

0.031
0.097
0.35
0.384
2.61

Figure 2: Results of experiments in the Doorman Domain.
The ﬁrst half of the table presents the results of the user stud-
ies while the lower half presents the results of the simulation.

evaluated using Hr. For these experiments, we did not con-
duct user studies, due to the high cost of such studies, but
simulated the human users by choosing actions according to
policies learned from their observed actions. The results are
presented in the bottom half of Figure 2. We see that sparse
sampling increased the average run time by an order of mag-
nitude, but is able to produce a reduction in average cost for
the user. This is not surprising, for in the simulated experi-
ments, sparse sampling is able to sample from the exact user
policy (i.e. it is sampling from the learned policy, which is
also being used for simulations).It remains to be seen whether
these beneﬁts can be realized in real experiments with only
approximate user policies.

5.2 Kitchen Domain

In the kitchen domain, the goals of the agent are to cook vari-
ous dishes. There are 2 shelves with 3 ingredients each. Each
dish has a recipe, represented as a partially ordered plan. The
ingredients can be fetched in any order, but should be mixed
before they are heated. The shelves have doors that must be
opened before fetching ingredients and only one door can be
open at a time.

Figure 3: The kitchen domain. The user is to prepare the
dishes described in the recipes on the right. The assistant’s
actions are shown in the bottom frame.

There are 8 different recipes. The state consists of the lo-
cation of each of the ingredient (bowl/shelf/table), the mixing
state and temperature state of the ingredient (if it is in the
bowl) and the door that is open. The state also includes the
action history to preserve the ordering of the plans for the
recipes. The user’s actions are: open the doors, fetch the in-
gredients, pour them into the bowl, mix, heat and bake the

contents of the bowl, or replace an ingredient back to the
shelf. The assistant can perform all user actions except for
pouring the ingredients or replacing an ingredient back to the
shelf. The cost of all non-pour actions is -1. Experiments
were conducted on 12 human subjects. Unlike in the doorman
domain, here it is not necessary for the assistant to wait at ev-
ery alternative time step. The assistant continues to act until
the noop becomes the best action according to the heuristic.
Since this domain has much bigger state-space than the ﬁrst
domain, both heuristics use the default user’s policy. In other
words, we compare Hd and Hd,r. The results of the user stud-
ies are shown in top half of the Figure 4. Hd,r performs better
than Hd. It was observed from the experiments that the Hd,r
technique was more aggressive in choosing non-noop actions
than the Hd, which would wait until the goal distribution is
highly skewed toward a particular goal. We are currently try-
ing to understand the reason for this behavior.

Total

User

Heuristic

Actions Actions

Hd,r
Hd
Hd,r

d = 2, b = 1
d = 2, b = 2
d = 3, b = 1
d = 3, b = 2

N

3188
3175

6498
6532
6477
6536
6585

U

1175
1458

2332
2427
2293
2458
1408

Average

1 − (U/N )

0.6361 ±0.15
0.5371± 0.10
0.6379 ± 0.14
0.6277 ± 0.14
0.646 ± 0.14
0.6263 ±0.15
0.6367 ± 0.14

Time
per

action
0.013
0.013

0.013
0.054
0.190
0.170
0.995

Figure 4: Results of experiments in the Kitchen Domain. The
ﬁrst half of the table presents the results of the user studies
while the lower half presents the results of the simulation.

We compared the use of sparse sampling and our heuristic
on simulated user trajectories for this domain as well(see bot-
tom half of Figure 4). There is no signiﬁcant difference be-
tween the solution quality of rollouts and sparse sampling on
simulations, showing that our myopic heuristics are perform-
ing as well as sparse sampling with much less computation.

6 Related Work

Much of the prior work on intelligent assistants did not take
a sequential decision making or decision-theoretic approach.
For example, email ﬁltering is typically posed as a supervised
learning problem [Cohen et al., 2004], while travel planning
combines information gathering with search and constraint
propagation [Ambite et al., 2002].

There have been other personal assistant systems that are
explicitly based on decision-theoretic principles. Most of
these systems have been formulated as POMDPs that are ap-
proximately solved ofﬂine. For instance, the COACH system
helped people suffering from Dementia by giving them ap-
propriate prompts as needed in their daily activities [Boger et
al., 2005]. They use a plan graph to keep track of the user’s
progress and then estimate the user’s responsiveness to deter-
mine the best prompting strategy. A distinct difference from
our approach is that there is only a single ﬁxed goal of wash-
ing hands, and the only hidden variable is the user respon-
siveness. Rather, in our formulation there are many possible
goals and the current goal is hidden to the assistant. We note,

IJCAI07

1883

that a combination of these two frameworks would be use-
ful, where the assistant infers both the agent goals and other
relevant properties of the user, such as responsiveness.

In Electric Elves, the assistant is used to reschedule a meet-
ing should it appear that the user is likely to miss it. Since
the system monitors users in short regular intervals, radical
changes in the belief states are usually not possible and are
pruned from the search space[Varakantham et al., 2005]. In a
distinct but related work [Doshi, 2004], the authors introduce
the setting of interactive POMDPs, where each agent mod-
els the other agent’s beliefs. Our model is simpler and as-
sumes that the agent is oblivious to the presence and beliefs
of the assistant. Relaxing this assumption without sacriﬁcing
tractability would be interesting.

Our work is also related to on-line plan recognition and
can be naturally extended to include hierarchies as in the hi-
erarchical versions of HMMs [Bui et al., 2002] and PCFGs
[Pynadath and Wellman, 2000]. Blaylock and Allen describe
a statistical approach to goal recognition that uses maximum
likelihood estimates of goal schemas and parameters [Blay-
lock and Allen, 2004]. These approaches do not have the
notion of cost or reward. By incorporating plan recognition
in the decision-theoretic context, we obtain a natural notion
of assistance, namely maximizing the expected utility.

There has been substantial research in the area of user
modelling. Horvitz et.al took a Bayesian approach to model
whether a user needs assistance based on user actions and
attributes and used it to provide assistance to user in a spread-
sheet application[Horvitz et al., 1998]. Hui and Boutilier
used a similar idea for assistance with text editing[Hui and
Boutilier, 2006]. They use DBNs with handcoded parameters
to infer the type of the user and computed the expected utility
of assisting the user. It would be interesting to explore these
kind of user models in our system to determine the user’s in-
tentions and then compute the optimal policy for the assistant.

7 Summary and Future Work

We described the assistant POMDP as a model for selecting
assistive actions. We also described an approximate solution
approach based on iteratively estimating the agent’s goal and
selecting actions using myopic heuristics. Our evaluation us-
ing human subjects in two game-like domains show that the
approach can signiﬁcantly help the user. One future direction
is to consider more complex domains where the assistant is
able to do a series of activities in parallel with the agent. An-
other possible direction is to assume hierarchical goal struc-
ture for the user and do goal estimation in that context. Our
framework can be naturally extended to the case where the
environment is partially observable to the agent and/or the
assistant. This requires recognizing actions taken to gather
information, e.g., opening the fridge to decide what to make
based on what is available. Another important direction is to
extend this work to domains where the agent MDP is hard to
solve. Here we can leverage the earlier work on learning ap-
prentice systems and learning by observation [Mitchell et al.,
1994]. The user’s actions provide training examples to the
system which can be used for learning.

Acknowledgements
This material is based upon work supported by the Defense
Advanced Research Projects Agency (DARPA), through the
Department of the Interior, NBC, Acquisition Services Di-
vision, under Contract No. NBCHD030010. Any opinions,
ﬁndings, and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily
reﬂect the views of DARPA.

References
[Ambite et al., 2002] J. L. Ambite, G. Barish, C. A. Knoblock,
M. Muslea, J. Oh, and S. Minton. Getting from here to there:
Interactive planning and agent execution for optimizing travel. In
IAAI, pages 862–869, 2002.

[Bertsekas and Tsitsiklis, 1996] D. P. Bertsekas and J. N. Tsitsiklis.

Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.

[Blaylock and Allen, 2004] N. Blaylock and J. F. Allen. Statistical

goal parameter recognition. In ICAPS, 2004.

[Boger et al., 2005] J. Boger, P. Poupart, J. Hoey, C. Boutilier,
G. Fernie, and A. Mihailidis. A decision-theoretic approach to
task assistance for persons with dementia. In IJCAI, 2005.

[Boutilier et al., 1999] C. Boutilier, T. Dean, and S. Hanks.
Decision-theoretic planning: Structural assumptions and compu-
tational leverage. JAIR, 11:1–94, 1999.

[Bui et al., 2002] H. Bui, S. Venkatesh, and G. West. Policy recog-

nition in the abstract hidden markov models. JAIR, 17, 2002.

[CALO, 2003] CALO. Cognitive agent that learns and organizes,

http://calo.sri.com., 2003.

[Cohen et al., 2004] W. W. Cohen, V. R. Carvalho, and T. M.
In Pro-

Mitchell. Learning to classify email into speech acts.
ceedings of Empirical Methods in NLP, 2004.

[Doshi, 2004] P. Doshi. A particle ﬁltering algorithm for interactive

pomdps, 2004.

[Guestrin et al., 2003] C. Guestrin, D. Koller, R. Parr,

and
S. Venkataraman. Efﬁcient solution algorithms for factored
MDPs. JAIR, pages 399–468, 2003.

[Horvitz et al., 1998] E. Horvitz,

J. Breese, D. Heckerman,
D. Hovel, and K. Rommelse. The lumiere project: Bayesian user
modeling for inferring the goals and needs of software users. In
In Proc UAI, pages 256–265, Madison, WI, July 1998.

[Hui and Boutilier, 2006] B. Hui and C. Boutilier. Who’s asking for
help?: a bayesian approach to intelligent assistance. In IUI, pages
186–193, 2006.

[Kearns et al., 1999] M. J. Kearns, Y. Mansour, and A. Y. Ng. A
sparse sampling algorithm for near-optimal planning in large
markov decision processes. In IJCAI, 1999.

[Mitchell et al., 1994] T. M. Mitchell, R. Caruana, D. Freitag,
J.McDermott, and D. Zabowski. Experience with a learning per-
sonal assistant. Communications of the ACM, 37(7):80–91, 1994.
[Moore and Atkeson, 1993] A. W. Moore and C. G. Atkeson. Prior-
itized sweeping: Reinforcement learning with less data and less
time. Machine Learning, 13:103–130, 1993.

[Pynadath and Wellman, 2000] D. V. Pynadath and M. P. Wellman.
Probabilistic state-dependent grammars for plan recognition. In
UAI, pages 507–514, 2000.

[Varakantham et al., 2005] P. Varakantham, R. T. Maheswaran, and
M. Tambe. Exploiting belief bounds: practical pomdps for per-
sonal assistant agents. In AAMAS, 2005.

IJCAI07

1884


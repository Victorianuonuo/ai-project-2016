Parametric Kernels for Sequence Data Analysis

Young-In Shin and Donald Fussell

The University of Texas at Austin
Department of Computer Sciences

{codeguru,fussell}@cs.utexas.edu

Abstract

A key challenge in applying kernel-based methods
for discriminative learning is to identify a suitable
kernel given a problem domain. Many methods in-
stead transform the input data into a set of vectors
in a feature space and classify the transformed data
using a generic kernel. However, ﬁnding an effec-
tive transformation scheme for sequence (e.g. time
series) data is a difﬁcult task. In this paper, we in-
troduce a scheme for directly designing kernels for
the classiﬁcation of sequence data such as that in
handwritten character recognition and object recog-
nition from sensor readings. Ordering information
is represented by values of a parameter associated
with each input data element. A similarity met-
ric based on the parametric distance between corre-
sponding elements is combined with their problem-
speciﬁc similarity metric to produce a Mercer ker-
nel suitable for use in methods such as support
vector machine (SVM). This scheme directly em-
beds extraction of features from sequences of vary-
ing cardinalities into the kernel without needing
to transform all input data into a common feature
space before classiﬁcation. We apply our method to
object and handwritten character recognition tasks
and compare against current approaches. The re-
sults show that we can obtain at least comparable
accuracy to state of the art problem-speciﬁc meth-
ods using a systematic approach to kernel design.
Our contribution is the introduction of a general
technique for designing SVM kernels tailored for
the classiﬁcation of sequence data.

1 Introduction
A common technique in mechanical classiﬁcation and regres-
sion is to deﬁne a feature space for the domain of interest,
transform the input data into vectors in that space, and then
apply an appropriate classiﬁcation or regression technique in
the feature space. However, some data are more naturally rep-
resented in a much more irregular form than feature vectors.
In this paper, we propose a new approach based on direct
matching using parametric kernels, where inputs are vari-
able length sequences of elements. Each element is associ-

ated with a point in a parameter space, and similarity metrics
are deﬁned independently for the elements and their parame-
ters. The parameter space is decomposed into possibly over-
lapping ranges, and the overall similarity of the sequences
is computed by taking the summation kernel over the ele-
ments and their associated parameters for each range. The
intuition behind the use of parameters is to encode informa-
tion about the ordering of input data in the kernel in a ﬂexi-
ble way. Such information is frequently lost in feature-space
based approaches. Since we can choose to apply a wide range
of order-preserving transformations on the distances between
elements in our parameterizations, we obtain additional con-
trol over the similarity metrics we can deﬁne as well. Note
that our use of the term “parametric” here does not imply that
we assume any sort of a priori parametric model of the data,
as for example in the case of generative approaches [Jaakkola
and Haussler, 1998]. Note also that sequences of any ﬁxed
dimensional feature vectors can be used instead of input data
elements in our method as needed since all we assume about
elements is that they are ﬁxed-dimensional.

Consider an input x represented as a sequence of n ele-
ments xi ∈ X, i.e. x = [x1, · · · , xn]. We could choose to
associate each element xi with parameter τ (xi) in parameter
space T as follows

(cid:2) (cid:3)i

0

if i > 1,
otherwise.

(1)

τ (xi) =

k=2 (cid:3)xk − xk−1(cid:3)

The associated parameters form a non-decreasing sequence
of non-negative real numbers, starting from zero. For any two
such sequences, a parametric kernel at each iteration picks
one element from each of the sequences and computes the
similarities between the elements and their associated param-
eters separately. These are multiplied to return an overall sim-
ilarity between the elements. This product of the similarities
of the elements and their parameters implies that two similar
elements may contribute signiﬁcantly to the overall sequence
similarity only when their associated parameters are similar
as well. This step is repeated for all pairs of elements from
the two sequences, and the results are summed to return the
overall sequence similarity.

Na¨ıve application of this approach has the potential to be
swamped by the computational expense of performing many
comparisons between elements with widely divergent param-

IJCAI-07

1047

x
2

x
3

z

3

z

4

z

5
z

6

z

2

z

1

x
1

x
4
x = [x1, · · · , x4]

z
z = [z1, · · · , z7]

7

x
1
z

1

0

x
2
z

3

z

4

x
3
z

5

6

z

x
4
z

7

z

2

Δ

2Δ

3Δ

T

Figure 1: Mapping Sequence to Parameter Space

eters which contribute little or nothing to the ﬁnal result. In-
tuitively, we could handle this by limiting the comparisons
only to subsets of elements that are close in parameter space.
This closeness in parameter space is easily speciﬁed by the
decomposition of parameter space into ranges, so that close
elements are deﬁned to be those whose parameters fall into
the same range. For instance, we could decompose T into
non-overlapping intervals of equal length Δ, and elements
from the two sequences are close if their associated parame-
ters fall into the same interval. See Figure 1, where elements
in x and z are grouped into three ranges based on the afore-
mentioned decomposition scheme. x1, for instance, will be
compared against only z1 and z2, both in X and T . Any
sequence data types fall into this category, e.g. handwritten
characters, laser sensor readings, digital signals, and so on.

Ideally, we wish to ﬁnd a method that meets all of the fol-
lowing requirements : a) computationally efﬁcient to handle
large inputs, b) no need to assume any probabilistic models, c)
kernels are positive semi-deﬁnite, d) no requirement of ﬁxed
form inputs, and e) the ability to ﬂexibly embed structural in-
formation from the input. While previous approaches fail to
satisfy some or all of these, all of the requirements are sat-
isﬁed by our approach. With an appropriate decomposition
scheme, parametric kernels may be computed in time linear to
the number of elements in inputs. Parametric kernels are posi-
tive semi-deﬁnite, thus they are admissible by kernel methods
such as SVMs that require Mercer kernels for optimal solu-
tion.

We have applied our method to handwritten character
recognition and object recognition from sensor readings. Our
results compare favorably to the best previously reported
schemes using only very simple similarity metrics.

2 Related Work

A great deal of work has been done on the application of
kernel-based methods to data represented as vectors in a fea-
ture space [Shawe-Taylor and Christianini, 2004; Sch¨olkopf
and Smola, 2002]. Discriminative models can ﬁnd complex
and ﬂexible decision boundaries and their performance is of-
ten superior to that of alternative methods, but they often rely
on a heuristic preprocessing step in which the input data is

transformed into a set of uniform size feature vectors. The
similarity of two input data is then assessed by evaluating
generic kernel functions on the feature vectors. To handle
sequence data in this traditional kernel framework, features
based on distance metrics are often extracted, e.g. the mean
coordinates, second order statistics such as median, variance,
minimum and maximum distances, area, etc. However, such
features do not generalize well because they often impose
restrictions that input patterns must be of equal lengths or
sampled at equal rates. Alternatively, histograms can be con-
structed from locations, speed, size, aspect ratio, etc. His-
tograms are an effective scheme to map varying length se-
quences into uniform dimensional feature vectors [Porikli,
2004; Grauman and Darrell, 2005]. Unfortunately, structural
information between elements in input patterns is inevitably
lost in histogramming.

Methods based on direct matching do not suffer from such
drawbacks as they retain the structural information. They
map input sequences into sequences of equal dimensional lo-
cal features, which are further compared using well known
direct sequence matching techniques. For instance, a number
of methods based on Dynamic Time Warping (DTW) have
recently been proposed for classifying sequence data such
as speech or handwritten characters [Bahlmann et al., 2002;
Shimodaira et al., 2002].
Indeed, an extensive survey has
shown that DTW methods are among the most effective tech-
niques for classifying sequence data [Lei and Govindaraju,
2005; Keogh and Kasetty, 2002]. In [Bahlmann et al., 2002],
normalized coordinates and the tangent slope angle is com-
puted at each of the points in a stroke sequence to form a
feature vector. DTW computes the optimal distance between
two sequences along a Viterbi path, which is then used as
the exponent of a radial basis function (RBF). In [Tapia and
Rojas, 2003], a ﬁxed size feature vector is computed from
strokes and an SVM classiﬁer is used. They achieved a high
accuracy over 98% but since they allowed only ﬁxed feature
vectors for each stroke, the method’s application is quite lim-
ited. In [Lei and Govindaraju, 2005], Extended R-squared
(ER2
) is proposed as the similarity measure for sequences.
It uses coordinates of points directly as features, but can only
operate on ﬁxed-length windows of such points.

DTW methods deﬁne kernels which can be shown to be
symmetric and satisfy the Cauchy-Schwartz inequality [Shi-
modaira et al., 2002]. SVM classiﬁers may employ such ker-
nels to classify sequential data. However, kernels based on
DTW are not metrics. The triangle inequality is violated in
many cases and the resulting kernel matrices are not positive
semi-deﬁnite. Therefore, they are not admissible for kernel
methods such as SVM in that they cannot guarantee the exis-
tence of a corresponding feature space and any notion of opti-
mality with respect to such a space. In contrast, our paramet-
ric kernels are positive and semi-deﬁnite and are thus well-
suited for use in kernel-based classiﬁers.

3 Approach

Kernel-based discriminative learning algorithms can ﬁnd
complex non-linear decision boundaries by mapping input
examples into a feature space F with an inner product that can

IJCAI-07

1048

be evaluated by a kernel function κ : X × X → R on exam-
ples in the input space X. A linear decision function in F is
found, which corresponds to a non-linear function in X. This
“kernel trick” lets us ﬁnd the decision boundary without ex-
plicit evaluation of the feature mapping function φ : X → F
and taking the inner product, which is computationally very
expensive and may even be intractable. For this to guarantee
a unique optimal solution, the kernel functions must satisfy
Mercer’s condition, i.e. the Gram matrix obtained from the
kernel functions must be positive and semi-deﬁnite.

3.1 Parameters
The underlying intuition in our work is to associate a param-
eter with each of the elements so that enforcing parametric
similarity is equivalent to the similarity in the structure of el-
ements in the input patterns. Our work assumes that input
patterns are varying length sequences of elements from some
common input or feature space. For example, handwritten
characters are sequences of 2D points, while images are 2D
grids of ﬁxed dimensional color values. The structure of a
sequence is a 1D manifold of 2D vectors and that of an image
is a 2D manifold of 3D vectors, assuming colors are repre-
sented as RGB for instance. A parameter of an element is
then a point in this manifold that corresponds to the element.
Therefore, if two parameters of any two elements are close,
then they are structurally close. This parametric association
must be deﬁned as part of making the choice of kernel func-
tions.

3.2 Parametric Kernel
Our input pattern x is a sequence of |x| elements, where each
element xi is a d-dimensional vector, i.e. x = [x1, · · · , x|x|]
and xi ∈ Rd
. The number of elements may vary across se-
quences. We associate each element with a parameter in pa-
rameter space T via a function τ : Rd → T . Consider a
decomposition of T into N non-overlapping ranges

N −1(cid:4)

t=0

Tt.

T =

(2)

For instance, recall the earlier sequence example, where T
was the set of non-negative real numbers and τ is deﬁned as
(1). T was decomposed as shown in Figure 2.

T0

T

1

T

2

T

3

T4

0

Δ

2Δ

3Δ

4Δ

5Δ

...

Figure 2:
overlapping ranges of length Δ.

Parameter space is decomposed into non-

For the derivation of parameter kernel functions, we
ﬁrst deﬁne decomposed element set for Tt as It(x) =
{xi|τ (xi) ∈ Tt}, which is the set of elements of x whose
associated parameters are in Tt.
In our previous example
shown in Figure 1, for instance, I1(x) = {x2, x3} and
I1(z) = {z3, z4, z5, z6}. We then compute a similarity for
each range by taking a weighted sum of the similarities of

every pair of elements of the sets being compared whose pa-
rameters fall within the range. For T1, we will compare x2
with z3, x2 with z4, · · · , and x3 with z6. The similarity for
a given pair of elements is obtained by taking the product of
the similarity κτ : T × T → R between those elements’
parameters and a similarity κx : Rd × Rd → R deﬁned di-
rectly on the elements themselves, each of which is a Mercer
kernel function. Then, the feature extraction function φ of a
parametric kernel is deﬁned as

φ(x) = [φ0(x), φ1(x), · · · φN −1(x)],

(3)

where

φt(x) =

(cid:5)

xi∈It(x)

wxi

κx(xi, ·)κτ (τ (xi), τ (·)),

(4)

and wxi is a non-negative weighting factor. Given two se-
quences x = [x1, · · · , x|x|] and z = [z1, · · · , z|z|], the para-
metric kernel function before normalization is then deﬁned as
the sum of similarity for all ranges :

κ(x, z) = (cid:5)φ(x) · φ(z)(cid:6) =

N −1(cid:5)

(cid:5)φt(x) · φt(z)(cid:6),

t=0

where

=

(cid:5)

(cid:5)φt(x) · φt(z)(cid:6)
wzj

wxi

κx(xi, zj)κτ (τ (xi), τ (zj)).

(5)

(6)

(7)

xi∈It(x)
zj ∈It(z)

Note that since the product of κx and κτ is taken in (4),
both must score high to have signiﬁcance in (5). Also note
that no comparison is made between elements that are not
from a common range. If, instead, we have to compare all
elements from one input with all from the other input, we will
face a number of undesirable consequences. For instance,
in Figure 1, we will compare x1 with {z1, · · · , z7}, rather
than {z1, z2}. This will result in a higher similarity value,
which may be helpful for certain cases. But, at the same time,
we are more likely to be confused by inputs where there are
too many far elements, in which case we are swamped by
bad comparisons. Of course, we may need dramatically more
time to compute (5) as the number of kernel evaluations is
signiﬁcantly increased.

Parameter space decomposition solves such problems.
However, such a decomposition scheme can introduce quanti-
zation errors. To overcome this problem, we allow the ranges
to overlap. For instance, ranges in Figure 2 may overlap by
Δ/2, as shown in Figure 3. To suppress over-contribution of
elements that fall into the intersections of ranges, we intro-
duce weighting factors in (4).

The simplest weighting scheme is to take the average of the
similarity at the overlapped regions. In this scheme, the de-
fault value for wxi is 1/|Txi
= {Tt|τ (xi) ∈ Tt}.
| = 1 and there-
When no ranges overlap, we have |Txi
fore, wxi
= 1. Otherwise, overlapped ranges may yield

|, where Txi

IJCAI-07

1049

0T

Δ/2

T1
Δ

T2

3Δ/2

3T
2Δ

T4

...

5Δ/2

0

Figure 3: Ranges overlap by Δ/2.

< 1. For instance, with the decomposition scheme in
wxi
Figure 3, at intersection [Δ/2, Δ), we will set wxi
= 1/2,
| = 2. Note that this scheme will not result in
since |Txi
→ ∞, since (4) will be evaluated only
|Txi
when It(x) (cid:8)= ∅. If It(x) = ∅, then φt(x) ≡ 0 and the term
is just ignored. Further discussion of different decomposition
schemes is given in 3.3.

| = 0, i.e. wxi

Finally, to avoid favoring large inputs, we normalize (5) by

dividing it with the product of their norms,

κ(x, z) =

(cid:6)

κ(x, z)

κ(x, x)κ(z, z)

.

(8)

This is equivalent to computing the cosine of the angle be-

tween two feature vectors in a vector space model.

3.3 Parameter Space Decomposition Scheme
In this section, our decomposition scheme is discussed in gen-
eral in terms of pros and cons with respect to additional cost
of computation and changes in classiﬁcation performance due
to range overlapping and similarity weighting. Then, a num-
ber of different decomposition schemes are presented.

As mentioned above, decomposition lets us avoid swamp-
ing by bad comparisons and dramatically reduces the compu-
tational cost of kernel evaluation but introduces quantization
error. This is alleviated by allowing for range overlapping
and similarity weighting. However, overlapping must be al-
lowed with care. Increasing the size of range overlaps will re-
quire additional computation since it is likely to involve more
kernel evaluations as more elements are found in each inter-
section. The gain of decreasing quantization error may pro-
vide little improvement in classiﬁcation performance if we
are swamped by bad comparisons. Thus there is a tradeoff
between quantization error and classiﬁcation performance.

One issue left is the time to compute the decomposed ele-
ment sets. The more complicated a decomposition scheme
gets,
the more difﬁcult the implementation becomes and
the more time it takes to run. Fortunately, our experimen-
tation has revealed that classiﬁcation performance is rel-
atively insensitive to minor changes in the decomposition
scheme. Therefore, we can often favor simpler decomposi-
tion schemes for ease of implementation and efﬁcient kernel
evaluation with the expectation of only minimal losses in per-
formance.

We now present a number of example parameter space de-

composition schemes.

Regular Decomposition Parameter ranges all have a com-
mon length Δ as in Figure 2 or 3. Implementation is simple,
and it shows good classiﬁcation performance in general. But
we have limited freedom to ﬁt the data.

Irregular Decomposition Parameter ranges are of varying
lengths.
Implementation is complicated and often decom-
posed element sets take longer to compute. But we can freely
decompose the parameter space to better ﬁt the data.

Multi-scale Decomposition Parameter ranges form a hier-
archical structure at different resolutions. For instance, we
may consider a decomposition where ranges form a pyramid
as shown in Figure 4. Elements from non-adjacent ranges
could be compared at coarser resolutions. Along with a
proper weighting scheme, this may improve the performance.
But implementation is more complex and kernel evaluation
may take longer.

T

6

T

4

Δ

T

0

0

T

5

3Δ

T

1

T

2

2Δ

...
...

...

T

3

T

7

4Δ

5Δ

Figure 4: Pyramidal Parameter Space Decomposition

3.4 Mercer Condition

According to Mercer’s theorem, a kernel function corre-
sponds to an inner product in some feature space if and only
if it is positive and semi-deﬁnite. A unique optimal solution
is guaranteed by kernel methods only when kernels are posi-
tive and semi-deﬁnite. To see that our proposed method pro-
duces positive and semi-deﬁnite kernels, we ﬁrst note that (4)
is positive and semi-deﬁnite. In [Haussler, 1999], such ker-
nels are called summation kernels and proven to be positive
and semi-deﬁnite. It is not difﬁcult to see that (5) is just a sum
of summation kernels. Since we can synthesize a new Mercer
kernel by adding Mercer kernels, (5) is a Mercer kernel.

3.5 Efﬁciency

The time complexity to compute parametric kernels depends
greatly on the particular decomposition scheme used. Here
we provide a brief analysis only for regular decomposition
schemes.

Assume that constant time is needed to evaluate κx and
κτ and that we are using a regular decomposition scheme
as in Figure 2 or 3. Then the time complexity of evaluat-
ing (5) for sequences x and z composed of |x| and |z| el-
ements, respectively, is, on average, O(|x||z|Δ/L), where
L = max(τ (x|x|), τ (z|z|)). In the worst case without de-
composition, Δ = L, so the complexity is O(|x||z|). In gen-
eral, we expect decomposition to produce Δ (cid:10) L since we
would like to have only a reasonably small subset of elements
in each range.

The storage complexity is O(1) since we only keep the sum
of kernel evaluations in memory. The time complexity to de-
compose the sequences into their respective element sets is
O(|x| + |z|), if constant time is taken for each element.

IJCAI-07

1050

4 Results

For our handwritten character and object recognition experi-
ments, an implementation based on Matlab SVM Toolbox in
[Gunn, 1997] was used. For handwritten character recogni-
tion, we limited the scope of our testing to the recognition of
isolated characters. We built our own training and test sets,
similar to those in UNIPEN data1.

4.1 Handwritten Character Recognition

Handwritten characters are represented as sequences of 2D
(x, y) coordinates of pixels (points) on the screen. Our ob-
jective is to learn from a training set a function that correctly
classiﬁes an unseen handwritten character. We normalize by
scaling inputs to ﬁt a 300×300 pixel bounding box and align-
ing them at the center. The input data points in each sequence
are chosen as the elements. Our training set is composed of
200 labeled examples created by two writers, each of whom
wrote numeric characters from ’0’ to ’9’ ten times. Our test
data is composed of 500 labeled examples created by other
authors, 50 for each character. Figure 5 shows some training
examples for characters ’1’ and ’5’, with the number of points
in each of them shown below.

400

350

300

250

200

150

400

350

300

250

200

150

400

380

360

340

320

300

280

260

240

220

300

280

260

240

220

200

180

160

140

120

100

100

150

200

250

300

350

400

100
−100

−50

0

50

100

150

200

200

150

200

250

300

350

100

150

200

250

300

350

19 points

9 points

14 points

12 points

360

340

320

300

280

260

240

220

200

180

160

140

150

200

250

300

350

440

420

400

380

360

340

320

300

280

260

240

220

150

360

340

320

300

280

260

240

220

200

180

200

250

300

350

200

250

300

350

400

350

300

250

200

150

50

100

150

200

250

300

14 points

15 points

17 points

21 points

Class # SVs
’1’
’2’
’3’
’4’
’5’

17
18
15
19
17

Error
0.98 %
1.12 %
0.10 %
0.89 %
1.01 %

Class # SVs
’6’
’7’
’8’
’9’
’0’

18
17
19
14
18

Error
1.15 %
0.41 %
0.23 %
0.09 %
0.01 %

Figure 6: Handwritten Number Recognition

2002; Bahlmann et al., 2002]. However, unlike those ap-
proaches, we achieved this without any sophisticated feature
extraction or restrictions on the input structures.

4.2 Object Recognition from Sensor Data

We also analyzed sensor data captured at 0.1 second intervals
by a Hokuyo URG-04LX laser range ﬁnder2 mounted on the
front side of a Segway RMP3. The Segway RMP navigates
in its environment under the control of an attached tablet PC
communicating with it via USB commands. Our tablet PC
program reads the sensor data and detects nearby objects and
obstacles during the navigation. The speciﬁc objective here
is to locate a subregion in the sensor data that corresponds to
a soccer ball. Each frame of input data is represented as an
array of 768 regular directional samples in a range of −120◦
to 120◦
. Each of these is the distance to the nearest object
in millimeters, ranging from 20 to 4095 with maximum 1%
error. See Figure 7 (a) for a snapshot.

120

90

  1

60

  0.5

150

180

210

30

0

330

150

180

210

120

90

  1

60

  0.5

30

0

330

240

270

300

240

270

300

120

150

90

  1

  0.5

Figure 5: Sample Raw Inputs for Handwritten Numbers

(a) Raw Input

(b) Segmented

(c) Detected

We trained one-versus-all multi-class support vector nov-
elty detectors (SVNDs) with the parametric kernel. See
[Shawe-Taylor and Christianini, 2004; Sch¨olkopf and Smola,
2002] for an introduction to SVND. Parameter space was
regularly decomposed with overlapping allowed as shown in
Figure 3. Window and hop sizes were set to 60 and 30, re-
spectively. We chose RBF for κx and κτ with σ = 30 and
ν = 0.8, where ν is the lower bound on the ratio of support
vectors and at the same time the upper bound on the ratio of
outliers. SNVD predicts how novel a test sample is in terms
of novelty. Each of the test examples is classiﬁed as the class
that has the minimum novelty value, and the result is com-
pared against its label. The result is shown in Figure 6. The
classiﬁcation error was measured as the percentage of mis-
classiﬁed examples in the test set. We obtained an error rate
less than 1% in most cases.

This represents a classiﬁcation accuracy that is compara-
ble to the recognizer in [Tapia and Rojas, 2003] and superior
to those in [Lei and Govindaraju, 2005; Keogh and Kasetty,

1http://unipen.nici.kun.nl/

Figure 7: Raw sensor input is shown in (a) and thick curves
in (b) are the blobs after segmentation. In (c), the thick round
blob at angle about 90◦
and distance 0.2 is detected as the
soccer ball.

After normalizing the sensor data by dividing it by the
maximum distance, it is segmented into several subregions
called blobs. A blob is a subregion of a frame where all dis-
tance values are in (θ, 1) and every consecutive values are
at most δ apart. In this experiment, we used θ = 0.05 and
δ = 0.02. Blobs are further normalized by scaling and trans-
lating so that the min and max distance values in each blob
are 0 and 1.

Blobs are sequences of scalar values. Ball blobs resemble
semi-circles distorted by some small sensor noise because the
laser range ﬁnder scans from only one side of the ball. How-
ever, since this is also the case for all round objects such as
human legs or beacons, they will confuse our classiﬁer. For

2http://www.hokuyo-aut.jp/
3http://www.segway.com/products/rmp/

IJCAI-07

1051

demonstration purposes, it is assumed that no such confus-
ing objects exist in the scene. Some of the ball and non-ball
examples are shown in Figure 8.

Our data is composed of examples for 442 ball blobs and
2199 non-ball blobs. Ball and non-ball blobs are labeled as
+1 and -1, respectively. We trained a soft-margin support
vector classiﬁer (SVC) with a quadratic loss function and the
parametric kernel. See [Shawe-Taylor and Christianini, 2004;
Sch¨olkopf and Smola, 2002] for an introduction to SVCs. We
used a similar parameter space decomposition scheme as in
the handwritten character recognition task. Window and hop
sizes were set to 0.1 and 0.05, respectively. We chose RBF
for κx and κτ with σ = 0.1 and C = 1000, where C is
a positive number that deﬁnes the relative trade-off between
norm and loss in regularized risk minimization. We measured
the classiﬁcation error from cross-validation. We randomly
sampled 20 positive and 22 negative examples from our data
for training, and the classiﬁcation error was measured by the
percentage of examples misclassiﬁed by the trained classiﬁer
among the rest of the data. On average, we obtained error
rates of 0.71% and 0.92% for positive and negative test data,
respectively, and 78.6% of the training data were support vec-
tors. The error rate for non-ball blobs was a bit higher because
of over-ﬁtting when we treated all non-ball blobs as negative
examples. Each scene on average contained about 10 to 20
blobs, among which only one corresponds to a soccer ball.
Our C++ implementation running on a tablet PC with Intel
Pentium M 1.2 GHz processor on average required less than
0.1 seconds to classify blobs in each scene.

1

0.5

0

0

20

40

60

80

100

Figure 8: Solid (dashed) curves are ball (non ball) examples,
where vertical and horizontal axis correspond to the normal-
ized distance and the number of points in blobs. Clearly, ball
blobs are semi-circular, while others are irregular.

5 Conclusions and Future Work

Our parametric kernel provides an efﬁcient and very ﬂexible
framework for extending discriminative learning to sequen-
tial input data. Our approach allows for direct use of input
data without computing sophisticated feature vectors, which
makes the problem simpler to solve and easier and more ef-
ﬁcient to implement. Thus we believe that this scheme of-
ten provides a more systematic, ﬂexible, and intuitive way
to build effective kernel functions for discriminative classi-
ﬁers than previous methods while providing similar or supe-
rior performance.

While we have designed our techniques to apply to sequen-
tial input data, we believe that the advantages of a systematic
approach that is free of the need for uniform input data can
be applied to other types of data with natural input orderings
such as grids, graphs, sets, etc. We believe that this is due
to the intuitive binding of an application-dependent feature

comparison scheme with kernel functions based on the natu-
ral ordering of the input data.

References
[Bahlmann et al., 2002] C. Bahlmann, B. Haasdonk, and
H. Burkhardt. On-line handwriting recognition with sup-
port vector machines - a kernel approach. In Proceedings
of the 8th International Workshop on Frontiers in Hand-
writing Recognition, pages 49–54, 2002.

[Grauman and Darrell, 2005] K. Grauman and T. Darrell.
The pyramid match kernel : Discriminative classiﬁcation
with sets of image features. In IEEE International Confer-
ence on Computer Vision, October 2005.

[Gunn, 1997] S. R. Gunn. Support vector machines for clas-
siﬁcation and regression. In Technical Report, University
of Southampton, 1997.

[Haussler, 1999] D. Haussler. Convolution kernels on dis-
crete structures. In UC Santa Cruz Technical Report UCS-
CRL-99-10, 1999.

[Jaakkola and Haussler, 1998] T. S. Jaakkola and David
Haussler. Exploiting generative models in discriminative
classiﬁers. In Advances in Neural Information Processing
Systems, Vol. 11, 1998.

[Keogh and Kasetty, 2002] E. Keogh and S. Kasetty. On the
need for time series data mining benchmarks: A survey
and empirical demonstration. In SIGKDD, pages 102–111,
2002.

[Lei and Govindaraju, 2005] H. Lei and V. Govindaraju.
Similarity-driven sequence classiﬁcation based on support
vector machines. In ICDAR, pages 252–261, 2005.

[Porikli, 2004] F. M. Porikli. Trajectory distance metric us-
ing hidden markov model based representation. In Euro-
pean Conference on Computer Vision (ECCV), May 2004.

[Sch¨olkopf and Smola, 2002] B. Sch¨olkopf and A. J. Smola.
Learning with Kernels - Support Vector Machines, Reg-
ularization, Optimization, and Beyond. The MIT Press,
Cambridge, MA, 2002.

[Shawe-Taylor and Christianini, 2004] J. Shawe-Taylor and
N. Christianini. Kernel Methods for Pattern Analysis.
Cambridge University Press, 2004.

[Shimodaira et al., 2002] H. Shimodaira, K. Ichi, N. M.
Nakai, and S. Sagayama. Dynamic time-alignment ker-
nel in support vector machines.
In Advances in Neural
Information Processing Systems, Vol. 14, 2002.

[Tapia and Rojas, 2003] E. Tapia and Ra´ul Rojas. Recogni-
tion of on-line handwritten mathematical formulas in the
e-chalk system. In IDCAR, 2003.

IJCAI-07

1052


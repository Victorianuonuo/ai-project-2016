Updates for Nonlinear Discriminants

Edin Andeli´c Martin Schaff¨oner Marcel Katz Sven E. Kr ¨uger Andreas Wendemuth

Cognitive Systems Group, IESK
Otto-von-Guericke-University

Magdeburg, Germany

edin.andelic@e-technik.uni-magdeburg.de

Abstract

A novel training algorithm for nonlinear discrimi-
nants for classiﬁcation and regression in Reproduc-
ing Kernel Hilbert Spaces (RKHSs) is presented.
It is shown how the overdetermined linear least-
squares-problem in the corresponding RKHS may
be solved within a greedy forward selection scheme
by updating the pseudoinverse in an order-recursive
way. The described construction of the pseudoin-
verse gives rise to an update of the orthogonal de-
composition of the reduced Gram matrix in linear
time. Regularization in the spirit of Ridge regres-
sion may then easily be applied in the orthogonal
space. Various experiments for both classiﬁcation
and regression are performed to show the competi-
tiveness of the proposed method.

1 Introduction

Models for regression and classiﬁcation that enforce square
loss functions are closely related to Fisher discriminants
[Duda and Hart, 1973]. Fisher discriminants are Bayes-
optimal in case of classiﬁcation with normally distributed
classes and equally structured covariance matrices [Duda and
Hart, 1973][Mika, 2002]. However, in contrast to SVMs,
Least Squares Models (LSMs) are not sparse in general and
hence may cause overﬁtting in a supervised learning scenario.
One way to circumvent this problem is to incorporate regular-
ization controlled by a continuous parameter into the model.
For instance, Ridge regression [Rifkin et al., 2003] penalizes
the norm of the solution yielding ﬂat directions in the RKHS,
which are robust against outliers caused by e. g. noise.
In
[Suykens and Vandewalle, 1999] Least-Squares SVMs (LS-
SVMs) are introduced which are closely related to Gaussian
processes and Fisher discriminants. A linear set of equa-
tions in the dual space is solved using e. g. the conjugate
gradient methods for large data sets or a direct method for
a small number of data. The solution is pruned [De Kruif
and De Vries, 2003][Hoegaerts et al., 2004] in a second
stage. The close relation between the LS-SVM and the Ker-
nel Fisher Discriminant (KFD) was shown in [Van Gestel et
al., 2002]. It follows from the equivalence between the KFD
and a least squares regression onto the labels [Duda and Hart,

1973][Mika, 2002] that the proposed method is closely re-
lated to the KFD and the LS-SVM. However, the proposed
method imposes sparsity on the solution in a greedy fashion
using subset selection like in [Billings and Lee, 2002][Nair et
al., 2002]. For the SVM case similar greedy approaches exist
[G. Cauwenberghs, 2000][Ma et al., 2003].

Especially in case of large data sets subset selection is a
practical method. It aims to eliminate the most irrelevant or
redundant samples. However, ﬁnding the best subset of ﬁxed
size is an NP-hard combinatorial search problem. Hence, one
is restricted to suboptimal search strategies. Forward selec-
tion starts with an empty training set and adds sequentially
one sample that is most relevant according to a certain cri-
terion (e. g. the mean square error). In [Nair et al., 2002]
an external algorithm which is based on elementary Givens
rotations is used to update the QR-decomposition of the re-
duced Gram matrix in order to construct sparse models. The
Gram Schmidt orthogonalization is used in [Billings and Lee,
2002] and [Chen et al., 1991] for the orthogonal decomposi-
tion of the Gram matrix. They also apply forward selection in
a second step to obtain sparse models. This method is known
as Orthogonal Least Squares (OLS). However, the OLS al-
gorithm requires the computation and the storage of the full
Gram matrix which is prohibitive for large datasets.

In this paper a very simple and efﬁcient way for construct-
ing LSMs in a RKHS within a forward selection rule with
much lower memory requirements is presented. The pro-
posed method exploits the positive deﬁniteness of the Gram
matrix for an order-recursive thin update of the pseudoin-
verse, which reveals to the best of our knowledge a novel kind
of update rule for the orthogonal decomposition. The solution
is regularized in a second stage using the Generalized Cross
Validation to re-estimate the regularization parameter.

The remainder of this paper is organized as follows.

In
section 2, computationally efﬁcient update rules for the pseu-
doinverse and the orthogonal decomposition are derived. In
section 3, it is shown how the solution may be regularized. In
section 4 some experimental results on regression and classi-
ﬁcation datasets are presented. Finally, a conclusion is given
in section 5.

2 Update of the Orthogonal Decomposition
In a supervised learning problem one is faced with a training
data set D = {xi, yi}, i = 1 . . . M . Here, xi denotes an

IJCAI-07

660

Noting that the pseudoinverse of a vector is given by

q†

m =

qT
m

(cid:3)qm(cid:3)2

equation (11) may be written as
m(I − Km−1K†
qT

m−1)y

ˆαm0 =

(cid:3)qm(cid:3)2

(12)

(13)

=

The matrix

m(I − Km−1K†
kT

m−1)T (I − Km−1K†

m−1)y

(cid:3)qm(cid:3)2

.

Pm = I − Km−1K†

m−1

(14)

is an orthogonal projection matrix which implies being sym-
metric and idempotent and thus equation (13) simpliﬁes to

ˆαm0 = q†

my.

(15)

Combining (15) with (8) the current weight vector ˆαm may
be updated as

(cid:2)

(cid:3)

(cid:2)

(cid:3)

ˆαm =

ˆαm−1
ˆαm0

K†

m−1 − K†
q†
m

m−1kmq†

m

=

(cid:2)

(cid:3)

y

(16)

(17)

input vector of ﬁxed size and yi is the corresponding target
value which is contained in R for regression or in {1, −1} for
binary classiﬁcation. It is assumed that xi (cid:2)= xj , for i (cid:2)= j.
We focus on sparse approximations of models of the form

ˆy = Kα.

(1)
The use of Mercer kernels k(·, x) [Mercer, 1909] gives rise
to a symmetric positive deﬁnite Gram Matrix K with ele-
ments Kij = k(xi, xj) deﬁning the subspace of the RKHS
in which learning takes place. The weight vector α =
{b, α1, . . . , αM } contains a bias term b with a corresponding
column 1 = {1, . . . , 1} in the Gram matrix.

Consider the overdetermined least-squares-problem

ˆαm = argmin

(cid:3)Kmαm − y(cid:3)2

(2)

αm

in the m-th forward selection iteration with the reduced Gram
matrix Km = [1 k1 . . . km] ∈ RM ×(m+1)
where ki =
(k(·, x1), . . . , k(·, xM ))T , i ∈ {1, . . . , m} denotes one pre-
viously unselected column of the full Gram matrix. We de-
note the reduced weight vector as αm = {b, α1, . . . , αm} ∈
Rm+1
and the target vector as y = (y1, . . . , yM )T . Among
all generalized inverses of Km the pseudoinverse

K†

m = (KT

mKm)−1KT

m

(3)

revealing the update

is the one that has the lowest Frobenius norm [Ben-Israel and
Greville, 1977]. Thus, the corresponding solution

K†

m =

K†

m−1 − K†
q†
m

m−1kmq†

m

ˆαm = K†

my

has the lowest Euclidean norm.

Partitioning Km and αm in the form
Km = [Km−1km]
αm = (αm−1αm)T

and setting αm = αm0 = const, the square loss becomes
L(αm−1, αm0) = (cid:3)Km−1αm−1 − (y − kmαm0)(cid:3)2.

The minimum of (7) in the least-squares-sense is given by

ˆαm−1 = K†

m−1(y − kmαm0).

(4)

(5)

(6)

(7)

(8)

Inserting (8) into (7) yields
L(αm0) = (cid:3)(I−Km−1K†

m−1)kmαm0−(I−Km−1K†

m−1)y(cid:3)2

with I denoting the identity matrix of appropriate size.

Note that the vector

qm = (I − Km−1K†

m−1)km

(9)

(10)

is the residual corresponding to the least-squares regression
onto km. Hence, qm is a nullvector if and only if km is a
nullvector unless K is not strictly positive deﬁnite. To ensure
strictly positive deﬁniteness of K, it is mandatory to add a
small positive constant ε to the main diagonal of the full Gram
matrix in the form K → K + εI. Forward selection may then
be performed using this strictly positive deﬁnite Gram matrix.
In the following km (cid:2)= 0 is assumed.

The minimum of (9) is met at

ˆαm0 = q†

m(I − Km−1K†

m−1)y

(11)

for the current pseudoinverse.

Since every projection qm = Pmkm lies in a subspace
which is orthogonal to Km−1 it follows immediately that
i qj = 0, for i (cid:2)= j. Hence, an orthogonal decomposi-
qT
tion

Km = QmUm

(18)

of the reduced Gram matrix is given by the orthogonal matrix

Qm = [Qm−1qm]

and the upper triangular matrix

(cid:2)(cid:4)

(cid:5)

Um =

Um−1
0T

m−1

(QT

mQm)−1QT

mkm

(cid:3)

.

(19)

(20)

In the m-th iteration O(M m) operations are required for all
these updates. Note that the inversion of the matrix QT
mQm
is trivial since this matrix is diagonal. However, the condition
number of the matrix Qm increases as the number of selected
columns m grows. Thus, to ensure numerical stability it is
important to monitor the condition number of this matrix and
to terminate the iteration if the condition number exceeds a
predeﬁned value unless another stopping criterion is reached
earlier.

3 Regularization and Selection of Basis

Centers

The goal of every forward selection scheme is to select the
columns of the Gram matrix that provide the greatest re-
duction of the residual. Methods like basis matching pur-
suit [Mallat and Zhang, 1993], order-recursive matching pur-
suit [Natarajan, 1995] or probabilistic approaches [Smola and

IJCAI-07

661

Sch¨olkopf, 2000] are several contributions to this issue. In
[Nair et al., 2002], forward selection is performed by simply
choosing the column that corresponds to the entry with the
highest absolute value in the current residual. The reasoning
is that the residual provides the direction of the maximum
decrease in the cost function 0.5α
Ty, since the
Gram matrix is strictly positive deﬁnite. The latter method
is used in the following experiments. But note that the de-
rived algorithm may be applied within any of the above for-
ward selection rules. In the following we will refer to the pro-
posed method as Order-Recursive Orthogonal Least Squares
(OROLS).

T Kα − α

Consider the residual

˜em = y − ˆym = y − Qm ˜αm

(21)
in the m-th iteration. The vector ˜αm contains the orthogonal
weights.

The regularized square residual is given by

Minimizing the GCV with respect to λ gives rise to a re-
estimation formula for λ. An alternative way to obtain a
re-estimation of λ is to maximize the Bayesian evidence
[MacKay, 1992].

Differentiating (27) with respect to λ and setting the result

to zero gives a minimum when

yT ˜Pm

∂ ˜Pmy
∂λ trace( ˜Pm) = yT ˜P2

my

∂trace( ˜Pm)

∂λ

.

(28)

Noting that

yT ˜Pm

∂ ˜Pmy
∂λ = λ ˜α

T

m(QT

mQm + λIm)−1 ˜αm

(29)

equation (28) can be rearranged to obtain the re-estimation
formula

λ :=

[∂trace( ˜Pm)/∂λ]yT ˜P2

my

trace( ˜Pm) ˜α

T
m(QT

mQm + λIm)−1 ˜αm

(30)

˜Em = ˜eT

m˜em + λ ˜α

T
m ˜αm

(22)

where

= yT ˜Pmy

where λ denotes a regularization paramter. The minimum of
(22) is given by

˜Pm = I − Qm(QT

(23)

= ˜Pm−1 −

mQm + λIm)−1QT
qmqT
m
λ + qT
mqm

.

m

Thus, the current residual corresponding to the regularized
least squares problem may be updated as
qmqT
m
λ + qT
mqm
yT qm

˜em = ( ˜Pm−1 −

)y

= ˜em−1 − qm

.

λ + qT

mqm

(24)

The orthogonal weights

( ˜αm)i =

yT qi
λ + qT

i qi

,

1 ≤ i ≤ m.

(25)

can be computed when the forward selection is stopped. The
original weights can then be recovered by

ˆαm = U−1

m ˜αm

(26)

which is an easy inversion since Um is upper triangular.

In each iteration one chooses the qi which corresponds to
the highest absolute value in the current residual and adds it to
Qm−1. It is possible to determine the number of basis func-
tions using crossvalidation or one may use for instance the
Bayesian Information Criterion or the Minimum Description
Length as alternative stopping criteria. Following [Gu and
Wahba, 1991] and [Orr, 1995] it is possible to use the Gen-
eralized Cross Validation (GCV ) as a stopping criterion. We
will now summarize the results. For details see [Orr, 1995].

The GCV is given by

GCVm =

1
M

(cid:6)

(cid:3) ˜Pmy(cid:3)2

(1/M ) trace( ˜Pm)

(cid:7)2 .

(27)

m(cid:8)

=

i=1

∂trace( ˜Pm)

∂λ

qT

i qi
(λ + qT

i qi)2

.

(31)

The forward selection is stopped when λ stops changing sig-
niﬁcantly.

The computational cost for this update is O(m). The ORO-
LOS algortihm is summarized in pseudocode in Algorithm 1.

4 Experiments
To show the usefulness of the proposed method empirically,
some experiments for regression and classiﬁcation are per-
formed. In all experiments the Gaussian kernel

(cid:9)

(cid:10)

k(x, x(cid:2)

) = exp

−

(cid:3)x − x(cid:2) (cid:3)2

2σ2

(32)

is used. The kernel parameter σ is optimized using a 5-fold
crossvalidation in all experiments. For the classiﬁcation ex-
periments the one-vs-rest approach is used to obtain a multi-
class classiﬁcation hypothesis.

4.1 Classiﬁcation
For classiﬁcation, 5 well-known benchmark datasets were
chosen. The USPS dataset contains 256 pixel values of hand-
written digits as training and testing instances.

The letter dataset contains 20000 labeled samples. The
character images were based on 20 different fonts and each
letter within these fonts was randomly distorted to produce
a dataset of unique stimuli. For this dataset no predeﬁned
split for training and testing exist. We used the ﬁrst 16000
instances for training and the remaining 4000 instances for
testing.

Optdigits is a database of digits handwritten by Turkish
writers. It contains digits written by 44 writers. The train-
ing set is generated from the ﬁrst 30 writers and digits writ-
ten by the remaining independent writers serve as testing in-
stances. The database was generated by scanning and pro-
cessing forms to obtain 32 × 32 matrices which were then
reduced to 8 × 8.

IJCAI-07

662

Algorithm 1 Order-Recursive Orthogonal Least Squares
(OROLS)
Require: Training data X, labels y, kernel

1 = 1

M

Initializations: λ ← 0, m ← 1, K1 = 1, K†
Q1 = 1, U1 = [1], I = {1, . . . , M }, Iopt = {}

1T ,

while λ changes signiﬁcantly and Qm is not illconditioned
do

Update ˜em

ﬁnd the index iopt of the entry of ˜em with the highest

absolute value

Iopt ← {Iopt, iopt}

I ← I \ {iopt}

Compute kiopt
Compute qiopt
Km ← [Km−1kopt]

Qm ← [Qm−1qopt]

Update K†

m and Um using kopt and qopt

Update λ

m ← m + 1

end while
return ˆαm, Iopt

Pendigits contains pen-based handwritten digits. The digits
were written down on a touch-sensitive tablet and were then
resampled and normalized to a temporal sequence of eight
pairs of (x, y) coordinates. The predeﬁned test set is formed
entirely from written digits produced by independent writers.
The satimage dataset was generated from Landsat Multi-
Spectral Scanner image data. Each pattern contains 36 pixel
values and a number indicating one of the six classes of the
central pixel.

The caracteristics of the datasets are summarized in table 1.
The results can be seen in table 2. Especially for the optdig-
its and pendigits datasets OROLS appears to be signiﬁcantly
superior compared with SVMs. The performance on the re-
maining 3 datasets is comparable with SVMs.

DATA SET
USPS
LETTER
OPTDIGITS
PENDIGITS
SATIMAGE

# CLASSES

# TRAINING

# TESTING

10
26
10
10
6

7291
16000
3823
7494
4435

2007
4000
1797
3498
2000

Table 1: Datasets used for the classiﬁcation experiments.

DATA SET
USPS
LETTER
OPTDIGITS
PENDIGITS
SATIMAGE

SV M OROLS

4.3
2.75
2.73
2.5
7.8

4.4(10)
2.61(4.3)
1.11(10.2)
1.66(1.9)
8.2(7.5)

Table 2: Test errors in % on 5 benchmark datasets. The one-
vs-rest approach is used. Average fraction of selected basis
centers in % within parantheses.

4.2 Regression

For regression, we ﬁrst perform experiments on a synthetic
dataset based on the function sinc(x) = sin(x)/x, x ∈
(−10, 10) which is corrupted by Gaussian noise. All training
and testing instances are chosen randomly using a uniform
distribution on the same interval. The results are illustrated in
ﬁgures 1-3 and table 3.

Additionally,

the two real world datasets Boston and
Abalone, which are available from the UCI machine learning
repository, are chosen. The hyperparameters are optimized in
a 5-fold crossvalidation procedure. For both datasets, ran-
dom partitions of the mother data for training and testing
are generated (100 (10) partitions with 481 (3000) instances
for training and 25 (1177) for testing for the Boston and
Abalone dataset, respectively). All continuous features are
rescaled to zero mean and unit variance for both Abalone and
Boston. The gender encoding (male / female /infant) for the
Abalone dataset is mapped into {(1, 0, 0), (0, 1, 0), (0, 0, 1)}.
The Mean Squared Error (MSE) of OROLS is compared with
a forward selection algorithm based on a QR-decomposition
of the Gram matrix [Nair et al., 2002]. The results in table
4 show that the MSE is improved signiﬁcantly by OROLS.
In contrast to OROLS the QR method uses an external algo-
rithm with reorthogonalization for the update of the orthog-
onal decomposition. We observed that our update scheme
which is to the best of our knowledge a novel update needs
not to be reorthogonalized as long as the Gram matrix has
full rank. This improvement of accuracy could be one rea-
son for the good performance of OROLS. Furthermore, it
should be noted that the best performance of OROLS for the
Boston dataset is quite favourable compared with the best per-
formance of SVMs (MSE 8.7 ± 6.8) [Sch¨olkopf and Smola,
2002].

METHOD

SVM
RVM

OROLS

RMSE
0.0519
0.0494
0.0431

Table 3: Average RMSE for the sinc experiment. 50 / 1000
randomly generated points are used for training / testing. The
standard deviation of the Gaussian noise is 0.1 in all runs.
The results are avereged over 100 runs.

IJCAI-07

663

1.2

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−10

training points
basis centers
true function
estimated function

−8

−6

−4

−2

0

2

4

6

8

10

E
S
M
R

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0

0.2

0.4

0.6

0.8

1

1.2

1.4

noise standard deviation

Figure 1: Example ﬁt to a noisy sinc function using 50 / 1000
randomly generated points for training / testing. The standard
deviation of the Gaussian noise is 0.1. The Root Mean Square
Error (RMSE) is 0.0269 in this case. 9 points are selected as
basis centers.

Figure 3: RMSE of ﬁts to a noisy sinc function w. r. t. dif-
ferent noise levels. 100 / 1000 randomly generated points are
used for training / testing. The results are avereged over 100
runs for each noise level.

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

E
S
M
R

0

0

50

100

150

200
number of training data

250

300

350

400

450

500

Figure 2: RMSE of ﬁts to a noisy sinc function w. r. t. dif-
ferent training set sizes. 1000 randomly generated points are
used for testing. The standard deviation of the Gaussian noise
is 0.1 in all runs. The results are avereged over 100 runs for
each size.

DATASET
BOSTON
ABALONE

QR

OROLS

8.35±5.67
4.53±0.29

7.9±3.28(26)

4.32±0.17(10.8)

Table 4: Mean Square Error (MSE) with standard deviations
for the Boston and Abalone dataset using different methods.
Average fraction of selected basis centers in % within paran-
theses.

5 Conclusion
A computationally efﬁcient training algorithm for orthogo-
nal least squares models using Mercer kernels is presented.

The pseudoinverse is updated order-recursively and reveals
the current orthogonal decomposition of the reduced Gram
matrix within a forward selection scheme. The Generalized
Cross Validation serves as an effective stopping criterion and
allows to adapt the regularization parameter in each iteration.
Extensive empirical studies using synthetic and real-world
benchmark datasets for classiﬁcation and regression suggest
that the proposed method is able to construct models with a
very competitive generalization ability. The advantage of the
proposed method compared with e. g. SVMs is its simplic-
ity. Sparsity is achieved in a computationally efﬁcient way
by constuction and can hence better be controlled than in the
SVM case where a optimization problem is to be solved. Fur-
thermore, in contrast to SVMs OROLS allows an easy incor-
poration of multiple kernels, i e. the kernel parameters may be
varied for different training instances in order to obtain more
ﬂexible learning machines. This possibility is not examined
here and may be an interesting direction for future work. A
further step for future work could be the development of the
proposed algorithm for tasks like dimensionality reduction or
online-learning.

References

[Ben-Israel and Greville, 1977] A. Ben-Israel and T. N. E.
Greville. Generalized Inverses: Theory and Applications.
Wiley, 1977.

[Billings and Lee, 2002] S. A. Billings and K. L. Lee. Non-
linear ﬁsher discriminant analysis using a minimum
squared error cost function and the orthogonal
least
squares algorithm. Neural Networks, 15:263–270, 2002.

[Chen et al., 1991] S. Chen, C. F. N. Cowan, and P. M.
Grant. Orthogonal least squares learning for radial ba-
sis function networks. IEEE Transactions on Neural Net-
works, 2(2):302–309, 1991.

IJCAI-07

664

[Smola and Sch¨olkopf, 2000] A. J. Smola and B. Sch¨olkopf.
Sparse greedy matrix approximation for machine learn-
ing. In Proceedings of the 17th International Conference
on Machine Learning, pages 911–918. Morgan Kaufmann,
2000.

[Suykens and Vandewalle, 1999] J. A. K. Suykens and
J. Vandewalle. Least squares support vector machine clas-
siﬁers. Neural Processing Letters, 9:293–300, 1999.

[Van Gestel et al., 2002] T. Van Gestel, J. A. K. Suykens,
G. Lanckriet, A. Lambrechts, B. De Moor, and J. Van-
dewalle. A bayesian framework for least squares sup-
port vector machine classiﬁers, gaussian processes and
kernel ﬁsher discriminant analysis. Neural Computation,
14(5):1115–1147, May 2002.

[De Kruif and De Vries, 2003] B. J. De Kruif and T. J. A. De
Vries. Pruning error minimization in least squares support
vector machines. IEEE Transactions on Neural Networks,
14(3):696–702, 2003.

[Duda and Hart, 1973] R. O. Duda and P. E. Hart. Pattern
classiﬁcation and scene analysis. John Wiley and Sons,
1973.

[G. Cauwenberghs, 2000] T. Poggio G. Cauwenberghs. In-
cremental and decremental support vector machine learn-
ing. In Advance in Neural Information Processing Systems
(NIPS 2000), 2000.

[Gu and Wahba, 1991] C. Gu and G. Wahba. Minimizing
gcv/gml scores with multiple smoothing parameters via
the newton method. SIAM Journal on Scientiﬁc and Sta-
tistical Computing, 12(2):383–398, 1991.

[Hoegaerts et al., 2004] L. Hoegaerts, J. A. K. Suykens,
J. Vanderwalle, and B. De Moor. A comparison of prun-
ing algorithms for sparse least squares support vector ma-
chines. In Proceedings of the 11th International Confer-
ence on Neural Information Processing (ICONIP 2004),
Calcutta, India, Nov 2004.

[Ma et al., 2003] J. Ma, J. Theiler, and S. Perkins. Accurate
on-line support vector regression. Neural Computation,
15:2683–2703, 2003.

[MacKay, 1992] D. J. C. MacKay. Bayesian interpolation.

Neural Computation, 4(3):415–447, 1992.

[Mallat and Zhang, 1993] S. Mallat and Z. Zhang. Matching
pursuit in a time-frequency dictionary. IEEE Transactions
on Signal Processing, 41:3397–3415, 1993.

[Mercer, 1909] J. Mercer. Functions of positive and negative
type and their connections to the theory of integral equa-
tions. In Philos. Trans. Roy. Soc., pages A 209:415–446,
London, 1909.

[Mika, 2002] S. Mika. Kernel Fisher Discriminants. PhD

thesis, Technical University Berlin, 2002.

[Nair et al., 2002] P. Nair, A. Choudhury, and A. J. Keane.
Some greedy learning algorithms for sparse regression and
classiﬁcation with mercer kernels. Journal of Machine
Learning Research, 3:781–801, 12 2002.

[Natarajan, 1995] B. K. Natarajan. Sparse approximate so-
lutions to linear systems. SIAM Journal of Computing,
25:227–234, 1995.

[Orr, 1995] M. Orr. Regularisation in the selection of radial
basis function centres. Neural Computation, 7:606–623,
1995.

[Rifkin et al., 2003] R. Rifkin, G. Yeo, and T. Poggio. Regu-
larized least squares classiﬁcation.
In J. A. K. Suykens,
G. Horvath, S. Basu, C. Micchelli, and J. Vandewalle,
editors, Advances in Learning Theory: Methods, Models
and Applications, volume 190 of NATO Science Series III:
Computer and Systems Sciences, chapter 7, pages 131–
154. IOS Press, Amsterdam, 2003.

[Sch¨olkopf and Smola, 2002] B. Sch¨olkopf and A. J. Smola.

Learning with Kernels. MIT Press, 2002.

IJCAI-07

665


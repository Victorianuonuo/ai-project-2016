Phase Transitions within Grammatical Inference

Nicolas Pernot, Antoine Cornu´ejols & Mich`ele Sebag

Laboratoire de Recherche en Informatique, CNRS UMR 8623

Bˆat.490, Universit´e de Paris-Sud, Orsay

91405 Orsay Cedex (France)

antoine,sebag@lri.fr

Abstract

It is now well-known that the feasibility of induc-
tive learning is ruled by statistical properties link-
ing the empirical risk minimization principle and
the “capacity” of the hypothesis space. The dis-
covery, a few years ago, of a phase transition phe-
nomenon in inductive logic programming proves
that other fundamental characteristics of the learn-
ing problems may similarly affect the very possi-
bility of learning under very general conditions.
Our work examines the case of grammatical in-
ference. We show that while there is no phase
transition when considering the whole hypothesis
space, there is a much more severe “gap” phe-
nomenon affecting the effective search space of
standard grammatical induction algorithms for de-
terministic ﬁnite automata (DFA). Focusing on the
search heuristics of the RPNI and RED-BLUE algo-
rithms, we show that they overcome this problem
to some extent, but that they are subject to over-
generalization. The paper last suggests some direc-
tions for new generalization operators, suited to this
Phase Transition phenomenon.

Introduction

1
It is now well-known that the feasibility of inductive learn-
ing is ruled by statistical properties linking the empirical risk
minimization principle and the “capacity” of the hypothesis
space [Vapnik, 1995]. While this powerful framework leads
to a much deeper understanding of machine learning and to
many theoretical and applicative breakthroughs, it basically
involves only statistical information on the learning search
space, e.g. the so-called VC-dimension. The dynamics of the
learning search is not considered.

Independently, a new combinatoric paradigm has been
studied in the Constraint Satisfaction community since the
early 90s, motivated by computational complexity concerns
[Cheeseman et al., 1991]: where are the really hard prob-
lems?
Indeed, the worst case complexity analysis poorly
accounts for the fact that, despite an exponential worst-case
complexity, empirically, the complexity is low for most CSP
instances. These remarks led to developing the so-called

phase transition framework (PT) [Hogg et al., 1996], which
considers the satisﬁability and the resolution complexity of
CSP instances as random variables depending on order pa-
rameters of the problem instance (e.g. constraint density and
tightness). This framework unveiled an interesting structure
of the CSP landscape. Speciﬁcally, the landscape is divided
into three regions: the YES region, corresponding to under-
constrained problems, where the satisﬁability probability is
close to 1 and the average complexity is low; the NO region,
corresponding to overconstrained problems, where the satis-
ﬁability probability is close to 0 and the average complexity
is low too; last, a narrow region separating the YES and NO
regions, referred to as phase transition region, where the sat-
isﬁability probability abruptly drops from 1 to 0 and which
concentrates on average the computationally heaviest CSP in-
stances.

The phase transition paradigm has been transported to re-
lational machine learning and inductive logic programming
(ILP) by [Giordana and Saitta, 2000], motivated by the fact
that the covering test most used in ILP [Muggleton and Raedt,
1994] is equivalent to a CSP. As anticipated, a phase transi-
tion phenomenon appears in the framework of ILP: a wide
YES (respectively NO) region includes all hypotheses which
cover (resp. reject) all examples, and the hypotheses that can
discriminate the examples lie in the narrow PT, where the av-
erage computational complexity of the covering test reaches
its maximum.

Besides computational complexity, the PT phenomenon
has far-reaching effects on the success of relational learning
[Botta et al., 2003]. For instance, a wide Failure Region is ob-
served: for all target concepts/training sets in this region, no
learning algorithms among the prominent ILP ones could ﬁnd
hypotheses better than random guessing [Botta et al., 2003].
These negative results lead to a better understanding of the
intrinsic limits of the existing ILP algorithms and search bi-
ases. Formally, consider a greedy specialization (top-down)
search strategy: starting its exploration in the YES region,
the system is almost bound to make random specialization
choices, for all hypotheses in this region cover every exam-
ple on average. The YES region constitutes a rugged plateau
from a search perspective, and there is little chance that the
algorithm ends in the right part of the PT region, where good
hypotheses lie. A similar reasoning goes for algorithms that
follow a greedy generalization strategy.

The phase transition paradigm thus provides another per-
spective on the pitfalls facing machine learning, focusing on
the combinatoric search aspects while statistical learning fo-
cuses on the statistical aspects.

The main question studied in this paper is whether the PT
phenomenon is limited to relational learning, or threatens the
feasibility and tractability of other learning settings as well.
A learning setting with intermediate complexity between
full relational learning and propositional learning is thus
considered, that of grammatical inference (GI) [Pitt, 1989;
Sakakibara, 1997]. Only the case of Finite-State Automata
(FSA, section 2), will be considered through the paper.
Speciﬁcally, the phase transition phenomenon will be inves-
tigated with respect to three distributions on the FSA space,
incorporating gradually increasing knowledge on the syntac-
tical and search biases of GI algorithms.

The ﬁrst distribution incorporates no information on the al-
gorithm and considers the whole space of FSA. Using a set of
order parameters, the average coverage of automata is studied
analytically and empirically.

The second one reﬂects the bias introduced by the gener-
alization relations deﬁned on the FSA space and exploited
by GI algorithms. The vast majority of these algorithms ﬁrst
construct a least general generalization of the positive exam-
ples, or Preﬁx Tree Acceptor (PTA), and restrict the search to
the generalizations of the PTA, or generalization cone1.

The third distribution takes into account the heuristics used
by GI algorithms, guiding the search trajectory in the general-
ization cone. Due to space limitations, the study is restricted
to two prominent GI algorithms, namely RPNI [Oncina and
Garcia, 1992] and RED-BLUE [Lang et al., 1998].

This paper is organized as follows. Section 2 brieﬂy in-
troduces the domain of Grammatical Inference, the princi-
ples of the inference algorithms and deﬁnes the order param-
eters used in the rest of the paper. Section 3 investigates the
existence and potential implications of phase transition phe-
nomenons in the whole FSA space (section 3.1) and in the
generalization cone (section 3.2). Section 4 focuses on the
actual landscape explored by GI algorithms, speciﬁcally con-
sidering the search trajectories of RPNI and RED-BLUE. Sec-
tion 5 discusses the scope of the presented study and lays out
some perspectives for future research.

2 Grammatical inference
After introducing general notations and deﬁnitions, this sec-
tion brieﬂy discusses the state of the art and introduces the
order parameters used in the rest of the paper.

2.1 Notations and deﬁnitions
Grammatical inference is concerned with inferring grammars
from positive (and possibly negative) examples. Only reg-
ular grammars are considered in this paper. They form the
bottom class of the hierarchy of formal grammars as deﬁned

1More precisely, the Preﬁx Tree Acceptor is obtained by merging
the states that share the same preﬁx in the Maximal Canonical Au-
tomaton (MCA), which represents the whole positive learning set as
an automaton. A PTA is therefore a DFA with a tree-like structure.

by Chomsky, yet are sufﬁciently rich to express many inter-
esting sequential structures. Their identiﬁcation in the limit
from positive examples only is known to be impossible, while
it is feasible with a complete set of examples [Gold, 1967].

It is known that any regular language can be produced by
a ﬁnite-state automaton (FSA), and that any FSA generates
a regular language. In the remaining of the paper, we will
mostly use the terminology of ﬁnite-state automata. A FSA
is a 5-tuple A = hΣ,Q,Q0, F, δi where Σ is a ﬁnite alphabet,
Q is a ﬁnite set of states, Q0 ⊆ Q is the set of initial states,
F ⊆ Q is the set of ﬁnal states, δ is the transition function
deﬁned from Q × Σ to 2Q.

A positive example of a FSA is a string on Σ, produced by
following any path in the graph linking one initial state q0 to
any accepting state.
A ﬁnite state-automaton (FSA) is deterministic (DFA) if
Q0 contains exactly one element q0 and if ∀q ∈ Q,∀x ∈
Σ, Card(δ(q, x)) ≤ 1. Otherwise it is non-deterministic
(NFA). Every NFA can be translated into an equivalent DFA,
but at the price of being possibly exponentially more complex
in terms of number of states. Given any FSA A’, there exists
a minimum state DFA (also called canonical DFA) A such
that L(A) = L(A0) (where L(A) denotes the set of strings
accepted by A). Without loss of generality, it can be assumed
that the target automaton being learned is a canonical DFA.

A set S+ is said to be structurally complete with respect
to a DFA A if S+ covers each transition of A and uses every
element of the set of ﬁnal states of A as an accepting state.
Clearly, L(P T A(S+)) = S+.
Given a FSA A and a partition π on the set of states Q of
A, the quotient automaton is obtained by merging the states
of A that belong to the same block in partition π (see [Dupont
et al., 1994] for more details). Note that a quotient automa-
ton of a DFA might be a NFA and vice versa. The set of
all quotient automata obtained by systematically merging the
states of a DFA A represents a lattice of FSAs. This lattice
is ordered by the grammar cover relation (cid:22). The transitive
closure of (cid:22) is denoted by (cid:28). We say that Aπi (cid:28) Aπj iff
L(Aπi) ⊆ L(Aπj ). Given a canonical DFA A and a set S+
that is structurally complete with respect to A, the lattice de-
rived from P T A(S+) is guaranteed to contain A.

From these assumptions, follows the paradigmatic ap-
proach of most grammatical inference algorithms (see, e.g.,
[Coste, 1999; Dupont et al., 1994; Pitt, 1989; Sakakibara,
1997]), which equates generalization with state merging op-
erations starting from the PTA.

2.2 Learning biases in grammatical inference
The core task of GI algorithms is thus to select iteratively
a pair of states to be merged. The differences among al-
gorithms is related to the choice of: (i) the search criterion
(which merge is the best one); (ii) the search strategy (how is
the search space explored); and (iii) the stopping criterion.

We shall consider here the setting of learning FSAs from
positive and negative examples, and describe the algorithms
studied in section 3. In this setting, the stopping criterion is
determined from the negative examples: generalization pro-
ceeds as long as the candidate solutions remain correct, not

covering any negative example2

The RPNI algorithm [Oncina and Garcia, 1992] uses a
depth ﬁrst search strategy with some backtracking ability, fa-
voring the pair of states which is closest to the start state,
such that their generalization (FSA obtained by merging the
two states and subsequently applying the determinisation op-
erator) does not cover any negative example.

The RED-BLUE algorithm (also known as BLUE-FRINGE)
[Lang et al., 1998] uses a beam search from a candidate list,
selecting the pair of states after the Evidence-Driven State
Merging (EDSM) criterion, i.e. such that their generalization
involves a minimal number of ﬁnal states. RED-BLUE thus
also performs a search with limited backtracking, based on a
more complex criterion and a wider search width than RPNI.
2.3 Order Parameters
Following the methodology introduced in [Giordana and
Saitta, 2000], the PT phenomenon is investigated along so-
called order parameters chosen in accordance with the pa-
rameters used in the Abbaddingo challenge [Lang et al.,
1998]:

• The number Q of states in the DFA.
• The number B of output edges on each state.
• The number L of letters on each edge.
• The fraction a of accepting states, taken in [0,1].
• The size |Σ| of the alphabet considered.
• The length ‘ of the test examples. Also the maximal
length ‘ of the learning examples in S+ (as explained
below).

The study ﬁrst focuses on the intrinsic properties of the
search space (section 3) using a a random sampling strategy
(all ‘ letters in the string being independently and uniformly
drawn in Σ). In section 4, we examine the capacity of the
studied learning algorithms to approximate a target automa-
ton, based on positive sampling, where each training string is
produced by following a path in the graph, randomly select-
ing an output edge in each step3.

3 Phase Transitions: the FSA space and the

generalization cone

This section investigates the percentage of coverage of deter-
ministic and non-deterministic Finite-State Automata, either
uniformly selected (section 3.1), or selected in the subspace
actually investigated by grammatical inference algorithms,
that is, the generalization cone (section 3.2).
3.1 Phase Transition in the whole FSA space
The sampling mechanism on the whole deterministic FSA
space (DFA) is deﬁned as follows. Given the order param-
eter values (Q, B, L, a,|Σ|):

2In this paper, after the standard Machine Learning terminology,
a string is said to be covered by a FSA iff it belongs to the language
thereof.

3The string is cut at the last accepting state met before arriving

at length ‘, if any; otherwise it is rejected.

• for every state q, (i) B output edges (q, q0) are cre-
ated, where q0 is uniformly selected with no replacement
among the Q states; (ii) L × B distinct letters are uni-
formly selected in Σ; and (iii) these letters are evenly
distributed among the B edges above.
• every state q is turned into an accepting state with prob-

ability a.

The sampling mechanism for NFA differs from the above in
a single respect:
two edges with same origin state are not
required to carry distinct letters.

For each setting of the order parameters, 100 independent
problem instances are constructed. For each considered FSA
(the sampling mechanisms are detailed below), the cover-
age rate is measured as the percentage of covered examples
among 1,000 examples (strings of length ‘) uniformly sam-
pled.
Fig. 1 shows the average coverage in the (a, B) plane, for
|Σ| = 2, L = 1 and ‘ = 10, where the accepting rate a varies
in [0, 1] and the branching factor B varies in {1,2}. Each
point reports the average coverage of a sample string s by a
FSA (averaged over 100 FSA drawn with accepting rate a and
branching factor B, tested on 1, 000 strings s of length ‘).

These empirical results are analytically explained from the
simple equations below, giving the probability that a string of
length ‘ be accepted by a FSA deﬁned on an alphabet of size
|Σ|, with a branching factor B and L letters on each edge, in
the DFA and NFA cases (the number of states Q is irrelevant
here).

(

P (accept) =

a · ( B·L|Σ| )‘
a · [1 − (1 − L|Σ|)B]‘

for a DFA
for a NFA

The coverage of the FSA decreases as a and B decrease.
The slope is more abrupt in the DFA case than in the NFA
case; still, there is clearly no phase transition here.

Figure 1: Coverage landscapes for Deterministic and Non-
Deterministic FSA, for |Σ|=2, L=1 and ‘=10. The density
of accepting states a and the branching factor B respectively
vary in [0, 1] and {1, 2}.

3.2 PT in the Generalization Cone
The coverage landscape displayed in Fig. 1 might suggest
that grammatical inference takes place in a well-behaved
search space. However, grammatical inference algorithms do

not explore the whole FSA space. Rather, as stated in section
2.1, the search is restricted to the generalization cone, the set
of generalizations of the PTA formed from the set S+ of the
positive examples. The next step is thus to consider the search
space actually explored by GI algorithms.

A new sampling mechanism is deﬁned to explore the DFA
generalization cone:
1. |S+| (= 200 in the experiments) examples of length
‘ are uniformly and independently sampled within the
space of all strings of length < ‘, and the corresponding
PTA is constructed;

2. N (= 50 in the experiments) PTAs are constructed in

that way.

3. K (= 20 in the experiments) generalization paths, lead-
ing from each PTA to the most general FSA or Universal
Acceptor (UA), are constructed;
In each generalization path (A0 = P T A, A1, . . . , At =
U A), the i-th FSA Ai is constructed from Ai−1 by merg-
ing two uniformly selected states in Ai−1, and subse-
quently applying the determinisation operator.

4. The generalization cone sample is made of all the FSAs
in all generalization paths (circa 270,000 FSAs in the
experiments).

The sampling mechanism on the non-deterministic gener-
alisation cone differs from the above in a single respect: the
determinisation operator is never applied.
Fig. 2 shows the behaviour of the coverage in the DFA
generalisation cone for |Σ| = 4 and ‘ = 8. Each DFA A is
depicted as a point with coordinates (Q, c), where Q is the
number of states of A and c is its coverage (measured as in
section 3). The coverage rate for each FSA in the sample is
evaluated from the coverage rate on 1000 test strings of length
‘.
NFA generalisation cone, with |Σ| = 4 and ‘ = 16.
Fig 2, typical of all experimental results in the range of ob-
servation (|Σ| = 2, 4, 8, 16, and ‘ = 2, 4, 6, 8, 16, 17), shows
a clear-cut phase transition. Speciﬁcally, here, the coverage
abruptly jumps from circa 13% to 54%; and this jump co-
incides with a gap in the number of states of the DFAs in
the generalization cone: no DFA with a number of states in
[180, 420] was found. The gap is even more dramatic as the
length of the training and test sequences ‘ is increased.

Fig. 3 similarly shows the behaviour of the coverage in the

Interestingly, a much smoother picture appears in the non-
deterministic case (ﬁg. 3); although the coverage rapidly in-
creases when the number of states decreases from 300 to 200,
no gap can be seen, neither in the number of states nor in the
coverage rate itself4.

In the following, we focus on the induction of DFAs.
4 Phase transition and search trajectories
The coverage landscape, for the DFAs, shows a hole in the
generalization cone, with a density of hypotheses of coverage

4The difference with the DFA case is due to the determinisation
process that forces further states merging when needed. A diffusion
like analytical model was devised, that predicts the observed start of
the gap with 15% precision.

Figure 2: Coverage landscape in the DFA generalization cone
(|Σ| = 4, ‘ = 8). At the far right stand the 50 PTA sam-
pled, with circa 1150 states each. The generalization cone of
each PTA includes 1,000 generalization paths, leading from
the PTA to the Universal Acceptor. Each point reports the
coverage of a DFA, evaluated over a sample of 1,000 strings.
This graph shows the existence of a large gap regarding both
the number of states and the coverage of the DFAs that can
be reached by generalization.

Figure 3: Coverage landscape in the NFA generalization
cone, with same order parameters as in ﬁg. 2.

in between a large interval (typically between less than 20%
to approximately 60%) falling abruptly. Therefore, a random
exploration of the generalization cone would face severe difﬁ-
culties in ﬁnding a hypothesis in this region and would likely
return hypotheses of poor performance if the target concept
had a coverage rate in this “no man’s land” interval.

It is consequently of utmost importance to examine the
search heuristics that are used in the classical grammatical
inference systems. First, are they able to thwart the a pri-
ori very low density of hypotheses in the gap? Second, are
they able to guide the search toward hypotheses of appropri-
ate coverage rate, specially if this coverage falls in the gap?

The study will thus focus on two standard algorithms in
grammatical inference, namely the RPNI and the RED-BLUE
algorithms [Oncina and Garcia, 1992; Lang et al., 1998].

NumberofstatesCoveragerate4.1 Experimental setting
Previous experiments considered training sets made of pos-
itive randomly drawn strings sequences only. However, in
order to assess the performance of learning algorithms, the
hypothesis learned must now be compared to the target au-
tomaton. Therefore, another experimental setting is used in
this section, with the sampling of target automata, and the
construction of training and test sets. These data sets include
positive and negative examples as most GI algorithms (and
speciﬁcally RPNI and RED-BLUE) use negative examples in
order to stop the generalization process.

In our ﬁrst experiments, we tested whether heuristically
guided inference algorithms can ﬁnd good approximations of
the target automata considering target automata with approx-
imately (i) 50% coverage rate (as considered in the inﬂuential
Abbadingo challenge, and in the middle of the “gap”), and
(ii) 5% coverage rate.

For each target coverage rate, we used the experimental
setting described in [Lang et al., 1998] in order to retain a cer-
tain number of target automata with a mean size of Q states
(Q = 50, in our experiments). For each automaton then, we
generated N (=20) training sets of size |S| (= 100) labeled
according to the target automaton, with an equal number of
positive and negative instances (|S+| = |S−| = 50) of length
‘ = 14. The coverage rate was computed as before on 1000
uniformly drawn strings (with no intersection with the train-
ing set).

In a second set of experiments, we analyzed the learning
performances of the algorithms with respect to test errors,
both false positive and false negative.

In these experiments, we chose the type of target automata
by setting the number of states Q and some predetermined
structural properties5.

4.2 The heuristically guided search space
Due to space limitation, only the graph obtained for the RPNI
algorithm is reported (see ﬁgure 4), with three typical learn-
ing trajectories. Similar results were obtained with the RED-
BLUE algorithm.

One immediate result is that both the RPNI and the EDSM
heuristics manage to densely probe the “gap”. This can ex-
plain why the gap phenomenon was not discovered before,
and why the RED-BLUE algorithm for instance could solve
some cases of the Abbadingo challenge where the target con-
cepts have a coverage rate of approximately 50%. How-
ever, where RPNI tends to overspecialize the target automa-
ton, RED-BLUE tends to overgeneralize it by 5% to 10%.

In order to test the capacity of the algorithms to return au-
tomata with a coverage rate close to the target coverage, we
repeated these experiments with target automata of coverage
rate of approximately 3%. The results (ﬁgure 5) shows that,
in this case, RPNI ends up with automata of coverage 4 to
6 times greater than the target coverage. The effect is even
more pronounced with RED-BLUE which returns automata
of average coverage rate around 30%!

5The datasets and more detail are available at http://www.

lri.fr/∼antoine/www/pt-gi/

Figure 4: Three RPNI learning trajectories for a target concept
of coverage=56%. Their extremity is outlined in the oval on
the left. The doted horizontal line corresponds to the cover-
age of the target concept. The cloud of points corresponds to
random trajectories.

Figure 5: Same as in ﬁgure 4, except for the coverage of the
target concept, here 3%.

4.3 Generalization error
Table 1, obtained for different sizes of the target automata
and for training sets of structural completeness above 40%,
conﬁrms that both RPNI and RED-BLUE return overgener-
alized hypotheses. On one hand, their average coverage is
vastly greater than the coverage of the target automata, on the
other hand, they tend to cover only part of the positive test
instances, while they cover a large proportion of the nega-
tive test instances. This shows that the heuristics used in both
RPNI and RED-BLUE may be inadequate for target concepts
of low coverage.

5 Conclusion
This research has extended the Phase Transition-based
methodology [Botta et al., 2003] to the Grammatical Infer-
ence framework. Ample empirical evidence shows that the
search landscape presents signiﬁcant differences depending
on the search operators that are considered.

0102030405060708090100050100150200250300350400450CoverageRateNumberofStates0102030405060708090100050100150200250300350400450CoverageRateNumberofStatesAlgo. Qc
15
RB
RB
25
50
RB
100
RB
15
RPNI
RPNI
25
50
RPNI
RPNI
100

ucovc Qf
5.97
4.88
4.2
3.39
5.95
4.7
3.87
3.12

10.38
12.77
14.23
13.13
5.14
7.56
14.08
26.41

ucovf
33.81
40.35
45.38
30.35
22.9
23.07
23.45
23.151

pcovf
60.93
62.68
66.14
42.81
57.51
56.38
51.89
50.12

ncovf
34.69
37.87
42.23
28.69
26.99
25.98
24.42
24.40

Table 1: Performances of RED-BLUE (RB) and RPNI for tar-
get DFA of sizes Q = 15, 25, 50 and 100 states. Qf , ucovf ,
pcovf and ncovf respectively denote the average size of the
learned automata, their average coverage, the true positive
and the false positive rates.

A ﬁrst result is that random search appears to be more dif-
ﬁcult in the DFA generalization cone than in the whole search
space: a large gap was found, in terms of hypothesis cover-
age and size. This remark explains why sophisticated search
biases are needed for grammatical inference algorithms in the
problem range corresponding with the hole in the generaliza-
tion cone.

A second ﬁnding regards the limitations of the search oper-
ators in RPNI and RED-BLUE, especially outside the region
of the Abbadingo target concepts. Experiments with artiﬁcial
learning problems built from target concepts with coverage
less than 10% reveal that RPNI and RED-BLUE alike tend to
learn overly general hypotheses; with respect to both the size
(estimated by the number of states) and the coverage of the
hypotheses, often larger by an order of magnitude than that of
the target concept. What is even more worrying, is that this
overgeneralization does not imply that the found hypotheses
are complete: quite the contrary, the coverage of the positive
examples remains below 65%, in all but one setting.

The presented study opens several perspectives for fur-
ther research. First, it suggests that the learning search, and
especially the stopping criterion, could be controlled using
a hyper-parameter:
the coverage rate of the target concept
(possibly supplied by the expert, or estimated e.g. by cross-
validation). In other words, the stopping criterion of the algo-
rithms might be reconsidered. Secondly, more conservative
generalisation operators will be investigated. Preliminary ex-
periments done with e.g. reverted generalisation (same op-
erator as in RPNI, applied on the reverted example strings)
show that such operators can delay the determinisation cas-
cade, and offer a ﬁner control of the ﬁnal coverage rate of the
hypotheses.

Finally, the main claim of the paper is that the phase tran-
sition framework can be used to deliver indications regarding
when and where speciﬁc search biases might fail − hope-
fully leading to understand and ultimately alleviate their lim-
itations.

Acknowledgments
The last two authors are partially supported by the PASCAL
Network of Excellence IST-2002-506 778.

References
[Botta et al., 2003] Marco Botta, Attilio Giordana, Lorenza
Saitta, and Mich`ele Sebag. Relational learning as search in
a critical region. Journal of Machine Learning Research,
4:431–463, 2003.

[Cheeseman et al., 1991] Peter Cheeseman, Bob Kanefsky,
and William M. Taylor. Where the Really Hard Prob-
In Proceedings of the Twelfth International
lems Are.
Joint Conference on Artiﬁcial Intelligence, IJCAI-91, Sid-
ney, Australia, pages 331–337, 1991.

[Coste, 1999] Francois Coste. State merging inference of ﬁ-

nite state classiﬁers. Technical report, Irisa, May 1999.

[Dupont et al., 1994] Pierre Dupont, Laurent Miclet, and En-
rique Vidal. What Is the Search Space of the Regular Infer-
ence? In Proceedings of the Second International Collo-
quium on Grammatical Inference and Applications, ICGI-
94, pages 25–37, 1994.

[Giordana and Saitta, 2000] Attilio Giordana and Lorenza
Saitta. Phase transitions in relational learning. Machine
Learning, 41:217–251, 2000.

[Gold, 1967] E. M. Gold. Language identiﬁcation in the

limit. Information and Control, 10:447–474, 1967.

[Hogg et al., 1996] Thomas Hogg, Bernard Hubberman, and
C. Williams. Phase transitions and the search problem.
Artiﬁcial Intelligence, 81:1–15, 1996.

[Lang et al., 1998] Kevin Lang, Barak Pearlmutter, and Rod-
ney Price. Results of the abbadingo one dfa learning com-
petition and a new evidence driven state merging algo-
rithm. In Fourth International Colloquium on Grammati-
cal Inference (ICGI-98), volume LNCS-1433, pages 1–12.
Springer Verlag, 1998.

[Muggleton and Raedt, 1994] Stephen Muggleton

and
Luc De Raedt. Inductive logic programming: Theory and
methods. Journal of Logic Programming, 19/20:629–679,
1994.

[Oncina and Garcia, 1992] J. Oncina and P. Garcia.

Infer-
ring regular languages in polynomial update time. Pattern
Recognition and Image Analysis, pages 49–61, 1992.

[Pitt, 1989] Leonard Pitt. Inductive inference, dfas, and com-
In Proceedings of the Workshop
putational complexity.
on Analogical and Inductive Inference (AII-89), volume
LNCS-397, pages 18–44. Springer Verlag, 1989.

[Sakakibara, 1997] Yasubumi Sakakibara. Recent advances
of grammatical inferences. Theoretical Computer Science,
185:15–45, 1997.

[Vapnik, 1995] Vladimir Vapnik. The Nature of Statistical

Learning. Springer, 1995.


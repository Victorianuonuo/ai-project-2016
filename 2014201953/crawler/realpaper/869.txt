Marginalized Multi-Instance Kernels

James T. Kwok

Pak-Ming Cheung

Department of Computer Science and Engineering

Hong Kong University of Science and Technology, Hong Kong

{jamesk,pakming}@cse.ust.hk

Abstract

Support vector machines (SVM) have been highly
successful in many machine learning problems.
Recently, it is also used for multi-instance (MI)
learning by employing a kernel that is deﬁned di-
rectly on the bags. As only the bags (but not the in-
stances) have known labels, this MI kernel implic-
itly assumes all instances in the bag to be equally
important. However, a fundamental property of MI
learning is that not all instances in a positive bag
necessarily belong to the positive class, and thus
different instances in the same bag should have dif-
ferent contributions to the kernel.
In this paper,
we address this instance label ambiguity by using
the method of marginalized kernels.
It ﬁrst as-
sumes that all the instance labels are available and
deﬁnes a label-dependent kernel on the instances.
By integrating out the unknown instance labels, a
marginalized kernel deﬁned on the bags can then
be obtained. A desirable property is that this ker-
nel weights the instance pairs by the consistencies
of their probabilistic instance labels. Experiments
on both classiﬁcation and regression data sets show
that this marginalized MI kernel, when used in a
standard SVM, performs consistently better than
the original MI kernel. It also outperforms a num-
ber of traditional MI learning methods.

1 Introduction
In supervised learning, each training pattern has a known
class label. However, many applications only have weak la-
bel information and thus cannot be formulated as supervised
learning problems. A classic example as discussed by [Diet-
terich et al., 1997] is drug activity prediction, where the task
is to predict whether a drug molecule can bind to the targets
(enzymes or cell-surface receptors). A molecule is considered
useful as a drug if one of its conformations can bind to the
targets. However, biochemical data can only tell the binding
capability of a molecule, but not a particular conformation. In
other words, we only have class labels associated with sets of
patterns (bags), instead of the individual patterns (instances).
Dietterich et al. called this multi-instance (MI) learning. An-
other well-known MI application is content-based image re-

trieval [Chen and Wang, 2004], where each image is a bag
and each local image patch an instance.

Following Dietterich et al. ’s seminal work, a number of
new MI learning methods, such as the Diverse Density (DD)
algorithm [Maron and Lozano-Perez, 1998], have emerged.
Here, we will focus on methods based on the support vec-
tor machines (SVM), which have been highly successful in
many machine learning problems. There are two main ap-
proaches to extend the standard SVM for MI data. One is to
modify the SVM formulation, as in [Andrews et al., 2003;
Cheung and Kwok, 2006]. However, unlike the standard
SVM, they lead to non-convex optimization problems which
suffer from local minima. Another approach is to design ker-
nels directly on the bags [G¨artner et al., 2002]. These so-
called MI kernels can then be used in a standard SVM. As the
instance labels are unavailable, the MI kernel implicitly as-
sumes all instances in the bag to be equally important. How-
ever, a central assumption in MI learning is that not all the in-
stances in a positive bag are necessarily positive. Thus, many
of these instances should have small contributions and the as-
sumption made by the MI kernel is very crude.

Recall that the major difference between MI learning and
traditional single-instance learning lies in the ambiguity of
the instances labels.
If the instance labels were available,
then the MI problem could be solved much more easily. A
related idea is explored in the EM-DD algorithm [Zhang and
Goldman, 2002]. In each bag, the instance that is most con-
sistent with the current hypothesis is selected as the represen-
tative of the whole bag. This converts the multiple-instance
data to single-instance data and leads to improved speed and
accuracy over the DD algorithm. However, it implicitly as-
sumes that there is only one positive instance in each pos-
itive bag.
In practice, both DD and EM-DD are often in-
ferior to kernel-based MI algorithms [Andrews et al., 2003;
Cheung and Kwok, 2006].

In this paper, we address the ambiguity of instance labels
in the design of MI kernels by using the method of marginal-
ized kernels [Tsuda et al., 2002]. Similar to the Expectation-
Maximization (EM) algorithm, it transforms an incomplete
data problem (with only the observed data) to a complete data
problem (with both the observed and hidden data), which is
then often easier to solve. This method has been successfully
used to deﬁne marginalized kernels for strings, [Tsuda et al.,
2002], trees and graphs [Mah´e et al., 2004].

IJCAI-07

901

The rest of this paper is organized as follows. Section 2
ﬁrst gives a brief introduction to the marginalized kernel. Sec-
tion 3 then describes the proposed marginalized kernel for MI
classiﬁcation, which is followed by an extension to MI regres-
sion in Section 4. Experimental results on a number of clas-
siﬁcation and regression data sets are presented in Section 5,
and the last section gives some concluding remarks.

2 Marginalized Kernels
Like the EM algorithm, the data are assumed to be generated
by a latent variable model, with observed variable x and hid-
den variable θ. The task is to deﬁne a (marginalized) kernel
between two observed variables x1 and x2. With the help
of the hidden variables, one can ﬁrst deﬁne a joint kernel
kz(z1, z2) over the pairs z1 = (x1, θ1) and z2 = (x2, θ2).
As the hidden information is indeed unobserved, the poste-
rior distribution of θ1 ad θ2 are obtained by some probabilistic
model p(θ|x), which in turn is estimated from the data. The

marginalized kernel is then obtained by taking expectation of
the joint kernel w.r.t. the hidden variables, as:
k(x1, x2) =

P (θ1|x1)P (θ2|x2)kz(z1, z2)dθ1 dθ2,

X

(1)

θ1 ,θ2∈Θ

where Θ is the domain of the hidden variables. When the
hidden variable is continuous, the summation is replaced by
an integration. Note that computing (1) may be intractable
when Θ is large, and so this is an important issue in designing
marginalized kernels.

, where cij ∈ {0, 1}.

3 Marginalized Kernels for MI Classiﬁcation
In MI classiﬁcation, we are given a set of training bags
{(B1, y1), . . . , (Bm, ym)}, where Bi = {xi1, . . . , xini} is
the ith bag containing instances xij ’s, and yi ∈ {0, 1}. For
each bag Bi, the observed information is then the associated
xij ’s while the hidden information is the unknown instance
labels ci = [ci1, . . . , cini](cid:2)
3.1 The Joint Kernel
The joint kernel is deﬁned on two combined variables z1 =
(B1, c1) and z2 = (B2, c2). It can thus utilize information
on both the input (xij’s) and instance labels (ci). Note that
while traditional kernels (such as the polynomial and Gaus-
sian kernels) are deﬁned on the input part only, recent results
show that the use of label information can lead to better ker-
nels (e.g, [Cristianini et al., 2002]).

In this paper, we deﬁne the joint kernel as:

n1(cid:2)

n2(cid:2)

kz(z1, z2) =

kc(c1i, c2j)kx(x1i, x2j ),

(2)

where kx(·,·) and kc(·,·) are kernels deﬁned on the input and

i=1

j=1

label parts of the instances, respectively. A simple and rea-
sonable deﬁnition of kc is1
where I(·) returns 1 when the predicate is true, and 0 other-

kc(c1i, c2j) = I(c1i = c2j),

(3)

wise. In this case, (2) is also equal to the alignment between
kernel kx and the instance labels [Cristianini et al., 2002]. A
high alignment thus implies a high kernel value (or similarity)
between B1 and B2.

1Other deﬁnitions are also possible, e.g., one can have

kc(c1i, c2j ) = 1 if c1i = c2j = 1; and 0 otherwise.

(cid:2)

c1,c2

3.2 Marginalizing the Joint Kernel
To obtain the marginalized kernel, we take expectation of the
joint kernel in (2) w.r.t. the hidden variables c1 and c2, as:

k(B1, B2) =

P (c1|B1)P (c2|B2)kz(z1, z2).

(4)

Computation of the conditional probability P (ci|Bi) will be
postponed to Section 3.3. Note that even when P (ci|Bi) is
infeasible (and takes Ω(2n1+n2) time) as ci ∈ {0, 1}ni.

known, a direct computation of (4) is still computationally

be simpliﬁed as:

With the joint kernel deﬁned in (2), k(B1, B2) in (4) can
X

n1X

n2X

P (c1|B1)P (c2|B2)

kc(c1i, c2j )kx(x1i, x2j )

kx(x1i, x2j )

{kc(c1i, c2j )

i=1

j=1

X

c1i,c2j

X

c1,c2

=

n2X
X

j=1

n1X

i=1

·

P (c1i, c1 \ c1i|B1)

P (c2j , c2 \ c2j |B2)},

c1\c1i

c2\c2j

(cid:3)
where ci \ cij = [ci1, . . . , ci,j−1, ci,j+1, . . . , cini ](cid:2)
ci\cij P (cij , ci \ cij|Bi) = P (cij|Bi) and the conditional
independence assumption that P (cij|Bi) = P (cij|xij ),
k(B1, B2) can be reduced to
n1X

. Using

n2X

X

kx(x1i, x2j )kc(c1i, c2j )P (c1i|x1i)P (c2j |x2j ).

(5)

i=1

j=1

c1i ,c2j

As will be shown in Section 3.4, this can now be computed
in polynomial time. In particular, on using (3) as the instance
label kernel, k(B1, B2) is simply

n2(cid:2)
n1(cid:2)
P (c1i = 1|x1i)P (c2j = 1|x2j)kx(x1i, x2j)
n2(cid:2)
n1(cid:2)

P (c1i = 0|x1i)P (c2j = 0|x2j)kx(x1i, x2j ),

j=1

i=1

+

i=1

j=1

√

which is very intuitive. Finally, to avoid undesirable scaling
problems, we normalize the kernel as in [G¨artner et al., 2002]:
k(Bi, Bj) ←
(cid:3)n2
(cid:3)n1
Note that (5) reduces to the MI kernel k(Bi, Bj) =
j=1 kx(x1i, x2j) in [G¨artner et al., 2002] if kc(·,·)
i=1

k(Bj ,Bj ) .

k(Bi,Bj )

k(Bi,Bi)

√

is constant. Thus, while [G¨artner et al., 2002] assumes that
all the instance pairs between Bi and Bj are equally impor-
tant, kc in (5) weights them differently according to the con-
sistency of their probabilistic labels. Moreover, compared to
EM-DD which chooses only one representative instance from
each bag during inference, here we perform marginalization
over all possible instance labels and is thus more consistent
with the Bayesian framework.

3.3 Deﬁning the Conditional Probabilities
In this section, we consider how to obtain the conditional
probability P (ci|Bi). As mentioned in [Tsuda et al., 2002],

an advantage of the marginalized kernel approach is that the
deﬁnitions of the joint kernel and probabilistic model are
completely separated. Thus, one has a lot of freedom in pick-
ing a good probabilistic model.

IJCAI-07

902

Using the Probabilistic Model for Diverse Density
Motivated by the success of the DD algorithm in MI learning
[Maron and Lozano-Perez, 1998], we ﬁrst consider using its
probabilistic model for estimating P (cij|xij ). The DD algo-
rithm ﬁnds a hypothesis h over the whole instance space2 X
1 − e−(cid:2)h−xij (cid:2)2”

1−e−(cid:2)h−xij(cid:2)2”1
AY

by maximizing the following DD function:

0
Y
@1−

DD(h) =

Y

Y

“

“

Algorithm 1 Marginalized MI kernel for classiﬁcation.

Input: Training set D; A pair of bags B1 and B2
Output: k(B1, B2)
1: Run the DD algorithm with different initializations and

store the hypotheses obtained in the array H.

2: for all h ∈ H do

3:

.

Compute P (h|D) using (6) or (11) and store the value
in array Q.

B+
i

xij

−
+
i and B
i

B−

i

xij

Here, B
index all the positive and negative bags
in the training set, respectively. Intuitively, h has a high DD
value if all positive bags have instances close to h, while neg-
ative instances from all the negative bags are far away from
h. Under this model, we can then deﬁne
P (h|D) = DD(h)/Z,
where Z is a normalizing factor such that

(cid:3)
−(cid:3)xij−h(cid:3)2
and P (cij = 0|xij) = 1 − P (cij = 1|xij ).

P (cij = 1|xij) = e

h P (h|D) = 1,

(6)

(7)

However, the DD function is highly nonlinear with many
local minima. Hence, instead of using only one h obtained
from the optimization process, it is often advantageous to use
multiple hypotheses [Chen and Wang, 2004]. We then have3

P (cij|xij ) =

P (h|xij )P (cij|xij , h)
P (h|D)P (cij|xij , h),

(8)

(cid:2)
(cid:2)

h

h

=

where the summation is over the set of hypotheses obtained.
Note that (8) automatically weights each hypothesis by its
likelihood. On the contrary, the DD-SVM in [Chen and
Wang, 2004] has no such weighting and has to rely on addi-
tional heuristics to ﬁlter away the less important hypotheses.

Substituting all these equations into (5), we ﬁnally have

n1X

n2X

X

X

k(B1, B2)=

kx(x1i, x2j )kc(c1i, c2j )

i=1

j=1

h1,h2

c1i ,c2j

P (h1|D)P (h2|D)P (c1i|x1i, h1)P (c2j|x2j , h2). (9)

The complete algorithm is shown in Algorithm 1.

Very recently, [Rahmani and Goldman, 2006] proposed a
graph-based MI semi-supervised learning method, where the
edge weight between bags B1 and B2 is roughly equal to4

n1X

n2X

DD(x1i) + DD(x2j )

i=1

j=1

2

kx(x1i, x2j ).

(10)

This is quite similar to (9). However, in general, (10) is not
positive semi-deﬁnite and so not a valid kernel function.

2Searching over a large X can be prohibitive. To be computa-
tionally efﬁcient, one often performs a gradient-based optimization
of the DD function by initializing from every instance in every pos-
itive bag [Chen and Wang, 2004; Maron and Lozano-Perez, 1998].
3Note that all the probabilities here are implicitly conditioned
on the training data D. For clarity, in (8) we write P (h|D) (the
posterior probability of h given the data), instead of P (h).

4In [Rahmani and Goldman, 2006], the DD values in (10) are
ﬁrst normalized, nonlinearly transformed and then ﬁltered. More-
over, this weight is deﬁned on positive bags only.

4: end for

5: for all xij ∈ B1 ∪ B2 do

Compute P (cij|xij ) using (7) and (8).

6:
7: end for

8: Compute kx(x1i, x2j) for all instance pairs in B1 × B2.
9: Compute k(B1, B2) as in (9), using the pre-computed

values in the array Q, kx and P (cij|xij ).

Using the Training Accuracy to Improve P (h|D)

As mentioned earlier, the DD algorithm has been very suc-
cessful. Note that one only has to use the hypotheses, but not
their DD values, on classiﬁcation. Recall that the obtained
hypotheses are local minima of the DD function. While many
of them may have comparable classiﬁcation accuracies, as
will be demonstrated in Section 5, a few hypotheses can have
DD values that are much higher than the others. This is not
surprising as DD value may drop signiﬁcantly even if the hy-
pothesis is close to only one negative instance. Consequently,
the summation in (8) is often dominated by a few hypotheses.
To alleviate this problem while still retaining the merits of

DD, we will instead deﬁne P (h|D) of each DD hypothesis
to be proportional to its training accuracy. Let predh(Bi)
be the label predicted by h on Bi, which is equal to 1 if
maxxij∈Bi e

−(cid:3)xij−h(cid:3)2 ≥ 0.5, and 0 otherwise. Then,
P (h|D) =

I(predh(Bi) = yi)/Z,

m(cid:2)

(11)

i=1

where m is the number of training bags and Z is again for
normalization. Its superiority over the deﬁnition in (6) will
be experimentally veriﬁed in Section 5.

Because of the modular design of the marginalized kernel,

we can easily plug in other good estimators of P (cij|xij ).

For example, an EM-based approach may also be used.

3.4 Time Complexity
Let Nh be the number of DD hypotheses. In Algorithm 1,
computing all the P (cij|xij )’s at Step 6 takes O((n1 +
n2)Nhd) time (assuming that P (h|D)’s can be obtained in
constant time or have been pre-computed), where d is the data
dimensionality. Assuming that each evaluation of the kernel
kx takes O(d) time, then computing all the kx(·,·)’s in Step 8
takes O(n1n2d) time. The summation involved in k(B1, B2)
of Step 9 takes O(N 2
hn1n2) time (as cij only takes 0 or 1).
Thus, the total time complexity for computing k(B1, B2) is
O((N 2

h + d)n1n2 + (n1 + n2)Nhd), which is polynomial.

4 Marginalized Kernels for MI Regression
In regression, we assume that each bag output yi is normal-
ized to the range [0, 1]. Subsequently, each (hidden) instance

IJCAI-07

903

(cid:4)

label cij is also in [0, 1]. In this real-valued setting, we follow
the extended DD model in [Amar et al., 2001], and have:

P (cij|xij ) =

1 − |cij − e
(cid:6) 1
0 P (cij|xij )dcij = 1. It can be

/Z(xij ),

(12)

where Z(xij ) ensures that
easily shown that Z(xij ) = 0.75 − (e

−(cid:3)h−xij(cid:3)2 − 0.5)2

.

(cid:5)
−(cid:3)h−xij(cid:3)2|

Proceeding as in Section 3, we employ the same joint ker-
nel in (2) (but with summations of c1i and c2j replaced by in-
tegrations over [0, 1]) in the marginalized kernel k(B1, B2) of
(5). Again, this can then avoid integrating over an (n1 + n2)-
dimensional space.

The probability that two instances share the same real-
valued label is zero, and so (3) is a poor measure of in-
stance label similarity. Intuitively, the larger the difference
between c1i and c2j, the smaller is kc(c1i, c2j). Noting that
|c1i − c2j| ∈ [0, 1], we have two natural choices for kc (Fig-
ure 1)5:

(linear) kc(c1i, c2j) = (1 − |c1i − c2j|),
(cid:7)−β(c1i − c2j)2

(Gaussian) kc(c1i, c2j) = exp

(cid:8)

.

(13)

(14)

)

j

2

c
,

i

1

c
(

k

c

1

0.8

0.6

0.4

0.2

 

0
0

0.2

exp(- β(c
1i

-c

)2)
2j

 

1-|c

-c

1i

|
2j

0.6
|
2j

0.8

1

0.4

|c

-c

1i

Figure 1: Two possible deﬁnitions of the label kernel
kc(c1i, c2j) in the regression setting.

Putting all these together, and after long but straightfor-
ward calculations, it can be shown that the marginalized MI
kernel between bags B1 and B2 is given by:

k(B1, B2) =

n1(cid:2)

n2(cid:2)

i=1

j=1

k(x1i, x2j )G(e

−(cid:3)h−x1i(cid:3)2

, e
Z(x1i)Z(x2j )

−(cid:3)h−x2j(cid:3)2))

,

where G(u, v) is equal to
Z

Z

1

1

(1 − |c1i − c2j |) (1 − |c1i − u|)(1 − |c2j − v|)dc1i dc2j , (15)

0

0

Z

Z

1

`

1

exp

when (13) is used; and is equal to

−β(c1i −c2j )2

´
(1−|c1i − u|)(1−|c2j − v|)dc1i dc2j , (16)

0

0

when (14) is used. Closed form expressions for these two
integrals can be found in the Appendix. Moreover, as in clas-
siﬁcation, both deﬁnitions of k(B1, B2) can be computed in
polynomial time.

5In the experiments, we set β such that kc(c1i, c2j ) = 0.0001

when |c1i − c2j | = 1.

ilarly generalized to:

When multiple hypotheses are used, k(B1, B2) can be sim-
X

P (h1|D)P (h2|D)k(x1i, x2j )G(e−(cid:3)h1−x1i(cid:3)2

, e−(cid:2)h2−x2j (cid:2)2

i,j,h1 ,h2

As for the probability P (h|D), again one can follow the ex-

Z(x1i)Z(x2j )

)

.

tended DD model and obtain:

P (h|D) =

(cid:10)
1 − |yi − max
xij∈Bi

e

(cid:11)
−(cid:3)h−xij(cid:3)2|

/Z.

(cid:9)

Bi

Alternatively, as in (11), we can also use the train-
Following [Dooly et
ing accuracy of each hypothesis.
al., 2002], we use maxxij∈Bi e
as h’s pre-
dicted output of bag Bi. The error of h is then eh =

−(cid:3)h−xij(cid:3)2

(cid:4)

(cid:3)m

i=1

yi − maxxij∈Bi e

(cid:10)
1 −

P (h|D) =

(cid:5)2
−(cid:3)h−xij(cid:3)2
eh − minh(cid:2) eh(cid:2)

, and we can deﬁne

(cid:11)

maxh(cid:2) eh(cid:2) − minh(cid:2) eh(cid:2)

/Z.

(17)

5 Experiments
In this section, we compare the performance of the proposed
marginalized kernels with the MI kernel in [G¨artner et al.,
2002] on a number of MI data sets. As for the kernel deﬁned
on the input parts, we use the Gaussian kernel kx(x1i, x2j) =
exp (−γ (cid:7)x1i − x2j(cid:7)2), where γ is the width parameter.
5.1 MI Classiﬁcation
For simplicity, the marginalized kernels deﬁned in (6) and
(11) are denoted by MG-DDV and MG-ACC, respectively.

Drug Activity Prediction
The ﬁrst experiment is performed on the popular Musk1 and
Musk2 data sets6. The task is to predict if a drug is musky.
As mentioned in Section 1, each drug is considered a bag,
and each of its low-energy conformations an instance. We
use 10-fold cross-validation. In each fold, the DD algorithm
is applied only on the training data and so the DD hypotheses
cannot capture any information in the test set. For compari-
son, the DD-SVM [Chen and Wang, 2004] is also run.

As can be seen from Table 1, the MG-ACC kernel always
performs better than the MI kernel. It also outperforms the
DD-SVM, which is also using multiple DD hypotheses that
are assumed to be equally important. This thus demonstrates
the importance of weighting instance pairs based on the con-
sistency of their underlying labels (Section 3.2).

Table 1: Testing accuracies (%) on the Musk data sets.

Musk1 Musk2

DD-SVM
MI kernel

MG-DDV kernel
MG-ACC kernel

89.0
89.3
87.8
90.1

87.9
87.5
88.6
90.4

Moreover, the MG-ACC kernel is better than the MG-DDV

kernel. Figure 2(a) compares their corresponding P (h|D)

6

ftp://ftp.ics.uci.edu/pub/machine-learning-databases/musk/

IJCAI-07

904

values ((6) for MG-DDV and (11) for MG-ACC) for all the
DD hypotheses obtained from a typical fold of the Musk1
data. As can be seen, for the MG-DDV kernel, a small subset

(Figure 2(b)), and hence more hypotheses can be utilized in

is dominated by these few hypotheses. On the other hand, for

of P (h|D) values are very high, and so the summation in (8)
the MG-ACC kernel, the P (h|D) values vary much gradually
computing P (cij|xij ). Moreover, recall that P (h|D) in (6) is
proportional to h’s DD value, while the one in (11) is propor-
tional to h’s training accuracy. Hence, the almost linear trend
in Figure 2(b) shows that the higher the DD value, the higher
is the training accuracy of the hypothesis. It thus provides
another evidence for the success of the DD algorithm.

)

|

D
h
(
P

0.2

0.15

0.1

0.05

 

0
0

50

 

MG-DDV
MG-ACC

x 10-3

)

|

D
h
(
P

6

4

0
0

150

1

0.9

0.8

0.7

y
c
a
r
u
c
c
a
g
n
n
a
r
t

 

i

i

100

DD hypothesis

0.6

50
50

100
100

DD hypothesis

150
150

200
200

(a) P (h|D) values for
the
MG-DDV and MG-ACC ker-
nels.

(b) A closer
look on the
P (h|D) values for the MG-
ACC kernel.
Figure 2: P (h|D) values obtained from a typical fold of the
Musk1 data. The x-axis shows the different hypotheses ob-
tained by the DD algorithm, sorted in increasing P (h|D) val-

ues as obtained for the MG-ACC kernel.

Image Categorization
The second experiment is on an image categorization task us-
ing the data set7 in [Chen and Wang, 2004]. There are 10
classes (beach, ﬂowers, horses, etc.), with each class contain-
ing 100 images. Each image is regarded as a bag, and each
segment an instance. We follow exactly the same setup as
[Chen and Wang, 2004]. The data is randomly divided into a
training and test set, each containing 50 images of each cat-
egory. Since this is a multi-class classiﬁcation problem, we
employ the standard one-vs-rest approach to convert it to a
number of binary classiﬁcation problems. The experiment is
repeated 5 times, and the average testing accuracy reported.

Table 2 shows the results, along with those of the DD-
SVM, Hist-SVM [Chapelle et al., 1999] and MI-SVM as re-
ported in [Chen and Wang, 2004]. As can be seen, the MG-
ACC kernel is again superior to the MI kernel and others. To
ensure that the improvement over the MI kernel is statisti-
cally signiﬁcant, we repeat the experiment 50 times (instead
of only 5). The difference is conﬁrmed to be signiﬁcant at the
0.05 level of signiﬁcance by using the paired t-test.

5.2 MI Regression: Synthetic Musk Molecules

We perform MI regression on the data set used in [Dooly et
al., 2002], where the goal is to predict the binding energies
of the musk molecules. We use three data sets (LJ-16.30.2,
LJ-80.166.1 and LJ-160.166.1) downloaded from the author’s

7

http://www.cs.uno.edu/∼yixin/ddsvm.html

Table 2: Testing accuracies (%) on the image data set.

DD-SVM
Hist-SVM
MI-SVM
MI kernel

MG-DDV kernel
MG-ACC kernel

5 repetitions
81.50 ± 3.00
66.70 ± 2.20
74.70 ± 0.60
84.12 ± 0.90
76.52 ± 17.29
84.64 ± 1.28

50 repetitions

-
-
-

84.12 ± 1.22

-

84.54 ± 1.21

website8, and three more9 (LJ-16.50.2, LJ-80.206.1 and LJ-
160.566.1) used in a recent study [Cheung and Kwok, 2006].
The latter were obtained by adding irrelevant features to the
former data sets, while keeping its real-valued outputs intact.
As demonstrated in Section 5.1, the MG-ACC kernel is su-
perior than the MG-DDV kernel. Hence, we will only exper-
iment with the MG-ACC kernel deﬁned with (17) here. As
in [Dooly et al., 2002], we report both the percentage error
(%err)10 and mean squared error (MSE).

Table 3 shows the results, along with those of DD, EM-
DD and citation-kNN [Wang and Zucker, 2000] as reported
in [Cheung and Kwok, 2006]. As can be seen, the proposed
MG-ACC kernel with the Gaussian instance label kernel kc in
(14) consistently outperforms the others. Its superiority over
the MG-ACC kernel (with a linearly decaying instance label
kernel) indicates that a much smaller weight should be as-
signed to instance pairs whose labels are quite different (Fig-
ure 1). This also explains the relatively inferior performance
of the MI kernel, which weights all instance pairs equally.

6 Conclusion

In this paper, we show how to design marginalized kernels in
multiple-instance learning. First, we pretend that all the hid-
den instance labels are known, and deﬁne a joint kernel using
both the instance inputs and labels. By integrating out the
hidden data, the marginalized kernel is obtained. This kernel
differs from the existing MI kernel in that different instance
pairs are weighted by the consistency of their probabilistic
labels. Experimentally, this marginalized MI kernel always
performs better than the MI kernel and also often outperforms
other traditional MI learning methods.

Note that the proposed kernel can be straightforwardly
used in the regularization framework recently proposed in
[Cheung and Kwok, 2006]. Moreover, because of the mod-
ularity of the marginalized kernel, we will also explore even
better deﬁnitions of the joint kernel and probabilistic model.

Acknowledgments

This research has been partially supported by the Research
Grants Council of the Hong Kong Special Administrative Re-
gion.

8

http://www.cs.wustl.edu/∼sg/multi-inst-data. There are four more data
sets available. However, they are easier variations of the three that
we used, and so are dropped in this study.

http://www.cs.ust.hk/∼jamesk/papers/icml06 data.zip
10%err is the classiﬁcation error obtained by thresholding both the

9

target output and the predicted output at 0.5.

IJCAI-07

905

Table 3: Performance of MI regression on the synthetic afﬁnity data sets.

data set

LJ-16.30.2
LJ-80.166.1
LJ-160.166.1
LJ-16.50.2
LJ-80.206.1
LJ-160.566.1

DD

%err MSE
0.0240
6.7
(not available)
23.9
0.0852

-
-
-

-
-
-

EM-DD

%err MSE
0.0153
6.7
0.1825
37.0
21.7
0.0850
0.2357
40.0
0.2449
37.0
37.0
0.2414

citation-kNN
%err MSE
0.0260
16.7
0.0109
8.6
4.3
0.0014
0.0916
53.3
0.0463
30.4
34.8
0.0566

SVM

(MI kernel)
%err MSE
0.0197
8.3
0.0121
7.6
0.0053
0.0
0.0202
10.0
0.0110
6.5
1.1
0.0056

SVM (MG-ACC kernel)

linear (13)

%err MSE
0.0197
8.3
0.0119
7.6
0.0052
0.0
0.0202
10.0
0.0098
6.0
0.5
0.0050

Gaussian (14)
%err MSE
0.0186
8.3
0.0102
6.5
0.0044
0.0
0.0191
6.7
0.0089
5.4
0.0
0.0046

A Appendix

Using Mathematica, it can be shown that (16),

G(u, v) =

+

1
12β2 (2+g0(u, v)+g1(u, v)+g1(v, u)
√
24β3/2 (g5(u, v)+g5(v, u)+g6(u, v)+g6(v, u)

+g2(u, v)+g2(v, u)+g3(u, v)+g4(u, v))

π

+g7(u, v)+g8(u, v)) ,

where

2),

g0(u, v)=4(β(u − v)2 + 1) exp (−β(u − v)2),
g1(u, v)=(β(1−u)(1+2u−6v)−2) exp (−β(1 − u)2),
g2(u, v)=(βu(6v − 2u − 3) − 2) exp (−βu
g3(u, v)=(β(6u + 6v − 12uv + 8) + 2) exp (−β),
(cid:4)(cid:12)
g4(u, v)=6β(u + v − 2uv − 2),
(cid:8)
(cid:4)(cid:12)
(cid:5)
g5(u, v)=
,
β(1−u)
g6(u, v)=(2β(1+2u−6v)(1−u)2+6(u−v)−3)erf
(cid:5)
β(u − v)
(cid:5)
g7(u, v)=

2 + 1)(6v − 2u − 3) − 4u
(cid:4)(cid:12)
(u − v)erf
(cid:4)(cid:12)

(cid:7)
(2βu
(cid:7)
8β(u − v)2 + 12
(cid:7)
12β(u − y)2 + 16β + 6

β

,

(cid:5)

βu

(cid:8)

(cid:8)

g8(u, v)=
and erf(u) = 2√
tion. Similarly, (15) can be shown to be

(cid:6) u

erf

π

0

exp (−t2)dt is the so-called error func-

,

,

erf

G(u, v) =

−1
30
− 1
6

|u − v|5 +
1
6
(u + v)3 − 1
3

(u − v)4 +
(u − v)2 +

2 + v

1
3 uv(u
1
(u + v) +
3

2)

11
60 .

References
[Amar et al., 2001] R.A. Amar, D.R. Dooly, S.A. Goldman, and
Q. Zhang. Multiple-instance learning of real-valued data. In Pro-
ceedings of the Eighteenth International Conference on Machine
Learning, pages 3–10, Williamstown, MA, USA, 2001.

[Andrews et al., 2003] S. Andrews, I. Tsochantaridis, and T. Hof-
mann. Support vector machines for multiple-instance learning.
In S. Becker, S. Thrun, and K. Obermayer, editors, Advances
in Neural Information Processing Systems 15, Cambridge, MA,
2003. MIT Press.

[Chapelle et al., 1999] O. Chapelle, P. Haffner, and V.N. Vapnik.
Support vector machines for histogram-based image classiﬁca-
tion. IEEE Transactions on Neural Networks, 10(5):1055–1064,
September 1999.

[Chen and Wang, 2004] Y. Chen and J.Z. Wang. Image categoriza-
tion by learning and reasoning with regions. Journal of Machine
Learning Research, 5:913–939, 2004.

[Cheung and Kwok, 2006] P.M. Cheung and J.T. Kwok. A regular-
ization framework for multiple-instance learning. In Proceedings
of the Twenty-Third International Conference on Machine Learn-
ing, pages 193–200, Pittsburgh, USA, June 2006.

[Cristianini et al., 2002] N. Cristianini, J. Shawe-Taylor, A. Elisse-
eff, and J. Kandola. On kernel-target alignment. In T.G. Diet-
terich, S. Becker, and Z. Ghahramani, editors, Advances in Neu-
ral Information Processing Systems 14, Cambridge, MA, 2002.
MIT Press.

[Dietterich et al., 1997] T.G. Dietterich, R.H. Lathrop,

and
T. Lozano-Perez. Solving the multiple instance problem with
axis-parallel rectangles. Artiﬁcial Intelligence, 89:31–71, 1997.

[Dooly et al., 2002] D.R. Dooly, Q. Zhang, S.A. Goldman, and
R.A. Amar. Multiple-instance learning of real-valued data. Jour-
nal of Machine Learning Research, 3:651–678, 2002.

[G¨artner et al., 2002] T. G¨artner, P.A. Flach, A. Kowalczyk, and
A.J. Smola. Multi-instance kernels. In Proceedings of the Nine-
teenth International Conference on Machine Learning, pages
179–186, Sydney, Australia, July 2002.

[Mah´e et al., 2004] P. Mah´e, N. Ueda, T. Akutsu, J.L. Perret, and
J.P. Vert. Extensions of marginalized graph kernels. In Proceed-
ings of the Twenty-First International Conference on Machine
Learning, pages 552–559, Banff, Alberta, Canada, July 2004.

[Maron and Lozano-Perez, 1998] O. Maron and T. Lozano-Perez.
A framework for multiple-instance learning. In M.I. Jordan, M.J.
Kearns, and S.A. Solla, editors, Advances in Neural Informa-
tion Processing Systems 10. Morgan Kaufmann, San Mateo, CA,
1998.

[Rahmani and Goldman, 2006] R. Rahmani and S.A. Goldman.
MISSL: Multiple-instance semi-supervised learning. In Proceed-
ings of the Twenty-Third International Conference on Machine
Learning, pages 705–712, Pittsburgh, PA, USA, 2006.

[Tsuda et al., 2002] K. Tsuda, T. Kin, and K. Asai. Marginalized
kernels for biological sequences. Bioinformatics, 18:S268–S275,
2002.

[Wang and Zucker, 2000] J. Wang and J.-D. Zucker.

Solving
multiple-instance problem: A lazy learning approach.
In Pro-
ceedings of the Seventeenth International Conference on Ma-
chine Learning, pages 1119–1125, Stanford, CA, USA, 2000.

[Zhang and Goldman, 2002] Q. Zhang and S.A. Goldman. EM-
DD: An improved multiple-instance learning. In T.G. Dietterich,
S. Becker, and Z. Ghahramani, editors, Advances in Neural In-
formation Processing Systems 14, Cambridge, MA, 2002. MIT
Press.

IJCAI-07

906


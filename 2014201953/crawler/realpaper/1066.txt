Fast Incremental Square Root Information Smoothing∗

Michael Kaess, Ananth Ranganathan, and Frank Dellaert

Center for Robotics and Intelligent Machines, College of Computing

Georgia Institute of Technology, Atlanta, GA 30332

{kaess,ananth,dellaert}@cc.gatech.edu

Abstract

We propose a novel approach to the problem of
simultaneous localization and mapping (SLAM)
based on incremental smoothing, that is suitable for
real-time applications in large-scale environments.
The main advantages over ﬁlter-based algorithms
are that we solve the full SLAM problem without
the need for any approximations, and that we do
not suffer from linearization errors. We achieve
efﬁciency by updating the square-root information
matrix, a factored version of the naturally sparse
smoothing information matrix. We can efﬁciently
recover the exact trajectory and map at any given
time by back-substitution. Furthermore, our ap-
proach allows access to the exact covariances, as it
does not suffer from under-estimation of uncertain-
ties, which is another problem inherent to ﬁlters.
We present simulation-based results for the linear
case, showing constant time updates for exploration
tasks. We further evaluate the behavior in the pres-
ence of loops, and discuss how our approach ex-
tends to the non-linear case. Finally, we evalu-
ate the overall non-linear algorithm on the standard
Victoria Park data set.

1 Introduction
The problem of simultaneous localization and mapping
(SLAM) has received considerable attention in mobile
robotics, as it is one way to enable a robot to explore and nav-
igate previously unknown environments. For a wide range of
applications, such as commercial robotics, search-and-rescue
and reconnaissance, the solution must be incremental and
real-time in order to be useful. The problem consists of two
components: a discrete correspondence and a continuous es-
timation problem. In this paper we focus on the continuous
part, while keeping in mind that the two components are not
completely independent, as information from the estimation
part can simplify the correspondence problem.

∗This work was supported in part by the National Science Foun-
dation award IIS - 0448111 “CAREER: Markov Chain Monte Carlo
Methods for Large Scale Correspondence Problems in Computer Vi-
sion and Robotics”.

Figure 1: Updating the upper triangular factor R and the right hand
side (rhs) with new measurement rows. The left column shows the
ﬁrst three updates, the right column shows the update after 50 steps.
The update operation is symbolically denoted by ⊕, and entries that
remain unchanged are shown in light blue.

Current real-time approaches to SLAM are typically based
on ﬁltering. While ﬁlters work well for linear problems, they
are not suitable for most real-world problems, which are in-
herently non-linear [Julier and Uhlmann, 2001]. The reason
for this is well known: Marginalization over previous poses
bakes linearization errors into the system in a way that can-
not be undone, leading to unbounded errors for large-scale
applications. But repeated marginalization also complicates
the problem by making the naturally sparse dependencies be-
tween poses and landmarks, which are summarized in the
information matrix, dense. Sparse extended information ﬁl-
ters [Thrun et al., 2005] and thin junction tree ﬁlters [Paskin,
2003] present approximations to deal with this complexity.

Smoothing approaches to SLAM recover the complete
robot trajectory and the map, avoiding the problems inher-
ent to ﬁlters. In particular, the information matrix is sparse
and remains sparse over time, without the need for any ap-
proximations. Even though the number of variables increases
continuously for smoothing, in many real-world scenarios,
especially those involving exploration, the information ma-
trix still contains far less entries than in ﬁlter based methods
[Dellaert, 2005]. When repeatedly observing a small number

IJCAI-07

2129

of landmarks, ﬁlters seem to have an advantage, at least when
ignoring the linearization issues. However, the correct way of
dealing with this situation is to switch to localization after a
map of sufﬁcient quality is obtained.

In this paper, we present a fast incremental smoothing ap-
proach to the SLAM problem. We revisit the underlying
probabilistic formulation of the smoothing problem and its
equivalent non-linear optimization formulation in Section 2.
The basis of our work is a factorization of the smoothing in-
formation matrix, that is directly suitable for performing op-
timization steps. Rather than recalculating this factorization
in each step from the measurement equations, we incremen-
tally update the factorized system whenever new measure-
ments become available, as described in Section 3. This fully
exploits the natural sparsity of the SLAM problem, resulting
in constant time updates for exploration tasks. We apply vari-
able reordering similar to [Dellaert, 2005] in order to keep the
factored information matrix sparse even when closing multi-
ple loops. All linearization choices can be corrected at any
time, as no variables are marginalized out. From our fac-
tored representation, we can apply back-substitution at any
time to obtain the complete map and trajectory in linear time.
We evaluate the incremental algorithm based on the standard
Victoria Park dataset in Section 4.

As our approach is incremental, it is suitable for real-time
applications. In contrast, many other smoothing approaches
like GraphSLAM [Thrun et al., 2005] perform batch process-
ing, solving the complete problem in each step. Even though
explicitly exploiting the structure of the SLAM problem can
yield quite efﬁcient batch algorithms [Dellaert, 2005], for
large-scale applications an incremental solution is needed.
Examples of such systems that incrementally add new mea-
surements are Graphical SLAM [Folkesson and Christensen,
2004] and multi-level relaxation [Frese et al., 2005]. How-
ever, these and similar solutions have no efﬁcient way of ob-
taining the marginal covariances needed for data association.
In Section 5 we describe how our approach allows access to
the exact marginal covariances without a full matrix inver-
sion, and how to obtain a more efﬁcient conservative estimate.

Figure 2: Bayesian belief network representation of the SLAM prob-
lem, where x is the state of the robot, l the landmark locations, u the
control input and z the measurements.

where P (x0) is a prior on the initial state, P (xi|xi−1, ui) is
the motion model, parametrized by the control input ui, and
P (zk|xik
, ljk ) is the landmark measurement model, assuming
known correspondences (ik, jk) for each measurement zk.

As is standard in the SLAM literature, we assume Gaus-
sian process and measurement models. The process model
xi = fi(xi−1, ui) + wi describes the robot behavior in re-
sponse to control input, where wi is normally distributed
zero-mean process noise with covariance matrix Λi. The
Gaussian measurement equation zk = hk(xik
, ljk )+ vk mod-
els the robot’s sensors, where vk is normally distributed zero-
mean measurement noise with covariance Σk.

2.2 SLAM as a Least Squares Problem
In this section we discuss how to obtain an optimal estimate
for the set of unknowns given all measurements available to
us. As we perform smoothing rather than ﬁltering, we are
interested in the maximum a posterior (MAP) estimate for
the entire trajectory X = {xi} and the map of landmarks
L = {lj}, given the measurements Z = {zk} and the control
inputs U = {ui}. Let us collect all unknowns in X and L
in the vector Θ = (X, L). The MAP estimate Θ∗ is then ob-
tained by minimizing the negative log of the joint probability
P (X, L, Z) from equation (1):

Θ∗ = arg min

Θ

− log P (X, L, Z)

(2)

Combined with the process and measurement models, this
leads to the following non-linear least-squares problem

2 SLAM and Smoothing
In this section we review the formulation of the SLAM
problem in a smoothing framework, following the notation
of [Dellaert, 2005].
In contrast to ﬁltering methods, no
marginalization is performed, and all pose variables are re-
tained. We describe the underlying probabilistic model of this
full SLAM problem, and show how inference on this model
leads to a least-squares problem.
2.1 A Probabilistic Model for SLAM
We formulate the SLAM problem in terms of the belief net-
work model shown in Figure 2. We denote the robot state at
the ith time step by xi, with i ∈ 0 . . . M , a landmark by lj,
with j ∈ 1 . . . N, and a measurement by zk, with k ∈ 1 . . . K.
The joint probability is given by

M(cid:2)

K(cid:2)

P (X, L, Z) = P (x0)

P (xi|xi−1, ui)

i=1

k=1

P (zk|xik

, ljk )

(1)

IJCAI-07

2130

(cid:3)
M(cid:4)
K(cid:4)

i=1

+

k=1

Θ∗ = arg min

Θ

||fi(xi−1, ui) − xi||2
Λi

(cid:5)

||hk(xik

, ljk ) − zk||2
Σk

(3)

where we use the notation ||e||2
Mahalanobis distance given a covariance matrix Σ.

Σ = eT Σ−1e for the squared

In practice one always considers a linearized version of this
problem. If the process models fi and measurement equa-
tions hk are non-linear and a good linearization point is not
available, non-linear optimization methods solve a succession
of linear approximations to this equation in order to approach
the minimum. We therefore linearize the least-squares prob-
lem by assuming that either a good linearization point is avail-
able or that we are working on one iteration of a non-linear

(cid:3)
M(cid:4)
K(cid:4)

i=1

+

k=1

Figure 3: Using a Givens rotation to transform a matrix into upper
triangular form. The entry marked ’x’ is eliminated, changing some
or all of the entries marked in red (dark), depending on sparseness.

optimization method, see [Dellaert, 2005] for the derivation:

δΘ∗ = arg min
δΘ

||F i−1

i

δxi−1 + Gi

iδxi − ai||2
Λi

(cid:5)

||H ik

k δxik + J jk

k δljk

− ck||2
Σk

(4)

k , J jk

where H ik
k are the Jacobians of hk with respect to a
change in xik and ljk respectively, F i−1
the Jacobian of fi at
xi−1, and Gi
i = I for symmetry. ai and ck are the odometry
and observation measurement prediction errors, respectively.
We combine all summands into one least-squares problem
after dropping the covariance matrices Λi and Σk by pulling
their matrix square roots inside the Mahalanobis norms:

i

(cid:6)

(cid:7)T (cid:6)

||e||2

Σ = eT

Σ−1e =

Σ−T /2e

Σ−T /2e

= ||Σ−T /2e||2
(5)
For scalar matrices this simply means dividing each term by
the measurement standard deviation. We then collect all Ja-
cobian matrices into a single matrix A, the vectors ai and ck
into a right hand side vector b, and all variables into the vector
θ, to obtain the following standard least-squares problem:

(cid:7)

θ∗ = arg min
θ

||Aθ − b||2

(6)

3 Updating a Factored Representation
We present an incremental solution to the full SLAM problem
based on updating a matrix factorization of the measurement
Jacobian A from (6). For simplicity we initially only consider
the case of linear process and measurement models. In the
linear case, the measurement Jacobian A is independent of
the current estimate θ. We therefore obtain the least squares
solution θ∗ in (6) directly from the standard QR matrix factor-
m×n:
ization [Golub and Loan, 1996] of the Jacobian A ∈ R

(cid:8)

(cid:9)

R
0

A = Q

(7)

m×m is orthogonal, and R ∈ R

n×n is upper
where Q ∈ R
triangular. Note that the information matrix I Δ
= AT A can
also be expressed in terms of this factor R as I = RT R,
which we therefore call the square-root information matrix
R. We rewrite the least-squares problem, noting that multi-
plying with the orthogonal matrix QT does not change the
norm:

(cid:8)

(cid:9)

(cid:8)

(cid:9)

(cid:8)

(cid:9)

||Q

θ − b||2 = ||

R
0

R
0

θ −

||2

d
e

= ||Rθ − d||2 + ||e||2

(8)

Figure 4: Complexity of a simulated linear exploration task, show-
ing 10-step averages. In each step, the square-root factor is updated
by adding the new measurement rows by Givens rotations. The av-
erage number of rotations (red) remains constant, and therefore also
the average number of entries per matrix row. The execution time
per step (dashed green) slightly increases for our implementation
due to tree representations of the underlying data structures.

where we deﬁned [d, e]T Δ
= QT b. The ﬁrst term ||Rθ − d||2
vanishes for the least-squares solution θ∗, leaving the second
term ||e||2 as the residual of the least-squares problem.

The square-root factor allows us to efﬁciently recover the
complete robot trajectory as well as the map at any given
time. This is simply achieved by back-substitution using the
current factor R and right hand side (rhs) d to obtain an up-
date for all model variables θ based on

Rθ = d

(9)

Note that this is an efﬁcient operation for sparse R, under
the assumption of a (nearly) constant average number of en-
tries per row in R. We have observed that this is a reasonable
assumption even for loopy environments. Each variable is
then obtained in constant time, yielding O(n) time complex-
ity in the number n of variables that make up the map and
trajectory. We can also restrict the calculation to a subset of
variables by stopping the back-substitution process when all
variables of interest are obtained. This constant time opera-
tion is typically sufﬁcient unless loops are closed.

3.1 Givens Rotations
One standard way to obtain the QR factorization of the mea-
surement Jacobian A is by using Givens rotations [Golub and
Loan, 1996] to clean out all entries below the diagonal, one
at a time. As we will see later, this approach readily extends
to factorization updates, as will be needed to incorporate new
measurements. The process starts from the bottom left-most
non-zero entry, and proceeds either column- or row-wise, by
applying the Givens rotation:

(cid:8)

(cid:9)

cos φ
sin φ
− sin φ cos φ

(10)

to rows i and k, with i > k. The parameter φ is chosen
so that the (i, k) entry of A becomes 0, as shown in Figure
3. After all entries below the diagonal are zeroed out, the
upper triangular entries will contain the R factor. Note that
a sparse measurement Jacobian A will result in a sparse R
factor. However, the orthogonal rotation matrix Q is typically
dense, which is why this matrix is never explicitly stored or

IJCAI-07

2131

even formed in practice. Instead, it is sufﬁcient to update the
rhs b with the same rotations that are applied to A.

When a new measurement arrives, it is cheaper to update
the current factorization than to factorize the complete mea-
surement Jacobian A. Adding a new measurement row wT
and rhs γ into the current factor R and rhs d yields a new
system that is not in the correct factorized form:

(cid:9)(cid:8)

(cid:9)

(cid:8)

(cid:8)

(cid:9)

(cid:8)

QT

1

(cid:9)

A
wT

=

R
wT

, new rhs:

d
γ

(11)

(a) Simulated double 8-loop at interesting stages of loop closing (for
simplicity only a quarter of the poses and landmarks are shown).

Note that this is the same system that is obtained by applying
Givens rotations to eliminate all entries below the diagonal,
except for the last (new) row. Therefore Givens rotations can
be determined that zero out this new row, yielding the updated
factor R(cid:3). In the same way as for the full factorization, we
simultaneously update the right hand side with the same rota-
tions to obtain d(cid:3). In general, the maximum number of Givens
rotations needed to add a new measurement is n. However, as
R and the new measurement rows are sparse, only a constant
number of Givens rotations are needed. Furthermore, new
measurements typically refer to recently added variables, so
that often only the right most part of the new measurement
row is (sparsely) populated. An example of the locality of the
update process is shown in Figure 1.

It is easy to add new landmark and pose variables to the
QR factorization, as we can just expand the factor R by the
appropriate number of zero columns and rows, before updat-
ing with the new measurement rows. Similarly, the rhs d is
augmented by the same number of zero entries.

For an exploration task in the linear case, the number of
rotations needed to incorporate a set of new landmark and
odometry measurements is independent of the size of the tra-
jectory and map, as the simulation results in Figure 4 show.
Our algorithm therefore has O(1) time complexity for explo-
ration tasks. Recovering all variables after each step requires
O(n) time, but is still very efﬁcient even after 10000 steps, at
about 0.12 seconds per step. However, only the most recent
variables change signiﬁcantly enough to warrant recalcula-
tion, providing a good approximation in constant time.

(b) Cholesky factor R.

(c) Cholesky factor after
variable reordering.

3.2 Loops and Variable Reordering
Loops can be dealt with by a periodic reordering of variables.
A loop is a cycle in the trajectory that brings the robot back
to a previously visited location. This introduces correlations
between current poses and previously observed landmarks,
which themselves are connected to earlier parts of the trajec-
tory. Results based on a simulated environment with multiple
loops are shown in Figure 5. Even though the information
matrix remains sparse in the process, the incremental updat-
ing of the factor R leads to ﬁll-in. This ﬁll-in is local and does
not affect further exploration, as is evident from the example.
However, this ﬁll-in can be avoided by variable reordering, as
has been described in [Dellaert, 2005]. The same factor R af-
ter reordering shows no signs of ﬁll-in. However, reordering
of the variables and subsequent factorization of the new mea-
surement Jacobian itself is also expensive when performed
in each step. We therefore propose fast incremental updates
interleaved with occasional reordering, yielding a fast algo-
rithm as supported by the dotted blue curve in Figure 5(d).

(d) Execution time per step for different updating strategies are
shown in both linear and log scale.

Figure 5: For a simulated environment consisting of an 8-loop that
is traversed twice (a), the upper triangular factor R shows signiﬁ-
cant ﬁll-in (b), yielding bad performance (d, continuous red). Some
ﬁll-in occurs when the ﬁrst loop is closed (A). Note that this has no
negative consequences on the subsequent exploration along the sec-
ond loop until the next loop closure occurs (B). However, the ﬁll-in
then becomes signiﬁcant when the complete 8-loop is traversed for
the second time, with a peak when visiting the center point of the
8-loop for the third time (C). After variable reordering, the factor
matrix again is completely sparse (c). Reordering after each step (d,
dashed green) can be less expensive in the case of multiple loops. A
considerable increase in efﬁciency is achieved by using fast incre-
mental updates interleaved with periodic reordering (d, dotted blue),
here every 100 steps.

IJCAI-07

2132

When the robot continuously observes the same land-
marks, for example by remaining in one small room, this ap-
proach will eventually fail, as the information matrix itself
will become completely dense. However, in this case ﬁlters
will also fail due to underestimation of uncertainties that will
ﬁnally converge to 0. The correct solution to deal with this
scenario is to eventually switch to localization.

3.3 Non-linear Systems
Our factored representation allows changing the linearization
point of any variable at any time. Updating the linearization
point based on a new variable estimate changes the measure-
ment Jacobian. One way then to obtain a new factorization
is to refactor this matrix. For the results presented here, we
combine this with the periodic reordering of the variables for
ﬁll-in reduction as discussed earlier.

However, in many situations it is not necessary to perform
relinearization [Steedly et al., 2003]. Measurements are typ-
ically fairly accurate on a local scale, so that relinearization
is not needed in every step. Measurements are also local and
only affect a small number of variables directly, with their
effects rapidly declining while propagating through the con-
straint graph. An exception is a situation such as a loop clos-
ing, that can affect many variables at once. In any case it is
sufﬁcient to perform selective relinearization only over vari-
ables whose estimate has changed by more than some thresh-
old. In this case, the affected measurement rows are ﬁrst re-
moved from the current factor by QR-downdating [Golub and
Loan, 1996], followed by adding the relinearized measure-
ment rows by QR-updating as described earlier.

4 Results
We have applied our approach to the Sydney Victoria Park
dataset (available at http://www.acfr.usyd.edu.au/
homepages/academic/enebot/dataset.htm), a popu-
lar test dataset in the SLAM community. The trajectory con-
sists of 7247 frames along a trajectory of 4 kilometer length,
recorded over a time frame of 26 minutes. 6968 frames are
left after removing all measurements where the robot is sta-
tionary. We have extracted 3631 measurements of 158 land-
marks from the laser data by a simple tree detector.

The ﬁnal optimized trajectory and map are shown in Figure
6. For known correspondences, the full incremental recon-
struction, including solving for all variables after each new
frame is added, took 337s (5.5 minutes) to calculate on a Pen-
tium M 2 GHz laptop, which is signiﬁcantly less than the 26
minutes it took to record the data. The average calculation
time for the ﬁnal 100 steps is 0.089s per step, which includes
a full factorization including variable reordering, which took
1.9s. This shows that even after traversing a long trajec-
tory with a signiﬁcant number of loops, our algorithm still
performs faster than the 0.22s per step needed for real-time.
However, we can do even better by selective reordering based
on a threshold on the running average over the number of
Givens rotations, and back-substitution only every 10 steps,
resulting in an execution time of only 116s. Note that this
still provides good map and trajectory estimates after each
step, due to the measurements being fairly accurate locally.

Figure 6: Optimized map of the full Victoria Park sequence. Solving
the complete problem after every step takes less than 2 minutes on
a laptop for known correspondences. The trajectory and landmarks
are shown in yellow (light), manually overlayed on an aerial image
for reference. Differential GPS was not used in obtaining the results,
but is shown in blue (dark) where available. Note that in many places
GPS is not available.

5 Covariances for Data Association
Our algorithm is helpful for performing data association,
which generally is a difﬁcult problem due to noisy data and
the resulting uncertainties in the map and trajectory estimates.
First it allows to undo data association decisions, but we have
not exploited this yet. Second, it allows for recovering the
underlying uncertainties, thereby reducing the search space.
In order to reduce the ambiguities in matching newly ob-
served features to already existing landmarks, it is advanta-
geous to know the projection Ξ of the combined pose and
landmark uncertainty Σ(cid:3) into the measurement space:

Ξ = HΣ(cid:3)H T

(12)
where H is the Jacobian of the projection process, and Γ mea-
surement noise. This requires knowledge of the marginal co-
variances

(cid:8)

+ Γ

(cid:9)

Σ(cid:3)

ij =

Σjj ΣT
ij
Σij Σii

(13)

between the current pose mi and any visible landmark xj,
where Σij are the corresponding blocks of the full covariance
matrix Σ = I −1. Calculating this covariance matrix in order
to recover all entries of interest is not an option, because it is
always completely populated with n2 entries.

Our representation allows us to retrieve the exact values of
interest without having to calculate the complete dense co-
variance matrix, as well as to efﬁciently obtain a conservative

IJCAI-07

2133

estimate. The exact pose uncertainty Σii and the covariances
Σij can be recovered in linear time. By design, the most re-
cent pose is always the last variable in our factor R – if vari-
able reordering is performed, it is done before the next pose
is added. Therefore, the dim(mi) last columns X of the full
covariance matrix (RT R)−1 contain Σii as well as all Σij,
as observed in [Eustice et al., 2005]. But instead of having
to keep an incremental estimate of these entries, we can re-
trieve the exact values efﬁciently from the factor R by back-
substitution. We deﬁne B as the last dim(mi) unit vectors
and solve RT RX = B by two back-substitutions RT Y = B
and RX = Y . The key to efﬁciency is that we never have to
recover a full dense matrix, but due to R being upper triangu-
lar immediately obtain Y = [0, ..., 0, R−1
ii ]T . Hence only one
back-substitution is needed to recover dim(mi) dense vec-
tors, which only requires O(n) time due to the sparsity of R.
Conservative estimates for the structure uncertainties Σjj
can be obtained as proposed by [Eustice et al., 2005]. As the
covariance can only shrink over time during the smoothing
process, we can use the initial uncertainties as conservative
estimates. A more tight conservative estimate can be obtained
after multiple measurements are available.

Recovering the exact structure uncertainties Σjj is more
tricky, as they are spread out along the diagonal. As the in-
formation matrix is not band-diagonal in general, this would
seem to require calculating all entries of the fully dense co-
variance matrix, which is infeasible. Here is where our fac-
tored representation, and especially the sparsity of the factor
R are useful. Both, [Golub and Plemmons, 1980] and [Triggs
et al., 2000] present an efﬁcient method of recovering exactly
all entries of the covariance matrix that coincide with non-
zero entries in the factor R. As the upper triangular parts
of the block diagonals of R are fully populated, and due to
symmetry of the covariance matrix, this algorithm provides
access to all blocks on the diagonal that correspond to the
structure uncertainties Σjj. The inverse Z = (AT A)−1 is
obtained based on the factor R in the standard way by not-
ing that AT AZ = RT RZ = I, and performing two back-
substitutions RT Y = I and RZ = Y . As the covariance
matrix is typically dense, this would still require calculation
of O(n2) elements. However, we exploit the fact that many
entries obtained during back-substitution of one column are
not needed in the calculation of the next column. To be exact,
only the ones are needed that correspond to non-zero entries
in R, which leads to:

n(cid:4)

⎞
⎠

⎛
⎝ 1
l(cid:4)

rll

1
rll

zll =

⎛
⎝−

zil =

1
rii

−

rlj zjl

n(cid:4)

j=l+1,rlj (cid:4)=0

rijzjl −

(14)

⎞
⎠

rij zlj

j=i+1,rij (cid:4)=0

j=l+1,rij (cid:4)=0

(15)
for l = n, . . . , 1 and i = l − 1, . . . , 1. Note that the sum-
mations only apply to non-zero entries of single columns or
rows of the sparse R matrix. The algorithm therefore has
O(n) time complexity for band-diagonal matrices and matri-
ces with only a constant number of entries far from the diag-
onal, but can be more expensive for general sparse R.

6 Conclusion
We presented a new SLAM algorithm that overcomes the
problems inherent to ﬁlters and is suitable for real-time ap-
plications. Efﬁciency arises from incrementally updating a
factored representation of the smoothing information matrix.
We have shown that at any time the exact trajectory and map
can be retrieved in linear time. We have demonstrated that
our algorithm is capable of solving large-scale SLAM prob-
lems in real-time. We have ﬁnally described how to obtain
the uncertainties needed for data association, either exact or
as a more efﬁcient conservative estimate.

An open research question at this point is if a good in-
cremental variable ordering can be found, so that full matrix
factorizations can be avoided. For very large-scale environ-
ments it seems likely that the complexity can be bounded by
approaches similar to submaps or multi-level representations,
that have proven successful in combination with other SLAM
algorithms. We plan to test our algorithm in a landmark-free
setting, that uses constraints between poses instead, as pro-
vided by dense laser matching.

References
[Dellaert, 2005] F. Dellaert. Square Root SAM: Simultaneous lo-
In

cation and mapping via square root information smoothing.
Robotics: Science and Systems (RSS), 2005.

[Eustice et al., 2005] R. Eustice, H. Singh, J. Leonard, M. Walter,
and R. Ballard. Visually navigating the RMS titanic with SLAM
information ﬁlters.
In Robotics: Science and Systems (RSS),
Cambridge, USA, June 2005.

[Folkesson and Christensen, 2004] J. Folkesson and H. I. Chris-
tensen. Graphical SLAM - a self-correcting map. In IEEE Intl.
Conf. on Robotics and Automation (ICRA), volume 1, pages 383
– 390, 2004.

[Frese et al., 2005] U. Frese, P. Larsson, and T. Duckett. A multi-
level relaxation algorithm for simultaneous localisation and map-
ping. IEEE Trans. Robototics, 21(2):196–207, April 2005.

[Golub and Loan, 1996] G.H. Golub and C.F. Van Loan. Matrix
Computations. Johns Hopkins University Press, Baltimore, third
edition, 1996.

[Golub and Plemmons, 1980] G.H. Golub and R.J. Plemmons.
Large-scale geodetic least-squares adjustment by dissection and
orthogonal decomposition. Linear Algebra and Its Applications,
34:3–28, Dec 1980.

[Julier and Uhlmann, 2001] S.J. Julier and J.K. Uhlmann.

A
counter example to the theory of simultaneous localization and
map building. In IEEE Intl. Conf. on Robotics and Automation
(ICRA), volume 4, pages 4238–4243, 2001.

[Paskin, 2003] M.A. Paskin. Thin junction tree ﬁlters for simulta-
neous localization and mapping. In Intl. Joint Conf. on Artiﬁcial
Intelligence (IJCAI), 2003.

[Steedly et al., 2003] D. Steedly, I. Essa, and F. Dellaert. Spectral

partitioning for structure from motion. In ICCV, 2003.

[Thrun et al., 2005] S. Thrun, W. Burgard, and D. Fox. Probabilis-

tic Robotics. The MIT press, Cambridge, MA, 2005.

[Triggs et al., 2000] B. Triggs, P. McLauchlan, R. Hartley, and
A. Fitzgibbon. Bundle adjustment – a modern synthesis.
In
W. Triggs, A. Zisserman, and R. Szeliski, editors, Vision Algo-
rithms: Theory and Practice, LNCS, pages 298–375. Springer
Verlag, 2000.

IJCAI-07

2134


Inferring Image Templates from Classiﬁcation Decisions

Arnab Dhua, Florin Cutzu
Computer Science Department

Indiana University

Bloomington, IN 47405

adhua,ﬂorin@cs.indiana.edu

Abstract

Assuming human image classiﬁcation decisions are
based on estimating the degree of match between a
small number of stored internal templates and cer-
tain regions of the input images, we present an al-
gorithm which infers observers classiﬁcation tem-
plates from their classiﬁcation decisions on a set of
test images. The problem is formulated as learning
prototypes from labeled data under an adjustable,
prototype-speciﬁc elliptical metric. The matrix of
the elliptical metric indicates the pixels that the
template responds to. The model was applied to
human psychophysical data collected in a simple
image classiﬁcation experiment.

Introduction

1
Consider a psychophysical experiment in which subjects clas-
sify a set of images into two classes. The image classes are
designed such that correct responses require that the subjects
detect the presence of certain features in certain regions of
the image. Given the images, and the subjects’ classiﬁcation
decisions, is it possible to infer the feature templates the sub-
jects have developed and employed?

This represents an inverse problem in perception, in which
the goal is to invert a “direct” model for the subject’s re-
sponses with the goal of deriving the perceptual features used
by the subject. The idea of employing inverse problem tech-
niques is certainly not new in vision research, although it has
been initially applied to low-level vision phenomena [Bertero
et al., 1988]. The psychological literature on the subject of
inverting perceptual similarity data is dominated by research
on multidimensional scaling methods (MDS). Metrical MDS
[Shepard, 1980] is probably the oldest inversion method al-
lowing the derivation of features from similarities. A neural
net-based inversion method for extracting features from judg-
ments of similarity is presented in [Lee, 1998]. More recent
work in this direction has addressed the derivation of fea-
tures involved in higher-level perceptual phenomena [Cutzu
and Tarr, 1999; Cutzu, 2000].

The model for the subjects’ decisions in the image classi-
ﬁcation task considered in this work is a generalized nearest-
neighbor (1-NN) classiﬁer, presented in detail in Section 2.
Each of the two image classes is represented by a small set

of image templates, or prototypes. Given an input image
that must be classiﬁed, its degree of match to all templates
from all classes is estimated, and its class is determined by
the class of the best matching template. Our goal is to infer
the prototypes of each class from a set of images labeled by
human subjects. Therefore, we are seeking to invert the near-
est neighbor problem. This particular inverse problem has
received considerable attention in the machine learning com-
munity. A comprehensive, recent review of the ﬁeld is given
in [Toussaint, 2002]. One large class of algorithms (Hart’s al-
gorithm [Hart, 1968] and its numerous subsequent improve-
ments) selects prototypes from among the labeled data points,
a restriction that cannot be applied to our problem due to the
presence of pixel noise in the test images.

Our algorithm belongs to the so-called prototype genera-
tion methods, which creates prototypes at new locations in
feature space. The ﬁrst such algorithm ([Chang, 1974]) re-
peatedly merged nearest neighboring points from the same
class as long as the classiﬁcation error rate did not increase.
[Bezdek et al., 2001] and later [Mollineda et al., 2002] re-
ﬁned [Chang, 1974] by recursively merging clusters based on
geometrical criteria.

The nearest neighbor rule has been generalized by the use
of an adaptable metric, a technique also employed in this
In [Hastie and Tibshirani, 1996]
paper, albeit differently.
the metric is locally adapted to the training point neighbor-
hood surrounding the query point at the time of classiﬁcation.
In [Friedman, 1994] also a ﬂexible metric is used for dis-
tance calculation in a neighborhood around the query point.
However the ideas of recursive partitioning are used in de-
In [Ricci
termining the neighborhood for the query point.
and Avesani, 1999] a reduced set of prototypes is selected
from the training examples and a different metric is associ-
ated with each of the selected prototypes. This varies from
our approach in two ways. Firstly, the authors in [Ricci and
Avesani, 1999] select the prototypes essentially in a random
manner. Secondly, we select prototypes that in general do not
coincide with the training data points;

A work whose spirit is similar to the present paper is [Xing
et al., 2003]: the authors discuss the problem of learning a
global (that is, valid throughout feature space) elliptical met-
ric for 1-NN classiﬁcation from classiﬁcation data. The met-
ric is ﬁtted to the subject data by solving a convex optimiza-
tion problem.

2 A model for prototype-based image

classiﬁcation

Perceptual considerations required the use of a prototype-
based image classiﬁer in which the various prototypes (the
image templates) “specialize” in various regions of the im-
age, i.e., a template pays more “attention” to some pixels than
to others. This was modeled as follows. Let x ∈ R
d be an
input image and ck ∈ R
d one of the templates. This proto-
type is characterized by a positive (semi) deﬁnite matrix Qk
which modulates the distance (degree of match) between the
template and the image:

d(ck, x)2 = (x − ck)T Qk(x − c).

(1)
The elliptical metric matrix Qk speciﬁes the combination of
image pixels the template ck “specializes” in. In the special
case where Qk is diagonal, the distance above reduces to:

d(cid:1)

d(ck, x)2 =

Qk(i, i) [x(i) − ck(i)]2

i=1

(2)
where Qk(i, i) ≥ 0. The pixels i for which Qk(i, i) = 0
are ignored by the template ck. This distance measure is
termed elliptical. Under the elliptic norm, the set of points
(each point in pixel space represents an input image) that lie
at equal distance to the prototype is no longer a hypersphere,
as for the Euclidean metric, but a hyper-ellipsoid of arbitrary
shape and orientation. Each template is characterized by a
different ellipsoid. Interestingly, by endowing each prototype
with its own, elliptical metric, the resulting Voronoi cells are
not necessarily convex or even connected.

An equivalent formulation is derived by using similarity
(rather than distance) to prototype: a point is assigned to the
most similar prototype. We deﬁne the similarity to proto-
type as the probability of the point “belonging” to the pro-
totype and we associate a Gaussian pdf with each proto-
type. The shape of the Gaussian varies from prototype to
prototype, playing the role of the metric matrix Qk. The
similarity of point i to prototype k is, therefore: sik =
exp
. Or, expressed in Gaussian
pdf form:

(cid:2)−(xi − ck)T Qk(xi − ck)
(cid:3)
s(ck, xi) = exp

(cid:2)−(xi − ck)T W −1

(cid:3)
k (xi − ck)

where the covariance W k = Q−1
k . Therefore, under the 1-
NN rule, point xi is assigned to the prototype whose Gaussian
is the largest at xi.

(3)

1-NN classiﬁers

3 Finding prototypes for elliptical-metric
Each of the N points xi ∈ R
d (the images) is labeled
class 1 or class 2. We seek class-1 and class-2 prototypes
which correctly label the N points under the 1-NN rule using
the elliptical-norm-based similarity measure in Equation (3).
Computing a prototype entails determining its location ck as
well as its metric matrix Qk.

Our prototype ﬁnding algorithm is as follows:
1. Start with two prototypes, derived by applying Gaussian
clusters (algorithm explained below) with two centers to
the full data set. Go to step 2.

(cid:5)

(cid:4)

nk1
nk , nk2
nk

2. At iteration step t there are Kt prototypes. Consider
the points assigned to prototype k (k = 1, . . . , Kt) by
Gaussian clusters. There are nk1 class 1 and nk2 class
2 such points, with nk = nk1 + nk2. If nk1 > nk2,
then the prototype is labeled 1, otherwise it is labeled
2. The points of the minority class are, therefore, mis-
classiﬁed by this prototype. The “impurity” of the pro-
totype was deﬁned as the misclassiﬁcation rate: rk =
1 − max
. If the impurity of prototype k is
greater than some user-deﬁned threshold g, rk > g, then
prototype k is considered for being split into two proto-
types (just like a node in a classiﬁcation tree is split when
too impure). The splitting is performed only if the split-
ting reduces the impurities in the two resulting clusters.
If a node does get split, we obtain a class 1 and a class
2 prototype. These “child” prototypes are placed at the
centers of mass of the nk1, respective nk2 points. Due
to splitting, at the end of this step there are Kt+1 ≥ Kt
prototypes.
If no prototypes are split, go to Step 4, else go to Step 3.
3. Re-apply Gaussian clusters to the data set, ignoring
point labels. Gaussian clusters is initialized with the
Kt+1 prototypes determined at Step 2. Go to Step 2.

4. The algorithm also returns a global classiﬁcation error
rate which measures the errors caused by using the pro-
totypes for classiﬁcation. This error rate is deﬁned by
the total number of minority class points at all proto-
types. The prototypes are labeled according to the ma-
jority class at the prototype. Thus, for each prototype a
center (ck), a covariance matrix (W k), and a class label
are returned.

Observations:

i) The algorithm has as sole adjustable pa-
rameter the impurity threshold g. The number of prototypes
of either class is determined automatically. ii) The class la-
bels of the points are solely used in splitting decisions, and
are irrelevant to the Gaussian k-means algorithm.
iii) Note
the similarity to classiﬁcation and regression trees (CART).

The Gaussian clusters algorithm ﬁts K Gaussian clusters
to input data set. The Gaussian clusters algorithm is ba-
sically like k-means clustering except that each center also
has a covariance matrix associated with it. So for every itera-
tion of the Gaussian clusters algorithm when the new centers
are being estimated we also estimate the new covariance ma-
trices. This covariance matrix distorts the attraction of each
center along the different dimensions.

4 Results: Artiﬁcial data
We applied our prototype ﬁnding algorithm to several syn-
thetic data sets, illustrated in Figures 1-3. These synthetic
data sets were generated in two dimensions (two pixel im-
ages), so that the structure of the point clouds are clearly vis-
ible.

In experiment 1 (Figure 1) there were 320 points of class 1
(blue) and 380 points of class 2 (red). The class 1 points were
arranged in a single vertical band and the class 2 points were
shared by 1 vertical and 1 horizontal band. As expected three

centers have been assigned to the three distinct point clouds
based on the desired purity tolerance.

Figure 1: Left: Training set: class 1 points are blue (dots)
and class 2 points, red (circles). Right: The algorithm has
found three prototypes, whose centers are indicated by yellow
circles. The green lines indicate the axes of the Gaussians.
The cyan/magenta points (if present) are misclassiﬁed.

In experiment 2 (Figure 2) there were 340 points of class
1 (blue) and 340 points of class 2 (red). The points were ar-
ranged into two non-overlapping, non-concentric circles. As
seen in the ﬁgure two prototypes were assigned, one to each
circle. Note that the prototype centers are at locations where
there were no training examples.

Figure 2: Left: Training set: class 1 points are blue (dots)
and class 2 points, red (circles). Right: The algorithm has
found two prototypes, whose centers are indicated by yellow
circles. The green lines indicate the axes of the Gaussians.
The cyan/magenta points (if present) are misclassiﬁed.

In experiment 3 (Figure 3), there were 1849 points of class
1 (blue) and 1849 points of class 2 (red). Both the classes
were drawn from a gaussian distribution, with different means
and standard deviations. Notice the overlap of the two point
clouds. In the results obtained from the algorithm, we can see
that the points assigned to the class 2 prototype surround the
class 1 cluster: this non-convexity is an effect of the elliptical
norm in determining nearest-neighbors.

5 Results: Psychophysical experiment data
The prototype ﬁnding algorithm was applied to simulated
data and also to human image classiﬁcation data collected in
a psychophysical experiment in which the subjects had to dis-
criminate between two classes of visual patterns. The visual
pattern discrimination task is illustrated in Figure 4. In this
task, the observer is shown a single white square corrupted

Figure 3: Left: Training set: class 1 points are blue (dots)
and class 2 points, red (circles). Right: The algorithm has
found two prototypes, whose centers are indicated by yellow
circles. The green lines indicate the axes of the Gaussians.
The cyan/magenta points (if present) are misclassiﬁed.

by additive Gaussian pixel noise at one of four possible lo-
cations. The location of the square is randomly chosen on
each trial. Two of the possible locations are above the ﬁxa-
tion point and two are below the ﬁxation point, with each lo-
cation being equidistant from the central ﬁxation point. Noise
is added to each pixel in each of the four locations, with each
location divided into a 4× 4 grid of pixels. No noise is added
to the regions in between the four square locations, and the
ﬁxation point remains on the screen throughout the experi-
ment. The observers task is to indicate whether the white
square signal appeared above or below ﬁxation. The contrast
of the white square is manipulated across trials according to a
2-down 1-up adaptive staircase procedure to place it at a level
where performance is at approximately 71% correct. Accu-
racy feedback is given in the format of a high or low beep.

Before collecting human subject data two simulations were
performed using the above experimental setup. The two mod-
els simulated were the “exemplar” model and the “prototype”
model. In an “exemplar” model the observers are assumed
to use multiple noisy “templates” to form a representation for
each category. Each of these templates are compared to the
input to reach a classiﬁcation decision. The templates used
in the “exemplar” model simulation were the ideal templates
(the four signals shown in Figure 4). In a “prototype” model
observers are assumed to use a single summary representa-
tion for each category. Thus the templates used in the pro-
totype model simulation were combined versions of the two
signals within each category. This resulted in a ’top’ proto-
type template composed of the two top squares and a ’bottom’
prototype template composed of the two bottom squares. For
each of these two models a simulation with 16000 trials was
performed.

Four human subjects were tested. For each subject, 4000
experimental trials were conducted. The staircase proce-
dure was used to keep the performance rate for each subject
pegged at around 71% correct.
Since each white square was 4× 4 pixels, the classiﬁcation
images viewed by the subjects were 8 × 8 pixels (the cen-
ter of the display was a constant gray-level). Therefore, the
algorithm learned prototypes in 64-dimensional space. The
simplifying assumption was made that there were no percep-

lus is clearly visible.

The difference in the behavior of the mask across the “ex-
emplar” and “prototype” simulations is interesting.
In the
case of the “prototype” simulation it seems that the classi-
ﬁcation into top (class 1) is made by the presence of darker
pixels in both the bottom quadrants and vice versa.

The algorithm yielded interesting results on the human
subjects. Three subjects (subjects 1,3 and 4) appeared to have
used four centers, one for each corner square, and thus two
per class, as can be seen in Figures 9, 11, 12. The masks
obtained for the subject data show that the pixels considered
most important do not seem to follow any particular pattern.
Also the difference in importance across pixels is very small.
This seems to lead to the conclusion that the human subjects
based their decision on the entire image rather than on a par-
ticular quadrant.

Interestingly, subject two appears to have used two cen-
ters, one per class (Figure 10). The two centers correspond
to the two top and bottom corners taken together. Note that
in no actual stimulus both corner squares are “on” simulta-
neously. This would correspond, in the language of the psy-
chology of categorization, to a so-called “prototype model”,
while the other three subjects operated using an “exemplar
model”. The masks in this case are more distinct and like in
the “prototype” simulation show a stronger response for the
pixels in which the stimulus is absent. This can intuitively be
explained by noting that for example in class ‘top’ no input
image actually contains a stimulus in both the ‘top left’ and
the ‘top right’ quadrant. However for class ‘top’ both the bot-
tom quadrants can be expected to have a lower contrast value,
because the stimulus is absent in the lower two quadrants. A
similar intuition can be applied to the ‘bottom’ class.

# 1, class 2

mask

# 2, class 1

mask

# 3, class 2

mask

# 4, class 1

mask

Figure 5: Templates obtained: The 4 “exemplar” centers and
corresponding masks obtained by running our algorithm on
the data generated by running an “exemplar” model simula-
tion.

6 Discussion
We introduced a prototype generating method for nearest-
neighbor classiﬁcation. Each prototype is endowed with its
own elliptical metric, and our method is therefore related
to adaptive metric methods such as [Hastie and Tibshirani,

Figure 4: Top: The four types of stimuli used in the experi-
ments, presented without noise. Top left and top right repre-
sent the class “above ﬁxation”, and the bottom left and right
represent the class “below ﬁxation”. Bottom: An actual test
image used during the experiments. The correct response is
‘bottom’, since the bottom right square is (slightly) brighter.
Note the large noise level.

tual interactions between the pixels, and therefore the covari-
ance matrices associated with the prototypes were restricted
to being diagonal. A diagonal covariance matrix has one en-
try per pixel, and can be interpreted as an image-sized mask
that indicates which pixels the corresponding prototype pays
attention (responds) to.

The algorithm yielded the expected templates when ap-
plied to the simulation data. Figure 51shows the templates
obtained on running the algorithm on the “exemplar” model
simulation. As expected the centers represent the four dif-
ferent stimuli. The masks shown alongside each center show
the relative importance of each pixel for that template. The
masks are actually the inverses of the diagonal values of the
diagonal covariance matrices. A whiter pixel in the mask in-
dicates that the template is more sensitive to that particular
pixel. The masks are quite noisy but one can see that the pix-
els of the image that include the stimuli are considered more
important by the mask. To show the relative importance of
each of the four quadrants we also displayed the masks aver-
aged over each of the four quadrants (Figure 6). In this ﬁgure
the higher importance of the quadrant containing the stimulus
is clearly visible.

Figure 7 shows the templates obtained on running the algo-
rithm on the “prototype” model simulation. As expected the
two centers represent the combination of the stimuli that form
each class. The masks shown alongside each center show the
relative importance of each pixel for that template. The masks
are again quite noisy but one can see that the pixels of the
image that do not include the stimuli are considered more im-
portant by the mask. To show the relative importance of each
of the four quadrants we also displayed the masks averaged
over each of the four quadrants (Figure 8). In this ﬁgure the
higher importance of the quadrants not containing the stimu-

1The results (Figures 5 - 12) are displayed in a more concise
format than the format (example Figure 4) used during experiments.
In particular, the “ﬁxation point” is not shown and the four, 4 × 4
pixel squares are shown adjacent to each other in the results.

# 1, class 2

mask# 1

# 2, class 1

mask# 2

# 1, class 2

mask# 1

# 3, class 2

mask# 3

# 4, class 1

mask# 4

# 2, class 1

mask# 2

Figure 6: Templates obtained: The 4 “exemplar” centers and
corresponding masks obtained by running our algorithm on
the data generated by running an “exemplar” model simula-
tion. The masks are averaged over each of the four quadrants
to give an idea of the overall importance of each quadrant in
the classiﬁcation.

# 1, class 2

mask

Figure 8: Templates obtained: The 2 “prototype” centers and
corresponding masks obtained by running our algorithm on
the data generated by running a “prototype” model simula-
tion. The masks are averaged over each of the four quadrants
to give an idea of the overall importance of each quadrant in
the classiﬁcation.

# 1, class 1

mask

# 2, class 2

mask

# 2, class 1

mask

# 3, class 2

mask

# 4, class 1

mask

Figure 7: Templates obtained: The 2 “prototype” centers and
corresponding masks obtained by running our algorithm on
the data generated by running a “prototype” model simula-
tion.

Figure 9: Templates obtained: The 4 “exemplar” centers and
corresponding masks obtained for subject 1 by running our
algorithm on the subjects classiﬁcation data.

1996] and especially [Xing et al., 2003]. The prototypes are
not a subset of the data points, and therefore our method be-
longs in the prototype generation category. However, as op-
posed to Chang’s method and its later reﬁnements ([Bezdek et
al., 2001], [Mollineda et al., 2002]) we ﬁnd new prototypes
by recursively splitting clusters, rather than by aggregating
them.

The main contribution of this paper lies in the application
of the prototype learning algorithm to understanding human
image classiﬁcation. If an image classiﬁcation experiment is
designed such that correct classiﬁcation decisions require the
detection of one of several class-speciﬁc features in certain
regions of the image, then nearest-neighbor is an appropri-
ate classiﬁcation model. In the experiment described in this
paper, class 1 was deﬁned by the presence of feature A (Top

Left) or feature B (Top Right) and class 2 by the presence of
feature C (Bottom Left) or feature D (Bottom Right). Any
image contains only one of the four features, and, moreover,
each feature occupies a speciﬁc image region. The algorithm
correctly identiﬁed the regions of the image each prototype
“specializes” in.

In this situation, there are two equally effective decision
strategies available to a nearest-neighbor classiﬁer: 1. store
two prototypes per class, one for each of the features A, B,
C, D; or, 2. store one prototype per class, one for the com-
posite feature (A or B) and one for the composite feature (C
or D). The ﬁrst strategy corresponds, in the terminology of
the psychology of categorization, to categorization by exem-
plar, and the second, to categorization by prototype. Most
interestingly, our algorithm revealed that one of the subjects
employed the second strategy, and the other three the ﬁrst. It

# 1, class 2

mask

# 2, class 1

mask

Figure 10: Templates obtained: The 2 “prototype” centers
and corresponding masks obtained for subject 2 by running
our algorithm on the subjects classiﬁcation data.

# 1, class 2

mask

# 2, class 2

mask

# 3, class 1

mask

# 4, class 1

mask

Figure 11: Templates obtained: The 4 “exemplar” centers
and corresponding masks obtained for subject 3 by running
our algorithm on the subjects classiﬁcation data.

# 1, class 1

mask

# 2, class 1

mask

# 3, class 2

mask

# 4, class 2

mask

Figure 12: Templates obtained: The 4 “exemplar” centers
and corresponding masks obtained for subject 4 by running
our algorithm on the subjects classiﬁcation data.

is important to note that a conventional analysis of error rates
would not be able to distinguish between the two strategies.

References
[Bertero et al., 1988] M. Bertero, T. Poggio, and V. Torre.
Ill-posed problems in early vision. In Proceedings of the
IEEE, volume 76, pages 869–889, 1988.

[Bezdek et al., 2001] J. C. Bezdek, T. R. Reichherzer, G. S.
Lim, and Y. Atikiouzel. Multiple-prototype classiﬁer de-
sign. IEEE Trans. Systems, Man and Cybernetics – Part
B, 31:408–413, 2001.

[Chang, 1974] C. L. Chang. Finding prototypes for nearest
IEEE Transactions on Computers,

neighbor classiﬁers.
23:1179–1184, 1974.

[Cutzu and Tarr, 1999] F. Cutzu and M. Tarr. Inferring per-
ceptual saliency ﬁelds from viewpoint-dependent recogni-
tion data. Neural Computation, 11:1331–1348, 1999.

[Cutzu, 2000] Florin Cutzu. Computing 3D Object Parts
from Similarities among Object Views. In Computer Vi-
sion and Pattern Recognition, volume 2, pages 2095–
2100, 2000.

[Friedman, 1994] J.H. Friedman. Flexible metric nearest
neighbor classiﬁcation. Technical report, Stanford Uni-
versity, Stanford, CA., Nov 1994.

[Hart, 1968] Peter E. Hart. The condensed nearest neighbor
rule. IEEE Transactions on Information Theory, 14:515–
516, 1968.

[Hastie and Tibshirani, 1996] T. Hastie and R. Tibshirani.
Discriminant Adaptive Nearest Neighbor Classiﬁcation.
IEEE Transactions on Pattern Analysis and Machine In-
telligence, 18:607–616, 1996.

[Lee, 1998] M. D. Lee. Neural Feature Abstraction from
Judgments of Similarity. Neural Computation, 10:1815–
1830, 1998.

[Mollineda et al., 2002] R. A. Mollineda, F. J. Ferri, and
E. Vidal. An efﬁcient prototype merging strategy for the
condensed 1-NN rule through class-conditional hierarchi-
cal clustering. Pattern Recognition, 35:2771–2782, 2002.
[Ricci and Avesani, 1999] F. Ricci and P. Avesani. Data
compression and local metrics for nearest neighbor classi-
ﬁcation. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 1999.

[Shepard, 1980] R. N. Shepard. Multidimensional scaling,

tree-ﬁtting, and clustering. Science, 210:390–397, 1980.

[Toussaint, 2002] Godfried Toussaint. Proximity Graphs for
Nearest Neighbor Decision Rules: Recent Progress.
In
Proceedings of INTERFACE-2002, 34th Symposium on
Computing and Statistics, Montreal, Canada, 2002.

[Xing et al., 2003] Eric P. Xing, Andrew Y. Ng, Michael I.
Jordan, and Stuart Russell. Distance Metric Learning
with Application to Clustering with Side-Information. In
S. Thrun S. Becker and K. Obermayer, editors, Advances
in NIPS 15, pages 505–512. 2003.


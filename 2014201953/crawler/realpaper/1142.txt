Inside-Outside Probability Computation for Belief Propagation

Taisuke Sato

Tokyo Institute of Technology

2-12-1 ˆOokayama Meguro-ku Tokyo Japan 152-8552

Abstract

In this paper we prove that the well-known cor-
respondence between the forward-backward algo-
rithm for hidden Markov models (HMMs) and be-
lief propagation (BP) applied to HMMs can be gen-
eralized to one between BP for junction trees and
the generalized inside-outside probability compu-
tation for probabilistic logic programs applied to
junction trees.

1 Introduction
Bayesian networks (BNs) and probabilistic context free
grammars (PCFGs) are two basic probabilistic frameworks
in uncertainty modeling (BNs) and in statistical natural lan-
guage processing respectively. Although they are indepen-
dently developed, there is a strong indication of their close
relationship. For example both include hidden Markov mod-
els (HMMs) as a common subclass. Furthermore belief prop-
agation (BP) applied to HMMs coincides with the forward-
backward algorithm for HMMs [Smyth et al., 1997] which is
a specialization of probability computation used in the Inside-
Outside (IO) algorithm for PCFGs [Baker, 1979]. Nonethe-
less, however, no exact correspondence beyond this one is
known to our knowledge.

In this paper1 we upgrade this correspondence. We prove
that the inside-outside probability computation in the IO algo-
rithm, when generalized for probabilistic logic programs and
applied to junction trees, yields BP. In particular we prove
that collecting evidence (resp. distributing evidence) in BP
coincides with the computation of inside probabilities (resp.
outside probabilities) in this generalized IO computation.

To prove the computational correspondence between BNs
and PCFGs in a uniﬁed manner, we need a general language
that can describe BNs and PCFGs2. We choose PRISM [Sato
and Kameya, 2001] as a ﬁrst-order probabilistic language
for this purpose. We also need “propositionalization” of

1We assume in this paper that BNs are discrete and BP is without

normalization.

2Note that BNs are a propositional framework that deal with
ﬁnitely many random variables while PCFGs allow recursion and
have to deal with inﬁnitely many random variables describing prob-
abilistic choices in a sentence derivation.

BNs [Sato and Kameya, 2001; McAllester et al., 2004;
Chavira and Darwiche, 2005]. By propositionalization we
mean to represent a discrete random variable X having n val-
} of mutually ex-
ues {v1, . . . , vn} by a set {Xv1 , . . . , Xvn
= 1 (true) iff
clusive binary random variables such that Xvi
X = vi (1 ≤ i ≤ n). This propositionalization explodes
the number of states in a BN. However the beneﬁt often out-
weighs the explosion because it makes possible to share com-
putation with ﬁner grain size value-dependently at runtime
by dynamic programming and rule out 0 probability compu-
tation at compile time. It explains, though we omit details,
why probability computation in polynomial time cannot be
expected of the direct application of BNs to PCFGs [Pyna-
dath and Wellman, 1996] while it is carried out in O(n3) time
where n is the sentence length by PRISM and by case-factor
diagrams (CFDs) [McAllester et al., 2004].

In what follows, we ﬁrst review basic notions in Section 2.
We then prove main theorems after a series of lemmas in Sec-
tion 3. Due to space limitations, the description is sketchy
and the reader is assumed to be familiar with logic program-
ming [Doets, 1994] and BP in junction trees [Jensen, 1996;
Lauritzen and Spiegelhalter, 1988; Shafer and Shenoy, 1990].
PRISM, a probabilistic logic programming language used in
this paper, is detailed in [Sato and Kameya, 2001].

2 Background
2.1 Bayesian networks and junction trees
A Bayesian network BN is a directed acyclic graph deﬁn-
ing a joint distribution P (X1 = x1, . . . , XN = xN ) =
ΠN
i=1P (Xi = xi | Πi = πi) such that nodes are random
variables X1, . . . , XN and if a node Xi has parent nodes
Πi = Xs1 , . . . , Xsk
(k ≥ 0), a conditional probability table
representing a conditional distribution P (Xi = xi | Πi = πi)
is associated with Xi (1 ≤ i ≤ N ). Here a lower case
letter xi denotes a value of Xi and similarly for πi. We
denote the range of Xi by R(Xi) and the direct product
R(Xs1 ) × · · · × R(Xsk
) by R(Πi) and write xi ∈ R(Xi) and
πi ∈ R(Πi). Hereafter we use P (x1, . . . , xN ) for P (X1 =
x1, . . . , XN = xN ) etc. When we consider P (xi | πi) as
a function of xi, πi, the set {xi} ∪ πi is called the argu-
| πi). Let α = {s1, . . . , sk} be a set of
ments of P (xi
variable indices (⊆ {1, . . . , N }). Xα stands for the set of
}. For example if α = {1, 2, 3},
variables {Xs1 , . . . , Xsk

IJCAI-07

2605

Xα = {X1, X2, X3}. This notation extends to vectors.

A junction (join) tree T = (V, E) for BN is a tree sat-
isfying the following conditions. First a node is a set Xα
of variables in BN. In what follows we use Xα and its in-
dex set α interchangeably. An edge connecting α and β is
labeled by α ∩ β. Second P (x1, . . . , xN ) = Πα∈V φα(xα)
must hold where φα(xα), a potential (function), is a prod-
uct of some (or no) conditional distributions such that their
arguments are included in xα. The third condition is the run-
ning intersection property (RIP) which dictates that if nodes
α and β have a common variable, it must be contained in
every node on the path between α and β. RIP ensures the
conditional independence of the subtrees given variables in
the node and is the key property for efﬁcient probability com-
putation by BP. Given a BN, a junction tree is constructed by
way of triangulation or variable elimination [Jensen, 1996;
Kask et al., 2001; Lauritzen and Spiegelhalter, 1988].

2.2 PCFGs and inside-outside probabilities
A PCFG is a CFG with probabilities assigned to produc-
tion rules in such way that if a nonterminal A has N rules,
A → α1, . . . , A → αN , a probability pi is assigned to
A → αi for i (1 ≤ i ≤ N ) and
i=1 pi = 1 holds. pi is the
probability of choosing A → αi to expand A in a probabilis-
tic derivation. PCFGs are a basic tool for statistical natural
language processing and include HMMs as a subclass.

(cid:2)N

Let A(i, j) denote an event that a nonterminal A prob-
abilistically derives the substring wi, . . . , wj of a sentence
L = w1, . . . , wn (1 ≤ i ≤ j ≤ n). The probability of A(i, j)
is called the inside probability of A(i, j) and deﬁned as the
sum of the products of probabilities associated with rules in a
derivation belonging to A(i, j). Similarly the outside proba-
bility of A(i, j) w.r.t. L is the sum of products of the probabil-
ities associated rules used in a derivation that starts from the
start symbol and derives w1, . . . , wi−1Awj+1, . . . , wn. The
product of inside-outside probabilities of A(i, j) gives the
probability of deriving L via A(i, j). Inside-outside probabil-
ities are computed by dynamic programming in time O(|L|3).
We next generalize inside-outside probabilities in the context
of probabilistic logic programming.

2.3 Probabilistic logic programming language

PRISM

We brieﬂy explain a probabilistic logic programming lan-
guage PRISM. In a nutshell, PRISM is Prolog extended with
tabling3, a probabilistic built-in predicate called msw (multi-
ary random switch) and a generic parameter learning routine
that learn parameters embedded in a program by computing
generalized inside-outside probabilities [Sato and Kameya,
2001].

One of the basic ideas of PRISM is propositionalization
of random variables using a special built-in predicate msw/3.
Let V be a discrete random variable with a set R(V ) of
ground terms as its range. To represent a proposition V = v

3Tabling here means to memoize goals whose predicate symbol
is declared as table predicate and to cache successful goals in a table
for later reuse [Zhou and Sato, 2003]. An atom containing a table
predicate is called a tabled atom.

(v ∈ R(V )), we introduce a ground term i as a name (iden-
tiﬁer) for V and a ground msw atom msw(i, n, v) which is
true iff the outcome of an n-th trial of V named i is v. Here
n is a natural number. V as iids are represented by the set
{msw(i, n, v) | v ∈ R(V ), n = 0, 1, . . .} of msw atoms.
These msw atoms must satisfy certain conditions4. The prob-
ability of msw(i, n, v) being true is called a parameter.

In PRISM a program DB = R ∪ F consists of a set R of
deﬁnite clauses whose head is not an msw atom and a set F
of msw atoms together with a base distribution PF deﬁning
probabilities (parameters) of msw atoms in F . Simple distri-
butions are deﬁnable soley in terms of msw atoms but com-
plex distributions are constructed by using deﬁnite clauses.
In our semantics, DB deﬁnes a probability measure PDB (·)
over the set of Herbrand interpretations (distribution seman-
tics [Sato and Kameya, 2001]). Hereafter we consider PDB (·)
as a distribution on ground atoms as well.

2.4 Propositionalization through tabled search

In our approach, PDB (G), the probability of an atom G de-
ﬁned by a program DB, is computed in two steps. The ﬁrst
step is propositionalization. We apply the SLD refutation pro-
cedure [Doets, 1994] to DB and ⇐ G as a top-goal, we search
for all SLD proofs of G and reduce G to a logically equiv-
alent but propositional DNF formula E1 ∨ · · · ∨ Ek where
Ei (1 ≤ i ≤ k), an explanation of G, is a conjunction of
ground msw atoms. They record probabilistic choices made
in the process of constructing an SLD proof of G and each
msw atom represents a probabilistic event V = v for some
random variable V . Then in the second step, we compute the
probability of G as PDB (G) = PDB (E1 ∨ · · · ∨ Ek).

In general since there are exponentially many proofs and
so are explanations, the naive proof search would produce
an exponential size DNF. Fortunately however by introduc-
ing tabling in the ﬁrst step, we can often produce an equiva-
lent but polynomial size boolean formula such that common
subexpressions in the Ei’s are factored out as tabled atoms.
The factored formula, Expl(G), becomes a set (conjunction)
of deﬁnitions of the form H ⇔ W where a tabled atom
H is deﬁned by W which is a conjunction of tabled atoms
and msw atoms. We assume the deﬁning relation of these
tabled atoms is acyclic. For convenience we sometimes think
of each deﬁnition as an AND-OR graph and conventionally
call Expl(G) an explanation graph as the collection of such
AND-OR graphs. Hereafter Expl(G) stands for the factored
formulas and their graphical representation as well.

2.5 Generalized inside-outside probabilities

To compute PDB (G), we convert each deﬁnition H ⇔ A1 ∨
. . .∨AL in Expl(G) where Ai = B1∧· · ·∧BMi
∧msw1∧· · ·∧

for v
P

(cid:3)= v(cid:2) ∈ R(V ) and PF (

4We require that PF (msw(i, n, v) ∧ msw(i, n, v(cid:2))) = 0
v∈R(V ) msw(i, n, v)) =
v∈R(V ) PF (msw(i, n, v)) = 1 holds for any n. Also when i (cid:3)= i(cid:2)
or n (cid:3)= n(cid:2), msw(i, n, v) and msw(i(cid:2), n(cid:2), v) must be independent and
msw(i, n, v) and msw(i, n(cid:2), v) must be identically distributed.

W

IJCAI-07

2606

mswNi

(1 ≤ i ≤ L) to a numerical sum-product equation.

T

PDB (H) = PDB (A1) + · · · + PDB (AL)
PDB (Ai) = PDB (B1) · · · PDB (BMi

)·

PDB (msw1) · · · PDB (mswNi

(1)

)

, msw1, . . . , mswNi

Note that this conversion assumes the mutual exclusiveness of
disjuncts {A1, . . . , AL} and the independence of conjuncts
}. Although guaranteeing
{B1, . . . , BMi
these two conditions is basically the user’s responsibility,
they are automatically satisﬁed as far as the PRISM pro-
gram describing junction trees is concerned (see Section 3
and Lemma 3.3). We denote by Eq(G) the set of converted
equations.

For a ground atom A, we call PDB (A) a P-variable.
P-variables are just numerical variables named by ground
atoms. As we assume the deﬁning relation of tabled atoms
is acyclic, P-variables in Eq(G) can be linearly ordered so
that Eq(G) is efﬁciently solved in a bottom-up manner by
dynamic programming in time proportional to the size of
Eq(G). Also the acyclicity implies that a higher P-variable is
a multivariate polynomial in the lower P-variables, and hence
we can take the derivative of a higher P-variable as a function
of the lower P-variables.

Suppose we are given a program DB.

In an analogy to
inside-outside probabilities in PCFGs, we deﬁne a gener-
alized inside probability inside(A) of a ground atom A by
inside(A) def= PDB (A) and extend the deﬁnition to a con-
junction W of ground atoms by inside(W ) def= PDB (W ).

We

also deﬁne

a generalized outside probability
outside(G ; A) of A w.r.t. a top-goal G as follows. First
enumerate A’s occurrences in Expl(G) as

(cid:3)

H1 ⇔ (A ∧ W1,1) ∨ · · · ∨ (A ∧ W1,i1 )

· · ·

root

node

q

(x

)

W

Figure 1: Junction tree T = (cid:13)V, E(cid:14)

3 Belief propagation as the generalized IO

computation

In this section, we prove that the generalized IO computation,
i.e. the computation of generalized inside-outside probabili-
ties, subsumes BP in junction trees.

3.1 Program for BP messages
Suppose we have a BN deﬁning a joint distribution P (X1 =
x1, . . . , XN = xN ) = ΠN
i=1P (xi = Xi | Πi = πi) and
a junction tree T = (V, E) such that P (x1, . . . , xN ) =
Πα∈V φα(xα). Let δ be the root node of T 6.

We construct a PRISM program DB T = FT ∪ RT
Introduce for each
| πi) a ground msw atom
If Xi has no parent put πi = []

that describes BP in T as follows7.
conditional probability P (xi
msw(bn(i, πi), once, xi).
(null list). Deﬁne a ﬁnite set FT of msw atoms by

FT

def= { msw(bn(i, πi), once, xi) |

1 ≤ i ≤ N, xi ∈ R(Xi), πi ∈ R(Πi) }

HJ ⇔ (A ∧ WJ,1) ∨ · · · ∨ (A ∧ WJ,iJ

).

and set parameters θbn(i,πi),xi by

Then outside(G ; A) is recursively computed by Eq. 25.

θbn(i,πi),xi

= PF (msw(bn(i, πi), once, xi)) = P (xi | πi).

(cid:4)i1
(cid:4)iJ

outside(G ; G) = 1
outside(G ; A) = outside(G ; H1)

j=1 inside(W1,j )

+ · · · + outside(G ; HJ )

j=1 inside(WJ,j).

(2)
Using Eq. 2, all outside probabilities are computed in time in
the size of Eq(G) [Sato and Kameya, 2001]. We can prove
that for a ground atom A, the product inside(A) · outside(A)
is the average number of occurrences of A in a proof of G and
that our deﬁnition is a generalization of the usual deﬁnition of
inside-outside probabilities in PCFGs [Lafferty, 1993].

5As mentioned above, PDB (G), a P-variable,

is a function
of other P-variables PDB (A) and the mathematical deﬁnition of
outside(G ; A) is

outside(G ; A) def=

∂ PDB (G)
∂ PDB (A)

which derives Eq. 2.

Then it is easy to see that every joint probability is represented
by a conjunction of these ground msw atoms, i.e. we have

(cid:7)

P (x1, . . . , xN ) = PF

i=1 msw(bn(i, πi), once, xi)

.

(cid:5)(cid:6)N

Next introduce an atom msw(bn(i, Πi), once, Xi) con-
taining variables Xi and Πi for each i (1 ≤ i ≤ N ).
msw(bn(i, Πi), once, Xi) represents the conditional distribu-
tion P (Xi = xi | Πi = πi)8. For every node α in the junction
tree T , deﬁne a conjunction Wα(Xα) representing the poten-
tial φα(xα) of α and introduce a clause Cα deﬁning a mes-
sage atom qαγ(Xα∩γ) that describes a message in BP sent

6In what follows, for simplicity we assume no evidence is given.
When some variables are observed however, all conclusions remain
valid, except that they are ﬁxed to the observed values.

7Programming convention follows Prolog.
8Here we use intentionally Xi both as a logical variable and as a
random variable to make explicit the correspondence between gen-
eral msw atoms and conditional distributions in BN.

IJCAI-07

2607

⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩

RT1

qγ10 ⇐ msw(bn(1, once, []), X1) ∧

msw(bn(5, once, [X4, X1]), X5)∧
qα1γ1(X1, X4)

qα1γ1(X1, X4) ⇐

qβ1α1 (X2, X4) ⇐

msw(bn(2, once, [X1]), X2) ∧ qβ1α1 (X2, X4)

msw(bn(3, once, [X2]), X3) ∧
msw(bn(4, once, [X3]), X4)

Figure 3: The deﬁnitions of message atoms for T1.

(cid:7)

(cid:6)K

deﬁnition of tabled atom qαγ(xα∩γ) for every node α in T as
shown below.

(cid:13)

(cid:5)

xα\γ

Wα(xα) ∧

qαγ(xα∩γ) ⇔

i=1 qβiα(xβi∩α)

.
(3)
Here xα∩γ denotes an arbitrary ground instantiation of Xα∩γ
and qαγ(xα∩γ) represents the message sent from α to γ, α’s
parent node, after receiving messages from α’s child nodes
β1, . . . , βK (see Figure 1).

We next prove that the recursive equations Eq. 3 are
“solved” uniquely. Let Tα be the subtree of T rooted at Xα
and Xξα be the set of variables appearing in Tα. We ﬁrst in-
troduce a formula τα(Xξα
) for α by Eq. 4 and rewrite it to
Eq. 5. It represents the potential of the subtree Tα.

τα(Xξα

)

def=

Wρ(Xρ)

ρ∈Tα

= Wα(Xα) ∧

i=1 τβi

(4)

(5)

(Xξβi

)

(cid:6)K

(cid:6)

(cid:14)K

Lemma 3.2

ξα = α ∪
\ γ = (ξβi

ξβi
ξα \ γ = (α \ γ) ∪

i=1 ξβi

(cid:14)K

\ α) ∪ (βi ∩ (α \ γ)) (from RIP of T )

i=1(ξβi

\ α)

(cid:13)

(6)

(cid:2)

(cid:2)

(cid:6)

from α to its parent node γ. They are respectively deﬁned as

Wα(Xα) def=
Cα

def= qαγ(Xα∩γ) ⇐ Wα(Xα)∧

P (xi|πi)∈φα

msw(bn(i, Πi), once, Xi),

qβ1α(Xβ1∩α) ∧ · · · ∧ qβK α(XβK ∩α).

Here β1, . . . , βK (K ≥ 0) are the child nodes of α in T .
The next lemma states that Wα(Xα) correctly describes the
potential of node α. The proof is straightforward and omitted.
Lemma 3.1

(cid:8)

PF (Wα(xα)) =

i:P (xi|πi)∈φα

P (xi | πi) = φα(xα).

(cid:2)

For the root node δ in T , it has no parent but we add a special
parent node 0 to V and deﬁne Cδ as

Cδ

def= qδ0 ⇐ Wδ(Xδ)∧
1δ(Xβ(cid:2)

qβ(cid:2)

1∩δ) ∧ · · · ∧ qβ(cid:2)

K(cid:2) δ(Xβ(cid:2)

K(cid:2) ∩δ)

1, . . . , β(cid:4)

where β(cid:4)
K (cid:2) are the child nodes of δ. qδ0 has no ar-
guments but calls every message atom directly or indirectly.
Finally put

RT

def= {Cα | α ∈ V, T = (V, E)}.

We illustrate a small example. Take a discrete Bayesian
network BN1 on {X1, . . . , X5} on the left-hand side of Fig-
ure 2 and its junction tree T1 on the right-hand side with the
root node γ1. Dotted lines in BN1 indicate edges added by tri-
angulation. Figure 3 shows the deﬁnitions of message atoms
for T1.

X 3

X 2

X 4

P(x 1)

P(x 5 | x 1, x 4)
X 1, X 4, X 5

1={1,4,5}

X
1,

X

4

P(x 2 | x 1)
X 1, X 2, X 4

1={1,2,4}

,X 4
X 2

P(x 3 | X 2)
P(x 4 | x 3)
X 2, X 3, X 4

X 1

X 5

BN 1

Junction tree T 1

1={2,3,4}

Figure 2: BN1 and a junction tree T1

3.2 Explanation graphs for BP messages
Let DB T = FT ∪ RT be the program constructed in Subsec-
tion 3.1. After declaring every qαγ predicate as a table predi-
cate, we apply tabled search for all proofs to a top-goal ⇐ qδ0
where δ is the root node of T . The search always terminates
and yields an explanation graph Expl(qδ0) which contains a

Proposition 3.1 qαγ(xα∩γ) =

τα(xξα

)

xξα\γ

(Proof) By well-founded induction on T . When α is a leaf in
T , the proposition is obviously true. Assume it is true w.r.t.
the child nodes of α.

τα(xξα

(cid:15)

(cid:15)
(cid:15)

xα\γ

)

· · ·

(cid:15)

xβ1\α

(cid:15)
⎛
⎝Wα(xα) ∧
(cid:21)

xα\γ
((α \ γ) and (βi \ α)’s are mutually disjoint)

xβK \α

τα(xξα

) by Eq. 6

⎛
⎝ (cid:15)

K(cid:18)
K(cid:18)

i=1

)

τβi

(xξβi

(cid:22)

xξβi

\α

⎞
⎠

⎞
⎠ by Eq. 5

Wα(xα) ∧

qβiα(xβi∩α)

by induction

xα\γ

i=1
= qαγ(xα∩γ) by Eq. 3

Q.E.D.

(cid:15)

xξα\γ

=

=

=

IJCAI-07

2608

inside

3.3 BP and the generalized IO computation
Let PDB T be the distribution deﬁned by DB T .
The
generalized
atom
tabled
inside(qαγ(xα∩γ)) and the generalized outside proba-
bility outside(qδ0 ; qαγ(xα∩γ)) w.r.t. qδ0 can be computed
using Eq. 3 by sum-product computation speciﬁed in Eq. 1 if
the independence of conjuncts and the mutual exclusiveness
of disjuncts on the right-hand side of Eq. 3 are guaranteed.

probability

the

of

Since each msw atom msw(bn(i, Πi), once, Xi) occurs
only once in some Wα reﬂecting the fact that a conditional
| πi) in the BN belongs exclu-
distribution function P (xi
sively to one potential φα in the junction tree, Wα(xα) and
the qβiα(xβi∩α)’s do not share any msw atoms. Hence the
ﬁrst condition, the independence condition, is satisﬁed au-
tomatically. On the other hand, proving the mutual exclu-
siveness condition is not straightforward. Lemma 3.3 be-
low assures the exclusiveness condition when combined with
Proposition 3.1.

Note that ξα ∩ γ = α ∩ γ holds thanks to RIP of T . So we

rewrite τα(Xξα

) as

τα(Xξα

) = τα(Xξα\γ, Xξα∩γ)
= τα(Xξα\α, Xα∩γ).

(cid:5)

and x(cid:4)
Lemma 3.3 Let xξα\γ
two different
ground instantiations of Xξα\γ.
Then for an arbi-
trary ground instantiation xα∩γ of Xα∩γ, we have
¬

τα(xξα\γ , xα∩γ) ∧ τα(x(cid:4)

ξα\γ , xα∩γ)

(cid:7)

ξα\γ

be

(cid:2)

.

iM

, xis

, . . . , x(cid:4)

, . . . , xis−1 = x(cid:4)

(Sketch of proof) Without
loss of generality, we can
write Xξα\γ = Xi1 , . . . , XiM in such a way that if Xij
is a parent node of Xik in the original BN, Xij pre-
cedes Xik in this list. As xξα\γ = (xi1 , . . . , xiM
) (cid:15)=
ξα\γ = (x(cid:4)
x(cid:4)
), there is a variable Xis such that
i1
xi1 = x(cid:4)
is (1 ≤ s ≤ M ).
is−1
i1
Then ﬁrst we note Πis
⊆ Xξα holds since Xis appears
only in Tα and the conditional distribution P (Xis
| Πis
)
must be contained in some potential in Tα by RIP. Second
∩ Xξα\γ ⊆ {Xi1 , . . . , Xis−1 }. We also have
we have Πis
∩ Xα∩γ. We can conclude from
Πis
) to
these facts that (xξα\γ , xα∩γ) instantiates P (Xis
| Πis
)
P (xis
to P (x(cid:4)
is. The rest is immediate and
is
omitted.
Q.E.D.

, xα∩γ) instantiates P (Xis
(cid:15)= x(cid:4)

ξα\γ
) where xis

∩ Xξα∩γ = Πis

) while (x(cid:4)

| Πis

| πis

| πis

(cid:15)= x(cid:4)

We now prove main theorems by applying computation
in Eq. 1 to the tabled atoms deﬁned by Eq. 3. Recall that
(A) for a ground atom A where PDB T
inside(A) = PDB T
is the distribution deﬁned by DB T . We derive an equation
satisﬁed by inside probabilities of tabled atoms.

Theorem 3.1

inside(qαγ(xα∩γ )) =

(cid:23)

K(cid:8)

φα(xα)

xα\γ

i=1

inside(qβiα(xβi∩α)).

(Proof)
inside(qαγ(xα∩γ))

= PDB T
= PDB T
=

xξα\γ

(cid:5)(cid:13)
(cid:2)

(qαγ(xα∩γ))

xξα\γ
PDB T

τα(xξα
(τα(xξα
· · ·

(cid:2)

xβ1\α
i=1PDB T

(τβi

(cid:7)

)

by Proposition 3.1

)) by Lemma 3.3

PDB T

(Wα(xα)) ·

xβK \α

))

(xξβi

K(cid:8)

by Eq. 6

⎛
⎝ (cid:15)

⎞
⎠

)

PDB T

(Wα(xα))

PDB T

τβi

(xξβi

i=1

xβi\α

(qβiα(xξβi

∩α)) by Lemma 3.1

φ(xα)

PDB T

xα\γ

(cid:2)
(cid:2)
(cid:4)K
(cid:23)
(cid:23)
(cid:23)

xα\γ

xα\γ

=

=

=

=

K(cid:8)
K(cid:8)

i=1

φ(xα)

inside(qβiα(xξβi

∩α))

Q.E.D.

xα\γ

i=1

Theorem 3.1 tells us that the generalized inside probabilities
of tabled atoms satisfy exactly the same equations as mes-
sages in the collecting evidence phase of BP in T with the
root node δ [Jensen, 1996; Lauritzen and Spiegelhalter, 1988;
Shafer and Shenoy, 1990]. Hence, the bottom-up computa-
tion of generalized inside probabilities is identical to BP in
the collecting evidence phase.

Let P1 be the distribution deﬁned by BN1 in Figure 2. The
equations for generalized inside probabilities of tabled atoms
for the junction tree T1 are:

inside(qα1γ1 (x1, x4))

(cid:2)
(cid:2)
(cid:2)
(cid:2)

=

=

=

=

inside(qβ1α1 (x2, x4))

(msw(bn(2, [x1]), x2)) ·

PDB T1
inside(qβ1α1 (x2, x4))
P1(x2 | x1)inside(qβ1α1(x2, x4))

x2

x2

(msw(bn(3, [x2]), x3)) ·
(msw(bn(4, [x3]), x4))

PDB T1
PDB T1
P1(x3 | x2) · P1(x4 | x3).

x3

x3

We next compute generalized outside probabilities of
tabled atoms. Without loss of generality, we compute the
outside probability of a tabled atom for β1. We apply
the deﬁnition of generalized outside probability in Eq. 2 to
Expl(qδ0) while noting that a tabled atom qβ1α(xβ1∩α) oc-
curs in Expl(qδ0) as in Eq. 3. We obtain recursive equations
about generalized outside probabilities as follows.

Theorem 3.2

outside(qβ1α(xβ1∩α)) =

outside(qαγ (xα∩γ))

(cid:2)
(cid:4)K

φα(xα)

xα\β1
i=2inside(qβiα(xβi∩α)).

(cid:2)

IJCAI-07

2609

(cid:2)

(Proof)
outside(qβ1α(xβ1∩α))
=

x(α∩γ)\(β1 ∩α)

(cid:23)
(cid:4)K

outside(qαγ(xα∩γ))

(cid:23)

φα(xα)

i=2inside(qβiα(xβi∩α))

outside(qαγ(xα∩γ))φα(xα)

x(α\γ)\(β1 ∩α)
by Eq. 2

K(cid:8)

inside(qβiα(xβi∩α))

xΔ
where Δ = ((α ∩ γ) \ (α ∩ β1)) ∪ ((α \ γ) \ (β1 ∩ α))

i=2

= α \ β1

(cid:23)

(cid:23)

=

=

φα(xα)outside(qαγ(xα∩γ))

inside(qβiα(xβi∩α)).

xα\β1

i=2

Q.E.D.

outside(qδ0) = 1 holds for the top-node δ. Therefore we
have the following corollary:
1, . . . , β(cid:4)
Corollary 3.1 Let β(cid:4)

K (cid:2) be δ’s child nodes.

(cid:23)

outside(qβ(cid:2)

1δ(xβ(cid:2)

1∩δ)) =

φδ(xδ)

inside(qβ(cid:2)

i

δ(xβ(cid:2)

i

∩δ)).

xδ\β(cid:2)
1

i=2

K(cid:8)

K (cid:2)(cid:8)

1997] and provides a missing link between BP and PCFGs
for the ﬁrst time.

The most closely related work to ours is CFDs proposed by
McAllester et al. [McAllester et al., 2004]. CFDs are a propo-
sitional framework for probabilistic inference of Markov ran-
dom ﬁelds. They proved that a single algorithm can efﬁ-
ciently compute probabilities both for PCFGs and for BNs in
their framework but the relationship between BP and their al-
gorithm remains unclear. Since PRISM also generates propo-
sitional expressions (explanation graphs) from ﬁrst order ex-
pressions by (tabled) search, it is an interesting future topic to
relate CFDs to PRISM.

References
[Baker, 1979] J. K. Baker. Trainable grammars for speech recog-
In Proceedings of Spring Conference of the Acoustical

nition.
Society of America, pages 547–550, 1979.

[Chavira and Darwiche, 2005] M. Chavira and A. Darwiche. Com-
piling bayesian networks with local structure. In Proceedings of
the 19th International Joint Conference on Artiﬁcial Intelligence
(IJCAI’05), pages 1306–1312, 2005.

(cid:2)

[Doets, 1994] K. Doets. From Logic to Logic Programming. The

MIT Press, 1994.

Theorem 3.2 in conjunction with Corollary 3.1 clearly shows
that the computation of generalized outside probabilities of
tabled atoms in a top-down manner that starts from the top-
node δ and proceeds to lower layers in Expl(qδ0) is exactly
the same as the distributing evidence phase of BP in T with
the root node δ [Jensen, 1996; Lauritzen and Spiegelhalter,
1988; Shafer and Shenoy, 1990]. We illustrate below the
computation of the generalized outside probabilities of atoms
in RT1 w.r.t. the junction tree T1 in Figure 2.

outside(qα1γ1 (x1, x4))

(cid:2)
(cid:2)
(cid:2)
(cid:2)

=

=

=

=

outside(qβ1α1(x2, x4))

PDB T1

x5

(msw(bn(1, []), x1) ∧

msw(bn(5, [x1, x4]), x5))

P1(x1)P1(x5 | x1, x4)

x5

outside(qα1γ1(x1, x4)) ·

x1

(msw(bn(2, [x1]), x2))

PDB T1
P1(x1)P1(x5 | x1, x4)P1(x2 | x1).

x1,x5

[Jensen, 1996] F. V. Jensen. An Introduction to Bayesian Networks.

UCL Press, 1996.

[Kask et al., 2001] K. Kask, R. Dechter, J Larrosa, and F. Cozman.
Bucket-tree elimination for automated reasoning. ICS Technical
Report Technical Report No.R92, UC Irvine, 2001.

[Lafferty, 1993] J.D. Lafferty. A derivation of the Inside-Outside
Algorithm from the EM algorithm. Technical Report TR-IT-
0056, IBM T.J.Watson Research Center, 1993.

[Lauritzen and Spiegelhalter, 1988] S.L.

and D.J.
Spiegelhalter. Local computations with probabilities on graphi-
cal structures and their applications to expert systems. Journal
of the Royal Statistical Society, B, 50:157–224, 1988.

Lauritzen

[McAllester et al., 2004] D. McAllester, M. Collins, and F. Pereira.
Case-factor diagrams for structured probabilistic modeling.
In
Proceedings of the 20th Annual Conference on Uncertainty in
Artiﬁcial Intelligence (UAI-04), pages 382–391, Arlington, Vir-
ginia, 2004. AUAI Press.

[Pynadath and Wellman, 1996] D. V. Pynadath and M. P. Wellman.
Generalized queries on probabilistic context-free grammars. In
Proceedings of the 14th National Conference on Artiﬁcial Intel-
ligence (AAAI’96), pages 1285–1290, 1996.

[Sato and Kameya, 2001] T. Sato and Y. Kameya. Parameter learn-
ing of logic programs for symbolic-statistical modeling. Journal
of Artiﬁcial Intelligence Research, 15:391–454, 2001.

[Shafer and Shenoy, 1990] G.R. Shafer and P.P. Shenoy. Probabil-
ity propagation. Annals of Mathematics and Artiﬁcial Intelli-
gence, 2:327–352, 1990.

[Smyth et al., 1997] P. Smyth, D. Heckerman, and M. Jordan. Prob-
abilistic independence networks for hidden markov probability
models. Neural Computation, 9(2):227–269, 1997.

[Zhou and Sato, 2003] Neng-Fa Zhou and T. Sato. Efﬁcient Fix-
point Computation in Linear Tabling.
In Proceedings of the
Fifth ACM-SIGPLAN International Conference on Principles
and Practice of Declarative Programming (PPDP2003), pages
275–283, 2003.

Finally we conﬁrm that since inside(qδ0) = 1 and every
tabled atom occurs only once in the proof of qδ0, the product
of generalized inside-outside probabilities equals a marginal
probability as follows.

inside(qβ1α1(x2, x4))outside(qβ1α1(x2, x4))

=

P1(x3 | x2)P1(x4 | x3)

P1(x1)P1(x5 | x1, x4)P1(x2 | x1)

x1,x5

= P1(x4 | x2)P1(x2) = P1(x2, x4).

(cid:2)
(cid:2)

x3

4 Conclusion
We have proved that BP in junction trees is nothing but the
generalized IO computation applied to junction trees (Theo-
rem 3.1 and 3.2, Corollary 3.1). This equivalence is a general-
ization of the well-known equivalence between the forward-
backward algorithm and BP applied to HMMs [Smyth et al.,

IJCAI-07

2610


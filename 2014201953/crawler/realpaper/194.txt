Learning Partially Observable Deterministic Action Models

Eyal Amir

Computer Science Department

University of Illinois, Urbana-Champaign

Urbana, IL 61801, USA

eyal@cs.uiuc.edu

Abstract

We present the ﬁrst tractable, exact solution for
the problem of identifying actions’ effects in
partially observable STRIPS domains. Our al-
gorithms resemble Version Spaces and Logical
Filtering, and they identify all the models that
are consistent with observations. They apply in
other deterministic domains (e.g., with condi-
tional effects), but are inexact (may return false
positives) or inefﬁcient (we could not bound
the representation size). Our experiments ver-
ify the theoretical guarantees, and show that
we learn STRIPS actions efﬁciently, with time
that is signiﬁcantly better than approaches for
HMMs and Reinforcement Learning (which
are inexact). Our results are especially surpris-
ing because of the inherent intractability of the
general deterministic case. These results have
been applied to an autonomous agent in a vir-
tual world, facilitating decision making, diag-
nosis, and exploration.

Introduction

1
Autonomous agents have limited prior knowledge of
their actions’ preconditions and effects when they ex-
plore new domains. They can act intelligently if they
learn how actions affect the world and use this knowl-
edge to respond to their goals. This is important when
their goals change because then they can reason about
their actions instead of trying them in the world.

Learning actions’ effects and preconditions is difﬁcult
in partially observable domains. The world state is not
known completely, actions’ effects mix with each other,
and it is hard to associate change in one feature with
a speciﬁc action or situation. Thus, it is not surprising
that work so far has been limited to fully observable do-
mains (e.g., [Wang, 1995; Pasula et al., 2004]) and to
hill-climbing (EM) approaches that have unbounded er-
ror in deterministic domains (e.g., [Ghahramani, 2001;
Boyen et al., 1999]).

This paper presents an approach called SLAF (Simul-
taneous Learning and Filtering) for exact learning of ac-
tion’s effects and preconditions in partially observable
deterministic domains. This approach determines a set of
possible transition relations, given a sequence of actions
and partial observations. It is online, and updates a log-
ical formula that models the possible transition relations
and world states with every time step. It ﬁnds exactly
those transition relations when actions are STRIPS (i.e.,
no conditional effects) or they map states 1:1.

Our algorithms take polynomial time in the number of
features, n, and the number of time steps, T , for many
cases. Their exact complexity varies with properties of
the domain. For example, the overall time for learning
STRIPS actions’ effects is O(T · n). For other cases the
update per time step takes linear time in the representa-
tion size. We can bound this size by O(nk) if we approx-
imate the representation with a k-CNF formula, yielding
an overall time of O(T · nk) for the entire algorithm.

Our experiments verify these theoretical results and
show that our algorithms are faster and better qualita-
tively than related approaches. For example, we can
learn STRIPS actions’ effects in domains of > 100 fea-
tures exactly. In contrast, work on learning in Dynamic
Bayesian Networks (e.g., [Boyen et al., 1999]), rein-
forcement learning in POMDPs (e.g., [Littman, 1996]),
and Inductive Logic Programming (ILP) (e.g., [Wang,
1995]) either approximate the solution with unbounded
error for deterministic domains, or take time Ω(22n
)
(thus, are inapplicable in domains larger than 10 fea-
tures). Section 7 provides a comparison with these and
other works.

Our technical advance for deterministic domains is im-
portant for many applications such as automatic software
interfaces, internet agents, virtual worlds, and games.
Other applications, such as robotics, human-computer
interfaces, and program and machine diagnosis can use
deterministic models as approximations. Finally, under-
standing the deterministic case better can help us develop
better results for stochastic domains.

In the following, Section 2 deﬁnes SLAF precisely,

Section 3 provides a deduction-based exact SLAF al-
gorithm, Section 4 presents tractable model-update al-
gorithms, Section 5 gives sufﬁcient conditions and al-
gorithms for keeping the model representation compact
(thus, overall polynomial time), and Section 6 presents
experimental results.

2 SLAF Semantics
We deﬁne our SLAF problem with the following formal
tools, borrowing intuitions from work on Bayesian learn-
ing of Hidden Markov Models (HMMs) [Ghahramani,
2001] and Logical Filtering [Amir and Russell, 2003].

A transition system is a tuple hP, S, A, Ri, where
• P is a ﬁnite set of propositional ﬂuents;
• S ⊆ P ow(P) is the set of world states;
• A is a ﬁnite set of actions;
• R ⊆ S × A × S is the transition relation.

Thus, a world state, s ∈ S, is a subset of P that contains
propositions true in this state, and R(s, a, s′) means that
state s′ is a possible result of action a in state s. Our
goal in this paper is to ﬁnd R, given known P, S, A, and
a sequence of actions and partial observations (logical
sentences on any subset of P).

A transition belief state is a set of tuples hs, Ri where
s is a state and R a transition relation. Let R = P ow(S×
A × S) be the set of all possible transition relations on
S, A. Let S = S × R. When we hold a transition belief
state ρ ⊆ S we consider every tuple hs, Ri ∈ ρ possible.
For example, consider the situation presented in Fig-
ure 1. There, we have two rooms, a light bulb, a switch,
an action of ﬂipping the switch, and an observation, E
(we are in the east room). The real states of the world,
s2, s2′ (shown in the top part), are unknown to us.

The bottom part of Figure 1 demonstrates how our
knowledge evolves after performing the action sw-on.
ρ1, ρ2 are our respective transition belief states.
In ρ1
we know that the possible world states are s2, s1, or
s3 (s1, s3 are arbitrary world states), with R2, R1, and
R3, their respective transition relations. ρ2 is the re-
sulting transition belief state after action sw-on. Action
sw-on takes state s2 to s2′ according to transition rela-
tion R2, so hs2′, R2i is a pair ρ2. Similarly, it takes
state s1 to one of s1′, s1′′ according to transition rela-
tion R1, and it takes s3 to s3′ according to R3. Thus,
hs1′, R1i, hs1′′, R1i, hs3′, R3i are in pair ρ2. Finally,
observing E eliminates the pair hs3′, R3i from ρ2, if E
is false in s3′.
Deﬁnition 2.1 (SLAF Semantics) Let ρ ⊆ S be a tran-
sition belief state. The SLAF of ρ with actions and ob-
servations haj, oji1≤j≤t is deﬁned by

1. SLAF [a](ρ) =

{hs′, Ri | hs, a, s′i ∈ R, hs, Ri ∈ ρ};

2. SLAF [o](ρ) = {hs, Ri ∈ ρ | o is true in s};
3. SLAF [haj, ojii≤j≤t](ρ) =

SLAF [haj, ojii<j≤t](SLAF [oi](SLAF [ai](ρ))).

2

West

East

West

East

off

off

on

on

=⇒
s2 = ¬sw ∧ ¬lit ∧ E sw-on

<s1,R1>

ρ1

<s2,R2>

<s3,R3>

s2′ = sw ∧ lit ∧ E

<s1’’,R1>

<s3’,R3>

<s1’,R1>

<s2’,R2>

ρ2

Figure 1: Top: Two rooms and ﬂipping the light switch.
Bottom: SLAF semantics; progressing an action (the ar-
rows map state-transition pairs) and then ﬁltering with an
observation (crossing out some pairs).

Step 1 is progression with a, and Step 2 ﬁltering with o.

We assume that observations (and observation model
relating observations to state ﬂuents) are given to us as
logical sentences over ﬂuents after performing an action.
They are denoted with o. When actions can be inexe-
cutable or may fail, it is useful to assume that P contains
a special boolean ﬂuent, OK, whose value is the success
of the last action attempted.

Transition belief state ρ generalizes version spaces
(e.g., [Wang, 1995]) as follows: If the current state, s,
is known, then the version space’s lattice contains the
set of transition relations ρs = {R | hs, Ri ∈ ρ}.
It
also generalizes belief states: If the transition relation,
R, is known, then the belief state (set of possible states)
is ρR = {s | hs, Ri ∈ ρ} (read ρ restricted to R), and
Logical Filtering [Amir and Russell, 2003] of belief state
σ and action a is equal to (thus, we deﬁne it as)

F ilter[a](σ) = (SLAF [a]({hs, Ri | s ∈ σ}))R.

3 SLAF via Logical Inference
Learning transition models using Deﬁnition 2.1 directly
is intractable because it takes space Ω(22|P|
) in many
cases. Instead, in this section we represent transition be-
lief states more compactly (sometimes) using logic, and
compute SLAF using general purpose logical inference.
We represent every deterministic transition relation,
G whose mean-
R, with a language, L, of propositions aF
ing is “If G holds, then F will hold after executing a”.
We have aF
G ∈ L for every action a ∈ A, F a literal
(possibly negated proposition) from P, and G a complete
term over P (a conjunction of literals from P such that
every ﬂuent appears exactly once). We call F , G the sets
of effects, F , and preconditions, G, respectively. Thus,
we have 2|P| · 2|P| · |A| new propositional variables (in
Section 5 we decrease this number).

Deﬁne ϕR, the logical theory that represents R, by
G ∈ L | ∀hs, a, s′i ∈ R, s |= G ⇒ s′ |= F }
R = {aF

ϕ0
and ϕR = ϕ0

R ∪ {¬aF

G | aF

G ∈ L \ ϕ0

R}.

Some intuitive properties hold for this representation.
ϕR is a complete theory for every R (a sentence or its
negation is always implied from ϕR). Also, ϕR |=
¬aF

G ⇔ a¬F
Thus, for every transition belief state ρ we can de-
ﬁne a theory in L(L ∪ P) that corresponds to it: ϕρ =

G , for every F ∈ F , G ∈ G.

Whs,Ri∈ρ(s ∧ ϕR). Similarly, for every theory ϕ in

L(L ∪ P) we deﬁne a transition belief state ρϕ =
{hs, Ri | s ∈ S, s∧ϕR |= ϕ}, i.e., all the state-transition
pairs that satisfy ϕ. We say that theory ϕ is a transition
belief formula, if ϕρϕ ≡ ϕ (note: ρϕρ = ρ always holds).
For a deterministic (possibly conditional) action, a,

deﬁne the effect model of a for time t to be

G ∧ Gt) ⇒ lt+1) ∧
G ∧ Gt)))

Teff(a, t) = Vl∈F ,G∈G((at ∧ al

Vl∈F (lt+1 ∧ at ⇒ (WG∈G(al

(1)
where at asserts that action a occurred at time t, and we
use the convention that ϕt is the result of adding sub-
script t to every ﬂuent symbol of ϕ. The ﬁrst part of (1)
says that if a executes at time t, and it causes l when G
holds, and G holds at time t, then l holds at time t + 1.
The second part says that if l holds after a’s execution,
G holds, with G corresponding to
then it must be that al
the current state. These two parts correspond to effect
axioms and explanation closure axioms used in Situation
Calculus.

Now, we are ready to describe our zeroth-level algo-
rithm (SLAF0) for SLAF of a transition belief formula.
Denote by CnV (ϕ) the set of consequences of ϕ that are
in vocabulary V , and let Lt+1 = Pt+1 ∪ L the vocab-
ulary that includes only ﬂuents of time t + 1 and effect
propositions from L. At time t we apply progression for
the given action a and current transition belief formula,
ϕt, and then apply ﬁltering with the current observations:

1. SLAF0[a](ϕt) = CnLt+1 (ϕt ∧ at ∧ Teff(a, t))
2. SLAF0[o](ϕt) = ϕt ∧ ot
We can implement CnV (ϕ) using consequence ﬁnd-
ing algorithms, such as resolution and some of its vari-
ants (e.g., [Simon and del Val, 2001]). The following
theorem shows that this formula-SLAF algorithm is cor-
rect and exact.
Theorem 3.1 For ϕ transition belief formula, a action,
SLAF [a]({hs, Ri ∈ S | hs, Ri satisﬁes ϕ}) =

{hs, Ri ∈ S | hs, Ri satisﬁes SLAF0[a](ϕ)}
Our zeroth-level algorithm may enable more compact
representation, but it does not guarantee it, nor does it
guarantee tractable computation.
In fact, no algorithm
can maintain compact representation or tractable com-
putation in general. SLAF is NP-hard because Logi-
cal Filtering is NP-hard [Eiter and Gottlob, 1992] even
for deterministic actions. Also, any representation of
transition belief states that uses poly(|P|) propositions
grows exponentially (in the number of time steps and
|P|) for some starting transition belief states and action
sequences.

4 Efﬁcient Model Update
Learning world models is easier when SLAF distributes
over logical connectives. The computation becomes
tractable, with the bottleneck being the time to compute
SLAF for each part separately.
In this section we ex-
amine when such distribution is possible, and present a
linear-time algorithm that gives an exact solution in those
cases (and a weaker transition belief formula otherwise).
Distribution properties that always hold for SLAF fol-
low from set theoretical considerations and Theorem 3.1:
Corollary 4.1 For ϕ, ψ transition belief formulae, a ac-
tion,
SLAF [a](ϕ ∨ ψ) ≡ SLAF [a](ϕ) ∨ SLAF [a](ψ)
|= SLAF [a](ϕ ∧ ψ) ⇒ SLAF [a](ϕ) ∧ SLAF [a](ψ)
Stronger distribution properties hold for SLAF when-

ever they hold for Logical Filtering.
Theorem 4.2 Let ρ1, ρ2 be transition belief states.
SLAF [a](ρ1 ∩ ρ2) = SLAF [a](ρ1 ∩ ρ2) iff for every R
2 ).
F ilter[a](ρR
We conclude the following corollary from Theorems

2 ) = F ilter[a](ρR

1 ) ∩ F ilter[a](ρR

1 ∩ ρR

3.1, 4.2 and theorems in [Amir and Russell, 2003].
Corollary 4.3 For ϕ, ψ transition belief formulae, a ac-
tion, SLAF [a](ϕ∧ψ) ≡ SLAF [a](ϕ)∧SLAF [a](ψ) if
for every relation R in ρϕ,ρψ one of the following holds:

1. a in R maps states 1:1
2. a has no conditional effects (and we know if it fails),

and ϕ ∧ ψ includes all its prime implicates.

3. The state is known for R: for at most one s, hs, Ri ∈

ρϕ ∪ ρψ.

Figure 2 presents Procedure Factored-SLAF, which
computes SLAF exactly when the conditions of Corol-
lary 4.3 hold. Consequently, Factored-SLAF returns an
exact solution whenever our actions are known to be 1:1.
If our actions have no conditional effects and their suc-
cess/failure is observed, then a modiﬁed Factored-SLAF
can solve this problem exactly too (see Section 5).

If we pre-compute (and cache) the 2n possible re-
sponses of Literal-SLAF, then every time step t in this
procedure requires linear time in the representation size
of ϕt, our transition belief formula. This is a signiﬁcant
improvement over the (super exponential) time taken by
a straightforward algorithm, and over the (potentially ex-
ponential) time taken by general-purpose consequence
ﬁnding used in our zeroth-level SLAF procedure above.
Theorem 4.4 Step-SLAF(a, o, ϕ) returns a formula ϕ′
such that SLAF [a, o](ϕ) |= ϕ′. If every run of Literal-
SLAF takes time c, then Step-SLAF takes time O(|ϕ|c).
Finally, if we assume one of the assumptions of Corollary
4.3, then ϕ′ ≡ SLAF [a, o](ϕ).

We can give a closed-form solution for the SLAF of
a belief-state formula (a transition belief formulae that
has no effect propositions aF
G). This makes procedure
Literal-SLAF tractable, and also allows us to examine
the structure of belief state formulae in more detail.

3

PROCEDURE Factored-SLAF(hai, oii0<i≤t,ϕ)
∀i, ai action, oi observation, ϕ transition belief formula.

1. For i from 1 to t do,

(a) Set ϕ ← Step-SLAF(oi,ai,ϕ).
(b) Eliminate subsumed clauses in ϕ.

2. Return ϕ.

PROCEDURE Step-SLAF(o,a,ϕ)
o an observation formula in L(P), a an action, ϕ a transition
belief formula.

1. If ϕ is a literal, then return o∧Literal-SLAF(a,ϕ).
2. If ϕ = ϕ1 ∧ ϕ2, return Step-SLAF(o,a,ϕ1)∧Step-

3. If ϕ = ϕ1 ∨ ϕ2, return Step-SLAF(o,a,ϕ1)∨Step-

SLAF(o,a,ϕ2).

SLAF(o,a,ϕ2).

PROCEDURE Literal-SLAF(a,ϕ)
a an action, ϕ a proposition in Lt or its negation.

1. Return CnLt+1 (ϕt ∧ at ∧ Teff(a, t)).

Figure 2: SLAF using distribution over ∧, ∨

Theorem 4.5 For belief-state formula ϕ ∈ L(P), time t,
G ), and G1, ..., Gm ∈

action a, Ca = VG∈G,l∈F (al

G all the terms in G such that Gi |= ϕ,

G ∨ a¬l

SLAF [a](ϕt) ≡ ^l1,...,lm∈F

m

_i=1

i ∨ a¬li
(lt+1
Gi

) ∧ Ca

PROOF SKETCH

We follow the characterization
offered by Theorem 3.1 and Formula (1). We take
ϕt ∧ at ∧ Teff(a, t) and resolve out the literals of time t.
This resolution is guaranteed to generate a set of conse-
quences that is equivalent to CnLt+1 (ϕt ∧at ∧Teff(a, t)).
Assuming ϕt ∧ at, Teff(a, t) is logically equivalent to
G ∧ Gt) ⇒ lt+1) ∧
G)). This follows
from two observations. First, notice that ϕt implies that
for any G ∈ G such that G 6|= ϕ we get Gt ⇒ al
G
and (al
G ∧ Gt) ⇒ lt+1 (the antecedent does not hold, so
the formula is true). Second, notice that, in the second
G ∧ Gt)) is

Teff(a, t)|ϕ = Vl∈F ,G∈G,G|=ϕ((al
Vl∈F ,G∈G,G|=ϕ(lt+1 ⇒ (Gt ⇒ al

part of the original Teff(a, t), (WG∈G,G|=ϕ(al
equivalent (assuming ϕ) to (VG∈G,G|=ϕ(Gt ⇒ al

Now, resolving out the literals of time t from ϕt ∧ at ∧
Teff(a, t)|ϕ should consider all the resolutions of clauses
(Gt is a term) of the form al
G ∧ Gt ⇒ lt+1 and all the
clauses of the form lt+1 ⇒ (Gt ⇒ al
G) with each other.
This yields the equivalent to

G)).

m

lt+1
i

[

_i=1

∨

m

_i=1
(a¬li
Gi

∧ ¬ali
Gi

)]

^G1, ..., Gm ∈ G
ϕ |= W i≤m Gi

l1, ..., lm ∈ F

because to eliminate all the literals of time t we have to
resolve together sets of clauses with matching Gi’s such

that ϕ |= Wi≤n Gi. The formula above encodes all the

4

∧ ¬ali
Gi

resulting clauses for a chosen set of G1, ..., Gm and a
chosen set of literals l1, ..., lm. The reason for including
(a¬li
) is that we can always choose a clause with
Gi
Gi, li of a speciﬁc type (either one that includes ¬al
G or
one that produces a¬l
G .

Finally, we get the formula in our theorem because
G (G characterizes exactly one state in S),
G ≡ ¬a¬F
aF
and the fact that there is one set of G1, ..., Gm that is
stronger than all the rest (it entails all the rest) because
G1, ..., Gm are complete terms. This set is the complete
ﬂuent assignments Gi that satisfy ϕ.

A consequence of Theorem 4.5 is that we can imple-

ment Procedure Literal-SLAF using the equivalence
L(l) ⊆ P

SLAF [a](l) ≡ (cid:26) Theorem 4.5

l ∧ SLAF [a](TRUE) otherwise (cid:27)

Notice that the computation in Theorem 4.5 for ϕ = l a
literal is simple because G1, ..., Gm are all the complete
terms in L(P) that include l.

5 Compact Model Representation
In Theorem 4.5, m could be as high as 2|P|, the num-
ber of complete terms in G. Consequently, clauses may
have exponential length (in n = |P|) and there may be a
super-exponential number of clauses in this result.

In this section we restrict our attention to STRIPS ac-
tions [Fikes et al., 1972], and provide an overall polyno-
mial bound on the growth of our representation, its size
after many steps, and the time taken to compute the re-
sulting model. STRIPS is a class of actions that has been
at the focus of interest for the area of AI planning and
execution for many years. It includes deterministic, un-
conditional (but sometimes not executable) actions.
5.1 Actions of Limited Effect
In many domains we can assume that every action a af-
fects at most k ﬂuents, for some small k > 0.
It is
also common to assume that our actions are STRIPS,
and that they may fail without us knowing, leaving the
world unchanged. Those assumptions together allow
us to progress SLAF with a limited (polynomial factor)
growth in the formula size.

We use a language that is similar to the one in Section
3, but which uses only action propositions al
G with G be-
ing a ﬂuent term of size k (instead of a ﬂuent term of size
n in G). Semantically, al
.
al
l1∧...∧ln
Theorem 5.1 Let ϕ ∈ L(P) be a belief-state formula, t
time, and a a STRIPS action with ≤ k ﬂuents affected or
in the precondition term. Let Gk be the set of all terms of
k ﬂuents in L(P) that are consistent with ϕ. Then,

≡ Vlk+1,...,ln

l1∧...∧lk

SLAF [a](ϕt) ≡

k

_i=1

^G1, ..., Gk ∈ Gk

G1 ∧ ... ∧ Gk |= ϕ
l1, ..., lk ∈ F

i ∨ a¬li
(lt+1
Gi

) ∧ Ca

The proof (omitted) uses two insights. First, if a has
only one case in which change occurs, then every clause
in Theorem 4.5 is subsumed by a clause that is entailed
by SLAF [a](ϕt), has at most one ali
per literal li (i.e.,
Gi
li 6= lj for i 6= j) and Gi is a ﬂuent term (has no disjunc-
G with G term is equivalent to a
tions). Second, every al
with Gi terms of length k, if a affects
formula on al
Gi
only k ﬂuents.

Thus, we can encode all of the clauses in the con-
junction using a subset of the (extended) action effect
propositions, al
G, with G being a term of size k. There
are O(nk) such terms, and O(nk+1) such propositions.
Every clause is of length 2k, with the identity of the
clause determined by the ﬁrst half (the set of action ef-
fect propositions). Consequently, SLAF [a](ϕt) takes
O(nk2+k ·k2) space to represent using O(nk2+k) clauses
of length ≤ 2k.

5.2 Always-Executable Actions
Many times we can assume that our actions are always
executable. For example, our sequence of actions does
not include action failures when this sequence is chosen
by a knowledgeable agent (e.g., a human) that has better
observations than we do.

.

al
l1∧...∧ln

We use a language that is similar to the one in Sec-
tion 3, but which uses only action propositions al (l
always holds after executing a) and al◦ (l is not af-
, and

fected by a). Semantically, al ≡ Vl1,...,ln
ali◦ ≡ Vl1,...,li−1,li+1,...,ln

ali
l∧l1∧...∧ln

We assume that transition-belief formula ϕ is ﬂuent-
factored, i.e., it is the conjunction of ϕf such that each
ϕf concerns only one ﬂuent, f, and actions’ effects on it.
Also, for every f, ϕf ≡ (¬f ∨explf )∧(f ∨expl¬f )∧Af ,
with explf , expl¬f , Af formulae over action propositions
af , a¬f , and af ◦ (possibly multiple different actions).
We call this format ﬂuent-factored. The intuition here
is that explf and expl¬f are all the possible explanations
for f being true and false, respectively. Also, Af holds
knowledge about actions’ effects on f, knowledge that
does not depend on f’s current value. For example, if we
know nothing about actions that affect f (e.g., when we
start our exploration), then ϕf = (¬f ∨ T RU E) ∧ (f ∨
T RU E) ∧ T RU E.

With this representation, SLAF [a](ϕf ) is

(¬f ∨(af ∨(af◦∧explf )))∧(f ∨(a¬f ∨(af◦∧expl¬f )))∧Af
A similar formula holds for observations.

This contributes to a much simpler algorithm, AE-
STRIPS-SLAF, that we present in Figure 3. It computes
SLAF of a ﬂuent-factored transition belief state in this
form with a sequence of actions and observations.
Theorem 5.2 Let ϕ be a ﬂuent-factored transition belief
formula, and assume a1, ..., aT are always-executable
STRIPS actions, and o1, ..., oT are observation terms.

5

PROCEDURE AE-STRIPS-SLAF(hai, oii0<i≤T ,ϕ)
ai actions, oi observations, ϕ = Vf ∈P ϕf ﬂuent-factored
with ϕf = (¬f ∨ explf ) ∧ (f ∨ expl¬f ) ∧ Af .
1. For every f ∈ P do: For i from 1 to T do,
(a) Set ϕf ← (¬f ∨ (af ∨ (af◦ ∧ explf ))) ∧ (f ∨

(a¬f ∨ (af◦ ∧ expl¬f ))) ∧ Af .

(b) If oi |= f (we observed f), then set ϕf ← (¬f ∨

T RU E) ∧ (f ∨ F ALSE) ∧ Af ∧ explf .

(c) If oi |= ¬f (we observed ¬f), then set ϕf ←
(¬f ∨ F ALSE) ∧ (f ∨ T RU E) ∧ Af ∧ expl¬f .

2. Eliminate subsumed clauses in ϕ = Vf ∈P ϕf .

3. Return ϕT = ϕ.

Figure 3: SLAF with always-executable STRIPS.

Then, Procedure AE-STRIPS-SLAF(hai, oii0<i≤t,ϕ) re-
turns ϕT ≡ SLAF [hai, oii0<i≤t](ϕ0) in time O(T ·
|P|), and |ϕT | ≤ O(T · |P|).

The reason for the space bound on the end formula
|ϕT | is that at every time step we increase the size of the
formula by adding to it at most 4 new elements per ﬂuent.
We can further improve the implementation to take
time O(T · |o|), for |o| the largest number of ﬂuents ob-
served at once. Also, the bound on the space taken by ϕT
can be improved in two ways. First, O(|P| · 2|A| log |A|),
for A our set of actions, is a bound on the CNF repre-
sentation of ϕT because every part ϕT
f of it is similar
to (f ⇒ (af
2 ∧ ...))))). Thus, if
|A| is small, then this bound is manageable. Second, if
every ﬂuent f is observed at least once every k actions
(or an action which affects f occurs, and we know its
effect), then the CNF representation of ϕT takes space
O(|P| · |A|k). Again, if k is small, this bound is useful.

1 ∨ (af ◦

2 ∨ (af ◦

1 ∧ (af

5.3 Action Preconditions
Unfortunately, a simple procedure such as AE-STRIPS-
SLAF (Figure 3) does not provide an exact solution in
general. The reason is that action failure makes ﬂu-
ents co-dependent (each of them could have caused this
failure). For instance, consider Figure 1, and assume
that we add the ﬂuent OK which indicates that the
last action succeeds.
If we know only ¬on (the light
is off),
then SLAF[sw-on](¬on) ∧ ¬OK (i.e., sw-on
failed, e.g., because we are in the West room) implies
sw ∨ E ∨ a¬OK
E∧¬sw∧¬lit ∨
E∧sw∧¬lit, by Theorem 4.5.
a¬OK
Instead, we compute the effect for every literal using
Theorem 5.1, (assuming exactly k ﬂuents are changed by
action a) and then approximate by dropping all clauses
with more than one ﬂuent (other than OK). Thus, we can
augment step 1(a) in AE-STRIPS-SLAF with a condition
that a succeeded (oi |= OK), and add that if a fails, then
we use the following formula instead: Set ϕf ← (¬f ∨
(explf ∧expl¬OK,f ))∧(f ∨(expl¬f ∧expl¬OK,¬f ))∧Af ,

¬E∧¬sw∧¬lit ∨ a¬sw

¬E∧sw∧¬lit ∨ a¬E

G1

G2

(al

G1 ∧ G2 |= l

∨ a¬OK

for expl¬OK,l = V G1, G2 ∈ Gk
The size of the resulting formula is larger by an ad-
ditive O(|expl¬OK,l|) = O(n2k) per ﬂuent. Thus, the
result after T steps is a formula ϕT with |ϕT | ≤ O(T ·
|P|2k), which is polynomial and feasible for small k’s.

).

5.4 Using the Model and Adding Bias
Our algorithms compute a solution to SLAF as a logical
formula. We can use a SAT solver to answer queries over
this formula, such as checking if it entails af , for action
a and ﬂuent f. The number of variables in the result
formula is always independent of T , and is linear in |P|
for some of our algorithms, so we can use current SAT
solvers to treat domains of 1000 features and more.

Many times it is also desirable to prefer some models
over others. We can represent such bias using a prefer-
ence model (e.g., [McCarthy, 1986]) or a probabilistic
prior over transition relations. We add this bias on top
of our SLAF result, ϕT , and get an exact solution if we
can run inference on the combined model. Preferential
bias is well studied and ﬁts easily with logical formula
ϕT (e.g., we can use implementations of Circumscrip-
tion for inference with such bias).

Also, algorithms for inference with probabilistic bias
and logical sentences are now emerging and can be used
here too [Hajishirzi and Amir, 2005]. We can use these
algorithms to apply probabilistic bias to the resulting log-
ical formula. For example, given a probabilistic graphi-
cal model (e.g., Bayesian Networks) and a set of propo-
sitional logical sentences, we can consider the logical
sentences as observations. With this approach, a log-
ical sentence ϕ gives rise to a characteristic function
δϕ(−→x ) which is 1 when −→x satisﬁes ϕ and 0 otherwise.
For a conjunction of clauses we get a set of such func-
tions (one per clause). Thus, inference in the combined
probabilistic-logical system is a probabilistic inference
(such as variable elimination (e.g., [Dechter, 1999])) in
which there are additional potential functions.

6 Experimental Evaluation
We tested our AE-STRIPS-SLAF algorithm in partially
observable blocks-world domains. We generated random
action-observation sequences of 4000 steps from a de-
scription of the domain in PDDL. Our SLAF algorithm
receives no domain information besides the sequence of
actions and observations that is given by the generator.

We ran the algorithm with blocks-world domains of
increasing sizes (31 to 157 features). We collected the
time and space taken for each time step in the execution,
starting with zero knowledge. The results are shown in
Figure 4. Each of the graphs belongs to a different do-
main size. They show that the time per step approaches a
(very small, order of < 9 milliseconds) limit as the exe-
cution proceeds. The time that is taken to perform SLAF
of different domains grows linearly with domain size.

6

)
c
e
s

i
l
l
i

m

(
 

p
e

t
s
 
r
e
p

 

e
m

i
t

10
9
8
7
6
5
4
3
2
1
0

Time per Step

31 features
43 features
57 features
73 features
91 features
111 features
133 features
157 features

0

3 0 0

7 0 0

1 1 0 0

1 5 0 0

1 9 0 0

2 3 0 0

2 7 0 0

3 1 0 0

3 5 0 0

3 9 0 0

number of steps

Figure 4: AE-STRIPS-SLAF in the blocks world.

Also, [Hlubocky and Amir, 2005] has included a mod-
iﬁed version of this algorithm in their architecture and
tested it on a suite of adventure-game-like virtual envi-
ronments that are generated at random. These include ar-
bitrary numbers of places, objects of various kinds, and
conﬁgurations and settings of those. There, an agent’s
task is to exit a house, starting with no knowledge about
the state space, available actions and their effects, or
characteristics of objects. Their experiments show that
the agent learns the effects of its actions very efﬁciently.
This agent makes decisions using the learned knowledge,
and inference with the resulting representation is fast (a
fraction of a second per SAT problem in domains includ-
ing more than 30 object, modes, and locations).

7 Comparison with Related Work
HMMs [Boyen and Koller, 1999; Boyen et al., 1999;
Murphy, 2002; Ghahramani, 2001] can be used to es-
timate a stochastic transition model from observations.
Unfortunately, HMMs require an explicit representation
of the state space, and our smallest domain (31 features)
requires a transition matrix of (231)2 entries. This pre-
vents initializing HMMs procedures on any current com-
puter.

Structure learning approaches in Dynamic Bayes Nets
(DBNs and also fHMMs) use EM and additional ap-
proximations (e.g., using factoring, variation, or sam-
pling), and are more tractable. However, they are still
limited to small domains (e.g., 10 features [Ghahramani
and Jordan, 1997; Boyen et al., 1999]), and also have un-
bounded errors in discrete deterministic domains, so are
not usable in our settings.

Reinforcement Learning (RL) approaches compute a
mapping between world states and preferred actions.
They are highly intractable in partially observable do-
mains [Kaelbling et al., 1998], and approximation (e.g.,
[Kearns et al., 2000] is practical only for small domains
(e.g., 10-20 features) with small horizon time T .

In contrast to HMMs, DBNs, and RL, our algorithms
are exact and tractable in large domains (> 300 features).
We take advantages of properties common to discrete do-
mains, such as determinism, limited effects of actions,
and observed failure.

Previous work on learning action models assumes
fully observable deterministic domains. These learn
parametrized STRIPS actions using, e.g., version spaces
[Gil, 1994; Wang, 1995], general classiﬁers [Oates and
Cohen, 1996], or hill-climbing ILP [Benson, 1995]. Re-
cently, [Pasula et al., 2004] gave an algorithm that learns
stochastic actions with no conditional effects. [Schmill et
al., 2000] approximates partial observability by assum-
ing that the world is fully observable. We can apply these
to partially observable learning problems (sometimes) by
using the space of belief states instead of world states, but
this increases the problem size to exponentially, so this is
not practical for our problem.

8 Conclusions
We presented a framework for learning the effects and
preconditions of deterministic actions, and showed that
in several common situations this can be done exactly in
time that is polynomial (sometime linear) in the number
of time steps and features. We can add bias and com-
pute an exact solution for large domains (hundreds of
features), in many cases.

The results that we presented are promising for many
applications, including reinforcement learning, agents in
virtual domains, and HMMs. Already, this work is being
applied to autonomous agents in adventure games, and
exploration is guided by the transition belief state that
we compute and information gain criteria. In the future
we plan to extend these results to stochastic domains, and
domains with continuous features.

9 Acknowledgements
I wish to thank Megan Nance for providing the code and
samples for the generator. I also wish to acknowledge
support from DAF Air Force Research Laboratory Award
FA8750-04-2-0222 (DARPA REAL program).

References
[Amir and Russell, 2003] Eyal Amir and Stuart Russell. Log-

ical ﬁltering. In IJCAI ’03, pages 75–82. MK, 2003.

[Benson, 1995] Scott Benson.

Inductive learning of reactive

action models. In Proc. ICML-94, 1995.

[Boyen and Koller, 1999] Xavier Boyen and Daphne Koller.
Approximate learning of dynamic models.
In Michael S.
Kearns, Sara A. Solla, and David A. Kohn, editors, NIPS
1998, pages 396–402. Cambridge: MIT Press, 1999. Avail-
able at http://www.cs.stanford.edu/ xb/nips98/.

[Boyen et al., 1999] Xavier Boyen, Nir Friedman, and Daphne
Koller. Discovering the hidden structure of complex dy-
namic systems. In Proc. UAI ’99, pages 91–100. MK, 1999.

7

[Dechter, 1999] Rina Dechter. Bucket elimination: A unify-
ing framework for reasoning. Artiﬁcial Intelligence, 113(1–
2):41–85, 1999.

[Eiter and Gottlob, 1992] T. Eiter and G. Gottlob. On the com-
plexity of propositional knowledge base revision, updates,
and counterfactuals. AIJ, 57(2-3):227–270, 1992.

[Fikes et al., 1972] R. Fikes, P. Hart, and N. Nilsson. Learning

and executing generalized robot plans. AIJ, 3, 1972.

[Ghahramani and Jordan, 1997] Z. Ghahramani and M. I. Jor-
dan. Factorial Hidden Markov Models. Machine Learning,
29:245–275, 1997.

[Ghahramani, 2001] Zoubin Ghahramani. An introduction to
Hidden Markov Models and Bayesian networks. Intl J. of
Pattern Recog. and AI, 15(1):9–42, 2001.

[Gil, 1994] Yolanda Gil. Learning by experimentation: Incre-
mental reﬁnement of incomplete planning domains. In Proc.
ICML-94, pages 10–13, 1994.

[Hajishirzi and Amir, 2005] Hannaneh Hajishirzi and Eyal
Amir. Tractable exact inference in some dynamic Bayesian
networks. In submitted for publication, 2005.

[Hlubocky and Amir, 2005] Brian Hlubocky and Eyal Amir.
Logic-based approach to decision making in expanding
worlds. In submitted for publication, 2005.

[Kaelbling et al., 1998] L. P. Kaelbling, M. L. Littman, and
A. R. Cassandra. Planning and acting in partially observ-
able stochastic domains. AIJ, 101:99–134, 1998.

[Kearns et al., 2000] M. Kearns, Y. Mansour, and A. Y. Ng.
Approximate planning in large pomdps via reusable trajec-
tories. In Proc. NIPS’99, pages 1001–1007, 2000.

[Littman, 1996] M. L. Littman. Algorithms for sequential de-

cision making. PhD thesis, Brown U., 1996.

[McCarthy, 1986] John McCarthy. Applications of Circum-
scription to Formalizing Common Sense Knowledge. Arti-
ﬁcial Intelligence, 28:89–116, 1986.

[Murphy, 2002] K. Murphy. Dynamic Bayesian Networks:
Representation, Inference and Learning. PhD thesis, UC
Berkeley, 2002.

[Oates and Cohen, 1996] T. Oates and P. R. Cohen. Searching
for planning operators with context-dependent and proba-
bilistic effects. In Proc. AAAI ’96, 1996.

[Pasula et al., 2004] H. M. Pasula, L. S. Zettlemoyer, and L. P.
Kaelbling. Learning probabilistic relational planning rules.
In Proc. ICAPS’04, 2004.

[Schmill et al., 2000] M. D. Schmill, T. Oates, and P. R. Co-
hen. Learning planning operators in real-world, partially
observable environments. In Proc. AIPS’00, 2000.

[Simon and del Val, 2001] Laurent Simon and Alvaro del Val.

Efﬁcient consequence-ﬁnding. In IJCAI ’01, 2001.

[Wang, 1995] Xuemei Wang. Learning by observation and
practice: an incremental approach for planning operator ac-
quisition. In Proc. ICML-95, pages 549–557. MK, 1995.


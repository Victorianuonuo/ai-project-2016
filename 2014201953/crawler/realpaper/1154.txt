Identifying Expressions of Opinion in Context∗

Eric Breck and Yejin Choi and Claire Cardie

Department of Computer Science

{ebreck,ychoi,cardie}@cs.cornell.edu

Cornell University

Abstract

While traditional information extraction systems
have been built to answer questions about facts,
subjective information extraction systems will an-
swer questions about feelings and opinions. A cru-
cial step towards this goal is identifying the words
and phrases that express opinions in text. Indeed,
although much previous work has relied on the
identiﬁcation of opinion expressions for a variety
of sentiment-based NLP tasks, none has focused
directly on this important supporting task. More-
over, none of the proposed methods for identiﬁca-
tion of opinion expressions has been evaluated at
the task that they were designed to perform. We
present an approach for identifying opinion expres-
sions that uses conditional random ﬁelds and we
evaluate the approach at the expression-level us-
ing a standard sentiment corpus. Our approach
achieves expression-level performance within 5%
of the human interannotator agreement.

1 Introduction
Traditional information extraction tasks can be viewed as be-
ginning with a set of questions about facts, such as who?,
where?, or how many?. Researchers then build systems to ex-
tract the answers to these questions from text [MUC7, 1998;
NIS, 2005; DUC2005, 2005]. More recently, researchers
have investigated the construction of language-processing
systems that extract answers to subjective questions, such as
how does X feel about Y? (see Section 4). Intelligence ana-
lysts, for example, could use such opinion-oriented systems
to monitor trouble spots around the world while marketing re-
searchers could use them to understand public opinion about
their product.

As with factual information extraction and question an-
swering, subjective information extraction and question an-
swering will require techniques to analyze text below the sen-
tence level. For example, Stoyanov et al. [2005] show that
identifying opinion expressions is helpful in localizing the an-
swers to opinion questions.

∗This work was supported by the Advanced Research and De-
velopment Activity (ARDA), by NSF Grants IIS-0535099 and IIS-
0208028, and by gifts from Google and the Xerox Foundation.

Consider the following sentences, in which we denote two
kinds of opinion expression in boldface and italic (described
below).

1: Minister Vedrine criticized the White House reaction.

2: 17 persons were killed by sharpshooters faithful to the

president.

3: Tsvangirai said the election result was “illegitimate” and

a clear case of “highway robbery”.

4: Criminals have been preying on Korean travellers in

China.

To understand, extract, or answer questions about the opin-
ions in these sentences a system must (minimally) determine
the basic attributes of the opinion: Is its polarity positive, neg-
ative, or neutral? With what strength or intensity is the opin-
ion expressed: mild, medium, strong or extreme? Who or
what is the source, or holder, of the opinion? What is its tar-
get, i.e. what is the opinion about? The opinion expressions
marked in the sentences above are the key to answering these
questions. In particular, the marked phrases denote the po-
larity of the opinion: for example, “criticized” and “faithful
to” (examples 1 and 2) denote negative and positive attitudes,
respectively. The opinion expressions also often provide lin-
guistic anchors for the automatic extraction of the source and
target of the opinion. The predicate “criticized”, for exam-
ple, organizes the semantic roles that denote the source of the
opinion (the agent role = “Minister Vedrine”) and the target
of the opinion (the object/theme role = “White House reac-
tion”). Similarly, the opinion expression “faithful to” orga-
nizes the semantic roles associated with the source (the agent
role = “sharpshooters”) and the target (the object/theme role
= “the president”) of the opinion in example 2.

Wiebe et al. [2005] distinguish two types of opinion ex-
pressions, and we follow their deﬁnitions here. Direct subjec-
tive expressions (DSEs), shown in boldface, are spans of text
that explicitly express an attitude or opinion. “Criticized” and
“faithful to” (examples 1 and 2), for example, directly denote
negative and positive attitudes towards the “White House re-
action” and “the president”, respectively. Speech events like
“said” in example 3 can be DSEs if they express subjectivity.
In contrast, expressive subjective elements (ESEs), shown in
italics, are spans of text that indicate, merely by the speciﬁc
choice of words, a degree of subjectivity on the part of the
speaker. The phrases “illegitimate” and “highway robbery”,

IJCAI-07

2683

IOB . . . faithful/B to/I the/O president/O ./O
. . . faithful/I to/I the/O president/O ./O
IO

Figure 1: How to encode the class variable: The IOB method
and the IO method. IO is used in this paper.

for example, indirectly relay Tsvangirai’s negative opinion of
“the election result” (example 3), and the use of “preying on”
(instead of, say, “mugging”) indicates the writer’s sympathy
for the Korean travellers in example 4.

While some previous work identiﬁes opinion expressions
in support of sentence- or clause-level subjectivity classiﬁ-
cation [Riloff and Wiebe, 2003; Wiebe and Wilson, 2002],
none has directly tackled the problem of opinion expression
identiﬁcation, developed methods for the task, and evaluated
performance at the expression level. Instead, previous work
in this area focuses its evaluation on the sentence-level sub-
jectivity classiﬁcation task.

In this work, we treat the identiﬁcation of opinion expres-
sions as a tagging task, and use conditional random ﬁelds to
address this problem. For comparison, we chose two lists
of clues to subjectivity from previous work [Wilson et al.,
2005b; Wiebe and Riloff, 2005]. These clues have previously
been evaluated only for their utility in clause- or sentence-
level classiﬁcation. Here we interpret the clues as expressions
of opinion and compare them to our results. We achieve F-
measures of 63.4% for ESEs and 70.6% for DSEs, within 5%
of the human interannotator agreement for DSEs and within
10% for ESEs.

The rest of this paper proceeds as follows. In Section 2, we
discuss our approach to identifying direct subjective expres-
sions and expressive subjective elements. In Section 3, we
present experimental results. In Section 4, we present related
work and in Section 5, we conclude and discuss future work.

2 The approach

As the example sentences in Section 1 suggest, identifying
subjective expressions is a difﬁcult task. The expressions can
vary in length from one word to over twenty words. They
may be verb phrases, noun phrases, or strings of words that
do not correspond to any linguistic constituent. Subjectivity
is a realm of expression where writers get quite creative, so no
short ﬁxed list can capture all expressions of interest. Also,
an expression which is subjective in one context is not al-
ways subjective in another context [Wiebe and Wilson, 2002].
Therefore, we present in this section our machine learning ap-
proach for the identiﬁcation of direct subjective expressions
and expressive-subjective elements. We treat the task as a tag-
ging problem, and use conditional random ﬁelds [Lafferty et
al., 2001]1. Below we discuss the encoding of the class vari-
able (Section 2.1), the features (Section 2.2), and the learning
method (Section 2.3).

1CRFs allow the use of a large, diverse set of features, while
offering the choice of modeling individual tokens or sequences.
Margin-based methods would be another natural option, but pilot ex-
periments on development data suggested the performance of SVMs
was inferior to CRFs for this task.

2.1 The class variable

A common encoding for extent-identiﬁcation tasks such as
named entity recognition is the so-called IOB encoding. Us-
ing IOB, each word is tagged as either Beginning an entity,
being In an entity (i.e. an opinion expression), or being Out-
side of an entity (see Figure 1). While we initially used this
encoding, preliminary investigation on separate development
data revealed that a simpler binary encoding produces better
results. We suspect this is because it is rarely the case in our
data that two entities are adjacent, and so the simpler model
is easier to ﬁt. Thus, we tag each token as being either In an
entity or Outside of an entity. When predicting, a sequence of
consecutive tokens tagged as In constitutes a single predicted
entity.

2.2 Features

In this section, we describe the features used in our model.
We include features to allow the model to learn at various lev-
els of generality. We include lexical features to capture spe-
ciﬁc phrases, local syntactic features to learn syntactic con-
text, and dictionary-based features to capture both more gen-
eral patterns and expressions already known to be opinion-
related. The same feature set is used for identifying both
types of subjective expression. For pedagogical reasons, we
present the features as categorically valued, but in our model
we encode all features in binary (i.e. a feature (f, v) is 1 for
a token t if f (t) = v, and 0 otherwise).

Lexical features We include features lexi, deﬁned to be the
word at position i relative to the current token. We include
lex−4, lex−3, . . . lex4. These are encoded into about 18,000
binary features per position (i.e. the vocabulary size).

Syntactic features We include a feature part-of-speech, de-
ﬁned to be the part of speech of the current token according
to the GATE part-of-speech tagger [Cunningham et al., 2002]
(encoded into 45 binary features). We also include three fea-
tures prev, cur, and next, deﬁned to be the previous, current,
or following constituent type, respectively, according to the
CASS partial parser [Abney, 1996]2. These are encoded into
about 100 binary features each.

Dictionary-based features We include features from four
sources. We include a feature WordNet, deﬁned to be all
synsets which are hypernyms of the current token in the
WordNet hierarchy [Miller, 1995]. This is encoded into
29,989 binary features, many of which may be 1 for a given
token. We include a feature Levin, deﬁned to be the section
of Levin’s [1993] categorization of English verbs in which a
given verb appears, and a feature Framenet, deﬁned to be the
category of a word in the categorization of nouns and verbs
in Framenet3. Finally, we include a feature that speciﬁcally
targets subjective expressions. Wilson et al. [2005b] identify

2Cass is available for download at http://www.vinartus.

net/spa/.

3http://www.icsi.berkeley.edu/˜framenet/

IJCAI-07

2684

number of sentences
number of DSEs
number of ESEs
average length of DSEs
average length of ESEs

8297
6712
8640
1.86 words
3.33 words

Table 1: Statistics for test data

a set of clues as being either strong or weak cues to the sub-
jectivity of a clause or sentence. We identify any sequence
of tokens included on this list, and then deﬁne a feature Wil-
son that returns the value ‘-’ if the current token is not in any
recognized clue, or strong or weak if the current token is in
a recognized clue of that strength. This clue is encoded into
two binary features (the ‘-’ case is not encoded).

2.3 The learning method
We chose to use a linear-chain conditional random ﬁeld
model for all of our experiments, using the MALLET toolkit
[McCallum, 2002]. This discriminatively-trained sequence
model has been found to perform extremely well on tagging
tasks such as ours [Lafferty et al., 2001]. Based on pilot ex-
periments on development data, we chose a Gaussian prior of
0.25.

3 Experiments

In this section, we describe the data and evaluations used in
our experiments, describe the baselines we compare to, and
present our results.

3.1 Data
The MPQA corpus [Wiebe et al., 2005]4 consists of 535
newswire documents annotated with a variety of annotations
of interest for subjectivity research. In particular, all DSEs
and ESEs in the documents have been manually identiﬁed. In
this work, we used 135 documents for development of our
features and determination of parameters, and kept the re-
maining 400 blind for evaluation. All of our evaluation results
use 10-fold cross-validation on the 400 documents. Table 1
presents some statistics on these 400 documents.

3.2 Evaluation

|C∩P |

|P |

|C∩P |

and recall is

As with other information extraction tasks, we use preci-
sion, recall and F-measure to evaluate our method’s per-
, where
formance. Precision is
C and P are the sets of correct and predicted expression
spans, respectively. F is the harmonic mean of precision
2P R
P +R . Our method often identiﬁes expressions
and recall,
that are close to, but not precisely the same as, the man-
ually identiﬁed expressions. For example, for the expres-
sion “roundly criticized,” our method might only identify
“criticized.” We therefore introduce softened variants of
precision and recall as follows. We deﬁne soft precision

|C|

4Available at http://www.cs.pitt.edu/mpqa/. We use
version 1.1 of the corpus. Code and data used in our experiments
are available at http://www.cs.cornell.edu/˜ebreck/
breck07identifying.

|P |

|C|

as SP a = |{p|p ∈ P ∧ ∃ c ∈ C s.t. a(c,p)}|
and soft recall as
SRa = |{c|c ∈ C ∧ ∃ p ∈ P s.t. a(c,p)}|
, where a(c, p) is a
predicate true just when expression c “aligns” to expression
p in a sense deﬁned by a. We report results according to two
predicates: exact and overlap. exact(c, p) is true just when
c and p are the same spans - this yields the usual notions of
precision and recall. A softer notion is produced by the pred-
icate overlap(c, p), which is true when the spans of c and p
overlap5.

3.3 Baselines

For baselines, we compare to two dictionaries of subjectiv-
ity clues identiﬁed by previous work [Wilson et al., 2005b;
Wiebe and Riloff, 2005]. These clues were collected to help
recognize subjectivity at the sentence or clause level, not at
the expression level, but the clues often correspond to subjec-
tive expressions. Each clue is one to six consecutive tokens,
possibly allowing for a gap, and matching either stemmed or
unstemmed tokens, possibly of a ﬁxed part of speech. In the
following experiments, we report results of the Wiebe base-
line, which predicts any sequence of tokens matching a clue
from Wiebe and Riloff [2005] to be a subjective expression,
and the Wilson baseline, using similar predictions based on
clues from Wilson et al. [2005b]. When predicting DSEs, we
remove all clues from the list which never match a DSE in the
test data, to make the baseline’s precision as high as possible
(although since many potentially subjective expressions are
often not subjective, the precision is still quite low). We sim-
ilarly trim the lists when predicting the other targets below.
Apart from this trimming, the lists were not derived from the
MPQA corpus. Note that the higher-performing of these two
baselines, from Wilson et al. [2005b], was incorporated into
the feature set used in our CRF models6.

3.4 Results

Tables 2 and 3 present experimental results on identifying
direct subjective expressions and expressive subjective ele-
ments in several settings, as well as presenting the two base-
lines for comparison purposes. We experiment with two vari-
ants of conditional random ﬁelds, one with potentials (fea-
tures) for Markov order 1+0 (similar to the features in a hid-
den Markov model, labeled crf-1 in the tables), and one with
features only for order 0 (equivalent to a maximum entropy

5A potential issue with soft precision and recall is that the mea-
sures may drastically overestimate the system’s performance. A sys-
tem predicting a single entity overlapping with every token of a doc-
ument would achieve 100% soft precision and recall with the over-
lap predicate. We can ensure this does not happen by measuring the
average number of expressions to which each correct or predicted
expression is aligned (excluding expressions not aligned at all). In
our data, this does not exceed 1.13, so we can conclude these evalu-
ation measures are behaving reasonably.

6The CRF features based on the Wilson dictionary were based
on the entire dictionary, including clues not relevant for the partic-
ular problem being tested. Also, the choice to use only the Wilson
dictionary and not the Wiebe for features was made during develop-
ment of the model on a separate development dataset. So the model
tested was in no way developed using the test data.

IJCAI-07

2685

recall
method
45.692.4
Wiebe baseline
55.152.2
Wilson baseline
60.221.8
crf-1-DSE
crf-1-DSE&ESE 62.732.3
65.482.0
crf-0-DSE
crf-0-DSE&ESE 69.221.8

overlap
precision
31.102.5
30.731.9
79.343.2
77.993.1
74.853.5
72.163.2

F
36.972.3
39.441.9
68.442.0
69.512.4
69.832.4
70.652.4

recall
21.521.8
25.651.7
42.652.9
43.232.9
39.952.4
42.132.3

exact

precision
13.911.4
13.321.0
57.652.8
55.382.8
44.522.2
42.692.5

F
16.871.4
17.521.2
49.012.8
48.542.8
42.102.2
42.402.3

Table 2: Results for identifying direct subjective expressions. Superscripts designate one standard deviation.

recall
method
56.362.1
Wiebe baseline
66.102.6
Wilson baseline
46.364.1
crf-1-ESE
crf-1-DSE&ESE 48.793.2
61.223.4
crf-0-ESE
crf-0-DSE&ESE 63.463.3

overlap
precision
43.034.5
40.944.7
75.216.6
74.096.7
64.845.4
63.765.7

F
48.663.3
50.384.0
57.143.6
58.703.7
62.823.3
63.433.3

recall
15.091.1
17.231.9
15.111.7
15.581.1
18.311.7
18.961.4

exact

precision
9.911.6
8.761.5
27.282.3
26.182.1
17.113.0
16.792.5

F
11.921.4
11.561.6
19.351.5
19.460.8
17.612.2
17.741.8

Table 3: Results for identifying expressive subjective elements. Superscripts designate one standard deviation.

recall
method
51.592.0
Wiebe baseline
61.232.1
Wilson baseline
64.772.2
crf-1-DSE+ESE
crf-1-DSE&ESE 62.362.1
74.702.5
crf-0-DSE+ESE
crf-0-DSE&ESE 71.912.2

overlap
precision
61.354.6
58.484.7
81.334.4
81.904.1
71.644.5
74.044.5

F
55.992.8
59.733.1
72.032.2
70.742.2
73.052.8
72.882.6

recall
17.700.8
20.611.4
26.682.7
28.242.7
30.932.5
30.302.2

exact

precision
19.612.0
17.681.5
39.232.6
42.641.9
28.202.3
29.642.3

F
18.581.2
19.001.3
31.702.4
33.922.3
29.442.0
29.911.8

Table 4: Results for identifying expressions which are either DSEs or ESEs. Superscripts designate one standard deviation.
DSE&ESE indicates a model trained to make a three-way distinction among DSEs, ESEs, and other tokens, while DSE+ESE
indicates a model trained to make a two-way distinction between DSEs or ESEs and all other tokens.

feature set
base
base + Levin/FN
base + Wilson
base + Wilson + Levin/FN
base + WordNet
base + Wilson + WordNet
base + Levin/FN + WordNet
base + Levin/FN + WordNet + Wilson

recall
47.142.6
50.573.1
54.922.4
57.212.6
68.292.4
68.932.1
68.482.4
69.221.8

overlap
precision
70.914.4
70.514.1
70.734.0
70.794.1
71.823.5
72.063.3
71.873.3
72.163.2

F
56.603.0
58.863.3
61.812.9
63.263.0
70.002.8
70.452.6
70.132.8
70.652.4

recall
30.552.7
32.203.1
34.612.5
35.772.4
41.802.5
42.102.5
41.922.2
42.132.3

exact

precision
45.123.1
44.113.3
43.602.9
43.422.8
42.712.5
42.712.6
42.802.5
42.692.5

F
36.412.8
37.203.1
38.572.5
39.212.5
42.242.4
42.402.5
42.342.3
42.402.3

Table 5: Results for feature ablation for identifying DSEs. FN is the FrameNet dictionary features. “base” indicates the lexical
features and the syntactic features. The bottom line represents the same model as CRF-0-DSE&ESE in Table 2.

IJCAI-07

2686

model, labeled crf-0 in the tables). Orthogonally, we com-
pare models trained separately on each task (classifying each
token as in a DSE versus not or in an ESE versus not, labeled
just DSE or ESE in the tables) to models trained to do both
tasks at once (classifying each token into one of three classes:
in a DSE, in an ESE, or neither7, labeled DSE&ESE in the ta-
bles).

Because the baselines were not designed to distinguish be-
tween DSEs and ESEs, we run another set of experiments
where the two categories are lumped together. The rows la-
beled DSE&ESE use the models trained previously to dis-
tinguish three categories, but are here evaluated only on the
binary decision of opinion expression or not. The rows la-
beled DSE+ESE are trained to classify a token as I if it is in
either a DSE or ESE, or O otherwise. The results of these
experiments are reported in Table 4.

Finally, to determine the effect of the various dictionar-
ies, we examine all combinations of the various dictionaries -
WordNet, Framenet, Levin, and the clues from Wilson et al.
[2005b] (to save space, we combine the two smallest dictio-
naries, Framenet and Levin, into one). These results, on the
DSE task, are reported in Table 5.

3.5 Discussion
We note that the order-0 models outperform the order-1 mod-
els according to overlap F-measure and recall, but by exact
F-measure and either precision metric, the order-1 models are
superior. The creators of the dataset state “we did not attempt
to deﬁne rules for boundary agreement in the annotation in-
structions, nor was boundary agreement stressed during train-
ing.” [Wiebe et al., 2005, page 35]. For example, whether a
DSE ought to be annotated as “ﬁrmly said” or just “said” is
left up to the annotator. Therefore, we hypothesize that the
model with greater capacity (the order 0+1) may overﬁt to
the somewhat inconsistent training data.

The ablation results (in Table 5) indicate that the Word-
Net features are by far the most helpful. The other two dic-
tionary sets are individually useful (with the Wilson features
being more useful than the Levin/Framenet ones), but above
the WordNet features the others make only a small difference.
This is interesting, especially since the WordNet dictionary is
entirely general, and the Wilson dictionary was built speciﬁ-
cally for the task of recognizing subjectivity. Ablation tables
for the other two targets (ESEs and DSE&ESE) look similar
and are omitted.

In looking at errors on the development data, we found
several causes which we could potentially ﬁx to yield higher
performance. The category of DSEs includes speech events
like “said” or “a statement,” but not all occurrences of speech
events are DSEs, since some are simply statements of objec-
tive fact. Adding features to help the model make this distinc-
tion should help performance. Also, as others have observed,
expressions of subjectivity tend to cluster, so incorporating
features based on the density of expressions might help as
well [Wiebe and Wilson, 2002].

7A small number of tokens are manually annotated as being part
of both a DSE and an ESE. For training, we label these tokens as
DSEs, while for testing, we (impossibly) require the model to anno-
tate both entities.

Finally, we note that the interannotator agreement results
for these tasks are relatively low; 0.75 for DSEs and 0.72 for
ESEs, according to a metric very close to overlap F-measure8.
Our results are thus quite close to the human performance
level for this task.

4 Related work

Subjectivity research has been quite popular in recent years.
While ultimately research in lexicon building [Hatzivas-
siloglou and McKeown, 1997, e.g.], and classiﬁcation [Dave
et al., 2003, e.g.] may be relevant, we focus here on work in
extracting sub-sentential structure relevant to subjectivity.

Bethard et al. [2004] address the task of extracting propo-
sitional opinions and their holders. They deﬁne an opinion
as “a sentence, or part of a sentence that would answer the
question ‘How does X feel about Y?’ ” A propositional opin-
ion is an opinion “localized in the propositional argument”
of certain verbs, such as “believe” or “realize”. Their task
then corresponds to identifying a DSE, its associated direct
source, and the content of the private state. However, in each
sentence, they seek only a single verb with a propositional
argument, whereas we may identify multiple DSEs per sen-
tence, which may be multi-word expressions of a variety of
parts of speech.

Another group of related work looks at identifying a class
of expressions similar to the DSEs we identify [Wiebe et al.,
2003; Munson et al., 2005; Wilson et al., 2005a] 9. We can-
not compare our results to theirs because this previous work
does not distinguish between DSEs and objective speech ex-
pressions, and because the prior results only address ﬁnding
single word expressions.

Another area of related work is reputation or sentiment
analysis [Morinaga et al., 2002; Nasukawa and Yi, 2003].
This work is in the context of marketing research, and in-
volves identifying polarity and sentiment terminology with
respect to particular products or product categories. Their no-
tions of sentiment terms are related, but not identical, to ours.
However, they do provide evidence that working at the ex-
pression level is of interest to consumers of opinion-oriented
information extraction.

5 Conclusions and Future Work

Extracting information about subjectivity is an area of great
interest to a variety of public and private interests. We have
argued that successfully pursuing this research will require
the same expression-level identiﬁcation as in factual infor-
mation extraction. Our method is the ﬁrst to directly ap-
proach the task of extracting these expressions, achieving per-
formance within 5% of human interannotator agreement. In
the future, we hope to improve performance even further by

8Using the agr statistic, the interannotator agreement for ESEs
on the MPQA data is 0.72 [Wiebe et al., 2005, page 36], and for
DSEs is 0.75 (Theresa Wilson, personal communication). agr is the
arithmetic (rather than harmonic) mean of overlap recall and preci-
sion between two annotators.

9The category is referred to as an on or private state frame.

IJCAI-07

2687

the methods discussed earlier, and build on our expression-
level identiﬁcation towards systems that present the user with
a comprehensive view of the opinions expressed in text.

[Munson et al., 2005] M Arthur Munson, Claire Cardie, and
Rich Caruana. Optimizing to arbitrary NLP metrics using
ensemble selection. In HLT-EMNLP05, 2005.

Acknowledgements

The authors would like to thank Oren Kurland, Alexandru
Niculescu-Mizil, Filip Radlinski, and Theresa Wilson for
helpful comments on earlier versions of this paper.

References
[Abney, 1996] Steven Abney. Partial parsing via ﬁnite-state
Journal of Natural Language Engineering,

cascades.
2(4):337–344, 1996.

[Bethard et al., 2004] Steven Bethard, Hong Yu, Ashley
Thornton, Vasileios Hatzivassiloglou, and Dan Jurafsky.
Automatic extraction of opinion propositions and their
holders. In Working Notes of the AAAI Spring Symposium
on Exploring Attitude and Affect in Text: Theories and Ap-
plications, 2004. March 22-24, 2004, Stanford.

[Cunningham et al., 2002] Hamish Cunningham, Diana
Maynard, Kalina Bontcheva, and Valentin Tablan. GATE:
A framework and graphical development environment
for robust NLP tools and applications.
In Proceedings
of the 40th Anniversary Meeting of the Association for
Computational Linguistics (ACL ’02), Philadelphia, July
2002.

[Dave et al., 2003] Kushal Dave, Steve Lawrence,

and
David M Pennock. Mining the peanut gallery: Opinion
extraction and semantic classiﬁcation of product reviews.
In WWW2003, 2003.

[DUC2005, 2005] Proceedings of

the Document Under-
standing Workshop, Vancouver, B.C., Canada, October
2005. Presented at the HLT-EMNLP Annual Meeting.

[Hatzivassiloglou and McKeown, 1997] Vasileios Hatzivas-
siloglou and Kathleen R. McKeown. Predicting the se-
mantic orientation of adjectives. In ACL97, 1997.

[Lafferty et al., 2001] John Lafferty, Andrew McCallum,
and Fernando Pereira. Conditional random ﬁelds: Proba-
bilistic models for segmenting and labeling sequence data.
In ICML, 2001.

[Levin, 1993] Beth Levin. English Verb Classes and Alter-

nations. University of Chicago Press, 1993.

[McCallum, 2002] Andrew Kachites McCallum. MAL-
toolkit.

language

for

LET: A machine
http://mallet.cs.umass.edu, 2002.

learning

[Miller, 1995] George A. Miller. WordNet:
Communications of

database for English.
38(11):39–41, November 1995.

a lexical
the ACM,

[Morinaga et al., 2002] Satoshi Morinaga, Kenji Yamanishi,
Kenji Tateishi, and Toshikazu Fukushima. Mining product
reputations on the web. In KDD 2002, 2002.

[MUC7, 1998] Proceedings of the Seventh Message Under-
standing Conference (MUC-7). Morgan Kaufman, April
1998.

[Nasukawa and Yi, 2003] Tetsuya Nasukawa and Jeonghee
Yi. Sentiment analysis: capturing favorability using nat-
ural language processing. In Proceedings of the interna-
tional conference on Knowledge capture, 2003.

[NIS, 2005] NIST.

Proceedings of The Fourteenth Text
REtrieval Conference (TREC 2005), Gaithersburg, MD,
November 2005.

[Riloff and Wiebe, 2003] Ellen Riloff and Janyce M Wiebe.
Learning extraction patterns for subjective expressions. In
Proceedings of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, 2003.

[Stoyanov et al., 2005] Veselin Stoyanov, Claire Cardie, Di-
ane Litman, and Janyce Wiebe. Evaluating an opinion an-
notation scheme using a new multi-perspective question
and answer corpus. In Qu Shanahan and Janyce Wiebe,
editors, Computing Attitude and Affect in Text: Theory and
Practice. Springer, 2005.

[Wiebe and Riloff, 2005] Janyce Wiebe and Ellen Riloff.
Creating subjective and objective sentence classiﬁers from
unannotated texts.
In Proceedings of the Sixth Interna-
tional Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-2005). Springer-Verlag,
2005. Invited paper.

[Wiebe and Wilson, 2002] Jaynce Wiebe and Theresa Wil-
son. Learning to disambiguate potentially subjective ex-
pressions.
In Sixth Conference on Natural Language
Learning, Taipei, Taiwan, August 2002.

[Wiebe et al., 2003] J. Wiebe, E. Breck, C. Buckley,
C. Cardie, P. Davis, B. Fraser, D. Litman, D. Pierce,
E. Riloff, T. Wilson, D. Day, and M. Maybury. Recog-
nizing and Organizing Opinions Expressed in the World
Press. In Papers from the AAAI Spring Symposium on New
Directions in Question Answering (AAAI tech report SS-
03-07), 2003. March 24-26, 2003. Stanford.

[Wiebe et al., 2005] Janyce Wiebe, Theresa Wilson, and
Claire Cardie. Annotating expressions of opinions and
emotions in language. Language Resources and Eval-
uation (formerly Computers and the Humanities), 39(2-
3):165–210, 2005.

[Wilson et al., 2005a] Theresa Wilson, Paul Hoffmann,
Swapna Somasundaran, Jason Kessler, Janyce Wiebe,
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. OpinionFinder: A system for subjectiv-
ity analysis.
In Human Language Technology Confer-
ence/Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, Canada, October 2005.
Demo abstract.

[Wilson et al., 2005b] Theresa Wilson, Janyce Wiebe, and
Recognizing contextual polarity in
Paul Hoffmann.
phrase-level sentiment analysis.
In Proceedings of
Human Language Technologies Conference/Conference
on Empirical Methods in Natural Language Processing
(HLT/EMNLP 2005). Vancouver, Canada, 2005.

IJCAI-07

2688


Session  25  Hardware  and  Software 
for  A r t i f i c i al 

Intelligence 

BACKTRACKING IN MLISP2 

An Efficient  Backtracking Method tor LISP 

David Canfield Smith, Horace J.  Enes 

Computer  Science  Department 

Stanford  University 
Stanford, California 

to  the  desired  goal.  At  each  branching  point  in  the  tree,  a 
decision  must  be  made  as  to  which  alternative  to  try  next. 
Backtracking 
t he 
p r o g r a m m i ng  of  this  type  of  problem. 

s i m p l i fy 

designed 

to 

is 

A b s t r a ct 

An  efficient  backtracking  method 

in  the 
MLISP2  language,  is  described.  The  method  is  optimal  in  the 
following  senses: 

for  LISP,  used 

The 
necessary 
<1)  Only 
backtracking  system 
to 
require  less  than  ten  percent  of  the  execution  time  of  typical 
jobs. 

state 
routines  are  sufficiently  efficient 

information 

is  saved. 

(2)  Most  common  operations  —  fetching/storing  the  value  of  a 
variable  or  the  property  of  an  atom,  function  entry/exit  — 
take  no 
it.  This  is 
achieved  by  not  changing  the  way  values  are  stored. 

longer  with  backtracking  than  without 

If  backtracking  is  not  used,  an  insignificant  overhead  is 

(3) 
involved  in  maintaining  the  backtracking  capability. 

The  MLISP2  algorithm  and  philosophy  are  briefly  contrasted 
with 
those  of  several  existing  backtracking  systems,  with 
historical  comments  on  the  development  of  the  theory  of 
backtracking. 

A branching point  is called a "DECISION POINT" in this paper. 
Frequently,  insufficient  information  is  available  at  decision 
points  to  determine  which  branches  will  lead  to the  goal.  If  a 
wrong  branch  is  tried,  it  "FAILs";  that  is,  the  program  returns 
to  the  decision  point,  pretends  that  the  incorrect  branch  had 
never  been  attempted,  and  selects  another  alternative,  This 
process  is  called  "BACKTRACKING."  It  is  algorithmic  because 
it  covers 
tree;  every  branch  at  every 
branching point  will  eventually be tried in the worst case. 

the  entire  goal 

Several  languages have incorporated backtracking, but  none 
of  them  have incorporated exactly  the same features and none 
of  them  have  implemented  backtracking  in  exactly  the  same 
way.  The  differences  are  in  terms  of  objectives  and  in 
terms  of  methods.  We  shell  first  discuss  the  nature  and 
then 
the  origin  of  these  differences,  beginning  with  the 
different  implementation methods used 

Backtracking 4,5,6,7,8,9,10,11,12,13 has begun to be used in the past 

five  years  as  a  control  structure  for  programming  languages 
dealing  with  problems  in  artificial  intelligence.  This  paper  is 
an  attempt  to  contribute  to  a  better  understanding  of  what 
backtracking  is,  what  it  is  useful  for,  and  how to  implement  it 
efficiently.  Briefly,  backtracking  is  an  algorithmic  device  for 
solving problems expressible as a set of possible alternatives, 
called  a  goal  tree,  where  not  all  of  the  alternatives  will  lead 

Before  continuing,  we  should  mention  briefly 

that 
backtracking  has  been criticized in some quarters as being an 
inherently  bad  control  structure.'4  Criticisms  of  any  concept 
come  in two  flavors: (1)  "theoretical criticisms" of the concept, 
and  (2)  "pragmatic  criticisms"  of  machine  implementations  of 
the  concept.  The  criticisms  of  backtracking have  mostly fallen 
into  this  latter  category;  they  criticize  the  early,  pioneering 
systems  like  PLANNER.  More  recent  systems  like  ECL  and 
MLISP2  incorporate  enough  flexibility  to  answer  many  of  the 
published  objections  to  backtracking.  Finally,  drawing  on  our 
personal  experience,  we  wish  to  point  out  that  MLISP2  has 
been  used 
intelligence 
community  at  Stanford. 
It  has  shown  itself  to  be  versatile, 
easy to  use, and efficient  enough to be eminently practical. 

two  years  by 

the  artificial 

for 

677 

T wo  Views  of  Computations 

two  viewpoints,  which  we  shall  call 

All  existing  backtracking  methods  have  been  implemented 
from  one  of 
the 
"sequential"  view  and  the  "state"  view  of  backtracking.  These 
correspond  to 
the  way  computations  are  ordinarily  viewed 
The 
that  a 
computation  is  a  sequence  of  discrete  steps,  the  last  of  which 
yields 
result.  All  algorithms  consist  of  such 
sequences.  For  example,  the  Algol  statements 

computations  holds 

the  desired 

"sequential" 

view  of 

may  be  viewed  as  a  sequence  of  steps 
computation  of  a  value 
statements 
primitive  steps. 

to  the 
fourth 
themselves  consist  of  a  sequence  of  more 

leading 
third  and 

for  W.  The 

to  view  computations  is  in  terms  of  state 
Another  way 
transformations, 
the  "state"  approach.  The  "STATE  OF  A 
COMPUTATION"  is  defined  to  be  the  set  of  current  values  of 
all  variables.  The  program  counter,  stacks,  etc.  are  considered 
to  be  system  variables.  A  computation  is  a  sequence  of  state 
transformations,  the  result  of  which is  a state  representing the 
desired  computation.  Continuing  the  previous  example  but 
representing  the  state  of  the  machine  by  a  partially-specified 
tuple; 

the  computation  becomes 

Though  we  have  not  mentioned  the  intermediate  states  (e.g. 
3*Y),  a  complete  description  must  include  them. 

is  again  reached.  At  this  point,  Golomb  and 
decision  point 
its  original 
Floyd  agree,  everything  has  been 
condition,  and  processing  may  again  proceed  in  a  forward 
direction. 

reset 

to 

Each  command  [computation  step]  is  expanded 
into  one  or  more  commands,  some  of  which 
carry  out  the  effect  of  the  original  command  in 
the  nondeterministic  algorithm,  and  which  also 
stack 
the 
effect  of  the  command  when  backtracking  is 
needed, 
the 
backtracking  by  undoing  all  the  effects  of  the 
first  set.  [4,  p.638] 

information 

required 

reverse 

others 

while 

carry 

out 

to 

This  has  several  advantages.  Expanding  each  computation 
step  is  a  mechanical  process  that  can  be  added  to  existing 
compilers  and  interpreters.  The  same  code  is  generated  as 
before  to  carry  out  the  forward  steps,  plus  some  additional 
code  to  save  the  backtracking  information.  Furthermore,  the 
facilitates  adding  backtracking  to  a  system. 
modular  design 
The 
type  can  be  added  and 
debugged  separately. 

inverse  of  each  statement 

'  But 

P L A N N ER 

tree  that  cannot  affect 

the  argument  goes,  why  bother 

the  sequential  ("inverse  statement") 
implementing  backtracking  soon  changed 
its 
m e t h od  of 
o b j e c t i v e s. 
It  was  observed  that  not  ell  statements  need 
to  be  undone.  Sometimes  variables  will  be  set  on  one  branch 
the  execution  of  other 
in  the  goal 
branches.  Therefore, 
to 
backtrack 
these  variables?  This  Is  the  approach  taken  by 
PLANNER,  a  LISP-based  pat tern-matching  system  incorporating 
features  such  as  goal-
several  powerful  non-procedural 
directed 
to 
(Actually,  PLANNER  has  not  been 
implement  these  features. 
fully 
this  discussion  really  deals  with  a 
subset 
implemented  by  Sussman, 
Winc-grad  and  Charniak.  Nevertheless,  we  shall  continue  to 
use 
the  name  PLANNER.)  Not  all  statement-types  are 
undoable.  Functions  that  may  be  backtracked  are  designated 
by  the  letters  "TH"  on  the  front  of  their  names.  For  example, 

called  MICRO-PLANNER 

implemented  yet; 

PLANNER  uses 

computation. 

backtracking 

Sequential  Backtracking 

cannot  be  backtracked,  but 

Theoretical  Systems  5,6  These  two  views  of 
computations  have  affected  the  theory  and  development  of 
backtracking  systems.  Golomb  and  Baumert  were  among  the 
first  to  explore  the  computational  applications  of  backtracking. 
They  took  the  state  view  of  computations  and  demonstrated 
that  backtracking  could  be  used  to  reduce  the  search  of  the 
space  of  solution  states.  Floyd's  early  investigations  took  the 
sequential  approach.  He  proposed  having  an  "inverse"  for 
including  assignments, 
each  statement 
conditionals, 
I/O. 
Backtracking  then  consists  of  stopping  the  forward  execution 
of  statements  end  executing 
the 

Inverse  statements  until 

the  computation, 

subroutine 

entries 

exits, 

and 

and 

in 

678 

can  be.  Similarly,  THPUTPROP,  THREMPROP,  THASSERT  end 
THERASE  are  functions  for  changing  the  data  base  that  are 
backtrackable  whereas  PUTPROP  and  REMPROP  are  not.  But 
function  entries  and  exits  are  always  undoable;  i.e.  PLANNER 
will  always  restore  the  control  environment  when  a  failure 
happens,  but 
It  will  not  restore  the  values  of  variables  or 
properties  of  atoms  unless  explicitly  instructed  to.  This  gives 
the  programmer  more  control  over  backtracking,  and  enables 
him  to  eliminate  the  saving  of  superfluous  information  On  the 
other  hand 
to  k n ow  what 
information  mutt  be  saved,  shifting  the  burden  of  backtracking 

the  programmer 

requires 

it 

to  the  programmer's  shoulders,  and  in  the  end  making  such 
systems  harder  to  use.  A  further  disadvantage  is  that  the 
programmer  may  overspeolfy  the  amount  of  information to 
be  saved;  it  is  frequently  difficult  and  non-intuitive  to decide 
what  is the  optimum amount  of information to save. 

the 

issues 
is 

a l t e r n a t i v es 

This  implement  ion  of  backtracking  has  had  a  profound  and 
confusing  effect  on  its  objectives.  A  distinction  has  come  to 
be  made  between  automatic control  and data backtracking. 
1  In  the  authors'  view, this distinction is  a  negative aspect  of 
the  theory  of  backtracking  that  has  obscured  rather  than 
involved.  The  main  purpose  of 
clarified 
t ry 
to  enable  a  program 
b a c k t r a c k i ng 
if 
l a t er 
unsuccessful  ones  h ad  never  been 
e a r l i er 
a t t e m p t ed 
If  the  state  of  the  computation  is  not  reset  at 
the  beginning  of  each  alternative,  then  the  alternatives  will 
behave  differently  depending  on  the  order  in  which they  are 
tried.  This 
is  a  red  herring  that  merely  obscures  the 
understanding 
the 
programmer  wants  some 
information  preserved  when  an 
alternative  fails,  he  should  explicitly  say  so,  and  any  good 
nondeterministic  programming  language  should  provide  him 
with  facilities  for doing so. 

to 
tree  as 

nondeterministic 

programs. 

goal 

of 

If 

in 

a 

State  B a c k t r a c k i ng 

to 

the  saved  state 

Corresponding  to the  "state" view  of computations, there is 
a  "state"  view  of  backtracking.  This  approach  may  be 
summarized  as  follows:  When a decision point  is encountered 
during  a  computation,  the  complete  state  of  the  machine  (the 
current  values  of  all  variables  including  system  variables)  is 
"saved."  When  a  failure  occurs,  the  state  of  the  machine  is 
"restored" 
in  one  operation,  and 
computation proceeds along another alternative at the decision 
point. 
there  are  no  more  alternatives,  the  failure  is 
propagated  to  the  preceding decision point.  The state method 
has  led  to  a  more  faithful  adherence  to  Floyd's  and  Golomb's 
view  of  backtracking,  namely  that  everything  is  restored  to 
the  way 
incorrect  alternative  was 
attempted. 
In  addition  to  the  control  environment  being 
always  restored,  as in PLANNER, the data environment  is also 
always  restored. 

it  was  before 

the 

If 

The  state-saving  and  -restoring  approach  (state  method) 
has  one  main  advantage  over  the  inverse-statement  approach 
(sequential  method):  the  process  of  failing  and  trying another 
alternative  is  usually  more  efficient.  In the inverse-statement 
approach,  statements  may  be  backtracked  unnecessarily.  For 
example,  suppose  a  decision point  occurs, and then the value 
of  a  variable  is  changed  100 times  before  a  failure happens. 
Each  of  these  100  stores  to  the  variable  will  have  to  be 
undone  by  an  "inverse  store,"  a  restoration  of  the  previous 
value  of  the  variable.  But  each  inverse  store  will  just  undo 
the  effect  of 
inverse  store 
executed  effectively  undoes them  all.  99  of the  100 inverse 
stores  will  be  wasted. 
In  the  state-saving  approach,  failure 
transfers  control  directly  back  to  the  decision  point.  Once 

the  previous  one;  the  last 

679 

there,  the  entire  state  is  restored  in  one  operation. 
MLISP2, this operation is a very rapid one. 

In 

E CL  10  ECL  is  a  blend  of  the  sequential  and  state 
methods. 
It  has  an  NASSIGN  operator  corresponding  to 
PLANNER'S  THSETQ,  but  it  also  uses  a  "backup  stack"  much 
like  MLISP2's  state  stack  (see  below). 
In  an  interesting 
variation on MLISP2's stack saving algorithm, the working stack 
is  saved  on  the  backup  stack  when  functions  are  about  to 
e x i t, rather than when decision points are executed.  In some 
cases  this  will  result  in  less  information  being  saved  than  in 
MLISP2.  However,  it  has  the  peculiar  property  that  when  a 
failure  occurs,  variables  not  explicitly  saved  will  be  restored 
to  the  l a st  values  they  had  in  the  fuction,  not  to  the  values 
theny  had  when  the  decision  point  was  set.  As  in  PLANNER 
only  the  control  environment  is  automatAically  restored.  The 
programmer  has  the  responsibility  for  explicitly  saving  (via 
NASSIGN)  variable  values  he  wants  restored  correctly. 
In 
most  other  respects  the  ECL  algorithm  is  very  similar  to 
MLISP2's. 

to 

the 

language, 

S A IL  3  SAIL  has  taken  a  different  approach.  Rather than 
the  SAIL 
add  a  context  mechanism 
implementers  added  a  coroutine  structure.  Coroutining  is 
logically  equivalent  to backtracking.  The set  of alternatives at 
a decision  point  is represented by  a set  of coroutines.  Failure 
is  achieved  by  deactivating  the  coroutine  representing  the 
current  path  and  activating  another  one. 
In  SAIL,  the 
programmer  is  required  to  do  all  state  saving  himself.  Two 
new  statements have been added to do this: 

REMEMBER <list of variables> IN <context> 

RESTORE <list  of variables> FROM <context> 

The  special  word  ALL  may  be  used  instead  of  the  list  of 
variables  and  means  "do  the  operation  on  all  the  variables 
p r e v i o u s ly  remembered  in  that  context,"  Thus  SAIL  is 
similar  to  PLANNER  and  ECL  in  that  it  automatically  restores 
the  control  environment  but  not  the  data  environment;  the 
data  must  be explicitly restored.  If a programmer really wants 
SAIL  to  behave  like  a  state-saving  system,  he must  at  every 
decision  point  do  a  REMEMBER  of  every  variable  (including 
every  array  element)  in  the  system,  and  at  failure  do  a 
RESTORE  ALL. 
It  then  becomes  equivalent  to  P0P-2  (see 
below).  One  seldom  uses  SAIL  in  this  manner,  however;  the 
REMEMBER/RESTORE  primitives  are  intended  to  give  the 
programmer  a  convenient  way to keep certain data from being 
destroyed  during  processing  in  hypothetical  situations,  not  to 
implement  full  backtracking. 

While coroutining is logically equivalent to backtracking, it is 
neither  physically  nor  conceptually  the  same.  The  internal 
structures  are  quite  different;  for  example,  there  is  no 
"backup  stack"  in  coroutine  systems.  Conceptually  the  main 
difference  seems 
issue  of  whether 
information  should  be  saved  automatically  or  explicitly.  The 
emphasis  on  s i m p l i f y i ng  programming  has 
led  most 
things 
backtracking  systems 

large  number  of 

to  center  on 

to  do  a 

the 

automatically  —  saving  and  restoring  data,  and  control 
management  —  whereas coroutines have  been regarded  as a 
language tool, like iteration, that should be invoked explicitly. 

M L I S P2  l3  In  MLISP2, the  system automatically  saves the 
necessary  information.  The  advantages  of  automatic  state 
saving  are  that  ft  is  simple  to  use,  and  it  eliminates  human 
error.  The  programmer  never  has  to  figure  out  what  values 
to  save;  indeed,  he  may often be  unaware that  state  saving is 
going  on. 
In  addition  to  eliminating  a possible source  of bugs 
(not  saving  enough  information),  possible  inefficiencies (saving 
too  much  information)  are also prevented.  Programming in this 
type  of  a  system  is  not  much  more  difficult  than  programming 
in a system without  backtracking. 

P O P2  2  The  efficiency  of  the  state  approach  depends  on 
the  efficiency  of  saving  and  restoring 
the  state.  The 
straightforward  but  inefficient  method  is  to copy  the  value  of 
every  currently-active  variable  into  some  area  of  memory  or 
secondary  storage  every  time a decision point  is encountered. 
Thereafter,  no  further  attention is  paid to changes  in  variable 
values.  When  a  failure  occurs, the  saved values  are reloaded 
from  storage,  and  every  variable  is  restored  to  its  old  value 
regardless  of  whether  or  not  its  value  had  changed.  This  is 
the  method  used  by POP-2  when backtracking was introduced 
into  that  language. 

Q A4  12  QA4 is much more efficient  in its state saving than 
POP-2.  At  decision  points  not  much  happens  except  that  a 
context  number  is  incremented.  Thereafter,  when a  value  is 
stored  in  a  variable, the context  number is associated with the 
new  value. 
In  fact  nothing  much  happens  at  failures  either, 
except  that  the  context  number is  decremented and a jump is 
executed!  This  is  faster  than  any  other  system's  failure. 
Furthermore,  In  QA4  it  is  possible  to  modify  or  restore  a 
backtracking  context  anywhere  in  the  goal  tree  and  then 
resume 
in  QA4  the  penalty  is  paid  in 
referencing  a  variable's  value  (or  any  atom's  property).  All 
properties  including  the  VALUE  property  are  stored  in  a 
structured  AL1ST  under  each  atom.  This  ALIST  must  be 
searched  every  time  a  variable  or  property  is  referenced 
The  net  effect  is that  fetches  and stores slow down by one to 
two  orders  of  magnitude.  MLISP2  stores  variables  and 
properties  in  such  a  way  that  fetches take  virtually no longer 
with  backtracking  than  without,  and  stores  are  slower  only in 
some cases. 

from  there.  But 

The  MLISP2  Algorithm 

MLISP2  is  an  extensible  language  using  backtracking  and 
based  on  the  Stanford  LISP  1.6  system  "  on  the  PDP-10 
computer.  The  language  is  intended to be  a  practical  tool  for 
implementing product/on compilers and translators, as well as 
being  a  research  tool.  ML1SP2  has  been  operational  for  two 
years, which has  given us a good deal  of experience with the 
practical  problems  of production backtracking systems.  A logic 
compiler  (LCF  9),  a deduction  system (FOL), an English parser, 

680 

an  English  to  French  translator,  an  ALGOL  compiler,  and  the 
MLISP2  translator  itself  are  major  systems  that  have  been 
written 
in  MLISP2.  This  experience  has  led  us  to  the 
conclusion  that  at  this  point  not  just  one  more  feature,  but 
e f f i c i e n cy  and  s i m p l i c i ty  are  the  fundamental  needs  to 
make  backtracking practical. 

The  philosophy  of  MLISP2 

is  that  backtracking  should 
s i m p l i fy  programming.  The  programmer  should  never  be 
concerned  with  explicitly  saving  information  just  so  that 
statements  can  be  backtracked, and he should seldom have to 
rewrite  routines so that  they can be included in a backtracking 
program. 
(In  PLANNER,  existing  routines  may  have  to  be 
rewritten  by  changing  SETQs  to  THSETQs,  PUTPROPs  to 
THPUTPROPs,  etc,  before 
in  a 
nondeterministic  program.  The  inverse  changes  have  to  be 
made  when  a  backtracking 
in  a 
deterministic  system.)  Such considerations  have  nothing to do 
with  problem  solving. 

included 

they  can  be 

included 

routine 

is 

MLISP2 

implements  backtracking  by  modifying  the  LISP 
interpreter  and  the  LAP  assembly  program.  The  principle 
change  is  to  the  BIND  code,  by  which  LAMBDAs  and  SETQs 
bind  their  variables  In  interpreted  functions. 
In  addition,  a 
"STATE  STACK"  is  maintained  on  which  information  from  the 
normal  pushdown  stack  (P)  and  the  special  pushdown  stack 
(SP)  is  saved. 
In  the  Stanford  LISP  1.6  system,  all  control 
information  and  the  values  of  local  variables  in  compiled 
functions  are  put  on  the  P  stack.  Therefore,  saving  the 
contents  of  the  P  stack will  save this  information,  The values 
of  all  variables  in  interpreted  functions  and  of  variables  used 
free  in compiled  functions  are put  on  the property lists  of the 
variables.  These  will  be  called  "property  list  variables." 
Saving the  property lists of atoms will  save the current  values 
of  property  list  variables,  as  well  as  any  other  property  list 
changes.  Recursive  calls  on  functions  using  property  list 
variables  stack the old variable values on the SP stack before 
rebinding,  so  that  saving  the  SP  stack  will  save  their  old 
values.  These  three  structures  —  the P  stack, the  SP stack, 
and  the  property  lists  of  atoms,  —  together  with  a  control 
point  to  transfer  to  when  a  failure  occurs,  completely  specify 
the  state  of  any  LISP  l.C  imputation. 

This  is  a  lot  of  information  to  handle  at  once,  causing many 
implementers  to  shy  away  from  the  state-saving  approach. 
But  the  volume may be reduced by saving just  the incremental 
state,  just  the  changes  to  the  state  that  have  occurred  since 
the  last  decision  point.  The  work  load  at  decision  points  may 
be  further  reduced  by  distributing  the  state  saving throughout 
the  computation. 

When  an  MLISP2  decision  point  is  encountered,  there  are 

three  major effects on  the system: 

1.  A  unique positive integer is associated with each dynamic 
decision  point.  This  number is called the "CONTEXT NUMBER," 
and  the  context  number  in effect  during any given part  of the 
computation 
the  "CURRENT  CONTEXT  NUMBER" 
When  a  decision  point  is  encountered,  the  current  context 

is  called 

is 

specific 

It  monotonically 

incremented  by  one. 

number 
increases 
unless  a  decision  point  is  deleted.  (Normally  a  decision  point 
is  deleted  only  when  all  the  alternatives  at  it  have  been  tried 
and  have  been  unsuccessful,  although  it  may  also  be  deleted 
explicitly.)  The  context  number  provides  a  means  of 
referencing 
of 
In  MLISP2,  any  context 
communicating  between  contexts. 
which  knows 
the  number  of  an  earlier  context  may  pass 
information  back  to  that  context,  called  "setting  variables  in 
context."  Thus  when  a  branch 
fails  but  gains  valuable 
information  in  the  process  of  trying,  it  can  pass  the  information 
up  the  tree  to  be  used  in  selecting  another  branch  or  by  the 
next  branch  selected. 

backtracking 

contexts 

and 

2.  The  second  thing  that  happens  at  decision  points  is  the 
"incremental"  P  and  SP  stacks  are  saved  on  the  state  stack. 
The  definition  of 
is  somewhat 
complicated.  Consider  the  situation  represented  in  the  figure 
below. 

incremental  stacks 

the 

the 

variable  called 

l ow  w a t er  m a rk 

Two  decision  points  have  already  been  set  and 
the 
incremental  P  stack  at  each  has  been  copied  into  the  state 
stack:  A  -*  A',  B  -*  B'. 
(Since  basically  the  same  operations 
are  performed  on  the  SP  stack,  we  will  only  discuss  the  P 
stack  here.)  Now  a  third  decision  point  is  about  to  be  set. 
The  question  is;  how  much  should  be  copied  this  time?  A 
system 
(LWM) 
always  points  to  the  level  of  the  P  stack  below  which  the 
state 
contains  a  complete  copy  of  everything. 
Therefore,  just  the  information  from  LWM  to  the  top  of  the  P 
stack  must  be  copied  to  the  state  stack,  This  is  accomplished 
by  a  memory-to-memory  block 
(one  multi-cycle 
instruction  on  the  PDP-10  after  some  initial  set-up).  Finally 
the  new  LWM 
location  of  the  return 
address  of  the  function  containing  the  decision  point,  which  is 
usually  the  first  return  address  from  the  top  of  the  P  stack. 
This  requires  searching  the  P  stack.  We  cannot  simply  use 

to  the  stack 

transfer 

is  set 

stack 

681 

the  top  of  the  P  stack  since  the  function's  local  variables  and 
temporary  results,  which  are  stored  above  its  return  address, 
may  be  modified  before  the  function  exits. 
(If  this  happened, 
the  state  stack  would  no  longer  contain  a  complete  copy  of 
everything  below  LWM.)  Also  the  return  address  to  which 
LWM  points  is  changed  to  jump  to  a  system  routine;  this 
routine  moves  LWM  down  to  the  previous  one  whenever  the 
etack  is  about  to  become  smaller  than  the  current  LWM.  This 
Is  similar  to  ECL's  method  which  moves  LWM  down  to  the  next 
return  address,  rather  than  to  the  previous  LWM, 

later.  Associated  with  every  property 

3.  No  property  list  variables  are  explicitly  saved  at  decision 
points,  but  incrementing  the  context  number  affects  property 
list  variables 
list 
is  the  context  number  in  effect  when  the  variable 
variable 
was  last  set.  Whenever  a  property  list  variable  is  about  to 
is  compared  with  the  current  context 
be  set, 
then  the  value  is  simply 
number. 
changed  and  processing  continues.  No  information  is  saved 
If 
they  are  different,  then 

its  context 
If 

the  same, 

they  are 

(a) 

the  variable's  old  value  and  context,  together  with  the 
current  context  number,  are  saved  on  a  context  list  (see 
below); 

(b)  the  variable's  context  is  changed  to  the  current  context 
number,  to  reflect  the  fact  that  the  variable  has  now  been 
set  in  this  context; 

(c)  finally,  the  variable's  value  is  changed to  the  new  value. 

The  same  process  occurs  whenever  any  property  under  an 
atom  is  changed,  not  just  the  VALUE  property.  The  context 
associated  with  an  atom  is  actually  in  the  form 

where  in  the  case  of  variables  one  of  the  indicators  is  VALUE. 
This  list  must  be  searched  whenever  a  property  Is  about  to 
be  changed.  The  old  values  are  saved  on  a  "CONTEXT  LIST," 
in  the  form 

the  properties 

that  had  been  changed 

When  a  feilure  occurs,  the  system  runs  down this  list  restoring 
all 
the  current 
list  is  generally  short  since  only  one 
context.  The  context 
atom/indicator  pair  will  occur 
in  each  context,  namely  the 
f i r st  change  to  that  pair.  This  is  a  large  improvement  over 
the  inverse  statement  method  ot  restoring  values. 

in 

Saving 

distributes 

the  values  when  variables  are  about 
to  be  set 
the  state  saving  throughout  the  program.  Most 

existing  backtracking  methods  have 
this  property.  The 
advantage  is  that  the  amount  of  information  saved  becomes 
roughly  proportional  to  the  amount  of  work  done  on  any  one 
branch  of  the  goal  tree. 
If  the  branch  fails  early,  then  little 
state-saving work will have been done; if processing continues 
longer  on  the branch, then more  state information comes to be 
saved. 

M L I S P2  Improvements  In  State  Saving 

(a)  Not  e v e ry  change  to  a  nondeterministic  variable  is 
saved  (as  in  PLANNER  and  ECL),  just  the  f i r st  change  in 
each  backtracking context. 

(b)  MLISP2  uses  the  standard  LISP  1.6  value  cell  for 
variable  values,  so  that  fetching  a  variable's  value  is  just  as 
fast  with  as  without  backtracking, 
in  fact,  MLISP2  uses  the 
standard  LISP  representation  for  all  properties  on  property 
lists,  so  that  all  GETs  are the  same  speed nondeterministicaily 
as  deter  ministically.  This  is  the  main  source  of  the  efficiency 
that  MLISP2 has over QA4. 

(c)  When  LISP  1.6  functions  are  compiled,  many  things  are 
done  more  efficiently.  Local  variables  are  stored  on  the  P 
stack,  and  their  values  are  fetched  or  stored  in  one  machine 
instruction.  Fetches  on  free  variables  require  only  one 
instruction.  Function  entry  and  exit  require  one  instruction. 
All  of  these  efficiencies  are  preserved  by  MLISP2.  The  only 
inefficiency  introduced is in stores to free variables, which are 
represented  as  property  list  variables  in  LISP  J.6  and  thus 
must  go  through  the  process  explained in (3)  above.  Since  in 
LISP,  functions  are  usually  debugged  in  interpreted  mode  and 
it 
only  compiled 
in  a 
production  backtracking  system 
the  efficiency  of 
compilation  be  preserved.  In  research  systems  like  PLANNER 
and  QA4,  efficiency  comparable  to  MLISP2  has  not  been 
attained. 

increase  efficiency, 
that 

is  crucial 

to 

Language  Features  for  Backtraeking 

MLISP2  uses  backtracking  to  implement  a  context  sensitive 
pattern  matcher.  Pattern  matching  routines  are  written  using 
a  new  expression,  the  LET  expression.  As  in  PLANNER,  these 
routines  may  be  invoked  when  their  syntax  pattern  matches 
an 
input  stream,  though  unlike  PLANNER  the  MLISP2  input 
stream  must  be  unstructured  (e.g.  tokens  in  a  file  or  In  a 
linear  list).  The  MLISP2  pattern  matcher  is  designed  to  assist 
and  simplify  writing  translators  for  other  languages. 
In  addition 
to  pattern  matching,  a  second  new  expression  has  been  added 
to  MLISP,  the  SELECT  expression,  as  the  way  to  incorporate 
backtracking  in  ordinary  programs.  We  will  not  explain  these 
expressions  in  great  detail  here;  they  are  explained  fully  in 

682 

the  MLISP2  report.13  However  we  will  discuss  their  use  of 
backtracking. 

L ET  expression 

The 

pattern 

three 

includes 

language 

"meta  syntax" 
constructions  which  directly  create  decision  points;  REP 
(repeat),  OPT  (optional)  and  ALT  (alternative).  Their  primary 
purpose  is  to  simplify,  clarify,  and  reduce  the  number  of  rules 
necessary  to  specify  a  complex  syntax.  Examples: 

Meaning:  Let  the  production  "identifier_list"  be  zero  or  more 
identifiers  separated  by  commas,  and  have  as  its  value  a  list 
of  the  identifiers  scanned.  This  corresponds  to  the  BNF  rules: 

2.  {  OPT  <PATTERN>  } 

followed  by 

Meaning:  Let  the  production  "if"  be the  word  IF  followed  by  an 
expression, 
the  word  THEN  and  another 
expression,  optionally  followed  by  the  word  ELSE  and  a  third 
expression,  and  have  as 
its  value  the  corresponding  LISP 
"COND"  expression.  This corresponds  to the  BNF rules: 

the  production 

Meaning:  Let 
the 
"begin_end,"  the  "if,"  or  the  "for"  production,  and  have  the 
value  of 
its  value.  This 
corresponds  to  the  BNF  rules: 

respective  production  as 

"expression"  be  either 

the 

termination  function to the domain.  Exit  with this value as 
the  value  of  the  SELECT.  (The  termination  function  may 
call  FAILURE). 
If  the  value  of  the  termination  condition  is 
FALSE, proceed to the next  step. 

(d)  Apply  the  value  function  to  the  domain,  and  exit  with this 

value as the  value  of the SELECT. 

<e)  If  a  failure  returns  to  the  SELECT,  apply  the  successor 

function to the domain to yield a new domain. 

(f)  Go to step (c). 

We  will  give  a  few  examples  of  how  the  SELECT  may  be 

used, 

(1)  Floyd's CHOICE function may be written: 

Calling CHOICEUO) will  give ten choices.  The initial domain is 
just  the  integer  1.  The value  function is the identity function 

(LAMBDA  (3)  1). 

The  successor  function is addition by one 

(LAMBDA  (I) 

(PLUS  I D ). 

The  termination condition  is a check if the maximum has been 
exceeded 

(LAMBDA  (I)  (GREATERP  I  N)). 

The  termination  function 
(LAMBDA  (I) 

(FAILURE)) 

propagates  the  failure  if  the  termination  condition  becomes 
true.  This  illustrates the  use of the  intrinsic  function FAILURE, 
3  function  of  no  arguments that  fails  to the  last  decision point 
set  and restores the  state  of the computation at that point. 

(2)  The  most  common  use of  SELECT is to  select  items one 
at  a  time  from a list, and try out  each item selected.  For this 
reason,  several  of  the  clauses  in  the  SELECT  expression 
syntax  are  optional.  The  following two forms are equivalent. 

The  FINALLY  expression  need  not  propagate  failure;  it  may 
pass  information  back to  earlier contexts, or  simply compute a 
final  value. 

(3)  One  of  the  most  interesting  uses  of  the  SUCCESSOR 
expression,  which  computes  a  new domain,  is  to  reorder  the 
old  domain  based  on the  information  gained  by trying the last 
alternative.  QA4 and other systems have this ability. 

We  will  give  a  brief  example  of  how  nondeterminism  is 
these  expressions.  The  execution  of  an  OPT 

in 

used 
expression proceeds as follows: 

(a)  Set  a decision point 

(b)  Match the OPT pattern against  the current  input  stream. 

<c)  If  the  match  succeeds,  the  OPT  returns  with  a  list  of  the 
values  of  the  pattern elements  matched.  If the  match fails, 
FAILURE is called. 

(d)  If  a  FAILURE  happens,  either  in  the  matching  of  the  OPT 
pattern  or  later,  the  state  of  the computation  is  restored 
to  its  state  at  the  beginning  of the  OPT, the decision point 
is  deleted,  and  computation  proceeds  with  the  empty  list 
NIL as the value of the OPT. 

SELECT  expression 

The  other  way to  incorporate backtracking into a program is 
by  the  SELECT  expression.  The  SELECT  expression  is  like  a 
nondetemninistic  FOR-loop.  It  sets a decision point  and allows 
each  of  a  set  of  alternatives  to  be  tried. 
It  has  a  very 
general 
form,  and  is  a  generalization  of  Floyd's  CHOICE 
function  and  Fikes*  CHOICE-CONDITION  combination.4  The 
various  expressions  are  converted 
the 
following expansion: 

functions  by 

to 

(LAMBDA  <<formal_variable>) <expression>) 

These LAMBDA  functions are defined as: 

The execution of a SELECT expression proceeds as follows: 

(a)  Evaluate  the domain, which may be any expression, to get 

an initial domain. 

(b)  Set  a decision point. 

(c)  Apply the  termination condition to the domain.  If the value 
the 

the  decision  point  and  apply 

is  TRUE,  delete 

683 

the  MLISP2 

implementation.  Finally, 

Other  backtracking  systems  have  been  discussed  and 
compared  with 
the 
language  features  available  in  MLISP2  for  using  backtracking 
have  been  presented.  Our  main  conclusion  is  that  it  is 
possible  to  incorporate  backtracking  into  a  production system 
in  such  a  way  that  the  most  frequent  operations  are  not 
degraded  in  performance.  Backtracking  is  a  more  useful  and 
more used control structure when this ie done. 

With  some  simplifications  and  omissions, 

implementations  of  backtracking  are  summarized 
following table. 

the  existing 
the 

in 

Here  the  functions  all  operate  on  a  goal  list,  represented  by 
the  variable  GOALS.  The  successor  function  modifies  this 
variable, 
in  a  way  that  may  be  influenced  by  the  last 
alternative  tried  (for  example, by  passing some  information in 
context),  thus  affecting  subsequent  alternatives. 

O t h er 

features 

Two  final  backtracking  features  available  in  MLISP2  are  a 
function called FLUSH, which prunes a part  of the goal tree by 
flushing  elements  off  of  the  state  stack,  and  a  notation  for 
setting  variables  in  other  backtracking  contexts.  Normally 
assignments  such  as  "X 
Y"  take  effect  in  the  current 
context,  but  "X{10} 
Y"  sets  X  to  the  value  of  Y  in  the 
context  10.  X  will  now  not  be  backtracked  until  a  failure to 
context  10  occurs  or  until  it  is  again  set  in  the  current 
context.  This  enables 
that 
information  is to  be  saved until  a certain context  is destroyed. 
Any  expression to compute a context  number may occur inside 
the  braces. 

the  programmer 

to  specify 

REP,  OPT,  ALT,  SELECT,  FAILURE,  FLUSH  and  setting  in 
context  constitute  the  complete  set  of  backtracking  facilities 
available 
in  MLISP2.  Note  that  unlike  Floyd's  theoretical 
system,  there  is  no  SUCCESS function; success is the absence 
of  a  failure,  just  as  TRUE  is  anything  but  NIL  in  LISP.  These 
primitives  are  slightly  less  powerful  than  those  available  in 
some  other  systems,  particularly  PLANNER  and  ECL  which 
allow  failing  to  a  label.  For  example,  in  ECL  the  programmer 
may declare TAG  points, which are like nondeterministic labels, 
and 
these  TAG  points.  However, 
although  they  are  very powerful  (any control  structure can be 
be  buiit  up  out  of  GO  TO's),  the  arguments  against  the  use  of 
GO TO's  in deterministic languages apply as well to the use of 
these  nondeterministic  GO  TO's  and  labels.  Just  as  many 
languages  are  trying  to  replace  GO  TO's  with  WHILE-  and 
FOR-loops,  we have tried to replace nondeterministic GO TO's 
with a nondeterministic FOR-loop; the SELECT expression. 

fail  explicitly  to 

then 

B i b l i o g r a p hy 

1.  Bobrow,  D.G.  and  Wegbreit,  B.  A  Model  and  Stack 
I m p l e m e n t a t i on 
Environments 
Report  No.2334, Bolt, Beranek and Newman,  1972. 

of  M u l t i p le 

2.  Bur  stall,  R.M.,  Collins,  J.S.  and  Popple  stone,  R.J, 
P r o g r a m m i ng 
in  Pop2,  University  Press,  Edinburg, 
Scotland, 1971, 279-282. 

3.  Feldman,  J.A.,  Low,  J.R.,  Swinehart,  D.C.  and  Taylor,  R.H. 
K e o e nt  Developments 
b a s ed 
Artificial 
University,  1972. 

L a n g u a ge 
Intelligence  Project  Memo  AIM-176,  Stanford 

in  S A I L,  An  A L G OL 
Intelligence 

f or  A r t i f i c i al 

Summary 

4.  Fikes,  R.E.  A  H e u r i s t ic  P r o g r am 
P r o b l e ms 
P r o c e d u r es PH.D. Thesis, Carnegie-Mellon University, 1968. 

for  Solving 
Nondeterministic 

S t a t ed 

as 

An  efficient  implementation  of  backtracking  used  In  the 
MLISP2 language has been described.  The efficiencies are due 
to  a  smooth  integration  of  backtracking  into  an  existing  LISP 
system;  in  particular,  the  way  variables  are  stored  has  not 
been  changed,  so  that  fetches  and  most  stores  are  not 
degraded.  The  theory  of  existing  backtracking  systems  has 
been  related 
two  ways  of  viewing  the  structure  of 
computations.  MLISP2  has  a  "state-saving"  structure,  as 
opposed  to  a  "sequential"  or  "inverse  statement"  structure. 

to 

5.  Floyd, R.W.  "Nondeterministic Algorithms" J.ACM 14, A (Oct. 
1967),  636-644. 

6.  Golomb,  S.W,  and  Baumert,  L.D.  "Backtrack  Programming" 
J.ACM  12, 4 {Oct.  1965), 516-524. 

7.  Hewitt,  C. 
PLANNER" Proc. IJCA1 2, 1971, 167-182. 

"Procedural  Embedding  of  Knowledge 

in 

BR4 

8.  Hewitt,  C.  "PLANNER:  A  Language  for  Manipulating  Models 
and  Proving  Theorems  in  a  Robot"  Al  Memo  168  (rev),  MIT, 
1970. 

9.  Milner,  R.  Logic 
f or  Computable  Functions 
D e s c r i p t i on  Artificial  Intelligence  Project  Memo  AIM-169, 
Stanford  University,  1972. 

10.  Prenner,  C.J.,  Spitzen,  J.M.,  and  Wegbreit,  B.  "An 
Implementation  of  Backtracking  for  Programming  Languages" 
Sigplan Notices 7,  11  (Nov.  1972), 36-44. 

11.  Quam,  L.H.  and  Diffie,  W.  Stanford  LISP  16 
M a n u al  At Operating Note  28.7, Stanford University,  1972. 

12.  Rulifson,  J.F.  Q A4  P r o g r a m m i ng  Conoepts  Al 
Technical Note 60, Stanford Research Institute, 1971. 

13.  Smith,  D.C.  and  Enea,  H.J.  M L I 8 P2  Artificial  Intelligence 
Project  Memo AIM-195, Stanford University,  1973. 

14.  Sussman,  G.J.  and  McDermott  D.V.  "Why  Conniving  is 
Better  Than  Planning"  Proc.  FJCC  41  (Dec.  1972),  1171-
1180. 

685 


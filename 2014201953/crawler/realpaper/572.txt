First-order probabilistic inference 

David Poole 

Department of Computer Science 
University of British Columbia 

Vancouver, B.C., Canada V6T 1Z4 

poole@cs.ubc.ca 

http://www.cs.ubc.ca/spider/poole/ 

Abstract 

There have been many proposals for first-order be(cid:173)
lief networks (i.e., where we quantify over individ(cid:173)
uals) but these typically only let us reason about the 
individuals that we know about.  There are many 
instances where we have to quantify over all of the 
individuals in a population.  When we do this the 
population size often matters and we need to rea(cid:173)
son about all of the members of the population (but 
not necessarily individually).  This paper presents 
an algorithm to reason about multiple individuals, 
where we may know particular facts about some of 
them, but want to treat the others as a group. Com(cid:173)
bining unification with variable elimination lets us 
reason about classes of individuals without needing 
to ground out the theory. 

Introduction 

1 
Belief networks or Bayesian networks [Pearl, 1988] are a pop(cid:173)
ular representation for independencies amongst random vari(cid:173)
ables. They are, however zeroth-order representations; to rea(cid:173)
son about multiple individuals, we have to make each property 
of each individual into a separate node. 

There have been proposals to allow for ways to lift belief 
networks to a first order representation.  The  first  tradition 
[Breese, 1992; Horsch and Poole, 1990; Wellman, Breese and 
Goldman, 1992] essentially allowed for parameterized belief 
networks, which could be grounded for each individual. The 
second tradition [Poole, 1993; Koller and Pfeffer, 1998; Pfef-
fer, Koller, Milch, and Takusagawa, 1999] allowed for richer 
first order probabilistic  representations that have belief net(cid:173)
works as special cases.  In all of these the only individuals 
assumed to exist are those that we know about. 

There are many cases where we want to reason about a set 
of individuals as a group. We'd like avoid explicit reasoning 
about each individual separately. There was a great advance 
in theorem proving in the 1960s with the invention of of reso(cid:173)
lution and unification [Robinson, 1965]. The big advance was 
that doing resolution on clauses that contain free variables 
implements  a potentially  unbounded number of resolutions 
on the grounded representation. The goal of the current work 
is to allow for similar savings in probabilistic reasoning. 

Probabilistic reasoning is more challenging than logical rea(cid:173)

soning for a number of reasons: 

•  We have to use all of our information when making a 
probabilistic inference; new information can change old 
conclusions. 

•  We don't want to double count evidence.  If we  have 
some probabilistic information about all people and we 
use it for one particular individual, say Fred, then we 
can't reuse that information for Fred. 

•  There are cases where the size of the domain affects the 

probability.  In the example developed below, determin(cid:173)
ing the probability that a person is guilty of a particular 
crime depends on the population; the population size af(cid:173)
fects the number of other people who could be guilty. 

The following example shows why we may need to reason 
about the individuals we don't know about as well as the indi(cid:173)
viduals we do know about and shows why we need to consider 
population size. 
Example 1 A person in a town was seen committing a crime. 
This person had the same (unusual) hair colour and car colour 
as Joe (both purple) and the person was very tall and we know 
Joe has big feet (and being tall is correlated with having big 
feet).  What is the probability that Joe is guilty?  We need to 
model the probabilities of the observations and their depen(cid:173)
dence, which would lead us to consider belief networks as a 
representation. The probability Joe is guilty also depends on 
the population.  If the population of the town  is very small 
then he is probably guilty (as it is unlikely there is anyone else 
fitting this description).  If the town was a city containing a 
large number of people then he is probably innocent (as we 
would expect many other people to also fit this description). 
This example points to a number of features that have been 
ignored in first-order probabilistic models.  Not only do we 
need to take population sizes into account but we need to be 
able to reason about the individuals we know about as well 
as the individuals who we know exist, but we don't know 
anything particular about.  We don't have to ground out our 
theory and reason about millions of people in a city separately, 
but we also cannot ignore the relevant information about the 
people we know something about. 
2  Representation 
We assume that the domain is represented as a parametrized 
belief  network,  where  the  nodes  are  parametrized  with 

PROBABILISTIC  INFERENCE 

985 

domain-restricted types (or populations).  Such an idea has 
been explored previously |Horsch and Poole, 1990; Kersting 
and De Raedt, 2000], and is similar to the plates of Buntine 
[ 1994]. We will not dwell on the semantics in this paper. We 
just assume that the program means the grounding:  the sub(cid:173)
stitution of constants for each individual in the domain of a 
parameter. 

This paper is explicitly Bayesian. In terms of the first-order 
probability of Halpern [1990], the probabilities are all degrees 
of belief, not statistical assertions about proportions of a pop(cid:173)
ulation.  We are not quantifying over random individuals (as 
does Bacchus [1990]), but over all individuals (i.e., with stan(cid:173)
dard universal quantification).  All  of the individuals about 
which we have the same information have the same probabil(cid:173)
ity. It is this property that we exploit computationally. 

This paper is built on two different traditions, namely that 
of logic programming and theorem proving on one side and 
that of probabilistic  reasoning on  the other side.  Unfortu(cid:173)
nately they use the same terms or notations (e.g., "variable", 
"domain 
for very different things (or at least they are 
used in very different ways in this paper).  In order to avoid 
confusion, we use neutral terms, but use :he traditions of the 
different communities as appropriate. 

A population is a set of individuals.  A population corre(cid:173)
sponds to a domain in logic. The cardinality of the population 
is called the population size.  For example, the population 
may be the set of people living in Vancouver, and the popula(cid:173)
tion size is 2 million people. The population size can be finite 
or infinite. 

where 

We allow for random variables to be parametrized, where 
parameters start with an upper case letter and constants start 
with a lowercase letter. Parameters correspond to logical vari(cid:173)
ables (e.g., as a variable in Prolog). All of the parameters are 
typed with a population.  A parametrized random variable is 
of the form 
is a functor (either a function 
symbol or a predicate symbol) and each 
is a parameter or a 
constant. Each functor has a set of values called the range of 
the functor. 
Examples 

variables 
are 
or 
townjeonservativeness (the latter being a functor of no argu(cid:173)
ments), where hairjcolour, likes and townjeonservativeness 
are functors. 

hair_colour(X), 

parametrized 

likes(joe,X) 

likes(X,Y), 

random 

of 

Given  an  assignment  of  a  constant  to  each  parameter, 
a parametrized random variable represents a random vari(cid:173)
able.  For example,  suppose hair colour has range  {black, 
blond, red, grey, purple, orange, none, multicoloured}. Then 
hair_colour{sam) is a random variable with domain (black, 
blond, red, grey, purple, orange, none, multicoloured). Thus 
a parametrized random variable represents a set of random 
variables, one for each parameter assignment.  Different pa(cid:173)
rameter assignments for the variables in a parametrized ran(cid:173)
dom  variable  result  in  different  random  variables;  for  ex(cid:173)
ample,  hair_colour(jred)  is a different random variable to 
hairjcolour (joe). 

A  parametrized primitive proposition is an expression 
which means that parametrized random 
of the form 
variable  has value  A parametrized proposition is built 
from the parametrized primitive propositions using the normal 

Figure 1: Robbing example parametrized belief network (Ex(cid:173)
ample 2) using plates and parametrized random variables. 

logical connectives. 

A probabilistic assertion is of the form: 

are parameters, dx  are populations,  C is a set of 
where 
are parametrized 
inequality 
on 
propositions using only the parameters 
, /; specifies 
a probability distribution over a.  We omit the corresponding 
syntactic constructs when " 

constraints 

A parametrized belief network consists of 

a DAG where the nodes are parametrized random vari(cid:173)
ables, 
an association of a population to each parameter, 
an assignment of a range to each functor, 
a set of probability  assertions  for each  node  given  its 
parents. 

formalize  Example  1,  consider 

Example 2  To 
the 
parametrized  belief  network  of  Figure  1.  Here  we  have 
shown  it  using  the  plates1  [Buntine,  1994]  as  well  as 
with  parametrized  random  variables.  We  assume  that the 
hair-colour of the different people are not independent; they 
depend on how conservative the town is. Associated with this 
network are conditional probabilistic assertions such as 2: 

If we knew as background knowledge that sam was an ex(cid:173)
ception and has purple hair with probability 0.5, we would 
replace the last probabilistic assertion with: 

P(hair_colour(X)=purple\conservative) = 0.001 

*Herc we use plates just as a visual help; the real meaning is as 
probabilistic assertions.  The plates get difficult to interpret when 
there are complex interactions between the parameters, but we can 
always interpret these as probabilistic assertions. 

2We  use  conservative  as  an  abbreviation 

townjeonservativeness = conservative, male(X) as an abbreviation 
for sex(X) = male and very Jall (X) for height (X) = very jall. 

for 

986 

PROBABILISTIC  INFERENCE 

P(hair_colour(sam)=purple) = 0.5 

(We will not use this in our continuing example). 

A grounding of a parametrized belief network is a belief 
network that consists of all instances of the parametrized ran(cid:173)
dom variables where each parameter is replaced by an in(cid:173)
dividual in the population (or a ground term denoting that 
individual). 

Intuitively, a parametrized belief network represents a huge 
belief network where the parametrized random variables are 
repeated for each individual in the population associated with 
the parameter for which the constraint is true.  The above 
example, if there were 10,000 people, would represent a belief 
network with 60,001 nodes. 
3  First order variable elimination 
The problem that we consider is: given a parametrized belief 
network, a set of observations, and a (possibly parametrized) 
query to determine the conditional probability of the instances 
of a query given the observations. We assume that the evidence 
is a conjunction of existentially quantified parametrized prim(cid:173)
itive propositions. 

We consider the problem of parametric probabilistic infer(cid:173)
ence in two stages:  first,  where every parameter that appears 
in the parents of a node also appears in the node and where the 
observations are variable-free. In this case, when we ground 
out the theory, each random variable only has a limited number 
of parents.  In Section 3.5 we present the second case where 
we have parents that contain extra parameters; in this case we 
need a way to aggregate over populations.  That section also 
considers existential observations and queries. 

The algorithm is based on variable elimination, VE, [Zhang 
and Poole, 1996], where we eliminate the non-observed non-
query  random  variables one  at a time.  For first-order VE 
(FOVE) we eliminate all the instances of a functor at once3. 
There is a strong relationship between this work and lifting 
in theorem proving [Chang and Lee, 1973]:  given a ground 
proof procedure, construct a proof procedure with logical vari(cid:173)
ables (or in our case with parametrized random variables). In 
general, correctness can be shown by proving that we get the 
same answer as if we first grounded the theory and then carried 
out variable elimination. See Figure 2. 

3.1  Parametric Factors 
In VE, a factor is the unit of data used during computation. 
A VE factor is a function from a set of random variables into 
a non-negative real number. The initial factors are the condi(cid:173)
tional probabilities. The main operations are multiplying fac(cid:173)
tors and summing out random variables from factors.  After 
conditioning on the observed random variables and summing 
out the non-observed, non-query random variables, we can 
extract posterior probabilities from the remaining factors by 
multiplying them and normalizing the remaining factor. 

In first-order variable elimination, we use a generalization 
of a factor where we want to treat the many instances of factors 

3In general, we can potentially eliminate some instances of a func(cid:173)
tor, but not necessarily all instances, and leave the other instances to 
be eliminated later. In this paper we will only do this when eliminat(cid:173)
ing all instances except for a query instance. 

Figure 2: We design FOVE so that we get the same answer as 
if we had grounded the representation and carried out variable 
elimination. 

as a unit. We only instantiate parameters when we need to. In 
general we reason with all of the individuals (except the ones 
that we know extra information about) as a unit. 

A  parametric  factor  or  parfactor  is  a  triple 

where C is a set of constraints on parameters, V is a set of 
parametrized random variables and Ms a table representing a 
factor from the random variables to the non-negative reals. 

Intuitively the parametric factor represents all of the ground 
instances of the factor where the instantiation of the parameters 
satisfies the constraints. 
Example 3 A parametric factor that represents one of the con(cid:173)
ditional probability tables of Figure 1 is: 

where t is the table that represents a function from hair_colour 
and conservativeness into non-negative numbers, t is not in(cid:173)
dexed by X. t looks like: 

hairjcolour 
purple 
purple 
blue 
blue 

conservativeness 
conservative 
liberal 
conservative 
liberal 

Val 
0.001 
0.01 
0.1 
0.05 

When there are two instances of the same parametrized ran(cid:173)
dom variable in V; in this case we need to mark the random 
variable in the table to distinguish these instances. 
Example 4  Suppose that whether person X is a friend of Y 
depends on whether X likes Y and whether Y likes X. There are 
two distinct cases here, the first is when X = Y, in which case 
friends{X, Y) has one parent, and the second is when 
in which case friends(X, Y) has two parents. To represent this, 
we could use 

where the subscripts in likes1 and likesn represent different 
instances of likes in table t2- That is, 
is a factor on friends, 
likesi and likesi. When we eliminate all of the likes relation(cid:173)
ships, we have to consider likes\ and Ukes2. 
3.2  Splitting 
The foundation of parametric variable elimination is the split(cid:173)
ting operation. Splitting plays the analogous role to applying 
substitutions in theorem proving, except that we not only have 
to be concerned about the instance created, but also about the 
instances left over. 

PROBABILISTIC  INFERENCE 

987 

Definition 1  Suppose parametric factor 
rameter X. A split of 
constant or another parameter, and C does not contain 

contains pa(cid:173)
on X  = w h e re  y is either a 

results in the two parametric factors: 

where 
instance of X (i.e., we substitute 
metric factor is called a residual parametric factor. 

is the same as V\ but with y replacing every 
for X).  The second para(cid:173)

A substitution is of the form 
where the 
are distinct variables and the are terms. We assume that all 
substitutions are in normal form: does  not  contain  for any 
and  The substitutions resulting from standard unification 
algorithms are in normal form [Chang and Lee,  1973].  If a 
substitution is in normal form, we get the same result from 
replacing each X, by the corresponding f, sequentially (in any 
order) or in parallel. 

Instead of just applying substitutions as in normal theorem 
proving, we need to split. The general idea is that whenever 
applying a substitution to a parfactor restricts the set of ground 
instances, we need to split the parfactor.  We don't need to 
split when just renaming variables or substituting a value for 
a variable that doesn't appear in the parfactor. 
on substitution 

We can split a parfactor 

by totally 

ordering the elements of the substitution4, 
by carrying out the following procedure which results in a final 
instance of  and a set of residual parfactors: 
For i from 1 to k 

Note that the final value of 
the substitution to 

, but we also create residuals. 

is the same as if we had applied 

There is a close relationship between the splitting on equal(cid:173)
ity in this paper and the splitting on the value of a variable in 
contextual variable elimination fPoole and Zhang, 2003]. 
3.3  Observations 
When we observe a ground value, we carry out the analogous 
operation to VE. We project the tables onto the observed val(cid:173)
ues. However we must first split to ensure that we only affect 
the appropriate ground variables. 
Example 5 if we condition on the fact that Joe has purple hair, 
a purple car, and a shoe size of 12, we now reason separately 
about Joe than we do about the other individuals who can be 
treated as a group. 

The parametric factor of example 3 becomes the two para(cid:173)

metric factors: 

3.4  Multiplying Parametric Factors 
In VE, when we eliminate a variable, we multiply all of the 
factors that contain that variable then sum out the variable 
from the resultant factor. 

As in variable elimination, we need to multiply the factors 
that contain the variable to be eliminated. In parametric vari(cid:173)
able elimination, given two parametric factors, some instances 
may need to be multiplied, and some instances may not. Also, 
the dimension of the resulting factors can be different for dif(cid:173)
ferent instances. 

The product of two parametric factors, in general, results in 
a set of parametric factors.  Intuitively, we keep splitting the 
parametric factors (and renaming parameters) until we can 
guarantee  that  we  get parametric  factors  that,  if grounded, 
result in  the same factors as if we were to first ground the 
factors and then multiply. 
Determining which parfactors to multiply 
Suppose there are two parfactors 

and 
with variables renamed to be differ(cid:173)

ent, wherepi and/?2 are unifiable instances of 
where 
We will 
split  on  and split  on  putting all the residuals in the set 
of all parfactors. All instances of the resulting non-residual 

is to be eliminated.  Let 

parfactors would be multiplied in VE. 

The following abstract example is designed to show what 
needs to be considered when multiplying parfactors.  It isn't 
meant to be meaningful. 
Example 6  Suppose we were to eliminate p and multiply the 
two parametric factors: 

If we were to ground the parameters some of the instances of 
these would be multiplied in VE and some of them wouldn't. 
Unification finds the most general instances that are identical. 
resulting in the substitution 

(1) 
(2) 

We can split parametric factor (1) on 

resulting in 

(3) 
(4) 
Parametric factor (4) is a residual parametric factor.  No in(cid:173)
stance of parametric factor (4) ever needs to be multiplied by 
any instance of parametric factor (2) when eliminating  and 
doesn't participate further in the product. 

Similarly, we can split (2) on 0 resulting in: 

(5) 

(6) 
Parametric factor (6) is a residual factor and doesn't partici(cid:173)
pate further in the elimination of p. All instances of (3) and 
(5) would be multiplied together when eliminating p(b, a) in 
variable elimination. 
Determining the dimensionality of the product 
In Example 6, all instances of parametric factors (3) and (5) 
would be multiplied if we were to ground parametric factors 
(1) and (2) and carry out VE. However, not all of the product 
factors have the same dimension; some have two different 

4The total order does not affect the instance created but does affect 

which residuals are created. 

988 

PROBABILISTIC  INFERENCE 

instances, and some have one. We need to do more splitting 
to ensure that all of the products have the form of parametric 
factors. 

Suppose we have parfactors 

that need to be multiplied.  If for all 

and 
either /?' and p" are identical or non-unifiable 
for all 
or if mgu(p\ p") is incompatible with the constraints, we know 
that all instances of their product has the same dimension.  If 
p"V" that are not identical but unify 
there is a p' 
with mgu consistent with C' and C", we can split  and 
on 
mgu(p\ /?"). The resulting instances and residuals either have 
identical instances of// and/?" or non-unifiable instances. We 
then multiply each instance created from 
by each instance 
created from 0". 

and 

When we know all the instances 

where 

have the same dimension, we create the parfactor 
is the product of the 
where we maintain one dimension for each 
tables for 
Thus, those members in common in V 
member of 
and 
are treated as the same variable in the product, but 
those members that don't unify, even with the same functor, 
in 
are treated as different variables in the product 
table. 
Example 7  When  we  need  to  multiple  parfactors  (3)  and 
(5), we notice that 
unify  (with unifier 

and 

and 

We thus split parametric factor (5) on 

and rename W to K, replacing (5) with: 

(7) 
(8) 
We can now multiply parametric factors (3) and (7) producing: 

(9) 
is  the product  of factors.  Parametric  factor 
where 
(9) represents all of the factors of dimension four created by 
multiplying factors that are instances of parametric factors (1) 
and (2). 

We also multiply parametric factors (3) and (8) producing: 

is the factor 
but with 

, but with  abelled as q\ and 

(10) 
where 
is the 
factor 
is a factor 
on 
Parametric factor (10) represents all of the 
factors of dimension five created by multiplying factors that 
are instances of parametric factors (1) and (2). 

labelled  as  Thus 

While this may seem very complicated, remember that para(cid:173)
metric factor (10) represents mr(m — l)2 factors (assuming 
that all populations have size 
is 10, this is 8100 
factors. 

Even if 

When we have multiplied all of the appropriate parfactors 
we are ready to sum out the variables being eliminated.  We 
must remember that when we are eliminating p 
we are not just eliminating one random variable, but we are 
eliminating a number of variables equal to the product of the 
population sizes of 
If some parameters only ap(cid:173)
pear in the parametrized random variable that is being elimi(cid:173)
nated, in the grounding we are multiplying the number of fac(cid:173)
tors equal to the effective population size of those parameters. 

The effective population size is the product of the populations 
of the parameters less the number excluded by the constraints. 
Thus we have to take each element in the table and put it to 
the power of the effective population size. 

3.5  Aggregation over populations 
When we have parameters in the parents of a node and not in 
the node itself, the number of parents grows with the popula(cid:173)
tion size, and we need to specify how a node is a function of 
its parents. There are many possibilities, such as a node being 
the "logical or" of it's parents, the "logical and", the max of 
its parents, the average if its parents, true iff greater than k 
of its parents are true, or the vote of its parents (according to 
some voting scheme, for example when the majority wins), or 
some other function. Zhang and Poole [1996] give an analysis 
where there is an arbitrary associative and commutative oper(cid:173)
ator between the parents.  In order to make the presentation 
simpler, in this paper we assume that the operator is a logical 
"or" [Diez and Galan, 2002].  It is straightforward to use the 
techniques of Zhang and Poole [1996] to extend this to other 
operators. 

PROBABILISTIC  INFERENCE 

989 

We can transform the problem into one of the form 

where 

is the "logical or", over all of 

with parents 
the  values. 

To eliminate 

, we multiply all of the compatible clauses 
and as we eliminate each  we accumulate 
that contain 
the  probability  over the 
(as in [Diez and Galan, 2002; 
Zhang and Poole, 1996]). The only difference is when there 
is a free (perhaps with inequality constraints) parameter in the 
In this case we need to do the "or" over the effective 
population size.  We can determine the effective population 
size by determining the effective population size of each free 
parameter by subtracting the number of excluded values from 
the population (e.g., if we have 
with a 
population size of 10, we have an effective population size of 
10-2=8) and multiplying by the effective populations of all of 
the free parameters. 

and 

If we have an effective population size of  and each one 
then, assuming they are all inde(cid:173)
contributes a probability of 
pendent, the probability that they are all false is 
So 
the probability that at least one is true (i.e., the logical "or") 
is 1  - 
If the effective population size is countably 
infinite, the probability that at least one is true is 1 if 
0 
and is 0 if 

However, we cannot assume the instances of the q's are in(cid:173)
dependent.  They are only dependent if they either have (1) a 
common ancestor in the grounding or (2) a common observed 
descendent.  If we condition on the observation after elimi(cid:173)
we can get around the second condition. The 
nating the 
only way that the 
can have a common ancestor is if there is 
an ancestor that doesn't involve one of the parameters.  If we 
make sure that we eliminate the 
before we eliminate the 
common ancestors that separate the other common ancestors, 
then we can just use the equation above that assumes inde(cid:173)
pendence.  Effectively we are doing the logical "or" for each 
value of the ancestors, and we know they are independent for 
each value of the ancestors. 

If we have an existential observation or query (e.g., condi(cid:173)
tioning on the fact that someone who fits a certain description 
is guilty), we need to construct the "or" and either condition 
on or query the resulting node. 
Example 8 Let's return to Example 1 (see Figure 1). Suppose 
that as well as observing the hair-colour, car-colour, and shoe 
size of Joe, we also observed that there exists a person who is 
guilty and fits a certain description of hair-colour, car-colour, 
height. This is depicted in Figure 4. 

Suppose we want to compute whether Joe is guilty given 
Joe's hair-colour, car-colour and shoe size and given the wit(cid:173)
ness description.  We can first instantiate all of the observed 
random variables except the witness observation. This splits 
off joe as a special case. We can now eliminate all of the ran(cid:173)
dom variables except for townjconservativeness, guilty (joe), 
descn{X) (for X = joe and 
and witness. This results 
in two parametric factors: 

can 

We 
and notice that 
the instances are independent given conservativeness. This 

eliminate 

descn\ 

now 

Figure 4: Robbing example with the witness observation. 

results in the parametric factor: 

We can now sum out conservativeness and condition on 
witness and end up with a parametric factor 

We can now determine the probability that Joe is guilty from 
normalising t4. If we were to carry out this computation leav(cid:173)
ing the population as a parameter, the probability of guilty (joe) 
can be computed as a ftmction of the population. We get a re(cid:173)
sult that looks like Figure 5. The graph has this shape because 
it is the linear combination of exponential functions (for each 
value of townjoonservativeness we have an exponential dis(cid:173)
tribution). 
4  Conclusion 
This paper has made three main contributions: 

•  a way to do inference over populations without grounding 
out the theory and a way to use unification, where as 
well as the unifiers we also need to take into account the 
residuals; 

•  the idea that we need to take population size into account 
when we have aggregation over populations either as part 
of the model or as an observation; and 

•  a way to handle existential observations where we know 

someone exists but don't know who it is. 

This paper extends the inference in object-oriented Bayesian 
networks  [Pfeffer  et  al.,  1999]  where  the  reasoning  with 

990 

PROBABILISTIC  INFERENCE 

Breese, J. S. [1992]. Construction of belief and decision net(cid:173)

works, Computational Intelligence 8(4): 624-647. 

Buntine, W. L. [1994].  Operations for learning with graph(cid:173)
ical models, Journal of Artificial Intelligence Research 
2: 159-225. 

Chang, C. L. and Lee, R. C. T. [1973]. Symbolic Logical and 
Mechanical Theorem Proving, Academic Press, New 
York. 

Diez, F. J. and Galdn, S. F. [2002].  Efficient computation 
for the noisy max, International Journal of Intelligent 
Systems p. to appear. 

Halpern,  J. Y.  [1990].  An  analysis of first-order logics of 

probability, Artificial Intelligence 46(3): 311-350. 

Horsch, M. and Poole, D.  [1990].  A dynamic approach to 
probabilistic inference using Bayesian networks, Proc. 
Sixth Conference on Uncertainty inAI, Boston, pp. 155— 
161. 

Jaeger,  M.  [2000J.  On  the  complexity  of inference  about 
probabilistic  relational  models,  Artificial  Intelligence 
111. 297-308. 

Kersting, K. and De Raedt, L. [2000].  Bayesian logic pro(cid:173)
grams, Linkbping Electronic Articles in Computer and 
Information Science 5(34). 
URL: http://www.ep.liu.se/ea/cis/2000/034/ 

Kollcr, D. and Pfeffer, A. [1998].  Probabilistic frame-based 

systems, AAAI-98, AAA1 Press, Madison, Wisconsin. 

Pasula, H. and Russell,  S.  [2001].  Approximate inference 
for first-order probabilistic languages, IJCAI-OI, Seattle, 
WA, pp. 741-748. 

Pearl, J. [1988], Probabilistic Reasoning in Intelligent Sys(cid:173)
tems: Networks of Plausible Inference, Morgan Kauf-
mann, San Mateo, CA. 

Pfeffer,  A.,  Roller,  D.,  Milch,  B.,  and  Takusagawa,  K. 
[1999].  SPOOK:  A  system  for  probabilistic  object-
oriented knowledge representation, UAl-99, Stockholm, 
Sweden, pp. 541-550. 

Poole, D. [1993]. Probabilistic Horn abduction and Bayesian 

networks, Artificial Intelligence 64(1): 81-129. 

Poole, D. and Zhang, N. L. [2003]. Exploiting contextual inde(cid:173)
pendence in probabilistic inference, Journal of Artificial 
Intelligence Research 18: 263-313. 

Robinson, J. A. [1965],  A machine-oriented logic based on 

the resolution principle, Journal ACM 12(1): 23—41. 

Wellman, M., Breese, J. and Goldman, P. [19921. From knowl(cid:173)
edge bases to decision models, Knowledge Engineering 
Review 7(1): 35-53. 

Zhang, N. and Poole, D. [1996].  Exploiting causal indepen(cid:173)
dence in Bayesian network inference, Journal of Artifi(cid:173)
cial Intelligence Research 5: 301-328. 

Figure 5: Probability of guilty(joe) for various populations. 

generic class models corresponds to reasoning with free pa(cid:173)
rameters in this paper. They do not use the power of unification 
as in this paper. 

This paper contradicts the pessimistic conclusions of Jaeger 
12000], but not the results.  While in the worst case we may 
effectively ground the representation, in many cases we can 
do much better. How much we save in practice is still an open 
question. 

There are still a number of open questions: 
•  What are good elimination orderings? We don't have to 

eliminate all instances of a functor at once. 

•  How to utilize other combination rules (apart from "or"). 
While the description here was in terms of noisy-or [Diez 
and Galdn, 2002] or inter-causal independencies [Zhang 
and  Poole,  1996],  the actual  use of splitting  is much 
closer to the work on contextual independence  [Poole 
and Zhang, 2003]. 

•  We are interested in combining the work in this paper 
with richer languages (e.g., Poole [ 1993]), where much of 
the power comes from mixing contextual independence 
and free variables and allowing for function symbols and 
recursion. 

•  It is also interesting to think about combining this with 
MCMC [Pasula and Russell, 2001], however it may not 
be straightforward because we are representing a set of 
individuals as a unit (and so are completely dependent), 
which may cause problems when we separate one indi(cid:173)
vidual from the class and reason about that individual 
separately. 

Acknowledgements 
This work was supported by NSERC Research Grant OG-
P0044121 and The Institute for Robotics and Intelligent Sys(cid:173)
tems. Thanks to Valerie McRae and Mark Paskin for valuable 
comments. 
References 
Bacchus, F. [1990]. Representing and Reasoning with Uncer(cid:173)
tain Knowledge, MIT Press, Cambridge, Massachusetts. 

PROBABILISTIC  INFERENCE 

991 


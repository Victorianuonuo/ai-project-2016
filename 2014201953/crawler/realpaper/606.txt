Faster Heuristic Search Algorithms for Planning 

with Uncertainty and Full Feedback 

Blai  Bonet 

Computer Science Department 

University of California, Los Angeles 

Los Angeles, CA 90024, USA 

bonet@cs.ucla.edu 

Hector  Geffner 

Departamento de Tccnologfa 

1CREA - Universitat Pompeu Fabra 

Barcelona 08003, Espafia 

hector.geffner@tecn.upf.es 

Abstract 

Recent  algorithms  like  RTDP  and  LAO*  combine 
the strength of Heuristic Search (HS) and Dynamic 
Programming  (DP)  methods  by  exploiting  knowl-
edge  of the  initial  state  and  an  admissible  heuris(cid:173)
tic  function for producing optimal  policies without 
evaluating  the  entire  space.  In  this  paper,  we  in(cid:173)
troduce and  analyze three new  HS/DP algorithms. 
A  first  general  algorithm  schema  that  is  a  simple 
loop  in  which  'inconsistent'  reachable  states  (i.e., 
with residuals greater than a given c) are found and 
updated until no such states are found, and serves 
to  make  explicit the  basic  idea  underlying HS/DP 
algorithms,  leaving  other  commitments  aside.  A 
second algorithm, that builds on the  first and adds 
a  labeling  mechanism  for  detecting  solved  states 
based  on  Tarjan's  strongly-connected components 
procedure, which is very competitive with existing 
approaches.  And  a  third  algorithm,  that  approx(cid:173)
imates  the  latter  by  enforcing  the  consistency  of 
the value function over the  likely' reachable states 
only, and leads to great time and memory savings, 
with no much apparent loss in quality, when transi(cid:173)
tions have probabilities that differ greatly in value. 

Introduction 

1 
Heuristic  search  algorithms  have  been  successfully used  for 
computing optimal  solutions to  large deterministic  problems 
(e.g.,  iKorf,  1997]).  In the presence of non-determinism and 
feedback,  however,  solutions  are  not  action  sequences  but 
policies,  and  while  such  policies  can  be  characterized  and 
computed by dynamic programming (DP) methods [Bellman, 
1957; Howard, 1960], DP methods take all states into account 
and  thus  cannot  scale  up  to  large  problems  [Boutilier et al, 
1999].  Recent algorithms like RTDP iBarto et  al,  19951 and 
LAO*  [Hansen and Zilberstein, 2001], combine the strength 
of Heuristic  Search  and Dynamic  Programming methods by 
exploiting knowledge of the initial state SQ  and an admissi(cid:173)
ble  heuristic  function  (lower  bound)  h  for  computing  opti(cid:173)
mal  policies  without having to evaluate the  entire space.  In 
this  paper  we  aim  to  contribute  to  the  theory  and  practice 
of Heuristic  Search/Dynamic  Programming  methods  by  for(cid:173)
mulating  and  analyzing  three  new  HS/DP  algorithms.  The 
first algorithm, called FlND-and-REVlSE is a general schema 

that comprises a loop in  which states reachable from s0  and 
the greedy policy that have Bellman residuals greater than a 
given 6 (we call them 'inconsistent' states), are found and up(cid:173)
dated until  no one  is  left.  FIND-and-REViSE  makes explicit 
the basic idea underlying HS/DP algorithms, including RTDP 
and  LAO*.  We prove the convergence, complexity, and op-
timality  of  FIND-and-REViSE,  and  introduce  the  second  al(cid:173)
gorithm,  HDP,  that builds on  it,  and  adds  a labeling mech(cid:173)
anism  for detecting solved states  based  on  Tarjan's  efficient 
strongly-connected components procedure [Tarjan,  1972].  A 
state  is  solved  when  it reaches only  'consistent'  states,  and 
solved states  are skipped in all  future searches.  HDP termi(cid:173)
nates when the  initial  state  is  solved.  HDP inherits the con(cid:173)
vergence and optimality  properties of the  general  FlND-and-
REVISE schema and is strongly competitive with existing ap(cid:173)
proaches.  The  third algorithm,  HDP(i),  is  like  HDP,  except 
that  while  HDP  computes  a  value  function  by  enforcing  its 
consistency over all  reachable states (i.e., reachable from s0 
and the greedy policy), H D P ( I) enforces consistency over the 
likely'  reachable states only.  We show that this approxima(cid:173)
tion, suitably formalized, can lead to great savings in time and 
memory, with no much apparent loss in quality, when transi(cid:173)
tions have probabilities that differ greatly in value. 

Our motivation is twofold:  to gain a better understanding 
of HS/DP methods for planning with uncertainty, and to de(cid:173)
velop more effective HS/DP algorithms for both optimal and 
approximate planning. 

2  Preliminaries 
2.1  Model 
We  model  non-deterministic  planning  problems  with  full 
feedback with state models that differ from those used in the 
classical  setting  in  two  ways:  first,  state  transitions  become 
probabilistic;  second,  states are fully observable.  The result(cid:173)
ing models are known as Markov Decision Processes (MDPS) 
and  more  specifically  as  Stochastic Shortest-Path Problems 
iBertsekas,  1995], and they are given by:1 

SEARCH 

1233 

0, and 

M6.  positive action costs 
M7.  fully observable states. 
Due  to  the  presence of full  feedback  (M7)  and  the  standard 
Markovian  assumptions,  the  solution  of  an  MDP  takes  the 
form of a function  mapping states s into actions a 
A(s). 
Such a function is called a policy.  A policy  assigns a prob(cid:173)
ability to every state  trajectory 
starting in state 
so  which is given by the product of the transition probabili(cid:173)
ties 
If we further assume that 
actions in goal states have no costs and produce no changes 
the expected cost 
associated with a policy R starting in state SQ is given by the 
weighted average of the probability of such trajectories times 
An  optimal solution  is  a  policy 
their cost 
7r*  that has a minimum expected cost for all  possible  initial 
states. An optimal solution is guaranteed to exist provided the 
following assumption holds [Bertsekas, 19951: 
M8.  the  goal  is  reachable  from  every  state  with  non-zero 

where 

probability. 

Since  the  initial  state  s0  of the  system  is  fixed,  there  is  in 
principle no need to compute a complete policy but a partial 
policy prescribing the action to take in the states that can be 
reached  following  the  policy  from  s 0.  Traditional  dynamic 
programming methods like value or policy iteration compute 
complete policies,  while recent heuristic search DP methods 
like RTDP and LAO* compute partial  policies.  They achieve 
this by means of suitable heuristic functions h(s)  that provide 
admissible estimates  (lower bounds)  of the  expected cost  to 
reach the goal from any state s. 

2.2  Dynamic  Programming 
Any heuristic or value function h defines a greedy policy 7T/t: 

(1) 

where  the  expected  cost  from  the  resulting  states  ,s'  is  as(cid:173)
the greedy action 
sumed to be given by 
in  s  for the  value  function h.  If we  denote the optimal  (ex(cid:173)
pected) cost  from  a  state  s  to  the goal  by 
it  is  well 
known that the greedy policy 7r/t  is optimal when h is the op(cid:173)
timal  cost  function,  i.e.  h  =   

We call 

While due to the possible presence of ties in (1), the greedy 
policy  is  not  unique,  we  will  assume  throughout the  paper 
that these ties are broken systematically using an static order(cid:173)
ing on actions.  As a result, every value function V defines a 
unique greedy  policy 
and  the  optimal  cost  function  K* 
We define the relevant 
defines a unique optimal policy 
states as the states that are reachable from SQ using this opti(cid:173)
mal policy; they constitute a minimal set of states over which 
the optimal value function needs to be defined.2 

Value  iteration  (vi)  is  a  standard  dynamic  programming 
method for solving MDPs and is based on computing the op(cid:173)
timal cost function  V*  and plugging it into the greedy policy 
(1). This optimal cost function is the only solution to the fixed 

2This definition of 'relevant states*  is more restricted than the 
one in [Barto et al, 1995] that includes the states reachable from so 
by any optimal policy. 

point equation: 

(2) 

also  known  as  Bellman's  equation.  For  stochastic  short(cid:173)
est  path  problems  like  M1-M8  above,  the  border condition 
V(s)  =  0  is  also  needed  for  goal  states  s 
G.  Value  it(cid:173)
eration  solves (2)  by  plugging an  initial  guess for V*  in  the 
right-hand side of (2) and obtaining a new guess on the left-
hand side.  In  the  form of VI  known  as asynchronous value 
iteration  [Bertsekas,  19951,  this operation can  be expressed 
as: 

(3) 

where V is a vector of size |S| initialized arbitrarily (normally 
to 0) and where the equality in (2) is replaced by assignment. 
The  use  of expression  (3)  for updating a state  value  in  V  is 
called a state update or simply an update.  In standard (syn(cid:173)
chronous)  value  iteration,  all  states  are  updated  in  parallel, 
while in asynchronous value iteration, only a selected subset 
of states  is selected for update at a time.  In both cases,  it is 
known that if all  states are updated infinitely often, the value 
function  V  converges eventually  to  the  optimal  value  func(cid:173)
tion.  From a practical point of view, value iteration is stopped 
when the Bellman error or residual defined as the difference 
between left and right in (2): 

over all states s  is sufficiently small.  In the discounted  MDP 
formulation,  a  bound  on  the  policy  loss  (the  difference  be(cid:173)
tween  the expected cost of the  policy and  the expected  cost 
of the  optimal  policy)  can  be  obtained  as  a  simple  expres(cid:173)
sion  of the  discount  factor  and  the  maximum  residual. 
In 
stochastic shortest path models, no similar closed-form bound 
is known, although such bound can be computed [Bertsekas, 
1995].  Thus, one can execute value iteration until the max(cid:173)
imum  residual  becomes  smaller  than  a given 
then  if  the 
bound on the policy loss is higher than desired, the same pro(cid:173)
cess can be repeated with a smaller 
and so on (see 
[Hansen and Zilberstein, 2001] for a similar idea).  For these 
reasons, we will take as our basic task below, the computa(cid:173)
tion of a value function V(s)  with residuals no greater than a 
given parameter  

One last definition and a few known results before we pro(cid:173)

ceed.  We say that cost function V  is monotonic iff 

(4) 

for  every  s 
S.  Notice  that  a  monotonic  value  function 
never decreases when updated, and moreover, must increase 
by  more  than  when  updated  in  a  state  whose  residual 
is  greater  than 
As  in  the  deterministic  setting,  a  non(cid:173)
monotonic cost  function  can  be  made  monotonic by  simply 
taking  the  value  V(s)  to  be  the  max  between  V(s)  and  the 
right-hand side of Bellman's equation.  The following results 
are well known. 
Theorem  1 a) The optimal values V* (s) of a model MJ-M8 
are  non-negative  and  finite;  b)  the  monotonicity  and  admis(cid:173)
sibility of a value function are preserved through updates. 

1234 

SEARCH 

start with a lower bound function 
repeat 

FIND a state s in the greedy graph Gv with  
REVISE  V  at  s 

until no such state is found 
return V 

Algorithm 1:  FlND-and-REVlSE 

3  Find-and-Revise 
The FlND-and-REVlSE schema is a general asynchronous VI 
algorithm that exploits  knowledge of the  initial  state  and  an 
admissible heuristic for computing optimal or nearly optimal 
policies  without having to evaluate the entire space.  Let  us 
say that a value function V 
-consistent (inconsistent) over 
a state ,s when the residual over s is no greater (greater) than f, 
and that V  itself is  -consistent when it is  -consistent over all 
the states reachable from s0  and the greedy policy 
Then 
FlND-and-REVlSE computes an  -consistent value function by 
simply  searching  for inconsistent  states  in  the  greedy graph 
and updating them until no such states are left; see Alg. 1. 

The  greedy  graph 

refers to the  graph resulting from 
the execution of the greedy policy 
starting in s0; i.e., s0 is 
the single root node in  Gv,and for every non-goal state  s  in 
6 Vi its children are the states that may result from executing 
the action 

in s. 

The procedures FIND and REVISE are the two parameters of 
the FlND-and-REVlSE procedure.  For the convergence, opti-
mality, and complexity of FlND-and-REVlSE, we assume that 
FIND searches the graph systematically, and REVISE of V at 
s  updates  V  at  s  (and  possibly  at  some  other  states),  both 
operations taking 0(|S|)  time. 
Theorem 2 (Convergence)  For  a  planning  model  MI-M8 
with  an  initial  value  function  h  that  is  admissible  and 
monotonia  FlND-a/w/-REVlSE  yields  an 
value 
function  in  a  number  of  loop  iterations  no  greater  than 
where each iteration has time com-

Theorem 3  (Optimality)  For a planning model Ml -MS with 
an initial admissible and monotonia value function, the value 
function  computed by  FIND-and-REVlSE  approaches the op-
timal value function over all relevant states as e goes to 0. 

4  Labeling 
We  consider next  a  particular instance  of the  general  FIND-
and-REViSE  schema  in  which  the  FIND  operation  is  carried 
out  by  a  systematic  Depth-First  Search  that  keeps  track  of 
the states visited.  In addition, we consider a labeling scheme 
on top of this  search  that detects,  with  almost  no overhead, 
when  a  state  is  solved,  and  hence,  when  it  can  be  skipped 
in  all future  searches.  A  state  s  is  defined  as  solved when 
the value function V is e-consistent over s and over all states 
reachable from s and the greedy policy 
Clearly, when this 
condition holds no further updates are needed in s or the states 
reachable from 5.  The resulting algorithm terminates when 
the initial  state s0  is solved and hence when an e-consistent 
value function has been obtained. 

Figure 1:  A graph and its strongly-connected components. 

Due to the presence of cycles in the greedy graph, bottom-
up  algorithms  common  in  AO*  implementations  cannot  be 
used.  Indeed,  if s  is  reachable  (in  the  greedy  graph)  from 
a descendant s'  of s, then  bottom-up approaches will  be un(cid:173)
able to label either state as solved. A labeling mechanism that 
works  in  the  presence  of cycles  is  presented  in  [Bonet  and 
Geffner, 2003] for improving the convergence of RTDP.  Basi(cid:173)
cally, after each RTDP trial, an attempt is made to label the last 
unsolved state .s in the trial by triggering a systematic search 
for inconsistent  states from  .s.  If one  such  state  is  found,  it 
is updated, and a new trial is executed.  Otherwise, the state 
s and all its unsolved descendants are labeled as solved, and 
a new cycle of RTDP trials  and  labeling checks is  triggered. 
Here we take this  idea and improve it by removing the need 
of an extra search for label checking. The label checking will 
be done as part of the FIND (DFS) search with almost no over(cid:173)
head,  exploiting  Tarjan's  linear  algorithm  for  detecting  the 
strongly-connected components of a  directed graph  iTarjan, 
1972], and a correspondence between the strongly-connected 
components of the greedy graph and the minimal collections 
of states that can be labeled at the same time. 

Consider the  (directed) greedy graph 

and the relation 

between pairs of states s and 

that holds when 

and 

or when  s  is  reachable  from 
is  reachable  from  s 
in  Gv.  The  strongly-connected  components  of Gv  are  the 
equivalence classes defined by this relation and form a parti(cid:173)
tion  of the  set  of states  in  Gv.  For example,  for the  greedy 
graph in Fig.  1, where 2 and 4 are terminal (goal) states, the 
components are 
and 
C\  — 
Tarjan's  algorithm  detects  the  strongly-
connected components of a directed graph in  time 
while traversing the graph depth-first,  where n  stands for the 
in  Gv)  and  e  for  the  number of 
number  of states 
edges. 

C  is  solved.  Let's then define 

The relationship between labeling and strongly-connected 
components  in  Gv  is  quite  direct.  Let  us  say  first  that  a 
component C  is  e-consistent  when  all  states s 
consistent, and that a component C is solved when every state 
s 
as the graph whose 
and whose directed edges 
nodes are the components  of 
are 
is reachable from some 
state  in  C.  Clearly, 
is an acyclic graph  as two compo(cid:173)
nents which are reachable from each other will be collapsed 
into the same equivalence class. In addition, 

when some state in 

C  are 

1.  a  state  s  is  solved  iff its  component C  is  solved,  and 

furthermore, 

SEARCH 

1235 

2.  a component C is solved iff C is consistent and all com(cid:173)

ponents 

are solved. 

The  problem  of labeling  states  in  the  cyclic  graph  Gy  can 
thus be mapped into the problem of labeling the components 
in  the  acyclic  graph  Gy,  which  can  be  done  in  bottom  up 
fashion. 

From Fig. 1 is easy to visualize the component graph asso(cid:173)
ciated to the greedy graph. Thus, if 2 is the only inconsistent 
state,  for example, we can  label  the components C\  and Ci 
as solved, while leaving C3 and C4 unsolved. 

The code that simultaneously checks in depth-first fashion 
the  consistency  of the  states  and  the  possibility  of  labeling 
them  is  shown  in  Alg.  2.  We  call  the  resulting  algorithm, 
HDP.  HDP inherits its convergence and optimality properties 
from the FiND-and-REViSE schema and the correctness of the 
labeling mechanism. 

We do not have space to explain HDP code in detail, yet it 
should be clear to those familiar with Tarjan's algorithm;  in 
particular,  the  use  of the  state  visit  number,  S.IDX,  and  the 
'low-link' number, s.LOW, for detecting when a new compo(cid:173)
nent has been found.  The  flag flag and the (normal) propa(cid:173)
gation of the visit numbers prevent a component from being 
labeled as solved when it is inconsistent or can reach an  in(cid:173)
consistent component. 
Theorem 4 (Correctness)  The  value function  computed by 
HDP for a planning model M1-M8, given an initial admissible 
and monotonic value function, is t-consistent. 

5  Experimental Results 
We now evaluate the performance of HDP in comparison with 
other recent Heuristic Search/DP algorithms such as the sec(cid:173)
ond code for LAO* in [Hansen and Zilberstein, 2001], that we 
call Improved LAO* (ILAO*), and Labeled RTDP (LRTDP), a 
recent improvement of RTDP that accelerates its convergence 
[Bonet and Geffner, 2003]. We use parallel Value Iteration as 
the baseline.  We've implemented all these algorithms in C++ 
and the experiments have been run on a Sun Fire-280R with 
750 MHz and  1Gb of RAM. 

The domain that we use for the experiments is the racetrack 

from [Barto et al,  1995].  The states are tuples 
that  represent  the  position  and  speed  of the  car  in  the  x, y 
dimensions.  The  actions  are  pairs  a  —  (ax, ay)  of instan(cid:173)
taneous  accelerations  where 
Uncer(cid:173)
tainty  in  this  domain comes from assuming that the road  is 
'slippery'  and  as  a  result,  the  car may  fail  to  accelerate or 
desaccelerate.  More  precisely,  an  action  a  =  (ax, ay)  has 
its  intended  effect  with  probability  1  - p,  while  with  prob(cid:173)
ability p the  action effects correspond to those of the action 
=  (0,0).  Also, when the car hits a wall, its velocity is set 
to zero and its position is left intact (this is different than in 
[Barto et  al.,  1995] where for some reason the car is moved 
to the start position). 

We consider the track  l a r g e -b from [Barto et al,  1995], 
h- t r a ck  from  [Hansen  and  Zilberstein,  2001], 3  and  five 
other  tracks  (squares  and  rings  of different  size).  Informa(cid:173)
tion about these instances can be found in the first three rows 

3 T , Taken from the source code of LAO*. 

of Table 1, including number of states, optimal cost, and per(cid:173)
centage of states that are relevant. 

As heuristic, we follow [Bonet and Geffner, 2003], and use 
the domain  independent admissible and monotonic heuristic 
hmini  obtained  by  replacing  the  expected  cost  in  Bellman 
equation by the best possible cost.  The total time spent com(cid:173)
puting  heuristic  values  is  roughly  the  same  for the  different 
algorithms  (except VI),  and  is  shown  separately  in  the fifth 
row in the table, along with its value for s 0.  The experiments 
are carried with  three heuristics: 
and h  =  0. 

The results are shown in Table 1. HDP dominates the other 
algorithms over all  the  instances  for 
while LRTDP 
is best (with one or two exceptions) when the weaker heuris(cid:173)
tics 
and 0 are used.  Thus, while HDP seems best for 
exploiting  good  heuristic  information  over  these  instances, 
LRTDP  bootstraps  more quickly  (i.e.,  it quickly computes a 
good value function). We hope to understand the reasons for 

1236 

SEARCH 

Table  1:  Problem  data  and  convergence time  in  seconds  for  the  different algorithms  with  different  heuristics.  Results  for 
e —  10 -3 and probability p = 0.2. Faster times are shown in bold. 

these differences in the future. 

6  Approximation 
HDP, like FIND-and-REViSE, computes a value function V by 
enforcing  its  consistency  over the  states  reachable  from So  
and  the  greedy  policy  ny.  The  final  variation  we  consider, 
that we call H D P ( I ), works in the same way, yet it enforces 
the  consistency  of the  value  function  V  only over the  states 
that are reachable from so  and the greedy policy with some 
minimum  likelihood. 

For efficiency, we formalize this notion of likelihood, using 
a non-negative integer scale, where 0 refers to a normal out(cid:173)
come, 1 refers to a somewhat surprising outcome, 2 to a still 
more surprising outcome, and so on. We call these measures 
plausibilities, although it should be kept in mind, that 0 refers 
to the most plausible outcomes, thus 'plausibility greater than 
f, means  'a plausibility measure smaller than or equal to i.y 
We  obtain  the  transition  plausibilities  Ka(s'\s)  from  the 
corresponding  transition  probabilities  by  the  following  dis(cid:173)
cretization: 

(5) 

when 

with 
Plausibilities  are 
thus formalized': the most plausible next states have always 
plausibility  0.  These  transition  plausibilities  are  then  com(cid:173)
bined by the rules of the K calculus  iSpohn,  1988]  which is 
a calculus isomorphic to the probability calculus (e.g.  [Gold-
szmidt and Pearl, 1996]). The plausibility of a state trajectory 
given  the  initial  state,  is  given  by  the  sum  of the  transition 
plausibilities  in  the  trajectory,  and the  plausibility  of reach(cid:173)
ing a state,  is given by the plausibility of the most plausible 
trajectory reaching the state. 

The HDP(i)  algorithm, for a non-negative integer z, com(cid:173)
putes a value function V by enforcing its 
consistency over 
the  states  reachable  from  So  with  plausibility  greater  than 
or equal to  i.  HDP(i)  produces approximate policies  fast by 
pruning certain paths in the search. The simplest case results 

from i  = 0, as the code for HDP(O) corresponds exactly to the 
code for HDP, except that the possible successors of a state a 
in the greedy graph are replaced by the plausible successors. 
HDP(i)  computes lower bounds that tend to be quite tight 
over the states that can be reached with plausibility no smaller 
than  i.  At run  time,  however,  executions may  contain  'sur(cid:173)
prising*  outcomes,  taking the  system  'out'  of this  envelope, 
into states where the quality of the value function and its cor(cid:173)
responding policy,  are  poor.  To  deal  with  those  situations, 
we  define  a  version  of  HDP(i),  called  HDP(t,j),  that  inter(cid:173)
leaves planning  and  execution  as  follows.  HDP(i, j)  plans 
from  s  =  so  by  means  of the  HDP(i)  algorithm,  then  exe(cid:173)
cutes this policy until a state trajectory with plausibility mea(cid:173)
sure  greater than  or equal  to j,  and  leading  to  a  (non-goal) 
state s' is obtained.  At that point, the algorithm replans from 
s' with HDP(i), and the same execution and replanning cycle 
is followed until  reaching the goal.  Clearly,  for sufficiently 
large j,  HDP(i,j)  reduces  to  HDP(i),  and  for large  i,  HDP(i) 
reduces to HDP. 

Table  2  shows  the  average  cost  for  HDP(i,j)  for  i  =  0 
(i.e., most plausible transitions considered only), and several 
values  for j  (replanning  thresholds).  Each  entry  in  the  ta(cid:173)
ble  correspond  to  an  average  over  100  independent execu(cid:173)
tions.  We also include the average cost for the greedy policy 
with  respect to 
as  a  bottom-line reference for the fig(cid:173)
ures.  Memory in  the table refers  to the number of evaluated 
states.  As these results show, there is a smooth tradeoff be(cid:173)
tween  quality  (average  cost  to  the  goal)  and  time  (spent  in 
initial  planning  and  posterior replannings)  as  the  parameter 
j  vary.  We  also  see  that  in  this  class  of problems  the  hmtn 
heuristic  delivers  a  very  good  greedy  policy.  Thus,  further 
research  is necessary  to assess  the goodness of HD?(i,j)  and 
the hmin heuristic. 

7  Related Work 
We have built on IBarto et al., 1995] and [Bertsekas,  1995], 
and  more  recently  on  [Hansen  and  Zilberstein,  2001]  and 
[Bonet  and Geffner,  2003].  The  crucial  difference between 

SEARCH 

1237 

algorithm 
HDP(hmin) 
HDP(0,2) 
HDP(0,4) 
HDP(0,16) 
HDP(0,64) 
grccdy(hmi„) 

time 
7.547 
0.853 
0.826 
0.701 
0.698 
N/A 

h - t r a ck 
quality  memory 
35835 
41.894 
42.950 
7132 
7034 
44.000 
6100 
46.800 
5899 
46.800 
356 
47.150 

square- -2 
quality  memory 
7245 
11.130 
11.250 
819 
650 
11.500 
11.500 
556 
11.610 
564 
12.450 
104 

time 
0.275 
0.021 
0.022 
0.017 
0.014 
N/A 

time 
4.145 
0.311 
0.327 
0.284 
0.264 
N/A 

r i n g -: \ 
quality  memory 
43358 
22.047 
22.000 
7145 
6882 
23.300 
6331 
23.600 
6322 
25.500 
25.390 
192 
10  3andp = 0.2. Each value 

r i n g -4 
quality  memory 
152750 
27.785 
28.500 
24195 
23857 
28.400 
21823 1 
30.800 
21823 
32.400 
241 
31.600 

time 
30.511 
1.758 
1.821 
1.580 
1.518 
N/A 

Table 2:  Results of HDP(0,y)  for  j  =  2,4,16,64 and greedy policy with respect to 
is the average over 100 executions. N/A in time for the greedy policy means "Not Applicable" since there is no planning. 

FiND-and-REViSE and general  asynchronous value  iteration 
is the focus of the former on the states that are reachable from 
the initial state SO and the greedy policy. In RTDP, the FIND 
procedure is not systematic and is carried out by a stochas(cid:173)
tic  simulation  that may  take time greater than  0(|5|)  when 
the inconsistent states are reachable with low probability (this 
explains why RTDP final convergence is slow; see [Bonet and 
Geffner, 2003]).  LAO*, on the other hand, keeps track of a 
subset  of states,  which  initially  contains  .s0  only,  and  over 
which  it  incrementally  maintains  an  optimal  policy  through 
a somewhat expensive REVlSE (full  DP) procedure.  This is 
then relaxed in the second algorithm ir. [Hansen and Zilber-
stein, 2001], called Improved LAO* here.  The use of an ex(cid:173)
plicit envelope that is gradually expanded is present also  in 
iDean  et ai,  1993]  and  [Tash and  Russell,  1994].  Interest(cid:173)
ingly,  these  envelopes are expanded  by  including  the  most 
likely'  reachable states not yet  in  the envelope.  The algo(cid:173)
rithm HDP(i) exploits a similar idea but formulates it in a dif(cid:173)
ferent form and has a crisp termination condition. 

8  Summary 
We  have  introduced  and  analyzed  three  HS/DP  algorithms 
that exploit knowledge of the initial  state and an  admissible 
heuristic function for solving planning problems with uncer(cid:173)
tainty  and  feedback:  FiND-and-REViSE,  HDP,  and  HDP(I). 
FiND-and-REViSE  makes  explicit  the  basic  idea  underlying 
HS/DP algorithms: inconsistent states are found and updated, 
until no one is left. We have proved its convergence, complex(cid:173)
ity,  and optimality.  HDP adds a  labeling mechanism based 
on Tarjan's SCC algorithm and is strongly competitive with 
current algorithms.  Finally,  HDP(i)  and  HDP(i, j)  offer great 
time  and  memory  savings,  with  no  much  apparent  loss  in 
quality, in problems where transitions have probabilities that 
differ greatly in value, by focusing the updates on the states 
that are more likely to be reached. 

Acknowledgements: We thank Eric Hansen and Shlomo Zil-
berstein for making the code for LAO* available to us.  Blai 
Bonet is supported by grants from NSF, ONR, AFOSR, DoD 
MUR1  program,  and  by  a  USB/CON1C1T  fellowship from 
Venezuela.  Hdctor Geffner is  supported by  grant TIC2002-
04470-C03-02 from MCyT, Spain. 

References 
[Bario et  al,  1995]  A.  Barto,  S.  Bradtke,  and  S.  Singh. 
Learning to act using real-time dynamic programming. Ar(cid:173)
tificial  Intelligence,  72:81-138,1995. 

[Bellman,  1957]  R.  Bellman. 

Princeton University Press, 1957. 

Dynamic  Programming. 

[Bertsekas,  1995]  D. Bertsekas. Dynamic Programming and 

Optimal Control, (2 Vols), Athena Scientific, 1995. 

[Bonet and Geffner, 2003]  B. Bonet and H. Geffner. Labeled 
RTDP:  Improving the  convergence of real-time  dynamic 
programming. In Proc. ICAPS-03. To appear, 2003. 

iBoutilier et al., 1999]  C. Boutilicr, T. Dean, and S. Hanks. 
Decision-theoretic  planning:  Structural  assumptions  and 
computational  leverage.  Journal  of  Artificial  Intelligence 
Research, 11:1-94, 1999. 

IDean era/., 1993]  T.  Dean,  L.  Kaelbling,  J.  Kirman,  and 
A.  Nicholson.  Planning with  deadlines  in  stochastic  do(cid:173)
mains.  In  R.  Fikes and W.  Lehnert,  editors,  Proc.  11th 
National  Conf.  on  Artificial Intelligence,  pages  574-579, 
Washington, DC, 1993. AAA1 Press / MIT Press. 

[Goldszmidt and Pearl,  1996]  M.  Goldszmidt  and  J.  Pearl. 
Qualitative probabilities for default reasoning, belief revi(cid:173)
sion,  and causal  modeling.  Artificial Intelligence,  84:57-
112,1996. 

[Hansen and Zilberstein, 2001]  E.  Hansen  and  S.  Zilber-
stein.  LAO*:  A heuristic search algorithm that finds solu(cid:173)
tions with loops.  Artificial Intelligence,  129:35-62,2001. 
[Howard,  1960]  R.  Howard.  Dynamic  Programming  and 

Markov Processes. MIT Press, Cambridge, MA, 1960. 

[Korf,  1997]  R.  Korf.  Finding optimal  solutions  to  rubik's 
cube using patterns databases.  In B. Kuipers and B. Web(cid:173)
ber, editors, Proc.  14th National Conf. on Artificial Intelli(cid:173)
gence, pages 700-705, Providence, RI, 1997. AAA1 Press 
/ MIT Press. 

[Puterman, 1994]  M. Puterman. Markov Decision Processes 
- Discrete Stochastic Dynamic Programming.  John Wiley 
and Sons, Inc., 1994. 

[Spohn, 19881  W. Spohn. A general non-probabilistic theory 
of inductive reasoning.  In Proc. 4th Conf. on Uncertainty 
in Artificial Intelligence,  pages  149-158,  New  York,  NY, 
1988. Elsevier Science Publishing Company, Inc. 

[Tarjan, 1972]  R.  E.  Tarjan.  Depth  first  search  and  linear 
graph algorithms. S1AM Journal on Computing, 1(2): 146-
160,1972. 

[Tash and Russell, 1994]  J.  Tash  and  S.  Russell.  Control 
strategies for a stochastic planner.  In B.  Hayes-Roth and 
R.  Korf,  editors,  Proc.  12th  National  Conf  on  Artificial 
Intelligence, pages 1079-1085, Seattle, WA,  1994. AAAI 
Press / MIT Press. 

1238 

SEARCH 


Query-driven Constraint Acquisition

Christian Bessiere

LIRMM-CNRS

Remi Coletta

Barry O’Sullivan

LRD

4C, Computer Science Dept.

Mathias Paulin
LIRMM-CNRS

U. Montpellier, France

Montpellier, France

UCC, Ireland

U. Montpellier, France

bessiere@lirmm.fr

coletta@l-rd.fr

b.osullivan@4c.ucc.ie

paulin@lirmm.fr

Abstract

The modelling and reformulation of constraint net-
works are recognised as important problems. The
task of automatically acquiring a constraint net-
work formulation of a problem from a subset of its
solutions and non-solutions has been presented in
the literature. However, the choice of such a subset
was assumed to be made independently of the ac-
quisition process. We present an approach in which
an interactive acquisition system actively selects a
good set of examples. We show that the number of
examples required to acquire a constraint network
is signiﬁcantly reduced using our approach.

1 Introduction
Constraint Programming (CP) provides a powerful paradigm
for solving combinatorial problems. However, the speciﬁca-
tion of constraint networks still remains limited to special-
ists in the ﬁeld. An approach to automatically acquiring con-
straint networks from examples of their solutions and non-
solutions has been proposed by [Bessiere et al., 2005]. Con-
straint acquisition was formulated as a concept learning task.
The classical version space learning paradigm [Mitchell,
1982] was extended so that constraint networks could be
learned efﬁciently. Constraint networks are much more com-
plex to acquire than simple conjunctive concepts represented
in propositional logic. While in conjunctive concepts the
atomic variables are pairwise independent, in constraint sat-
isfaction there are dependencies amongst them.

In [Bessiere et al., 2005] the choice of the subset of solu-
tions and non-solutions to use for learning was assumed to be
made before and independently of the acquisition process. In
this paper we present an approach in which the acquisition
system actively assists in the selection of the set of exam-
ples used to acquire the constraint network through the use
of learner-generated queries. A query is essentially a com-
plete instantiation of values to the variables in the constraint
network that the user must classify as either a solution or non-
solution of her ‘target’ network. We show that the number of
examples required to acquire a constraint network is signiﬁ-
cantly reduced if queries are selected carefully.

When acquiring constraint networks computing good
queries is a hard problem. The classic query generation strat-

egy is one in which, regardless of the classiﬁcation of the
query, the size of the version space is reduced by half. There-
fore, convergence of the version space can be achieved using
a logarithmic number of queries. Furthermore, in the clas-
sic setting, a query can be generated in time polynomial in
the size of the version space. When acquiring constraint net-
works, query generation becomes NP-hard. This is further
aggravated by the fact that in constraint acquisition, while the
ordering over the hypothesis space is most naturally deﬁned
in terms of the solution space of constraint networks, we usu-
ally learn at the constraint level, i.e. a compact representation
of the set of solutions of a hypothesis. Our main contribution
is a number of algorithms for identifying good queries for ac-
quiring constraint networks. Our empirical studies show that
using our techniques the number of examples required to ac-
quire a constraint network is signiﬁcantly reduced. This work
is relevant to interactive scenarios where users are actively in-
volved in the acquisition process.

2 Constraint Acquisition using CONACQ

A constraint network is deﬁned on a (ﬁnite) set of variables X
and a (ﬁnite) set of domain values D. This common knowl-
edge shared between the learner and the user is called the
vocabulary. Furthermore, the learner has at its disposal a
constraint library from which it can build and compose con-
straints. The problem is to ﬁnd an appropriate combination of
constraints that is consistent with the examples provided by
the user. For the sake of notation, we shall assume that every
constraint deﬁned from the library is binary. However, the re-
sults presented here can be easily extended to constraints of
higher arity, and this is demonstrated in our experiments.

A binary constraint cij is a binary relation deﬁned on D
that speciﬁes which pairs of values are allowed for variables
xi, xj. The pair of variables (xi, xj) is called the scope of cij.
For instance, ≤12 denotes the constraint speciﬁed on (x1, x2)
with relation “less than or equal to”. A binary constraint net-
work is a set C of binary constraints. A constraint bias is a
collection B of binary constraints built from the constraint li-
brary on the given vocabulary. A constraint network C is said
to be admissible for a bias B if for each constraint cij in C
there exists a set of constraints {b1
ij} in B such that
cij = b1

ij, · · · , bk

ij ∩ · · · ∩ bk
ij.

An instance e is a map that assigns to each variable xi in

IJCAI-07

50

X a domain value e(xi) in D. Equivalently, an instance e
can be regarded as a tuple in Dn
. An instance e satisﬁes a
binary constraint cij if the pair (e(xi), e(xj )) is an element
of cij; otherwise we say that cij rejects e. If an instance e
satisﬁes every constraint in C, then e is called a solution of C;
otherwise, e is called a non-solution of C.

Finally, a training set Ef

consists of a set E of instances
and a classiﬁcation function f : E → {0, 1}. An element
e in E such that f (e) = 1 is called positive example (of-
ten denoted by e+
) and an element e such that f (e) = 0 is
called negative example (often denoted by e−
). A constraint
network C is said to be consistent with a training set Ef
if
every positive example e+
is a solution of C and every
negative example e−
in Ef
is a non-solution of C. We also
say that C correctly classiﬁes Ef
. Given a constraint bias B
and a training set Ef
, the Constraint Acquisition Problem is
to ﬁnd a constraint network C admissible for the bias B and
consistent with the training set Ef

in Ef

.

A SAT-based algorithm, called CONACQ, was presented
in [Bessiere et al., 2005] for acquiring constraint networks
based on version spaces. Informally, the version space of a
constraint acquisition problem is the set of all constraint net-
works that are admissible for the given vocabulary and bias,
and that are consistent with the given training set. We denote
as VB(Ef ) the version space corresponding to the bias B and
the training set Ef
. In the SAT-based framework this version
space is encoded in a clausal theory K. Each model of the
theory K is a constraint network of VB(Ef ).

p

More formally, if B is the constraint bias, a literal is either
an atom bij in B, or its negation ¬bij . Notice that ¬bij is
not a constraint: it merely captures the absence of bij in the
acquired network. A clause is a disjunction of literals (also
represented as a set of literals), and the clausal theory K is a
conjunction of clauses (also represented as a set of clauses).
An interpretation over B is a map I that assigns to each con-
straint atom bij in B a value I(bij) in {0, 1}. A transforma-
tion is a map φ that assigns to each interpretation I over B
the corresponding constraint network φ(I) deﬁned according
to the following condition: cij ∈ φ(I) iff cij =
ij ∈ B :
ij ) = 1}. An interpretation I is a model of K if K is true in
I(b
I according to the standard propositional semantics. The set
of all models of K is denoted Models(K). For each instance
e, κ(e) denotes the set of all constraints bij in B rejecting e.
For each example e in the training set Ef
, the CONACQ al-
gorithm iteratively adds to K a set of clauses so that for any
I ∈ M odels(K), the network φ(I) correctly classiﬁes all al-
ready processed examples plus e. When an example e is pos-
itive, unit clauses {¬bij} are added to K for all bij ∈ κ(e).
bij ∈κ(e) bij}
When an example e is negative, the clause {
is added to K. The resulting theory K encodes all candi-
date networks for the constraint acquisition problem. That
is, VB(Ef ) = {φ(m) | m ∈ M odels(K)}.

{b

(cid:2)

(cid:3)

p

Example 1 (CONACQ’s Clausal Representation) We wish
to acquire a constraint network involving 4 variables,
x1, . . . , x4, with domains D(x1) = . . . = D(x4) =
{1, 2, 3, 4}. We use a complete and uniform bias, with L =
{≤, (cid:6)=, ≥} as a library. That is, for all 1 ≤ i < j ≤ 4, B con-
tains ≤ij , (cid:6)=ij and ≥ij . Assume that the network we wish to

Table 1: An example of the clausal representation built by
CONACQ, where each example e?

i = (x1, x2, x3, x4).

Ef
{e+
1 }
{e+
2 }
{e−
3 }

example

(1,2,3,4)
(4,3,2,1)
(1,1,1,1)

clauses added to K
¬ ≥12 ∧¬ ≥13 ∧¬ ≥14 ∧¬ ≥23 ∧¬ ≥24 ∧¬ ≥34
¬ ≤12 ∧¬ ≤13 ∧¬ ≤14 ∧¬ ≤23 ∧¬ ≤24 ∧¬ ≤34
((cid:6)=12 ∨ (cid:6)=13 ∨ (cid:6)=14 ∨ (cid:6)=23 ∨ (cid:6)=24 ∨ (cid:6)=34)

acquire contains only one constraint, namely x1 (cid:6)= x4; there
is no constraint between any other pair of variables. For each
example e (ﬁrst column), Table 1 shows the clausal encoding
constructed by CONACQ after e is processed, using the set
κ(e) of constraints in the bias B that can reject e.
(cid:2)

The learning capability of CONACQ can be improved by
exploiting domain-speciﬁc knowledge [Bessiere et al., 2005].
In constraint programming, constraints are often interdepen-
dent, e.g. two constraints such as ≥12 and ≥23 impose a re-
striction on the relation of any constraint deﬁned on the scope
(x1, x3). This is a crucial difference with conjunctive con-
cepts where atomic variables are pairwise independent. Be-
cause of such interdependency, some constraints in a network
can be redundant. cij is redundant in a network C if the con-
straint network obtained by deleting cij from C has the same
solutions as C. The constraint ≥13 is redundant each time
≥12 and ≥23 are present.

Redundancy must be carefully handled if we want to have
a more accurate idea of which parts of the target network are
not precisely learned. One of the methods to handle redun-
dancy proposed in [Bessiere et al., 2005], was to add redun-
dancy rules to K based on the library of constraints used to
build the bias B. For instance, if the library contains the
constraint type ≤, for which we know that ∀x, y, z, (x ≤
y) ∧ (y ≤ z) → (x ≤ z), then for any pair of constraints
≤ij, ≤jk in B, we add the Horn clause ≤ij ∧ ≤jk→≤ik in
K. This form of background knowledge can help the learner
in the acquisition process.

3 The Interactive Acquisition Problem
In reality, there is a cost associated with classifying instances
to form a training set (usually because it requires an answer
from a human user) and, therefore, we should seek to min-
imise the size of training set required to acquire our target
constraint network. The target network is the constraint net-
work CT expressing the problem the user has in mind. That is,
given a vocabulary X, D, CT is the constraint network such
that an instance on X is a positive example if and only if it is
a solution of CT .

During the learning process the acquisition system has
knowledge that can help characterise what next training ex-
ample would be ideal from the acquisition system’s point of
view. Thus, the acquisition system can carefully select ‘good’
training examples (which we will discuss in Section 4 in more
depth), that is, instances which, depending on how the user
classiﬁes them, can help reduce the expected size of the ver-
sion space as much as possible. We deﬁne a query and the
classiﬁcation assigned to it by the user as follows.

Deﬁnition 1 (Queries and Query Classiﬁcation) A query q
is an instance on X that is built by the learner. The user

IJCAI-07

51

classiﬁes a query q using a function f such that f (q) = 1 if q
is a solution of CT and f (q) = 0 otherwise.

Angluin [Angluin, 2004] deﬁnes several classes of queries,
among which the membership query is exactly the kind used
here. The user is presented with an unlabelled instance, and
is asked to classify it. We can now formally deﬁne the inter-
active constraint acquisition problem.

Deﬁnition 2 (Interactive Constraint Acquisition Problem)
Given a constraint bias B and an unknown user classiﬁcation
function f , the Interactive Constraint Acquisition Problem is
to ﬁnd a converging sequence Q = q1, . . . , qm of queries,
that is, a sequence such that: qi+1 is a query relative to B
and VB(E

i ) where Ei = {q1, . . . , qi}, and |VB(Ef

m)| = 1.

f

Note that the sequence of queries is built incrementally,
that is, each query qi+1 is built according to the classiﬁca-
tion of q1, . . . , qi. In practice, minimising the length of Q is
impossible because we do not know in advance the answers
from the user. However, in the remainder of the paper we
propose techniques that are suitable for interactive learning.

4 Query Generation Strategies

4.1 Polynomial-time Query Generation
In practice, it can be the case that an example e from the train-
ing set does not bring any more information than that which
has already been provided by the other examples that have
been considered so far. If we allow for queries to be gener-
ated whose classiﬁcation is already known based on the cur-
rent representation of the version space, K, then we will ask
the user to classify an excessive number of examples for no
improvement in the quality of our representation of the ver-
sion space of the target network. We exemplify this problem
with a short example.

Example 2 (A Redundant Query) Consider an acquisition
problem over the three variables x1, x2, x3, with the do-
mains D(x1) = D(x2) = D(x3) = {1, 2, 3, 4} using the
same constraint library as in Example 1. Given the posi-
+
1 = (cid:10)(x1, 1), (x2, 2), (x3, 3)(cid:11), K = ¬ ≥12
tive example e
∧¬ ≥13 ∧¬ ≥23. Asking the user to classify e2 =
(cid:10)(x1, 1), (x2, 2), (x3, 4)(cid:11) is redundant, since all constraints
rejecting it are already forbidden by K. Then any constraint
network in the version space accepts e2.
(cid:2)

We propose a simple (poly-time) technique that avoids
proposing such redundant queries to the user. This irredun-
dant queries technique seeks a classiﬁcation only for an ex-
ample e that cannot be classiﬁed, given the current represen-
tation K of the version space. An example e can be classiﬁed
by VB(Ef ) if it is either a solution in all networks in VB(Ef )
or a non-solution in all networks in VB(Ef ). e is a solution
in all networks in VB(Ef ) iff the subset κ(e)[K] of κ(e), ob-
tained by removing from κ(e) all constraints that appear as
negated literals in K, is empty. Alternatively, e is a non-
solution in all networks in VB(Ef ), if κ(e)[K] is a superset
of an existing clause of K.

Example 3 (An Irredundant Query) Consider again Ex-
+
ample 2 in which the positive example e
1 has been consid-
ered. The query e = (cid:10)(x1, 1), (x2, 2), (x3, 2)(cid:11) is irredundant.

This can be seen by considering the literals that would be
added to K by this query. If the query is classiﬁed as positive,
the clauses (¬ ≥12), (¬ ≥13) and (¬ (cid:6)=23) will be added to
K, otherwise the clause (≥12 ∨ ≥13 ∨ (cid:6)=23) will be added.
+
1 that both ≥12 and ≥13 must
Since we know from example e
be set to false, the only extra literal this new example adds is
either (¬ (cid:6)=23) or ((cid:6)=23) (indeed κ(e)[K] = {(cid:6)=23}). Regard-
less of the classiﬁcation of e, something new is learned, so
this is an irredundant query.
(cid:2)

4.2 Towards Optimal Query Generation
The technique presented in Section 4.1 guarantees that each
newly classiﬁed query e adds something new to K. How-
ever, different irredundant examples give us different gains
in knowledge. In fact, the gain for a query q is directly re-
lated to the size k of κ(q)[K] and its classiﬁcation f (q). If
f (q) = 1, k unary negative clauses will be added to K, then
k literals will be ﬁxed to 0.
In terms of CONACQ, we do
not have direct access to the size of the version space, unless
we wish to perform very expensive computation through the
clausal representation K. But assuming that the models of K
are uniformly distributed, ﬁxing k literals divides the number
of models by 2k
. If f (q) = 0, a positive clause of size k
is added to K, thus removing 1/2k
models. We can distin-
guish between queries that can be regarded as optimistic, or
as optimal-in-expectation.

An optimistic query is one that gives us a large gain in
knowledge when it is classiﬁed “in our favour”, but which
tells us very little when it is classiﬁed otherwise. More specif-
ically, in CONACQ the larger the κ(q)[K] of a query q, the
more optimistic it is. When classiﬁed as positive, such a
query allows us to set |κ(q)[K]| literals to 0. If the query is
classiﬁed as negative we just add a clause of size |κ(q)[K]|.
Therefore, an optimistic query is maximally informative –
sets all literals it introduces to 0 – if it is classiﬁed as positive,
but is minimally informative if it is classiﬁed as negative.

The optimal query strategy is one that involves proposing
a query that will reduce the size of the version space in half
regardless of how the user classiﬁes it. We deﬁne a query
as being optimal-in-expectation if we are guaranteed that one
literal will be ﬁxed to either a 0 or a 1 regardless of the clas-
siﬁcation provided by the user. Formally, such a query will
have a κ(q)[K] of size 1, therefore, if it is classiﬁed as positive,
we can set the literal in κ(q)[K] to 0, otherwise it is set to a 1.
We illustrate a sequence of queries that are sufﬁcient for
the version space of the problem presented as Example 1 to
converge using queries that are optimal-in-expectation.

Example 4 (Optimal-in-Expectation Queries) We want to
converge on the target network from Example 1 (i.e., the only
constraint x1 (cid:6)= x4 in a network with four variables and the
complete bias of constraints {≤, (cid:6)=, ≥}). Recall that hav-
−
3 }, the
ing processed the set of examples E = {e
unique positive clause in K is Cl = ((cid:6)=12 ∨ (cid:6)=13 ∨ (cid:6)=14
∨ (cid:6)=23 ∨ (cid:6)=24 ∨ (cid:6)=34). All other atoms in K are ﬁxed to 0
because of e
2 } refers to
(¬ ≥12) ∧ . . . (¬ ≥34) ∧ (¬ ≤12) ∧ . . . ∧ (¬ ≤34). Accord-
ing to this notation, the clausal theory K built by CONACQ
2 } ∧ Cl. Table 2 shows
having processed E is K = K{e+

+
2 . In the following K{e+

+
1 and e

1 ,e+

+
1 , e

+
2 , e

1 ,e+

IJCAI-07

52

Table 2: Optimal-in-expectation query generation strategy on Example 4.

e

e4 = (1, 1, 2, 3)

e5 = (2, 1, 1, 3)
e6 = (2, 3, 1, 1)
e7 = (1, 3, 1, 2)
e8 = (2, 1, 3, 1)

κ(e)[K]
{(cid:6)=12}

{(cid:6)=23}
{(cid:6)=34}
{(cid:6)=13}
{(cid:6)=24}

f (e)

+

+

+

+

+

K

K

K

K

K

{e

{e

{e

}

+
+
1 ,e
2
+
+
2 }
1 ,e
+
+
2 }
1 ,e
+
+
2 }
1 ,e
+
+
1 ,e
2

}

{e

{e

∧ (¬ (cid:6)=12)

∧

((cid:6)=13 ∨ (cid:6)=14 ∨ (cid:6)=23 ∨ (cid:6)=24 ∨ (cid:6)=34)

K

∧

((cid:6)=13 ∨ (cid:6)=14 ∨ (cid:6)=24 ∨ (cid:6)=34)
∧ (¬ (cid:6)=12) ∧ (¬ (cid:6)=23)
∧ (¬ (cid:6)=12) ∧ (¬ (cid:6)=23) ∧ (¬ (cid:6)=34)
((cid:6)=13 ∨ (cid:6)=14 ∨ (cid:6)=24)
∧ (¬ (cid:6)=12) ∧ (¬ (cid:6)=23) ∧ (¬ (cid:6)=34) ∧ (¬ (cid:6)=13) ∧ ((cid:6)=14 ∨ (cid:6)=24)
∧ (¬ (cid:6)=12) ∧ (¬ (cid:6)=23) ∧ (¬ (cid:6)=34) ∧ (¬ (cid:6)=13) ∧ (¬ (cid:6)=24)∧((cid:6)=14)

∧

a sequence of queries that are optimal-in-expectation on the
version space obtained after the three ﬁrst examples are pro-
cessed. The goal is to reduce VB(E) to contain a single hy-
pothesis. The ﬁrst column is a query e generated according to
the optimal-in-expectation strategy. The second column gives
the set κ(e)[K] of constraints still possible in a network of the
version space that could reject e. The third column is the clas-
siﬁcation of e by the user, and the fourth column is the update
of K. The query e4 is such that (cid:6)=12 is the only constraint
still possible in the version space that can reject it. Because
it is classiﬁed as positive, we are sure (cid:6)=12 cannot belong to
a network in the version space. CONACQ adds (¬ (cid:6)=12) to K
and the literal (cid:6)=12 is removed from Cl by unit propagation.
The process repeats with e5, e6 and e7, decreasing the size
of Cl by one literal at a time, and thus reducing the version
space by half. Finally, e8 is the last example required to en-
sure that the version space converges on the target network,
which contains the single constraint x1 (cid:6)= x4.

Note that at the beginning of this example, the version
space VB(E) contained 26
possible constraint networks, and
we could converge using O(log2|VB(E)|) queries, which is
an optimal worst-case [Mitchell, 1982].

(cid:2)

In Example 4, we always found an example e with
|κ(e)[K]| = 1, as the optimal-in-expectation strategy requires.
However, redundancy can prevent us from being able to gen-
erate an example e with a given size for its κ(e)[K]. For in-
stance, consider the acquisition problem, using a complete
and uniform bias, with L = {≤, (cid:6)=, ≥} as a library, and with
x1 = x2 = x3 as a target network. After processing an initial
+
1 = (2, 2, 2)), the possible
positive example (for instance e
constraints in the version space are ≤12, ≤13, ≤23, ≥12, ≥13
, ≥23. Hence, every further negative example e has either a
κ(e)[K] of size 3 (if no variables equal) or a κ(e)[K] of size 2
(if two variables equal). Therefore, no example with a κ(e)[K]
of size 1 can be generated. Redundancy prevents us from gen-
erating such examples.

5 Implementing our Strategies
In Section 4.2, we presented two strategies for generating
queries: optimal-in-expectation and optimistic. These two
strategies are characterised by the target number t of con-
straints still possible in the version space that reject the in-
stances q they try to produce. However, it may be the case
that, due to redundancy between constraints, there does not
exist any network in the version space that has a solution s
with |κ(s)[K]| = t. (And it is useless to ask classiﬁcation of
an instance if it is not a solution of some network in the ver-
sion space – see Section 4.1). We then must allow for some

uncertainty in the number of constraints rejecting an instance.
We implement the query generation problem as a two
step process. First, Algorithm 1 tries to ﬁnd an interpre-
tation I on B such that any solution s of φ(I) is such that
t −  ≤ |κ(s)[K]| ≤ t + , where  is the variation accepted
on the size of the κ(q)[K] of the query q we want to generate.
This algorithm takes another input parameter which is the set
L of constraints in which κ(q)[K] must be included. We will
explain later that this is a way to monitor the ‘direction’ in
which we want to improve our knowledge of the target net-
work of the user. Second, once I has been found, we take a
solution of φ(I) as a query. We ﬁrst present the algorithm,
then we will discuss its complexity and describe how we can
use it to implement our strategies (by choosing the values t
and ).

Algorithm 1: QUERY GENERATION PROBLEM

input : B the bias, K the clausal theory, L a set of
literals, t a target size and  the variation

output: An interpretation I
F ← K
foreach bij ∈ B \ {bij | (¬bij ) ∈ K} do

if bij (cid:6)∈ L then F ← F ∧ (bij )
else F ← F ∧ (bij ∨ bij)

lower ← max(|L| − t − , 1)
upper ← min(|L| − t + , |L|)
F ← F ∧ atLeast(lower, L) ∧ atM ost(upper, L)
if M odels(F) (cid:6)= ∅ then return a model of F
else return “inconsistency”

1

2

3

4

5

6

7

8

Algorithm 1 works as follows.

It takes as input the tar-
get size t, the allowed variation  and the set L of literals
on which to concentrate. The idea is to build a formula F
for which every model I will satisfy the requirements listed
above. F is initialised to K to guarantee that any model will
correspond to a network in the version space (line 1). For
each literal bij not already negated in K (line 2), if bij does
not belong to L, we add the clause (bij ) to F to enforce the
constraint bij to belong to the network φ(I) for all models I
of F (‘then’ instruction of line 3). Hence, any solution s of
φ(I) will be rejected either by a constraint in L or a constraint
bij already negated in K (so no longer in the version space).
Thus, κ(s)[K] ⊆ L. We now have to force the size of κ(s)[K]
to be in the right interval. If bij belongs to L (‘else’ instruc-
tion of line 3), we add the clause (bij ∨ bij ) to F to ensure
that either bij or its complementary constraint bij is in the re-

IJCAI-07

53

sulting network.1 bij is required because ¬bij only expresses
the absence of the constraint bij . ¬bij is not sufﬁcient to en-
force bij to be violated. We now just add two pseudo-Boolean
constraints that enforce the number of constraints from L vi-
olated by solutions of φ(I) to be in the interval [t −  .. t + ].
This is done by forcing at most |L| − t +  constraints and at
least |L| − t −  constraints to be satisﬁed (lines 4-6). The
‘min’ and ‘max’ ensure we avoid trivial cases (no constraint
from L is violated) and to remain under the size of L. Line 7
searches for a model of F and returns it. But remember that
redundancy may prevent us from computing a query q with a
given κ(q)[K] size (Section 4.2). So, if  is too small, F can be
unsatisﬁable and an inconsistency is returned (line 8).

The following property tells us when the output of Algo-

rithm 1 is guaranteed to lead to a query.

Property 1 (Satisﬁability) Given a bias B, a clausal theory
K, and a model I of K. If K contains all existing redundancy
rules over B, then φ(I) has solutions.

If not all redundancy rules belong to K, Algorithm 1 can
return I such that φ(I) is inconsistent. In such a case, we
extract a conﬂict set of constraints S from φ(I) and add the
bij ∈S ¬bij to K to avoid repeatedly generating mod-
clause
els I (cid:8)

with this hidden inconsistency in φ(I (cid:8)).

(cid:3)

The next property tells us that generating a given type of

query can be hard.

Property 2 Given a bias B, a theory K, a set L of constraints,
a target size t and a variation , generating a query q such
that: κ(q)[K] ⊂ L and t −  ≤ |κ(q)[K]| ≤ t +  is N P -hard.

The experimental section will show that despite its com-
plexity, this problem is handled very efﬁciently by the tech-
nique presented in Algorithm 1. The algorithm can be used
to check if there exists a query rejected by a set of constraints
from the version space of size t ±  included in a given set L.
The optimal-in-expectation strategy requires t = 1 and op-
timistic requires a larger t. In the following, we chose to be
“half-way” optimistic and to ﬁx t to |L|/2. There still remains
the issue of which set L to use and which values of  to try. 
is always initialised to 0. Concerning L, we take the smallest
non-unary positive clause of K. A positive clause represents
the set of constraints that reject a negative example already
processed by CONACQ. So, we are sure that at least one of
the constraints in such a set L rejects an instance. Choosing
the smallest one increases the chances to quickly converge
on a unary clause. If K does not contain any such non-unary
clauses we take the set containing all non-ﬁxed literals in K.
Since Algorithm 1 can return an inconsistency when called
for a query, we have to ﬁnd another set of input parameters on
which to call the algorithm. t is ﬁxed by the strategy, so we
can change L or . If there are several non-unary clauses in
K, we set L to the next positive clause in K (ordered by size).

1Not all libraries of constraints contain the complement of each
constraint. However, the complements may be expressed by a con-
junction of other constraints. For instance, in library ≤, (cid:3)=, ≥, ≤
does not exist but it can be expressed by (≥ ∧ (cid:3)=). If no conjunction
can express the complement of a constraint, we can post an approx-
imation of the negation (or nothing). We just lose the guarantee on
the number of constraints in L that will reject the generated query.

If we have tried all the clauses without success, we have to
increase . We have two options. The ﬁrst one, called closest,
will look for a query generated with a set L instantiated to
the clause that permits the smallest . The second one, called
approximate, increases  by ﬁxed steps. It ﬁrst tries to ﬁnd a
set L where a query exists with  = 0.25 · |L|. If not found, it
looks (repeatedly) with 0.50 · |L|, 0.75 · |L| and then |L|.

We thus have four policies to generate queries: optimistic
and optimal-in-expectation combined with closest and ap-
proximate: optimistic means t = L/2 whereas optimal-in-
expectation means t = 1; closest ﬁnds the smallest  whereas
approximate increases  by steps of 25%.

6 Experimental Results
We implemented CONACQ using SAT4J2 and Choco3. In
our implementation we exploit redundancy to the largest ex-
tent possible, using both redundancy rules and backbone de-
tection [Bessiere et al., 2005].

Problem Classes. We used a mix of binary and non-binary
problem classes in our experiments. We studied random bi-
nary problems, with and without structure, as well as ac-
quiring a CSP deﬁning the rules of the logic puzzle Sudoku.
CONACQ used a learning bias deﬁned as the set of all edges
in each problem using the library {≤, ≥, (cid:6)=}. The random
binary problems comprised 14 variables, with a uniform do-
main of size 20. We generated target constraint networks by
randomly selecting a speciﬁed number of constraints from
{<, ≤, =, ≥, >, (cid:6)=}, retaining only those that were soluble.
We also considered instances in which we forced some con-
straint patterns in the constraint graph to assess the effect of
structure [Bessiere et al., 2005]. We did this by selecting the
same constraint relation to form a path in the target network.
Finally, we used a 4 × 4 Sudoku as the target network. The
acquisition problem in this case was to learn the rules of Su-
doku from (counter)examples of grid conﬁgurations.

As an example of a non-binary problem, we considered
the Schur’s lemma, which is Problem 15 from the CSPLIB4.
In this case, CONACQ used the library of ternary constraints
{ALLDIFF, ALLEQUAL, NOTALLDIFF, NOTALLEQUAL}.

In Table 3 we report averaged results for 100 ex-
Results.
periments of each query generation approach on each of the
problem classes we studied. In each case the initial training
set contained a single positive example. In the table the ﬁrst
column contains a description of the target networks in terms
of number of variables and constraints. We report results for
each of the query generation approaches we studied. Random
is a baseline approach, generating queries entirely at random,
which may produce queries that are redundant with respect
to each other. The Irredundant approach generates queries at
random, but only uses those that can provide new information
to reﬁne the version space. Finally, Optimistic and Optimal-
in-expectation refer to approaches described in Section 5 and

2Available from: http://www.sat4j.org.
3Available from: http://choco.sourceforge.net.
4Available from: http://www.csplib.org.

IJCAI-07

54

Table 3: Comparison of the various queries generation approaches on different classes of problems. Time is measured in
milliseconds on a Pentium IV 1.8 GHz processor. We highlight the smallest number of queries for each problem class in bold.

Target Network
|X|

|C|

Random

Irredundant

Optimistic

Optimal-in-expectation

approximate

closest

approximate

closest

#q

time

#q

time #q

time #q

time #q

time #q

time

Random Binary Problem

14
14
14
14
14

14

16

6
8

1
2
4
14
40

14

72

6
12

48
118
> 1000
> 1000
> 1000

36
1
71
1
1
729
1 > 1000
1 > 1000

> 1000

1 > 1000

> 1000

1 > 1000

88
298

1
1

27
66

1
1
1
1
1

1

1

1
1

24
55
101
235
298
Pattern Binary Problem

19
87
237
412
1314

24
50
94
219
273

46
204
573
918
3048

220

178

21
56

17

197

34

Sudoku 4 × 4

154

168

186

Schur’s lemma

167
274

19
51

382
772

106
102
81
72
71

42

69

24
46

12
13
19
23
27

45

31

198
218

99
97
75
58
44

32

57

23
44

57
58
63
67
66

76

82

432
563

for both we consider the approximate and the closest vari-
ants. Each column is divided in two parts. The left part is
the number of queries needed to converge on the target net-
work; a limit was set 1000 queries. The right part measures
the average time needed to compute a query.

With the exception of very sparse random problems and
Schur’s Lemma, generating queries with Random is never
able to converge on the target hypothesis, even with a large
number of queries. The Irredundant approach is strictly bet-
ter than Random and successfully converged in a number of
cases. However, when the density of the target network in-
creases, Irredundant begins to struggle to converge.

Optimistic and Optimal-in-expectation are more accurate,
since they always enable us to converge, regardless of the
target network used. Their closest variants require an aver-
age computation time between 2 and 5 times longer than the
approximate ones, as to be expected. However, the closest
strategies have the advantage of being able to converge on the
target network by asking up to 40% fewer queries than the ap-
proximate strategies. Optimistic is the best approach on very
sparse networks, but as the number of constraints in the tar-
get network grows, Optimal-in-expectation becomes the best
strategy, since it requires both fewer queries to converge and
less computation time. The number of queries for Optimal-in-
expectation decreases when density increases because redun-
dancy rules apply more frequently, deriving more constraints.
Despite this, Optimistic performance decays when density in-
creases because the probability that a query is classiﬁed neg-
ative (unlucky case) grows with density.

7 Related Work

Recently, researchers have become interested in techniques
that can be used to acquire constraint networks in situations
where a precise statement of the constraints of the problem is
not available [Freuder and Wallace, 1998; Rossi and Sperduti,
2004]. The use of version space learning [Mitchell, 1982] as
a basis for constraint acquisition has received most attention
from the constraints community [O’Connell et al., 2003], but
the problem of query generation for acquiring constraint net-
works has not been studied.

8 Conclusion

In this paper we have tackled the question of how a constraint
acquisition system, based on CONACQ, can help improve the
interactive acquisition process by seeking fewer, but better se-
lected, examples to be proposed as queries for classiﬁcation
by a user. We have provided a theoretical and empirical eval-
uation of query generation strategies for interactive constraint
acquisition, with very positive results.

Acknowledgments

The authors would like to thank Frederic Koriche for very
useful discussions and comments.
This work has re-
ceived support from Science Foundation Ireland under Grant
00/PI.1/C075.

References
[Angluin, 2004] D. Angluin. Queries revisited. Theoretical

Computer Science, 313:175–194, 2004.

[Bessiere et al., 2005] C. Bessiere, R. Coletta, F. Koriche,
and B. O’Sullivan. Acquiring constraint networks using
a SAT-based version space algorithm.
In ECML, pages
23–34, 2005.

[Freuder and Wallace, 1998] E.C. Freuder and R.J. Wallace.
Suggestion strategies for constraint-based matchmaker
agents. In Proceedings of CP-1998, pages 192–204, 1998.

[Mitchell, 1982] T. Mitchell. Generalization as search. AI

Journal, 18(2):203–226, 1982.

[O’Connell et al., 2003] S. O’Connell, B. O’Sullivan, and
E.C. Freuder. A study of query generation strategies for
interactive constraint acquisition. In Applications and Sci-
ence in Soft Computing, pages 225–232. 2003.

[Rossi and Sperduti, 2004] F. Rossi and A. Sperduti. Acquir-
ing both constraint and solution preferences in interactive
constraint systems. Constraints, 9(4):311–332, 2004.

IJCAI-07

55


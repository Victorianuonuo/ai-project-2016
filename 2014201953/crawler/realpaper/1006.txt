Correlation Clustering for Crosslingual Link Detection

Jurgen Van Gael and Xiaojin Zhu

Computer Sciences Department
University of Wisconsin-Madison

Madison, WI 53706

{JVANGAEL, JERRYZHU@CS.WISC.EDU}

Abstract

The crosslingual link detection problem calls for
identifying news articles in multiple languages that
report on the same news event. This paper presents
a novel approach based on constrained clustering.
We discuss a general way for constrained cluster-
ing using a recent, graph-based clustering frame-
work called correlation clustering. We introduce
a correlation clustering implementation that fea-
tures linear program chunking to allow processing
larger datasets. We show how to apply the corre-
lation clustering algorithm to the crosslingual link
detection problem and present experimental results
that show correlation clustering improves upon the
hierarchical clustering approaches commonly used
in link detection, and, hierarchical clustering ap-
proaches that take constraints into account.

1 Introduction

Crosslingual link detection is the problem of identifying news
articles in multiple languages that report on the same news
event.
It is an important component in online information
processing systems, with applications in security and infor-
mation retrieval. Existing link detection systems are mostly
monolingual, with a small number of bilingual link detec-
tion systems [Allan et al., 2000; Chen and Chen, 2002;
Spitters and Kraaij, 2002] and very few crosslingual link
detection systems [Pouliquen et al., 2004] that work across
many languages. Like the latter, we assume monolingual link
detection has been done, such that news articles on the same
event in a single language already form a single group. This
assumption is mild, as existing systems like Google News
(http://news.google.com, the ‘all n related’ links)
do just this. Our goal is thus to cluster these monolingual
groups from different languages over a period of time, so that
groups reporting on the same event are in the same cluster.
One needs to take two things into consideration: 1. We would
rather not cluster any monolingual groups from the same lan-
guage together since we assume monolingual link detection
has done a reasonable job. This is known as ‘cannot-links’
in constrained clustering as we will discuss later; 2. We in
general do not know the number of clusters in advance.

In this paper we propose a principled approach to the
crosslingual link detection task using correlation cluster-
ing [Bansal et al., 2004; Demaine and Immorlica, 2003]. Cor-
relation clustering is a recent graph-based clustering frame-
work with interesting theoretical properties.
It can be for-
mulated to solve constrained clustering (also known as semi-
supervised clustering), which we will use for crosslingual link
detection. In constrained clustering, one performs clustering
with additional constraints (or preferences) on the data points.
Two typical constraints are must-link (where two items must
be in the same cluster) and cannot-link (where two items can-
not be in the same cluster). Constrained clustering has re-
ceived considerable attention in machine learning [Bilenko et
al., 2004; Wagstaff et al., 2001; Xing et al., 2003]; we point
to [Basu et al., 2006] for further references. Solving the cor-
relation clustering problem is hard but one natural way to ap-
proximate the best solution is to encode it in a linear program-
ming optimization framework. We combine correlation clus-
tering with a large-scale linear program solution technique
known as ‘chunking’ in order to solve larger crosslingual link
detection problems. The contribution of our paper is twofold:

1. we introduce a practical way for solving the complex
correlation clustering algorithm in [Demaine and Im-
morlica, 2003];

2. we demonstrate good performance on crosslingual link

detection using the correlation clustering approach.

In the rest of the paper, we start by reviewing correlation
clustering and discuss how to implement it using linear pro-
gramming chunking in section 2. We discuss related work in
constrained clustering and crosslingual link detection in sec-
tion 3. Finally we present experiments in section 4 where we
improve upon existing crosslingual link detection systems.

2 Correlation Clustering
Consider the following problem: we are given a weighted
graph for which we want to partition the nodes into clusters.
If two nodes share an edge with positive weight, we prefer
they be in the same cluster; if they share an edge with negative
weight, we prefer they end up in different clusters. The goal
of correlation clustering is to partition the graph into clusters
to maximally satisfy these preferences.

We review the discussion in [Demaine and Immorlica,
2003] on how to formally describe correlation clustering as

IJCAI-07

1744

an integer program (IP). Let G = (V, E) be a graph with
weight we for every edge1 e ∈ E. Let E+
be the set of edges
with positive weights, E+ = {e ∈ E|we > 0} and E−
be the
set of edges with negative weight, E− = {e ∈ E|we < 0}.
We now associate a binary variable xuv with every edge
(uv) ∈ E with the following interpretation: if xuv = 1 then
u, v are in different partitions, if xuv = 0 then u, v are in the
same partition. Intuitively xuv is the binary indicator variable
for whether we cut the edge or not. Correlation clustering
(cid:2)
minimizes the following objective

(cid:2)

wexe +

−we(1 − xe).

(1)

e∈E+

e∈E−

We want the variables to correspond to a valid partitioning:
if u, v are in the same cluster and v, t are in the same cluster,
then u, t must be so too. This can be achieved by the triangle
inequality constraints xuv+xvt ≥ xut below. Simplifying the
objective function we ﬁnd the correlation clustering integer
program:

(cid:3)

e∈E wexe
minx
subject to xe ∈ {0, 1},

xuv + xvt ≥ xut,
xuv = xvu,

∀e ∈ E

∀uv, vt, ut ∈ E

∀u, v ∈ V

(2)

The weights w are input to the algorithm, and can encode
must-links and cannot-links besides similarities between data
items. As formulated above, correlation clustering has two
attractive properties that make it suitable for crosslingual link
detection in particular and constrained clustering in general.
First of all, one does not need to specify the number of clus-
ters; the algorithm determines the optimal number of clusters
automatically. Secondly, the graph edge weights can be arbi-
trary and do not need to satisfy any metric condition.

2.1 Linear Program Approximation
Unfortunately solving the correlation clustering IP in (2) ex-
actly is N P -Hard. Recent theoretical results on approxima-
tion algorithms [Bansal et al., 2004], in particular [Demaine
and Immorlica, 2003], propose practical approaches to cor-
relation clustering. We build on the work in [Demaine and
Immorlica, 2003] where the authors describe an O(log n) ap-
proximation by relaxing the IP to a linear program (LP), and
rounding the solution of the LP by a region growing tech-
nique. We replace constraint xe ∈ {0, 1} by xe ∈ [0, 1] in
equation (2) to relax the IP to an LP. The solution to this LP
might include fractional values which we will have to round.
We point to [Demaine and Immorlica, 2003] for a detailed
description and theoretical analysis of the rounding algorithm
and limit ourselves to a qualitative description in this paper.
One can interpret the value of the LP variables as distances:
when a variable has value 0, the two adjacent nodes go in the
same cluster as their distance is 0 while if a variable is 1, the
two adjacent nodes go into different clusters. The rounding
procedure now needs to decide on how to partition the graph
given that some nodes are at fractional distances away from
each other.
Intuitively, the rounding algorithm will pick a

1We will denote an edge both as e ∈ E and as a pair of vertices

(uv) ∈ E

node in the graph and gradually grow a ball centered around
this node. While increasing the radius of the ball, all the
nodes that are at a distance smaller than the radius away from
the center of the ball will be included in the ball. The radius
grows until some technical termination condition is met. All
the nodes in the ball are then put into one cluster and removed
from the graph. This procedure is repeated until there are no
more nodes left in the graph. [Demaine and Immorlica, 2003]
prove that the original objective function (equation (1)) of the
LP relaxation will be bounded above by O(log n) times the
objective function of the IP where n is the number of nodes
in the graph.2

Unfortunately, the triangle inequalities could introduce up
to O(n3) constraints in the LP, which puts a heavy burden
on memory requirements. Next we discuss how we tradeoff
memory for runtime so we can solve correlation clustering
for larger problem sizes.

2.2 LP Chunking
Linear program chunking [Bradley and Mangasarian, 2000]
is a technique to convert a large linear program into an iter-
ative procedure on much smaller sub-problems, thus reduc-
ing the memory need. The iterative procedure produces the
same solution and is guaranteed to terminate. It works as fol-
lows: one ﬁrst breaks up all the constraints into chunks and
solves the optimization problem using only the ﬁrst chunk of
constraints. The active constraints are those inequality con-
straints that achieve equality at the solution. Next, one keeps
only the active constraints from the ﬁrst chunk, adds all con-
straints from the second chunk, and solves the LP again. This
procedure is repeated, looping through all chunks over and
over until some convergence criterion is met. One can arbi-
trarily set the size of the chunks to reduce the memory load
of the iterative procedure.

Let a general linear program be described as,

{c(cid:3)x|Hx ≥ b},

min

x

(3)

n, H ∈ R

with c ∈ R
. Let the constraints
[H b] be partitioned into l blocks, possibly of different
sizes, as follows:

m×n, b ∈ R

m

⎡
⎢⎣

H 1
...
H l

b1
...
bl

⎤
⎥⎦

[H b] =

(4)

At iteration j we compute xj
program,

by solving the following linear

{c(cid:3)xj |H (j mod l)xj ≥ b(j mod l) ∧ ¯H jxj ≥ ¯bj}, (5)

min

j

x

2Although the theoretical analysis in [Demaine and Immorlica,
2003] requires the ball to grow continuously, this is not practical.
By redeﬁning the volume in [Demaine and Immorlica, 2003] to be
the total volume of edges inside the ball as well as the total volume
inside the cut, i.e., replace pvw · xvw · (r − xuv) by pvw · xvw
in their deﬁnition, and modifying the description of step 3 in their
algorithm as: ‘Grow r by min{xuv − r > 0, v /∈ B(u, r)} so
that B(u, r) includes another entire edge’, the theoretical guarantee
stays the same but we only need to check the radius r a ﬁnite number
of times.

IJCAI-07

1745

¯b0] is empty and [ ¯H j

where [ ¯H 0
¯bj] is the set of active
constraint, i.e. all inequalities satisﬁed as equalities by xj
at
iteration j. We stop iterating when cT xj = cT xj+ν
for some
pre-speciﬁed integer ν. We point to [Bradley and Mangasar-
ian, 2000] for more details and proofs of the ﬁnite termination
of this algorithm.

3 Related Work

Constrained or semi-supervised clustering has enjoyed some
recent attention [Basu et al., 2006; Bilenko et al., 2004;
Davidson and Ravi, 2005; Wagstaff et al., 2001; Xing et
al., 2003]. In [Basu et al., 2006], the authors categorize all
semi-supervised methods into two classes: constraint-based
and distance-based methods. The constraint-based methods,
such as [Wagstaff et al., 2001] and to which our approach
belongs, rely on the must-link and cannot-link constraints to
guide the clustering algorithm in ﬁnding a partitioning that
does not violate the constraints. Distance-based methods,
such as [Xing et al., 2003], learn a metric using the con-
straint information and then apply existing clustering algo-
rithms to the data points in the learned metric space. These
approaches require specifying the number of clusters before-
hand. One solution to this issue is to use variants of hierarchi-
cal clustering that take constraints into account, e.g. [David-
son and Ravi, 2005]. By changing where to cut the dendro-
gram, one can control the number of clusters. The main dif-
ference between hierarchical clustering with constraints and
correlation clustering is that the former makes local, greedy
decisions at every step while correlation clustering optimizes
the clustering over the whole graph at once. One motivation
for our work is the observation that the crosslingual link de-
tection systems in [Pouliquen et al., 2004; Allan et al., 2000;
Chen and Chen, 2002] do not use constrained clustering tech-
niques.

So far, correlation clustering has not been applied to ma-
chine learning tasks very often. We are only aware of [Mc-
Callum and Wellner, 2005] who implement a more restricted
version of correlation clustering in [Bansal et al., 2004] for
noun co-reference.

The only crosslingual link detection system that cov-
ers a large set of languages we are aware of is described
in [Pouliquen et al., 2004]. The authors describe a system
which performs crosslingual link detection as well as mono-
lingual news tracking, i.e. the identiﬁcation of related news
over time in one particular language. Their approach uses
a very rich article representation based on extracting named
entities, keywords and geographical names.
In addition,
the articles are mapped onto the multilingual thesaurus EU-
ROVOC [Steinberger et al., 2002] which categorizes the arti-
cles in several of 6000 hierarchically organized subjects. Our
system, on the other hand, uses machine translation tools to
represent articles in a uniform way. This is a common [Diab
and Resnik, 2001] way of working with multilingual corpora.
Our experiments show that although the translation is noisy,
it does not signiﬁcantly affect performance. Our crosslingual
link detection task is also related to the work in [Diaz and
Metzler, 2007], where the authors introduce a framework for
aligning documents in parallel corpora based on topical cor-

Figure 2: Toy dataset

respondence.

4 Experiments

In this section, we ﬁrst illustrate correlation clustering on a
toy dataset. We then discuss how to solve the crosslingual
link detection problem using a correlation clustering based
constrained clustering approach and show how this improves
upon existing hierarchical clustering approaches.

4.1 Correlation Clustering on a Toy Dataset

It is straightforward to adapt correlation clustering for con-
strained clustering. Say we are given a set of items U =
{u1, u2, · · · , ul}, a pairwise similarity measure S : U ×U →
R, a set CM ⊂ U × U of must-link constraints and a set
CC ⊂ U × U of cannot-link constraints. We build a graph
G where the set of vertices is U . As a ﬁrst step, we add an
edge for all pairs of nodes not in CM ∪ CC and set the edge
weight according to the similarity measure S. Let M be a
constant that is sufﬁciently larger than the sum of the abso-
lute values of all weights in the graph so far. In the second
step, for all the pairs in CM and CC , we add either hard or
soft preferences: if we assume that the constraints are hard,
we add an edge for every must-link constraints with weight
M and an edge for every cannot-link constraint with weight
−M . If we want soft preferences, we can use values smaller
than M according to the strength of the preferences.

Figure 2 shows a toy dataset consisting of four nodes with
a cannot-link constraint between nodes 1 and 2. The weights
are speciﬁed in the ﬁgure. The edge not shown in the ﬁgure
has a similarity of zero. We use −1000 for the cannot-link
constraint edge weight. The objective function for this dataset
is to minimize −1000x(1,2)+30x(1,3) +25x(2,3) +20x(2,4) +
15x(3,4) subject to the triangle inequality constraints. Solv-
ing the IP exactly would give us a solution that assigns 1 to
all variables except x(2,3) = x(2,4) = x(3,4) = 0; this cor-
responds to the clustering {1}, {2, 3, 4}. Although nodes 1
and 3 have the highest similarity, the cannot-link constraint
guides the correlation clustering algorithm to not take node 1
into the cluster with 2 and 3. Note how a hierarchical clus-
tering algorithm would start off wrong as it merges nodes 1
and 3 together and thus fails to ﬁnd the best clustering. Even
a hierarchical clustering algorithm that takes constraints into
account will not ﬁnd the best clustering as it will greedily
merge nodes 1 and 3 together.

IJCAI-07

1746

Figure 1: Samples from the large dataset.

4.2 Crosslingual link detection
We generated ﬁve datasets by crawling Google News. We
speciﬁcally focused our experiments on news articles which
Google categorized as ‘world news’ as we assume this is the
category where the most interesting cross-lingual links can be
made. The ﬁrst four datasets each consist of 60 monolingual
‘world news’ article groups from three languages: English,
German and French. Each of these four datasets was gener-
ated one week apart by selecting the top 20 article groups for
each language in April 2006. This results in a total of 60 ar-
ticle groups in each dataset. In May 2006, we generated the
ﬁfth dataset which is larger and consists of 160 article groups
from the ‘world news’ category in eight different languages:
English, German, Italian, French, Portuguese, Spanish, Ko-
rean and Chinese. Figure 1 shows a sample from the larger
dataset. For all ﬁve datasets, we manually created a ground
truth clustering3.

For correlation clustering, we construct a fully-connected
graph where each node is a monolingual article group. We
create cannot-links between all pairs of article groups from
the same language and choose −108
as the weight for these
cannot-link edges. We compute similarity values between ar-
ticle groups from different languages with the following pro-
cedure: ﬁrst we concatenate all the article titles in a mono-
lingual group to form a ‘document representation’ for the
group. We then use Google machine translation to automati-
cally translate the ‘document’ into English, and remove stop-
words from the translation. Therefore monolingual groups
in different languages are represented by their corresponding
(noisy) English translation, providing a way to compute their
similarities. Empirically we found no difference in perfor-
mance using different machine translation tools such as Ba-
belﬁsh and Wordlingo. Next, for each monolingual group,
we convert the translated document into a TF.IDF vector
¯w = (w1w2 · · · w|V |), with wi = ni · log (|D|/|Di|), where
ni is the number of times word i appears in the document
representing the article group, D represents the set of arti-
cle groups in the dataset and Di represents the set of article
groups that include word wi. We compute the similarity swv
between any two TF.IDF vectors ¯w, ¯v as their inner product,

|V |(cid:2)

s ¯w¯v =

wi · vi.

(6)

i=1

Note that even with stop-word removal, two unrelated ar-

3The datasets are available at http://www.cs.wisc.edu/

∼jvangael/newsdata/.

ticle groups often have a small but positive similarity due
to common words.
If we use the similarity (6) directly as
graph edge weights for correlation clustering, many irrele-
vant groups will be clustered together. For the problem of
link detection, this is clearly not desirable. We therefore
subtract a bias constant t from all similarity values so that
wuv = s¯u¯v − t. Intuitively, too small a similarity (6) between
two article groups is in fact evidence that they should not be
in the same cluster. By changing the bias t we change the re-
sulting clustering, which is how we generate precision-recall
curves. For all the experiments presented below, we chose our
bias values as follows: we started with a bias such that only
one edge in the graph remains positively weighted. Next, we
steadily increase the bias such that another 0.1% of the edges
becomes positively weighted. On the small datasets, we re-
peated the experiments until 75% of the edges are positively
weighted while on the larger datasets we repeat the experi-
ments until 10% of the edges are positively weighted. We
compute precision and recall values relative to our manually
labeled ground truth. We count an edge as true positive (TP),
if its two article groups appear in the same cluster in both
ground truth and our results, false positive (FP) if they do not
appear in the same cluster in ground truth but do appear to-
gether in our results, and so on. Precision and recall is a better
measure than accuracy for our task, since the baseline of clas-
sifying every edge as ‘not in same cluster’ would have high
accuracy because of the large number of true negatives. We
used CPLEX 9.0 on a 3.0 GHz machine with 2GB RAM to
solve the linear programs.

Our ﬁrst round of experiments are designed to illustrate
how taking constraints into account improves performance
on the crosslingual link detection problem. We compare
our correlation clustering algorithm to the hierarchical clus-
tering approach which has commonly been used for the
crosslingual link detection problem, [Chen and Chen, 2002;
Pouliquen et al., 2004], and constrained hierarchical cluster-
ing such as [Davidson and Ravi, 2005]. Hierarchical clus-
tering is done by choosing a bias value and adding edges to
the graph in descending order according to their weight until
the edge weights become smaller than the bias. We then out-
put the connected components as the resulting clusters. Con-
strained hierarchical clustering is similar, except that at every
step we only add an edge if it does not introduce a path be-
tween two nodes in a cannot-link constraint. Again, we out-
put the connected components as the resulting clusters. The
left plot in Figure 3 shows the average precision-recall over
our four small datasets. If we keep the number of positively
weighted edges small (large bias) then both types of hierar-

IJCAI-07

1747

1

0.8

0.6

0.4

0.2

0
0

LP Solution
IP Solution
HC Solution
CHC Solution

0.2

0.4

0.6

0.8

1

1

0.8

0.6

0.4

0.2

0
0

LP
IP

103

102

101

100

s
d
n
o
c
e
s
 

n

i
 

e
m

i
t

n
u
R

LP Solution
IP Solution
HC Solution
CHC Solution

0.2

0.4

0.6

0.8

1

Bias t

102

103

104

Figure 3: Left: average precision-recall over four small datasets. Middle: precision-recall for the large dataset. Right: average
runtime over four small datasets.

chical clustering perform as well as correlation clustering.
Inspecting the datasets, this behavior can be explained by the
fact that there are a number of news events for which the arti-
cles use a subset of the vocabulary that is not commonly used
in other articles. Our similarity measure assigns large weights
among article groups in different languages on these events
and very small weight between these article groups and arti-
cle groups on a different topic. In a sense these are ‘easy’ in-
stances which both hierarchical clustering approaches as well
as correlation clustering get right. If we increase the num-
ber of positive edges (small bias) then the simple hierarchi-
cal clustering algorithm performs much worse than correla-
tion clustering. As a simple hierarchical clustering approach
has no notion of cannot-link constraints, it will cluster groups
from the same language together. Usually, crosslingual link
detection systems choose to leave these clusters out, but this
decision comes at the price of lower recall. Constrained hi-
erarchical clustering performs a little better as it takes our
assumption about the correctness of the monolingual groups
into account. Nonetheless, Figure 3 also shows that corre-
lation clustering, which takes the whole graph into account
instead of making local greedy decisions can still outperform
constrained hierarchical clustering. We attempted to compare
our approach to the constrained clustering in [Bilenko et al.,
2004] using their UTWeka implementation. The implemen-
tation ended up returning many empty clusters, resulting in
low precision and recall.

The middle plot in Figure 3 shows the precision-recall for
the large dataset; it indicates the trend we observed with the
smaller datasets: taking into account constraints can still im-
prove the performance of crosslingual link detection.

Next, let us consider the solution found by the approxi-
mation algorithm and the exact integer solution. Figure 3
shows that on the small datasets the two solution are exactly
equal. Inspecting the LP solutions, we ﬁnd that in the high
bias regime, almost no rounding is necessary as the LP so-
lution is the exact IP solution. Only in the low bias regime,
when more edges are positively weighted, rounding becomes
necessary. On the large dataset, Figure 3 shows that although
there is a small difference between the two solutions, the LP
relaxation with rounding does well to ﬁnd a good approx-
imation to the integer solution. We observed rather unex-
pected behavior from the rounding algorithm that inﬂuences

the precision-recall curves: at very low bias, due to the sym-
metry of the graph, the optimal LP solution has a number of
variables with 0.5 values. From the theoretical analysis of the
rounding algorithm, we know that a radius cannot grow to be
0.5. As a result of these properties, in the low bias regime
a large number of nodes will end up as singleton clusters.
This prohibits recall from increasing to 1.0 and we observe
the precision-recall curve loop back towards lower recall and
higher precision. Because the curve essentially follows the
ﬁrst path ‘in the opposite direction’ we did not include this in
Figure 3 for clarity.

Our next experiment was designed to evaluate how much
the LP approximation algorithm improves the runtime over
solving the IP exactly. The rightmost plot in Figure 3 shows
the average runtime over the four small datasets of solving the
exact IP compared to solving the approximation algorithm.
Every dot in the graph represents the time required to solve
either the IP or the LP with rounding for a speciﬁc bias. It
is clear from this ﬁgure that the LP approximation algorithm
for correlation clustering is signiﬁcantly faster than solving
the IP directly. However, even on the larger dataset the main
bottleneck is not so much the runtime but rather the memory
requirements. On this large dataset, the underlying graph has
160 nodes which results in over 2, 000, 000 constraints for
both the IP and LP. This is about as large a correlation clus-
tering instance we can solve without using chunking on our
machine with 2GB RAM.

Our last experiment shows the results of applying chunk-
ing to the LP for correlation clustering. Our experimental
setup is the following: we create instances of the correlation
clustering with random edge weights, distributed roughly ac-
cording to the instances of interest to crosslingual link detec-
tion. We chose our chunk size to be as large as possible while
still having some workspace memory for the processing in
between iterations: this resulted in 106
constraints per chunk.
Finally we use a value of ν = 4 as our stop condition. Table 1
shows the runtime for chunking versus solving the whole LP
at once. Correlation clustering instances of size 128 are the
ﬁrst instances where the number of constraints is larger than
the chunk size. At this size, the runtime overhead for chunk-
ing is mostly due to the stop condition. Starting from graphs
with around 200 nodes we cannot ﬁt the whole LP in memory
anymore and we must apply chunking to tradeoff memory for

IJCAI-07

1748

runtime. Table 1 shows that chunking is useful for scaling up
the size of solvable correlation clustering problem but has its
limitations too. First of all, the runtime increases fast: this
is due to the fact that doubling the size of the graph roughly
corresponds to an eight-fold increase in the number of con-
straints and equivalently an eight-fold increase in the number
of chunks. Another problem that arises is that the set of ac-
tive constraints ( ¯H) can become larger than the chunk size
and exhaust available memory. We believe these problems
are inherent to correlation clustering approximations based
on integer programming.

# nodes
64
128
192
256
320
384
448
512

# constraints whole LP
1 × 105
1 × 106
3 × 106
8 × 106
1 × 107
2 × 107
4 × 107
6 × 107

48
203
out of memory
out of memory
out of memory
out of memory
out of memory
out of memory

chunking LP
72
1065
1402
2708
5070
17298
52803
out of memory

Table 1: Runtime in Seconds

5 Conclusion
In this paper we introduce an implementation for correlation
clustering using linear program chunking that scales beyond
the implementation of the algorithm in [Demaine and Immor-
lica, 2003]. However, we ﬁnd that even our chunking method
which can trade off memory for runtime has its limits due to
the growth (O(n3)) of the linear program size. Nonetheless,
we believe that for constrained clustering problems of lim-
ited size (a few hundred data points) correlation clustering is
worth pursuing. Moreover, our experiments on the crosslin-
gual link detection task show that correlation clustering out-
performs both hierarchical clustering and hierarchical clus-
tering with constraints.

In future work, we plan to investigate whether other al-
gorithms for correlation clustering have smaller time and
space complexity. Also, we believe it would be interesting
to combine correlation clustering and our machine transla-
tion based representation with the rich document representa-
tion from [Pouliquen et al., 2004] to improve performance of
crosslingual link detection even more.

Acknowledgments
We thank Shuchi Chawla for helpful comments on correla-
tion clustering, Michael Thompson and Ted Wild for linear
program chunking.

References
[Allan et al., 2000] J. Allan, V. Lavrenko, D. Frey, and
V. Khandelwal. UMass at TDT 2000. Proceedings of Topic
Detection and Tracking Workshop, 2000.

[Bansal et al., 2004] N. Bansal, A. Blum, and S. Chawla.
Correlation Clustering. Machine Learning, 56(1):89–113,
2004.

[Basu et al., 2006] Sugato Basu, Mikhail Bilenko, Arindam
Banerjee, and Raymond J. Mooney. Probabilistic semi-
supervised clustering with constraints.
In O. Chapelle,
B. Sch¨olkopf, and A. Zien, editors, Semi-Supervised
Learning, pages 71–98. MIT Press, 2006.

[Bilenko et al., 2004] Mikhail Bilenko, Sugato Basu, and
Raymond J. Mooney. Integrating constraints and metric
learning in semi-supervised clustering. In ICML, 2004.

[Bradley and Mangasarian, 2000] PS Bradley and OL Man-
gasarian. Massive data discrimination via linear support
vector machines. Optimization Methods and Software,
13(1):1–10, 2000.

[Chen and Chen, 2002] Y.J. Chen and H.H. Chen. NLP and
IR approaches to monolingual and multilingual link detec-
tion. Proceedings of the 19th international conference on
Computational linguistics-Volume 1, pages 1–7, 2002.

[Davidson and Ravi, 2005] I. Davidson and S.S. Ravi. Ag-
glomerative Hierarchical Clustering with Constraints:
Theoretical and Empirical Results. Lecture notes in com-
puter science, pages 59–70, 2005.

[Demaine and Immorlica, 2003] E. Demaine and N. Immor-
lica. Correlation clustering with partial information. Proc.
of 6th APPROX, pages 1–13, 2003.

[Diab and Resnik, 2001] M. Diab and P. Resnik. An unsu-
pervised method for word sense tagging using parallel cor-
pora. Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 255–262, 2001.

[Diaz and Metzler, 2007] F. Diaz and D. Metzler. Pseudo-
Aligned Multilingual Corpora. Proceedings of the 20th
IJCAI, 2007.

[McCallum and Wellner, 2005] A. McCallum and B. Well-
ner. Conditional models of identity uncertainty with appli-
cation to noun coreference. Advances in NIPS, 17, 2005.

[Pouliquen et al., 2004] B. Pouliquen, R. Steinberger, C. Ig-
nat, E. K¨asper, and I. Temnikova. Multilingual and cross-
lingual news topic tracking. 20:23–27, 2004.

[Spitters and Kraaij, 2002] M. Spitters and W. Kraaij. Un-
supervised event clustering in multilingual news streams.
Proceedings of the LREC2002 Workshop on Event Mod-
eling for Multilingual Document Linking, pages 42–46,
2002.

[Steinberger et al., 2002] Ralf

Steinberger,

Bruno
Cross-lingual doc-
similarity calculation using the multilingual

Pouliquen, and Johan Hagman.
ument
thesaurus eurovoc. In CICLing, page 415, 2002.

[Wagstaff et al., 2001] K. Wagstaff, C. Cardie, S. Rogers,
and S. Schroedl. Constrained k-means clustering with
background knowledge. Proceedings of the 18th ICML,
pages 577–584, 2001.

[Xing et al., 2003] E.P. Xing, A.Y. Ng, M.I. Jordan, and
S. Russell. Distance metric learning, with application
to clustering with side-information. Advances in NIPS,
15:505–512, 2003.

IJCAI-07

1749


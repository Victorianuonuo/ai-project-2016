Color Learning on a Mobile Robot: Towards Full Autonomy under Changing

Illumination

Mohan Sridharan and Peter Stone
University of Texas at Austin, USA

smohan@ece.utexas.edu, pstone@cs.utexas.edu

Abstract

A central goal of robotics and AI is to be able to de-
ploy an agent to act autonomously in the real world
over an extended period of time. It is commonly
asserted that in order to do so, the agent must be
able to learn to deal with unexpected environmental
conditions. However an ability to learn is not suf-
ﬁcient. For true extended autonomy, an agent must
also be able to recognize when to abandon its cur-
rent model in favor of learning a new one; and how
to learn in its current situation. This paper presents
a fully implemented example of such autonomy in
the context of color map learning on a vision-based
mobile robot for the purpose of image segmenta-
tion. Past research established the ability of a robot
to learn a color map in a single ﬁxed lighting con-
dition when manually given a “curriculum,” an ac-
tion sequence designed to facilitate learning. This
paper introduces algorithms that enable a robot to i)
devise its own curriculum; and ii) recognize when
the lighting conditions have changed sufﬁciently to
warrant learning a new color map.

1 Motivation
Mobile robotic systems have recently been used in ﬁelds as
diverse as medicine, rescue, and surveillance [1; 10]. One
key enabler to such applications has been the development of
powerful sensors such as color cameras and lasers. However,
with these rich sensors has come the need for extensive sensor
calibration, often performed manually, and usually repeated
whenever environmental conditions change signiﬁcantly.

Here, we focus on the visual sensor (camera), arguably the
richest source of sensory information. One important subtask
of visual processing is color segmentation: mapping each im-
age pixel to a color label. Though signiﬁcant advances have
been made in this ﬁeld [3; 6], most of the algorithms are com-
putationally expensive to implement on a mobile robot and/or
involve a time consuming off-line preprocessing phase. Fur-
thermore, the resulting segmentation is typically quite sensi-
tive to illumination variations. A change in illumination could
require a repetition of the entire training phase.

Past research established that a robot can efﬁciently train
its own color map based on knowledge of the locations of

colored objects in the environment, but only when manually
given a sequence of actions to execute while learning (a cur-
riculum) [19]. Separately, it has been shown that a robot can
recognize illumination changes and switch among color maps
at appropriate times, given a ﬁxed set of pre-trained color
maps [18]. The prior work was also limited to controlled en-
vironments with only solid-colored objects.

This paper signiﬁcantly extends these results by enabling a
robot i) to recognize when the illumination has changed sufﬁ-
ciently to require a completely new color map rather than us-
ing one of the existing ones; and ii) to plan its own action se-
quence for learning the new color map on-line. Furthermore,
we introduce a hybrid color-map representation that enables
the robot to learn in less controlled environments, including
those with textured surfaces. All algorithms run in real-time
on the physical robot enabling it to operate autonomously in
an uncontrolled environment with changing illumination over
an extended period of time.

2 Problem Speciﬁcation

Here, we formulate the problem and describe our solution.
Section 2.1 presents the hybrid color-map representation used
for autonomous color learning. Section 2.2 describes our ap-
proach to detecting signiﬁcant illumination changes.

2.1 What to learn: Color Model

To be able to recognize objects and operate in a color-coded
world, a robot typically needs to recognize a discrete number

of colors (ω ∈ [0, N − 1]). A complete mapping identiﬁes a
∀p, q, r ∈ [0, 255], {C1,p, C2,q, C3,r} (cid:4)→ ω|ω∈[0,N −1]

color label for each point in the color space:

(1)

where C1, C2, C3 are the color channels (e.g. RGB, YCbCr),
with the corresponding values ranging from 0 − 255.

We start out modeling each color as a three-dimensional
(3D) Gaussian with mutually independent color channels.
Using empirical data and the statistical technique of boot-
strapping [5], we determined that this representation closely
approximates reality. The Gaussian model simpliﬁes calcula-
tions and stores just the mean and variance as the statistics
for each color, thereby reducing the memory requirements
and making the learning process feasible to execute on mobile
robots with constrained resources.

IJCAI-07

2212

The a priori probability density functions (color ω ∈
(cid:5)2

[0, N − 1]) are then given by:
(cid:2)3
p(c1, c2, c3|ω) ∼

ci − μCi

(cid:4)

3(cid:3)

1

· exp− 1
2

√
2π

i=1 σCi

i=1

σCi

(2)

= 0, Cimax

where, ci ∈ [Cimin
= 255] represents the value at
a pixel along a color channel Ci while μCi and σCi represent
the corresponding means and standard deviations.
Assuming equal priors (P (ω) = 1/N, ∀ω ∈ [0, N − 1]),

each color’s a posteriori probability is then given by:

p(ω|c1, c2, c3) ∝ p(c1, c2, c3|ω)

(3)

The Gaussian model for color distributions, as described in
our previous work [19], performs well inside the lab. In addi-
tion, it generalizes well with limited samples when the color
distributions are actually unimodal; it is able to handle minor
illumination changes. However, in settings outside the lab,
factors such as shadows and illumination variations cause the
color distributions to be multi-modal; the robot is now unable
to model colors properly using Gaussians.

In order to extend the previous work to less controlled
settings, we propose a hybrid color representation that uses
Gaussians and color histograms. Histograms provide an ex-
cellent alternative when colors have multi-modal distribu-
tions [20]. Here, the possible color values (0–255 along each
channel) are discretized into bins that store the count of pixels
that map into that bin. A 3D histogram can be normalized to
provide the probability density function:

p(c1, c2, c3|ω) ≡ Histω(b1, b2, b3)
SumHistV alsω

(4)

where b1, b2, b3 represent the histogram bin indices cor-
responding to the color channel values c1, c2, c3, and
SumHistV alsω is the sum of the values in all the bins of
the histogram for color ω. The a posteriori probabilities are
then given by Equation 3.

Unfortunately, histograms do not generalize well with lim-
ited training data, especially for samples not observed in the
training set, such as with minor illumination changes. Re-
source constraints prevent the implementation of operations
more sophisticated than smoothing. Also, histograms require
more storage, wasteful for colors that can be modeled as
Gaussians. We combine the two representations such that
they complement each other: colors for which a 3D Gaus-
sian is not a good ﬁt are modeled using 3D histograms. The
goodness-of-ﬁt decision is made online, for each color.

Samples for which a 3D Gaussian is a bad ﬁt can still be
modeled analytically using other distributions (e.g. mixture
of Gaussians, Weibull) through methods such as Expectation-
Maximization [4]. But most of these methods involve param-
eter estimation schemes that are computationally expensive
to perform on mobile robots. Hence, we use a hybrid repre-
sentation with Gaussians and histograms that works well and
requires inexpensive computation. In addition, the robot au-
tomatically generates the curriculum (action sequence) based
on the object conﬁguration, as described in Section 3.

2.2 When to learn: Detecting illumination changes
To detect signiﬁcant changes in illumination, we need a
mechanism for representing illumination conditions and for
differentiating between them.

We hypothesized that images from the same lighting condi-
tions would have measurably similar distributions of pixels in
color space. The original image is in the YCbCr format, with
values ranging from [0-255] along each dimension. To reduce
storage, but still retain the useful information, we transformed
the image to the normalized RGB space, (r, g, b):

r =

R+1

G+1

,

g =

R+G+B+3

R+G+B+3

,

b =

B+1

R+G+B+3

(5)

Since r + g + b = 1, any two of the three features are a
sufﬁcient statistic for the pixel values. We represent a partic-
ular illumination condition with a set of distributions in (r, g)
space, quantized into N bins in each dimension, correspond-
ing to several images captured by the robot.

Next, we need a well-deﬁned measure capable of detect-
ing the correlation between discrete distributions. Based on
experimental validation, we use KL-divergence, an entropy-
based measure. For two distributions A and B in the 2D (r, g)
space, N being the number of bins along each dimension:

KL(A, B) = − N −1(cid:3)

N −1(cid:3)

i=0

j=0

(Ai,j · ln

Bi,j
Ai,j

)

(6)

The more similar two distributions are, the smaller is the KL-
divergence between them. Since KL-divergence is a function
of the log of the observed color distributions, it is reasonably
robust to large peaks in the observed color distributions and is
hence less affected by images with large amounts of a single
color. The lack of symmetry in KL-divergence is eliminated
using the Resistor-Average KL-divergence (RA-KLD) [8].

Given a set of distributions corresponding to M different il-
lumination conditions, we have previously shown [18] that it
is possible to effectively classify the distribution correspond-
ing to a test image into one of the illumination classes. A ma-
jor limitation was that we had to know the illumination con-
ditions in advance and also had to provide manually trained
color maps for each illumination. Here, we make a signif-
icant extension in that we do not need to know the different
illumination conditions ahead of time.

For every illumination condition i, in addition to a set of
(r, g) distributions (rgsamp[i]), we calculate the RA-KL dis-
tances between every pair of (r, g) distributions to get a dis-
tribution of distances, (Di), which we model as a Gaussian.
When the illumination changes signiﬁcantly, the average RA-
KL distance between a test (r, g) distribution and rgsamp[i]
maps to a point well outside the 95% range of the intra-
illumination distances (Di). This feature is used as a measure
of detecting a change in illumination.

3 Algorithms: When, What, How to Learn
Our algorithms for color learning and adaptation to illumina-
tion change are summarized in Algorithm 1 and Algorithm 2.
Algorithm 1 enables the robot to decide when to learn. The
robot ﬁrst learns the color map for the current illumination by
generating a curriculum using the world model, as described
in Algorithm 2. Next, it represents this illumination condition

IJCAI-07

2213

and distribution of RA-KLD distances Di.

Algorithm 1 Adapting to Illumination Change – When to
learn?

Require: For each illumination i ∈ [0, M − 1], color map
1: Begin: M = 0, current = M .
2: Generate curriculum and learn all colors - Algorithm 2.
3: Generate rgsamp[current][], N (r, g) space distribu-
tions, and distribution of RA-KLD distances, Dcurrent.

4: Save color map and image statistics, M = M + 1.
5: if currentT ime − testT ime ≥ timeth then

6:
7:
8:

9:
10:

11:
12:

13:
14:

15:
16:
17:

rgtest = sample (r, g) test distribution.
(cid:6)
for i = 0 to M − 1 do
j KLDist(rgtest, rgsamp[i][j])

dAvgtest[i] = 1

N

Continue with current color map.

end for
if dAvgtest[current] lies within the threshold range
of Dcurrent then
else if dAvgtest[i] lies within the range of Di, i (cid:11)=
current then
else if ∀i ∈ [0, M − 1], dAvgtest[i] lies outside the
range of Di then

Use corresponding color map, current = i.

Re-learn color map autonomously: Algorithm 2.
Save (r, g) distributions for new illumination.
Transition to the new color map for subsequent op-
erations.
current = M , M = M + 1.

18:
19:
20:
21: end if

end if
testT ime = currentT ime.

by collecting sample image distributions in (r, g) and com-
puting the distribution of RA-KL distances, DcurrIll.

Periodically (timeth = 0.5), the robot generates a test dis-
tribution, rgtest, and computes its average distance to each set
of previously stored distributions, rgsamp[i]. If dAvgtest[i]
lies within the threshold range (95%) of the corresponding
Di, the robot transitions to the corresponding illumination
condition. But, if it lies outside the threshold range of all
known distribution of distances, the robot learns a new color
map and collects image statistics, which are used in subse-
quent comparisons. Changing the threshold changes the res-
olution at which the illumination changes are detected but the
robot is able to handle minor illumination changes using the
color map corresponding to the closest illumination condi-
tion (see Section 4.2). With transition thresholds to ensure
that a change in illumination is accepted iff it occurs over a
few frames, it also smoothly transitions between the learned
maps. The algorithm requires no manual supervision.

Next, we brieﬂy describe the planned color learning algo-
rithm, Algorithm 2, used in lines 2 and 15 of Algorithm 1.

Our previous algorithm [19] (lines 11, 12, 17 − 20) had the

robot move along a prespeciﬁed motion sequence, and model
each color as a 3D Gaussian. But, outside the controlled lab
setting, some color distributions are multi-modal and cannot
be modeled effectively as Gaussians. The current algorithm
signiﬁcantly extends the previous approach in two ways. It
automatically chooses between two representations for each

Algorithm 2 Autonomous Color Learning – How to learn?
Require: Known initial pose and color-coded model of the
robot’s world - objects at known positions. These can
change between trials.

Require: Empty Color Map; List of colors to be learned.
Require: Arrays of colored regions, rectangular shapes in
3D; A list for each color, consisting of the properties
(size, shape) of the regions of that color.

Require: Ability to approximately navigate to a target pose

(x, y, θ).

1: i = 0, N = M axColors
2: T imest = CurrT ime, T ime[] — the maximum time

allowed to learn each color.

3: while i < N do
4:
5:
6: M otion = RequiredMotion( T argetP ose )
7:

Color = BestColorToLearn( i );
T argetP ose = BestTargetPose( Color );
Perform M otion {Monitored using visual input and
localization}
if TargetRegionFound( Color ) then

8:
9:

10:
11:
12:
13:
14:
15:

16:
17:
18:
19:
20:
21:
22:
23:
24:

region,

samples

Collect
Observed[][3].
if PossibleGaussianFit(Observed) then

from the

candidate

LearnGaussParams( Colors[i] )
Learn Mean and Variance from samples

else { 3D Gaussian not a good ﬁt to samples }

LearnHistVals( Colors[i] )
Update the color’s 3D histogram using the sam-
ples
end if
UpdateColorMap()
if !Valid( Color ) then

RemoveFromMap( Color )

end if

else

Rotate at target position.

end if

if CurrT ime − T imest ≥ T ime[Color] or
RotationAngle ≥ Angth then

i = i + 1
T imest = CurrT ime

25:
26:
27:
28: end while
29: Write out the color statistics and the Color Map.

end if

it decides
color to facilitate color learning outside the lab:
what to learn. It also automatically determines how to learn,
i.e. it generates the curriculum for learning colors, for any
robot starting pose and object conﬁguration.

The robot starts off at a known location without any color
knowledge. It has a list of colors to be learned and a list of
object descriptions corresponding to each color (size, shape,
location of regions). Though this approach does require some
human input, in many applications, particularly when object
locations change less frequently than illumination, it is more
efﬁcient than hand-labeling several images. To generate the
curriculum, the robot has to decide the order in which the col-

IJCAI-07

2214

ors are to be learned and the best candidate object for learning
a particular color. The algorithm currently makes these deci-
sions greedily and heuristically, i.e.
it makes these choices
one step at a time without actually planning for the subse-
quent steps. The aim is to get to a large enough target object
while moving as little as possible, especially when not many
colors are known. The robot computes three weights for each
object-color combination (c, i):

w1 = fd( d(c, i) ), w2 = fs( s(c, i) ), w3 = fu( o(c, i) )

(7)
where the functions d(c, i), s(c, i) and o(c, i) represent the
distance, size and object description for each color-object
combination. Function fd( d(c, i) ) assigns larger weights to
smaller distances, fs( s(c, i) ) assigns larger weights to larger
candidate objects, and fu( o(c, i) ) assigns larger weights iff
the object i can be used to learn color c without having to wait
for any other color to be learned or object i consists of color
c and other colors that have already been learned.

The BestColorToLearn() (line 4) is then given by:
arg max
c∈[0,9]

( fd( d(c, i) )

max

i∈[0,Nc−1]

(cid:7)

+ fs( d(c, i) ) + fu( o(c, i) ) )

(cid:8)

(8)

where the robot parses through the different objects available
for each color (Nc) and calculates the weights. Once a color
is chosen, the robot determines the best target for the color,
using the minimum motion and maximum size constraints:

(cid:7)

arg max

i∈[0,Nc−1]

fd( d(c, i) )

+ fs( d(c, i) ) + fu( o(c, i) )

(cid:8)

(9)

For a chosen color, the best candidate object is the one with
the maximum weight for the given heuristic functions. The
robot chooses the BestTargetPose() (line 5) to learn color
from this object and moves there (lines 6,7). It searches for
candidate image regions satisfying a set of constraints based
on current robot location and target object description. If a
suitable image region is found (TargetRegionFound() – line
8), the pixels in the region are used as samples, Observed, to
verify goodness-of-ﬁt with a 3D Gaussian (line 10). The test
is done using bootstrapping [5] using KL-divergence as the
distance measure, as described in Algorithm 3.

If the samples generate a good Gaussian ﬁt, they are used
to determine the mean and variance of the color distribution
(LearnGaussParams() – line 11). If not, they are used to pop-
ulate a 3D histogram (LearnHistVals() – line 14). The learned
distributions are used to generate the Color Map, the mapping
from the pixel values to color labels (line 17). The robot uses
the map to segment subsequent images and ﬁnd objects. The
objects help the robot localize to positions suitable for learn-
ing other colors, and to validate the learned colors and remove
spurious samples (lines 18,19).

To account for slippage and motion model errors, if a suit-
able image region is not found, the robot turns in place to
ﬁnd it. If it has rotated in place for more than a threshold
angle (Angth = 360o
) and/or has spent more than a thresh-
old amount of time on a color (T ime[Color] ≈ 20sec), it

transitions to the next color in the list. Instead of providing

Algorithm 3 PossibleGaussianFit(), line 10 Algorithm 2 –
What to learn?

1: Determine Maximum-likelihood estimate of Gaussian

parameters from samples, Observed.

2: Draw N samples from Gaussian – Estimated, N = size

of Observed.

3: Dist = KLDist(Observed, Estimated).
4: Mix Observed and Estimated – Data, 2N items.
5: for i = 1 to N umT rials do
6:

Sample N items with replacement from Data – Set1,
remaining items – Set2.

7: Disti = KLDist(Set1, Set2)
8: end for
9: Goodness-of-ﬁt by p-value: where Dist lies in the distri-

bution of Disti.

a color map and/or the action sequence each time the envi-
ronment or the illumination changes, we now just provide the
positions of objects in the robot’s world and have it plan its
curriculum and learn colors autonomously. The adaptation to
illumination changes makes the entire process autonomous.
A video of the robot learning colors can be seen online:

www.cs.utexas.edu/∼AustinVilla/?p=research/auto vis.

4 Experiments
We ﬁrst provide a brief overview of the robotic platform used,
followed by the experimental results.

horz., 45.2o

4.1 Experimental Platform
The SONY ERS-7 Aibo is a four legged robot whose primary
sensor is a CMOS camera located at the tip of its nose, with
a limited ﬁeld-of-view (56.9o
vert.). The im-
ages, captured in the YCbCr format at 30Hz with a resolution
of 208 × 160 pixels, possess common defects such as noise
and distortion. The robot has 20 degrees-of-freedom, three in
each leg, three in its head, and a total of ﬁve in its tail, mouth,
and ears. It has noisy IR sensors and wireless LAN for inter-
robot communication. The legged as opposed to wheeled lo-
comotion results in jerky camera motion. All processing for
vision, localization, motion and action selection is performed
on-board using a 576MHz processor.

One major application domain for the Aibos is the
RoboCup Legged League [16], a research initiative in which
teams of four robots play a competitive game of soccer on an
indoor ﬁeld ≈ 4m × 6m. But applications on Aibos and mo-

bile robots with cameras typically involve an initial calibra-
tion phase, where the color map is produced by hand-labeling
images over a period of an hour or more (Section 5). Our
approach has the robot autonomously learning colors in less
than ﬁve minutes and adapting to illumination changes.

4.2 Experimental Results
We tested our algorithm’s ability to answer three main ques-
tions: When to learn - the ability to detect illumination
changes, How to learn - the ability to plan the action se-
quence to learn the colors, and How good is the learning -
the segmentation and localization accuracy in comparison to
the standard human-supervised scheme.

IJCAI-07

2215

When to Learn?
First, we tested the ability to accurately detect changes in il-
lumination. The robot learned colors and (r, g) distributions
corresponding to an illumination condition and then moved
around in its environment chasing a ball. We changed the
lighting by controlling the intensity of speciﬁc lamps and the
robot identiﬁed signiﬁcant illumination changes.

(%)

2.9
96.4

97.1
3.6

Change
Changec

Change Changec

Table 1 presents re-
sults averaged over
1000 trials with the
rows and columns
representing
the
Table 1: Illumination change detection:
few errors in 1000 trials.
ground truth and ob-
served values respectively. There are very few false positives
or false negatives. The errors due to highlights and shadows
are removed by not accepting a change in illumination unless
it is observed over a few consecutive frames.

To test the ability to transition between known illumina-
tions, the robot learned color maps and statistics for three con-
ditions: Bright(1600lux), Dark(450lux), Interim(1000lux).

The intensity of the
overhead lamps was
changed to one of the
three conditions once

every ≈ 10 sec. Ta-
eraged over ≈ 150

ble 2 shows results av-

Illum.

Bright
Dark

Interim

Transition Accuracy
Correct (%) Errors

97.3
100
96.1

4
0
6

Table 2: Illumination transition accu-
racy: few errors in ≈ 150 trials.

trials each. The few
false transitions, due to shadows or highlights, are quickly
corrected in the subsequent tests. When tested in conditions
in between the known ones, the robot ﬁnds the closest illumi-
nation condition and is able to work in the entire range.

How to Learn?
In previous work [19], ﬁxed object locations resulted in a sin-
gle curriculum to learn colors. To test the robot’s ability to
generate curricula for different object and robot starting po-
sitions, we invited a group of seven graduate students with
experience working with the Aibos to suggest challenging
conﬁgurations. It is difﬁcult to deﬁne challenging situations
ahead of time but some examples that came up include hav-
ing the robot move a large distance in the initial stages of
the color learning process, and to put the target objects close
to each other, making it difﬁcult to distinguish between them.
The success ratio and the corresponding localization accuracy
over 15 trials are shown in Table 3.

70
100

Success (%)

Conﬁg
Worst
Best
Avg

A trial is a success if all colors
are learned successfully. The
localization error is the differ-
ence between the robot’s esti-
mate and the actual target posi-
tions, measured by a human us-
ing a tape measure. We observe
that the robot is mostly able to
plan a suitable motion sequence and learn colors. In the cases
where it fails, the main problem is that the robot has to move
long distances with very little color knowledge. This, cou-
pled with slippage, puts it in places far away from the tar-

Table 3:
Planning Accu-
racy in challenging conﬁgu-
rations.

90 ± 10.7

get location and it is unable to learn the colors. The motion
planning works well and we are working on making the algo-
rithm more robust to such failure conditions. The localization
accuracy with the learned map is comparable to that with a

hand-labeled color map (≈ 8cm, 10cm, 6deg in comparison
to 6cm, 8cm, 4deg in X, Y , and θ).

How good is the learning?
To test the accuracy of learning under different illuminations,
we had the robot learn colors under controlled lab condi-
tions and in indoor corridors outside the lab, where the over-
head ﬂuorescent lamps provided non-uniform illumination
(between 700-1000lux) and some of the colors (ﬂoor, wall
etc) could not be modeled well with 3D Gaussians. We ob-
served that the robot automatically selected the Gaussian or
Histogram model for each color and successfully learned all
the colors.

Lab

Indoor

under

Conﬁg

Dist (cm)

9.8 ± 4.8
11.7 ± 4.4

Table 4: Localization accuracy: compa-
rable to that with a hand-labeled map.

Localization Error
θ (deg)
6.1 ± 4.7
7.2 ± 4.5

Table 4 shows the
accu-
localization
two
racies
different
illumina-
tion conditions (lab,
indoor
corridor)
based on the learned
color maps. We had the robot walk to 14 different points
and averaged the results over 15 trials. The differences
were not statistically signiﬁcant. The corresponding seg-
mentation accuracies were 95.4% and 94.3% respectively,
calculated over 15-20 images, as against the 97.3% and
97.6% obtained with a hand-labeled color map (differences
not statistically signiﬁcant). The learned maps are as good as
the hand-labeled maps for object recognition and high-level
task competence. But, our technique takes 5 minutes of
robot time instead of an hour or more of human effort.
Sample images under different testing conditions and a video
of the robot localizing in a corridor can be seen online:

www.cs.utexas.edu/∼AustinVilla/?p=research/gen color.

To summarize, our algorithm enables the robot to plan its
motion sequence to learn colors autonomously for any given
object conﬁguration. It is able to detect and adapt to illumi-
nation changes without manual training.

5 Related Work
Color segmentation is a well-researched ﬁeld in computer vi-
sion with several effective algorithms [3; 6]. Attempts to
learn colors or make them robust to illumination changes have
produced reasonable success [13; 14]. But they are compu-
tationally expensive to perform on mobile robots which typi-
cally have constrained resources.

On Aibos,

the standard approaches for creating map-
pings from the YCbCr values to the color labels [7; 11;
12] require hand-labeling of images (≈ 30) over an hour or

more. There have been a few attempts to automatically learn
the color map on mobile robots. In one approach, closed ﬁg-
ures are constructed corresponding to known environmental
features and the color information from these regions is used
to build color classiﬁers [2]. The algorithm is time consum-
ing even with the use of ofﬂine processing and requires hu-
man supervision. In another approach, three layers of color

IJCAI-07

2216

maps, with increasing precision levels are maintained; colors
being represented as cuboids [15]. The generated map is not
as accurate as the hand-labeled one. Schulz and Fox [17] es-
timate colors using a hierarchical Bayesian model with Gaus-
sian priors and a joint posterior on robot position and envi-
ronmental illumination. Ulrich and Nourbakhsh [21] model
the ground using color histograms and assume non-ground
regions to represent obstacles. Anzani et. al [9] model colors
using mixture of Gaussians and compensate for illumination
changes by modifying the parameters. But, prior knowledge
of color distributions and suitable initialization of parameters
are required. Our approach does not require prior knowledge
of color distributions. Instead, it uses the world model to au-
tomatically learns colors by generating a suitable curriculum,
and adapts to illumination changes.

6 Conclusions

Robotic systems typically require signiﬁcant amount of man-
ual sensor calibration before they can be deployed. We aim
to make the process more autonomous. We propose a scheme
that achieves this goal with regard to color segmentation, an
important subtask for visual sensors.

In our previous work [19], the robot learned colors within
the controlled lab setting using a pre-speciﬁed motion se-
quence. In other work [18], we demonstrated the ability to
transition between discrete illumination conditions when ap-
propriate color maps and image statistics were trained ofﬂine.
But the robot was given a lot of information manually, in-
cluding the object positions, the action sequence for learning
colors, and color maps for each illumination condition.

With the current method only the object locations need
to be speciﬁed. A hybrid representation for color enables
the robot to generate a curriculum to learn colors and local-
ize both inside the lab and in much more uncontrolled envi-
ronments with non-uniform overhead illumination and back-
ground clutter that can be confused with the objects of inter-
est. Other robots may use cameras of higher quality but color
maps are still needed. For full autonomy there are always
computational constraints at some level, irrespective of the
robot platform being used. This paper lays the groundwork
for the next step of testing the same algorithm on other robot
platforms that work outdoors.

In the end, the robot is able to detect changes in illumina-
tion robustly and efﬁciently, without prior knowledge of the
different illumination conditions. When the robot detects an
illumination condition that it had already learned before, it
smoothly transitions to using the corresponding color map.
Currently, we have the robot re-learn the colors when a sig-
niﬁcant change from known illumination(s) is detected. One
direction of future work is to have the robot adapt to minor il-
lumination changes by suitably modifying speciﬁc color dis-
tributions. Ultimately, we aim to develop efﬁcient algorithms
for a mobile robot to function autonomously under uncon-
trolled natural lighting conditions.

Acknowledgment

Special thanks to Suresh Venkat for discussions on the color
learning experiments, and to the UT AustinVilla team. This

work was supported in part by NSF CAREER award IIS-
0237699 and ONR YIP award N00014-04-1-0545.

References
[1] M. Ahmadi and P. Stone. A multi-robot system for continuous

area sweeping tasks. In ICRA, 2006.

[2] D. Cameron and N. Barnes. Knowledge-based autonomous
In The Seventh International

dynamic color calibration.
RoboCup Symposium, 2003.

[3] D. Comaniciu and P. Meer. Mean shift: A robust approach

toward feature space analysis. PAMI, 2002.

[4] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation.

Wiley Publishers, 2nd edition, 2000.

[5] B. Efron and R. J. Tibshirani. An Introduction to Bootstrap.

Chapman and Hall Publishers, 1993.

[6] B. Sumengen et. al.

Image segmentation using multi-region

stability and edge strength. In ICIP, 2003.

[7] D. Cohen et. al. UPenn TDP, RoboCup-2003: The Seventh

RoboCup Competitions and Conferences. 2004.

[8] D. H. Johnson et. al. Information-theoretic analysis of neural

coding. Journal of Computational Neuroscience, 2001.

[9] F. Anzani et. al. On-line color calibration in non-stationary en-
vironments. In The International RoboCup Symposium, 2005.

[10] J. Pineau et. al. Towards robotic assistants in nursing homes:
Challenges and results. RAS Special Issue on Socially Interac-
tive Robots, 2003.

[11] S. Chen et. al. UNSW TDP, RoboCup-2001: The Fifth

RoboCup Competitions and Conferences. 2002.

[12] William Uther et al. Cm-pack’01: Fast legged robot walking,
robust localization, and team behaviors. In The Fifth Interna-
tional RoboCup Symposium, 2001.

[13] Y. B. Lauziere et. al. Autonomous physics-based color learning
under daylight. In Conf. on Color Techniques and Polarization
in Industrial Inspection, 1999.

[14] T. Gevers and A. W. M. Smeulders. Color based object recog-

nition. In Pattern Recognition, 1999.

[15] M. Jungel. Using layered color precision for a self-calibrating

vision system. In The RoboCup Symposium, 2004.

[16] H. Kitano, M. Asada, I. Noda, and H. Matsubara. Robot world

cup. Robotics and Automation, 1998.

[17] D. Schulz and D. Fox. Bayesian color estimation for adaptive

vision-based robot localization. In IROS, 2004.

[18] M. Sridharan and P. Stone. Towards illumination invariance in

the legged league. In The RoboCup Symposium, 2004.

[19] M. Sridharan and P. Stone. Autonomous color learning on a

mobile robot. In AAAI, 2005.

[20] M. Swain and D. H. Ballard. Color indexing.
Journal of Computer Vision, 7(1):11–32, 1991.

International

[21] I. Ulrich and I. Nourbakhsh. Appearance-based obstacle de-

tection with monocular color vision. In AAAI, 2000.

IJCAI-07

2217


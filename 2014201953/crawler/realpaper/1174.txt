Feature Mining and Neuro-Fuzzy Inference System for Steganalysis of LSB 

Matching Stegangoraphy in Grayscale Images  

Qingzhong Liu1, Andrew H. Sung1, 2

1Department of Computer Science

1,2Institute for Complex Additive Systems Analysis

New Mexico Tech, Socorro, NM 87801, USA

{liu, sung}@cs.nmt.edu

Abstract

In this paper, we present a scheme based on
feature mining and neuro-fuzzy inference system
for detecting LSB matching steganography in
grayscale images, which is a very challenging
problem in steganalysis. Four types of features
are proposed, and a Dynamic Evolving Neural
Fuzzy Inference System (DENFIS) based feature
selection is proposed, as well as the use of
Support Vector Machine Recursive Feature
Elimination
better
In comparison with other
detection accuracy.
well-known
features
perform the best. DENFIS outperforms some
traditional
learning classifiers. SVM-RFE and
DENFIS based feature selection outperform
statistical significance based feature selection
such as t-test. Experimental results also indicate
that it remains very challenging to steganalyze
LSB matching steganography
in grayscale
images with high complexity.

(SVM-RFE)

features,

overall,

obtain

our

to

this

date, many

1 Introduction
Steganalysis is the science and art of detecting the
presence of hidden data in digital images, audios, videos
and other media. In steganography or the hiding of secret
data in digital media, the most common cover is digital
images. To
or
embedding methods, such as LSB embedding, spread
spectrum steganography, F5 algorithm and some other
JPEG steganography, have been very successfully
steganalyzed [Fridrich etal., 2003; Ker, 2005a; Fridrich et
al. 2002; Harmsen and Pearlman 2003; Choubassi and
Moulin 2005; Liu etal., 2006a]. Several other embedding
paradigms, include stochastic modulation [Fridrich and
Goljan, 2003; Moulin and Briassouli, 2004] and LSB
matching [Sharp 2001], however, are much more difficult
to detect.

steganographical

 Support for this research was received from ICASA (of
New Mexico Tech) and a DoD IASP Capacity Building grant

is

in

on

classifier

applied. Based

The literature does provide a few detectors for LSB
matching steganography. One of
the first papers on
detection of embedding by noise adding is the paper by
Harmsen and Pearlman [Harmsen and Pearlman, 2003],
wherein the measure of histogram characteristic function
center of mass (HCFCOM), is extracted and a Bayesian
multivariate
the
contribution of Harmsen and Pearlman [2003], Ker
[2005b] proposes two novel ways of applying the HCF:
calibrating the output using a down-sampled image and
computing the adjacency histogram instead of the usual
histogram. The best discriminators
are Adjacency
HCFCOM (A.HCFCOM) and Calibrated Adjacency
HCFCOM (C.A.HCFCOM) to improve the probability of
detection for LSB matching in grayscale images. Farid
and Lyu describe an approach to detecting hidden
messages
a wavelet-like
decomposition to build high-order statistical models of
natural images [Lyu and Farid, 2004 and 2005]. Fridrich
et al.
[2005] propose a Maximum Likelihood (ML)
estimator
for estimating the number of embedding
changes for non-adaptive ±K embedding in images. Based
on the stego-signal estimation, Holotyak et al. [2005]
present a blind steganalysis classifying on high order
statistics of the estimation signal.

images

using

by

Unfortunately, the publications mentioned above did
not fully address the issue of image complexity that is
very important in evaluating the detection performance
(though Fridrich etal. [2005] report that the ML estimator
starts to fail to reliably estimate the message length once
the variance of sample exceeds 9,
the
detection performance decreases with the increase in the
image complexity).

indicating that

Recently, the shape parameter of Generalized Gaussian
Distribution (GGD) in the wavelet domain is introduced
by the authors to measure the image complexity in
steganalysis [Liu et al., 2006b]; although the method
proposed therein is successful in detecting LSB matching
steganography in color images and outperforms other
well-known methods, its performance is not so good in
grayscale images, which is generally more difficult.

On the other side, many steganalysis methods are based
In feature

on feature mining and machine learning.

IJCAI-07

2808

feature

extraction,

mining, besides
another general
problem is how to choose the good measures from the
extracted features. Avcibas et al. [2003] propose a
steganalysis using image quality metrics. In their method,
they apply analysis of variance (ANOVA) to feature
selection, the higher the F statistic, the lower the p value,
and the better
the
ANOVA applied by Avcibas etal.[2003] is significance-
based feature selection like other statistics such as T-test,
etc. But these statistics just consider the significance of
individual feature, not the interaction of features. There
has been little research that addresses in depth the feature
selection problem with specific respect to steganalysis.

the feature will be. Essentially,

In this paper, we propose four types of features and a
Dynamic Evolving Neural Fuzzy Inference System
[Kasabov and Song, 2002; Kasabov, 2002] (DENFIS)-
based feature selection to the steganalysis of LSB
matching steganography in grayscale images. The four
types of features consist of the shape parameter of GGD
in the wavelet domain to measure the image complexity,
the entropy and the high order statistics in the histogram
of the nearest neighbors, and correlation features. We also
adopt the well-known gene selection method of Support
Vector Machine Recursive Feature Elimination (SVM-
RFE) [Guyon etal., 2002] for choosing good measures in
steganalysis.

Comparing against other well-known methods in terms
of steganalysis performance, our
feature set, overall,
performs the best. DENFIS outperforms some traditional
learning classifiers.
SVM-RFE and DENFIS-based
feature selection outperform statistical significance based
feature selection such as T-test in steganalysis.

2 Feature Mining

2.1 Image Complexity
Several papers [Srivastava et al., 2003; Winkler, 1996;
Sharifi and Leon-Garcia, 1995] describe the statistical
models of images such as probability models for images
based on Markov Random Field models
(MRFs),
Gaussian Mixture Model (GMM) and GGD model in
transform domains, such as DCT, DFT, or DWT.

Experiments show that a good Probability Density
Function (PDF) approximation for the marginal density of
coefficients at a particular subband produced by various
types of wavelet
transforms may be achieved by
adaptively varying two parameters of the GGD [Sharifi
and Leon-Garcia, 1995; Moulin and Liu, 1999], which is
defined as

( ;
p x

 
)

,

=


(1/



2


)

(|

|/
x



)



e

(1)

Where



( )
z

=





0




t z
1

e t dt z

,

>

0

is the Gamma function.

Here  models the width of the PDF peak, while  is
inversely proportional to the decreasing rate of the peak;
 is referred to as the scale parameter and  is called the

shape parameter. The GGD model contains the Gaussian
and Laplacian PDFs as special cases, using  = 2 and  =
1, respectively.

2.2 Entropy and High order Statistics of the
Histogram of the Nearest Neighbors
There is evidence that adjacent pixels in ordinary images
are highly correlated [Huang and Mumford, 1999; Liu et
al., 2006a]. Consider
the nearest
neighbors, denote the grayscale value at the point (i, j) as
x, the grayscale value at the nearest point (i+1, j) in the
horizontal direction as y, and the grayscale value at the
nearest point (i, j+1) in the vertical direction as z. The
variable H(x,y,z) denotes the occurrence of the pair (x,
y,z), or the histogram of the nearest neighbors (NNH).

the histogram of

The entropy of NNH (NNH_E) is calculated as follows:
(2)

NNH_E= 

log





2

Where denotes the distribution density of the NNH.

The symbol

 denotes the standard deviation of HH.

H

The rthhigh order statistics of NNH is given as:

NNH_HOS(r)=

1
3
N


1
N


1
N


1
N



=
x

0

=
y

0

=
x

0






zyxH
),

,(



1
3
N

r



H


1
N


1
N


1
N



=
x

0

=
y

0

=
z

0

zyxH
),

,(

r


	
	


(3)

Where N is the number of possible gray scales of the
image, e.g., for 8-bit grayscale image, N=256.

2.3 Correlation Features
The following three correlation features are extracted.

1. The correlation between the Least Significant Bit
Plane (LSBP) and the second Least Significant Bit Plane
(LSBP2) and the autocorrelation in the LSBP: M1(1:m,
1:n) denotes the binary bits of the LSBP and M2(1:m, 1:n)
denotes the binary bits of the LSBP2.
,
1
 

C cor M M

CovM M

(4)

) =

1

=

(

(

)

,

2

1

2

M M

1

2

2
M

where

=



).
C(k,l), the autocorrelation of LSBP is defined as:

Var M

Var M

and

(

)

(



=

1

2

1

2

2
M

C k l
( , )

=

cor X X
l

(

k

,

)

(5)

where

X M m k n l X M k ml
,

,1:

(1:

1:

);

(

=





=

+

+

1:

).
n

k

1

l

1

2. The autocorrelation in the image histogram: The
histogram probability density is denoted as (0, 1,
2…N-1). The histogram probability densities, He, Ho,
Hl1, and Hl2 are denoted as follows:

He = (0, 2, 4…N-2) ,
Hl1= (0, 1, 2…N-1-l), Hl2 = (l, l+1, l+2…N-1).
The autocorrelation coefficients C2 and CH(l), where l

Ho= (1, 3, 5…N-1);

is the lag distance, are defined as follows:

C2 =cor(He,Ho)

(6)

CH(l)=cor(Hl1,Hl2)

(7)
3. The correlation in the difference between the image
and the denoised version: The original cover is denoted as

IJCAI-07

2809

F, the stego-image is denoted as F´,D (·) denotes some
denoising function, the differences between the image and
the denoised are:

EF =F–D(F)
EF´= F´-D(F´)

the test

Generally, the statistics of EFand EF´ are different. The
correlation features in the difference domain are extracted
as follows. Firstly,
image is decomposed by
wavelet transform. Find the coefficients in HL, LH and
HH subbands with the absolute value smaller than the
threshold value, t, set
these coefficients to zero, and
reconstruct the image using the inverse wavelet transform
on the updated wavelet coefficients. The reconstructed
image is treated as denoised image. The difference
between test image and reconstructed version is Et, where
tis the threshold value.

C t k l
, )

( ;

E

=

cor E E

(

,

,
t k

,
t l

)

E

where
(
variables kand ldenote the lag distance.

E m k n l E E k ml
,

,1:

(1:

1:

);

,
t k

=





t

=

,
t l

t

+

(8)

+

1:

).
n

The

SC(ec) =

N



=
i
1

2

cor e ,e

(

c

)

i

(9)

Where, ec  C,ei  FN(i=1,2…N). The ec with the
minimal value of SC(ec) is chosen as eN+1. We call this
feature selection DENFIS-MSC (for Minimum of SC).

4 Experiments

4.1 Experimental Setup
The original images in our experiments are 5000 TIFF
raw format digital pictures from Olympus C740. These
images are 24-bit, 640480 pixels, lossless true color and
never compressed. According to the method in [Lyu and
Farid, 2004 and 2005], we cropped the original images
into 256256 pixels in order
the low
complexity parts of the images. The cropped color images
are converted into grayscales and we hid data in these
grayscales with different hiding ratio. LSB matching
stego-images are produced. The hidden data in different
covers are different. The hiding ratio is 12.5%.

to get

rid of

3 Neuro-Fuzzy Inference System Based

Feature Selection

Neuro-fuzzy inference systems and evolving neuro-fuzzy
inference systems are introduced in [Kasabov, 2002]. The
dynamic
system (DENFIS)
proposed by [Kasabov and Song, 2002] uses the Takagi-
Sugeno type of fuzzy inference method [Takagi and
Sugeno, 1985].

neuro-fuzzy

evolving

To improve the detection performance, based on our
previous work [Liu and Sung, 2006], we propose a feature
selection method based on the DENFIS supervised
learning, described as follows:

1. Each individual feature is ranked in the order from
the highest train accuracy to the lowest train accuracy
with the use of DENFIS.

2. The feature with the highest train accuracy is chosen
as the first feature. After this step, the chosen feature set,
F1, consists of the best feature, e1, corresponding to
feature dimension one.
3. The (N+1)st feature set, FN+1={e1, e2 ,…, eN , eN+1}
is produced by adding eN+1into the present N-dimensional
feature set, FN= {e1, e2, …, eN} according to the following
method: Each feature ei(i 1,2,…,N) outside of FN is
added into FN; the classification accuracy of each feature
set FN + {ei} is compared,
the ec with the highest train
into the set of candidates, C. The
accuracy is put
candidate set C generally includes multiple features, but
only one feature will be chosen. The strategy is to
measure the similarity of chosen features and each of the
candidates. Pearson’s correlation between the candidate
ec, ec  C and the element ei, ei  FN(i=1,2…N) is
calculated. To measure the similarity,
the sum of the
square of the correlation (SC) is defined as follows:

4.2 Feature Extraction and Comparison
The following features are extracted:

1. Shape parameter  of GGD of HH wavelet subband

to measure image complexity, described in (1).

2. Entropy of the histogram of the nearest neighbors,

NNH_E, defined in (2)

3. The high order statistics of the histogram of the
nearest neighbors, NNH_HOS(r) in (3). ris set from 3 to
22, total 20 high order statistics.

4. Correlations features consist of C1 in (4), C(k,l) in

(5), C2 in (6), CH(l) in (7), and CE(t;k,l).

We set the following lag distance to k and l in C(k,l)

and get 14 features:

a. k= 0, l= 1, 2, 3, and 4; l= 0, k= 1, 2, 3, and 4.
b. k= 1, l= 1; k= 2, l=2; k=3, l= 3; k= 4 and l= 4.
c. k= 1, l=2; k=2, l=1.
In (7), l is set to 1, 2, 3, and 4. In (8), we set the
following lag distance to k and l in CE(t; k,l) and get
following pairs: CE(t; 0,1), CE(t; 0,2), CE(t;1,0), CE(t; 2,0),
CE(t; 1,1), CE(t; 1,2), and CE(t;2,1). tis set 1, 1.5, 2, 2.5,
3, 3.5, 4, 4.5, and 5.

Henceforth, we use CF to denote the fourth type of
correlation features; and use EHCC (for Entropy, High
order statistics, Complexity, and Correlation features) to
denote types 1 to 4 features.

To compare EHCC with other well-known features, the
Histogram Characteristic Function Center of Mass
(HCFCOM) features [Harmsen and Pearlman, 2003] are
extracted because the hiding process of LSB matching
steganography can be modeled in the context of additive
noise. We extend HCFCOM feature set to the high order
moments. HCFHOM stands for HCF center of mass High
the rth order
Order Moments; HCFHOM(r) denotes
statistics. In our experiments, the HCFHOM feature set

IJCAI-07

2810

consists of HCFCOM and HCFHOM(r) (r = 2, 3, and 4).
Based on Harmsen and Pearlman’s work, Ker [2005b]
proposed A.HCFCOM and C.A.HCFCOM. Additionally,
Farid and Lyu [2004, 2005] described an approach to
detecting hidden messages in images by building High-
Order Moment statistics in Multi-Scale decomposition
domain (we call the features HOMMS), which consists of
72-dimension features in grayscale images.

4.3 Detection Performance on Feature Sets
To compare the detection performances on these feature
sets with different classifiers, Besides DENFIS, we apply
the following classifiers to each feature sets. These
classifiers are Naive Bayes Classifier (NBC), Support
(SVM), Quadratic Bayes Normal
Vector Machines
Classifier
(QDC), Nearest Mean Scaled Classifier
(NMSC), K-Nearest Neighbor classifier
(KNN) and
Adaboost that produces a classifier composed from a set
of weak rules [Vapnik, 1998; Schlesinger and Hlavac,
2002; Heijden et al., 2004; Webb, 2002; Schapire and
Singer, 1999; Friedman etal.,2000].

Thirty experiments are done on each feature set using
each classifier. Training samples are chosen at random
and the remaining samples are for test. The ratio of
training samples to test samples is 2:3. The average
classification accuracy is compared.

Table 1 compares the detection performances (mean
values and standard deviations) on each feature set with
the use of different classifiers. In each category of image
complexity, the highest test accuracy is in bold. Table 1
indicates that, regarding the classification accuracy of
feature sets, on the average, EHCC performs best,
followed by CF, HOMMS performs worst; regarding the
classification performance of classifiers, SVM is the best;
regarding image complexity, the test accuracy decreases
while the image complexity increases.
In the low
complexity of  < 0.4, the highest test accuracy is 86.9%;
in the high complexity of  > 1, the highest test accuracy
is 62.9%. It shows that image complexity is an important
factor to the detection performance.

To obtain a higher detection performance, we combine
EHCC, HCFCOM, HOMMS, C.A. HCFCOM, and A.
HCFCOM features, and apply three feature section
methods, SVM-RFE [Guyon etal., 2002], DENFIS-MSC,
and T-test (here we apply T-test
instead of ANOVA
because cover samples and steganography samples are
unpaired in each category of image complexity) to the
features, and compare different classifiers and the three
feature selections in the feature dimensions 1 to 40.

Fig. 1 compares the detection performance on the
SVM-RFE feature set with the use of DENFIS, SVM,
NBC, NMSC, and KNN.
It indicates that in the low
image complexity, DENFIS and SVM are the best; in the
mediate and high image complexity, DENFIS is the best.

Fig. 1 also shows
decreases as the image complexity increases.

that

the detection performance

Table 1. Detection performances (mean value ± standard deviation, %)
on the feature sets with the use of different classifiers. In different
image complexity, the highest test accuracy is in bold.

Classifier



Feature Set

SVM

ADABOOST

NBC

QDC

<
0.4

0.4-
0.6

0.6-
0.8

0.8-
1

>1

EHCC

86.9 ±± 1.1

84.5 ± 1.1

77.5 ± 1.6

57.9 ± 0.7

CF

85.9 ± 1.0

82.0 ± 1.2

77.0 ± 2.1

80.9 ± 1.7

HCFHOM

60.9 ± 1.3

57.6 ± 1.5

57.5 ± 1.5

53.4 ± 1.0

HOMMS

53.6 ± 1.0

50.6 ± 2.0

46.9 ± 1.7

42.1 ± 1.4

C.A.HCFCOM

55.3 ± 0.6

54.3 ± 1.1

53.8 ± 1.1

55.4 ± 1.1

A.HCFCOM

55.6 ± 0.9

55.4 ± 1.8

54.7 ± 1.4

55.5 ± 1.1

EHCC

81.4 ±± 0.7

74.3 ± 0.8

68.2 ± 0.8

60.9 ± 0.5

CF

77.6 ± 0.4

72.2 ± 1.0

67.6 ± 1.3

70.6 ± 1.3

HCFHOM

58.4 ± 0.6

56.6 ± 1.1

56.1 ± 0.9

54.5 ± 0.6

HOMMS

48.8 ± 1.6

47.6 ± 1.0

47.1 ± 0.8

44.0 ± 1.5

C.A.HCFCOM

58.1 ± 0.7

57.0 ± 1.5

57.8 ± 1.1

57.9 ± 0.8

A.HCFCOM

57.3 ± 0.6

56.6 ± 0.9

56.8 ± 0.7

56.6 ± 0.6

EHCC

772.4 ± 1.0

64.3 ± 1.2

61.4 ± 1.0

58.3 ± 0.5

CF

66.7 ± 0.7

63.9 ± 1.2

62.1 ± 1.1

62.3 ± 1.2

HCFHOM

57.6 ± 0.9

55.3 ± 1.1

54.2 ± 1.3

53.1 ± 0.7

HOMMS

47.3 ± 0.7

43.7 ± 1.3

45.4 ± 1.2

40.6 ± 2.4

C.A.HCFCOM

56.0 ± 1.1

56.4 ± 1.0

55.8 ± 1.0

56.2 ± 0.8

A.HCFCOM

56.6 ± 0.6

54.9 ± 1.2

55.2 ± 1.1

55.5 ± 1.2

EHCC

63.6 ±± 1.2

57.8 ± 1.3

56.9 ± 1.2

56.1 ± 0.5

CF

60.0 ± 1.0

57.4 ± 1.8

57.8 ± 1.5

57.5 ± 1.6

HCFHOM

53.9 ± 1.2

52.0 ± 1.6

53.2 ± 1.4

51.7 ± 0.6

HOMMS

/

42.0 ± 1.5

44.5 ± 0.8

41.6 ± 2.8

C.A.HCFCOM

52.4 ± 0.7

52.6 ± 1.5

52.1 ± 1.3

53.1 ± 1.2

A.HCFCOM

53.3 ± 1.0

50.3 ± 1.3

51.8 ± 1.2

50.8 ± 1.6

EHCC

62.9 ±± 1.6

58.6 ± 1.9

58.1 ± 1.4

56.8 ± 1.4

CF

59.7 ± 1.7

58.9 ± 2.3

57.1 ± 1.5

58.4 ± 1.3

HCFHOM

54.4 ± 0.8

52.7 ± 1.6

51.9 ± 1.7

53.2 ± 1.8

HOMMS

/

46.7 ± 1.8

50.4 ± 1.4

43.1 ± 1.5

C.A.HCFCOM

54.7 ± 0.5

52.7 ± 1.7

53.1 ± 1.4

54.4 ± 0.9

A.HCFCOM

54.3 ± 0.3

51.2 ± 1.6

51.6 ± 2.0

53.5 ± 1.4

It

that

indicates

Fig. 2 compares the three feature selections in the
image complexity of  < 0.4 with the use of SVM,
DENFIS, and NMSC.
the feature
selections SVM-RFE and DENFIS-MSC are better than
T-test.
Furthermore, applying SVM and DENFIS to
SVM-RFE and DENFIS-MSC feature sets,
the test
accuracies are better than the highest value of table 1.
Due to the page limit, we don’t list comparison of the
feature selections in the high image complexity. Our
experiments also indicate that SVM-RFE and DENFIS-
MSC outperform T-test.

IJCAI-07

2811

(a)  < 0.4

(b) 0.4 <  < 0.6

(c) 0.6 <  < 0.8

(d) 0.8 <  < 1

(e)  > 1

Fig. 1. Comparison of the detection performance on SVM-RFE feature set with the use of different classifiers.

Fig. 2. Comparison of feature selections DENFIS-MSC, SVM-RFE, and T-test in the image complexity of  < 0.4.

5 Conclusions
In this paper, we present a scheme of steganalysis of LSB
matching steganography in grayscale images based on
feature mining and neuro-fuzzy inference system. Four
types of features are extracted, a DENFIS-based feature
selection is used, and SVM-RFE is used as well to obtain
better detection accuracy. In comparison with other features
of HCFHOM, HOMMS, A.HCFCOM, and C.A.HCFCOM,
overall, our features perform the best. DENFIS outperforms
some
learning classifiers. SVM-RFE and
DENFIS based feature selection outperform statistical
significance based feature
selection such as T-test.
is still very
Experimental

results also indicate that

traditional

it

challenging
steganography in grayscale images with high complexity.

of LSB matching

steganalysis

for

the

References
[Avcibas etal., 2003] I. Avcibas, N. Memon and B. Sankur.
Steganalysis using Image Quality Metrics. IEEE Trans.
ImageProcessing, 12(2):221–229.

[Choubassi and Moulin, 2005] M. Choubassi and P. Moulin.
A New Sensitivity Analysis Attack. Proc. of SPIE
ElectronicImaging,vol.5681, pp.734–745.

[Fridrich, 2002]

J. Fridrich, M. Goljan, D. Hogea.
JPEG Images: Breaking the F5
Steganalysis of
Algorithm. Lecture Notes in Computer Science,

IJCAI-07

2812

vol.2578, pp.310–323. Springer-Verlag New York,
2002.

[Fridrich etal., 2003] J.Fridrich, M. Goljan, D. Hogea, and
D. Soukal. Quantitative Steganalysis: Estimating Secret
Message Length. ACM Multimedia Systems Journal,
Special Issue on Multimedia Security, 99(3):288–302.

[Fridrich and Goljan, 2003] J. Fridrich and M. Goljan.
Stochastic
Proc. of SPIE Electronic Imaging,

Steganography Using

Image

Digital
Modulation.
vol.5020, pp.191–202.

[Fridrich et al., 2005] J. Fridrich, D. Soukal, M. Goljan.
Maximum Likelihood Estimation of Length of Secret
Message Embedding using ±K Steganography in Spatial
Domain. Proc. of SPIE Electronic Imaging, vol.5681,
pp.595–606.

[Friedman et al., 2000] J. Friedman, T. Hastie and R.
Tibshirani. Additive Logistic Regression: A Statistical
View of Boosting.. TheAnnalsofStatistics, 338(2):337–
374.

[Guyon etal., 2002] I. Guyon, J. Weston, S. Barnhill, and
V. Vapnik. Gene Selection for Cancer Classification
using Support Vector Machines, Machine Learning,
446(1-3):389–422.

[Harmsen and Pearlman, 2003]

J. Harmsen and W.
Pearlman. Steganalysis of Additive Noise Modelable
Information Hiding. Proc. SPIE Electronic Imaging,
vol.5020, pp.131–142.

[Heijden et al., 2004] F. Heijden, R. Duin, D. Ridder, D.
Tax. Classification, Parameter Estimation and State
Estimation,John Wiley, 2004.

[Hootyak et al., 2005] T. Holotyak,

J. Fridrich, S.
Voloshynovskiy. Blind Statistical Steganalysis
of
Additive Steganography Using Wavelet Higher Order
Statistics. Proc.ofthe9th IFIPTC-6TC-11Conference
onCommunicationsandMultimediaSecurity.

[Huang and Mumford, 1999] J. Huang and D. Mumford.
Statistics of Natural Images and Models. Proc. of
CVPR, vol.1, June 23 – 25, 1999.

[Kasabov, 2002] N. Kasabov. Evolving Connectionist
Systems: Methods and Applications in Bioinformatics,
Brain Study and Intelligent Machines. London-New
York, Springer-Verlag, 2002.

[Kasabov and Song, 2002] N. Kasabov and Q.Song.
DENFIS: Dynamic Evolving Neural-Fuzzy Inference
System and Its Application for Time-Series Prediction.
IEEETrans.FuzzySystems,10(2):144–154.

[Ker, 2005a] A. Ker.

Improved Detection of LSB
Steganography in Grayscale Images. Lecture Notes in
Computer Science, vol.3200, Springer-Verlag New
York, 2005, pp.97–115.

[Ker, 2005b] A. Ker. Steganalysis of LSB Matching in
Grayscale Images. IEEE Signal Processing Letters,
112(6):441–444.

[Liu et al., 2006a] Q. Liu, A. H. Sung, J. Xu, V.
Venkataramana. Detecting JPEG steganography using

Polynomial Fitting, Proc of 2006 Artificial Neural
NetworksinEngineering(in press).

[Liu et al., 2006b] Q. Liu, A. H. Sung, J. Xu, and B.M.
Ribeiro. Image Complexity and Feature Extraction for
Steganalysis of LSB Matching Steganography. Proc.of
ICPR(2) 2006, pp.267–270.

[Liu and Sung, 2006] Q. Liu and A. H. Sung. Recursive
Feature Addition
for Gene Selection. Proc. of
International Joint Conference on Neural Networks
2006.pp.2339–2346.

[Lyu and Farid, 2004] S. Lyu and H. Farid. Steganalysis
using Color Wavelet Statistics and One-class Support
in Proc of SPIE Symposium on
Vector Machines.
ElectronicImaging, San Jose, CA, 2004.

[Lyu and Farid, 2005] S. Lyu and H. Farid. How Realistic is
Photorealistic. IEEE Trans. on Signal Processing,
553(2):845–850.

[Moulin and Briassouli, 2004] P. Moulin and A. Briassouli.
A Stochastic QIM Algorithm for Robust, Undetectable
Image Watermarking. Proc. of ICIP 2004, vol.2,
pp.1173–1176.

[Moulin and Liu, 1999] P. Moulin and J. Liu. Analysis of
Multiresolution
using
Generalized Gaussian and Complexity priors. IEEE
Trans.Inform.Theory, 45:909–919.

Image Denoising

Schemes

[Schapire and Singer, 1999] R. Schapire, and Y. Singer.
Improved Boosting Algorithms using Confidence-rated
Predictions. MachineLearning, 37(3):297–336.

[Schlesinger and Hlavac, 2002] M. Schlesinger, V. Hlavac.
Ten Lectures on Statistical and Structural Pattern
Recognition, Kluwer Academic Publishers, 2002.

[Sharifi and Leon-Garcia, 1995] K. Sharifi and A. Leon-
Garcia. Estimation of Shape Parameter for Generalized
Gaussian Distributions in Subband Decompositions of
Video, IEEETrans.CircuitsSyst.VideoTechnol., 55:52–
56.

[Sharp, 2001] T. Sharp. An Implementation of Key-Based
Digital Signal Steganography. Lecture Notes in
ComputerScience, vol.2137, pp.13–26. Springer-Verlag
New York, 2001.

[Srivastava et al., 2003] A. Srivastava, A. Lee, E. P
Simoncelli and S. Zhu. On Advances in Statistical
Modeling of Natural Images. Journal of Mathematical
ImagingandVision, 18(1):17–33.

[Takagi and Sugeno, 1985] T. Takagi and M. Sugeno.
Fuzzy Identification of Systems and Its Applications to
Modeling and Control. IEEE Trans. on Systems, Man,
andCybernetics. pp.116–132, 1985.

[Vapnik, 1998] V. Vapnik. Statistical Learning Theory.

John Wiley, 1998.

[Webb, 2002] A. Webb. Statistical Pattern Recognition,

John Wiley & Sons, New York, 2002.

[Winkler, 1996] G. Winkler. Image Analysis, Random
Fields and Dynamic Monte Carlo Methods, Springer-
Verlag, New York, 1996.

IJCAI-07

2813


Occam’s Razor Just Got Sharper

Computer Science Department, Technion—Israel Institute of Technology, Haifa 32000, Israel

{esaher, shaulm}@cs.technion.ac.il

Saher Esmeir and Shaul Markovitch

Abstract

Occam’s razor is the principle that, given two hy-
potheses consistent with the observed data, the sim-
pler one should be preferred. Many machine learn-
ing algorithms follow this principle and search for
a small hypothesis within the version space. The
principle has been the subject of a heated debate
with theoretical and empirical arguments both for
and against it. Earlier empirical studies lacked suf-
ﬁcient coverage to resolve the debate. In this work
we provide convincing empirical evidence for Oc-
cam’s razor in the context of decision tree induc-
tion. By applying a variety of sophisticated sam-
pling techniques, our methodology samples the ver-
sion space for many real-world domains and tests
the correlation between the size of a tree and its ac-
curacy. We show that indeed a smaller tree is likely
to be more accurate, and that this correlation is sta-
tistically signiﬁcant across most domains.

1 Introduction
Occam’s razor, attributed to the 14th-century English logi-
cian William of Ockham, is the principle that, given two hy-
potheses consistent with the observed data, the simpler one
should be preferred. This principle has become the basis for
many induction algorithms that search for a small hypothesis
within the version space [Mitchell, 1982]. Several studies at-
tempted to justify Occam’s razor with theoretical and empiri-
cal arguments [Blumer et al., 1987; Quinlan and Rivest, 1989;
Fayyad and Irani, 1990]. But a number of recent works have
questioned the utility of Occam’s razor, and provided theoret-
ical and experimental evidence against it.

Schaffer [1994] proved that no learning bias can outper-
form another bias over the space of all possible learning tasks.
This looks like theoretical evidence against Occam’s razor.
Rao et al. [1995], however, argued against the applicability
of this result to real-world problems by questioning the valid-
ity of its basic assumption about the uniform distribution of
possible learning tasks.

Domingos [1999] argued that the disagreement about the
utility of Occam’s razor stems from the two different inter-
pretations given to it: the ﬁrst is that simplicity is a goal in
and of itself, and the second is that simplicity leads to better

accuracy. While accepting the ﬁrst interpretation, Domingos
questioned the second one.

Webb [1996] presented C4.5X, an extension to C4.5 that
uses similarity considerations to further specialize consistent
leaves. Webb reported an empirical evaluation which shows
that C4.5X has a slight advantage in a few domains and ar-
gued that these results discredit Occam’s thesis.

Murphy and Pazzani [1994] reported a set of experiments
in which all possible consistent trees were produced and their
accuracy was tested. Their ﬁndings were inconclusive. They
found cases where larger trees had, on average, better accu-
racy. Still, they recommend using Occam’s principle when
no additional information about the concept is available. The
major limitation of their work is the exhaustive enumeration
of the version space. Such an approach is only applicable to
domains with very few features.

In this work we present an alternative approach that per-
forms statistical testing of Occam’s thesis on a sample of
the version space. This approach allows us to use high-
dimensional domains and complex concepts. One problem
with random sampling of the version space is the rarity of
small trees in the sample. We therefore use, in addition to
random sampling, biased sampling methods based on mod-
ern anytime induction algorithms [Esmeir and Markovitch,
2004]. These methods produce samples with much higher
concentrations of small trees.

The major contribution of this work is to provide convinc-
ing empirical evidence for Occam’s razor in the context of
classiﬁcation trees. Furthermore, the various sampling tech-
niques we applied help to better understand the space of con-
sistent decision trees and how top-down induction methods
explore it. Note that this empirical demonstration of the util-
ity of Occam’s principle does not pretend to provide a philo-
sophical proof for Occam’s thesis.

2 Occam’s Empirical Principle
In the context of machine learning, the widely accepted in-
terpretation of Occam’s razor is that given two consistent
hypotheses, the simpler one is likely to have a lower er-
ror rate. Fayyad and Irani [1990] have formally deﬁned
this notion: for two decision trees T1 and T2 and a ﬁxed ,
0 <  < 1, T1 is likely to have a lower error rate than T2 if
Pr {P (T1, ) < P (T2, )} > 0.5, where P (T, ) is the prob-
ability that T has an error rate greater than .

IJCAI-07

768

Procedure TDIDT(E, A)

If E = ∅

Return Leaf(nil)

If ∃c such that ∀e ∈ E Class(e) = c

Return Leaf(c)

a ← CHOOSE-ATTRIBUTE(A, E)
V ← domain(a)
Foreach vi ∈ V *

Ei ← {e ∈ E | a(e) = vi}
Si ← TDIDT(Ei, A − {a})

Return Node(a, {(cid:7)vi, Si(cid:8) | i = 1 . . . |V |})

* When a is numeric, a cutting point is chosen and
and a is not ﬁltered out when calling SID3.

Figure 1: Top-down induction of decision trees. E stands for
the training set and A stands for the set of attributes.

Fayyad and Irani [1990] also provided theoretical support
for favoring smaller trees. They showed that under a set of
assumptions, given two trees T1 and T2 consistent with the
observed data, T1 is likely to have a lower error rate than
T2 if T1 has fewer leaves. Berkman and Sandholm [1995],
however, have questioned this set of assumptions and argued
that the opposite conclusion can be drawn from it.

The main challenge we face in this work is to empirically
test the validity of Occam’s razor in decision tree induction.
We therefore deﬁne the Occam’s empirical principle:
Deﬁnition 1 Let Etrain and Etest be a training and a testing
set respectively. Let H be a set of hypotheses consistent with
Etrain. We say that H satisﬁes Occam’s empirical principle
with respect to Etrain and Etest if, for any h1, h2 drawn from
H × H,
P

(cid:2)
Acc (h1, Etest) ≥ Acc (h2, Etest)

(cid:3)
(cid:4)
(cid:3) |h1| ≤ |h2|

≥ 0.5,

where |h| is the size of hypothesis h and 0 ≤ Acc(h, E) ≤ 1
is the accuracy of h on a test set E.

3 Sampling the Version Space

Given a learning problem, we would like to produce all
the possible trees consistent with the observations and test
whether their size and accuracy are correlated. Such an ex-
haustive enumeration, however, is not practical for most real-
world domains. Therefore, in what follows we propose sam-
pling as an alternative. First we deﬁne the population, i.e., the
space of decision trees we sample from, and then we describe
3 different sampling techniques, each of which focuses on a
different part of the sampled space.

3.1 Deﬁning the Version Space
Given a set of attributes A, the hypothesis class we deal with
is the set of decision trees over A, denoted by DTA. Let
E be a set of examples. Because Occam’s razor is applica-
ble only to hypotheses that can explain the observations, we
limit our discussion to the version space—the set of all trees
consistent with E, denoted by DTA(E). Furthermore, since
most decision tree learners build a tree top-down, we focus

on a subset of the consistent trees—the trees obtainable by
top-down induction, denoted by TDIDT A(E). Under the
TDIDT scheme, the set of examples is partitioned into sub-
sets by testing the value of an attribute and then each subset is
used to recursively build a subtree. The recursion stops when
all the examples have the same class label. Figure 1 formal-
izes the basic procedure for top-down induction.

While TDIDT A(E) is a strict subset of DTA(E), we
claim that the trees in DTA(E) that are not in TDIDT A(E)
are not interesting for the purpose of model learning. There
are two types of trees in DTA(E) − TDIDT A(E):

1. A tree containing a subtree with all leaves marked with
the same class. Obviously, such a subtree could have
been replaced by a single node marked with the class.

2. A tree with an internal node that has no associated ex-
amples from E. The subtree rooted at this node is not
supported by training examples and is therefore not in-
teresting for induction.

Note that including the above trees could unjustly distort
In the ﬁrst case, larger trees that are logically
the results.
equivalent will be included, arbitrarily weakening the nega-
tive correlation. In the second case, the extra branches are not
supported by any training example. Thus, their leaves must
be labeled randomly, lowering the accuracy and hence arbi-
trarily strengthening the negative correlation.

While we restrict the deﬁnition of Occam’s empirical prin-
ciple to consistent hypotheses, in our experiments we also
examine its applicability to pruned TDIDT A(E) trees. This
allows us to draw conclusions for noisy datasets as well.

3.2 Sampling Techniques
Our goal is to sample the TDIDT A(E) space in order to
test Occam’s empirical principle. Our ﬁrst proposed sam-
pling technique uses TDIDT with a random selection of the
splitting attribute (and cutting point, where the attribute is nu-
meric). We refer to this method as the Random Tree Gen-
erator (RTG). Observe that although it has no bias with re-
spect to generalization quality, RTG does not uniformly sam-
ple TDIDT A(E). For example, if the concept was a1 and
the attributes were {a1, a2, a3}, the probability of construct-
ing the smallest tree (with a single split) is much higher than
that of constructing a speciﬁc large tree. We will later show
that the non-uniform sampling should not affect the validity
of our conclusions.

One problem with random sampling is the rarity of small
trees. Many induction methods, however, are likely to con-
centrate on small trees. Theoretically, the correlation could
have been statistically signiﬁcant when sampling the TDIDT
space but not when sampling a subspace consisting of small
trees. To test this hypothesis we need a sample of small
trees. Such a sample could be obtained by repeatedly in-
voking RTG and keeping the smaller trees. Nevertheless,
the number of RTG invocations needed to obatin a reason-
able number of small trees is prohibitively high. Another al-
ternative is to use ID3. Repeated invocations of ID3, how-
ever, result in similar trees that can vary only due to differ-
ent tie-breaking decisions. Esmeir and Markovitch [2004] in-
troduced SID3, a stochastic version of ID3 that is designed

IJCAI-07

769

Procedure SID3-CHOOSE-ATTRIBUTE(E, A)

Foreach a ∈ A

p (a) ← gain-1(E, a)

If ∃a such that entropy-1(E, a) = 0

a∗ ← Choose attribute at random from
{a ∈ A | entropy-1(E, a) = 0}

Else

a∗ ← Choose attribute at random from A;
for each attribute a, the probability
of selecting it is proportional to p (a)

Return a∗

Figure 2: Attribute selection in SID3

Procedure LSID3-CHOOSE-ATTRIBUTE(E, A, r)

If r = 0

Return ID3-CHOOSE-ATTRIBUTE(E, A)

Foreach a ∈ A

Foreach vi ∈ domain(a)

Ei ← {e ∈ E | a(e) = vi}
mini ← ∞
Repeat r times

T ← SID3(Ei, A − {a})
mini ← min (mini, |T |)

(cid:5)|domain(a)|

totala ←

i=1

mini

Return a for which totala is minimal

Figure 3: Attribute selection in LSID3

to sample the version space semi-randomly, with a bias to
smaller trees. In SID3, instead of choosing an attribute that
maximizes the information gain, we choose the splitting at-
tribute semi-randomly. The likelihood that an attribute will
be chosen is proportional to its information gain.1 However,
if there are attributes that decrease the entropy to zero, then
one of them is picked randomly. The attribute selection pro-
cedure of SID3 is listed in Figure 2.

For many hard learning tasks such as parity concepts,
ID3’s greedy heuristic fails to correctly estimate the useful-
ness of the attributes and can mislead the learner to produce
In such cases, SID3 can, in theory,
relatively large trees.
produce signiﬁcantly smaller trees. The probability for this,
however, is low and decreases as the number of attributes in-
creases. To overcome this problem, we use a third sampling
technique that is based on the recently introduced LSID3
algorithm for anytime induction of decision trees [Esmeir
and Markovitch, 2004]. LSID3 adopts the general TDIDT
scheme, and invests more time resources for making better
split decisions. For every candidate split, LSID3 attempts to
estimate the size of the resulting subtree were the split to take
place, and favors the one with the smallest expected size. The
estimation is based on a biased sample of the space of trees
rooted at the evaluated attribute. The sample is obtained using
SID3. LSID3 is parameterized by r, the sample size. When
r is greater, the sample is larger and the resulting estimate is
expected to be more accurate. Therefore, LSID3 is expected

to improve with the increase in r. Figure 3 lists the procedure
for attribute selection as applied by LSID3. Because our goal
is to sample small trees and not to always obtain the small-
est tree, we use LSID3(r = 1) as a sampler. Observe that
LSID3 is stochastic by nature, and therefore we do not need
to randomize its decisions.

4 Experimental Results
We tested Occam’s empirical principle, as stated in Deﬁnition
1, on 20 datasets, 18 of which were chosen arbitrarily from
the UCI repository [Blake and Merz, 1998], and 2 which are
artiﬁcial datasets that represent hard concepts: XOR-5 with 5
additional irrelevant attributes, and 20-bit Multiplexer. Each
dataset was partitioned into 10 subsets that were used to cre-
ate 10 learning problems. Each problem consisted of one sub-
set serving as a testing set and the union of the remaining 9
as a training set, as in 10-fold cross validation. We sampled
the version space, TDIDT A(E), for each training set E us-
ing the three methods described in Section 3 and tested the
correlation between the size of a tree (number of leaves) and
its accuracy on the associated testing set. The size of the sam-
ple was ten thousand for RTG and SID3, and one thousand
for LSID3 (due to its higher costs). We ﬁrst present and dis-
cuss the results for consistent trees and then we address the
problem of pruned, inconsistent trees.

4.1 Consistent Decision Trees
Figure 4 plots size-frequency curves for the trees obtained by
each sampling method for three datasets: Nursery, Glass, and
Multiplexer-20 (for one fold out of the 10). Each of the three
methods focuses on a different subspace of TDIDT A(E),
with the biased sampling methods producing samples consist-
ing of smaller trees. In all cases we see a bell-shaped curve,
indicating that the distribution is close to normal. Recall that
RTG does not uniformly sample TDIDT A(E): a speciﬁc
small tree has a better chance of being built than a speciﬁc
large tree. The histograms indicate, however, that the fre-
quency of small trees in the sample is similar to that of large
trees (symmetry). This can be explained by the fact that there
are more large trees than small trees. To further verify this,
we compared the distribution of the tree size in an RTG sam-
ple to that of all trees, as reported in [Murphy and Pazzani,
1994] (Mux-11 dataset). The size-frequency curves for the
full space and the sampled space were found to be similar.

Occam’s empirical principle states that there is a nega-
tive correlation between the size of a tree and its accuracy.
To test the signiﬁcance of the correlation, we used the non-
parametric Spearman correlation test on each of the samples.2
Spearman’s coefﬁcient ρ measures the monotonic association
of two variables, without making any assumptions about their
frequency distribution. For a paired sample of X and Y , ρ is
di/(n(n2 − 1)), where di is the differ-
deﬁned as 1 − 6
ence in the statistical rank of xi and yi. There is a special
correction for this formula in the presence of ties.

(cid:5)

Table 4.1 lists summarizing statistics for the RTG, SID3,
and LSID3 samplers. The validity of Occam’s empirical prin-

1SID3 ensures that attributes with gain of zero will have a posi-

2All statistics were computed using the The R Project package

tive probability to be selected.

[R Development Core Team, 2005].

IJCAI-07

770

y
c
n
e
u
q
e
r
F

 0.25

 0.2

 0.15

 0.1

 0.05

 0

RTG
SID3
LSID3

 1000

 2000

 3000

 4000

 5000

Tree size

y
c
n
e
u
q
e
r
F

 0.16
 0.14
 0.12
 0.1
 0.08
 0.06
 0.04
 0.02
 0

RTG
SID3
LSID3

 25

 50

 75

 100  125  150  175

Tree size

y
c
n
e
u
q
e
r
F

 0.3

 0.25

 0.2

 0.15

 0.1

 0.05

 0

 0

RTG
SID3
LSID3

 100

 200

 300

 400

 500

Tree size

Figure 4: Frequency curves for the Nursery (left), Glass (middle), and Multiplexer-20 (left) datasets

DATASET

ACC.

BREAST-W
BUPA
CAR
CLEVELAND
CORRAL
GLASS
HUNGERIAN
IRIS
MONKS-1
MONKS-2
MONKS-3
MUX-20
NURSERY
SCALE
SPLICE
TIC-TAC
VOTING
WINE
XOR-5
ZOO

92.9±2.8
59.7±8.0
72.8±4.6
51.6±6.9
73.3±22.9
55.6±9.9
72.8±7.4
88.9±7.3
91.1±4.6
77.8±4.4
88.3±5.4
55.7±6.3
77.2±5.9
72.2±4.2
61.1±3.7
72.3±4.8
89.2±5.8
78.6±10.2
50.7±10.6
90.0±7.4

RTG
SIZE

128±14
213±10
647±77
188±7
15±3
135±8
125±10
39±9
203±42
294±9
171±46
388±14
3271±551
394±11
1977±101
468±34
52±12
73±12
136±8
24±5

√

7
10
10
6
9
10
9
10
10
10
10
10
10
0
10
10
10
10
10
10

ρ

-0.1

0

-0.8

0

-0.1
-0.1
-0.1
-0.3
-0.5
-0.3
-0.6
-0.1
-0.9
0.1
-0.6
-0.4
-0.3
-0.3
-0.1
-0.2

ACC.

93.1±2.7
63.4±7.3
79.7±5.8
50.2±7.2
81.6±19.6
62.3±9.3
73.3±7.2
92.8±4.5
97.0±3.8
75.5±4.6
96.0±2.5
56.6±6.5
93.1±1.8
71.7±4.1
60.5±4.2
80.2±4.5
92.8±4.3
90.6±6.7
51.9±11.8
91.8±6.2

SID3
SIZE

108±11
94±6
520±95
134±7
10±2
57±5
65±6
12±2
113±55
289±8
77±33
249±13
1583±295
389±11
1514±112
311±30
26±5
13±3
108±11
18±4

√

8
7
10
7
10
10
8
8
10
10
10
10
10
0
10
10
9
9
10
9

ACC.

LSID3
SIZE

94.3±1.6
61.9±7.4
91.9±1.1
46.1±7.4
89.8±8.3
68.0±8.4
69.9±7.5
93.8±2.7
100.0±0.0
77.7±3.1
96.7±0.4
86.1±11.8
98.1±0.5
70.1±3.8
89.3±1.8
87.7±3.0
94.5±3.1
91.7±4.3
96.5±7.7
94.2±3.5

77±4
69±3
285±13
98±5
7±1
39±3
47±3
8±0
28±4
259±3
38±2
89±35
656±54
352±5
355±23
166±11
15±2
7±1
39±11
11±1

√

5
3
3
7
NA
9
8
7
NA
0
NA
10
10
3
10
9
8
6
10
NA

ρ

-0.1

0
0

-0.1
0.2
-0.1
-0.1
-0.1
NA
0.2
0.1
-0.9
-0.4
0.1
-0.5
-0.1
-0.2
-0.1
-0.8
-0.1

ρ

-0.1

0

-0.9

0

-0.2
-0.2
-0.1
-0.1
-0.7
-0.4
-0.5
-0.2
-0.8
0.1
-0.7
-0.4
-0.2
-0.2
-0.4
-0.2

Table 1: Testing Occam’s empirical principle using different sampling methods that produce consistent trees. For each method
we report the accuracy, tree size, and Spearman’s correlation coefﬁcient (ρ) averaged over all 10 partitions. We also report the
number of times (out of 10) that a negative correlation was found to be statistically signiﬁcant with p = 0.95.

ciple, tested by Spearman’s method, is listed in the rightmost
column. For each sampling method, for each dataset, we
count how many times, out of the 10 folds, the null hypoth-
esis H0 (which states that the variables are not correlated)
can be rejected at an α = 5% signiﬁcance level, against the
alternative hypothesis that the correlation is negative.

The results indicate that when random sampling (RTG) in
used, Occam’s empirical principle, as measured by Spear-
man’s test, is valid for almost all problems, except for Scale.
The results for the SID3 sampling method indicate that even
when focusing on smaller trees, simplicity is beneﬁcial as a
bias for accuracy. Again, except for the Scale dataset, there is
a strong inverse correlation between size and accuracy. The
numbers indicate that the correlation is weaker than the RTG
case, yet still signiﬁcant across most domains.

The LSID3 samples focus on very small trees. In several
cases, LSID3 could reach the smallest tree possible. Again
the negative correlation was found to be signiﬁcant for most
domains.3 However, the number of cases where the null hy-

3Note that in some cases, signiﬁcance tests could not be com-
puted due to the large number of ties (indicated by NA in the table).

pothesis could not be rejected is higher than in the previous
samplers. One possible reason for this phenomenon is that
LSID3 samples trees from a much tighter size-range. Hence,
there is an increased probability for ﬁnding two trees, T 1 and
T 2, with the size and accuracy of T 1 being greater than T2.
As indicated by the frequency curves, the different sam-
pling methods cover different portions of the space, but some-
times overlap. An interesting question is whether the conclu-
sions hold if we analyze all samples together. Note that it is
not statistically valid to merge the samples of RTG, SID3, and
LSID3 due to their different distributions. Nevertheless, we
measured the correlation statistics for the combined samples
and found that the results are very similar, with a strong neg-
ative correlation between the size of a tree and its accuracy.

To illustrate the correlation between the size of a tree and
its accuracy, we grouped the trees from a single run into bins,
according to their size, and calculated the average accuracy
for each bin. We did this for each of the sampling methods.
Bins with less than 5 observations were discarded. Figure 5
plots the results for the Nursery, Glass, and Multiplexer-20
datasets. The error bars represent conﬁdence intervals, with
α = 5%.

IJCAI-07

771

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

 0.95

 0.9

 0.85

 0.8

 0.75

 0.7

 0.65

 0.6

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

 0.5

 2000

 3000
Tree size

 4000

 0.45

 100  110  120  130  140  150  160

Tree size

 0.7

 0.65

 0.6

 0.55

 0.5

 0.96

 0.94

 0.92

 0.9

 0.88

 0.86

 0.84

 1000

 1500

 2000

 2500

Tree size

 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
 0.65
 0.6
 0.55
 0.5

 40  45  50  55  60  65  70  75  80

Tree size

 0.85

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

 0.45

 320  340  360  380  400  420  440

 0.5

 160  180  200  220  240  260  280  300

Tree size

Tree size

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

 0.988
 0.986
 0.984
 0.982
 0.98
 0.978
 0.976
 0.974
 0.972
 0.97

 550  600  650  700  750  800

Tree size

 0.9

 0.85

 0.8

 0.75

 0.7

 0.65

 0.6

 34

 36

 38

 40
 42
Tree size

 44

 46

 48

 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
 0.65
 0.6
 0.55

 20

 40

 60

 80  100  120  140  160
Tree size

Figure 5: Correlation between size and accuracy using RTG (left-most), SID3 (middle), and LSID3 (right-most). The upper
graphs represent the results for the Nursery dataset, the graphs in the middle row stand for the Glass dataset and the lower
graphs stand for the Multiplexer-20 dataset.

Again, the graphs show a strong correlation between size
and accuracy, conﬁrming Occam’s empirical principle. For
the Nursery and Multiplexer-20 datasets the correlation is
strong for all 3 samplers. For Glass, the correlation is weaker
when the trees are very small. These graphs, which represent
each size range in its own bin, indicate that the positive sup-
port we showed for Occam’s principle is not a result of a size
bias in our sampling methods.

4.2 Pruned Decision Trees

Formally, Occam’s empirical principle, as deﬁned in Section
2, is applicable only to consistent hypotheses. Most decision
tree learners, however, do not necessarily produce consistent
models because they output a pruned tree in attempt to avoid
overﬁtting the data. This is usually done in two phases: ﬁrst
a tree is grown top-down and then it is pruned. In our second
set of experiments, we examine whether taking simplicity as
a bias in the ﬁrst stage is beneﬁcial even when the tree is later
pruned. Therefore, we measure the correlation between the
size of unpruned trees and their accuracy after pruning.

Table 4.2 summarizes the results. The same statistics were
measured with a single change: the accuracy was measured
after applying error-based pruning [Quinlan, 1993]. As in the
case of consistent trees, examining the overall correlation be-
tween the size of a tree and its accuracy indicates that for most
datasets the inverse correlation is statistically signiﬁcant.

Table 4.2 also gives the percentages of pruned leaves.
While the trees produced by RTG were aggressively pruned,
the percentage of pruned leaves in LSID3 was relatively low.
This is due to the stronger support for the decisions made at
the leaves of smaller consistent trees.

5 Conclusions

Occam’s razor states that given two consistent hypotheses, the
simpler one should be preferred. This principle has been the
subject for a heated debate, with theoretical and empirical ar-
guments both for and against it. In this work we provided
convincing empirical evidence for the validity of Occam’s
principle with respect to decision trees. We state Occam’s em-
pirical principle, which is well-deﬁned for any learning prob-
lem consisting of a training set and a testing set, and show ex-
perimentally that the principle is valid for many known learn-
ing problems. Note that our study is purely empirical and
does not attempt to reach an indisputable conclusion about
Occam’s razor as an epistemological concept.

Our testing methodology uses various sampling techniques
to sample the version space and applies Spearman’s correla-
tion test to measure the monotonic association between the
size of a tree and its accuracy. Our experiments conﬁrm Oc-
cam’s empirical principle and show that the negative correla-
tion between size and accuracy is strong. Although there were
several exceptions, we conclude that in general, simpler trees
are likely to be more accurate. Observe that our results do not
contradict those reported by Murphy and Pazzani [1994], but
complement them: we do not claim that a smaller tree is al-
ways more accurate, but show that for many domains smaller
trees are likely to be more accurate.

We view our results as strong empirical evidence for the
utility of Occam’s razor to decision tree induction. It is im-
portant to note that the datasets used in our study do not nec-
essarily represent all possible learning problems. However,
these datasets are frequently used in machine learning re-
search and considered typical tasks for induction algorithms.

IJCAI-07

772

DATASET

ACC.

SIZE

%P

ρ

RTG

BREAST-W
BUPA
CAR
CLEVELAND
CORRAL
GLASS
HUNGERIAN
IRIS
MONKS-1
MONKS-2
MONKS-3
MUX-20
NURSERY
SCALE
SPLICE
TIC-TAC
VOTING
WINE
XOR-5
ZOO

92.8±2.9
62.7±7.2
80.8±3.5
54.6±5.8
74.6±20.3
58.7±9.4
77.5±5.3
91.5±6.6
90.9±6.3
65.5±2.1
93.6±4.9
57.4±6.4
88.4±2.3
69.4±4.2
63.3±4.8
78.0±4.3
94.2±4.5
82.2±9.6
54.8±11.7
90.0±8.0

128±14
213±10
647±77
188±7
15±3
135±8
125±10
39±9
203±42
294±9
171±46
388±14
3271±551
394±11
1977±101
468±34
52±12
73±12
136±8
24±5

82
86
93
73
67
73
92
82
89
97
92
84
97
90
91
88
92
84
85
53

-0.1

0

-0.6

0

-0.1
-0.1

0

-0.2
-0.6

0

-0.6
-0.1
-0.7

0

-0.6
-0.3
-0.3
-0.2
-0.2
-0.2

√

9
7
10
5
9
10
5
10
10
6
10
10
10
3
10
10
10
10
10
10

SID3

ACC.

SIZE

%P

ρ

93.3±2.8
64.9±7.0
84.0±4.1
52.9±6.5
78.2±19.5
63.5±9.1
76.1±6.3
93.5±3.8
95.2±6.2
65.5±2.2
97.0±2.6
58.2±6.7
92.1±1.7
69.2±4.2
66.1±5.4
81.0±4.1
96.2±2.2
91.1±6.5
55.6±12.5
91.0±7.1

108±11
94±6
520±95
134±7
10±2
57±5
65±6
12±2
113±55
289±8
77±33
249±13
1583±295
389±11
1514±112
311±30
26±5
13±3
108±11
18±4

77
46
87
48
46
25
65
47
77
97
84
61
90
90
81
77
89
22
77
34

-0.1

0

-0.7

0

-0.3
-0.2

0

-0.1
-0.7

0

-0.6
-0.2
-0.7

0

-0.6
-0.4
-0.1
-0.2
-0.4
-0.2

√

7
5
10
6
10
10
5
7
10
5
10
10
10
4
10
10
8
9
10
9

LSID3

ACC.

SIZE

%P

ρ

94.1±1.3
62.2±7.3
91.6±2.0
47.1±7.3
86.0±13.0
68.1±8.4
71.3±7.3
93.8±3.1
100.0±0.0
67.3±4.1
98.9±0.3
87.8±11.5
96.1±0.7
68.6±3.8
91.9±1.4
87.2±3.1
96.4±1.3
91.7±4.3
97.1±7.2
94.3±3.5

77±4
69±3
285±13
98±5
7±1
39±3
47±3
8±0
28±4
259±3
38±2
89±35
656±54
352±5
355±23
166±11
15±2
7±1
39±11
11±1

70
11
56
12
9
5
23
20
2
78
65
32
61
89
58
36
79
8
18
5

-0.3

0

-0.3
-0.1

0

-0.1
-0.1
-0.1
-0.1

0

-0.2
-0.9
-0.5
0.2
-0.4
-0.1

0

-0.1
-0.7

0

√

7
2
9
7
7
9
5
5
NA
4
NA
10
10
0
10
6
5
6
10
NA

Table 2: Testing Occam’s empirical principle when the trees are pruned. For each method we report the accuracy, tree size,
percentage of pruned leaves, and Spearman’s correlation coefﬁcient (ρ) averaged over all 10 partitions. We also report the
number of times (out of 10) that a negative correlation between the size of the unpruned trees and their accuracy after pruning
was found to be statistically signiﬁcant with p = 0.95.

We also examined the applicability of Occam’s razor to
pruned trees. For most domains we found the correlation be-
tween the size of a tree before pruning and its accuracy after
pruning to be statistically signiﬁcant. These results indicate
that taking Occam’s razor as a preference bias when growing
a tree is useful even if the tree is post-pruned.

An interesting research direction that we plan to pursue is
to test the correlation between a tree’s accuracy and a vari-
ety of other measures, such as expected error and description
length.

Acknowledgments

This work was partially supported by funding from the EC-
sponsored MUSCLE Network of Excellence (FP6-507752).

References

[Berkman and Sandholm, 1995] N. C. Berkman and T. W.
Sandholm. What should be minimized in a decision tree:
A re-examination. Technical report, 1995.

[Blake and Merz, 1998] C. L. Blake and C. J. Merz. UCI

repository of machine learning databases, 1998.

[Blumer et al., 1987] A. Blumer, A. Ehrenfeucht, D. Haus-
Information

sler, and M. K. Warmuth. Occam’s Razor.
Processing Letters, 24(6):377–380, 1987.

[Domingos, 1999] P. Domingos. The role of Occam’s razor
in knowledge discovery. Data Mining and Knowledge Dis-
covery, 3(4):409–425, 1999.

[Esmeir and Markovitch, 2004] S.

and
S. Markovitch. Lookahead-based algorithms for anytime

Esmeir

induction of decision trees. In ICML’04, pages 257–264,
2004.

[Fayyad and Irani, 1990] U. M. Fayyad and K. B. Irani.
What should be minimized in a decision tree? In AAAI’90,
pages 749–754, 1990.

[Mitchell, 1982] T. M. Mitchell. Generalization as search.

Artiﬁcial Intelligence, 18:203–226, 1982.

[Murphy and Pazzani, 1994] P. M. Murphy and M. J. Paz-
zani. Exploring the decision forest: an empirical investiga-
tion of Occam’s Razor in decision tree induction. Journal
of Artiﬁcial Intelligence Research, 1:257–275, 1994.

[Quinlan and Rivest, 1989] J. R. Quinlan and R. L. Rivest.
Inferring decision trees using the minimum descrip-
tion length principle.
Information and Computation,
80(3):227–248, 1989.

[Quinlan, 1993] J. R. Quinlan. C4.5: Programs for Machine

Learning. Morgan Kaufmann, San Mateo, CA, 1993.

[R Development Core Team, 2005] R Development Core
Team. R: A language and environment for statistical
computing.
R Foundation for Statistical Computing,
Vienna, Austria, 2005.

[Rao et al., 1995] R. Rao, D. Gordon, and W. Spears. For
every generalization action, is there really an equal or op-
posite reaction? In ICML’95, pages 471–479, 1995.

[Schaffer, 1994] C. Schaffer. A conservation law for gener-
alization performance. In ICML’94, pages 259–265, 1994.
[Webb, 1996] G. I. Webb. Further experimental evidence
against the utility of Occam’s razor. Journal of Artiﬁcial
Intelligence Research, 4:397–417, 1996.

IJCAI-07

773


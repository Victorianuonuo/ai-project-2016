Learning Payoff Functions in In(cid:2)nite Games

Yevgeniy Vorobeychik, Michael P. Wellman, and Satinder Singh

University of Michigan

Arti(cid:2)cial Intelligence Laboratory

1101 Beal Avenue

Ann Arbor, MI 48109-2110 USA

f yvorobey, wellman, baveja g@umich.edu

Abstract

We consider a class of games with real-valued
strategies and payoff information available only in
the form of data from a given sample of strategy
pro(cid:2)les. Solving such games with respect to the un-
derlying strategy space requires generalizing from
the data to a complete payoff-function representa-
tion. We address payoff-function learning as a stan-
dard regression problem, with provision for captur-
ing known structure (symmetry) in the multiagent
environment. To measure learning performance,
we consider the relative utility of prescribed strate-
gies, rather than the accuracy of payoff functions
per se. We demonstrate our approach and evalu-
ate its effectiveness on two examples: a two-player
version of the (cid:2)rst-price sealed-bid auction (with
known analytical form), and a (cid:2)ve-player market-
based scheduling game (with no known solution).

1 Introduction
Game-theoretic analysis typically begins with a complete de-
scription of strategic interactions, that is, the game. We con-
sider the prior question of determining what the game actually
is, given a database of game experience rather than any direct
speci(cid:2)cation. This is one possible target of learning applied
to games [Shoham et al., 2003]. When agents have few avail-
able actions and outcomes are deterministic, the game can
be identi(cid:2)ed through systematic exploration. For instance,
we can ask the agents to play each strategy pro(cid:2)le in the en-
tire joint strategy set and record the payoffs for each. If the
joint action space is small enough, limited nondeterminism
can be handled by sampling. Coordinating exploration of the
joint set does pose dif(cid:2)cult issues. Brafman and Tennenholtz,
for example, address these carefully for the case of common-
interest stochastic games [Brafman and Tennenholtz, 2003],
as well as the general problem of maintaining an equilib-
rium among learning algorithms [Brafman and Tennenholtz,
2004].

Further dif(cid:2)culties are posed by intractably large (or in(cid:2)-
nite) strategy sets. We can make this problem tractable by
reducing the number of pro(cid:2)les that agents are allowed to
play, but this comes at the cost of transforming the game of
interest into a different game entirely.
Instead, we seek to

identify the full game (or at least a less restrictive game) from
limited data, entailing some generalization from observed in-
stances. Approximating payoff functions using supervised
learning (regression) methods allows us to deal with contin-
uous agent strategy sets, providing a payoff for an arbitrary
strategy pro(cid:2)le. In so doing, we adopt functional forms con-
sistent with prior knowledge about the game, and also ad-
mit biases toward forms facilitating subsequent game analysis
(e.g., equilibrium calculation).

In this paper, we present our (cid:2)rst investigation of approxi-
mating payoff functions, employing regression to low-degree
polynomials. We explore two example games, both with in-
complete information and real-valued actions. First is the
standard (cid:2)rst-price sealed bid auction, with two players and
symmetric value distributions. The solution to this game is
well-known [Krishna, 2002], and its availability in analytical
form proves useful for benchmarking our learning approach.
Our second example is a (cid:2)ve-player market-based schedul-
ing game [Reeves et al., 2005], where time slots are allocated
by simultaneous ascending auctions [Milgrom, 2000]. This
game has no known solution, though previous work has iden-
ti(cid:2)ed equilibria on discretized subsets of the strategy space.

2 Preliminaries
2.1 Notation
A generic normal form game is formally expressed as
[I;f(cid:1)(Si)g;fui(s)g], where I refers to the set of players and
m = jIj is the number of players. Si is the set of strategies
available to player i 2 I, and the set (cid:1)(Si) is the simplex of
mixed strategies over Si. Finally, ui(s) : S1(cid:2)(cid:1)(cid:1)(cid:1)(cid:2) Sm ! R
is the payoff function of player i when all players jointly
play s = (s1; : : : ; sm), with each sj 2 Sj. As is com-
mon, we assume von Neumann-Morgenstern utility, allow-
ing an agent i’s payoff for a particular mixed strategy pro-
(cid:2)le to be ui((cid:27)) = Ps2S[(cid:27)1(s1)(cid:1)(cid:1)(cid:1) (cid:27)m(sm)]ui(s); where
: Sj ! [0; 1] is a mixed strategy of player i, assign-
(cid:27)j
ing a probability to each pure strategy sj 2 Sj such that
all probabilities over the agent’s strategy set add to 1 (i.e.,
(cid:27)j 2 (cid:1)(Sj)).
It will often be convenient to refer to the strategy (pure
or mixed) of player i separately from that of the remaining
players. To accommodate this, we use s(cid:0)i to denote the joint
strategy of all players other than player i.

2.2 Nash Equilibrium
In this paper, we are concerned with one-shot normal-form
games, in which players make decisions simultaneously and
accrue payoffs, upon which the game ends. This single-shot
nature may seem to preclude learning from experience, but
in fact repeated episodes are allowed, as long as actions can-
not affect future opportunities, or condition future strategies.
Game payoff data may also be obtained from observations of
other agents playing the game, or from simulations of hypo-
thetical runs of the game. In any of these cases, learning is
relevant despite the fact that the game is to be played only
once.

Faced with a one-shot game, an agent would ideally play
its best strategy given those played by the other agents. A
con(cid:2)guration where all agents play strategies that are best re-
sponses to the others constitutes a Nash equilibrium.
De(cid:2)nition 1 A strategy pro(cid:2)le s = (s1; : : : ; sm) constitutes
a (pure-strategy) Nash equilibrium of game [I;fSig;fui(s)g]
if for every i 2 I, s0i 2 Si, ui(si; s(cid:0)i) (cid:21) ui(s0i; s(cid:0)i):
A similar de(cid:2)nition applies when mixed strategies are al-
lowed.
De(cid:2)nition 2 A strategy pro(cid:2)le (cid:27) = ((cid:27)1; : : : ; (cid:27)m) con-
stitutes a mixed-strategy Nash equilibrium of game
[I;f(cid:1)(Si)g;fui(s)g] if for every i 2 I, (cid:27)0i 2 (cid:1)(Si),
ui((cid:27)i; (cid:27)(cid:0)i) (cid:21) ui((cid:27)0i; (cid:27)(cid:0)i):
In this study we devote particular attention to games that
exhibit symmetry with respect to payoffs.
De(cid:2)nition 3 A game [I;f(cid:1)(Si)g;fui(s)g] is symmetric if
8i; j 2 I; (a) Si = Sj and (b) ui(si; s(cid:0)i) = uj(sj; s(cid:0)j)
whenever si = sj and s(cid:0)i = s(cid:0)j
Symmetric games have relatively compact descriptions and
may present associated computational advantages. Given a
symmetric game, we may focus on the subclass of symmet-
ric equilibria, which are arguably most natural [Kreps, 1990],
and avoid the need to coordinate on roles. In fairly general
settings, symmetric games do possess symmetric equilibria
[Nash, 1951].

3 Payoff Function Approximation
3.1 Problem De(cid:2)nition
We are given a set of data points (s; v), each describing an
instance where agents played strategy pro(cid:2)le s and realized
value v = (v1; : : : ; vm). For deterministic games of complete
information, v is simply u. With incomplete information or
stochastic outcomes, v is a random variable, more speci(cid:2)cally
an independent draw from a distribution function of s, with
expected value u(s).

The payoff function approximation task is to select a func-
tion ^u from a candidate set U minimizing some measure of
deviation from the true payoff function u. Because the true
function u is unknown, of course, we must base our selection
on evidence provided by the given data points.

Our goal in approximating payoff functions is typically not
predicting payoffs themselves, but rather in assessing strate-
gic behavior. Therefore, for assessing our results, we measure

approximation quality not directly in terms of a distance be-
tween ^u and u, but rather in terms of the strategies dictated
by ^u evaluated with respect to u. For this we appeal to the
notion of approximate Nash equilibrium.
De(cid:2)nition 4 A strategy pro(cid:2)le (cid:27) = ((cid:27)1; : : : ; (cid:27)m) constitutes
an (cid:15)-Nash equilibrium of game [I;f(cid:1)(Si)g;fui(s)g] if for
every i 2 I, (cid:27)0i 2 (cid:1)(Si), ui((cid:27)i; (cid:27)(cid:0)i) + (cid:15) (cid:21) ui((cid:27)0i; (cid:27)(cid:0)i):
We propose using (cid:15) in the above de(cid:2)nition as a mea-
sure of approximation error of ^u, and employ it in evaluat-
ing our learning methods. When u is known, we can com-
pute (cid:15) in a straightforward manner. Let s(cid:3)i denote i’s best-
response correspondence, de(cid:2)ned by s(cid:3)i ((cid:27)(cid:0)i) = fx : x 2
arg maxsi ui(si; (cid:27)(cid:0)i)g: For clarity of exposition, we take
s(cid:3)i ((cid:27)(cid:0)i) to be single-valued. Let ^(cid:27) be a solution (e.g., a Nash
equilibrium) of game [I;f(cid:1)(Si)g;f^ui(s)g]. Then ^(cid:27) is an (cid:15)-
Nash equilibrium of the true game [I;f(cid:1)(Si)g;fui(s)g], for
(cid:15) = maxi2I [ui(s(cid:3)i (^(cid:27)(cid:0)i); ^(cid:27)(cid:0)i) (cid:0) ui(^(cid:27)i; ^(cid:27)(cid:0)i)] :
Since in general u will either be unknown or not amenable
to this analysis, we developed a method for estimating (cid:15) from
data. We will describe it in some detail below.

For the remainder of this report, we focus on a special case
of the general problem, where action sets are real-valued in-
tervals, Si = [0; 1]. Moreover, we restrict attention to sym-
metric games and further limit the number of variables in
payoff-function hypotheses by using some form of aggrega-
tion of other agents’ actions.1 The assumption of symmetry
allows us to adopt the convention for the remainder of the
paper that payoff u(si; s(cid:0)i) is to the agent playing si.
3.2 Polynomial Regression
One class of models we consider are the nth-degree separable
polynomials:

u(si; (cid:30)(s(cid:0)i)) = ansn

i + (cid:1)(cid:1)(cid:1) + a1si+

+ bn(cid:30)n(s(cid:0)i) + (cid:1)(cid:1)(cid:1) + b1(cid:30)(s(cid:0)i) + d;

(1)

where (cid:30)(s(cid:0)i) represents some aggregation of the strategies
played by agents other than i. For two-player games, (cid:30) is
simply the identity function. We refer to polynomials of the
form (1) as separable, since they lack terms combining si and
s(cid:0)i. We also consider models with such terms, for example,
the non-separable quadratic:
u(si; (cid:30)(s(cid:0)i)) = a2s2

i + a1si + b2(cid:30)2(s(cid:0)i)+
+ b1(cid:30)(s(cid:0)i) + csi(cid:30)(s(cid:0)i) + d:

(2)

Note that (2) and (1) coincide in the case n = 2 and c =
0. In the experiments described below, we employ a simpler
version of non-separable quadratic that takes b1 = b2 = 0.

One advantage of the quadratic form is that we can ana-
lytically solve for Nash equilibrium. Given a general non-
separable quadratic (2), the necessary (cid:2)rst-order condition
for an interior solution is si = (cid:0)(a1 + c(cid:30)(s(cid:0)i))=2a2: This
reduces to si = (cid:0)a1=2a2 in the separable case. For the
non-separable case with additive aggregation, (cid:30)sum(s(cid:0)i) =
1Although none of these restrictions are inherent in the approach,
one must of course recognize the tradeoffs in complexity of the hy-
pothesis space and generalization performance.

Pj6=i sj, we can derive an explicit (cid:2)rst-order condition for
symmetric equilibrium: si = (cid:0)a1=(2a2 + (m (cid:0) 1)c):
While a pure-strategy equilibrium will necessarily exist for
any separable polynomial model, it is only guaranteed to ex-
ist in the non-separable case when the learned quadratic is
concave. In the experiments that follow, when the learned
non-separable quadratic does not have a pure Nash equilib-
rium, we generate an arbitrary symmetric pure pro(cid:2)le as the
approximate Nash equilibrium.

Another dif(cid:2)culty arises when a polynomial of a degree
In

higher than three has more than one Nash equilibrium.
such a case we select an equilibrium arbitrarily.

3.3 Local Regression
In addition to polynomial models, we explored learning using
two local regression methods: locally weighted average and
locally weighted quadratic regression [Atkeson et al., 1997].
Unlike model-based methods such as polynomial regression,
local methods do not attempt to infer model coef(cid:2)cients from
data. Instead, these methods weigh the training data points
by distance from the query point and estimate the answer(cid:151)in
our case, the payoff at the strategy pro(cid:2)le point(cid:151)using some
function of the weighted data set. We used a Gaussian weight
function: w = e(cid:0)d2, where d is the distance of the training
data point from the query point and w is the weight that is
assigned to that training point.

In the case of locally weighted average, we simply take the
weighted average of the payoffs of the training data points as
our payoff at an arbitrary strategy pro(cid:2)le. Locally weighted
quadratic regression, on the other hand, (cid:2)ts a quadratic re-
gression to the weighted data set for each query point.

3.4 Support Vector Machine Regression
The third category of learning methods we used was Support
Vector Machines (SVMs). For details regarding this learn-
ing method, we refer an interested reader to [Vapnik, 1995].
In our experiments, we used SVM light package [Joachims,
1999], which is an open-source implementation of SVM clas-
si(cid:2)cation and regression algorithms.

3.5 Finding Mixed Strategy Equilibria
In the case of polynomial regression, we were able to (cid:2)nd ei-
ther analytic or simple and robust numeric methods for com-
puting pure Nash equilibria. With local regression and SVM
learning we are not so fortunate, as we do not have access
to a closed-form description of the function we are learning.
Furthermore, we are often interested in mixed strategy ap-
proximate equilibria, and our polynomial models and solution
methods yield pure strategy equilibria.

When a particular learned model is not amenable to a
closed-form solution, we can approximate the learned game
with a (cid:2)nite strategy grid and (cid:2)nd a mixed-strategy equi-
librium of the resulting (cid:2)nite game using a general-purpose
(cid:2)nite-game solver. We employed replicator dynamics [Fu-
denberg and Levine, 1998], which searches for a symmetric
mixed equilibrium using an iterative evolutionary algorithm.
We treat the result after a (cid:2)xed number of iterations as an
approximate Nash equilibrium of the learned game.

Pj6=i(sj)2. The third variant, (cid:30)identity(s(cid:0)i) = s(cid:0)i, sim-

3.6 Strategy Aggregation
As noted above, we consider payoff functions on two-
dimensional strategy pro(cid:2)les in the form u(si; s(cid:0)i) =
f (si; (cid:30)(s(cid:0)i)): As long as (cid:30)(s(cid:0)i) is invariant under different
permutations of the same strategies in s(cid:0)i, the payoff func-
tion is symmetric. Since the actual payoff functions for our
example games are also known to be symmetric, we constrain
that (cid:30)(s(cid:0)i) preserve the symmetry of the underlying game.
In our experiments, we compared three variants of (cid:30)(s(cid:0)i).
First and most compact is the simple sum, (cid:30)sum(s(cid:0)i). Sec-
ond is the ordered pair ((cid:30)sum; (cid:30)ss), where (cid:30)ss(s(cid:0)i) =
ply takes the strategies in their direct, unaggregated form. To
enforce the symmetry requirement in this last case, we sort
the strategies in s(cid:0)i.
4 First-Price Sealed-Bid Auction
In the standard (cid:2)rst-price sealed-bid (FPSB) auction game
[Krishna, 2002], agents have private valuations for the good
for sale, and simultaneously choose a bid price representing
their offer to purchase the good. The bidder naming the high-
est price gets the good and pays the offered price. Other
agents receive and pay nothing. In the classic setup (cid:2)rst ana-
lyzed by Vickrey [1961], agents have identical valuation dis-
tributions, uniform on [0; 1], and these distributions are com-
mon knowledge. The unique (Bayesian) Nash equilibrium of
this game is for agent i to bid m(cid:0)1
m xi, where xi is i’s valua-
tion for the good.

Note that strategies in this game (and generally for games
of incomplete information), bi
: [0; 1] ! [0; 1], are func-
tions of the agent’s private information. We consider a re-
stricted case, where bid functions are constrained to the form
bi(xi) = kixi; ki 2 [0; 1]: This constraint transforms the
action space to a real interval, corresponding to choice of
parameter ki. We can easily see that the restricted strategy
space includes the known equilibrium of the full game, with
si = ki = m(cid:0)1
m for all i, which is also an equilibrium of the
restricted game in which agents are constrained to strategies
of the given form.

We further focus on the special case m = 2, with corre-
sponding equilibrium at s1 = s2 = 1=2. For the two-player
FPSB, we can also derive a closed-form description of the
actual expected payoff function:

0:25
(s1(cid:0)1)[(s2)2(cid:0)3(s1)2]
s1(1(cid:0)s1)

6(s1)2

3s2

if s1 = s2 = 0,
if s1 (cid:21) s2,
otherwise:

(3)

u(s1; s2) =8><>:

The availability of known solutions for this example fa-
cilitates analysis of our learning approach. Our results are
summarized in Figure 1. For each of our methods (classes of
functional forms), we measured average (cid:15) for varying train-
ing set sizes. For instance, to evaluate the performance of
separable quadratic approximation with training size N, we
independently draw N strategies, fs1; : : : ; sNg, uniformly
on [0; 1]. The corresponding training set comprises O(N 2)
points: ((si; sj); u(si; sj)), for i; j 2 f1; : : : ; Ng, with u as
given by (3). We (cid:2)nd the best separable quadratic (cid:2)t ^u to

these points, and (cid:2)nd a Nash equilibrium corresponding to ^u.
We then calculate the least (cid:15) for which this strategy pro(cid:2)le is
an (cid:15)-Nash equilibrium with respect to the actual payoff func-
tion u. We repeat this process 200 times, averaging the results
over strategy draws, to obtain each value plotted in Figure 1.

e
 
e
g
a
r
e
v
A

0.03

0.025

0.02

0.015

0.01

0.005

0
5

Sample best
Separable quadratic
Non−separable quadratic
3rd degree poly
4th degree poly

10

Number of strategies in training set

15

20

Figure 1: Epsilon versus number of training strategy points
for different functional forms.

1

)
5
.
0
,

s
(
s
f
f
o
y
a
p

0.2

0.15

0.1

0.05

0
0

Actual function
Learned quadratic

0.2

0.4

s1

0.6

0.8

1

Figure 2: Learned and actual payoff function when the
other agent plays 0.5. The learned function is the separable
quadratic, for a particular sample with N = 5.

As we can see, both second-degree polynomial forms we
tried do quite well on this game. For N < 20, quadratic
regression outperforms the model labeled (cid:147)sample best(cid:148), in
which the payoff function is approximated by the discrete
training set directly. The derived equilibrium in this model
is simply a Nash equilibrium over the discrete strategies in
the training set. At (cid:2)rst, the success of the quadratic model
may be surprising, since the actual payoff function (3) is
only piecewise differentiable and has a point of discontinu-
ity. However, as we can see from Figure 2, it appears quite
smooth and well approximated by a quadratic polynomial.
The higher-degree polynomials apparently over(cid:2)t the data, as

indicated by their inferior learning performance displayed in
this game.

The results of this game provide an optimistic view of how
well regression might be expected to perform compared to
discretization. This game is quite easy for learning since
the underlying payoff function is well captured by our lower-
degree model. Moreover, our experimental setup eliminated
the issue of noisy payoff observations, by employing the ac-
tual expected payoffs for selected strategies.

5 Market-Based Scheduling Game
The second game we investigate presents a signi(cid:2)cantly more
dif(cid:2)cult learning challenge.
It is a (cid:2)ve-player symmetric
game, with no analytic characterization, and no (theoreti-
cally) known solution. The game hinges on incomplete infor-
mation, and training data is available only from a simulator
that samples from the underlying distribution.

The game is based on a market-based scheduling scenario
[Reeves et al., 2005], where agents bid in simultaneous auc-
tions for time-indexed resources necessary to perform their
given jobs. Agents have private information about their job
lengths, and values for completing their jobs by various dead-
lines. Note that the full space of strategies is quite complex: it
is dependent on multi-dimensional private information about
preferences as well as price histories for all the time slots.
As in the FPSB example, we transform this policy space to
the real interval by constraining strategies to a parametrized
form. In particular, we start from a simple myopic policy(cid:151)
straightforward bidding [Milgrom, 2000], and modify it by
a scalar parameter (called (cid:147)sunk awareness(cid:148), and denoted by
k) that controls the agent’s tendency to stick with slots that
it is currently winning. Although the details and motivation
for sunk awareness are inessential to the current study, we
note that k 2 [0; 1], and that the optimal setting of k involves
tradeoffs, generally dependent on other agents’ behavior.
To investigate learning for this game, we collected data
for all strategy pro(cid:2)les over the discrete set of values k 2
f0; 0:05; : : : ; 1g. Accounting for symmetry, this represents
53,130 distinct strategy pro(cid:2)les. For evaluation purposes, we
treat the sample averages for each discrete pro(cid:2)le as the true
expected payoffs on this grid.

The previous empirical study of this game by Reeves
et al. [2005] estimated the payoff function over a dis-
crete grid of pro(cid:2)les assembled from the strategies
f0:8; 0:85; 0:9; 0:95; 1g, computing an approximate Nash
equilibrium using replicator dynamics. We therefore
generated a training set based on the data for
these
strategies (300000 samples per pro(cid:2)le), regressed to the
quadratic forms, and calculated empirical (cid:15) values with
to the entire data set by computing the maxi-
respect
mum bene(cid:2)t from deviation within the data:
(cid:15)emp =
maxi2I maxsi2Si [ui(si; ^s(cid:0)i) (cid:0) ui(^s)] ; where Si
is the
strategy set of player i represented within the data set. Since
the game is symmetric, the maximum over the players can be
dropped, and all the agent strategy sets are identical.

From the results presented in Table 1, we see that the Nash
equilibria for the learned functions are quite close to that pro-
duced by replicator dynamics, but with (cid:15) values quite a bit

lower.
(Since 0.876 is not a grid point, we determined its
(cid:15) post hoc, by running further pro(cid:2)le simulations with all
agents playing 0.876, and where one agent deviates to any
of the strategies in f0; 0:05; : : : ; 1g.)

Method

Separable quadratic

Non-separable quadratic

Replicator Dynamics

Equilibrium si

0.876
0.876

(0,0.94,0.06,0,0)

(cid:15)

0.0027
0.0027
0.0238

Table 1: Values of (cid:15) for the symmetric pure-strategy equilib-
ria of games de(cid:2)ned by different payoff function approxima-
tion methods. The quadratic models were trained on pro(cid:2)les
con(cid:2)ned to strategies in f0.8,0.85,0.9,0.95,1g.

In a more comprehensive trial, we collected 2.2 million ad-
ditional samples per pro(cid:2)le, and ran our learning algorithms
on 100 training sets, each uniformly randomly selected from
the discrete grid f0; 0:05; : : : ; 1g. Each training set included
pro(cid:2)les generated from between (cid:2)ve and ten of the twenty-
one agent strategies on the grid. Since in this case the pro-
(cid:2)le of interest does not typically appear in the complete data
set, we developed a method for estimating (cid:15) for pure sym-
metric approximate equilibria in symmetric games based on
a mixture of neighbor strategies that do appear in the test set.
Let us designate the pure symmetric equilibrium strategy of
the approximated game by ^s. We (cid:2)rst determine the closest
neighbors to ^s in the symmetric strategy set S represented
within the data. Let these neighbors be denoted by s0 and
s00. We de(cid:2)ne a mixed strategy (cid:11) over support fs0; s00g as
the probability of playing s0, computed based on the relative
distance of ^s from its neighbors: (cid:11) = 1 (cid:0) j^s (cid:0) s0j=js0 (cid:0) s00j:
Note that symmetry allows a more compact representation of
a payoff function if agents other than i have a choice of only
two strategies. Thus, we de(cid:2)ne U (si; j) as the payoff to a
(symmetric) player for playing strategy si 2 S when j other
agents play strategy s0. If m (cid:0) 1 agents each independently
choose whether to play s0 with probability (cid:11), then the proba-
bility that exactly j will choose s0 is given by

Pr((cid:11); j) =(cid:18)m (cid:0) 1

j (cid:19)(cid:11)j(1 (cid:0) (cid:11))m(cid:0)1(cid:0)j :

We can thus approximate (cid:15) of the mixed strategy (cid:11) by

m(cid:0)1Xj=0

max
si2S

Pr((cid:11); j) (U (si; j) (cid:0) (cid:11)U (s0; j) (cid:0) (1 (cid:0) (cid:11))U (s00; j)) :
Using this method of estimating (cid:15) on the complete data
set, we compared results from polynomial regression to the
method which simply selects from the training set the pure
strategy pro(cid:2)le with the smallest value of (cid:15). We refer to
this method as (cid:147)sample best(cid:148), differentiating between the
case where we only consider symmetric pure pro(cid:2)les (la-
beled (cid:147)sample best (symmetric)(cid:148)) and all pure pro(cid:2)les (la-
beled (cid:147)sample best (all)(cid:148)).2

2It is interesting to observe in Figures 3 and 4 that when we re-
strict the search for a best pure strategy pro(cid:2)le to symmetric pro(cid:2)les,
we on average do better in terms of (cid:15) then when this restriction is not
imposed.

x 10−3

Separable quadratic

e
 
e
g
a
r
e
v
A

12

11

10

9

8

7
5

f:sum
f:(sum,sum squares)
f:identity
Sample best (symmetric)
Sample best (all)

6
9
Number of strategies in training set

7

8

10

Figure 3: Effectiveness of learning a separable quadratic
model with different forms of (cid:30)(s(cid:0)i).

x 10−3

Non−separable quadratic

e
 
e
g
a
r
e
v
A

12

11

10

9

8

7
5

f:sum
f:(sum,sum squares)
f:identity
Sample best (symmetric)
Sample best (all)

6
9
Number of strategies in training set

7

8

10

Figure 4: Effectiveness of learning a non-separable quadratic
model with different forms of (cid:30)(s(cid:0)i).

From Figure 3 we see that regression to a separable
quadratic produces a considerably better approximate equi-
librium when the size of the training set is relatively small.
Figure 4 shows that the non-separable quadratic performs
similarly. The results appear relatively insensitive to the de-
gree of aggregation applied to the representation of other
agents’ strategies.

The polynomial regression methods we employed yield
pure-strategy Nash equilibria. We further evaluated four
methods that generally produce mixed-strategy equilibria:
two local regression learning methods, SVM with a Gaussian
radial basis kernel, and direct estimation using the training
data. As discussed above, we computed mixed strategy equi-
libria by applying replicator dynamics to discrete approxima-
tions of the learned payoff functions.3 Since we ensure that

3In the case of direct estimation from training data, the data itself

the support of any mixed strategy equilibrium produced by
these methods is in the complete data set, we can compute (cid:15)
of the equilibria directly.

As we can see in Figure 5,

locally weighted average
method appears to work better than the other three for most
data sets that include between (cid:2)ve and ten strategies. Ad-
ditionally, locally weighted regression performs better than
replicator dynamics on four of the six data set sizes we con-
sidered, and SVM consistently beats replicator dynamics for
all six data set sizes.4

It is somewhat surprising to see how irregular our results
appear for the local regression methods. We cannot explain
this irregularity, although of course there is no reason for us
to expect otherwise: even though increasing the size of the
training data set may improve the quality of (cid:2)t, improvement
in quality of equilibrium approximation does not necessarily
follow.

x 10−3

Local Regression

Replicator dynamics
Locally Weighted Average
Locally Weighted Regression
SVM

7

6

5

4

3

2

e
 
e
g
a
r
e
v
A

5

6
9
Number of strategies in training set

7

8

10

Figure 5: Effectiveness of learning local and SVM regres-
sion to estimate mixed strategy symmetric equilibria, with
(cid:30)(s(cid:0)i) = ((cid:30)sum; (cid:30)ss).

6 Conclusion
While there has been much work in game theory attempting to
solve particular games de(cid:2)ned by some payoff functions, lit-
tle attention has been given to approximating such functions
from data. This work addresses the question of payoff func-
tion approximation by introducing regression learning tech-
niques and applying them to representative games of inter-
est. Our results in both the FPSB and market-based schedul-
ing games suggest that when data is sparse, such methods

was used as input to the replicator dynamics algorithm. For the other
three methods we used a (cid:2)xed ten-strategy grid as the discretized
approximation of the learned game.

4Note that we do not compare these results to those for the poly-
nomial regression methods. Given noise in the data set, mixed-
strategy pro(cid:2)les with larger supports may exhibit lower (cid:15) simply due
to the smoothing effect of the mixtures.

can provide better approximations of the underlying game(cid:151)
at least in terms of (cid:15)-Nash equilibria(cid:151)than discrete approxi-
mations using the same data set.

Regression or other generalization methods offer the po-
tential to extend game-theoretic analysis to strategy spaces
(even in(cid:2)nite sets) beyond directly available experience. By
selecting target functions that support tractable equilibrium
calculations, we render such analysis analytically convenient.
By adopting functional forms that capture known structure of
the payoff function (e.g., symmetry), we facilitate learnabil-
ity. This study provides initial evidence that we can some-
times (cid:2)nd models serving all these criteria.

In future work we expect to apply some of the methods

developed here to other challenging domains.

References
[Atkeson et al., 1997] Christopher G. Atkeson, Andrew W.
Moore, and Stefan Schaal. Locally weighted learning. Ar-
ti(cid:2)cial Intelligence Review, 11:11(cid:150)73, 1997.

[Brafman and Tennenholtz, 2003] Ronen I. Brafman and
Moshe Tennenholtz. Learning to coordinate ef(cid:2)ciently: A
model-based approach. Journal of Arti(cid:2)cial Intelligence
Research, 19:11(cid:150)23, 2003.

[Brafman and Tennenholtz, 2004] Ronen I. Brafman and
Moshe Tennenholtz. Ef(cid:2)cient learning equilibrium. Ar-
ti(cid:2)cial Intelligence, 159:27(cid:150)47, 2004.

[Fudenberg and Levine, 1998] D. Fudenberg and D. Levine.

The Theory of Learning in Games. MIT Press, 1998.

[Joachims, 1999] Thorsten Joachims. Making large-scale
SVM learning practical. In B. Scholkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Support
Vector Learning. MIT Press, 1999.

[Kreps, 1990] David M. Kreps. Game Theory and Economic

Modelling. Oxford University Press, 1990.

[Krishna, 2002] Vijay Krishna. Auction Theory. Academic

Press, 2002.

[Milgrom, 2000] Paul Milgrom. Putting auction theory to
work: The simultaneous ascending auction. Journal of
Political Economy, 108:245(cid:150)272, 2000.

[Nash, 1951] John Nash. Non-cooperative games. Annals of

Mathematics, 54:286(cid:150)295, 1951.

[Reeves et al., 2005] Daniel M. Reeves, Michael P. Well-
man, Jeffrey K. MacKie-Mason, and Anna Osepayshvili.
Exploring bidding strategies for market-based scheduling.
Decision Support Systems, 39:67(cid:150)85, 2005.

[Shoham et al., 2003] Yoav Shoham, Rob Powers,

and
Trond Grenager. Multi-agent reinforcement learning: A
critical survey. Technical report, Stanford University,
2003.

[Vapnik, 1995] Vladimir Vapnik. The Nature of Statistical

Learning Theory. Springer-Verlag, 1995.

[Vickrey, 1961] William Vickrey. Counterspeculation, auc-
tions, and competitive sealed tenders. Journal of Finance,
16:8(cid:150)37, 1961.


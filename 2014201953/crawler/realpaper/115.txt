Real-Time Path Planning for Humanoid Robot Navigation

Jens-Steffen Gutmann

Masaki Fukuchi

Intelligent Systems Research Laboratory
fsteffen,fukuchi,mfujitag@pdp.crl.sony.co.jp

Sony Corporation, Tokyo, Japan

Masahiro Fujita

Abstract

We present a data structure and an algorithm for
real-time path planning of a humanoid robot. Due
to the many degrees of freedom, the robots shape
and available actions are approximated for (cid:2)nding
solutions ef(cid:2)ciently. The resulting 3 dimensional
con(cid:2)guration space is searched by the A* algo-
rithm (cid:2)nding solutions in tenths of a second on low-
performance, embedded hardware. Experimental
results demonstrate our solution for a robot in a
world containing obstacles with different heights,
stairs and a higher-level platform.

1 Introduction
Path-planning is one of the fundamental problems in mobile
robot navigation. It has been shown long before that the prob-
lem of moving an object through space is PSPACE-hard with
a time complexity exponential in the degrees of freedom of
the object [Latombe, 1991]. In mobile robots the degrees of
freedom are usually small (three or less) which opens the ap-
plication of a range of search techniques speci(cid:2)cally tailored
to the problem. If one can neglect orientation, e.g. a cylin-
drical holonomic system, the resulting 2 dimensional con(cid:2)g-
uration space can be searched ef(cid:2)ciently by A*, D* [Stentz,
1995], or dynamic programming [Buhmann et al., 1995].

Humanoid robots while being mobile allow for more than
the usual three degrees of freedom. Their feet can be placed
with a greater choice and the change of body posture allows
to overcome obstacles where wheeled robots fail in pass-
ing through. This enables humanoids to step onto objects
[Kuffner et al., 2002], climb up and down stairs [Hirai et al.,
1998; Gutmann et al., 2004], step over obstacles [Guan et
al., 2004], or crawl underneath objects [Shiller et al., 2001;
Kanehiro et al., 2004].

The higher degrees of freedom can be tackled e.g. by prob-
abilistic roadmaps [Kavraki et al., 1996]. However, if one can
employ a non-probabilistic approach by (cid:2)nding a suitable ap-
proximation then the solution can be seen preferable due to its
deterministic nature. One possible approximation is to model
the robot as a cylinder or by its convex hull which allows for
ef(cid:2)cient collision checking [Okada et al., 2003].

Care should be taken though not to over-simplify the prob-
lem. Humanoid robots are non-holonomic, i.e. they cannot

turn in place without requiring additional space. The trajec-
tory of the body center describes a curve similar to a car-like
robot, although the turn radius is generally quite small. It is
possible to account for the extra turning space in a cylinder
model of the robot enlarged by twice the turn radius [Li et
al., 2003; Sabe et al., 2004]. However such an approxima-
tion is overly pessimistic and prevents the robot from passing
through narrow space as we will show.

Discretizing the possible actions of the robot into a small
set of well-chosen actions, e.g. different foot placements,
leads to another way for (cid:2)nding solutions ef(cid:2)ciently. The
induced search graph grows exponentially with the number
of steps and thus, only a small number of foot steps can be
explored in a real-time system [Lorch et al., 2002]. Such a
foot-step planning system is nevertheless a valuable tool in
a hybrid system where a higher level planner provides way-
points for this sub-system [Chestnutt and Kuffner, 2004].

Shiller et al. [2001] discretize the actions and orientations
of a digital (cid:2)gure that can walk forward, sideways, turn and
crawl. They maintain an obstacle space for each combination
of action and orientation and (cid:2)nd solutions ef(cid:2)ciently for tra-
jectories of several ten steps.

We compute a trajectory for the body-center of a humanoid
robot by approximating the shape of the robot using multiple
cylinders and discretizing con(cid:2)gurations into a set of orienta-
tions. When following this trajectory, the next few foot steps
are computed using an analytical method. This enables a hu-
manoid robot to (cid:2)nd paths of up to a few meters in real-time
and includes actions such as sideways walking through nar-
row space and climbing up and down stairs.

Our work differs from Shiller’s in that we do not plan be-
haviors but select the behavior depending on the environment
when following a found path, e.g. stair-climbing over a sill.
We also maintain only one obstacle space and show how to
perform collision checking ef(cid:2)ciently. Last but not least, our
system is demonstrated on a real robot where stereo range
data incrementally updates a representation of the world.

Our approximations of con(cid:2)guration space and actions are
presented in the next section. Section 3 brie(cid:3)y describes our
approach for generating a navigation map for collision check-
ing. Section 4 details our application of the A* algorithm
for path searching. Experimental results on Sony’s humanoid
robot QRIO are described in Section 5. Finally, we discuss
limitations and extensions of our approach in Section 6.

2 Con(cid:2)guration Space and Actions
The robot moves in a 2 dimensional world. Obstacles, differ-
ent (cid:3)oor levels and stairs are projected onto one plane. We do
not neglect the orientation of the non-holonomic robot, thus
the con(cid:2)gurations space C is 3 dimensional. For allowing
ef(cid:2)cient search we discretize C into a regular grid of evenly
spaced cells using 8 different orientations:

C = X (cid:2) Y (cid:2) (cid:8)
X; Y = fi (cid:1) cs j 0 (cid:20) i < ng
(cid:8) = fi (cid:1) 45(cid:14) j 0 (cid:20) i < 8g

(1)

where cs de(cid:2)nes the cell size and n the number of cells.
We restrict the movement of the robot’s body center to the
following six actions A = f forward, backward, turn-left,
turn-right, side-left, side-rightg. For each action the transi-
tion in C is given for orientations 0(cid:14) and 45(cid:14) in Figure 1. For
other orientations the transitions are analogous.

When concatenating turn-left (or turn-right) actions, the
resulting trajectory describes a curve, see Figure 2(a). The
turn actions thus model the non-holonomic constraint of a hu-
manoid where changing the orientation requires extra space
for turning. If we approximate this curve with an arc of a cir-
cle, we (cid:2)nd that the cell size cs coincides with the turn radius
of the robot and therefore cs should be chosen as such.

The shape of the robot is approximated by two cylinders
sharing the same axis as shown in Figure 2(b). Cylinders
allow for fast collision checking and the smaller cylinder for
the legs enables the robot to pass close to low obstacles where
upper body and arms are above the obstacle.

The cylinders are a tight (cid:2)t to the shape of the robot stand-
ing still. When the robot moves, however, additional space is
needed for the transition in C as body and legs are swinging
while walking. Figure 3 shows snapshots of the robot when
moving forward, turning left and stepping sideways. We ac-
count for the additional space by enlarging the cylinders at the
start and end con(cid:2)gurations depending on the action. Both,
upper and lower, cylinders are enlarged by the same amount.
Only when walking sideways, no additional space is needed.
As we will see in Section 5 this enables the robot to pass
through narrow space by choosing the sideways walk.

It is worth noting that the humanoid robot does not neces-
sarily need to be able to exactly perform the 6 actions de(cid:2)ned
in this section. For example, the robot could well use several
foot steps for performing the 45(cid:14) rotation in our turn actions.
The only requirement is that the enlarged cylinders cover all
the space the robot requires for completing the 45(cid:14) turn.

h(cid:13)

h(cid:13)u(cid:13)

h(cid:13)l(cid:13)

r(cid:13)u(cid:13)

r(cid:13)l(cid:13)

(b)

(a)

Figure 2: (a) Turn around by concatenating 4 turn-left actions.
(b) Multi-cylinder model for approximating robot shape.

One might think of our approach to be speci(cid:2)cally tailored
to humanoids as the proposed actions and the cell size match-
ing the turn radius are probably only found for this kind of
robot. While this seems to be true, the contribution of this
work is to demonstrate the abilities of such a system in a com-
plex real-world environment. We will discuss the limitations
and possible extensions of our approach in Section 6.
3 Navigation Map and Collision Checking
Collision checking is one of the crucial factors in path plan-
ning. Cylinder models of robots allow to perform this check
in constant time if the distance to the nearest obstacle is
known; the distance is simply compared to the radius of the
cylinder. Our two cylinder model allows to check for colli-
sion in the same amount of time.

For ef(cid:2)cient collision checking we make the following as-

sumptions about the world the robot is navigating in:
1. The world is partitioned into (cid:3)oor and obstacles.
2. Floor is planar and horizontal (else it is an obstacle).
3. There are no multiple (cid:3)oor levels at the same location.
4. The robot is able to distinguish between (cid:3)oor and ob-
stacles and can can estimate their relative position and
height using its sensors.

Assumption 1 is general, 2 rules out inclined surfaces and
treats them like obstacles, 3 is true for most but a few envi-
ronments and enables the use of a 2.5D map for representing

forward backward turn-left turn-right side-left side-right

forward

turn

sideways

Figure 1: Change in con(cid:2)guration for our 6 actions.

Figure 3: Space required for performing an action. Enlarged
upper cylinders approximate the additional space needed.

the world. Finally, assumption 4 is a practical consideration
and can be achieved e.g. by plane extraction from range data.1
Under these assumptions, the robot is able to build a (cid:3)oor
and obstacle height map F OG of its environment. We em-
ploy a combination of a 3D occupancy grid with a 2.5D (cid:3)oor
height map where range data from stereo vision is segmented
into planes and integrated in a probabilistic way [Gutmann et
al., 2005]. Each cell in F OG holds a type and a height:

F OG : (x; y)

(2)
with x 2 X; y 2 Y; t 2 T = ff loor; obstacle; unknowng,
and h 2 R is the height of the (cid:3)oor or obstacle.
From this map a navigation grid NAV is computed storing
re(cid:2)ned (cid:3)oor types and distances to nearest obstacles:

7! (t; h);

NAV : (x; y)

7! (t0; d)

(3)
where t0 2 T 0 = T [ fstairs; borderg and d 2 R is the
clearance, i.e. the distance to the closest obstacle for t0 2
ff loor; stairsg. NAV re(cid:2)nes the type of F OG(x; y) = (t; h)
for t = f loor in the following way. Let U (x; y) be a neigh-
borhood around position (x; y) and (cid:1)^h be the maximum ab-
solute difference in (cid:3)oor height to cells in U (x; y):
(x0;y0)2U (x;y);F OG(x0;y0)=(f loor;h0)jh (cid:0) h0j:

(cid:1)^h =

max

(4)

The re(cid:2)ned type is then computed as:

f loor; (cid:1)^h (cid:20) df loor
stairs;
border; dstairs < (cid:1)^h

df loor < (cid:1)^h (cid:20) dstairs

(5)

t 6= f loor

ref ine(f loor) = 8<
:

ref ine(t) = t;

where df loor is the maximum step height for regular walk and
dstairs the corresponding one for climbing up or down stairs.
The re(cid:2)nement of (cid:3)oor cells allows to treat stairs and bor-
ders differently from regular (cid:3)oor. We will make use of stairs
in path-planning by assigning a different cost for traversal.
Borders are treated similar to obstacles and as such prevent
the robot from getting close to or over the end of a (cid:3)oor layer.
The clearance of a cell is de(cid:2)ned in the following way.
For a cell (x; y) with F OG(x; y) = (f loor; h) we com-
pute the difference in height (cid:1)h(x0; y0) to cell (x0; y0) with
F OG(x0; y0) = (t0; h0), ref ine(t0) 2 fobstacle; borderg:
(6)
This height difference then decides which cylinder of our
two-cylinder model comes into effect for computing the dis-
tance d0(x0; y0) to this obstacle, or if it can be ignored:
0 (cid:20) (cid:1)h0(x0; y0) < hl
d0(x0; y0) = ( e (cid:0) rl;
e (cid:0) ru; hl (cid:20) (cid:1)h0(x0; y0) (cid:20) h
otherwise
1;

(cid:1)h0(x0; y0) = h0 (cid:0) h:

where e =p(x0 (cid:0) x)2 + (y0 (cid:0) y)2 is the Euclidean distance

between the cells, and rl, ru and hl, hu are the radii and
heights of the two cylinders as depicted in Figure 2 (b).

(7)

1It should be possible to relax these assumptions and allow for
inclined surfaces. However, the model for representing the world
would become more complicated which is why we do not consider
them in this paper.

(a)

obstacles

      

      

      

      

      

      

      























different 
floor heights

z

y

x

(b)

y

obs

0

30

60

obs

x

Figure 4: Example navigation map for a simulated world.

The clearance d(f loor) of cell (x; y) is then de(cid:2)ned as the

minimum distance to all obstacles and borders.
d(f loor) =

d0(x0; y0)
(x0;y0)2X(cid:2)Y;NAV(x0;y0)=(t0;(cid:1));t02fobstacle;borderg

min

(8)

The computation of this value can be implemented ef(cid:2)ciently
by region growing of obstacles and borders until a preset max-
imum distance is reached.
Note that (8) only applies for cells where N AV (x; y) =
(f loor;(cid:1)). For other types t 2 T the clearances are set to:

d(obstacle) = (cid:0)rl;

d(unknown) = 1:

(9)
For the robot being able to stand at a given position (x; y)
with NAV(x; y) = ((cid:1); d), the clearance d must be positive in
order not to collide with an obstacle. For unknown terrain we
are optimistic in that there could be a (cid:3)oor height well above
all obstacles and thus ignore obstacles.

We can de(cid:2)ne the free con(cid:2)guration space Cf ree as
Cf ree = f(x; y; (cid:18)) 2 C j NAV(x; y) = ((cid:1); d); d > 0g:
(10)
As an example Figure 4(a) shows a simulated world fully
observed by an ideal sensor. The resulting NAV grid is shown
in Figure 4(b). Floor heights are drawn in white, yellow and
green, stairs are marked with brown crosses, the border is
(cid:2)lled in brown, and obstacles are black. Different shades of
gray show the clearance of cells where brighter cells indicate
a larger clearance than darker ones. Note how the taller ob-
stacle in the bottom right grows more to the bottom and right
of the map (where the distance to the (cid:3)oor is larger than hl)
and less to the higher level (cid:3)oor.
4 Path-Planning
We have now the basic tools for de(cid:2)ning the path-planning
problem in our domain. Let c; c0 2 C be con(cid:2)gurations.
The transition c0 = succ(c; a) for a single action a 2 A is
given in Figure 1. We de(cid:2)ne succ for a sequence of actions
a1a2 : : : am in a recursive way as
(11)
succ(c; a1a2 : : : am) = succ(succ(c; a1); a2 : : : am):
In order to test whether action a is applicable in con(cid:2)gura-

tion c, we check the type and clearance of c in NAV:

app(c; a) = allow(c; a)^

clear(c; ra) ^ clear(succ(c; a); ra)

where allow(c; a) forbids certain actions depending on the
type t0 in NAV at the position of c (e.g. we do not allow back-
ward and sideways walk in unknown terrain and only allow

(12)

forward walk on stairs), ra is additional space required for
execution of a (see Figure 3), and
c = (x; y;(cid:1)); NAV(x; y) = ((cid:1); d): (13)
clear(c; r) = d > r;
We also de(cid:2)ne app on a sequence of actions a1a2 : : : am
(14)

app(c; a1a2 : : : am) =

app(c; a1) ^ app(succ(c; a1); a2 : : : am):

and call the sequence a path if (14) holds.
The problem of path-planning for given start and goal
con(cid:2)gurations cstart; cgoal 2 Cf ree is then to (cid:2)nd a path
(cid:25) = a1a2 : : : am such that
(15)
Problems for which a path exists are called solvable. In
general there can be more than one solution for a given solv-
able problem. We are interested in an optimal path, i.e. a path
^(cid:25) that minimizes a cost function g over the path:

cgoal = succ(cstart; (cid:25)):

^(cid:25) = argmin

(16)
In our system we found the following cost function valuable:
g(c; a1) = ga(a1) + gt0(c0) + go(c0) (17)
g(c; a1a2 : : : am) = g(c; a1) + gc(a1; a2) + (18)

g(cstart; (cid:25)):

(cid:25)

g(c0; a2 : : : am)

where c0 = succ(c; a1), ga is a cost depending on the action
(e.g. forward walk has less cost than sideways which takes
more time), gt0 a cost depending on the type t0 in the NAV grid
(f loor is preferred over stairs and unknown), gc a constant
for changes in action (the robot might have to do additional
foot steps), and go considers the clearance of c0 in NAV in
order to prefer safer paths more distant from obstacles:

go(c) = ^go max(dmax (cid:0) d; 0):

(19)
Here dmax is the maximum clearance above which no further
improvement in safety is expected, d is de(cid:2)ned as in (13), and
^go is a prede(cid:2)ned constant.

We employ the well-known A* algorithm for (cid:2)nding so-
lutions. A* maintains a priority queue where candidates are
ranked according to an evaluation function f composed of the
cost function g and an estimated heuristic h to cgoal:
f (cstart; (cid:25)) = g(cstart; (cid:25)) + h(succ(cstart; (cid:25))):

(20)
For our chosen discretization of C we can employ the fol-
lowing heuristic which is optimistic and a larger cost estimate
than the Euclidean distance:

h(c) = ^ga (cid:1)(cid:26) (cid:1)x (cid:0) (cid:1)y + p2(cid:1)y; (cid:1)x > (cid:1)y
(21)
(cid:1)y (cid:0) (cid:1)x + p2(cid:1)x; (cid:1)x (cid:20) (cid:1)y
where (cid:1)x = jx(cid:0) xgj and (cid:1)y = jy(cid:0) ygj are absolute coordi-
nate differences from c = (x; y;(cid:1)) to cgoal = (xg; yg;(cid:1)) and
^ga is the minimum action cost.
From the properties of the A* algorithm and our chosen
cost and heuristic functions it follows that A* (cid:2)nds the cost
optimal path ^(cid:25) according to (16) if a solution exists.

An important factor for the ef(cid:2)ciency of A* is to detect
whether a con(cid:2)guration has already been visited before. By
using a regular grid, we can pre-compute the search graph
(but not the optimal paths since the environment is unknown)
and check for duplicates in constant time. This is an advan-
tage over methods that build the tree at the time of search as
checking for duplicates needs further processing e.g. by ap-
plying nearest-neighbor methods.

Robot speci(cid:2)cations and grid dimensions

rl = 60mm hl = 100mm df loor = 15mm cs = 40mm
ru =140mm rh = 500mm dstairs = 50mm n = 100

Allowed actions depending on NAV type

allow(c; a) f loor unknown stairs border obstacle
f orward

turn

sideways
backward

true
true
true
true

true
true
false
false

true
false
false
false

false
false
false
false

false
false
false
false

Additional clearance ra depending on action

a
ra

f orward
60mm

turn
80mm

60mm
Cost of action ga(a) depending on orientation

0

sideways

backward

ga(a)
0(cid:14)
45(cid:14)

f orward

1
p2

turn
1.1
1.1

sideways
1.3
1:3p2

backward

2
2p2

Cost of action at0 depending on NAV type

gt0 (c)

f loor unknown stairs border obstacle

0.5

0
Other constants for cost function

1

1

1

gc(a; a) = 0
dmax = 200mm ^go = 3=dmax

gc(a1; a2) = 0.25,

a1 6= a2
^ga = 1

Table 1: Parameters used in our implementation

5 Results
We implemented our approach on QRIO, Sony’s small hu-
manoid robot with 38 degrees of freedom [Fujita et al., 2003].
The robot is equipped with 3 MIPS R5000 CPUs clocked at
400MHz and a stereo camera system with a 45(cid:14) (cid:2)eld of view
providing disparity images at 12.5 fps. This stereo data is
segmented into planes from which our F OG and NAV grid
representations are updated in real-time.

Table 1 lists parameters used in our implementation for all
variables introduced in the previous sections. For stairs we
restrict actions to forward walk only. Thus, the robot only
(cid:2)nds paths on straight staircases unless there is a larger (cid:3)oor
area where turns are possible. We forbid sideways and back-
ward walk in unknown terrain because the robot is not able to
see into its moving direction. We set a high cost for backward
motion since it is generally more unsafe. Costs are added for
walking in unknown terrain and on stairs which focuses the
algorithm to search for a path on known (cid:3)oor even if the dis-
tance traveled is larger than when moving on other terrain.

For our experiments we built a 4 meters long and 1 meter
wide stage where several obstacles, a 4cm high sill, a staircase
with 4 steps each 3cm high, and a higher-level platform were
arranged as shown in Figure 5.

We placed QRIO at the left end of the stage and let it ob-
serve the environment by rotating on the spot. We then com-
manded the robot to (cid:2)nd and follow a path to a position 3
meters ahead (the center of the higher-level platform). Our
path-planning system continuously evaluates the current NAV

grid and sends paths to a path-following system which com-
putes the next two foot steps and feeds them to the robot’s
motion control system. Figures 6(cid:150)8 show 3 snapshots of the
robot while moving on the stage. We use the same encod-
ing for (cid:3)oor, border, stairs and obstacles as before. The color
indicator on the right shows the (cid:3)oor height as estimated by
the robot. Unknown space is left blank. Green dots mark the
search space of the A* algorithm while the path is shown with
blue arrows. Brown bars on the path indicate stair transitions.
In the beginning the system has to (cid:2)nd a path leading be-
tween two close obstacles (see Figure 6). Since space be-
tween the obstacles is narrow, forward walk is not possible
due to the lack of the required clearance rf orward. Therefore,
the system decides to walk sideways between the obstacles.
After passing through the narrow passage, the robot plans
a path over the sill in the center of the stage (see Figure 7).
For overcoming the sill, the system employs a separately de-
veloped stair-climbing module [Gutmann et al., 2004].

Finally, as the robot walks closer to the higher platform, it
(cid:2)nds a path on the staircase to the target location (Figure 8).
The time required for (cid:2)nding a path on our (cid:2)xed-size con-
(cid:2)guration space depends on the environment and the path
length. In the examples above the time for planning was al-
ways below 100 ms on the robot’s embedded CPU. In order
to investigate the run-time dependency on path length we ran-
domly selected goal con(cid:2)gurations in the (cid:2)nal map after the
robot reached its target. If a path to the goal could be found,
we recorded the length of the path and the time needed for
computation. Figure 9 shows the runtime versus path length
measured on a Pentium III, 1.4GHz computer. Although the
curve shows a slope at least quadratic in path length, for our
chosen dimensions of C the runtime increases only slowly
and does not exceed 40 ms even for longer paths.

The worst case run-time occurs when the goal is placed at
an unreachable location, e.g. a free position completely sur-
rounded by obstacles. In this case all but a few of the 8n2
= 80,000 con(cid:2)gurations have to be examined. The robot’s
embedded processor needs close to 900 ms for exploring this
search space (110 ms on a Pentium III, 1.4GHz).

6 Conclusion
We presented and evaluated a path-planning algorithm that
provides collision free trajectories of up to a few meters for a
humanoid robot in a layered environment in real time.

40
35
30
25
20
15
10
5
0

 

s
m
n
i
 
e
m

i
t
 
e
g
a
r
e
v
A

0

10

30

20
40
Path length in cells

50

60

Figure 9: Time for path-planning depending on path length.

Figure 5: Experimental setup. Several obstacles and a sill are
placed on a stage. A staircase leads to a higher-level platform.

200

175

150

125

100

75

50

25

0
Figure 6: Path-planning in narrow environment: the robot de-
cides to use sideways walk for passing through the obstacles.

Planning time: 72 ms

200

175

150

125

100

75

50

25

0
Figure 7: Path-planning over a sill: the robot (cid:2)nds a straight
path over the sill and then moves around the next obstacle.

Planning time: 65 ms

200

175

150

125

100

75

50

25

0
Figure 8: Path-planning to the higher platform: (cid:2)rst the robot
aligns with the staircase and then climbs up in a straight way.

Planning time: 15 ms

In our approach unknown terrain is treated in an optimistic
way, i.e. their clearing is assumed arbitrary large. As the robot
moves closer to unknown area and observes the situation, the
navigation map is re(cid:2)ned and a new path is planned automat-
ically. This allows navigation in unknown environments.

Our perception system distinguishes between (cid:3)oor, border,
stairs and obstacles and interprets them in a meaningful way
for path planning. We believe our approach advances pre-
vious methods on simulated worlds, and real robot systems
with mainly reactive, short distance navigation capabilities.
The limitations of our approach are various. Our dis-
cretization onto a grid is an approximate cell decomposition
and as such is only complete up to the resolution of the grid.
Furthermore, using only 8 orientations can prevent (cid:2)nding
paths in certain environments, for example in a narrow corri-
dor oriented at an angle of 20(cid:14).

Our approach may be extended to 2m orientations (m > 3)
by changing cell size and actions such that repeated applica-
tion of a turn action describes a curve with the minimum turn
radius of the robot. Other actions might move the robot over
more than one cell in order to align with the grid.

Our set of actions and the navigation map do not consider
the possibility of stepping over an obstacle. We believe that
for such a capability, a foot step planner using precise 3D
information is of importance. However, our approach could
be combined with such a system by marking obstacles the
robot can step over similar to as we mark stairs.

At current, only straight paths are allowed on stairs. By in-
troducing new actions left-forward and right-forward we can
also (cid:2)nd paths on spiral staircases at the cost of a higher run-
time. In practice, we observed an increase of about 25% in
run-time for the worst case scenario described in Section 5
when adding these two actions.

We think it is also possible to extend our approach to (cid:2)nd
areas where the robot can crawl underneath an obstacle. Our
current (cid:3)oor and obstacle map and the navigation grid, how-
ever, do not support such a detection yet and have to be ex-
tended (cid:2)rst. We will follow this direction in the future.

Acknowledgment
We would like to thank Stefan Edelkamp, Thilo Weigel and
the anonymous reviewers for their valuable comments. We
also thank all developers of the Sony QRIO robot for making
this work possible.

References
[Buhmann et al., 1995] J. Buhmann, W. Burgard, A.B. Cre-
mers, D. Fox, T. Hofmann, F. Schneider, J. Strikos, and
S. Thrun. The mobile robot RHINO. AI Magazine,
16(2):31(cid:150)38, 1995.

[Chestnutt and Kuffner, 2004] J. Chestnutt and J.J. Kuffner.
In

A tiered planning strategy for biped navigation.
Int. Conf. on Humanoid Robotics (Humanoids), 2004.

[Fujita et al., 2003] M. Fujita, Y. Kuroki, T. Ishida, and T.T.
Doi. A small humanoid robot SDR-4X for entertainment
applications. In Int. Conf. on Advanced Intelligent Mecha-
tronics (AIM), Kobe, Japan, 2003.

[Guan et al., 2004] Y. Guan, K. Yokoi, N.E. Sian, and
K. Tanie. Feasibility of humanoid robots stepping over
obstacles. In Int. Conf. on Intelligent Robots and Systems
(IROS), Sendai, Japan, 2004.

[Gutmann et al., 2004] J.-S. Gutmann, M. Fukuchi, and
M. Fujita. Stair climbing for humanoid robots using stereo
In Int. Conf. on Intelligent Robots and Systems
vision.
(IROS), Sendai, Japan, 2004.

[Gutmann et al., 2005] J.-S. Gutmann, M. Fukuchi, and
M. Fujita. A (cid:3)oor and obstacle height map for 3D nav-
igation of a humanoid robot. In Int. Conf. on Robotics and
Automation (ICRA), Barcelona, Spain, 2005.

[Hirai et al., 1998] K. Hirai, M. Hirose, Y. Haikawa, and
T. Takenaka. The development of Honda humanoid robot.
In Int. Conf. on Robotics and Automation (ICRA), 1998.

Kanehiro,

[Kanehiro et al., 2004] F.

Hirukawa,
K. Kaneko, S. Kajita, K. Fujiwara, K. Harada, and
K. Yokoi. Loclomotion planning of humanoid robots to
pass through narrow spaces. In Int. Conf. on Robotics and
Automation (ICRA), New Orleans, 2004.

H.

[Kavraki et al., 1996] L. Kavraki, P. Svestka, J.-C. Latombe,
and M. Overmars. Probabilistic roadmaps for path plan-
IEEE
ning in high-dimensional con(cid:2)guration spaces.
Transactions on Robotics and Automation, 12(4), 1996.

[Kuffner et al., 2002] J.J. Kuffner, S. Kagami, K. Nishi-
waki, M. Inaba, and H. Inoue. Dynamically-stable mo-
tion planning for humanoid robots. Autonomous Robots,
12(1):105(cid:150)118, 2002.

[Latombe, 1991] Jean-Claude Latombe. Robot Motion Plan-

ning. Kluwer Academic Publishers, 1991.

[Lorch et al., 2002] O. Lorch, A. Albert,

[Li et al., 2003] T.-Y. Li, P.-F. Chen, and P.-Z. Huang. Mo-
tion planning for humanoid walking in a layered environ-
ment. In Int. Conf. Robotics and Automation (ICRA), 2003.
J. Denk,
M. Gerecke, R. Cupec, J. F. Seara, W. Gerth, and
G. Schmidt. Experiments in vision-guided biped walking.
In Int. Conf. Intelligent Robots and Systems (IROS), 2002.
[Okada et al., 2003] K. Okada, M. Inaba, and H. Inoue.
Walking navigation system of humanoid robot using stereo
vision based (cid:3)oor recognition and path planning with
In Int. Conf. on Intelligent
multi-layered body image.
Robots and Systems (IROS), October 2003.

[Sabe et al., 2004] K. Sabe, M. Fukuchi, J.-S. Gutmann,
T. Ohashi, K. Kawamoto, and T. Yoshigahara. Obstacle
avoidance and path planning for humanoid robots using
stereo vision. In Int. Conf. on Robotics and Automation
(ICRA), New Orleans, 2004.

[Shiller et al., 2001] Z. Shiller, K. Yamane, and Y. Naka-
mura. Planning motion patterns of human (cid:2)gures using a
multi-layered grid and the dynamics (cid:2)lter. In Int. Conf. on
Robotics and Automation (ICRA), Seoul, Korea, 2001.

[Stentz, 1995] A. Stentz. Optimal and ef(cid:2)cient path planning
for unknown and dynamic environments. Int. Journal of
Robotics and Automation, 10(3), 1995.


Reconstructing an Agent’s Epistemic State from Observations

Richard Booth

Macquarie University
Dept. of Computing

Sydney NSW 2109 Australia

rbooth@ics.mq.edu.au

Alexander Nittka
University of Leipzig

Dept. of Computer Science
Leipzig 04109 Germany

nittka@informatik.uni-leipzig.de

Abstract

We look at the problem in belief revision of trying
to make inferences about what an agent believed
– or will believe – at a given moment, based on
an observation of how the agent has responded to
some sequence of previous belief revision inputs
over time. We adopt a “reverse engineering” ap-
proach to this problem. Assuming a framework
for iterated belief revision which is based on se-
quences, we construct a model of the agent that
“best explains” the observation. Further consider-
ations on this best-explaining model then allow in-
ferences about the agent’s epistemic behaviour to
be made. We also provide an algorithm which com-
putes this best explanation.

1 Introduction
The problem of belief revision, i.e., of how an agent should
modify its beliefs about the world given some new informa-
tion which possibly contradicts its current beliefs, is by now a
well-established research area in AI [G¨ardenfors, 1988]. Tra-
ditionally, the work in this area is done from the agent’s per-
spective, being usually pre-occupied with constructing actual
revision operators which the agent might use and with ratio-
nality postulates which constrain how these operators should
behave. In this paper we change viewpoint and instead cast
ourselves in the role of an observer of the agent.
Imagine
the following scenario. Suppose we are given some sequence
(φ1, . . . , φn) of revision inputs which a particular agent, here-
after A, has received over a certain length of time and suppose
we are also given a sequence (θ1, . . . , θn) with the interpre-
tation that following the ith input φi, A believed (at least)
θi. Throughout the paper, we make the assumptions that A
received no input between φ1 and φn other than those listed,
and that the θi are correct (but possibly partial) descriptions
of A’s beliefs after each input. A couple of questions now
suggest themselves:

• What will A believe following a further revision input
φn+1?
• What did A believe immediately before it received the
ﬁrst input φ1? What, apart from θi did A believe after
the ith input φi?

One area in which such questions might arise is in human-
machine dialogues [del Cerro et al., 1998], where the φi cor-
respond to inputs given to A – the user – by a machine, and
the θi are the user’s responses. Here, it can be useful for the
machine to keep a model of the evolution of a user’s beliefs
during a dialogue. Another setting where reconstruction of
a prior belief set is needed is in law cases, where guilt and
innocence often depend on who knew what, when. Inquiry is
a possible method of approaching this type of problem. The
above mentioned sequences can be seen as the information
the inquiry yields and which is now used for drawing conclu-
sions. Our aim in this paper is to answer these questions.

Our strategy for dealing with these questions will be to
adopt a “reverse engineering” approach – constructing a
model of the agent. (Similar approaches have already been
tried in the context of trying to infer an agent’s goals from
observable actions, e.g., [Brafman and Tennenholtz, 1997;
Lang, 2004].) Having no access to the agent’s internals, we
assume a belief revision framework A uses for determining
its beliefs and for incorporating new information, and con-
struct a model of A that explains the observation about it.
By considering this model, we will then be able to make ex-
tra inferences or predictions about A’s epistemic behaviour.
Of course this raises the problem of which belief revision
framework to choose. Such a framework will obviously
need to support iterated revision [Darwiche and Pearl, 1997;
Lehmann, 1995b; Nayak et al., 2003], and preferably also
non-prioritised revision [Hansson et al., 2001; Makinson,
1997], i.e., revision in which new inputs are allowed to be
rejected.
In this paper, we restrict the investigation to one
such framework that has been studied in [Booth, 2005]. The
idea behind it is that an agent’s epistemic state is made up of
two components: (i) a sequence ρ of sentences representing
the sequence of revision inputs the agent has received thus
far, and (ii) a single sentence N standing for the agent’s set of
core beliefs, which intuitively are those beliefs of the agent it
considers “untouchable”. The agent’s full set of beliefs in the
state [ρ, N] is then determined by a particular calculation on
ρ and N, while new revision inputs are incorporated by sim-
ply appending them to the end of ρ. Note that our choice of
this framework does not imply that others are less worthy of
investigation. The challenge now becomes to ﬁnd that partic-
ular model of this form which best explains the observation
o = h(φ1, . . . , φn), (θ1, . . . , θn)i we have made of A.

The plan of the paper is as follows. In Sect. 2 we describe
in more detail the model of epistemic state we will be assum-
ing. This will enable us to pose more precisely the problem
we want to solve. We will see that the problem essentially re-
duces to trying to guess what A’s initial epistemic state [ρ, N]
(i.e., before it received φ1) was. In Sect. 3, inspired by work
done on reasoning with conditional beliefs, we propose a way
of ﬁnding the best initial sequence – or preﬁx – ρ(N) for any
given ﬁxed N. Then, in Sect. 4 we focus on ﬁnding the best
N. This will amount to equating best with logically weakest.
The epistemic state [ρ(N), N] obtained by combining our an-
swers will be our proposed best explanation for o, which we
will call the rational explanation. In Sect. 5 we present an
algorithm which constructs the rational explanation for any
given o, before giving some examples to show the type of in-
ferences this explanation leads to in Sect. 6. In Sect. 7 we
brieﬂy mention a related piece of work by Dupin de Saint-
Cyr and Lang, before concluding and giving some pointers
for future research.
2 Modelling the Agent
We assume sentences φi, θi, N, etc. are elements of some
ﬁnitely-generated propositional language L. In our examples,
p, q, r denote distinct propositional variables. The classical
logical entailment relation between sentences is denoted by
‘, while ≡ denotes classical logical equivalence. Wherever
we use a sentence to describe a belief set the intention is that
it represents all its logical consequences. The set of all pos-
sible observations o = hι, τi which can be made of A, where
ι = (φ1, . . . , φn) and τ = (θ1, . . . , θn) are two ﬁnite se-
quences of sentences of the same length, is denoted by O. The
operation · on sequences denotes sequence concatenation.

As indicated in the introduction, we follow [Booth, 2005]
by assuming that, at any given moment in time, an agent’s
epistemic state is represented by a pair [ρ, N]. ([Konieczny
and P´erez, 2000; Lehmann, 1995b] also use sequences to rep-
resent epistemic states, but without core beliefs). In order to
fully specify the agent’s epistemic processes, we also need to
formally specify (i) how the agent determines its set of beliefs
Bel([ρ, N]) in any given state [ρ, N], and (ii) how it incorpo-
rates new revision inputs into its epistemic state. Turning ﬁrst
to (i), we can describe Bel([ρ, N]) neatly with the help of a
function f, which takes as argument a non-empty sequence
σ = (αm, . . . , α1) of sentences, and returns a sentence. f is
deﬁned by induction on the length m of σ: if m = 1 then
f (σ) = α1. If m > 1 then

(cid:26) ϕ = αm ∧ f (αm−1, . . . , α1)

f (σ) =

if ϕ 6‘ ⊥
otherwise

f (αm−1, . . . , α1)

In other words f (σ) is determined by ﬁrst taking α1 and
then going backwards through σ, adding each sentence as
we go, provided that sentence is consistent with what has
been collected so far (cf. the “linear base-revision opera-
tion” of [Nebel, 1994] and the “basic memory operator” of
[Konieczny and P´erez, 2000].) The belief set associated to the
state [ρ, N] is then given by Bel([ρ, N]) = f (ρ · N). Hence
when calculating its beliefs from the sentences appearing in
its epistemic state, an agent gives highest priority to N. Af-
ter that, it prioritises more recent information received. Note

that N is always believed, and that Bel([ρ, N]) is inconsistent
if and only if N is inconsistent.
Example 2.1. Consider N = ¬p and ρ = (q, q → p).
Bel([ρ, N]) = f (q, q → p,¬p).
In order to determine
f (q, q → p,¬p) we need to know if q is consistent with
f (q → p,¬p). As f (¬p) = ¬p and q → p is con-
sistent with ¬p, f (q → p,¬p) = (q → p) ∧ ¬p ≡ ¬q ∧ ¬p.
So q is inconsistent with f (q → p,¬p). Consequently we
get f (q, q → p,¬p) = f (q → p,¬p) and Bel([ρ, N]) =
f (q → p,¬p) ≡ ¬q ∧ ¬p.
An agent incorporates a new revision input λ into its epis-
temic state [ρ, N] by simply appending λ to ρ, i.e., the agent’s
revision function ∗ is speciﬁed by setting, for every λ ∈ L,

[ρ, N] ∗ λ = [ρ · λ, N].

Given this, we see that a new input λ will not always be be-
lieved in the new state. Indeed (when N is consistent) it will
be so only if it is consistent with N. If it contradicts N then
it will not be accepted, and in fact in this case the agent’s be-
lief set will remain unchanged (c.f. screened revision [Makin-
son, 1997]). Note also that N remains unaffected by a revi-
sion input, i.e., ∗ is a core-invariant revision operator [Booth,
2005].1 Core beliefs are needed to ensure that revision inputs
can be rejected. If they were not allowed, which corresponds
to demanding N = > in the above deﬁnitions, any consistent
revision input would belong to the agent’s beliefs.

As is shown in [Booth, 2005], the above revision method
satisﬁes several natural properties.
In particular, it stays
largely faithful to the AGM postulates [G¨ardenfors, 1988]
(leaving aside the “success” postulate, which forces all new
inputs to be accepted), and satisﬁes slight,“non-prioritised”
variants of several postulates for iterated revision which
have been proposed, including those of [Darwiche and Pearl,
1997]. One characteristic property of this method is the fol-
lowing variant of the rule “Recalcitrance” from [Nayak et al.,
2003]:

If N 6‘ (λ2 → ¬λ1) then Bel([ρ, N] ∗ λ1 ∗ λ2) ‘ λ1

This entails if the agent accepts an input λ1, then it does so
wholeheartedly, in that the only way it can be dislodged from
the belief set by a succeeding input λ2 is if that input contra-
dicts it given the core beliefs N.
Returning to our agent A from the introduction, from
now on we assume A’s epistemic state is always of the
form [ρ, N], and that A determines its belief set and
incorporates new inputs into its epistemic state as de-
scribed above. Then, suppose we make the observation
o = h(φ1, . . . , φn), (θ1, . . . , θn)i about A.
Then after
receiving the ith input φi, A’s epistemic state must be
[ρ · (φ1, . . . , φi), N] and its belief set f (ρ · (φ1, . . . , φi) · N),
where [ρ, N] is A’s unknown initial (i.e., before φ1) epistemic
state. Observation o now amounts to the following:

f (ρ · (φ1, . . . , φi) · N) ‘ θi
(1)
We make the following deﬁnitions:
Deﬁnition 2.2. Let o = h(φ1, . . . , φn), (θ1, . . . , θn)i ∈ O.
Then [ρ, N] explains o (or is an explanation for o) iff (1) above
holds. We say N is an o-acceptable core iff [ρ, N] explains o
for some ρ.

i = 1, . . . , n

1In fact the model of [Booth, 2005] allows the core itself to be

revisable. We do not explore this possibility here.

(i)

[ρ, N] = [(p → q), r] explains
Example 2.3.
h(p, q), (q, r)i because f (p → q, p, r) ≡ p ∧ q ∧ r ‘ q and
f (p → q, p, q, r) ≡ p ∧ q ∧ r ‘ r.
(ii) [(p → q),>] does not explain h(p, q), (q, r)i because
f (p → q, p, q,>) ≡ p ∧ q 6‘ r.
If we had some explanation [ρ, N] for o then we would be
able to answer the questions in the introduction: following a
new input φn+1 A will believe f (ρ · (φ1, . . . , φn, φn+1) · N),
before receiving the ﬁrst input A believes f (ρ · N), and the
beliefs after the ith input are f (ρ · (φ1, . . . , φi) · N).
Note for any o ∈ O there always exists some explanation
[ρ, N] for o, since the contradiction ⊥ is an o-acceptable core
using any ρ. But this would be a most unsatisfactory expla-
nation, since it means we just infer A believes everything at
every step.

Our job now is to choose, from the space of possible ex-
planations for o, the best one. As a guideline, we consider
an explanation good if it only makes necessary (or minimal)
assumptions about what A believes. But how do we ﬁnd
this best one? Our strategy is to split the problem into two
parts, handling ρ and N separately. First, (i) given a ﬁxed
o-acceptable core N, ﬁnd a best sequence ρ(o, N) such that
[ρ, N] explains o, then, (ii) ﬁnd a best o-acceptable core N(o).
Our best explanation for o will then be [ρ(o, N(o)), N(o)].
3 Finding ρ
Given o = h(φ1, . . . , φn), (θ1, . . . , θn)i, let us assume a ﬁxed
core N. To ﬁnd that sequence ρ(o, N) such that [ρ(o, N), N]
is the best explanation for o, given N, we will take inspiration
from work done in the area of non-monotonic reasoning on
reasoning with conditional information.
Let’s say a pair (λ, χ) of sentences is a conditional be-
lief in the state [ρ, N] iff χ would be believed after revising
[ρ, N] by λ, i.e., Bel([ρ, N] ∗ λ) ‘ χ. In this case we will
write λ ⇒[ρ,N] χ.2 This relation plays an important role,
because it turns out A’s beliefs following any sequence of
revision inputs starting from [ρ, N] is determined entirely by
the set ⇒[ρ,N] of conditional beliefs in [ρ, N]. This is because,
for any sequence of revision inputs φ1, . . . , φm, our revision
method satisﬁes
Bel([ρ, N]∗ φ1∗···∗ φm) = Bel([ρ, N]∗ f(φ1, . . . , φm, N)).
Thus, as far as their effects on the belief set go, a sequence of
revision inputs starting from [ρ, N] can always be reduced to a
single input. (But note the set of conditional beliefs ⇒[ρ,N]∗λ
in the state [ρ, N] ∗ λ following revision by λ will generally
not be the same as ⇒[ρ,N].)
All this means observation o may be translated into a par-
tial description of the set of conditional beliefs that A has in
its initial epistemic state:

CN(o) = {f (φ1, . . . , φi, N) ⇒ θi | i = 1, . . . , n}.

Clearly, if we had access to the complete set of A’s condi-
tional beliefs in its initial state, this would give another way to
answer the questions of the introduction. Now, the problem of
determining which conditional beliefs follow from a given set
2The relation ⇒[ρ,N] almost satisﬁes all the rules of a rational in-
ference relation [Lehmann and Magidor, 1992]. More precisely the
[ρ,N] χ iff [N ‘ ¬λ or λ ⇒[ρ,N] χ].
modiﬁed version does, viz., λ ⇒0

C of such beliefs has been well-studied and several solutions
have been proposed, e.g., [Geffner and Pearl, 1992; Lehmann,
1995a]. One particularly elegant and well-motivated solution
is to take the rational closure of C [Lehmann and Magidor,
1992]. Furthermore, as is shown in, e.g., [Freund, 2004], this
construction is amenable to a relatively simple representation
as a sequence of sentences! Our idea is essentially to take
ρ(o, N) to be this sequence corresponding to the rational clo-
sure of CN(o). First let us describe the general construction.
3.1 The rational closure of a set of conditionals
Given a set of conditionals C = {λi ⇒ χi | i = 1, . . . , l}
we denote by ˜C the set of material counterparts of all the
conditionals in C, i.e., ˜C = {λi → χi | i = 1, . . . , l}. Then a
sentence ν is exceptional for C iff ˜C ‘ ¬ν, and a conditional
ν ⇒ µ is exceptional for C iff its antecedent ν is. To ﬁnd the
(sequence corresponding to the) rational closure ρR(C) of C,
we ﬁrst deﬁne a decreasing sequence of sets of conditionals
C0 ⊇ C1 ⊇ ··· ⊇ Cm by setting (i) C0 = C, (ii) Ci+1 equals
the set of conditionals in Ci which are exceptional for Ci, and
(iii) m is minimal such that Cm = Cm+1. Then we set

Writing αi forV ˜Ci, the rational closure of C is then the rela-
tion ⇒R given by λ ⇒R χ iff either αm ‘ ¬λ or(cid:2)αj ∧ λ ‘ χ
where j is minimal such that αj 6‘ ¬λ(cid:3). Since αm a ··· a α0

ρR(C) = (^ ˜Cm,

^ ˜Cm−1, . . . ,

^ ˜C0).

We now make the following deﬁnition:

it easy to check that in fact this second disjunct is equivalent
to f (αm, . . . , α0, λ) ‘ χ.
Deﬁnition 3.1. Let o ∈ O and N ∈ L. We call ρR(CN(o))
the rational preﬁx of o with respect to N, and will denote it by
ρR(o, N).
Example 3.2. Let o = h(p, q), (r,¬p)i and N = ¬p. Then

CN(o) = {f (p,¬p) ⇒ r, f (p, q,¬p) ⇒ ¬p}

= {¬p ⇒ r, (q ∧ ¬p) ⇒ ¬p}.

Since neither of the individual conditionals are exceptional
for CN(o) we get C0 = CN(o) and C1 = ∅. Clearly then

also C2 = ∅ = C1 so we obtain ρR(o, N) = (V∅,V ˜CN(o)).

Rewriting the sequence using logically equivalent sentences
we get ρR(o, N) = (>,¬p → r).

Now, an interesting thing to note about the rational pre-
ﬁx construction is that it actually goes through indepen-
dently of whether N is o-acceptable.
In fact a useful side-
effect of the construction is that it actually reveals whether
N is o-acceptable. Given we have constructed ρR(o, N) =
(αm, . . . , α0), all we have to do is to look at sentence αm
and check if it is a tautology:
Proposition 3.3. Let o ∈ O and N ∈ L, and let ρR(o, N) =
(αm, . . . , α0) be the rational preﬁx of o w.r.t. N. Then
(i) if αm ≡ > then [ρR(o, N), N] is an explanation for o.
(ii) if αm 6≡ > then N is not an o-acceptable core.
Thus this proposition gives us a necessary and sufﬁcient
condition for N to be an o-acceptable core. This will be used
in the algorithm of Sect. 5.
In Example 3.2 ρR(o, N) = (>,¬p → r) was cal-
culated. The above proposition implies [(>,¬p → r),¬p]

Justiﬁcation for using the rational preﬁx

is an explanation for o = h ( p , q) , ( r , ¬p )i. This is
veriﬁed by f (>,¬p → r, p,¬p) ≡ ¬p ∧ r ‘ r and
f (>,¬p → r, p, q,¬p) ≡ ¬p ∧ q ∧ r ‘ ¬p.
3.2
In the rest of this section we assume N to be some ﬁxed o-
acceptable core. As we just saw, [ρR(o, N), N] then provides
an explanation for o given this N. In this section we want to
show in precisely what sense it could be regarded as a best
explanation given N. Let Σ = {σ | [σ, N] explains o}.
One way to compare sequences in Σ is by focusing on
the trace of belief sets they (in combination with N) induce
through o, i.e., for each σ ∈ Σ we can consider the sequence
(Belσ
i is deﬁned to be the be-
liefs after the ith input in o (under the explanation [σ, N]). In
other words Belσ
0 gives
the initial belief set.)
Example 3.4. Let o, N and ρR(o, N) be as in Example 3.2.
Then the belief trace is (¬p ∧ r,¬p ∧ r,¬p ∧ q ∧ r).
The idea would then be to deﬁne a preference relation (cid:22)1
over the sequences in Σ (with more preferred sequences cor-
responding to those “lower” in the ordering) via some prefer-
ence relation over their set of associated belief traces. Given
any two possible belief traces (β0, . . . , βn) and (γ0, . . . , γn),
let us write (β0, . . . , βn) ≤lex (γ0, . . . , γn) iff, for all i =
deﬁne, for any ρ, σ ∈ Σ:

0, . . . , n,(cid:2)βj ≡ γj for all j < i implies γi ‘ βi

i = f (σ · (φ1, . . . , φi) · N). (So Belσ

(cid:3). Then we

n), where Belσ

0 , Belσ

1 , . . . , Belσ

ρ (cid:22)1 σ iﬀ (Belρ

0, . . . , Belρ

n) ≤lex (Belσ

0 , . . . , Belσ

n).

((cid:22)1 is a pre-order (i.e., reﬂexive and transitive) on Σ.) Thus,
given two sequences in Σ, we prefer that one which leads to A
having fewer (i.e., weaker) beliefs before any of the inputs φi
were received. If the two sequences lead to equivalent beliefs
at this initial stage, then we prefer that which leads to A hav-
ing fewer beliefs after φ1 was received. If they lead to equiv-
alent beliefs also after this stage, then we prefer that which
leads to A having fewer beliefs after φ2 was received, and
so on. Thus, under this ordering, we prefer sequences which
induce A to have fewer beliefs, earlier in o. The next result
shows ρR(o, N) is a best element in Σ under this ordering.
Proposition 3.5. ρR(o, N) (cid:22)1 σ for all σ ∈ Σ.

Another way to compare sequences is to look at their con-
sequences for predicting what will happen at the next step
after o.
ρ (cid:22)2 σ iff Bel([σ, N] ∗ φ1 ∗ ··· ∗ φn ∗ λ) ‘

Bel([ρ, N] ∗ φ1 ∗ ··· ∗ φn ∗ λ) for all λ
Thus, according to this preference criterion we prefer ρ to σ
if it always leads to fewer beliefs being predicted after the
next revision input. It turns out ρR(o, N) is a most preferred
element under (cid:22)2 amongst all minimal elements under (cid:22)1.
Proposition 3.6. For all σ ∈ Σ, if σ (cid:22)1 ρR(o, N) then
ρR(o, N) (cid:22)2 σ.
Thus if we take a lexicographic combination of (cid:22)1 and
(cid:22)2 (with (cid:22)1 being considered as more important), ρR(o, N)
emerges overall as a best, most preferred, member of Σ. Hav-
ing provided a method for ﬁnding the best explanation [ρ, N]
given N, we now turn our attention to ﬁnding the best N itself.

4 Minimising N
As argued earlier, core beliefs are needed, but at the same
time we try to minimise the assumptions about the agent’s
beliefs. This includes minimising N. The ﬁrst idea would
be to simply take the disjunction of all possible o-acceptable
cores, i.e., to take N∨(o), deﬁned by

N∨(o) ≡_{N | N is an o-acceptable core}.

But is N∨(o) itself o-acceptable? Thankfully the answer is
yes, a result which follows (in our ﬁnite setting) from the fol-
lowing proposition which says that the family of o-acceptable
cores is closed under disjunctions.
Proposition 4.1. If N1 and N2 are o-acceptable then so is
N1 ∨ N2.

So as a corollary N∨(o) does indeed satisfy:
(Acceptability) N(o) is an o-acceptable core
What other properties does N∨(o) satisfy? Clearly, N∨(o)
will always be consistent provided at least one consistent o-
acceptable core exists:
(Consistency)

If N(o) ≡ ⊥ then N0 ≡ ⊥ for every
o-acceptable core N0.

If o vright o0 then N(o0) ‘ N(o)
If o vleft o0 then N(o0) ‘ N(o).

Acceptability and Consistency would appear to be abso-
lute rock-bottom properties which we would expect of any
method for ﬁnding a good o-acceptable core. However for
N∨ we can say more. Given two observations o = hι, τi and
o0 = hι0, τ0i, let us denote by o · o0 the concatenation of o and
o0, i.e., o · o0 = hι · ι0, τ · τ0i. We shall use o vright o0 to de-
note that o0 right extends o, i.e., o0 = o· o00 for some (possibly
empty) o00 ∈ O, and o vleft o0 to denote o0 left extends o, i.e.,
o0 = o00 · o for some (possibly empty) o00 ∈ O.
Proposition 4.2. Suppose o vright o0 or o vleft o0. Then
every o0-acceptable core is an o-acceptable core.
As a result of this we see N∨ satisﬁes the following 2 prop-
erties, which say extending the observation into the future or
past leads only to a logically stronger core being returned.
(Right Monotony)
(Left Monotony)
Right- and Left Monotony provide ways of expressing that
N(o) leads only to safe conclusions that something is a core
belief of A – conclusions that cannot be “defeated” by addi-
tional information about A that might come along in the form
of observations prior to, or after o.
We should point out, though, that it is not the case that by
inserting any observation anywhere in o, N∨ will always lead
to a logically stronger core. Consider o1 = h(p, q), (p,¬p)i
and o2 = h(p,¬p, q), (p,¬p,¬p)i, i.e., h(¬p), (¬p)i was in-
serted in the middle of o1. N∨(o1) ≡ q → ¬p whereas
N∨(o2) ≡ >. So although o2 extends o1 in a sense, the cor-
responding N∨ is actually weaker. Looking at o1, assuming
as we do that A received no inputs between p and q, the only
way to explain the end belief in ¬p is to ascribe core belief
q → ¬p to A (cf. the “Recalcitrance” rule in Sect. 2). How-
ever, looking at o2, the information that A received (and ac-
cepted) intermediate input ¬p is enough to “explain away”
this end belief without recourse to core beliefs. Our assump-
tion that A received no other inputs between φ1 and φn dur-
ing an observation o = h(φ1, . . . , φn), (θ1, . . . , θn)i is rather

strong. It amounts to saying that, during o, we kept our eye
on A the whole time. The above example shows that relax-
ing this assumption gives us an extra degree of freedom with
which to explain o, via the inference of intermediate inputs.
This will be a topic for future work.
It turns out the above four properties are enough to actually
characterise N∨. In fact, given the ﬁrst two, just one of Right-
and Left Monotony is sufﬁcient for this task:
Proposition 4.3. Let N : O → L be any function which
returns a sentence given any o ∈ O. Then the following are
equivalent:
(i)N satisﬁes Acceptability, Consistency and Right Monotony.
(ii) N satisﬁes Acceptability, Consistency and Left Monotony.
(iii) N(o) ≡ N∨(o) for all o ∈ O.

Note that as a corollary to this proposition we get the sur-
prising result that, in the presence of Acceptability and Con-
sistency, Right- and Left Monotony are in fact equivalent.

Combining the ﬁndings of the last two sections, we are now
ready to announce our candidate for the best explanation for
o. By analogy with “rational closure”, we make the following
deﬁnition:
Deﬁnition 4.4. Let o ∈ O be an observation. Then we call
[ρR(o, N∨(o)), N∨(o)] the rational explanation for o.
In Sect. 6 we will give some examples of what we can infer
about A under the rational explanation. But how might we
ﬁnd it in practice? The next section gives an algorithm for
just that.
5 Constructing the Rational Explanation
The idea behind the algorithm is as follows. Given an obser-
vation o, we start with the weakest possible core N0 = > and
construct the rational preﬁx (αm, . . . , α0)=ρ0 of o w.r.t. N0.
We then check whether αm is a tautology. If it is then we
know by Prop. 3.3 that [ρ0, N0] is an explanation for o and so
we stop and return this as output. If it isn’t then Prop. 3.3 tells
us N0 cannot be o-acceptable. In this case, we modify N0 by
conjoining αm to it, i.e., by setting N1 = N0 ∧ αm. Con-
structing the rational preﬁx of o w.r.t. the new core then leads
to a different preﬁx, which can be dealt with the same way.
Algorithm 1 Calculation of the rational explanation
Input: observation o
Output: the rational explanation for o

{ρ = (αm, . . . , α0)}

N ⇐ >
repeat

ρ ⇐ ρR(o, N)
N ⇐ N ∧ αm
until αm ≡ >

Return [ρ, N]

Before showing that the output of this algorithm matches
the rational explanation, we need to be sure it always termi-
nates. This is a consequence of the following:
Lemma 5.1. Let N and αm be as after the calculation of
ρR(o, N). If αm 6≡ > then N 6≡ N ∧ αm.

This result assures us that if the termination condition of
the algorithm does not hold, the new core will be strictly log-
ically stronger than the previous one. Thus the cores gener-
ated by the algorithm become progressively strictly stronger.

Then,

In our setting, in which we assumed a ﬁnite propositional lan-
guage, this means, in the worst case, the process will continue
until N ≡ ⊥. However in this case it can be shown the ratio-
nal preﬁx of o w.r.t. ⊥ is just (>), and so the termination
condition will be satisﬁed at the very next step.
Now, to show the output matches the rational explana-
tion, consider the sequence [ρ0, N0], . . . , [ρk, Nk] of epis-
temic states generated by the algorithm. We need to show
Nk ≡ N∨(o). The direction Nk ‘ N∨(o) follows from the
fact that [ρk, Nk] is an explanation for o and so Nk is an o-
acceptable core. The converse N∨(o) ‘ Nk is proved by
showing inductively that N∨(o) ‘ Ni for each i = 0, . . . , k:
the case i = 0 clearly holds since N0 ≡ >. The inductive
step uses the following property:
Lemma 5.2. Let 0 < i ≤ k and suppose ρi−1 =
for any o-acceptable core N0,
(αm, . . . , α0).
if N0 ‘ Ni−1 then N0 ‘ αm.
This enables us to prove that, given N∨(o) ‘ Ni−1, we
must also have N∨(o) ‘ Ni. Thus N∨(o) ‘ Nk as required.
Since obviously ρk is the rational preﬁx of o w.r.t. Nk by
construction, we have:
Proposition 5.3. Given input observation o, the algorithm
outputs the rational explanation for o.
Example 5.4. Let o = h(p, q), (r,¬p)i. Starting with N =>,
in the ﬁrst run ˜C0 = {f (p,>) → r, f (p, q,>) → ¬p} =
{p → r, p ∧ q → ¬p}. Only the second conditional is ex-
ceptional, so ˜C1 = {p ∧ q → ¬p}. Now the remaining con-
ditional is exceptional for itself, so ˜C2 = ˜C1. N is updated to
N ≡ p → ¬q because ρ = (p → ¬q, p → (r ∧ ¬q)).
The next calculation yields ˜C0 = {f (p, p → ¬q) → r,
f (p, q, p → ¬q) → ¬p} = {p ∧ ¬q → r, q ∧ ¬p → ¬p}.
This time none of the conditionals are exceptional, so ˜C1 = ∅.
As this means α1 = >, no further run is necessary and
the result is ρ = (>, (p ∧ ¬q) → r), N = p → ¬q.
[(>, (p∧¬q) → r), p → ¬q] is the rational explanation for o.
6 Some Examples
In this section we want to give a few simple examples to il-
lustrate the rational explanation.
For o = h(p), (q)i, the rational explanation is [(>, p → q),
>]. So we infer A’s initial belief set is p → q. Indeed to
explain A’s belief in q following receipt of p it is clear A must
initially believe at least p → q since p itself does not entail q.
It seems fair to say we are not justiﬁed in ascribing to A any
initial beliefs beyond this. After A receives p we assume A
accepts this input – we have no reason to expect otherwise –
and so has belief set p∧q. If A is given a further input ¬(p∧q)
we predict A will also accept this input, but will hold on to its
belief in p. The reason being we assume A, having only just
been told p, now has stronger reasons to believe p than q. If,
instead, A is given further input ¬p we predict its belief set
will be just ¬p, i.e., we do not assume A’s belief in q persists.
Essentially the rational explanation assumes the prior input p
must have been responsible for A’s prior belief in q. And with
this input now being “overruled” by the succeeding input, A
can no longer draw any conclusions about the truth of q.
Another illustrative example is o = h(p), (¬p)i, for which
the rational explanation is [(>),¬p]. Indeed ¬p must be a

core belief, as that is the only possibility for p to be rejected.
And if p was not rejected, the agent could not consistently
believe ¬p.
In some cases the rational explanation gives only the trivial
explanation, i.e., N∨(o) ≡ ⊥. One of the simplest examples
extends the prior one: o = h(p,¬p), (¬p, p)i. The ﬁrst part
of the observation tells us that ¬p must be a core belief, but
when conﬁrmed of that belief A changes its opinion. This
behaviour of always believing the opposite of what one is told
can be called rational only in very speciﬁc circumstances that
are not in the scope of this investigation. Hence, failing to
provide a satisfactory explanation for this example is not to
be seen as a failure of the method.
7 Conclusion
Before concluding, one paper which deserves special men-
tion as having similarities with the present one is [de Saint-
Cyr and Lang, 2002] on belief extrapolation (itself an in-
stance of the general framework of [Friedman and Halpern,
1999]). A belief extrapolation operator takes as input a se-
quence of sentences representing a sequence of partial ob-
servations of a possibly changing world, and outputs another
sequence which “completes” the input. These operators pro-
ceed by trying to determine some history of the world which
“best ﬁts” (according to various criteria) the observations. A
fundamental difference between that work and ours is that be-
lief extrapolation is, like traditional operators of revision and
update, an “agent’s perspective” operator – it is concerned
with how an agent should form a picture of how the exter-
nal world is evolving, whereas we are interested in forming a
picture of how an observed agent’s beliefs are evolving. Nev-
ertheless the precise connections between these two works
seems worthy of further study.

To conclude, in this paper we made an attempt at recon-
structing an agent’s initial epistemic state in order to explain
a given observation of the agent and make predictions about
its future beliefs. We did so by assuming a simple yet pow-
erful model for epistemic states allowing for iterated non-
prioritised revision. The algorithm we provided constructs a
best explanation based on the rational closure of conditional
beliefs. This answer should be applicable to problems requir-
ing the modelling of agents’ beliefs, for example in the area
of user modelling.

Generalisations of this approach which are object of future
work include (i) relaxing the assumption that we are given an
unbroken sequence of revision inputs, i.e., allowing also for
intermediate inputs as an explanation for what the core ac-
counts for now, (ii) allowing our observations to incorporate
information about what the agent did not believe after a given
revision step, and (iii) allowing the core beliefs to be revised.
Further, it is of interest to compare our results with what other
models of epistemic states would yield as explanation.
Acknowledgements
Thanks are due to the reviewers for helpful comments. A.N.
acknowledges support by the EC (IST-2001-37004, WASP).
References
[Booth, 2005] R. Booth. On the logic of iterated non-
prioritised revision. In Conditionals, Information and In-

ference – Selected papers from the Workshop on Condi-
tionals, Information and Inference, 2002, pages 86–107.
Springer’s LNAI 3301, 2005.

[Brafman and Tennenholtz, 1997] R. I. Brafman and M. Ten-
nenholtz. Modeling agents as qualitative decision makers.
Artiﬁcial Intelligence, 94(1-2):217–268, 1997.

[Darwiche and Pearl, 1997] A. Darwiche and J. Pearl. On
the logic of iterated belief revision. Artiﬁcial Intelligence,
89:1–29, 1997.

[de Saint-Cyr and Lang, 2002] F. Dupin de Saint-Cyr and
J. Lang. Belief extrapolation (or how to reason about ob-
In Proceedings of
servations and unpredicted change).
KR’02, pages 497–508, 2002.

[del Cerro et al., 1998] L. Fari˜nas del Cerro, A. Herzig,
D. Longin, and O. Riﬁ. Belief reconstruction in cooper-
ative dialogues. In Proceedings of AIMSA’98, pages 254–
266. Springer’s LNCS 1480, 1998.

[Freund, 2004] M. Freund. On the revision of preferences
and rational inference processes. Artiﬁcial Intelligence,
152(1):105–137, 2004.

[Friedman and Halpern, 1999] N. Friedman and J. Halpern.
Modeling belief in dynamic systems, part II: Revision
and update. Journal of Artiﬁcial Intelligence Research,
10:117–167, 1999.

[G¨ardenfors, 1988] P. G¨ardenfors. Knowledge in Flux. MIT

Press, 1988.

[Geffner and Pearl, 1992] H. Geffner and J. Pearl. Condi-
tional entailment: Bridging two approaches to default en-
tailment. Artiﬁcial Intelligence, 53:209–244, 1992.

[Hansson et al., 2001] S. O. Hansson, E. Ferm´e, J. Cantwell,
and M. Falappa. Credibility-limited revision. Journal of
Symbolic Logic, 66(4):1581–1596, 2001.

[Konieczny and P´erez, 2000] S. Konieczny and R. Pino
P´erez. A framework for iterated revision. Journal of Ap-
plied Non-Classical Logics, 10(3-4):339–367, 2000.

[Lang, 2004] J. Lang. A preference-based interpretation of
In Proceedings of KR’04, pages

other agents’ actions.
644–653, 2004.

[Lehmann and Magidor, 1992] D. Lehmann and M. Magi-
dor. What does a conditional knowledge base entail? Ar-
tiﬁcial Intelligence, 55(1):1–60, 1992.

[Lehmann, 1995a] D. Lehmann. Another perspective on de-
fault reasoning. Annals of Mathematics and Artiﬁcial In-
telligence, 15(1):61–82, 1995.

[Lehmann, 1995b] D. Lehmann. Belief revision, revised. In

Proceedings of IJCAI’95, pages 1534–1540, 1995.

[Makinson, 1997] D. Makinson. Screened revision. Theoria,

63:14–23, 1997.

[Nayak et al., 2003] A. Nayak, M. Pagnucco, and P. Peppas.
Dynamic belief revision operators. Artiﬁcial Intelligence,
146:193–228, 2003.

[Nebel, 1994] B. Nebel.

schemes: Semantics, representation and complexity.
Proceedings of ECAI’94, pages 342–345, 1994.

Base revision operations and
In


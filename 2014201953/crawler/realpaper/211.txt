The COMPSET Algorithm for Subset Selection

Yaniv Hamo and Shaul Markovitch
{hamo,shaulm}@cs.technion.ac.il

Computer Science Department, Technion, Haifa 32000, Israel

Abstract

Subset selection problems are relevant in many do-
mains. Unfortunately, their combinatorial nature
prohibits solving them optimally in most cases. Lo-
cal search algorithms have been applied to subset
selection with varying degrees of success. This
work presents COMPSET, a general algorithm for
subset selection that
invokes an existing local
search algorithm from a random subset and its com-
plementary set, exchanging information between
the two runs to help identify wrong moves. Prelimi-
nary results on complex SAT, Max Clique, 0/1 Mul-
tidimensional Knapsack and Vertex Cover prob-
lems show that COMPSET improves the efﬁcient
stochastic hill climbing and tabu search algorithms
by up to two orders of magnitudes.

Introduction

1
The subset selection problem (SSP) is simply deﬁned: Given
a set of elements E = {e1, e2, . . . , en} and a utility function
U : 2E (cid:55)→ R, ﬁnd a subset S ⊆ E such that U(S) is opti-
mal. Many real-life problems are SSPs, or can be formulated
as such. Classic examples include SAT, max clique, indepen-
dent set, vertex cover, knapsack, set covering, set partitioning,
feature subset selection (classiﬁcation) and instance selection
(for nearest-neighbor classiﬁers) to name a few.

Since the search space is exponential in the size of E,
ﬁnding an optimal subset without relaxing assumptions is in-
tractable. Problems associated with subset selection are typ-
ically NP-hard or NP-complete. Local search algorithms are
among the common methods for solving hard combinatorial
optimization problems such as subset selection. Hill climb-
ing, simulated annealing [Kirkpatrick et al., 1983] and tabu
search [Glover and Laguna, 1993] have all been proven to
provide good solutions in a variety of domains. The gen-
eral technique of random restarts is applicable to all of them,
yielding anytime behavior as well as the PAC property (prob-
ability to ﬁnd the optimal solution converges to 1 as the run-
ning time approaches inﬁnity).

The problem with these local search algorithms is, how-
ever, that they are too general. The only heuristic they have
about the domain is in a form of a black-box, the utility

function which they try to optimize.
It is therefore com-
mon to have modiﬁcations to these algorithms that trade be-
ing general for additional domain-speciﬁc knowledge. The
SAT domain is full of such variants [Gent and Walsh, 1993;
Hoos and St¨utzle, 2005] but they are common in other do-
mains as well [Khuri and B¨ack, 1994; Evans, 1998].

In this paper we present a general modiﬁcation to known
local search algorithms which improve their performance on
SSPs. The idea behind it is to exploit attributes that are
speciﬁc to search spaces of subset selection. Knowing that
it is a subset search space allows us to infer which moves
were likely to be wrong. By reversing these moves and try-
ing again, we start from a new context, and the probability
to repeat the mistake is reduced. In experiments performed
on complex SAT, max clique, 0/1 multidimensional knapsack
and vertex cover problems, the new method has shown to sig-
niﬁcantly improve the underlying search algorithm.

2 The COMPSET Algorithm
Subset selection can be expressed as a search in a graph. Each
node (state) in the graph represents a unique subset. Edges
correspond to adding or removing an element from the subset,
thus there are n = |E| edges to every node. The following
ﬁgure shows the search graph for 3 elements.

A state S can be represented by a bit vector where bit S[i] is
1 iff ei ∈ S. Moving to a neighboring state in the graph is
equivalent to ﬂipping one bit.

Each state is associated with a utility value, which is the
value of U on the subset it represents. Local search algo-
rithms typically start from a random state and make suc-
cessive improvements to it by moving to neighboring states.
They vary from each other mainly in their deﬁnition of neigh-
borhood, and their selection method.

Using this representation, all local search algorithms are
also applicable to subset selection. However, being general,
they overlook the speciﬁc characteristics of subset selection.
COMPSET guides a given local search algorithm using knowl-
edge speciﬁc to subset selection.

2.1 Characteristics of Subset Selection
In a selection problem of n elements, there are n operators:
F = {f1, f2, . . . , fn} where fi is the operator of toggling the
membership of element i in a set (if it was in the set remove
it , or add it if it was out). Applying fi is equivalent to ﬂip-
ping the ith bit in the bit vector representation. Throughout
the following discussion we assume a single optimal subset
(solution) which we donate S∗.

We make the following observations about subset search:
Observation 1. Let S(cid:48) be an arbitrary state (subset). From
any state S, there exists a subset of the operators, σ(S, S(cid:48)) ⊆
F , that when applied to S results in S(cid:48).
Proof. Since S(cid:48), as S, is an n bits long vector, there are at
most n bits of S(cid:48) that do not agree with S and need to be
ﬂipped using an operator.

Observation 2. Let S be the complementary state of S i.e.,
the state derived from S by ﬂipping all its bits. The subset of
operators σ(S, S∗) leading from S to the solution S∗, is the
complementary subset of σ(S, S∗) in F . That is, σ(S, S∗) ∪
σ(S, S∗) = F and σ(S, S∗) ∩ σ(S, S∗) = ∅.
Proof. We need to show, that for every fi∈F either fi ∈
σ(S, S∗) or fi ∈ σ(S, S∗). If S[i] = S∗
[i] then fi /∈ σ(S, S∗)
(it does not need to be ﬂipped). Moreover, if S[i] = S∗
[i], then
necessarily S[i] (cid:54)= S∗
[i], since in S all bits are ﬂipped. Thus,
fi /∈ σ(S, S∗) → fi ∈ σ(S, S∗). The same goes for the other
possible case, in which S[i] = S∗
[i].

The inherent problem in ﬁnding σ(S, S∗) using local
search, is that operators are applied successively and their ef-
fect is not necessarily of monotonic improvement due to in-
terdependencies between elements. Such non-monotonic be-
havior of U confuses local search algorithms and often makes
them trapped in local optima. In such a case, there are two
options: either the search is progressing on the correct path
to the solution (but the algorithm does not see a way of con-
tinuing), or it is off the correct path altogether. It would be
beneﬁcial to distinguish between these two scenarios.

Consider two independent hill climbing runs, one from S
and one from S. Given that the optimal solution is not found,
the two runs have stopped in local optima, LS and LS respec-
tively. We consider the subsets of operators leading from S
to LS and from S to LS. By observation 2, it is not possible
that an operator fi appears in both operator subsets if they are
both on the path to S∗. If we do observe the same operator in
both, it is a clear sign that one of them is wrong. This is the
idea behind COMPSET, which is described next.

2.2 Description of the COMPSET Algorithm
Interdependencies between elements are distracting when
searching for good solutions. Had all elements been inde-
pendent, a simple linear search, which adds element after
element as long as the utility value improves, would sufﬁce.
Local optima are an example where such interdependency
brings the search to a full stop. It is likely that by applying
the operators in a different order, or eliminating some, the
local optimum would have been avoided. COMPSET uses

the above observations to try and identify wrong invocations
of operators. It then cancels them (reverses their effect) and
resumes the run towards another local optimum or, hopefully,
the solution.

Procedure COMPSET(S, LOCALSEARCHALG)

S1 ← S ; S2 ← S ; agree ← false
loop until agree
LS ← LOCALSEARCHALG(S1)
LS ← LOCALSEARCHALG(S2)
C ← σ(S, LS) ∩ σ(S, LS)
if C is empty then
agree ← true
else
S1 ← C(LS) // apply all operators in C on LS
S2 ← C(LS)

return the better between LS and LS

end

Given a start state S, COMPSET initiates two runs of
the given local search algorithm, one from S and one from
its complementary S. Once two local optima are achieved,
the series of operators that has led to each one is examined.
Every operator that appears in both series must be wrong
in one of them. We do not know which of the runs went
wrong, so we reverse the effect of such operators in both
local optima. Once all obviously wrong operators are
undone, the local search is continued. The process repeats
upon encountering the next pair of local optima. When no
conﬂicting operators exist, and the optimal solution was not
found, COMPSET ends and returns the better of the two local
optima in hand.

The rational behind COMPSET is illustrated here:

The solution S∗ is reachable by applying all operators from
σ(S, S∗) to S (in any order), and by applying all operators
from σ(S, S∗) to S. The underlying search algorithm from S
is in a correct path if it is using only operators from σ(S, S∗).
However, a search can easily divert from this path, and it
is often necessary for its overall convergence to the solution.
Diversion occurs if an operator from σ(S, S∗) is applied to
S, or an operator from σ(S, S∗) is applied S. The problem is,
that since we do not know σ(S, S∗) or σ(S, S∗), it is difﬁcult
to detect such diversions. However, what we do know, is that
the solution S∗ conforms to σ(S, S∗) ∩ σ(S, S∗) = ∅. We
can use this fact to try and identify diversions.
Once stopped in local optima LS from S and LS from S,
we check σ(S, LS) ∩ σ(S, LS).
If the intersection is not
empty then by deﬁnition either LS or LS are off the path.
Note, that if the intersection is empty, the local optima might
still be off path, because either σ(S, LS) uses an operator
from σ(S, S∗), or σ(S, LS) uses an operator from σ(S, S∗).
However, σ(S, LS) ∩ σ(S, LS) (cid:54)= ∅ means that for sure at

σ(S,S∗)σ(S,S∗)S∗SSLSLSS1S2least one of the local optima is off path.

In general it is possible that the search algorithm would
continue applying operators and ﬁnally return to the path, but
being in a local optimum means that it has essentially ”given
up”. Elimination of all conﬂicting operators from both sides
brings us to S1 and S2, LS → S1 and LS → S2. Since all
common operators were eliminated, σ(S, S1) ∩ σ(S, S2) =
∅ and thus S1 and S2 are on a possibly correct path.
It is
still possible that σ(S, S1) contains operators from σ(S, S∗)
or that σ(S, S2) contains operators from σ(S, S∗) thus still
being an obstacle for reaching the solution.

An important point to notice, is that S1 and S2 are not nec-
essarily states that the algorithm has visited before. Groups of
operators are simultaneously eliminated, an operation which
interdependencies would prohibit had the operators were suc-
cessively eliminated. COMPSET effectively switches to an-
other context which is mostly correct, in which the eliminated
operators can be tried again.

3 Empirical Evaluation
The following algorithms were considered:
• Stochastic Hill Climbing (SHC) - starts from a random sub-
set, iteratively picks a neighboring subset (differs in exactly
one element) in random and moves there if it has a better or
equal utility value. The simplicity of SHC often misleads;
several works [Mitchell et al., 1994; Baluja, 1995] showed
that SHC does not fall from the complex GA mechanism.
In the SAT domain such stochastic local search (SLS) meth-
ods have been shown to be comparable with state-of-the-art
domain-speciﬁc algorithms [Hoos and St¨utzle, 2005].
• Tabu Search (TS) [Glover and Laguna, 1993] - examines
the neighborhood of the current state for the best replace-
ment. It moves to the chosen state even if it does not improve
the current state, which might result in cycles. To avoid cy-
cles, TS introduces the notion of a tabu list that stores the
last t (tabu tenure) operators used. TS is prevented from us-
ing operators from the tabu list when it generates the neigh-
borhood to be examined, unless certain conditions called as-
piration criteria are met. In this paper we use a common
aspiration criterion that allows operators which lead to better
state than the best obtained so far.
• Simulated Annealing (SA) [Kirkpatrick et al., 1983] - be-
gins at high temperature which enables it to move to arbitrary
neighboring states, including those which are worse than the
current one. As the temperature declines, the search is less
likely to choose a non-improving state, until it settles in a
state which is a local minimum from which it cannot escape
in low temperatures.

To test the effectiveness of COMPSET we have applied it to
SHC and TS. COMPSET is not applicable to SA since SA be-
gins with a high temperature at which it randomly moves far
from the initial state. The concept of COMPSET is to set new
start points for the underlying algorithm and by randomly
moving away from them SA defeats its purpose.
• Propositional Satisﬁability (SAT) - the problem of ﬁnding a
truth assignment that satisﬁes a given boolean formula rep-

The algorithms were tested in the following domains:

resented by a conjunction of clauses (CNF) C1 ∧ . . . ∧ Cm.
SAT is a classic SSP since we look for a subset of variables
that when assigned a true value, makes the entire formula
true. The utility function is the number of unsatisﬁed clauses
when assigning true to all variables in S:

U(S) ≡ |{Ci|Ci is false under S, 0 ≤ i ≤ m}|

The global minimum for U is 0, for satisﬁed formulas. A
search algorithm using this utility function will attempt to
maximize the number of satisﬁed clauses, which is a general-
ization of SAT called MAX-SAT. Problem instances for SAT
were obtained from the SATLIB [Hoos and St¨utzle, 2000]
repository of random 3-SAT. We use problems from the sol-
ubility phase transition region1 [Cheeseman et al., 1991].
• Max Clique - another classic SSP, where the goal is to
ﬁnd the maximum subset of vertices that forms a clique in
a graph. Given a graph G = (V, E) and a subset S ⊆ V , we
deﬁne the following utility function:

U (S) ≡

|V | − |S|
|V | − |S| + |V | + |S| · (|S| − 1) − |ES|

S is a clique
else

(cid:189)

A clique should be maximized but our implementation al-
ways minimizes U, therefore we use |V | − |S|. Incomplete
solutions are penalized by the number of additional edges
they require for being a clique (|S| · (|S| − 1) − |ES|), plus
a ﬁxed value |V | that is used to separate them from the legal
solutions. By striving to minimize U, the search algorithm
ﬁnds feasible solutions ﬁrst, and then continues by minimiz-
ing their size. The global minimum of U corresponds to the
maximum clique. Problem instances were obtained from the
DIMACS [1993] benchmark for maximum clique.
• 0/1 Multidimensional Knapsack (MKP) - the problem of
ﬁlling m knapsacks with n objects. Each object is either
placed in all m knapsacks, or in none of them (hence ”0/1”).
The knapsacks have capacities of c1, c2, . . . , cm. Each ob-
ject is associated with a proﬁt pi and m different weights,
one for each knapsack. Object i weighs wij when put into
(cid:80)n
knapsack j. The goal is to ﬁnd a subset of objects yielding
the maximum proﬁt without overﬁlling any of the knapsacks.
i=1 S[i] · wij > cj.
Knapsack j is overﬁlled in state S iff
Let k be the number of overﬁlled knapsacks. We deﬁne:
i=0 S[i] · pi

(cid:110)−(cid:80)n

U(S) ≡

k=0
k > 0

k

The utility of feasible subsets is simply their proﬁt (with
minus sign for minimization purposes).
Infeasible solu-
tions are penalized for each knapsack they overﬁll. Prob-
lem instances for MKP were obtained from the OR-library
[Beasley, 1997].
• Vertex Cover - the goal is to ﬁnd the smallest subset of
vertices in a graph that covers all edges. Given a graph
G = (V, E), we deﬁne:

U(S) ≡

|S| + |V | + |E\ES|

S covers all edges
else

(cid:189)|S|

1Random 3-SAT problem with 4.26 clauses per variable that are

the hardest to solve using local search.

For legal vertex covers, U takes values less than or equal to
|V |. Incomplete solutions are penalized by the number of
edges they do not cover, plus a ﬁxed value |V | that is used to
separate them from the legal solutions. The global minimum
of U corresponds to the optimal vertex cover.
The complementary graphs of the instances from the original
DIMACS benchmark were taken, so that the known maxi-
mum clique sizes could be translated to corresponding mini-
mum vertex covers2.

3.1 Experimental Methodology
We have tested ﬁve algorithms: SHC, TS, SA (with T =
100, α = 0.95), COMPSET over SHC and COMPSET over TS.
TS was used with t = 5 for all domains other than SAT, and
t = 9 for SAT. Each run was limited to 107 U evaluations.

All algorithms use random restart to escape from local
optima when they have still not exhausted their evaluations
quota. They use random restart also when there is no im-
provement over the last k steps. We use k = 10 for domains
other than SAT, and k = 20 for SAT. SAT is characterized by
wide and frequent plateaus [Frank et al., 1997] therefore we
chose higher values of t and k for it.

100 runs of each algorithm were performed on each prob-
lem in the test sets. Each run started from a random state, that
was common to all algorithms. We measured the number of
U evaluations needed to obtain the optimal solution in each
run, as well as the time taken.

3.2 Results
The results are summarized in Tables 1, 2, 3 and 4. For
brevity, we did not include the timing information in these
tables. The considered algorithms do not introduce a signif-
icant overhead, so the execution time is a linear function of
the number of U evaluations. The tables show the charac-
teristics of the problem instance, followed by the number of
successful runs (columns titled #ok) and the average number
of U evaluations for each algorithm. A successful run is a
run in which the algorithm has found the optimal solution
within the limit. We tested the statistical signiﬁcance of the
improvement introduced by COMPSET using the Wilcoxon
matched-pairs signed-ranks test with the extension by Etzioni
and Etzioni [1994] to cope with censored data3. A ”+” sign in
the sg. column between SHC and COMPSET/SHC indicates
that COMPSET improved SHC with p < 0.05. A ”-” sign in-
dicates that SHC performed better with p < 0.05. A ”?” sign
indicates that the difference is not signiﬁcant. Whenever it is
not possible to draw deﬁnitive conclusions since there is too
much censored data, n/a appears. The same holds for the sg.
column between TS and COMPSET/TS.

The superiority of COMPSET over the other algorithms is
striking, both in the number of evaluations, and the num-
ber and difﬁculty of instances solved.
In the SAT do-
main, the best performing algorithms are COMPSET/TS and

2Note that while it is possible to take the complementary graph,
solve the Max Clique problem, and then translate back to Vertex
Cover, none of the algorithms in this paper has done so.

3The information about runs in which the solution was not found

within the given bound is called truncated or censored.

COMPSET/SHC. The average success ratio of COMPSET/TS
is 85% and of COMPSET/SHC is 76%. For comparison, the
success ratios for SA, TS and SHC are 37%, 23% and 28%
respectively. The speedup factor gained by using COMPSET
is as large as 462 for SHC (instance uf50-011) and as large
as 156 for TS (instance uf75-013). Note that these are lower
bounds since SHC and TS were terminated because of re-
source limit for some of the runs.

The best performing algorithms in the Max Clique do-
main are COMPSET/SHC and SHC with average success
ratios of 94% and 80% respectively. The speed up fac-
tor of COMPSET/SHC over SHC is as large as 14 (instnace
sanr200 0.7).

In the Knapsack domain, the best performing algorithm is
COMPSET/SHC with an average success ratio of 89%. For
comparison, the success ratios for TS, SA, COMPSET/SHC
and SHC are 50%, 25%, 19% and 17% respectively. The
speedup factor gained by using COMPSET is as large as 127
for TS (instance WEISH07) and as large as 3.8 for SHC (in-
stance WEISH04).

The best performing algorithms in the Vertex Cover do-
main are COMPSET/SHC and SHC with average success ra-
tios of 94% and 77% respectively. SA is relatively close with
71% but TS is far behind with 23%, improved by COMPSET
to 28%. The speedup factor gained by using COMPSET is as
large as 310 for TS (instance hamming6-2) and as large as 10
for SHC (instance sanr200 0.7).

Another interesting statistics is the number of random
restarts required by the underlying search algorithm and
COMPSET, as well as the number of operator elimina-
tions performed by COMPSET and how many operators they
spanned. We have collected this data throughout 100 runs on
the p hat1000-1 vertex cover problem, a graph of 1000 ver-
tices. SHC required 517.37 random restarts on average (in
each run), while COMPSET required only 3.13. COMPSET
has performed 20.52 operator eliminations, reversing the ef-
fect of 2.97 operators each time.

4 Conclusions and Future Work
In this paper, we have provided useful insights into the do-
main of subset selection. We have realized that using lo-
cal search, paths from complementary subsets to the solution
must be distinct in terms of the operators used. This has led
us to conclude that if the paths contain common operators, it
may serve as an indication of a mistake. To test our conjec-
ture, we introduced COMPSET, a new guiding policy for local
search algorithms in the context of SSP. The results show a
signiﬁcant improvement over both TS and SHC by up to two
orders of magnitudes.

We currently in the process of running COMPSET on other
subset selection domains, progressing towards a better under-
standing of its behavior. One interesting direction is to re-
search for ways to incorporate knowledge of the entire search
paths, instead of only the local minima at their end. In ad-
dition, it is beneﬁcial to ﬁnd out how characteristics of a
speciﬁc problem affect its performance. Overall, the general
idea of incorporating such SSP speciﬁc insights seems to be
a promising lead to better subset selection algorithms.

3-SAT Instances

name
uf20-011
uf20-012
uf20-013
uf20-014
uf20-015
uf50-011
uf50-012
uf50-013
uf50-014
uf50-015
uf75-011
uf75-012
uf75-013
uf75-014
uf75-015
uf100-011
uf100-012
uf100-013
uf100-014
uf100-015
uf125-011
uf125-012
uf125-013
uf125-014
uf125-015
uf150-011
uf150-012
uf150-013
uf150-014
uf150-015
uf200-011
uf200-012
uf200-013
uf200-014
uf200-015

vars
20
20
20
20
20
50
50
50
50
50
75
75
75
75
75
100
100
100
100
100
125
125
125
125
125
150
150
150
150
150
200
200
200
200
200

clauses
91
91
91
91
91
218
218
218
218
218
325
325
325
325
325
430
430
430
430
430
538
538
538
538
538
645
645
645
645
645
860
860
860
860
860

SHC

evals

211
115
576
590
243
5,920,013
1,162,273
1,682,981
1,995,884
1,005,804
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

#ok
100
100
100
100
100
74
100
100
100
99
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

sg.
+
?
?
?
?
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
n/a
n/a
+
+
n/a
+
n/a
n/a
n/a
n/a
n/a
n/a
n/a

evals

COMPSET/SHC
#ok
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
52
46
100
100
7
82
49
11
2
0
7
2
1

167
102
655
596
256
12,792
10,624
3,803
6,704
2,547
80,928
409,036
9,700
22,677
89,098
289,359
130,835
158,180
872,275
283,024
544,188
786,882
7,202,045
7,653,790
1,574,893
1,129,916
9,525,999
4,429,762
6,748,236
9,373,759
9,811,767
-
9,506,131
9,857,446
9,900,146

TS

evals

309
190
1,089
872
421
160,575
36,955
45,616
66,185
33,894
9,137,745
9,375,096
5,890,614
7,364,576
8,595,672
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

sg.
?
?
?
?
?
+
?
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
n/a
+
+
n/a
+
+
n/a
n/a
n/a
n/a
n/a
n/a

#ok
100
100
100
100
100
100
100
100
100
100
16
12
63
50
27
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Table 1: SAT: average over all 100 runs, including censored data

COMPSET/TS
evals

Graphs

name

brock200 1
hamming6-2
hamming6-4
hamming8-2
hamming8-4
hamming10-2
hamming10-4
johnson8-2-4
johnson8-4-4
johnson16-2-4
johnson32-2-4
p hat700-1
p hat700-2
p hat700-3
p hat1000-1
p hat1000-2
p hat1000-3
p hat1500-1
p hat1500-2
p hat1500-3
sanr200 0.7
sanr200 0.9
sanr400 0.5
sanr400 0.7

|V |
200
64
64
256
256
1024
1024
28
70
120
496
700
700
700
1000
1000
1000
1500
1500
1500
200
200
400
400

SHC

evals

7,953,200
659
302
4,317
8,924
36,837
6,716,761
81
1,208
740
4,661
8,029,518
361,250
912,534
3,764,601
1,177,655
7,490,258
-
2,215,890
4,982,543
2,189,803
2,871,830
8,561,953
8,029,818

#ok
41
100
100
100
100
100
62
100
100
100
100
38
100
100
92
100
43
0
99
86
100
96
26
37

sg.
+
?
?
?
+
?
n/a
?
+
?
?
+
+
+
+
+
+
n/a
+
+
+
+
+
+

opt
21
32
4
128
16
512
40
4
14
8
16
11
44
62
10
46
68
12
65
94
18
42
13
21

evals

COMPSET/SHC
#ok
98
100
100
100
100
100
70
100
100
100
100
92
100
100
100
100
100
3
100
100
100
100
86
95

2,485,541
633
275
4,519
6,609
38,312
6,336,176
90
898
631
4,661
3,471,758
92,712
224,936
891,693
211,710
1,253,011
9,926,760
369,480
1,943,943
151,344
530,846
4,167,521
3,493,629

TS

evals

-
2,984,249
1,794
9,402,266
5,514,223
9,438,856
-
299
5,575,650
6,470
1,798,068
9,902,542
9,902,564
-
9,619,971
-
-
-
-
-
-
-
-
-

sg.
n/a
+
?
n/a
n/a
n/a
n/a
?
+
?
+
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a

#ok
0
80
100
6
45
6
0
100
45
100
83
1
1
0
4
0
0
0
0
0
0
0
0
0

COMPSET/TS
evals

#ok
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
87
67
100
100
48
100
97
48
7
24
39
41
3

#ok
0
100
100
14
54
6
0
100
97
100
85
1
1
0
4
0
0
0
0
0
1
0
0
0

275
201
940
863
361
17,250
35,072
9,275
14,317
7,335
142,969
579,683
37,587
77,866
179,802
450,070
309,667
376,166
966,083
361,632
496,277
1,048,369
4,471,437
6,388,535
1,597,965
650,899
6,722,510
1,037,124
3,384,813
7,193,000
9,703,756
8,434,746
7,769,246
6,907,585
9,766,815

-
14,163
1,807
8,612,975
4,619,182
9,438,856
-
305
1,005,066
6,305
1,600,804
9,902,542
9,902,564
-
9,619,971
-
-
-
-
-
9,900,505
-
-
-

SA

evals

1,800,168
197
4,300,196
2,800,287
201,003
8,900,193
7,800,249
3,501,795
6,001,598
3,201,009
6,306,563
9,500,223
4,404,356
5,904,975
4,762,145
8,404,825
6,321,626
6,011,208
6,175,183
8,110,739
7,211,118
7,908,011
8,906,967
9,601,212
9,001,632
3,458,095
8,332,034
3,750,319
8,507,697
8,517,484
9,013,804
8,642,899
8,546,974
6,121,901
9,648,972

SA

evals

8,880,969
1,129
517
6,486
18,546
54,520
7,956,686
204
2,802
745
4,843
9,658,510
639,270
1,513,576
6,161,992
1,645,523
8,610,929
-
2,629,112
5,884,335
3,530,091
5,535,066
9,683,128
9,252,956

#ok
82
100
57
72
98
11
22
65
40
68
37
5
56
41
53
16
37
40
41
19
28
21
11
4
10
66
17
64
15
15
10
14
15
40
4

#ok
24
100
100
100
100
100
36
100
100
100
100
6
100
100
69
100
31
0
98
73
91
73
7
11

Table 2: Maximum Clique: average over all 100 runs, including censored data

References

[Baluja, 1995] S. Baluja. An empirical comparison of seven
iterative and evolutionary function optimization heuristics.
Technical Report CMU-CS-95-193, School of Computer
Science, CMU, 1995.

[Beasley, 1997] J. E. Beasley. OR-library: a collection of test
data sets. Technical report, Management School, Imperial
College, London, 1997.

[DIMACS, 1993] DIMACS. Challenge problems for maxi-

mum clique, dimacs.rutgers.edu., 1993.

[Etzioni and Etzioni, 1994] O. Etzioni and R. Etzioni. Statis-
tical methods for analyzing speedup learning experiments.
Machine Learning, 14(1):333–347, 1994.

[Evans, 1998] Isaac K. Evans. Evolutionary algorithms for
vertex cover. In V. W. Porto, N. Saravanan, D. Waagen,
and A. E. Eiben, editors, Evolutionary Programming VII,
pages 377–386, 1998.

[Cheeseman et al., 1991] P. Cheeseman, B. Kanefsky, and
In

W. M. Taylor. Where the really hard problems are.
Proceedings of IJCAI-91, pages 331–336, 1991.

[Frank et al., 1997] J. Frank, P. Cheeseman, and J. Stutz.
When gravity fails: Local search topology. Journal of Ar-
tiﬁcial Intelligence Research, 7:249–281, 1997.

COMPSET/TS
evals

name

HP1
HP2
PB1
PB2
PB4
PB5
PB6
PB7
SENTO1
SENTO2
WEING1
WEING2
WEING3
WEING4
WEING5
WEING6
WEISH01
WEISH02
WEISH03
WEISH04
WEISH05
WEISH06
WEISH07
WEISH08
WEISH09
WEISH10
WEISH11
WEISH12
WEISH13
WEISH14
WEISH15
WEISH16
WEISH17
WEISH18
WEISH19
WEISH20

Problems
n m
4
28
4
35
27
4
4
34
2
29
10
20
30
40
30
37
30
60
30
60
28
2
2
28
2
28
2
28
2
28
2
28
5
30
5
30
30
5
5
30
5
30
5
40
5
40
5
40
5
40
50
5
5
50
5
50
5
50
5
60
5
60
5
60
5
60
70
5
5
70
70
5

opt
3418
3186
3090
3186
95168
2139
776
1035
7772
8722
141278
130883
95677
119337
98796
130623
4554
4536
4115
4561
4514
5557
5567
5605
5246
6339
5643
6339
6159
6954
7486
7289
8633
9580
7698
9450

SHC

evals

4,911,555
9,252,758
3,072,586
9,158,395
9,400,912
352,913
5,420,914
-
-
-
9,951,662
9,988,929
9,669,592
7,854,172
9,355,108
9,896,760
-
9,256,801
9,367,427
7,207,287
3,749,094
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

#ok
80
13
96
16
12
100
74
0
0
0
1
1
7
45
9
2
0
15
13
50
93
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

sg.
n/a
n/a
-
n/a
n/a
?
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
+
+
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a

evals

COMPSET/SHC
#ok
56
7
87
8
14
100
74
2
0
0
1
1
3
39
7
1
8
19
44
100
100
0
2
1
1
0
0
0
0
0
0
0
0
0
0
0

6,544,038
9,594,786
3,952,437
9,670,627
9,278,001
299,587
5,318,641
9,866,520
-
-
9,967,393
9,920,886
9,851,757
8,067,189
9,720,010
9,961,823
9,627,601
9,029,940
7,691,588
1,860,648
1,856,242
-
9,904,913
9,993,609
9,965,165
-
-
-
-
-
-
-
-
-
-
-

TS

evals
719,239
6,704,029
670,184
5,895,593
141,267
80,253
7,715,915
7,526,288
9,900,037
-
430,212
4,325,976
8,002,117
352,464
4,962,961
2,190,253
142,532
203,812
133,341
27,689
45,793
7,450,036
7,047,862
9,186,377
6,243,931
9,041,645
9,393,532
8,933,081
9,123,859
9,901,672
9,008,543
-
9,906,889
9,843,393
-
9,935,892

sg.
-
n/a
-
n/a
-
-
+
+
+
+
?
?
n/a
+
n/a
?
+
+
+
+
+
+
+
+
+
+
n/a
+
+
+
+
n/a
+
n/a
n/a
+

#ok
100
53
100
61
100
100
30
44
1
0
100
86
37
100
79
100
100
100
100
100
100
34
41
15
51
12
7
12
11
1
11
0
1
2
0
1

#ok
98
65
100
76
100
100
100
100
100
93
100
90
38
100
80
99
100
100
100
100
100
100
100
99
100
100
79
100
99
94
100
49
100
49
4
83

SA

evals

3,404,960
9,235,896
1,982,159
8,258,914
9,958,448
318,187
8,949,831
7,654,095
-
-
9,913,058
-
9,922,652
8,968,454
9,894,039
9,950,248
6,542,273
3,118,697
4,043,836
1,645,413
2,753,007
9,752,363
9,189,652
9,797,464
9,981,328
-
-
-
-
-
-
-
9,844,304
-
-
-

#ok
96
17
100
31
2
100
25
40
0
0
1
0
1
19
3
1
56
96
92
100
96
5
16
5
1
0
0
0
0
0
0
0
2
0
0
0

1,968,905
6,402,176
1,200,320
5,213,605
186,156
735,297
17,418
168,889
167,722
3,742,251
295,032
4,249,753
7,917,398
122,820
4,560,088
2,263,775
6,808
4,981
15,052
1,944
3,300
237,437
55,207
1,988,697
62,502
185,651
4,126,644
287,267
1,041,062
2,668,066
729,395
6,847,342
1,344,074
7,382,229
9,705,819
4,260,796

Table 3: Knapsack: average over all 100 runs, including censored data

Graphs

name

brock200 1
hamming6-2
hamming6-4
hamming8-2
hamming8-4
hamming10-2
hamming10-4
johnson8-2-4
johnson8-4-4
johnson16-2-4
johnson32-2-4
p hat700-1
p hat700-2
p hat700-3
p hat1000-1
p hat1000-2
p hat1000-3
p hat1500-1
p hat1500-2
p hat1500-3
sanr200 0.7
sanr200 0.9
sanr400 0.5
sanr400 0.7

|V |
200
64
64
256
256
1024
1024
28
70
120
496
700
700
700
1000
1000
1000
1500
1500
1500
200
200
400
400

opt
179
32
60
128
240
512
984
24
56
112
480
689
656
638
990
954
932
1488
1435
1406
182
158
387
379

SHC

evals

7,703,031
705
325
4,442
9,399
44,727
6,412,744
75
1,122
618
4,564
8,778,686
461,639
1,189,058
3,917,288
1,562,316
8,093,992
-
2,210,577
5,533,569
1,922,341
3,074,483
9,083,301
8,500,584

#ok
44
100
100
100
100
100
58
100
100
100
100
24
100
100
86
100
32
0
99
75
99
98
15
27

sg.
+
?
?
?
+
+
n/a
?
?
?
?
+
+
+
+
+
+
n/a
+
+
+
+
+
+

evals

COMPSET/SHC
#ok
97
100
100
100
100
100
71
100
100
100
100
96
100
100
100
100
100
2
100
100
100
100
87
96

2,407,132
663
299
4,642
6,287
38,674
5,645,469
75
962
618
4,564
3,214,600
102,745
210,935
829,765
195,142
1,163,768
9,958,079
408,447
2,070,512
181,238
551,969
4,860,526
2,838,407

TS

evals

-
3,241,435
1,813
8,903,987
6,112,570
9,535,915
-
292
4,911,854
106,329
2,192,741
9,902,454
-
-
9,905,190
9,904,961
-
-
-
-
-
-
-
-

sg.
n/a
+
?
n/a
n/a
n/a
n/a
?
+
?
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a

#ok
0
74
100
11
39
5
0
100
52
99
79
1
0
0
1
1
0
0
0
0
0
0
0
0

COMPSET/TS
evals

#ok
0
100
100
27
49
7
0
100
98
100
79
1
0
0
1
1
0
0
0
0
0
1
0
0

-
10,444
1,790
7,320,998
5,117,786
9,353,124
-
292
632,835
6,423
2,192,741
9,902,454
-
-
9,905,190
9,904,961
-
-
-
-
-
9,900,390
-
-

SA

evals

8,706,894
1,258
535
6,176
18,168
59,437
7,811,420
201
2,889
742
4,339
9,588,452
630,340
1,507,522
6,770,941
2,048,317
8,633,648
-
2,669,373
5,381,934
4,625,237
5,005,884
9,662,423
9,543,268

#ok
24
100
100
100
100
100
37
100
100
100
100
10
100
100
60
100
22
0
96
74
85
78
8
9

Table 4: Vertex cover: average over all 100 runs, including censored data

[Gent and Walsh, 1993] Ian P. Gent and Toby Walsh. To-
wards an understanding of hill-climbing procedures for
SAT. In National Conference on AI, pages 28–33, 1993.

[Glover and Laguna, 1993] F. Glover and M. Laguna. Tabu
search. In C. Reeves, editor, Modern Heuristic Techniques
for Combinatorial Problems, 1993.

[Hoos and St¨utzle, 2000] Holger H. Hoos and Thomas
St¨utzle. SATLIB: An Online Resource for Research on
SAT. In SAT20000: Highlights of Satisﬁability Research
in the year 2000, pages 283–292. 2000.

[Hoos and St¨utzle, 2005] Holger H. Hoos and Thomas
St¨utzle. Stochastic Local Search - Foundations and Ap-

plications. Morgan Kaufman Publishers, 2005.

[Khuri and B¨ack, 1994] S. Khuri and T. B¨ack. An evolution-
ary heuristic for the minimum vertex cover problem.
In
Genetic Algorithms within the Framework of Evolutionary
Computation, pages 86–90, 1994.

[Kirkpatrick et al., 1983] S. Kirkpatrick, C. D. Gelatt, and
M. P. Vecchi. Optimization by simulated annealing. Sci-
ence, 220, 4598:671–680, 1983.

[Mitchell et al., 1994] M. Mitchell,

J. H. Holland, and
S. Forrest. When will a genetic algorithm outperform hill
climbing. In Advances in NIPS, volume 6, pages 51–58,
1994.


2D Shape Classiﬁcation and Retrieval

Graham McNeill, Sethu Vijayakumar

Institute of Perception, Action and Behavior

School of Informatics, University of Edinburgh, Edinburgh, UK. EH9 3JZ

Email: G.J.McNeill-2@sms.ed.ac.uk, sethu.vijayakumar@ed.ac.uk

Abstract

We present a novel correspondence-based tech-
nique for efﬁcient shape classiﬁcation and retrieval.
Shape boundaries are described by a set of (ad hoc)
equally spaced points – avoiding the need to ex-
tract “landmark points”. By formulating the corre-
spondence problem in terms of a simple generative
model, we are able to efﬁciently compute matches
that incorporate scale, translation, rotation and re-
ﬂection invariance. A hierarchical scheme with
likelihood cut-off provides additional speed-up. In
contrast to many shape descriptors, the concept of
a mean (prototype) shape follows naturally in this
setting. This enables model based classiﬁcation,
greatly reducing the cost of the testing phase. Equal
spacing of points can be deﬁned in terms of ei-
ther perimeter distance or radial angle. It is shown
that combining the two leads to improved classiﬁ-
cation/retrieval performance.

Introduction

1
In many object recognition problems, the examples are most
easily disambiguated on the basis of shape - as opposed to
features such as color or texture. Often the classiﬁcation of
objects extracted from an image database are most intuitively
formulated as a shape classiﬁcation task. Here we present a
fast, general algorithm for classiﬁcation and retrieval of 2D
objects.

Much of the recent work in this area uses a ﬁnite set of
points taken from the object’s boundary as the shape repre-
sentation. Points can be selected on the basis of maximal
curvature [Super, 2004], distance from the centroid [Zhang
et al., 2003] or any criteria deemed suitable to the class of
shapes involved. More sophisticated approaches parameter-
ize the boundary as a closed curve and slide points along the
outline to minimize an objective function (e.g. [Wang et al.,
2004]). These methods produce good results but generally
use expensive optimization algorithms. An alternative to ﬁnd-
ing the ‘correct points’ is to simply place points at roughly
equal intervals along the boundary. Belongie et al. [Belongie
et al., 2002] used this approach effectively in their work on
shape contexts.

Having chosen how to represent the objects, one must solve
the correspondence problem in order to compare shapes. This
is illustrated in Fig.1. Also, a valid set of transformations for

shape matching is required. These transformations represent
the chosen deﬁnition of shape, i.e. they are the operations that
leave shape unchanged.

In this paper, we introduce a simple, generic technique for
shape classiﬁcation and retrieval. Shapes are represented by
a potentially large number of points from their boundaries.
The points lie at ﬁxed intervals in terms of distance along
the boundary or radial angle. This gives an accurate and ro-
bust description of shape that avoids the somewhat arbitrary
decision of what constitutes a good point. The high compu-
tational cost associated with using many points is reduced in
three ways. Firstly, a hierarchical approach avoids the need to
consider all possible label assignments. Secondly, potential
correspondences are considered simultaneously and a clear
winner often emerges early in the computation. Finally, for
classiﬁcation problems, the algorithm produces class means
with associated variances at each point. This allows model
based classiﬁcation, requiring fewer shape comparisons at the
testing stage. By considering both the distance and angle
based descriptors, we can accurately describe a wider range
of shapes and increase the classiﬁcation/retrieval accuracy.
The effectiveness of the method is demonstrated on a bench-
mark data set and compared to other state-of-the-art methods
in the ﬁeld.

2 Method
We wish to develop generic techniques for classifying or re-
trieving shapes. There are two important issues to consider:
Firstly, the ultimate goal is to work with large data sets and
hence, computational expense is an important factor. Sec-
ondly, the shapes involved may be very diverse, with large
differences between some classes. For example, one class
might consist of very jagged shapes and the other smoothly
curved shapes. This implies that overly speciﬁc models of
shape variation should be avoided.

2.1 Shape Correspondence
Given two shapes, we wish to optimally align them in order
to observe the genuine differences in shape. When shapes
are represented by a set of points, it is necessary to ﬁnd the
best match over all possible label assignments (Fig.1). In this
paper, we use a deﬁnition of shape from statistical shape anal-
ysis (e.g. [Dryden and Mardia, 1998]): The shape of a set of
labelled points is all the information that is invariant to ro-
tation, scaling and translation.
In addition to this, we also
consider reﬂection to be a valid transformation. Allowing

Figure 1: Two identical shapes (left). Large symbols indicate
corresponding points under a random assignment of labels.
To align the shapes, we must ﬁnd the correct labels as well as
the correct transformations (right).

Figure 2: Two shapes from the same class, each described
by 100 points (left) - linear interpolation is used to aid visu-
alization. The correspondence algorithm correctly cycles the
labels and the shapes can be aligned (right).

only basic transformations reﬂects our lack of prior knowl-
edge about the shape classes involved. We now formulate
the correspondence problem in a probabilistic fashion. This
will enable correspondences to be chosen at minimal compu-
tational cost.

Correspondence Algorithm
Consider two shapes, each represented by k points from
their respective boundaries: X = (x1 . . . xk)T and Y =
(y1 . . . yk)T ∈ Rk×2. Both shapes are centered:
j=1 xj =
(cid:1)k
j=1 yj = 0. We assume that the yj are generated indepen-
dently from

(cid:1)k

yj ∼ N(sΓxj+m|k + t, σ2I2)

(1)
for j = 1, . . . , k, where s is a scaling parameter, t a trans-
lation vector and Γ a 2D rotation matrix - reﬂection is dealt
with later. m ∈ {0, . . . , k − 1} cycles the labels of the points
in X. Finding the maximum likelihood estimates (MLEs) of
the parameters is a restricted version of the matching prob-
lem for two unlabelled sets investigated in [Kent et al., 2004].
For the general case, the large number of potential correspon-
dences (k!) can make ﬁnding the MLEs prohibitively expen-
sive. However, here we are dealing with boundaries and need
only consider cycling of the labels. This means that there are
only k potential label assignments and the computational cost
of computing the MLEs ˆΓ, ˆs, ˆt and ˆm is more acceptable.

The likelihood of the parameters can be written as
d2(Y, X)}

(2πσ2)k exp{ −1

L(m, s, t, Γ) =

2σ2

1

(2)

where

d2(Y, X) =

k(cid:2)

j=1

(cid:3)yj − (sΓxj+m|k + t)(cid:3)2.

For a ﬁxed m ∈ {0, ..., k − 1}, ˆs, ˆt and ˆΓ take values that
minimize d2(Y, X) - a consequence of assuming isotropic
variance and independence between points. We now exploit
a closed form solution for the minimum of d2(Y, X) which
can be written compactly using complex notation. Letting
xj ≡ (xj1, xj2)T → xj1 + ixj2 ≡ zj ∈ C, the shapes
become complex vectors: X → z, Y → w ∈ Ck. Following

[Dryden and Mardia, 1998] we have, for a ﬁxed cycling m,
and unit sized shapes (

j=1 |wj|2 = 1)

(cid:1)k

j=1 |zj|2 =

(cid:1)k
d2(X, Y) = 1 − |w∗z|2

where w∗
denotes the conjugate transpose of w. Absorbing
the size normalization into the distance expression gives the
expression for the full Procrustes distance:

min
Γ,s,t

dF (z, w) = 1 − |w∗z|2
w∗wz∗z .

(3)

This is a standard metric in shape analysis. Substituting
F (z, w) into eq.(2) allows us to evaluate L(m, ˆs, ˆt, ˆΓ) for
d2
all m ∈ {0, ..., k − 1} and hence, ﬁnd ˆm. Fig.1 demonstrates
the correspondence algorithm applied to two identical shapes
- alignment is achieved using the full Procrustes ﬁt (Section
2.2). As expected, the shapes can be perfectly matched. In
Fig.2, the shapes are different examples from the same class.
A good match is possible after ﬁnding the best correspon-
dence.

Invariance with respect to reﬂection is not yet incorporated
into the distance computation.
It is worth mentioning that
Procrustes distances can directly incorporate reﬂection invari-
ance [Mardia et al., 1979]. Unfortunately, this idea is not
compatible with our formulation of the correspondence prob-
lem. A simple example demonstrates this: Imagine z is the re-
ﬂection of w. When faced with the task of corresponding the
two shapes, we are obviously unaware of the reﬂection and
proceed to label both shapes in an anticlockwise direction. w
will only be the same as its reﬂection (in the Procrustes sense)
if the labels are reﬂected along with the points. Doing this
would ﬂip the direction of the labels to clockwise. z cannot
be matched to the original w (due to reﬂection) nor to the re-
ﬂected w (because the labels run in opposite directions). The
reﬂection of z is given by z∗ ≡ ( ¯z1, . . . , ¯zk). This operation
changes the direction of the labels to clockwise. Reversing
the order of the entries: zR ≡ ( ¯zk, . . . , ¯z1)T , switches the
direction back to anticlockwise. The optimal correspondence
between zR and w can now be found as normal. This appar-
ent doubling in computational expense can be signiﬁcantly
reduced by tracking the correspondence calculations for the
non-reﬂected shapes.
Reducing the Computational Cost
Any classiﬁcation or retrieval problem will require a large
number of correspondences to be found. The cost of com-
puting correspondences is dominated by k - the number of

Reduce dimensions using
a coarse shape representation

zw*

Reduce number of columns
using likelihood cut-off

Figure 3: Example of how the speed-up techniques can affect the dimensions of the outer product matrix - see text for details.

To evaluate d2

boundary points used. Firstly, the number of cycles to con-
sider increase linearly with k. Secondly, the distance com-
putation used to evaluate a correspondence is essentially a
dot product of vectors in Ck. From an accuracy perspective,
it may be necessary to have a large k. Fortunately, for any
given k, there are options for reducing both the number of
cycles that are considered and the cost of each distance com-
putation.
F (z, w) it is necessary to compute the dot
products w∗w, z∗z and w∗z. The values of w∗w and z∗z
do not change with the cycle value and need only be com-
puted once. The cost of corresponding z and w is therefore
dominated by calculating the dot product between w∗
and all
cycled versions of z. This is equivalent to summing along
the diagonals of the outer product matrix zw∗
(for example,
summing the leading diagonal gives the value of w∗z when
the cycle value is zero - see Fig.3). The following procedures
reduce the dimensions of zw∗
and hence, the computational
cost of the algorithm.

The number of points used, k, is assumed to be large (say
k = 100). To avoid considering all k possible values of the
cycling parameter m, we initially consider a coarse represen-
tation of the shapes using k(cid:2) < k points (say k(cid:2) = 20 and
the points are found by selecting every ﬁfth point of the 100).
The likelihoods for all 20 values of the cycling parameter m(cid:2)
are then evaluated as normal. It seems reasonable to assume
that the most likely m will correspond the shapes in a simi-
lar way to that which would be achieved using all 100 points.
Since the 20 points come from the original 100, it is easy
to relate these approximate correspondences back to the true
shapes (those described by 100 points). The ﬁnal correspon-
dences are found in the standard way, but only cycle values
close to the approximated value are investigated. The outer
product matrix is in C20×20 rather than C100×100 for the ﬁrst
round of correspondences (Fig.3). At the ﬁne tuning stage,
only a small number of the potential 100 correspondences are
evaluated. Thus, the overall cost is greatly reduced.
Regardless of the number of points used to describe a
shape, it seems that the full matrix zw∗
must be evaluated
and used to compute the Procrustes distances. However,
by computing the columns of zw∗
incrementally, the like-
lihoods over all k possible cycles can be monitored and a
correspondence chosen when one of them exceeds a given
threshold (c.f. Fig.3). This approach captures the intu-
itive idea that some correspondences will be much easier to
ﬁnd than others. The likelihood threshold is reached after

fewer iterations when ‘new’ points are selected at random
¯w137z, ¯w49z, . . . ) so that the full boundary is explored
(e.g.
quickly.1 Clearly if the threshold is never reached and all
the points are needed, this incremental method will be more
costly than the basic algorithm. The additional cost comes
from updating the k different z∗z associated with each la-
belling of z and from evaluating the likelihood at each stage.2
It can be partly avoided by evaluating zw∗
in blocks, rather
than single columns. Often very few points are required to
ﬁnd the best correspondence (Section 3.3). It is worth not-
ing that our algorithm could assign correspondences on the
basis of minimum Procrustes distance (dF ) - so why bother
with likelihoods? Fig.4 shows the evolution of the likeli-
hoods compared to the squared full Procrustes distances for a
typical problem. The likelihood approach magniﬁes the best
correspondences and kills-off the worst, leading to a quicker
choice of cycle parameter m. Recall from eq.(1) that there is
isotropic variance, σ2, at each point. σ2 is a free parameter
that affects how quickly the likelihoods peak to a single cy-
cle value. A small variance encourages fast correspondence
selection (Fig.4 (right)) but assumes little within class vari-
ability. Note that this thresholding technique is applicable
whenever the correspondence algorithm is used, i.e. it is in-
dependent of the above method for reducing the number of
cycle values considered.

Shape Descriptors
The correspondence algorithm is independent of how the k
boundary points are selected in the ﬁrst place. To keep our
approach as generic as possible, very simple point selection
techniques are used. We either choose points at roughly equal
intervals along the boundary or points at equal increments
of the radial angle (the centroid radii model [Chang et al.,
1991]). We refer to the former method as the perimeter shape
descriptor and the latter as the radial descriptor.
In some
cases, a line from the origin can intercept the boundary at
more than one point. To make the radial descriptor consis-
tent, all boundary points close to these intercepts are consid-
ered and the point most distant from the origin is selected.
This can lead to parts of the shape being ignored. However,
the results in Section 3 demonstrate the advantage of using

1Discontinuing the likelihood computation for any m whose like-
lihood drops below a given threshold would further increase the
speed of the algorithm.

2A random subset of a centered shape will be approximately cen-

tered so there is no need to update the center incrementally.

Figure 4: Squared full Procrustes distances (left) and normalized likelihoods (middle) between two shapes from the mouse
class. The different lines indicate the distance/likelihood after using a given number of points - or equivalently, computing a
given number of columns of zw∗

. Decreasing σ2 exaggerates the peaked proﬁle of the likelihoods (right).

this sub-optimal descriptor.

2.2 Shape Model
The correspondence technique discussed above can be incor-
porated into a nearest neighbor classiﬁer: given a test shape,
ﬁnd its optimal correspondence with every training shape and
classify on the basis of minimum Procrustes distance. This
requires ﬁnding a large number of correspondences, a prob-
lem that can be avoided using prototypes.
Ideally, the full
Procrustes mean of each class would form the set of proto-
types. The deﬁnition of this mean follows naturally from the
deﬁnition of dF . It is deﬁned as ¯µ ∈ Ck such that ||¯µ|| = 1
and

¯µ = arg min
µ

F (zl, µ)
d2

(4)

n(cid:2)

l=1

Table 1: Algorithm for corresponding all shapes from a given
class to a running class mean.

Initialize: µ = ﬁnal shape in class

for i = 1:cycles

for j = 1:ﬁnal shape in class
1. get corresponding labels of shape j and µ
2. ﬁt shape j to µ
3. recompute µ using arithmetic mean at each point

end

end Fit all shapes to the ﬁnal µ

where z1, . . . , zn are all the shapes in the class. Bookstein
[Bookstein, 1996] has proposed an iterative method for ﬁnd-
ing ¯µ. By incorporating the correspondence algorithm into
this method, we can correspond shapes directly onto a run-
ning class mean.3 This procedure relies on the full Procrustes
ﬁt for aligning two shapes. Fitting z to w transforms z so that
the sum of squared distances between corresponding points is
equal to dF (z, w). In Fig.1, dF (z, w) is zero and a perfect ﬁt
is achieved. In Fig.2, dF (z, w) is small because the shapes
are similar and a reasonable ﬁt is possible. The ﬁt of z onto
w is given by

(5)
The algorithm for ﬁnding the correspondences and the mean
within a given class is shown in Table 1. Shapes are succes-
sively corresponded and ﬁtted to a running mean.

zf it = z∗wz/(z∗z).

Given a mean, the sample covariance matrices can be cal-
culated at each point. The ﬁnal class model consists of a mean
shape - k independent 2D points, each with an associated co-
variance matrix. Fig.5 shows the algorithm applied to twenty
objects from the same class.

2.3 Shape classiﬁcation
The most likely correspondence and ﬁt between a test shape
and each class mean is found as described in Section 2.1. For
classiﬁcation, the model for ﬁnding correspondences (eq.(1))
is modiﬁed. Points are still assumed independent, but the

3 ¯µ can be calculated analytically when the correct labels are

known(see e.g. [Dryden and Mardia, 1998]).

Figure 5: Left: class mean (bold) and the twenty class ex-
amples ﬁtted to the mean using 100 points (interpolated for
visualization); Right: class mean and scatter of correspond-
ing points at four positions.

isotropic covariance is replaced by the sample covariance ma-
trix4 at each point. Thinking of shapes as sets of points in R2,
a test shape X = (x1, . . . , xk)T is assigned to a class c(X)
using maximum likelihood (ML)
k(cid:3)

f(xj; µcj, Σcj))

c(X) = arg max

(6)
where C is the number of classes, (µc1, . . . µck)T ∈ Rk×2
is the mean of class c, Σcj ∈ R2×2 is the sample covariance
at µcj and f(·; µcj, Σcj) is the pdf of the bivariate normal
distribution N(µcj, Σcj).

(
j=1

c=1,...,C

4Computed during the training phase.

Table 2: Classiﬁcation performance(%) for the thirty class
data set (k = number of points, perimeter descriptor used).

k
N. neighbor
ML

12

95.67
93.17

24

97.83
96.00

48

97.83
96.83

72

97.50
96.67

96

98.00
87.17

Table 3: Classiﬁcation performance(%) for the seventy class
data set with k = 48.

Perimeter Radial Combined

N. neighbor
ML

95.71
90.36

91.00
83.79

96.29
92.64

Figure 6: Example boundaries from the MPEG-7 data set.

3 Evaluation and Results
The proposed technique is tested on the benchmark MPEG-
7 shape database [Latecki et al., 2000]. This consists of 70
classes with 20 observations in each class. Pixels lying on a
closed boundary are extracted in a clockwise direction using
Matlab’s image processing toolbox. Some of these bound-
aries are shown in Fig.6 (the same data was used for Figs. 1,
2 and 5). The ﬁrst principle component of these (2D) points
is aligned with the x-axis. This helps reduce the problem of
‘sub-interval shifts’ producing an artiﬁcially high Procrustes
distance between similar shapes – particulary for small k.
The shape is described by k points using either the perimeter
or radial descriptor as described in Section 2.1. Initially we
present results without using any speed-up techniques. The
impact of these are explored in Section 3.3.

3.1 Leave-one-out Shape Classiﬁcation
A thirty class subset of the MPEG-7 database was recently
used by [Kunttu et al., 2003] in their work on multiscale
Fourier descriptors. Their approach outperformed the stan-
dard contour Fourier descriptor in various classiﬁcation tasks.
Using leave-one-out classiﬁcation with a nearest neighbor
classiﬁer, they achieved 94.2-96.3% accuracy (depending on
the length of shape descriptor). We use the perimeter descrip-
tor with the same data set and testing procedure (i.e. leave-
one-out and nearest neighbor) to enable a direct comparison
with their results. Test shapes are ﬁtted to training shapes us-
ing the correspondence method described in Section 2.1. The
nearest neighbor is the training shape closest to the test shape
in terms of full Procrustes distance. Results are shown in the
ﬁrst row of Table 2. Classiﬁcation accuracy is slightly better
than that of [Kunttu et al., 2003] and the results are consistent
over a wide range of k. The same tests were carried out using
the ML method described in Section 2.3. Results are given
in the bottom row of Table 2. This prototype based technique
is almost as accurate as the nearest neighbor classiﬁer and re-
quires that many fewer correspondences are found - 30 versus
599 per test shape.

The ﬁrst column of Table 3 shows the corresponding re-
sults for the full seventy class data set with k = 48.5 One

5Classiﬁcation using distance to the nearest prototype is less ac-
curate than the ML classiﬁer. This indicates that the (point-wise)
scatter about the mean is sufﬁciently ‘tight’ so as to produce covari-
ance matrices that accurately reﬂect the within-class variability.

reason for the slightly poorer results on the full data set is
that there are now classes for which the perimeter descriptor
is deﬁnitely not suitable. Despite the generally inferior per-
formance of the radial descriptor (Table 3, middle column),
it performs signiﬁcantly better for some classes. This sug-
gests that using both descriptors may improve classiﬁcation
accuracy. Here, we demonstrate how a naive combination
of the descriptors can enhance performance. The ML clas-
siﬁer is modiﬁed as follows: Each descriptor (perimeter and
radial) gives a vector of class-conditional likelihoods for the
test shape. The two vectors are normalized so that they each
sum to one. For a given class, the maximum of the two can-
didate likelihoods is used for classiﬁcation. This reﬂects an
assumption that the descriptor with the most conﬁdent predic-
tion is correct. It is less clear how to normalize the distances
for the nearest neighbor classiﬁer. We assume that the major-
ity of shapes bear no resemblance to the test shape and hence,
give rise to meaningless correspondences and distances. Nor-
malization is therefore carried over the minimum q entries in
each distance vector (q (cid:6) total number of shapes). The dis-
tance used for classiﬁcation is the minimum of the two can-
didate distances. Here, and in Section 3.2, we ﬁx q = 200.
The results in Table 3 show a small rise in classiﬁcation ac-
curacy for the combined case over the perimeter only case.
Evidently, the increase in performance is limited by the al-
ready high performance of the perimeter descriptor and the
simplicity of the normalization technique.

3.2 Bullseye Test for Shape Retrieval
This is a frequently used test in shape retrieval and enables us
to compare our algorithm against many of the best perform-
ing shape retrieval techniques. A single shape is presented
as a query and the top forty matches are retrieved (from the
entire data set - the test shape is not removed). The task is
repeated for each shape and the number of correct matches
(out of a maximum possible 20) are noted. A perfect per-
formance results in 1400 × 20 = 28000 matches. Results
are given as a percentage of this perfect score. The ﬁrst two
rows of Table 4 show the result of the proposed technique us-
ing the perimeter and radial descriptors when no speed-ups
were used. The bullseye score was also computed using the
combined approach as described for nearest neighbor clas-
siﬁcation in the previous section. Results are shown in the
bottom row of Table 4. [Super, 2004] notes that the best per-
forming algorithms in this test have scored between 75.44%
and 78.17%. A naive combination of two simple descriptors

Table 4: Scores for the bullseye test(%).

k
Perimeter
Radial
Combined

12

67.29
53.33
70.67

24

74.83
64.02
77.77

48

76.28
67.57
79.14

72

76.00
68.03
79.19

96

74.73
68.08
78.56

Table 5: Classiﬁcation performance using speed-ups.
C-F/LC

Standard

LC

NN
ML

97.83
96.83

97.13 (8.53)
88.75 (9.73)

C-F
97.83
94.33

97.17 (3.91)
85.00 (4.13)

outperforms all of the previously tested algorithms. It should
be noted that these results depend on the free parameter q, the
number of shapes used in normalization. We are investigating
methods for automatically selecting this value.

3.3 Fast Correspondence
We repeat the classiﬁcation tests on the thirty class data set
using the speed-ups described in Section 2.1. The perime-
ter descriptor with k=48 is used throughout. The results are
given in Table 5 - results are averaged over 10 runs where
likelihood cutoff is used. Column one shows the results when
no speed-ups are used. The second column shows the re-
sults using likelihood cut-off. A correspondence is chosen
when the highest likelihood exceeds the second highest like-
lihood by 2/(2 × k) (i.e.
twice the probability assigned to
all correspondences under a uniform distribution - recall re-
ﬂection must be considered). The average number of points
needed to choose a correspondence is given in brackets. Col-
umn three gives results when the coarse-to-ﬁne correspon-
dence technique is implemented with k(cid:2)
=12. An approximate
shape correspondence is mapped back to the 48-point shapes
and the best correspondence is chosen over three shifts of the
labels in either direction. The ﬁnal column also shows results
using the coarse-to-ﬁne technique but with likelihood cut-off
implemented when corresponding the coarse shapes. Imple-
menting the speed-ups has little impact on the accuracy of
nearest-neighbor classiﬁcation. The ML classiﬁer is signiﬁ-
cantly affected; the coarse-to-ﬁne method produces a moder-
ate reduction in accuracy (94.33 from 96.83) whilst likelihood
cut-off leads to a dramatic decrease in performance. Since a
test-shape is only compared to its correct class once in the
ML case (the class prototype), it is not robust to the increased
probability of the correspondence algorithm failing when a
low threshold likelihood cut-off is used.

4 Discussion
We have proposed an efﬁcient, correspondence-based shape
classiﬁcation algorithm for 2D boundaries. Shapes are rep-
resented by points placed at equal intervals along the bound-
ary; hence, there is no heuristic procedure for choosing the
points. A hierarchical matching (going from coarse to ﬁne
matches) and likelihood thresholding signiﬁcantly reduce the
cost of ﬁnding correspondences with minimal impact on clas-
siﬁcation accuracy. This ensures that easier to match shapes
do not have to go through the complete correspondence com-
putations. The algorithm was tested on the MPEG-7 shape
database and was shown to outperform the popular contour

Fourier method. Our approach leads to an intuitive class
model that can be used for fast prototype-based classiﬁcation.
The results using this method were comparable to that of stan-
dard nearest-neighbor classiﬁcation. The approach was also
shown to perform well in the commonly used bullseye re-
trieval test. Combining two simple shape descriptors further
improved retrieval accuracy.

The ideas presented are most relevant to general shape clas-
siﬁcation and retrieval problems where there is insufﬁcient
knowledge to construct a priori models. Combining/choosing
between different shape descriptors is difﬁcult in retrieval
since there is no training data available. However, relevance
feedback (common in document-based information retrieval)
is a realistic mechanism for gaining additional information
and can be used to guide dynamic selection and weighting of
shape descriptors. Future work will investigate this possibil-
ity.

References
[Belongie et al., 2002] S. Belongie, J. Malik, and J. Puzicha.
Shape matching and object recognition using shape con-
texts.
IEEE Trans. on Patt. Anal. and Machine Intell.,
24:509–522, 2002.

[Bookstein, 1996] F.L. Bookstein. Landmark methods for
forms without landmarks: morphometrics of group dif-
ferences in outline shape. Med. Im. Anal., 1(3):225–243,
1996.

[Chang et al., 1991] C.C. Chang, S.M. Hwang, and Buehrer.
A shape recognition scheme based on relative distances
of feature points from the centroid.
Patt. Recog.,
24(11):1053–1063, 1991.

[Dryden and Mardia, 1998] I.L. Dryden and K.V. Mardia.

Statistical Shape Analysis. Wiley, 1998.

[Kent et al., 2004] J.T. Kent, K.V. Mardia, and C.C. Tay-
lor. Matching problems for unlabelled conﬁgurations. In
Bioinformatics, Images, and Wavelets, pages 33–36, 2004.
[Kunttu et al., 2003] I. Kunttu, L. Lepisto, J. Rauhamaa, and
A. Viss. Multiscale fourier de scriptor for shape-based im-
age retrieval. In Proc. of the 12th Int. Conf. on Patt. Recog.,
pages 536–541, 2003.

[Latecki et al., 2000] L. J. Latecki, R. Lak¨amper,

and
U. Eckhardt. Shape descriptors for non-rigid shapes with
a single closed contour. In IEEE Conf. on Comp. Vis. and
Patt. Recog., pages 424 –429, 2000.

[Mardia et al., 1979] K.V. Mardia, J.T. Kent, and J.M. Bibby.

Multivariate Analysis. Academic Press, 1979.

[Super, 2004] B.J. Super. Fast correspondence-based system
for shape-retrieval. Patt. Recog. Lett., 25:217–225, 2004.
[Wang et al., 2004] S. Wang, T. Kubota, and T. Richardson.
Shape correspondence through landmark sliding. In IEEE
Conf. on Comp. Vis. and Patt. Recog., volume 1, pages
143–150, 2004.

[Zhang et al., 2003] J. Zhang, X. Zhang, H. Krim, and G.G.
Walter. Object representation and recognition in shape
spaces. Patt. Recog., 36:1143–1154, 2003.


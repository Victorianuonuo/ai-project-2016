350 

Session No.  8 Robots and Integrated Systems 

AN  INTFLLTGENT ROBOT WITH COGNITION 

AND  DFCTSION-MAKING  ABILITY 
Masakazu  F j i r i,  Takeshi  Uno, 
Haruo  Yoda, 
Tatsuo  Goto, 
Kiyoo  Takeyasu 

Central  Research  Laboratory 

Hitachi  Ltd. 

Kokubunji,  Tokyo,  Japan 

Abstract 

An  intelligent  robot  that  recognizes  and 

assembles  three-dimensional  objects  is  described. 
The  instruction  to  the  robot  is  fed  through  a 
vidicon  camera  as  a  three-view  plan  of  the 
assemblage  whose  overall  spatial  configuration 
is  then  recognized  and  decomposed  into  component 
parts  by  a  computer. 

Another  camera  looks  at  the  real  world  for 

the  specific  parts  required  for  the  assembly 
and  confirms  their  geometric  features.  The 
computer  further  makes  the  decisions  on  the 

procedure  for  the  manipulation  of  the  parts,  and 
orders  the  articulated  mechanical  hand  to 
proceed  the  assembly  sequence. 

The  information  processing  is  so  far 

restricted  to  simple  polyhedral  objects  and 
their  co-relationships. 

Descriptive  terms 

Intelligent  robot,  drawing  recognition, 

assembly  drawing,  three-view  plan,  object 
recognition,  three-dimensional  objects, 
polyhedra,  decision-making,  assembly  procedure, 
computer  manipulation. 

1. 

Introduction 

In  order  to  complete  an  integrated  auto(cid:173)

mation  system,  a  visual  device  to  take  the  place 
of  human  eyes  and  a  coordinated  mechanical 
svstem  which  can  simulate  movements  of  human 
hands  are  necessary.  Particularly,  in  machining 
and  electronic  component  industries,  which 
mainly  handle  individual  solid  materials, 
automated  control  of  production  is  extremely 
d i f f i c u l t,  comparing  with  the  situation  in  some 
other  industries  which  handle  collective  fluidic 
matters. 

L i t t le  progress  has  been  made  of  late 

especially  in  the  techniques  of  automatic 
assembly,  where  numerous  component  parts  are  to 
be  put  together  into  a  complex  unit  and  of 
product  inspection,  which  s t i ll  depends  mostly 

upon  visual  screening  by  human  eyes.  Highly 
complex  cognition  a b i l i ty  of  human  visual 
system  and  intricately  coordinated  maneuvera(cid:173)
b i l i ty  of  human  hands  have  so  far  rejected  in 
these  processes  a  complete  take-over  by 
mechanically  oriented  systems. 

It  is  obvious,  however,  that  the  future 

industrial  production  w i ll  need  more  sophisti(cid:173)
cated  automation  techniques,  and  the  realiza(cid:173)
tion  of  human  visual  cognition  and  handling 
functions  by  machines  seems  to  be  the  only 
breakthrough  of  current  automation  techniques 
that  have  already  been  saturated. 

As  the  conversion  of  production  proceeds 

tastes, 

from  the  currently  prevalent  method  of 
producing  a  mass  of  uniform  products  to  a 
method  of  producing  varied  products  in  small 
or  medium  quantities,  in  response  to  the 
increasing  diversification  of  consumers' 
the  necessity  of  the  new  automated  techniques 
will  more  and  more  increase.  Tt  seems  obvious 
in  the  future  production  processes  that 
assembly  and  inspection  functions  w i ll  have  to 
be  substituted  by  versatile  systems  with  some 
intelligence,  to  cope  with  the  diversified 
production  lines  and  the  change  of  products. 

There  are  several  attempts  to  make 

machines  capable  of  performing  visual 
perception  of  three-dimensional  objects  (l),(2), 
(3)  and  solving  the  problems  concerning  robot 
behavior  (A), (5).  Our  present  work  is  also 
one  of  these  attempts  intending  to  develop 
useful  and  versatile  robot  systems  in  future. 

2.  Outline  of  the  intelligent  robot 
The  intelligent  robot  being  described 

below  is  a  prototype  system  developed  to  aim 
at  basic  researches  in  building  up  automated 
visual  cognition  and  handling  functions  to  be 
required  in  the  future  assembly  processes. 
The  robot,  named  HIVIP,  is  composed  of  three 
nuclear  sub-systems;  FYF,  BRATN  and  HAND  (see 
Figure  1). 

The  sub-system  FYF  is  composed  of  two 

television  cameras.  Camera  1  studies  the 
macroscopic  instruction  given  in  the  form  of 
three-view  drawing  and  recognizes  the  overall 
spatial  configuration  of  the  object  shown  in 
the  drawing,  as  well  as  the  shapes,  the  number, 
the  mode  and  order  of  assembly  of  the  compo(cid:173)
nents  that  make  up  the  object.  Camera  2 
studies  actual  component  parts  separately 
but  arbitrarily  placed  on  an  assembly  table 
and  confirms  the  shapes,  the  locations,  the 
postures  and  the  dimensions  of  the  individual 
components.  When  the  camera  2  finishes 
locating  the  parts,  it  then  finds  the  specific 
parts  required  for  each  step  of  assembly 
sequence,  which  is  automatically  programed 
through  the  analysis  of  the  drawing  by  the  sub(cid:173)
system  BRAIN,  a  digital  computer.  When  these 
recognition  and  decision-making  processes  are 

Session No  8 Robots and Integrated Systems 

351 

over,  the  HAND  mechanism  is  activated  and 
starts  the  assembly  process  according  to  the 
pre-conceived  order. 

3.  Major  characteristics 

Presently  available  industrial  robots  are 
lacking  in  intellectual  analyzing  capabilities 
to  control  their  movements  in  response  to 
various  conditions  of  the  objects  or  the 
circumstantial  situation  they  are  subjected  to. 
Furthermore,  they  are  s t i ll  insufficient  in 
versatility  to  comprehend  human  instruction 
and  to  adapt  themselves  to  the  new  jobs. 
In  general,  a ll  the  instructions  of 

existing  machines  have  to  be  fed  in  microscopic 
form  with  one-to-one  correspondence  to  each 
step  of  machine  movements,  as  seen  in  numeri(cid:173)
cally  controlled  machine  tools.  The  HTVTP,  on 
the  other  hand,  accepts  macroscopic  instructions 
given  in  the  form  of  graphically  illustrated 
plans,  thoueh  they  are  s t i ll  too  simple 
comparing  with  the  actual  industrial  drawings. 
When  the  camera's  scrutinization  takes  in  a ll 
the  information  the  plans  offer,  the  computer 
starts  planning  detailed  work  procedures 
required  to  accomplish  the  instructed  assign(cid:173)
ment.  Capability  of  studying  and  understanding 
macroscopic  instructions  may  be  a  necessity  in 
future  intelligent  production  system  where 
instant  and  autonomous  response  to  changes  in 
production  procedures  is  required.  The  HTVIP 
seems  to  be  a  simple  prototype  of  this  future 
generation  robot,  as  it  is  capable  of  studying 
and  understanding  the  graphic  plans  even  they 
have  never  been  exposed  to  before  and  of  taking 
necess.qrv  actions  accordingly. 

4.  Hardware  system 

The  HIVIP  is  composed  as  shown  in  Figures 
2  and  3.  The  two  vidicon  cameras  function  as 
viewing  eyes  and  are  operated  by  a  scanning 
system  which  closely  resembles  that  of  the 
standard  TV  system.  The  image  area  is  divided 
into  76,800  picture  elements;  240  vertical  and 
320  horizontal.  The  information  of  each 
picture  element  is  transformed  into  5-bit  (32-
level)  digital  information  through  an  analog-
to-digital  converter  and  is  transmitted  into 
the  process  control  computer,  HTTAC  7250, 
which  has  32,768-word  16-bit  core  memory  and 
operates  at  2-microsecond  cycle  time  and  4.5-
microsecond  add  time. 

The  computer  can  regulate  the  TV  cameras 

to  block  off  an  area  of  optional  size  of  the 
lens  view,  permitting  image  information  being 
viewed  by  the  remaining  oblong  opening  of  the 
lens.  The  computer  can  also  instruct  the 
camera  to  regulate  the  'fineness'  of  the 
photographed  image  by  thinning  out  the  matrix 

of  the  picture  element  layout  in  three  modes. 

Mode  1  :  rough  viewing. 

Every  four  picture  elements  on  both  v e r t i(cid:173)
cal  and  horizontal  axes  are  fed  i n. 

Mode  2  :  medium  viewing. 

Every  two  picture  elements  are  fed  in. 

Mode  3  :  fine  viewing. 

All  picture  elements  are  fed  i n. 
By reducing  the  size  of  the  area  of  the 

lens  view,  the  camera  narrows  the  eye  and 
focuses  on  the  object,  rejecting  a ll  unnecessa(cid:173)
ry  objects  from  the  field  of  vision.  These 
for  instance, 
v i s i b i l i ty  control  functions, 
enable  the  camera  to  roughly  scan  the  premises 
where  the  objects  are  positioned;  then,  when 
the  object's  locations  are  confirmed,  the 
camera  narrows  i ts  eye  by  blocking  out  the  lens 
area  viewing  unnecessary  objects  in  the 
premises,  leaving  open  the  oblong  window  to 
take  in  the  particular  object  for  a  closer 
study,  and  starts  making  scrutinizing  survey  of 
the  shape,  the  position  and  the  posture  of  the 
object. 

The  v i s i b i l i ty  control  functions  can  also 
separate  the  photo  image  into  two  contrasting 
areas,  bright  and  dark,  bordering  on  a  certain 
threshold  instructed  by  the  computer,  or  can 
differentiate  the  image  to  represent  the 
objects  with  only  their  outlines. 

The  handling  mechanism  of  the  HTVTP  is  an 

articulated  arm  with  a  parallel-jaw  type 
gripping  mechanism.  Seven  degrees  of  freedom 
of  motion  in  a ll  are  equipped  in  the  handling 
mechanism  which  can  be  controlled  simultaneous(cid:173)
ly  by  seven  independent  servo-mechanisms.  Tn 
handling  control  equipment,  there  are  seven 
data  registers,  pulse  distributors  and  digital 
phase  modulators  that  act  as  digital-to-analog 
converters  and  error  detectors  in  the  servo-
mechanisms.  The  control  signals  are  compared 
with  the  feedback  sicnals  from  synchro-resolver 
type  position  detectors  in  the  arm,  and  are  fed 
into  thyristor-driven  d.c.  servo-motors. 

5.  Software  system 

The  software  of  this  robot  system  is 

controlled  by  process  or  non-process  monitoring 
system  (PMS,  NPMS)  of  HTTAC  7250.  Tt  is 
classified  into  three  major  proprams;  a  drawing 
recognition  program,  an  object  recognition 
program  and  a  handling  program. 

5.1  Drawing  recognition  algorithm 

The  i n i t i al  instruction  to  the  HTVTP  is 

given  in  the  form  of  a  rough,  macroscopic  plan 
showing  three  sides  of  the  required  assemblage 
(see  Figure  4).  The  plan  is,  at  present, 
composed  only  of  straight  solid  lines, 
accordingly  the  assemblage  must  be  a  polyhedral 
structure  composed  only  of  flat  planes. 

352 

Session No.  8  Robots and Integrated Systems 

The  robot  f i r st  scans  the  plan  and  recog(cid:173)
nizes  a ll  the  lines  before  making  up  a  l i st  of 
nodes,  branches  and  loops.  From  the  two-
dimensional  information  thus  obtained,  the 
robot,  then,  checks  the  co-relationships  among 
them,  calculates  the  spatial  points,  lines  and 
planes  and  constructs  three-dimensional 
configuration  of  the  assemblage. 

The  robot  is  also  capable  of  suspecting 

and  confirming  whether  a  termination  of  a  line 
is  actuallv  a  point  edge  of  the  assemblage  or 
just  a  false  image  looking  like  a  point.  The 
assemblage  shown  in  Figure  4  is  made  up  of  a 
grooved  block  (an  octagonal  prism)  and  two 
triangular  prisms  mounted  on  top.  The  point  A 
in  the  front  view  of  the  assemblage  is  a  false 
point  created  by  two  crossing  lines.  The 
points  B,  C,  D  and  E  shown  in  the  front  view 
are  not  seen  in  side  and  top  views.  The  robot 
is  capable  of  speculating  that  the  point  A 
might  be  a  false  point  and  the  line  FA  be 
actually  a  visible  section  of  a  real  line  FG. 
It  can  also  imagine  the  existence  of  lines 
indicated  only  by  points  such  as  B,  C,  D  or  E. 

When  it  finishes  compiling  a  l i st  of 

points,  lines  and  planes  in  three-dimensional 
space,  it  then  begins  to  figure  out  a 
structural  image  of  the  assemblage  and  in  this 
process,  eliminates  a ll  unnecessary  infor(cid:173)
mation.  When  this  is  completed,  it  breaks 
down  the  created  structural  image  into 
independent  component  parts  and  begins  to 
examine  the  way  they  are  assembled  together  to 
relay  this  information  to  the  handling  system. 
In  order  to  break  down  the  assembled  image 
into  images  of  component  parts,  it  employs 
a  principle  that  a  polygonal  plane  with 
a  certain  number  of  sides  is  linked  with  other 
planes  with  an  equivalent  number  of  arms. 
When  it  finds  that  a  line  is  co-owned  by  two 
planes, 
as  a  tight  link  and  when  the  line  is  co-owned 
by  more  than  two  planes,  the  linkage  is  termed 
as  a  loose  link.  Thus,  the  robot  disintegrates 
the  set  of  planes  into  several  subsets  or 
'shells'.  Figure  5  shows  the  way  the  assembled 
planes  are  disintegrated  into  a  number  of 
shells.  After  the  planes  are  classified  into 
several  groups,  the  robot  put  together  these 
open  shells  to  construct  closed  shells  and 
registers  them  in  the  l i st  of  parts.  The 
surfaces  to  be  co-owned  by  two  parts  are 
regarded  as  the  contact  surfaces  when  the 
assembly  is  carried  out.  Figure  6  is  the 
example  of  computer  drawings  on  X-Y  plotter 
after  the  computer  completes  the  drawing  recog(cid:173)
nition  process. 

it  classifies  the  method  of  the  linkage 

5.2  Object  recognition  algorithm 

The  object  recognition  is  performed  by 
utilizing  the  brightness  of  light  reflection 
from  each  surface  of  the  objects  on  the  table. 

The  brightness  detected  by  the  vidicon  camera 
is  at  f i r st  transformed  into  5-bit  digital 
information  and  fed  into  the  computer  which, 
in  turn,  recognizes  the  positions,  postures, 
shapes  and  the  sizes  of  the  objects.  The 
addition  of  color  identification  capability 
would  further  make  the  object  recognition 
easier.  This,  on  the  other  hand,  is  not 
alwavs  practical  in  the  actual  use,  as  the 
unusual  coloring  of  component  parts  is 
required  in  some  cases.  Since  the  environ(cid:173)
mental  situation  seems  to  be  regulated 
easily  by  simplifying  the  background  and 
adjusting  the  lighting  even  in  the  future 
actual  factory  application,  the  NTVTP  is 
equipped  with  only  brightness  identification 
capability  so  far  and  white  colored  sample 
objects  are  used  in  the  demonstration. 

The  HIVTP  can  at  present  be  applied  to 
simple  polyhedral  objects  composed  of  plain 
surfaces,  especially  polygonal  prisms  with 
the  shapes  easily  processable  with  milling 
machines. 

The  f i r st  step  of  the  object  recognition 

algorithm  is  to  spatially  differentiate  the 
image  taken  into  the  core  memory  and 
transform  it  into  a  high-contrast  image  with 
the  edge  lines  of  the  objects  strongly 
emphasized.  Then,  one  of  the  objects  in  the 
image  is  selected  by  masking  a ll  other 
objects,  and  the  position  of  each  edge  line 
of  the  object  is  detected  by  'trenching' 
method  and  memorized  in  a  l i s t.  Figure  7 
shows  the  process  of  line  detection  drawn  by 
the  line-printer  output  device.  The  lines 
thus  detected  are  sometimes  unclear  as  shown 
in  Figure  8.  Therefore  the  robot  rearranges 
them  by  carrying  out  a  few  procedure  such  as 
the  elimination  of  unnecessary  twigs.  Even 
when  some  of  the  edge  lines  are  missing, 
the  robot  is  capable  of  f i l l i ng  them  in  by 
correctly  analyzing  the  spatial  structure  of 
the  object.  The  information  thus  obtained 
finally  determines  the  object's  coordinate 
position  on  the  table  and  its  posture.  Above 
processes  are  repeated  for  every  object 
until  the  recognition  of  a ll  objects  in  the 
image  is  completed, 

5.3  Handling  algorithm 

When  the  robot  completes  the  drawing 
recognition  and  the  object  recognition,  it 
then  starts  thinking  the  assembly  procedure 
by  evaluating  the  relationships  between  the 
parts  in  the  required  assembly  form.  At 
f i r st  the  objects  on  the  table  are  classified 
into  two  categories;  the  necessary  and  un(cid:173)
necessary  objects  for  the  instructed  assembly. 
Tf  there  are  excess  parts  with  the  same  shape, 
only  the  required  number  of  parts  at  the 
easier  handling  positions  and  postures  are 
selected  and  regarded  as  the  necessary  parts. 

Session No.  8 Robots and Integrated Systems 

353 

Then,  the  assembly  order  is  determined  as 

the  reverse  of  disintegration  order  by 
computing  the  possible  disintegration  sequences 
of  the  assigned  complex  into  pieces.  Tn  general, 
there  exists  several  sequential  orders  of 
disintegration  as  seen  in  Figure  9.  Tn  this 
case,  by  evaluating  the  postures  of  the  objects, 
the  most  preferable  order  is  selected  where 
the  duplicate  manipulation  of  the  object  by 
changing  the  grasping  position  is  not  necessary, 
or  minimum  in  number. 

As  shown  in  Figure  10,  the  robot  further 
selects  two  parallelling  surfaces  of  the  parts 
for  grasping,  excluding  the  contact  surfaces 
of  the  parts.  Then,  it  determines  the  paths  of 
the  hand,  the  positions  of  assembly  and  the 
directions  of  approach  by  computing  the  vector 
sum  of  the  contact  surfaces.  When  these  deci(cid:173)
sions  are  completed,  it  then  calculates  the 
angular  displacement  of  each  joint  of  the  arm 
and  dispatches  the  determined  data  to  the 
seven  data  registers  in  response  to  the  request 
from  the  pulse  distributor  in  the  handling 
control  equipment.  Thus,  the  seven  different 
movements  of  the  articulated  handling  system 
are  regulated  independently  and  driven  simul(cid:173)
taneously  with  a  continuous-path  mode. 

When  it  is  judged  at  this  decision(cid:173)

making  stage  that  there  is  the  lack  of  parts 
on  the  table  or  that  the  hand  mechanism  is 
hard  to  reach  the  objects  physically,  the  robot 
prints  out  the  message  on  the  typewriter,  gives 
up  the  assembly  and  waits  for  a  new  instruction 
of  the  human  master. 

6.  Discussion  and  conclusion 

As  described  above,  the  HIVTP  is  a  proto(cid:173)

type  of  the  intelligent  robot  that  can  under(cid:173)
stand  the  macroscopic  instruction  given  in 
the  form  of  a  three-view  plan  and  is  capable 
of  making  necessary  decisions  to  accomplish 
the  assignment  indicated  in  the  plan. 
The 
development  efforts  of  this  robot  have  mainly 
been  focused  on  the  exploration  of  the 
processing  techniques  that  can  deal  with  the 
information  of  co-relationships  between  three-
dimensional  objects  in  contact,  as  well  as 
the  information  of  individual  objects.  The 
drawing  recognition  capability  is  especially 
a  unique  achievement  of  this  effort  and  seems 
to  be  significant  not  only  in  giving  a  machine 
an  intelligence  to  understand  a  macroscopic 
instruction  but  also  in  establishing  an  impor(cid:173)
tant  basis  of  graphic  display  technique  which 
may  develop  into  a  sophisticated  computer-
aided  designing  technology. 

As  the  major  concern  of  the  present  work 
is  the  computer  algorithms,  no  special  effort 
has  been  taken  so  far  on  the  processor  hard-

ware,  and  the  conventional  digital  control 
computer  has  been  utilized.  The  entire 
software  system  of  the  robot,  including  the 
operating  system  software  and  the  working  area 
for  visual  images,  exceeds  400  kilowords  of 
memory,  and  is  stored  in  512-kiloword  magnetic 
drum  memory  as  chain  jobs. 
In  addition,  more 
than  ninety  percents  of  programs  are  written 
in  FORTRAN  of  this  particular  computer  whose 
speed  no  longer  seems  to  be  high  enough. 
These  are  the  main  reasons  of  takinp  much 
computation  time  as  240  seconds  for  image 
processing,  20  seconds  for  drawing  recognition, 
50  seconds  for  each  object  recognition,  10 
seconds  for  decision-making  and  180  seconds 
for  assembly  movements  on  the  average. 

Tt  may  be  possible  in  this  case  to  reduce 
the  time  to  one-tenth  or  even  to  one-twentieth 
by  refining  the  program,  yet  the  development 
of  low-cost  processors  with  totally  new 
concepts,  which  may  have  at  least  the  parallel 
processing  capability,  seems  to  be  essential 
for  the  solution  in  future  industrial  appli(cid:173)
cation.  Thus,  our  present  work  rather  posed 
the  questions  that  must  be  answered  in  future, 
and  the  quantitative  and  qualitative  improve(cid:173)
ment  of  recognizable  objects  is  one  of  the 
requisites  that  we  should  study  next  for 
intensifying  the  problem-solving  ability  and 
approaching  to  the  intelligent  systems. 

References 

(1)  L.G.Roberts,  "Machine  Perception  of  Three-
Dimensional  Solids",  Optical  and  Electro-
optical  Information  Processing  (MTT  Press, 
Cambridge,  Massachusetts,  196.5). 

(2)  G.E.Forsen,  "Processing  Visual  Data  with 

an  Automaton  Fye",  Pictorial  Pattern 
Recognition  (Thompson  Book  Company, 
Washington,  D.C.,  1968). 

(3)  A.Guzman,  "Decomposition  of  a  Visual  Scene 
into  Three-Dimensional  Bodies",  Proc.  FJCC, 
p.  291-304,  1968. 

(4)  J.McCarthy,  L.Earnest,  D.Reddy  &  P.Vicens, 

"A  Computer  with  Hands,  Eyes  and  Ears", 
Proc.  FJCC,  p.  329-338,  1968. 

(5)  N.J.Nilsson,  "A  Mobile  Automaton:  An  Appli(cid:173)

cation  of  A r t i f i c i al  Intelligence  Tech-
niques'',  Proc.  1st  International  Joint 
Conference  on  A r t i f i c i al  Intelligence, 
p.  509-520,  May  1969. 

354 

Session No.  8 Robots and Integrated Systems 

Session No  8 Robots and Integrated Systems 

355 

356 

Session No.  8 Robots and Integrated Systems 

Session No.  8 Robots and Integrated Systems 

357 

358 

Session No.  8 Robots and Integrated Systems 


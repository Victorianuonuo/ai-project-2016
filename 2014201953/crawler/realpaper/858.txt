Continuous Time Associative Bandit Problems

∗

Andr´as Gy¨orgy1 and Levente Kocsis1 and Ivett Szab´o1,2 and Csaba Szepesv´ari1,3

1Computer and Automation Research Institute of the Hungarian Academy

of Sciences, Machine Learning Research Group

gya@szit.bme.hu, kocsis@sztaki.hu, ivett7@gmail.hu, szcsaba@sztaki.hu

2 Budapest University of Technology and Economics, Department of Stochastics

3University of Alberta, Department of Computing Science

Abstract

In this paper we consider an extension of the multi-
armed bandit problem. In this generalized setting,
the decision maker receives some side information,
performs an action chosen from a ﬁnite set and then
receives a reward. Unlike in the standard bandit
settings, performing an action takes a random pe-
riod of time. The environment is assumed to be sta-
tionary, stochastic and memoryless. The goal is to
maximize the average reward received in one unit
time, that is, to maximize the average rate of return.
We consider the on-line learning problem where
the decision maker initially does not know anything
about the environment but must learn about it by
trial and error. We propose an “upper conﬁdence
bound”-style algorithm that exploits the structure
of the problem. We show that the regret of this al-
gorithm relative to the optimal algorithm that has
perfect knowledge about the problem grows at the
optimal logarithmic rate in the number of decisions
and scales polynomially with the parameters of the
problem.

1 Introduction
Multi-armed bandit problems ﬁnd applications in various
ﬁelds, such as statistics, control, learning theory or eco-
nomics. They became popular with the seminal paper by
Robbins [1952] and since then they enjoy perpetual popular-
ity.

The version of the bandit problem we consider here is mo-
tivated by the following example: Imagine that a sequence
of tasks arrive for processing in a computer center that has
a single supercomputer. For each of the tasks a number of
alternative algorithms can be applied to. Some information
about the tasks is available that can be used to predict which
of the algorithms to try. The processing time depends on the
task at hand and also on the algorithm selected and may take

∗This research was supported in part by the Ministry of Econ-
omy and Transport, Hungary (Grant No. GVOP-3.1.1.-2004-05-
0145/3.0), the Hungarian National Science Foundation (Grant No.
T047193), and by the J´anos Bolyai Research Scholarship of the
Hungarian Academy of Sciences.

continuous values, hence the time instants when the decisions
can take place take continuous values, too. The supercom-
puter has a ﬁxed cost of running, whilst the centre’s income
is based on the quality of solutions delivered. At any given
time only a single task can be executed on the supercomputer.
Admittedly, this assumption looks absurd at the ﬁrst sight in
the context of our example, however, we think that our results
can be extended to the more general case when the number of
algorithms that can run simultaneously is bounded by a con-
stant without much trouble. Hence we decided to stick to this
simplifying assumption.

An allocation rule decides, based on the side information
available about the task just received, which algorithm to use
for processing it, the goal being to maximize the return rate.
Note that this criterion is different from maximizing the total
reward. In fact, since processing a task takes some time dur-
ing which no other tasks can be processed, the rate maximiza-
tion problem cannot be solved by selecting the algorithm with
the highest expected payoff: Some tasks may look so difﬁcult
to solve that the best thing could be to drop them, which re-
sults in no payoff, but in exchange the learner does not suffer
any loss due to not processing other, possibly more rewarding
tasks. (Note that this would not be possible without the pres-
ence of side information; in the latter case the problem would
simplify to the usual multi-armed bandit problem where one
needs to ﬁnd the best option with highest reward rate.) This
example illustrates that a learner whose aim is to quickly learn
a good allocation strategy for rate maximization must solve
two problems simultaneously: Predicting the long-term val-
ues of the available algorithms given the information about
the task to be processed and balancing exploration and ex-
ploitation so that the loss due to selecting inferior options
(i.e., the regret) is kept at minimum. The problem we con-
sider can be thought of as a minimalistic example where the
learner faces these two problems simultaneously.

Bandit problems in continuous time have been studied ear-
[Kaspi and Mandel-
lier by a number of authors (see e.g.
baum, 1998; Karoui and Karatzas, 1997] and the references
therein). These earlier results concern the construction of op-
timal allocation policies (typically in the form of Gittins in-
dexes) given some parametric form of the distributions of the
random variables involved. In contrast, here we consider the
agnostic case when no particular parametric form is assumed,

IJCAI-07

830

but the environment is supposed to be stationary and stochas-
tic. The agnostic (or non-parametric) case has been studied
extensively in the discrete time case.
In fact, this problem
was ﬁrst considered by Robbins [1952], who introduced a
certainty-equivalence rule with forcing. In the same article
Robbins showed that this rule is asymptotically consistent in
the sense that the frequency of the time instants when the best
arm is selected converges to one almost surely. More recently,
Agrawal [1995] suggested a number of simple sample-mean
based policies and showed that the resulting policies’ regret
after n decisions is O(log n). Since it is known that no al-
location rule can achieve regret lower than Cp log n for an
appropriate (problem dependent) constant Cp [Lai and Rob-
bins, 1985], Agrawals’ policies are unimprovable apart from
[2002] strengthened
constant factors. Lately, Auer et al.
these results by suggesting policies that achieve logarithmic
regret uniformly over time, rather than just asymptotically.
An added beneﬁt of their policies is that they are simple to
implement.

We base our algorithm on algorithm UCB1 from [Auer et
al., 2002] (see also [Agrawal, 1995]). We assume a stationary
memoryless stochastic environment, where the side informa-
tion is an i.i.d. process taking values in a ﬁnite set, the payoff-
delay sequences are jointly distributed for any of the options
and their distribution depends on the side information (the
precise assumptions will be listed in the next section). Like
UCB1, our algorithm decides which option to choose based
on sample-means corrected by upper conﬁdence bounds. In
our case, however, separate statistics are kept for all option -
side-information pairs. Our main result shows that the result-
ing policy achieves logarithmic regret uniformly in time and
hence it is also unimprovable, apart from constant factors.

The paper is organized as follows: We deﬁne the problem
and the proposed algorithm in Section 2. Our main result, a
logarithmic regret bound on the algorithm’s performance is
presented in Section 3. Conclusions are drawn in Section 4.

2 The algorithm

The problem of the previous section is formalized as fol-
lows: Let K denote the number of options available, and let
X denote the set of possible values of the side information,
which is assumed to be ﬁnite. Let x1, x2, . . . , xt be a ran-
dom sequence of covariates representing the side information
available at the time of the t-th decision, generated indepen-
dently from a distribution p supported on X . At each deci-
sion point the decision maker may select an option It from
A = {1, . . . , K} and receives reward rt = rIt ,t(xt), where
rit(xt) is the reward the decision maker would have received
had it chosen option i. Unlike in classical bandit problems the
collection of the reward takes some random time. When op-
tion i is selected and the side information equals x, this time is
δit(x). We assume that for any ﬁxed x and i, (rit(x), δit(x))
is an i.i.d. sequence, independent of {xt}. We further as-
sume that rit(x) ∈ [rmin, rmax], δit(x) ∈ [δmin, δmax] with
δmin > 0. (We expect that the boundedness assumptions can
be relaxed to δit(x) ≥ 0 and appropriate moment conditions
on δit(x) and rit(x).) Let

ri(x) = E [ri1(x)]

and

δi(x) = E [δi1(x)]

denote the expected reward and delay, respectively, when op-
tion i is chosen at the presence of the side-information x.

The exact protocol of decision making is as follows: De-
cision making happens in discrete trials. Let τ0 = 0 and
let τt denote the time of the beginning of the t-th trial. At
the beginning of the tth trial the decision maker receives the
side information xt. Based on the value of xt and all infor-
mation received by the decision maker at prior trials, the de-
cision maker must select an option It. Upon executing It,
the decision maker receives a reward rt = rIt,t(xt) and suf-
fers a delay δt = δIt,t(xt). That is, the next time point
available when the decision maker can select an option is
τt+1 = τt + δIt,t(xt).

The goal of the decision maker is to ﬁnd a good allocation
policy. Formally, an allocation policy maps possible histories

to some index in the set A. The gain (average reward rate)
delivered by an allocation policy u is given by

E [
E [

λu = lim sup
n→∞

t=1 ru
t ]
,
t=1 δu
t ]
where {ru
t } is the reward sequence and {δu
t } is the delay se-
quence experienced when policy u is used. An optimal allo-
cation policy is one that maximizes this gain. Note that the
problem as stated is a special case of semi-Markov decision
problems [Puterman, 1994]. The theory of semi-Markov de-
cision problems furnishes us with the necessary tools to char-
acterize optimal allocation policies: Let us deﬁne the optimal
gain by

(cid:2)n
(cid:2)n

λ∗ = sup

λu.

u

A policy u is said to be optimal if it satisﬁes λ∗ = λu. It
follows from the generic theory that there exist determinis-
tic stationary policies that are optimal. An optimal action for

some x ∈ X can be determined by ordering the options by
their relative values. The relative value of option i upon ob-
serving x is the expected reward that can be collected minus
the expected reward that is not gained during the time it takes
to collect the reward:
i (x) = ri(x) − δi(x)λ∗.
q∗

Intuitively it should be clear that a policy that always selects
options with best relative values should optimize the over-
all gain. In fact, it follows from the theory of semi-Markov
decision problems that this is indeed the case. A stationary

deterministic policy u : X → A is optimal if and only if it

obeys the constraints

ru(x)(x) − δu(x)(x)λ∗ = max

[ri(x) − δi(x)λ∗]

(1)

i∈A

simultaneously for all x ∈ X .

The (total) regret of an allocation policy is deﬁned as the
loss suffered due to not selecting an optimal option in each
time step. Since we are interested in the expected regret only,
our regret deﬁnition uses the optimal gain λ∗

:

n(cid:3)

δt − n(cid:3)

t=1

t=1

rt.

Rn = λ∗

IJCAI-07

831

The value of the ﬁrst term is the maximum reward that could
be collected during the time of the ﬁrst n decisions. The ex-
pected regret thus compares the expected value of the latter
with the expected value of the actual total payoffs received.
It follows that an allocation policy that minimizes the regret
will optimize the rate of return.

When δit(x) = 1, and X has a single element, the problem

reduces to the classical stochastic bandit problem. Since for
the stochastic bandit problems the regret is lower bounded by
O(log n), we are seeking policies whose regret grows at most
at a logarithmic rate.

The idea underlying our algorithm is to develop upper
estimates of the values q∗
i (x) with appropriate conﬁdence
Just like in [Auer et al., 2002], the upper conﬁ-
bounds.
dence estimates are selected to ensure that for any given x
with p(x) > 0 all options are ultimately selected inﬁnitely
often, but at the same time suboptimal options are selected
increasingly rarely.

The algorithm is as follows: Let us consider the t-th deci-
sion. If we had a good estimate λt of λ∗
, then for any given
x we could base our decision on the estimates of the relative
i (x) of the options given by rit(x) − δit(x)λt. Here
values q∗
rit(x) denotes the average of rewards during the ﬁrst n deci-
sions for those time points when the side information is x and
option i was selected, and δit(x) is deﬁned analogously:

t(cid:3)
t(cid:3)

s=1

1

Ti(x, t)

1

Ti(x, t)

s=1

rit(x) =

δit(x) =

I (Is = i, xs = x) rs,

I (Is = i, xs = x) δs,

where Ti(x, t) denotes the number of times option i was
selected when side information x was present
in trials
1, 2, . . . , t:

t(cid:3)

Ti(x, t) =

I (It = i, xt = x) .

j=1

The plan is to combine appropriate upper bounds on ri(x)
and lower bounds on δi(x) based on the respective sample
averages rit(x), δit(x) and Ti(x, t), to obtain an upper es-
timate of q∗
i (x). However, in order to have a sample based
estimate, we also need an appropriate lower estimate of λ∗
.
This estimate is deﬁned as follows:
Let U denote the set of stationary policies: U = {u|u :

X → A}. Pick any u ∈ U . Let λ
estimate of the gain of policy u:

u
t denote the empirical

(cid:2)t
(cid:2)t

u
t =

λ

s=1 I (Is = u(xs)) rs
s=1 I (Is = u(xs)) δs

and let Tu(t) denote the number of times when an option
‘compatible’ with policy u was selected:

t(cid:3)

Tu(t) =

I (Is = u(xs)) .

Here ct,s is an appropriate deterministic sequence that is se-
lected such that simultaneously for all policies u ∈ U , λ
u
t is
in the ct,Tu(t)-vicinity of λu with high probability. This se-
quence will be explicitly constructed during the proof where
we will also make sure that it depends on known quantities
only. In words, λn is the optimal gain that the decision maker
can guarantee itself with high probability given the data seen
so far.

Our proposed allocation policy, {uUCB

}, selects the op-

tions It = uUCB

t

(xt) by the rule

t

(cid:4)
rit(x) − δit(x)λt + ˆct,Ti(x,t)

(cid:5)

,

UCB
t

u

(x) = argmax

i∈A

where, similarly to ct,s, ˆct,s is an appropriate deterministic
sequence that will be chosen later.

3 Main result
Our main result is the following bound on the expected regret:
Theorem 1 Let the assumptions of the previous section hold
(cid:6)(cid:7)
on rit, δit, xt. Let Rn be the n-step regret of policy uUCB
.
Then, for all n ≥ 1,
E [Rn] ≤ L∗

(cid:8)

2π2

2 +

(cid:3)

3(|U| + 1)2
(cid:3)

a log(n(

K|X| + 2K|X| log(n)
(cid:9)
|U| + 1))

(cid:10)

+

Δi(x)2

,

i:Δi>0

x∈X

where L∗ = δmaxλ∗ − rmin,
j (x) − q∗
q∗

Δi(x) = max
j∈A

i (x) ≥ 0,

i = 1, . . . , K,

and the positive constant a is given by (7) in the proof of the
theorem.
The proof follows similar lines to that of Theorem 1 of [Auer
et al., 2002], with the main difference being that now we have
to handle the estimation error of λ∗
. We prove the theorem
using a series of propositions.

The ﬁrst proposition bounds the expected regret in terms of
the number of times when some suboptimal option is chosen:
Proposition 2 The following bound holds for the expected
regret of an arbitrary policy, u = (u1, u2, . . .):
E [Rn] ≤

I (ut(x) (cid:7)∈ U ∗(x))

p(x)L∗(x)E

(cid:3)

n(cid:3)

(cid:12)

(cid:11)

(2)

,

x∈X

t=1

where

U ∗(x) = {i ∈ A| q∗

i (x) = max
j∈A

j (x)}
q∗

j

denotes the set of optimal options at x, and

L∗(x) = max

is the loss for the worst choice at x. Further, by L∗(x) ≤ L∗
E [Rn] ≤ L∗
(cid:12)

I (ut(x) (cid:7)∈ U ∗(x))

(δj(x)λ∗ − rj (x))
(cid:11)
n(cid:3)
n(cid:3)

(cid:3)
(cid:3)

p(x)E

(cid:12)

(cid:11)

x∈X

t=1

,

I (ut(x) (cid:7)∈ U ∗(x), xt = x)

.

E

x∈X

t=1

s=1
Then λt, the estimate of λ∗

is deﬁned by

u

t − ct,Tu(t)).

λt = max
u∈U

(λ

= L∗

IJCAI-07

832

Proof.
of

Let us consider the t-th term, E [δtλ∗ − rt],
(cid:2)
the expected regret. We have E [δtλ∗ − rt] =
i∈A E [(δtλ∗ − rt)I (It = i)] . Using It = ut(xt) and that
ut depends only on the past, i.e., if Ft is the sigma algebra of
x1, r1, δ1, . . . , xt, rt, δt then It = i is Ft−1 measurable, we

get that

E [(δtλ∗ − rt)I (It = i)]

= E [(δi,t(xt)λ∗ − ri,t(xt))I (It = i)]
= E [E [(δi,t(xt)λ∗ − ri,t(xt))I (It = i)|Ft−1, xt]]
= E [I (It = i) E [(δi,t(xt)λ∗ − ri,t(xt))|Ft−1, xt]]
= E [I (It = i) E [(δi(xt)λ∗ − ri(xt))|Ft−1, xt]]
= E [I (It = i) (δi(xt)λ∗ − ri(xt))] .

Now, using again that ut does not depend on xt, we get

(cid:3)
E [δtλ∗ − rt]
E [I (ut(xt) = i) (δi(xt)λ∗ − ri(xt))]
(cid:3)
(cid:3)
(cid:3)
(cid:3)

i (x)E [I (ut(x) = i)|xt = x]

p(x)q∗

x∈X

i∈A

i∈A

=
= −
= −

p(x)q∗

i (x)E [I (ut(x) = i)] .

i∈A

x∈X

Then

E [δtλ∗ − rt] = −

(cid:3)
(cid:3)

x∈X

(cid:3)
(cid:3)

i∈U ∗(x)

p(x)

p(x)

x∈X

i(cid:6)∈U ∗(x)

−

q∗
i (x)E [I (ut(x) = i)]

q∗
i (x)E [I (ut(x) = i)] .

Then

P

(cid:2)

Let wt(i|x) = E [I (ut(x) = i)] if i ∈ U ∗(x) and wt(i|x) = 0
otherwise, and let μt(i|x) = wt(i|x)/
j∈A wt(j|x). Then
j∈A wt(j|x) ≤ 1), the ﬁrst term
μt(i|x) ≥ wt(i|x) (since
(cid:3)
(cid:3)

of the last expression can be upper bounded by

(cid:2)

vt = −

p(x)

x∈X

i

i (x)μt(i|x).
q∗

Since μt(i|x) = 0 if i is not optimal, μt deﬁnes an optimal
(cid:3)
(stochastic) policy and hence, Bellman’s equation gives vt =
0. Therefore,
E [δtλ∗ − rt] ≤ −
(cid:3)
(cid:3)

q∗
i (x)E [I (ut(x) = i)]

E [I (ut(x) = i)]

p(x)L∗(x)

(cid:3)

(cid:3)

i(cid:6)∈U ∗(x)

i(cid:6)∈U ∗(x)

p(x)

≤

x∈X

x∈X

p(x)L∗(x)E [I (ut(x) (cid:7)∈ U ∗(x))] .

=

x∈X

Summing up this last expression over t gives the advertised
(cid:8)(cid:9)
bound.

The next statements are used to prove that with high prob-
. Here and in what follows

ability λt is a good estimate of λ∗
u∗

∗
u∗
denotes an arbitrary (ﬁxed) optimal policy and λ
t = λ
t

(3)

(4)

(5)

Proposition 3 Assume that the following conditions are sat-
isﬁed:

λu ≥ λ
λ∗ ≤ λ

u

t − ct,Tu(t),
∗
t + ct,Tu∗ (t).

where the ﬁrst condition is meant to hold for all stationary

policies u ∈ U . Then

λ∗ ≥ λt ≥ λ∗ − 2ct,Tu∗ (t).

u

u

, we get that λt = λ

be the policy that maximizes λ

t − ct,Tu(t).
Proof. Let u(cid:7)
≤
Since (3) holds for u(cid:7)
λu(cid:3) ≤ λ∗, proving the upper bound for λt. On the other hand,
t − ct,Tu∗ (t) which can be
because of the choice of u(cid:7)
further lower bounded by λ∗ − 2ct,Tu∗(t) using (4), proving
(cid:8)(cid:9)
the lower bound for λt.
The following proposition shows that λt is indeed a lower

t − ct,T

, λt ≥ λ

u(cid:3) (t)

∗

bound for λ∗
Proposition 4 Let

(cid:13)

with high probability.

(cid:9)
|U| + 1)

2c1 log(t

ct,s =

(cid:14)

where

c1 = 2 max

(rmax − rmin)2
(cid:17)

(cid:16)
λt < λ∗ − 2ct,Tu∗ (t)

δ2
min

,

(cid:15)

.

s
max(δmax − δmin)2
r2
(cid:16)
(cid:17) ≤ 2

δ4
min

λ∗ < λt

.

+ P

t

Proof. According to Proposition 3, if (3) holds for all station-

to hold, we
must have that one of the conditions in Proposition 3 is vio-
lated. Using a union bound we get

ary policies u and if (4) holds then λ∗ ≥ λt ≥ λ∗−2ct,Tu∗ (t).
Hence, in order λt < λ∗ − 2ct,Tu∗ (t) or λt > λ∗
(cid:16)
(cid:3)
λt < λ∗ − 2ct,Tu∗ (t)
≤
(cid:18)

(cid:16)
(cid:17)
t − ct,Tu(t)
(cid:18)
(cid:19)
t(cid:3)

Fix u. By the law of total probability,

∗
λ∗ < λ
t + ct,Tu(t)

λ∗ < λt

λu < λ

(cid:17)
(cid:18)

(cid:18)

(cid:19)

(cid:19)

(cid:19)

+ P

+ P

P

P

u

u

=

P

s=1

λu < λ

.

.

u

t −ct,Tu(t)
t(cid:3)
t(cid:3)

s=1

P

λu < λ

Deﬁne

ˆru
t =

ˆδu
t =

I (Is = u(xs)) rs,

u

t −cts, Tu(t) = s
(cid:3)
(cid:3)

p(x)ru(x)(x)

x∈X

p(x)δu(x)(x).

x∈X

ru =

δu =

I (Is = u(xs)) δs,

Using elementary algebra, we get that

s=1

(cid:18)
λu < λ
≤ P (ctsδmin/2 ≤ ˆru

t − cts, Tu(t) = s
(cid:18)

u

(cid:19)

P

t /s − ru, Tu(t) = s)

2

min/rmax ≤ δu − ˆδu

t /s, Tu(t) = s

(cid:19)

.

.

+P

ctsδ

IJCAI-07

833

t and ˆδu

Exploiting that ˆru
t are martingale sequences and re-
sorting to a slight variant of the Hoeffding-Azuma bound
(see, e.g. [Devroye et al., 1996]), we get the bound 2/(|U| +
1)t−2
. Summing over s and u and by an analogous argument
(cid:8)(cid:9)
∗
λ∗ < λ
t + ct,Tu(t)
for P

, we get the desired bound.

(cid:18)

(cid:19)

In the

. For example, r∗

Now we are ready to prove the main theorem.

proof we put a superscript ‘∗’ to any quantity that refers to
the optimal policy u∗
t (x) = ru∗(x),t(x),
δ∗
t (x) = δu∗(x),t(x), T ∗(x, t) = Tu∗(x)(x, t), etc.
Proof of Theorem 1. Proposition 2 applied to uUCB
that it sufﬁces if for any ﬁxed x ∈ X and suboptimal choice
i (cid:7)∈ U ∗(x) we derive an O(log n) upper bound on the ex-
pected number of times choice i would be selected by uUCB
when the side information is x. That is, we need to show

shows

UCB
t

(x) = i, xt = x

≤ O(log n).

(6)

(cid:11)

n(cid:3)

(cid:16)

E

I

u

t=1

(cid:17)(cid:12)

, if uUCB

uUCB
t
q∗
t (x) + ˆct,T ∗(x,t). Hence, for any integer A(n, x),

Let qit(x) = rit(x) − δit(x)λt. Using the deﬁnition of
(x) = i holds then qit(x) + ˆct,Ti(x,t) >
n(cid:3)
(cid:16)

t

(cid:17) ≤ A(n, x)

(x) = i

(cid:17)
(x) = i, Ti(x, t − 1) ≥ A(n, x), xt = x
(cid:17)
(cid:16)
(x) = i, Ti(x, t − 1) ≥ A(n, x)

UCB
t

u

I

(cid:18)

Zt(s, s(cid:7))I

The expectations of the second two terms will be bounded
by Proposition 4. The ﬁrst term, multiplied by Zt(s, s(cid:7)) is
bounded by

ri,t−1(x) − δi,t−1(x)λ∗ + ˆct−1,s(cid:3)

t−1(x) − δ

> r∗

∗

t−1(x)(λ∗ − 2ct,s) + ˆct−1,s

(cid:19)

.

When this expression equals one then at least one of the fol-
lowing events hold:

∗

t−1,s, Zt(s, s(cid:7)) =1},

t−1(x)λ∗≤ r∗(x)−δ∗(x)λ∗−c(cid:7)

At,s,s(cid:3) =
t−1(x)−δ
{r∗
Bt,s,s(cid:3) =
{ri,t−1(x)−δi,t−1(x)λ∗≥ri(x)−δi(x)λ∗+ˆct−1,s(cid:3), Zt(s, s(cid:7)) =1},
Ct,s,s(cid:3) = {r∗(x) − δ∗(x)λ∗ < ri(x) − δi(x)λ∗ + 2ˆct,s(cid:3)}.
∗
Here c(cid:7)
t ct−1,s. Now let us give the choices
for the conﬁdence intervals. Deﬁne

t−1,s = ˆct−1,s−2δ
(cid:20)

(cid:18)

(cid:19)

(cid:9)
|U| + 1

/s.

uts =

log

t

√
We have already deﬁned cts in Proposition 4: cts =
2c1uts,
where c1 was deﬁned there, too. We deﬁne ˆcts implicitly,
through a deﬁnition of c(cid:7)
ts which is deﬁned so as to keep the
probability of At,s,s(cid:3) small: Let

(cid:21)
8 max{(rmax − rmin)2, r2

max(δmax − δmin)/δ2

min},

a0 =

(cid:9)

c(cid:7)
ts = a0uts. and a1 =

2δ2

maxc1. Deﬁne

.

2
a = (a0 + a1)

,

(7)

∗

(cid:19)

and ˆcts = (a0 + a1)uts. Using these deﬁnitions we bound the
probabilities of the above three events. We start with At,s,s(cid:3) :
(cid:18)
ts/2 ≤ r∗(x) − r∗
P (At,s,s(cid:3) )≤ P (c(cid:7)
ts/(2λ∗) ≤ δ
c(cid:7)
+P
≤ exp
+ exp

(cid:17)
t (x) − δ∗(x), Zt(s, s(cid:7)) = 1
(cid:2)t
(δmax − δmin)

ts s/(2(rmax − rmin)
2
ts s/(2(λ∗)

(cid:16)−c(cid:7)2
(cid:16)−c(cid:7)2

t (x), Zt(s, s(cid:7)) = 1)

s=1 I (It = i, xt = x) rt,
s=1 I (It = i, xt = x) δt are martingales for any x, i,
and the above-mentioned variant of the Hoeffding-Azuma
inequality. Plugging in the deﬁnition of c(cid:7)
ts we get that the
probability of event At,s,s(cid:3) is bounded by 2t−4(|U| + 1)−2.
The probability of Bt,s,s(cid:3) can be bounded in the same way
and by the same expression since ˆcts > c(cid:7)

ts. Therefore

(cid:2)t

Here

used

(cid:17)

that

we

)

)

2

2

(cid:3)
n(cid:3)
≤ n(cid:3)

t=1

(s,s(cid:3))∈H(t)

(cid:3)

[P (At,s,s(cid:3) ) + P (Bt,s,s(cid:3) )]

4

2π2

t=1

t4(|U| + 1)2

3(|U| + 1)2 .
|U| + 1))/Δi(x)2
.
Now, if Ct,s,s(cid:3) holds then one must have Δi(x) > 2ˆct,s(cid:3) ,

Moreover, deﬁne A(t, x) = a log(t(

(s,s(cid:3))∈H(t)

≤
(cid:9)

IJCAI-07

834

UCB
t

n(cid:3)

t=1

I

t=1

UCB
t

u

n(cid:3)

(cid:16)

+

I

u

t=1

≤ A(n, x) +
(cid:16)

I

u

UCB
t

= I

I

=
(s,s(cid:3))∈H(t)

where

We write the t-th term in the last sum as follows:

(cid:17)
(cid:18)
(x) = i, Ti(x, t − 1) ≥ A(n)
qi,t−1(x) + ˆct,Ti(x,t−1) > q∗
(cid:3)

(cid:19)
Ti(x, t − 1) ≥ A(n)

(cid:16)

qi,t−1(x) + ˆct−1,s(cid:3)>q∗

t−1(x) + ˆct−1,s

t−1(x) + ˆct−1,T ∗(x,t−1),

(cid:17)

Zt(s, s(cid:7)),

H(t) = {(s, s(cid:7))|1 ≤ s ≤ t − 1, A(t) ≤ s(cid:7) ≤ t − 1},

I

Zt(s, s(cid:7)) = I (Ti(x, t − 1) = s(cid:7), T ∗(x, t − 1) = s) .
(cid:16)
Fix any s, s(cid:7) ∈ H(t). Using the deﬁnition of qit(x),
(cid:18)
qi,t−1(x) + ˆct−1,s(cid:3) > q∗
≤ I

(cid:17)
ri,t−1(x) − δi,t−1(x)λt−1 + ˆct−1,s(cid:3)
(cid:19)
(cid:16)

(cid:16)
(cid:17)
λ∗ ≥ λt−1 ≥ λ∗ − 2ct−1,Tu∗ (t−1)
λt−1 < λ∗ − 2ct−1,Tu∗ (t−1)

∗
t−1(x)λt−1 + ˆct−1,s,

t−1(x) − δ

t−1(x) + ˆct−1,s

> r∗

+ I

+I

λ∗ < λt−1

(cid:17)

.

(cid:17)(cid:12)

where s(cid:7) ≥ A(t, x). The above choice makes s(cid:7)
large enough
(cid:11)
so that Δi(x) > 2ˆct,s(cid:3) cannot hold. Hence P (Ct,s,s(cid:3) ) = 0.
n(cid:3)
Gathering all the terms, we have
≤ n(cid:3)

(cid:16)
(cid:16)
λt−1 < λ∗ − 2ct−1,Tu∗ (t−1)
n(cid:3)

(x) = i, xt = x

λ∗ < λt−1

I

UCB
t

u

(cid:3)

+ P

(cid:17)

E

(cid:17)

(cid:16)

t=1

P

t=1

+

[P (At,s,s(cid:3) ) + P (Bt,s,s(cid:3) )] + A(n, x)

t=1

(s,s(cid:3))∈H(t)

≤ 2(log(n)+1)+

a log

2π2

3(|U|+1)2 +

(cid:18)

(cid:19)
(cid:9)
|U|+1)

n(
Δi(x)2

.

This ﬁnishes the proof of (6) and hence, by Proposition 2 we
(cid:8)(cid:9)
get the desired bound, (2).

4 Conclusions and further work

We considered a generalization of the multi-armed bandit
problem, where performing an action (or collecting the re-
ward) takes a random amount of time. The goal of the de-
cision maker is to maximize the reward per unit time where
in each time step some side information is received before
the decision is made. In this setting one needs to consider
seriously the time needed to perform an action, since spend-
ing long times with less rewarding actions seriously limits the
performance of any algorithm in a given time period. There-
fore, efﬁcient methods must predict simultaneously the ex-
pected rewards and durations of all actions, as well as to esti-
mate the long term optimal performance. The latter is essen-
tial as each action has a hidden cost associated with it: since
actions take time, for their correct evaluation their immedi-
ate payoffs must be decremented by the optimal reward lost
during the time it takes to execute the action.

In this paper we proposed an algorithm to solve this prob-
lem, whose cumulative reward after performing n actions is
only O(log n) less than that of the best policy in hindsight.
The algorithm is based on the upper conﬁdence bound idea
of Auer et al. [2002]. Our algorithm, however, extends their
UCB1 algorithm proposed for the multi-armed bandit prob-
lem in two ways. First of all, it estimates the long term max-
imum reward per unit time. For this we proposed to adopt a
maximin approach: The estimate was chosen to be the opti-
mal gain that can be guaranteed in the worst-case, with high
probability, given all the data seen so far. Moreover, utilizing
the structure of the problem the algorithm chooses its actions
based on the sufﬁcient statistics of the problem instead of con-
sidering each policy separately. Note that doing so would
lead to a constant factor in the regret bound that grows lin-
early with the number of possible policies, i.e., exponentially
in the size of the problem. On the other hand, because of the
specialized form of our algorithm, the constants in our bound
depend only polynomially on these parameters. However, we
expect that the explicit dependence of the bound on the num-
ber of possible side information values can be relaxed. Note
however, that we have not attempted any optimization of the

actual constants that appear in our bounds. Therefore, we ex-
pect that our constants can be improved easily.

One problem with the algorithm as presented is that it
needs to enumerate all the policies in order to compute the
estimate of the optimal gain. However, we would like to note
that the problem of computing this quantity is very similar
to computing the value of minimax Markov games. In fact,
the actual deﬁnition of δt is not that important: Any estimate
that satisﬁes the conclusion of Proposition 4 would do. We
speculate that since efﬁcient methods are available for cer-
tain minimax Markov games (cf. [Szepesv´ari and Littman,
1999]), game theoretic techniques might yield an algorithm
that not only utilizes the available information effectively, but
is also computationally efﬁcient.

In the present work we restricted ourselves to the case
when the side information is allowed to take values only in
a ﬁnite set. Assuming appropriate smoothness conditions on
the reward and delay functions, it seems possible to extend
the algorithm to the case of continuous valued side informa-
tion. The extension of the algorithm presented seems possi-
ble to certain semi-Markov models when there is a state that
is recurrent under all stationary policies. Another interest-
ing avenue for further research is to consider continuous time
bandit problems in non-stochastic environments.

References
[Agrawal, 1995] R. Agrawal. Sample mean based index poli-
cies with o(logn) regret for the multi-armed bandit prob-
lem. Adv. in Appl. Probability, 27:1054–1078, 1995.

[Auer et al., 2002] P. Auer, N. Cesa-Bianchi, and P. Fischer.
Finite time analysis of the multiarmed bandit problem.
Machine Learning, 47(2-3):235–256, 2002.

[Devroye et al., 1996] L. Devroye, L. Gy¨orﬁ, and G. Lugosi.
A Probabilistic Theory of Pattern Recognition. Springer-
Verlag New York, 1996.

[Karoui and Karatzas, 1997] N. El Karoui and I. Karatzas.
Synchronization and optimality for multi-armed bandit
problems in continuous time. Computational and Applied
Mathematics, 16:117–152, 1997.

[Kaspi and Mandelbaum, 1998] H. Kaspi and A. Mandel-
baum. Multi armed bandits in discrete and continuous
time. Ann. of Appl. Probabability, 8:1270–1290, 1998.

[Lai and Robbins, 1985] T. L. Lai and H. Robbins. Asymp-
totically efﬁcient adaptive allocation rules. Advances in
Applied Mathematics, 6:4–22, 1985.

[Puterman, 1994] M.L. Puterman.

Markov Decision
Processes — Discrete Stochastic Dynamic Programming.
John Wiley & Sons, Inc., New York, NY, 1994.

[Robbins, 1952] H. Robbins. Some aspects of the sequential
design of experiments. Bulletin of the American Mathe-
matics Society, 58:527–535, 1952.

[Szepesv´ari and Littman, 1999] Cs. Szepesv´ari and M.L.
Littman.
A uniﬁed analysis of value-function-based
reinforcement-learning algorithms. Neural Computation,
11:2017–2059, 1999.

IJCAI-07

835


Data Clustering: Principal Components, Hopfield and Self-Aggregation Networks* 

Chris H.Q. Ding 

NERSC Division, Lawrence Berkeley National Laboratory 

University of California, Berkeley, CA 94720. chqding@lbl.gov 

Abstract 

We present  a coherent framework  for data cluster(cid:173)
ing.  Starting  with  a  Hopfield  network,  we  show 
the  solutions  for  several  well-motivated  cluster(cid:173)
ing  objective  functions  are  principal  components. 
For MinMaxCut objectives  motivated  for ensuring 
cluster  balance,  the  solutions  are  the  nonlinearly 
scaled  principal  components.  Using  scaled  PC  A, 
we  generalize  to  multi-way  clustering,  construct(cid:173)
ing  a  self-aggregation  network,  where  connection 
weights  between  different  clusters  are  automati(cid:173)
cally  suppressed  while  connection  weights  within 
same clusters are automatically enhanced. 

Introduction 

1 
Principal component analysis (PCA) is widely adopted as an 
effective unsupervised dimension reduction method.  PCA is 
extended  in  many  different  directions  [Hastie  and  Stuetzle, 
1989; Kramer,  1991; Lee and Seung,  1999; Scholkopf et al, 
1998; Collins et a/., 2001]. 

The main justification is that PCA uses  singular value de(cid:173)
composition  (SVD)  which  is  the  best  low  rank  approxima(cid:173)
tion  in  L2  norm  to  original data  due  to Eckart-Young  theo(cid:173)
rem.  However, this results alone is inadequate to explain the 
effectiveness  of PCA.  Here,  we  provide a  new derivation of 
PCA based on optimizing suitable clustering objective func(cid:173)
tions and show that principal components are actually cluster 
indicator vectors in clustering. 

Hopfield  networklHopfield,  1982]  provide  a  convenient 
framework  of  our  study. 
In  particular,  the  self-aggregation 
network proposed in this work uses Hebb rule to encode pat(cid:173)
tern  vectors.  One  feature  of Hopfield  associative-memory 
networks is that it can  be  adopted to solve  hard combinato(cid:173)
rial problems[Haykin, 1998 2nd ed]. 

Another thread of this work is the spectral graph partition(cid:173)
ing  [Fiedler,  1973;  Pothen  et  al.,  1990;  Hagen  and  Kahng, 
1992;  Shi  and  Malik,  2000;  Ding  et  al,  2001a;  2001b; 
Ng et al., 2001; Meila and Shi, 2001], which uses Laplacian 
matrix of a graph. This arises naturally for balancing the clus(cid:173)
ters (see 
Our  approach  differs  from  others  mainly  in 
* Work supported by Department of Energy (Office of Science, 

through a LBNL LDRD) under contract DE-AC03-76SF00098. 

LEARNING 

well-motivated clustering objective functions.  A further point 
is the recognition that spectral  graph clustering is embedded 
in the scaled PCA thus leading to self-aggregation networks. 
This  paper  combines  above  three  threads  and  develop  a 
coherent  framework for clustering.  We  begin  with  two-way 
and generalize to multi-way clustering in §3. 
clustering in 

2  Two-way clustering 
We start by formulate two-way clustering as a Hopfield net(cid:173)
work and derive PCA as cluster indicator vectors. 
2.1  Hopfield Network and PCA 
Given n data points and properly defined similarity or associ(cid:173)
0 between points i, j, we form a network 
ation 
(a  weighted  graph)  G  with 
as  the  connection  between 
nodes 
We  wish  to  partition  it  into  two  clusters  Cj,c2. 
The  result  of clustering  can  be  represented  by  an  indicator 
vector q,  where 

(1) 

Consider the clustering objective, 

ity of c1, and analogously for s(c2, c2). 

Now  we  propose  a  min-max  clustering  principle:  data 
points are grouped into clusters such that the overlap s(c1, c2) 
between different clusters are minimized while within-cluster 
similarities 

are maximized 

(3) 
These conditions can be simultaneously satisfied by maximiz(cid:173)
ing the energy (objective function)  J1.  Using Hopfield model 
[Hopfield, 1982], the solution is obtained by the update rule 

If  one  relaxes  q(i) 
or in  vector  form 
from discrete  indicators to continuous values  in  (-1,1),  the 
solution q is given by 

(4) 

479 

Since the matrix entries in W are non-negative, the first prin(cid:173)
cipal  eigenvector qi  has all positive (or all  negative) entries. 
Thus the desired solution is q2. 
2.2  Hopfield  network for bipartite graph 
Here  we  extend  the  Hopfield  networks for clustering  bipar(cid:173)
tite graph.  An  example  of bipartite graph  is  a  m  x  n  term-
document association matrix 
, where each row rep(cid:173)
resents a word, each column represents a document, and btJ 
the  counts  of  co-occurrence  of row  rt  and  column 
We 
show  that  the  solution  for  clustering  indicators  is  precisely 
the Latent Semantic Indexing[Deerwester el  al,  19901 

We wish  to partition the r-type  nodes of R into two parts 
R1, R2 and simultaneously partition the otype nodes of C into 
two  parts 
based  on  the  clustering  principle  of  mini(cid:173)
mizing  between-cluster association  and  maximizing  within-
cluster  association.  We  use  indicator  vector  f  to  determine 
how  to  split  R  into 
=  1,  or  -1  depending  on 
We use g to determine how to split C into 
-1 depending on 

(For presentation  purpose,  we  index  the  nodes  such  that 
nodes  within  same  cluster  are  indexed  contiguously.  The 
clustering  algorithms  presented  are  independent  to  this  as(cid:173)
sumption.  Bold face  lower case  letters  are  vectors.  Matri(cid:173)
ces  are  denoted  by  upper case  letters.)  Thus  we  may  write 

It  is  convenient  to convert  the  bipartite graph  into  an  undi(cid:173)
rected graph. We follow standard procedure and combine the 
two types nodes to one by setting 

(5) 

(6) 

The solutions to this equation are the singular value decom(cid:173)
position (SVD) of B:  {f;}  are left singular vectors  and  {g,} 
are right singular vectors of the SVD of £?, 

(9) 

Again,  since  the  matrix  entries  in  W  are  non-negative,  the 
first  principal  components 
have  entries of same  sign; 
thus the desired solutions are 112, V2. Note that this is also the 
SVD employed in LSI. We summarize these results in 
Theorem  1.  Principal  components  are  solutions  for  clus(cid:173)
tering  indicators for clustering undirect  graphs  and  bipartite 
graphs under appropriate Hopfield network models. 

2.3  Principal component clustering 
In  the  objective  J1,  we  may  explicitly  enforce  a balance  of 
clusters 
Thus we consider the clustering objective, 

From Eq.l,  we need to  minimize 

(10) 

is a parameter and e — 

where 
to control the levele of balance between 
eigenvector  q1  of 
vector. 

We adjust u 
The principal 
is  the desired indicator 

What is the reasonable choice of,  A natural choice is to 
the average of  wiJ.  With this 
set 
choice,  the modified  weight  matrix  satisfies  the sum-to-zero 
condition: 

This condition can be further refined by centering each col(cid:173)

umn and each rows, such that 

(11) 

This induces an undirected graph G, whose adjacency matrix 
is the symmetric  weight matrix  W. 

Consider the following objective function, 

where 

(12) 

(13) 

(7) 

where 
,  the  sum of association 
between  R1  and  C2,  and S(R1,C2) is  the  sum  of  association 
between  R1  and  c2.  S(RI,C2)  and  S(R2,CI)  should  be  mini(cid:173)
mized. S(R1 , C\) is the sum of association within cluster 1 (see 
Fig.l), and  s(R2,c2)  is the sum of association within cluster 
2. S(R1 , C1) and S(R2, C2) should be maximized. These condi(cid:173)
tions are simultaneously satisfied by maximizing J2. 

We can write down the Hopfield network update rule for q. 
If one relaxes q{i)  from discrete indicators to continuous val(cid:173)
ues, the solution q satisfies Eq.(4).  Now utilizing the explicit 
structures of W and q, we have 

(8) 

480 

(here column and  row sums, 
and  w  are standard notations in statistics.)  Now the desired 
cluster indicator vector is the principal eigenvector of 

The fully centered  W has a useful property that all eigen(cid:173)
vectors of W with nonzero eigenvalues have the sum-to-zero 
=  0.  This  is  because  (1)  q0  =  e  is  an 
property: 
eigenvector  of  W  with 
(2) all other eigenvectors are 
orthogonal to qo, i.e, 

The sum-to-zero condition 

=  0 does not necessar(cid:173)
ily  imply  that  the  sizes  of the two cluster, 
should be 
equal. In fact, the cluster indicator vector should be refined to 

(14) 

LEARNING 

(in this paper, all vectors are implicitly normalized to 1 using 
L2  norm).  Correspondingly, the clustering objective becomes 

To  overcome  this  problem,  we  seek  to  prevent  either 
s(c1,C2)  or s(c2,c2)  become  very  small.  We optimizelDing 
et a/., 2001b] 

(15) 
Clearly,  the  first  two  terms  represent  the  average  within-
clustcr  similarities  which  are  maximized,  and  the  3rd  term 
represent average betwcen-cluster similarities which are min(cid:173)
encourages  cluster  balance,  since 
imized.  The  factor 
= 

is reached when 

We summarize these results in 

Theorem 2.  Solution for clustering objective 
q i; and solution for clustering objective 

is given by 

We call  these clustering schemes  with objective functions 
as principal component clustering methods because 

of the use of principal components. 

All above discussions apply to bipartite graphs.  For exam(cid:173)

ple, J1 becomes 

(16) 

are  sizes of row clusters 

are  sizes  of column clusters 

where 
row size); and 

, — total 
+ 
column size).  Again, the first two terms represent 
the average within-cluster similarities which  are  maximized, 
and the last two terms represent average between-cluster sim(cid:173)
ilarities which are minimized. 

We  note  that 

is  partially  motived  by  the  classic  scal(cid:173)
ing  (also  called  principal  coordinate  analysis)  in  statistics 
iBorg  and  Gronen,  1997].  Suppose  we  are  given  pairwise 
for a dataset,  define  the  pairwise  similarity  as 
distances 
, follow through the same centering procedures 
and solve for the eigenvectors of W.  The classic  scaling the(cid:173)
orem  proves  that  if dtJ  are  the Euclidean  distances,  the first 
A"  eigenvectors  of W  will  recover  the  original  coordinates. 
This  theorem justifies  the  use  of the  K  principal  eigenvec(cid:173)
tors as the coordinates in multidimensional scaling. However, 
our clustering approach  does  not  emphasize  the  recovery  of 
original  coordinates,  and  the  objective  function 
is well-
motivated for clustering only. 

(17) 

Although 
curs that one cluster is much larger than another, 

are  maximized,  it  often  oc(cid:173)

or vice versa. 

'Since exp(rr)  is monotonic, max  J1  maxexp 

, which 
]. This is approx(cid:173)

is minexp 
imately Eq.17. 

(18) 

(19) 

(20) 

(21) 

which can be written as 

Again,  the  desired  solutions  is  the  eigenvector  associated 
with the second  largest eigenvalue.  Note that  by comparing 
Eq.(21) with  Eq.(4),  the  net  effect  for cluster balance  is  di(cid:173)
agonal scaling.  This diagonal scaling, however,  leads to an 
important feature of self-aggregation (see  §3.2). 
2.5  Cluster balance for bipartite graphs 
For balanced clustering of bipartite graphs, object function of 
Eq.(7) becomes 

Using the representation W and q of Eq.6, and similar deriva(cid:173)
tion [Ding, 2003; Zha et al, 2001 J, the solution for optimiza(cid:173)
tion of J4  is  also  given  by  Eq.21.  Let  Dr  =  diag(Be)  and 

we have 

clustering  balancing  using  MinMaxCut  objective  of Eq.(22) 
over the simple objective Eq.(7) is the scaling of the associ(cid:173)
ation matrix  B  in  Eq.(25).  However,  with  this  scaling,  the 
self-aggregation property emerges (see 

3  A-way clustering 
The above mainly focus on 2-way clustering. Below we gen(cid:173)
eralize  to  A'-way  clustering,  K  >  2.  The  generalization  is 
based on the key observation that the solution for cluster in(cid:173)
dicator vectors (see  §2.4 and §2.5) are scaled principal com(cid:173)
ponents, as we discuss next. 
3.1  Scaled principal components 
Associations among data objects are  mostly quantified by a 
similarity metric.  The scaled  principal component approach 
starts  with  a  nonlinear (non-uniform) scaling  of W.  Noting 
D1/2(D'x'2WD'l'2)Dl'2
that  W  - 
t 
we  apply  spectral 
decomposition on the scaled matrix 

Note Eq.(27) is  identical  to Eq.(20),  or Eq.(21),  thus scaled 
principal  components  are  cluster  indicator  vectors  in  Min-
MaxCut (see §2.3). 
3.2  Self-aggregation network 
In  Hopfield  networks,  a pattern  q1  is  encoded  into the net(cid:173)
work as 
(the Hebb rule); multiple patterns are encoded 
additively.  In our problem, a pattern is a cluster partitioning 
indicator vector.  We define  a self-aggregation network with 
the connection weights 

.  Here  we  highlights several  important 
properties  of  WSA  and  provides  several  example  applica(cid:173)
tions. (Self-aggregation is first studied in [Ding et al, 2002].) 
has an interesting self-aggregation property enforced 
by within-cluster association (connectivity). To prove, we ap(cid:173)
ply  perturbation  analysis  by  writing 
, 
where 
is  the similarity matrix  when clusters are  well-
separated  (zero-overlap) and 
accounts  for the  overlap 
among  clusters  and  is  treated  as  a perturbationlDing et aL, 
2001a].  This perturbation approach  is  standard  in  quantum 
physics[Mathews and Walker,  1971]. 

482 

are  the  K  eigenvectors  with eigenvalue 
is a lin(cid:173)
ear combination of K step functions, i.e., piece-wise constant 
function. Clearly, all data objects within the same cluster have 
identical elements  in  q.  The  coordinate of object i  in the  K-
dim  SPCA  space  is 
Thus  objects 
within a cluster are located at (self-aggregate into) the same 
point in SPAC space. 

Scaled principal components are not unique, since C could 

be any orthogonal matrix.  However, 

(30) 

Clusters overlap 

Consider the case  when  overlaps  among different clusters 
exist.  The overlaps are treated as a perturbation.  Theorem 
3.  At the first order,  the K  scaled  principal components  and 
their eigenvalues have the form 

Several features of SPCA can be obtained from Theorem 3: 
Corollary  3.1.  SA  network  
has  the  same block di(cid:173)
agonal  form  of Eq.(30),  within  the  accuracy  of Theorem  2. 
This  assures  that  objects  within  the  same  cluster  will  self-
aggregate as in Theorem 1. 
Corollary 3.2.  The first scaled principal component is qi  = 
and  qi 

are also the exact solutions to the original Eq.(20). 
Corollary 3.3.  When  K  =  2,  the  second  principal  compo(cid:173)
nent is 

(32) 

(33) 

LEARNING 

Note that this is precisely J3 of Eq.18, the clustering objective 
we started with: our clustering framework is consistent. 
Example  1.  A  dataset  of  3  clusters  with  substantial  ran(cid:173)
dom  overlap  between  the clusters.  All edge  weights  are  1. 
The  similarity matrix  and results are  shown in  Fig.l,  where 
nonzero matrix elements are shown as dots. The exact 
and 
approximate 

from Theorem 3 are close: 

WSA  is  much  sharper  than  the  original  weight  matrix  W 
clearly due to self-aggregation:  connections between  differ(cid:173)
ent  clusters  are  substantially  suppressed  while  connections 
within same clusters are substantially enhanced. 

Figure  1:  Left:  similarity matrix W.  Diagonal blocks repre(cid:173)
sent weights inside clusters and off-diagonal blocks represent 
overlaps between clusters. Right: Computed WSA-

In  DNA  micro-array  gene  expression  pro(cid:173)
Application  1. 
filing,  responses  of  thousands  of  genes  from  tumor  tissues 
are  simultaneously  measured.  We  apply  SPCA  framework 
to gene expression profiles of lymphoma cancer sampleslAl-
izadeh ex al, 2000].  Three cancer and three normal subtypes 
are  shown  in Fig.2.  This  is a difficult case  due to the large 
variations of cluster sizes (the number of samples in each sub(cid:173)
type are shown in parentheses in Fig.2B). Self-aggregation is 
evident  in  Figure 2B  and  2C.  The  computed clusters corre(cid:173)
spond quite well to the normal and cancer subtypes identified 
by human experts. 

3.3  Dynamic  aggregation 
The  self-aggregation  process  can  be  repeated  to  obtain 
sharper clusters.  W&  is the low-dimensional projection that 
contains the essential cluster structure. Combining this struc(cid:173)
ture with the original similarity matrix, we obtain a new sim(cid:173)
ilarity matrix containing sharpened cluster information: 

r

e, 

the  weight  matrix  at  f-th 
is crucial  for enforcing the cluster 

(34) 

e

h

w
iteration, Setting 
structure. 

Applying SA  net on  W^  leads  to further aggregation  (see 
Figure 2C).  The  eigenvalues  of the  1st  and  2nd  SA  net  are 
shown in the insert in Figure 1C. As iteration proceeds, a clear 
gap is developed, indicating that clusters becoming more sep(cid:173)
arated. 

Figure 2:  Gene expression profiles of cancerous and normal lym(cid:173)
phoma tissues samples from Alizadeh et al.  in original Euclidean 
space (A), in scaled FCA space (B), and in scaled PCA space after 
one iteration of Eq.34 (C). In all 3 panels, objects in original space 
are shown in 2D-view spanned by the first two PCA components. 
Cluster structures become clearer due to self-aggregation.  The in(cid:173)
sert in (C) shows the eigenvalues of the 1st and 2nd SA network. 

Noise  reduction 

SA net has noises. For example, WSfii has sometimes nega(cid:173)
tive weights  (WSA)ij  whereas we expect them to be nonnega-
tive. However, by Corollaries 2.1 and 3.1, WSA has a diagonal 
block structure and every elements  in the block are  identical 
(Eq.30) even when overlaps exit.  This property allows us to 
as the probability that two objects i, j 
interpret 
belong to the same cluster: 

To reduce noise in the dynamic aggregation, we set 

(35) 
and we  c h o s e=   0.8.  Noise reduction 
where 
In  our experiments,  final  re(cid:173)
is  an  integral  part  of SA  net. 
The above dynamic aggregation 
sults are insensitive to 
repeats self-aggregation process and forces data objects move 
towards the attractors, which are the desired clusters and their 
principal eigenvalues approach  1  (see insert in Fig.2C). Usu(cid:173)
ally, after one or two iterations the cluster structure becomes 
evident. 

LEARNING 

483 

3.4  Bipartite graphs 
SPCA  applies  to bipartite graph  problems as  well.  The  non(cid:173)
linear  scaling  factors  are  

(cf.  Eq.23).  Let  B  = 
is  given  in  Eq.(25).  Applying S VD 

where 

on  B,  we  obtain 

(36) 

for  row 
Scaled  principal  components  are  
objects  and 
They 
have  the  same  self-aggregation  and  related  properties.  We 
note  that  SPCA  on  bipartite  graph  leads  to  correspondence 
analysislGreenacre,  1984]  from a new perspective. 

for  column  objects. 

The  structure  of  self-aggregation  network  

can  be 

meaningfully  further  decomposed: 

[Collins et al, 2001]  M. Collins,  S.  Dasgupta, and R.  Schapire.  A 
generalization of principal component analysis to the exponential 
family.  Neural Info.  Processing Systems (NIPS 2001),2001. 

[Deerwester et al., 1990]  S.  Deerwester,  S.T.  Dumais,  T.K.  Lan-
dauer, G.W. Furnas, and R.A. Harshman.  Indexing by latent se(cid:173)
mantic analysis. J. Amer. Soc. Info. Sci, 41:391-407,1990. 

[Ding etal, 2001a]  C. Ding, X.  He, and H. Zha.  A spectral method 
to  separate  disconnected  and  nearly-disconnected  web  graph 
In  Proc.  ACM  Int'l  Conf Knowledge  Disc.  Data 
components. 
Mining  (KDD),  pages 275-280,2001. 

[Ding et al, 2001b]  C. Ding, X. He, H. Zha, M. Gu, and H. Simon. 
A min-max cut algorithm for graph partitioning and data cluster(cid:173)
ing.  Proc.  IEEE Int'l  Conf  Data  Mining,  pages  107-114,2001. 
[Ding etal, 2002]  C.  Ding,  X.  He,  H.  Zha,  and  H.  Simon.  Un(cid:173)
supervised learning:  self-aggregation in scaled principal compo(cid:173)
nent space.  Proc.  6th  European  Conf  Principles of Data  Mining 
and Knowledge Discovery (PDKK 2002),  pages  112-124,2002. 
[Ding, 2003]  C.  Ding.  Document  retrieval  and  clustering:  from 
principal component analysis to  self-aggregation networks.  Int'l 
Workshop onAl and Statistics,  pages 78-85,2003. 

[Fiedler,  1973]  M.  Fiedler. 

Algebraic  connectivity  of  graphs. 

Czech. Math. J., 23:298-305,1973. 

where 
provides  the  cluster  structure  for  row  objects,  while 
provides  the  cluster  structure  for  column  objects.  The  off-
diagonal  block  matrix  
provides the  sharpened  associa(cid:173)
tion between  row and column  objects[Ding, 2003]. 

4  Discussions 
In  this  paper,  we  present  a  data  clustering  framework  based 
on  properly  motivated  Hopfield  network,  and  show  the  clus(cid:173)
tering indicators are principal components.  Further motivated 
by  cluster  balance,  we  extend  the  framework  to MinMaxCut 
that  utilized  the  Laplacian  matrix  of  a  graph.  The  frame-
work  is  generalized  to  multi-way  clustering,  by  using  scaled 
principal components, and self-aggregation networks are con-
structed.  We prove the cluster member self-aggregation prop(cid:173)
erty  of the  network.  This  framework  extends  naturally  to  bi(cid:173)
partite  graphs  which  leads  to  row-row,  column-column  and 
row-column  SA  nets  that  simultaneously  cluster  the  row  and 
column  objects. 

In  self-aggregation,  data  objects  move  towards  each  other 
guided  by  connectivity.  This  is  similar to  the  self-organizing 
map  [Kohonen,  1989],  where  feature  vectors  self-organize 
into  a  2D  feature  map  while  data  objects  remain  fixed.  A ll 
these  have  a  connection  to  recurrent  networks  [Hopfield, 
1982;  Haykin,  1998  2nd  ed].  In  Hopfield  network,  features 
are stored as associative  memories.  In more complicated net(cid:173)
works,  connection  weights are  dynamically  adjusted  to  learn 
or  discover  the  patterns.  The  self-aggregation  network  pro(cid:173)
vides  a new  mechanism  to realize  this unsupervised learning. 

References 
[Alizadeh et al., 2000]  A.A.  Alizadeh, M.B.  Eisen, et al.  Distinct 
types  of dimise  large  B-cell  lymphoma  identified  by  gene  ex(cid:173)
pression profiling.  Nature, 403:503-511,2000. 

[Borg and Gronen, 1997]  I.  Borg  and P.  Gronen.  Modern multidi(cid:173)
theory  and  applications.  Springer  Verlag, 

mensional  scaling: 
1997. 

[Greenacre,  1984]  M.  J.  Greenacre.  Theory  and Applications  of 

Correspondence Analysis.  Academic press,  1984. 

lHagen and Kahng, 1992]  L. Hagen and A.B. Kahng. New spectral 
methods for ratio cut partitioning and clustering. IEEE.  Trans, on 
Computed Aided Desgin,  11:1074-1085,1992. 

[Hastie and Stuetzle,  1989]  T  Hastie  and  W.  Stuetzle.  Principal 

curves. J. Amer. Stat. Assoc, 84:502-516,1989. 

[Haykin,  1998 2nd ed]  S.S. Haykin.  Neural Networks: A  Compre(cid:173)

hensive Foundation.  Prentice Hall,  1998,2nd ed. 

[Hopfield, 1982]  J.J.  Hopfield.  Neural networks and physical sys(cid:173)
tems with emergent collective  computation abilities.  Proc.  Nat'I 
AcadSci  USA,  79:2554-2558,1982. 

[Kohonen, 1989]  T.  Kohonen.  Self-organization  and  Associative 

Memory.  Springer-Verlag,  1989. 

[Kramer, 1991] M. A. Kramer. Nonlinear principal component anal(cid:173)
ysis  using  autoassociativc  neural  networks.  AIChE  Journal, 
37:233-243,1991. 

[Lee and Seung, 1999]  D.  D.  Lee and H.  S.  Seung.  Learning the 
parts  of objects  with  nonnegative  matrix  factorization.  Nature, 
401:788-791,1999. 

[Mathews and Walker, 19711  J. Mathews and R.L. Walker.  Mathe(cid:173)

matical Methods of Physics. Addison-Wcsley, 1971. 

[Meila and Shi, 2001]  M.  Meila and J.  Shi.  A random walks view 

of spectral segmentation.  Al-statistics  Workshop, 2001. 

[Ng etal, 2001]  A.Y.  Ng,  M.I.  Jordan, and Y.  Weiss.  On spectral 
clustering:  Analysis  and an algorithm.  Proc.  Neural Info.  Pro(cid:173)
cessing Systems (NIPS 2001), 2001. 

[Pothen et al, 1990]  A. Pothen, H. D. Simon, and K. P. Liou. Parti(cid:173)
tioning sparse matrices with egenvectors of graph. SI AM Journal 
of Matrix Anal.  Appl,  11:430-452,  1990. 

[Scholkopfe/a/.,  1998]  B.  Scholkopf,  A.  Smola,  and  K.  Muller. 
Nonlinear component analysis  as  a  kernel  eigenvalue problem. 
Neural  Computation,  10:1299-1319,1998. 

[Shi and Malik, 2000]  J. Shi and J. Malik. Normalized cuts and im(cid:173)
age segmentation. IEEE.  Trans, on Pattern Analysis and Machine 
Intelligence,  22:888-905,2000. 

[Zha et al, 2001]  H. Zha, X. He, C. Ding, M. Gu, and H.D. Simon. 
Bipartite  graph partitioning  and data clustering.  Proc.  10th Int'l 
Conf  Information  and  Knowledge  Management  (CIKM  2001), 
pages 25-31,2001. 

484 

LEARNING 


Ranking Cases with Decision Trees: a Geometric Method that Preserves

Intelligibility

Isabelle Alvarez(1,2)

(1) LIP6, Paris VI University

Stephan Bernard (2)
(2) Cemagref, LISC

4 place Jussieu, F-75005 Paris, France

isabelle.alvarez@lip6.fr

F-63172 Aubiere Cedex, France
stephan.bernard@cemagref.fr

Abstract

This paper proposes a new method to rank the cases
classiﬁed by a decision tree. The method applies
a posteriori without modiﬁcation of the tree and
doesn’t use additional training cases.
It consists
in computing the distance of the cases to the deci-
sion boundary induced by the decision tree, and to
rank them according to this geometric score. When
the data are numeric it is very easy to implement
and efﬁcient. The distance-based score is a global
assess, contrary to other methods that evaluate the
score at the level of the leaf. The distance-based
score gives good results even with pruned tree, so
if the tree is intelligible this property is preserved
with an improved ranking ability. The main reason
for the efﬁcacity of the geometric method is that in
most cases when the classiﬁer is sufﬁciently accu-
rate, errors are located near the decision boundary.

Introduction

1
Decision Trees (DT) are a very popular classiﬁcation tool be-
cause they are easy to build and they provide an intelligible
model of the data, contrary to other learning methods. The
need for intelligibility is very important in artiﬁcial intelli-
gence for applications that are not fully automatic, if there
is an interaction with the end-user, expert or not. This is
the reason why DT algorithm are widely used for classiﬁca-
tion purpose (see Murthy [1998] for examples of real world
applications). But for some applications knowing the class
of each case is not sufﬁcient to make a decision, one needs
to compare the cases to one another in order to select the
most promising examples. This is often the case in marketing
applications, allocation of resources or of grants, etc. (See
[Zadrozny and Elkan, 2001] for a description of the chari-
table donation problem). The traditional idea in this case is
to look for the probability of each case to belong to the pre-
dicted class, rather than just the class. The cases are then
ranked according to the probability-based score. Unfortu-
nately, methods that are highly suitable for probability esti-
mate produces generally unintelligible models. This is the
reason why some recent works aim at improving decision tree
probability estimate. Smoothing methods are particularly in-
teresting for that purpose. They consist in replacing the raw

conditional probability estimate at the leaf by some corrected
ratio that shifts the probability toward the prior probability
of the class. The raw conditional probability estimate at the
leaf is deﬁned by pr(c|x) = k
n , where k is the number of
training cases of the class label classiﬁed by the leaf, and n
is the total number of training cases classiﬁed by the leaf.
It is the same for all the cases that are classiﬁed by a leaf.
The most general type of correction generally used are the
m-estimate pm (see equation (1)), which uses the prior prob-
ability of the class and a parameter m, and the Laplace cor-
rection pL which is a particular case of m-correction when
all the C classes have the same priors (see [Cestnik, 1990;
Zadrozny and Elkan, 2001]).

pm(c|x) =

k + p(c).m

n + m

pL(c|x) =

k + 1
n + C

(1)

The main interest of smoothing methods is that they don’t
modify the structure of the tree. But in order to improve the
probability estimate, these methods are often applied to un-
pruned trees (see [Provost and Domingos, 2003]), so the in-
telligibility of the model is very much reduced, although it is
one of the main interest of decision trees compared to other
classiﬁers (like Naive Bayes, Neural Networks for instance).
Ensemble methods like bagging are also used successfully to
rank cases, although the margin is not a priori an estimate of
the class membership. Nevertheless, ensemble methods loose
also the intelligibility of the model.

The method we propose here aims ﬁrstly at preserving the
intelligibility of the model, so the objective is to improve the
ranking without modifying the tree itself. This method is
based on the computation of the distance of the cases from
the decision boundary (the boundary of the inverse image of
the different classes in the input space), when it is possible
to deﬁne a metric on the input space. The distance of a case
from the decision boundary deﬁnes a score that is speciﬁc to
each case, unlike other methods for which the score is de-
ﬁned at the level of the leaf and so it is shared by all cases
classiﬁed by the same leaf. In other geometric methods, like
Support Vector Machine (SVM) it has been proved that the
distance to the decision boundary can be used to estimate the
posterior probabilities (see Platt [2000] for the details in the
two-class problem): an additional database is needed in order
to calibrate the probabilities. But since in many applications

Database

bupa
glass
ionosphere
iris
letter
newThyroid
optdigits
pendigits
pima
sat
segmentati
sonar
vehicle
vowel
wdbc
wine

Size of
dataset

∆N

(UT-PT)

N
(UT)

345
214
351
151
20000
215
5620
10992
768
6435
210
208
846
990
569
178

19.53±0.68
2.90±0.16
5.22±0.25
1.49±0.12
31.71±0.76
2.95±0.19
9.14±0.35
8.91±0.35
32.88±1.11
13.67±0.40
1.48±0.13
6.30±0.24
8.04±0.29
2.50±0.19
5.10±0.23
1.43±0.10

30.52±0.55
6.25±0.15
11.00±0.2
4.94±0.11
61.10±0.58
7.25±0.13
19.35±0.3
23.33±0.3
47.2±0.96
24.73±0.31
4.66±0.09
11.36±0.15
18.10±0.25
8.56±0.14
10.03±0.18
4.22±0.09

Table 1: Comparison of the size of pruned (PT) and uncollapsed
unpruned (UT) trees: Mean and standard deviation of the difference
of the number of leaves N over 100 resamples.

we don’t need the exact posterior probability, it is generally
possible to use directly the score induced by the distance to
rank and to select the most interesting cases.

The paper is organized as follow: Section 2 examines from
the intelligibility viewpoint the methods applied to decision
trees to rank cases or to estimate posterior probabilities. Sec-
tion 3 presents our method for obtaining a distance-based
score, and it explains why it is interesting from a theoreti-
cal point of view. Section 4 presents the experimental re-
sults which have been drawn from the numerical databases of
the UCI repository, in comparison with the results obtained
from the smoothing methods applied on the same databases.
We make further comments about geometric score and hybrid
method in the concluding section.

2 Decision Tree methods for ranking: the

intelligibility viewpoint

The success of Decision Trees as classiﬁcation method is for
a good part due to the intelligibility of the model produced
by the algorithms. Pruning methods [Breiman et al., 1984;
Bradley and Lovell, 1995; Esposito et al., 1997] produce
shorter trees with at least the same performance than longer
trees, since the generalization performance are enhanced.
They also produce shorter tree on purpose, seeking for a com-
promise between accuracy (or other performance criteria) and
the size of the tree. Table 1 shows that unpruned trees can
be very large compared to pruned trees with similar accuracy
(the mean absolute difference over the databases is 0.38% and
it is always less than 2.3%). Because of this size problem, it is
desirable to improve the probability estimate given by DT, in
order to allow a compromise between size and ranking ability.
With smoothing methods the probability estimate is the
same for all the examples classiﬁed by a leaf.
In order to
produce more speciﬁc probability estimates, other methods
learn directly the probability class membership at the leaf.
For instance, [Smyth et al., 1995] use kernel-based density

estimator at the leaf, without modiﬁcation of the tree struc-
ture. This method improves signiﬁcantly the class probability
estimates. But the practical use of kernel density estimator is
limited to very low dimension, and the setting of parameters
is not easy. Kohavi [1996] builds Naive Bayes classiﬁers at
the level of the leaf, using its own induction algorithm. The
objective of the tree partition is not to separate the classes but
to segment the data so that the conditional independence as-
sumption is better veriﬁed. The size of the tree is limited to
cover each leaf with enough data. In our experiment the size
of the Naive Bayes Trees (NBT) is comparable to the size of
the pruned trees (but the segmentation of the space is com-
pletely different). With different objectives and structures,
the interpretation of DT and NBT cannot compare easily.

Other methods try to correct the probability estimate at
each nodes by propagating a case through the different pos-
sible path from each node. These methods, like fuzzy trees
[Umano et al., 1994], fuzzy split [Quinlan, 1993], or more
recently [Ling and Yan, 2003] deal with a different issue:
Managing the uncertainty in the input case and in the train-
ing database. Generally the computation of the probability
estimate is very complex and in some cases difﬁcult to under-
stand: a lot of nodes can be involved, although non-convex
area of the input space corresponding to one class can be di-
vided arbitrarily into several leaves. So from the point of view
of intelligibility these methods are not totally convincing.

We propose here to keep the structure of the pruned tree
but to rank the cases accordingly to their distance from the
decision boundary which is deﬁned by the tree.

3 Distance ranking methods for decision trees
We consider here axis-parallel DT (ADT) operating on nu-
merical data: Each test of the tree involves a unique attribute.
We note Γ the decision boundary induced by the tree. Γ con-
sists of several pieces of hyperplanes which are normal to
axes.

We consider a multi-class problem, with a class of interest
c (the positive class). Let x be a case, c(x) the class label
assigned to x by the tree, d = d(x, Γ) the distance of x from
the decision boundary Γ. We use the distance of an example
from the decision boundary to deﬁne its geometric score.

3.1 Global and local geometric ranking
Deﬁnition 1 Geometric score

The geometric score g(x) of x is the distance of x from the

decision boundary if c(x) = c and its opposite otherwise.

g(x) = (cid:26)d(x, Γ)
−d(x, Γ)

if c(x) is the positive class,
otherwise.

(2)

Theorem 1 Global geometric ranking

The geometric score induce a quasi-order (cid:23) over the ex-

amples classiﬁed by the tree.

x (cid:23) y ⇔ g(x) ≥ g(y)

(3)

Cases are ranked in decreasing order relatively to the geomet-
ric score. The most promising cases have the highest geomet-
ric score, which means that their predicted class is the positive
one and that they are far from the decision boundary.

With the geometric score, examples are ranked individu-

ally, not leaf by leaf.

The geometric score is speciﬁc to each example, so it is
also possible to ﬁrst rank the leaves with a smoothing method
(or an equivalent method that ranks the leaves, not the cases)
and then to rank the cases inside a leaf.
Theorem 2 Local geometric ranking

The geometric score induce a quasi-order (cid:23)L over the ex-

amples classiﬁed by a leaf.

x (cid:23)L y ⇔ or(cid:26)p(c|x) > p(c|y)

p(c|x) = p(c|y) and g(x) ≥ g(y).

(4)

With the local geometric score, the leaves are ranked ac-
cording to the probability estimate, then inside each leaf (or
inside each group of leaves with the same output probability
estimate), examples are ranked according to their geometric
score.

The distance of a case x to the decision boundary is com-
puted with the algorithm described in [Alvarez, 2004]. It con-
sists in projecting x onto all the leaves f which class label
differs from c(x). The nearest projection gives the distance.
Algorithm 1 distanceFrom(x,DT)
0. d = ∞;
1. Gather the set F of leaves f which class c(f ) 6= c(x);
2. For each f ∈ F do: {
3.
4.
5.
6. Return d = d(x, Γ)
Algorithm 2 projectionOntoLeaf(x,f = (Ti)i∈I)
1. y = x;
2. For i = 1 to size(I) do: {
3.

compute pf (x) = projectionOntoLeaf(x,f);
compute df (x) = d(x, pf (x));
if (df (x) < d) then d = df (x) }

if y doesn’t verify the test Ti then yu = b }
where Ti involves attribute u with threshold value b

4. Return y

The projection onto a leaf is straightforward in the case of
ADT since the area classiﬁed by a leaf f is a hyper-rectangle
deﬁned by its tests. The complexity of the algorithm is in
O(N n) in the worst case where N is the number of tests of
the tree and n the number of different attributes of the tree.

3.2 Theoretical viewpoint
We expect geometric ranking to give interesting results when
errors occur near the decision.

If this property is veriﬁed, positive cases (cases which class
is the class of interest) that are not recognized have negative
geometric score but with a small absolute value. False pos-
itive, that is negative cases classiﬁed as positive have also
small (but positive) geometric score. On the contrary, true
positive have higher geometric score and true negative have
negative geometric score with high absolute value. So inde-
pendently from the score estimated at the leaf, the geometric
score tends to bring side by side false negative and false pos-
itive, and to repel true positive and true negative. This can be
seen on a Receiver Operating Characteristic (ROC) curve (in
the way described in [Adams and Hand, 1999]). The ratio of

positive examples is plotted against the ratio of all other (neg-
ative) examples as the score varies. With methods that give
constant probability estimates at the leaf, the points are plot-
ted from one leaf to another. The afﬁne interpolation between
consecutive points assumes that examples are selected ran-
domly inside a leaf (or a set of leaves with the same score). If
we use a ROC curve to visualize the ranking, geometric rank-
ing will be very good at the beginning of the curve, as seen in
Figure 1.

o

i
t

a
r
 

e
v
i
t
i
s
o
p

0

.

1

9

.

0

8

.

0

7
0

.

6

.

0

5
.

0

ranking method

m-estimate

local geometric

global geometric

0.00

0.05

0.10
negative ratio

0.15

0.20

0.25

Figure 1: ROC curves of different ranking method (WD Breast-
Cancer sample). The local geometric ranking curve intersects the
basic m-estimate curve at leaf points.

Since DT algorithms are generally designed to maximize
accuracy, it is not unreasonable to hypothesize that errors lie
near the decision boundary. In a very ideal case, it is even
possible to demonstrate that this hypothesis is true.

A DT builds a partition in the input space. In a two class
problem, it is possible to associate to a DT a unique function
g : E 7→ {0, 1} such that g(x) is the predicted class of x. (But
several decision trees are associated to the same function).
We consider an ideal case of statistical decision, where the
joint distribution P of observations would be uniform on the
graph of the indicator function f of a set S in E = [0, 1]n. We
also suppose that the size of all the maximal hypercubes in S
and E\S has a lower bound ν > 0 (to prevent pathological
situation for S and its boundary). In this case, decision trees
built on samples drawn from P can approach f as closely as
wanted if the size of the sample can grow indeﬁnitely. For
this particular case of function f, errors are located near the
decision boundary.
Theorem 3 Proximity of errors. For a DT which associated
function g is close enough to f, errors are near the decision
boundary ∂g of g.

If we note A the area where f − g 6= 0 (set of errors),

˙A
the interior of A, and if we consider ǫ small enough so that
ǫ < νn, we have:

(cid:18)ZE |f − g| < ǫ and x ∈ ˙A(cid:19) ⇒ d(x, ∂g) < αn

n√ǫ .

(5)

Proof. Let x be in ˙A, we consider B(x, d
2 ) the maximal
hypercube centered at x in its connected component. The
volume of B is included into RE |f − g|, so we have: dn <
ǫ < νn. So the size of B is smaller than ν and d < n√ǫ. The
boundary of B, ∂B, encounters the decision boundary of f −
g on at least two meeting points of two different hyperplanes,
since B is maximal. If f is not constant on B, then both ∂f
2√n. If f is
and ∂g cross B, and so necessarily d(x, ∂g) < d
constant on B, since the size of B is smaller than ν, at least
one of the meeting point lies on ∂g (otherwise the size of B
would be smaller than the lower bound of the maximal balls).
So once again d(x, ∂g) ≤ d
Even if real conditions are very far from this ideal case
(in ﬁrst place, generally f doesn’t exist), we can test if the
hypothesis of proximity of errors is generally veriﬁed. Table
2 shows the mean of the difference of the mean distance of
correctly classiﬁed cases (hits) and errors from the decision
boundary. We also computed for each sample λ, the inverse
of the coefﬁcient of variation for the difference of the means
deﬁned by (6), where dh and σh are the mean and the standard
deviation of the distance of correctly classiﬁed examples from
the decision boundary, and de and σe the same magnitude for
error examples.

n√ǫ. •

2√n <

√n

2

λ =

dh − de
pσ2
h + σ2
e

(6)

Table 2 shows the percentage of the samples for which λ ≥ 2,
which is the 97.5% conﬁdence coefﬁcient under the normal
assumption (the test is unilateral). We can see that errors are
closer from the decision boundary for a majority of databases.
Datasets for which this property is not veriﬁed have generally
a low mean accuracy (62% for bupa and 69% for sonar). If
we consider only the samples for which the accuracy is better
than 70%, the proportion shifts to 29% and 50% respectively.

Database

bupa
glass
ionosphere
iris
newThyroid
optdigits
pendigits
pima
sat
segment.
sonar
vehicle
vowel
wdbc
wine

∆ of the
means

0.009±0.002
0.038±0.011
0.037±0.006
0.092±0.003
0.059±0.002
0.542±0.008
0.339±0.005
0.036±0.001
0.135±0.005
0.181±0.005
0.027±0.003
0.026±0.004
0.215±0.003
0.113±0.002
0.152±0.004

λ

0.86±0.12
2.28±0.34
1.36±0.19
5.34±0.28
3.46±0.14
24.87±1.45
24.6±1.24
4.15±0.15
12.59±0.56
7.28±0.2
1.44±0.14
4.75±0.49
16.88±0.74
10.24±0.29
6.73±0.39

% of samples
with λ ≥ 2

17
51
35
95
91
100
100
94
100
98
34
55
100
100
97.7

Table 2: Comparison of the mean distance of errors and hits to the
decision boundary, over the test bases of 100 samples per database.
The mean of the difference is estimated for each sample. (Bad re-
sults are bold)

In this case the geometric score cannot be good. So we expect
the geometric score to be better with more accurate trees.

4 Experimental Results
4.1 Experimental Design
We have studied the geometric ranking on the database of the
UCI repository [Blake and Merz, 1998] that have numerical
attributes only and no missing values. We are not directly
concerned in this study with the problem of the prevalence of
the positive class, since our method doesn’t build the decision
tree: it applies on the grown tree. So we didn’t pay any par-
ticular attention to the relative frequency of the classes in the
datasets. We chose as positive class either the class with the
lowest frequency in the database, either a class which grouped
together several classes when it was more logical. When the
classes were equiprobable and with no particular meaning
we chose it randomly. Although there is a lot of work on
the analysis of multi-class problem, for simplicity we have
treated multi-class problem as a two class problem (class of
the examples were modiﬁed before growing the trees).

For each database, we divided 100 bootstrap samples into
separate training and test sets in the proportion 2/3 1/3, re-
specting the prior of the classes (estimated by their frequency
in the total database). Even if it is not the best way to build
accurate trees for unbalanced dataset or different error costs,
here we are not interested in building the most accurate or
efﬁcient tree, we just want to study the effect of geometric
ranking on pruned trees. For the same reason we grow trees
with the default options of j48 (Weka’s [Witten and Frank,
2000] implementation of C4.5) although in many cases dif-
ferent options would build better trees. For unpruned trees
we disabled the collapsing function.

We used Laplace correction and m-estimate smoothing
methods to correct the raw probability estimate at the leaf
for reduced-error pruned tree and normal pruned tree. The
value of m was chosen such that m × p(c) = 10 where p(c)
is the prior probability of the class of interest (as suggested in
[Zadrozny and Elkan, 2001]).

We used two different metrics in order to compute the dis-
tance from the decision boundary, the Min-Max (MM) metric
and the standard (s) metric. Both metrics are deﬁned with the
basic information available on the data: An estimate of the
range of each attribute i or an estimate of its mean Ei and
of its standard deviation si. The new coordinate system is
deﬁned by (7).

yMM
i

=

xi − M ini
M axi − M ini

or

ys
i =

xi − Ei

si

.

(7)

The parameters of the metric are estimated on each sample.
The choice of the metric has a very limited effect on the geo-
metric score; If we measure the difference between the Area
Under the ROC curve (AUC) , for each database, it is always
less than 2 10−3 ± 9 10−4, except for the thyroid and vehicle
databases (less than 410−3) and the glass database (9.510−3).
4.2 Comparison between distance-based ranking

and smoothing methods

A corollary of theorem 3 is that if a tree is not accurate, er-
rors may lie everywhere, not only near the decision boundary.

The geometric score is only used to rank the examples with-
out changing the tree structure. It is not used to estimate the

Dataset

Red.-Error

pruning

Normal
pruning

No

pruning

NBTree

0.46±0.48
bupa
1.78±0.72
glass
-1.11±0.4
iono.
4.69±0.40
iris
0.18±0.09
letter
4.48±0.43
thyroid
optdig.
0.53±0.08
pendig. 0.46±0.04
1.34±0.43
pima
sat
1.01±0.09
segment. 8.16±0.75
2.55±0.47
sonar
0.35±0.16
vehicle
4.18±0.34
vowel
wdbc
3.75±0.24
5.35±0.45
wine

-0.14±0.47 -0.82±0.50 0.08 ±0.79
-0.39±0.75 -1.87±0.73 -2.01±0.83
-2.30±0.4 -2.85±0.42 -5.14± 0.72
3.85±0.35 3.67±0.37 1.56±0.43
0.37±0.07 -0.26±0.05 0.40±0.12
3.08±0.38 2.54±0.38 -2.13±0.62
0.34±0.06 0.07±0.06 -0.12±0.06
0.40±0.03 0.28±0.03 0.58±0.05
-0.98±0.25 -1.07±0.30 2.25±0.55
0.89±0.07 0.46±0.05 0.97±0.09
5.27±0.63 5.31±0.64 3.25±0.61
1.99±0.51 1.80±0.49 -5.01±1.03
0.64±0.14 -0.12±0.16 0.78±0.30
3.09±0.29 2.83±0.26 0.78±0.44
2.24±0.18 2.15±0.18 2.09±0.22
3.32±0.30 2.91±0.30 -0.06±0.25

Table 3: Absolute difference of the AUC between global geometric
ranking with standard metric and smoothing methods at the leaf. The
last column shows the difference between global geometric ranking
on Red.-error pruning tree with NBTree. (All mean values and stan-
dard deviations are ×100. Insigniﬁcant values are italic. Bad results
are bold)

posterior probability of an example, so the appropriate mea-
sure of performance in that case is the AUC. Table 3 shows
the difference between global geometric ranking and Laplace
or m-estimate correction at leaf.

Apart from a few cases, global geometric ranking gives
better values than either Laplace or m-estimate correction
(with a 95% conﬁdence coefﬁcent). The differences are rel-
atively small (from 0.004 to 0.08), but since they are ab-
solute values the improvement can be important. We have
also shown the difference of the AUC between global geo-
metric ranking on reduced-error pruned tree and NBTree.

Table 4 shows the difference between local geometric rank-
ing and smoothing correction at leaf. Local geometric rank-
ing is always better (with a 95% conﬁdence coefﬁcent) than
smoothing method alone, except in one case which is not sig-
niﬁcant. But like for global ranking, the improvement can
vary a lot (absolute value from 0.002 to 0.078).

As we said in the theoretical viewpoint section, we expect
geometric ranking to outperform smoothing method at the be-
ginning of the ROC curve. To measure the relative behavior
of ROC curves for increasing value of the negative ratio, we
have computed AU C(x), 0 ≤ x ≤ 0.5, the integral func-
tion of the ROC curve, with a 0.001 step value, for the global
geometric score (g) and the smoothing correction (s). Ta-
ble 5 shows for normal pruned trees theshows the maximum
absisse value x such that AU Cg(y) ≥ AU Cs(y) with a con-
ﬁdence coefﬁcient of 0.95 (under the normal assumption) for
every y ≤ x. For all smaller values of the negative ratio, the
global geometric ranking outperforms the other method (in
term of AUC).

We can see in Table 5 that for most bases, the global
geometric ranking methods is rather efﬁcient at the begin-
ning of the ROC curve, even when on the total range it per-
forms badly (like for the Pima database, see Table 3). The

Dataset

bupa
glass
iono
iris
letter
thyroid
optd.
pend.
pima
sat
segment.
sonar
vehicle
vowel
wdbc
wine

Reduced-error

pruning

Normal
pruning

1.75±0.23
4.21±0.44
0.04±0.29
3.71±0.25
0.19±0.09
3.62±0.39
0.67±0.06
0.34±0.03
2.59±0.26
1.09±0.08
7.78±0.72
3.02±0.29
0.53±0.09
3.83±0.3
3.75±0.22
5.11±0.4

0.88±0.09
3.39±0.34
0.51±0.17
3.31±0.26
0.36±0.07
2.70±0.29
0.43±0.04
0.28±0.02
0.87±0.07
0.89±0.06
4.83±0.54
2.81±0.21
0.60±0.07
2.85±0.26
2.14±0.14
3.14±0.26

Unpruned

0.69±0.09
2.61±0.31
0.49±0.18
2.92±0.25
0.13±0.02
2.33±0.27
0.27±0.03
0.20±0.02
0.55±0.05
0.45±0.04
4.34±0.49
2.70±0.2
0.49±0.05
2.56±0.23
2.04±0.14
2.80±0.24

Table 4: Absolute difference of the AUC between local geometric
ranking with standard metric and the best smoothing method. (All
mean values and standard deviations are ×100. Insigniﬁcant values
are italic. There is no bad value.)

experiment partially conﬁrms the theoretical viewpoint con-
cerning the fact that geometric score gives interesting results
when misclassiﬁed examples are near the decision bound-
ary. This is particularly true for the bupa (liver-disorder)
and ionosphere databases. Table 2 shows that these datasets
doesn’t verify the hypothesis of proximity of errors on a ma-
jority of samples, and actually the global geometric score give
bad results for these datasets.

Concerning the improvement of the geometric ranking
when the accuracy of the tree is better, the experiment is not
conclusive. If we compute Table 3 and Table 4 for a subset
of the samples, the best quartile for tree accuracy, the global
geometric ranking is not improved (results are not signiﬁ-
cant). But local geometric ranking gives always better results
than on the total sample, except on the glass and ionosphere
database (for which the hypothesis of proximity of errors is
not much improved on the subset of the samples).

5 Conclusion
We have presented in this article a geometric method to rank
cases that are classiﬁed by a decision tree. It applies to every
axis-parallel tree that classiﬁes examples with numerical at-
tributes. We were not concerned here with the problem of
growing the tree (problem with unbalanced datasets or dif-
ferent misclassiﬁcation costs which lead to pre-processing of
the data or new pruning methods). The geometric method
doesn’t depend on the type of splitting or pruning criteria that
is used to build the tree. It only depends on the shape of de-
cision boundary induced by the tree. It consists in ranking
the case according to their distance to the decision boundary,
taking into account the class of interest and the class that is
predicted by the decision tree. Theoretical arguments suggest
that this method is interesting when the misclassiﬁed exam-
ples lie near the decision boundary, and this was partially con-
ﬁrmed by the experimentation. The combination of geomet-

Dataset

bupa
glass
iono
iris
thyroid
optdigits
pendigits
pima
sat
segment.
sonar
vehicle
vowel
wdbc
wine

MM metric

Standard metric

m-estimate

Laplace m-estimate

Laplace

0.001
0.1
0.02
≥ 0.5
≥ 0.5
0.01
≥ 0.5
0.02
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5

0.001
0.1
0.02
≥ 0.5
≥ 0.5
0.01
≥ 0.5
0.35
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5

0.001
0.05
0.02
≥ 0.5
≥ 0.5
0.01
≥ 0.5
0.03
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5

0.001
0.05
0.02
≥ 0.5
≥ 0.5
0.01
≥ 0.5
0.37
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5
≥ 0.5

Table 5: Abscissa below which the global geometric ranking AUC
is always greater. (Bad results are bold).

ric ranking and smoothing methods almost always improve
the global ranking (measured with the AUC). Different kind
of experiment should be performed in order to compare geo-
metric ranling (and particularly local geometric ranking) to
NBTree or other algorithm: since the structure of the trees
are different, the choice of pruning method can be important.
The main limit of the method is that it is limited to numer-
ical attributes. It could be extended to ordered attributes, but
without the deﬁnition of a utility function it cannot be used
with attributes that have unordered modalities.

Further work is in progress in order to understand more
precisely when the geometric ranking should perform well.
Following the idea from [Smyth et al., 1995], we think that
density estimator could be used on the distance itself rather
than on the attribute of the cases, in order to deal with 1-
dimension estimator (which are very efﬁcient). Another in-
teresting point is the deﬁnition of a geometric score for real
multi-class problem (with no particular class of interest). Ac-
tually the algorithm that computes the distance to the decision
boundary computes already the distance of an example to the
different classes, so these distances could be used for that pur-
pose.

References
[Adams and Hand, 1999] N. M. Adams and D. J. Hand.
Comparing classiﬁers when the misallocation costs are un-
certain. Pattern Recognition, 32(7):1139–1147, 1999.

[Alvarez, 2004] Isabelle Alvarez. Explaining the result of
a decision tree to the end-user.
In Proceedings of the
16th European Conference on Artiﬁcial Intelligence, pages
119–128, Valencia, Spain, Aout 2004. Morgan Kaufmann.
[Blake and Merz, 1998] C.L. Blake and C.J. Merz. UCI

repository of machine learning databases, 1998.

[Bradley and Lovell, 1995] Andrew P. Bradley and Brian C.
Lovell. Cost-sensitive decision tree pruning: Use of the roc
curve. In Eighth Australian Joint Conference on Artiﬁcial
Intelligence, pages 1–8, 1995.

[Breiman et al., 1984] L. Breiman, J. H. Friedman, R. A. Ol-
shen, and C. J. Stone. Classiﬁcation and Regression Trees.
Wadsworth, Belmont, 1984.

[Cestnik, 1990] B. Cestnik. Estimating probabilities: A cru-
cial task in machine learning. In Proceedings of the Euro-
pean Conference on Artiﬁcial Intelligence, pages 147–149,
1990.

[Esposito et al., 1997] F. Esposito, D. Malerba, and G. Se-
meraro. A comparative analysis of methods for pruning
decision trees. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 19(5):476–491, 1997.

[Kohavi, 1996] Ron Kohavi. Scaling up the accuracy of
naive-bayes classiﬁers: a decision-tree hybrid. In Proceed-
ings of the Second International Conference on Knowl-
edge Discovery and Data Mining, pages 202–207, 1996.

[Ling and Yan, 2003] C. X. Ling and R. J. Yan. Decision tree
with better ranking. In Proceedings of the 20th Interna-
tional Conference on Machine Learning, pages 480–487,
2003.

[Murthy, 1998] S.K. Murthy. A automatic construction of
decision trees from data: A multi-disciplinary survey.
Data Mining and Knowledge Discovery, 2(4):345–389,
1998.

[Platt, 2000] J. Platt. Probabilistic outputs for support vec-
tor machines. In Bartlett P. Schoelkopf B. Schuurmans D.
Smola, A.J., editor, Advances in Large Margin Classi-
ﬁers, pages 61–74, Cambridge, Massachusetts, 2000. MIT
Press.

[Provost and Domingos, 2003] F. Provost and P. Domingos.
Tree induction for probability-based ranking. Machine
Learning, 52(3):199–215, 2003.

[Quinlan, 1993] J.R. Quinlan. C4.5: Programs for Machine

Learning. Morgan Kaufmann, San Mateo, 1993.

[Smyth et al., 1995] Padhraic Smyth, Alexander Gray, and
Usama M. Fayyad. Retroﬁtting decision tree classiﬁers us-
ing kernel density estimation. In International Conference
on Machine Learning, pages 506–514, 1995.

[Umano et al., 1994] M. Umano, K. Okomato, I. Hatono,
H. Tamura, F. Kawachi, S. Umezu, and J. Kinoshita. Fuzzy
decision trees by fuzzy id3 algorithm and its application to
diagnosis systems. In 3rd IEEE International Conference
on Fuzzy Systems, pages 2113–2118, 1994.

[Witten and Frank, 2000] I. Witten and E. Frank. Data Min-
ing: Practical Machine Learning Tools and Techniques
with Java Implementation. Morgan Kaufmann, 2000.

[Zadrozny and Elkan, 2001] Bianca Zadrozny and Charles
Elkan. Obtaining calibrated probability estimates from de-
cision trees and naive bayesian classiﬁers. In Proc. 18th
International Conf. on Machine Learning, pages 609–616.
Morgan Kaufmann, 2001.


Structure Inference for Bayesian Multisensory Perception and Tracking

Timothy M. Hospedales Joel J. Cartwright Sethu Vijayakumar
School of Informatics, University of Edinburgh, EH9 3JZ, Scotland, UK

t.hospedales@ed.ac.uk, j.j.cartwright@sms.ed.ac.uk, sethu.vijayakumar@ed.ac.uk

Abstract

We investigate a solution to the problem of multi-
sensor perception and tracking by formulating it in
the framework of Bayesian model selection. Hu-
mans robustly associate multi-sensory data as ap-
propriate, but previous theoretical work has fo-
cused largely on purely integrative cases, leaving
segregation unaccounted for and unexploited by
machine perception systems. We illustrate a uni-
fying, Bayesian solution to multi-sensor perception
and tracking which accounts for both integration
and segregation by explicit probabilistic reasoning
about data association in a temporal context. Unsu-
pervised learning of such a model with EM is illus-
trated for a real world audio-visual application.

1 Introduction
There has been much recent interest in optimal multi-sensor
fusion both for understanding human multi-modal perception
[Ernst and Banks, 2002; Alais and Burr, 2004] and for ma-
chine perception applications [Beal et al., 2003; Perez et al.,
2004]. Most of these have considered the simple cases in
which the observations are known to be generated from the
same latent source, and the task is to make the best estimate
of the latent source state by fusing the observations. (We call
models assuming such a fused structure pure fusion models.)
However, in most real world situations any given pair of ob-
servations are unlikely to have originated from the same latent
source. A more general problem in multi-sensor perception
is therefore to infer the association between observations and
any latent states of interest as well as any fusion (integration)
or ﬁssion (segregation) that may be necessary. This data as-
sociation problem has been of more long standing interest in
the radar community [Bar-Shalom et al., 2005]. The data as-
sociation may not merely be a nuisance variable required for
correct sensor combination. It can be of intrinsic interest for
understanding the data. For example, a key task in interpret-
ing a meeting for a human or machine is not just to infer who
was there and what was said, but to correctly associate visual
and acoustic observations to understand who said what.

In this paper, we illustrate the commonality of these multi-
sensor perception problems and provide a unifying, princi-
pled Bayesian account of their solution, reasoning explicitly

about the association of observations with latent states. More-
over, we illustrate that using the EM algorithm, inference can
be performed simultaneously with parameter estimation for
unsupervised learning of perceptual models.

2 Theory

Humans and machines equipped with multiple sensor modal-
ities need to combine information from various senses to ob-
tain an accurate uniﬁed perception of the world. Perception
requires computing tangible quantities of interest in the world
(e.g. people’s locations) as well as the association between
sensor observations (e.g. who said what). To formalize the
perceptual problems faced by a human or machine equipped
with multiple sensor modalities, we use a probabilistic gener-
ative modelling framework. The task of perception then be-
comes that of performing inference in the generative model,
where object states and data association are both inferred.

We can frame the inference of data association equivalently
as a model selection or a structure inference problem. A
graphical model for the process of generating observations in
two different modalities D = {x1, x2} from a single source
with latent state l is illustrated in Fig. 1(a). The source state
is drawn independently along with binary visibility/occlusion
variables (M1, M2) specifying its visibility in each modality.
The observations are then generated with xi being dependent
on l if Mi = 1 or on a background distribution if Mi = 0.
Alternately, all the structure options could be explicitly enu-
merated into four separate models, and the generation process
then ﬁrst selects one of the four generative models before se-
lecting l and generating the observations according to the de-
pendencies encoded in that model. Inference then consists of
computing the posterior over the latent state and the gener-
ating model (either as speciﬁed by the two binary structure
variables Mi or a single model index variable) given the ob-
servations. An observation in modality i is perceived as being
associated with (having originated from) the latent source of
interest with probability p(Mi = 1|D), which will be large if
the observation is likely under the foreground distribution and
small if it is better explained by the background distribution.

An Illustrative Example
To illustrate with a toy but concrete example, consider the
problem of inferring a single dimensional latent state l rep-
resenting a location on the basis of two point observations

IJCAI-07

2122

(a)

(b)

(c)

, X
X
 Correlated
2
1

Infer X1, X2 Visible

Integrative Post

Pure Fusion Posts

(d)

(e)

(f)

(g)

d
o
o
h

i
l

e
k
L

i

d
o
o
h

i
l

e
k
L

i

d
o
o
h

i
l

e
k
L

i

X
 Discrepant
2

X1,X2 Both Discrepant

d
o
o
h

i
l

e
k
L

i

X1,X2 Medium Discrepancy

)

|

D
M
(
p

)

|

D
M
(
p

)

|

D
M
(
p

)

|

D
M
(
p

Infer X
 Visible
1

)

|

D
L
(
p

)

|

D
L
(
p

Integrate X
 Only
1

Infer Neither Visible

Segregation: Prior State

Visibility Uncertain

)

|

D
L
(
p

)

|

D
L
(
p

State Uncertain

)

|

D
L
(
p

)

|

D
L
(
p

)

|

D
L
(
p

)

|

D
L
(
p

Location Error

Precision Error

Precision Error

L

M1

M2

L

L

Figure 1: (a) Graphical model to describe un-reliable gen-
eration of multi-modal observations xi, occlusion semantic
(b,c) generation with one or two source objects, multi-object
semantic (c-g) Inference in occlusion semantic model. Ob-
servations (d) x1, x2 strongly correlated, (e) x2 strongly dis-
crepant, (f) x1, x2 both strongly discrepant, (g) x1, x2 both
moderately discrepant. Likelihoods of the observations in
each of two modalities in black, prior in grey.

in separate modalities. For the purpose this illustration, let
l be governed by an informative Gaussian prior centered at
zero, i.e., p(l) = N (l|0, pl) and the binomial visibility vari-
ables have prior probability p(Mi) = πi. (Note that we use
precisions rather than covariances throughout). If the state
is observed by sensor i (Mi = 1) then the observation in
that modality is generated with precision pi, such that xi ∼
N (xi|l, pi). Alternately, if the state is not observed by the
sensor, its observation is generated by the background distri-
bution N (xi|0, pb), which tends toward un-informativeness
with precision pb → 0. The joint probability can then be writ-
ten as in Eq. 1. If we are purely interested in computing the
posterior over latent state, we integrate over models or struc-
ture variables. For the higher level task of inferring the cause
or association of observations, we integrate over the state to
compute the posterior model probability, beneﬁting from the
automatic complexity control induced by Bayesian Occam’s
razor [MacKay, 2003]. Deﬁning for brevity mi ≡ (Mi = 1)
and mi ≡ (Mi = 0), we can write down the posteriors as in
Eq. 2.

p(D, l, M) = N (x1|l, p1)M1 N (x1|0, pb)(1−M1)N (x2|l, p2)M2

„

1
2

·N (x2|0, pb)(1−M2)N (l|0, pl)p(M1)p(M2)

p(m1, m2|D) ∝ N (x1|0, pb)N (x2|0, pb)

«

p(m1, m2|D) ∝ exp

−

x2

1p1pl/(p1 + pl)

N (x2|0, pb)

p(m1, m2|D) ∝
x2
1p1(p2 + pl) − 2x1x2p1p2 + x2

2p2(p1 + pl)

»

exp

−

1
2

p1 + p2 + pl

(1)

(2)

–

Intuitively,

the structure posterior (Eq. 2) is dependent
on the relative data likelihood under the background and

marginal foreground distributions. The posterior of the fully
segregative model depends on the background distributions
and hence tends toward being independent of the data except
via the normalization constant. In contrast, the posterior of
the fully integrative model depends on the three way agree-
ment between the observations and the prior. The assumption
of a one dimensional Gaussian prior and likelihoods is to fa-
cilitate illustrative analytical solutions; this is not in general a
restriction of our framework as can be seen in Sec. 3.

pl|x

ˆ
l = p1x1+p2x2

Fig. 1 illustrates a schematic of some informative types of
behavior produced by this model. If the data and the prior are
all strongly correlated (Fig. 1(d)) such that both observations
are inferred with near certainty to be associated with the latent
source of interest, the fused posterior over the location is ap-
proximately Gaussian with p(l|x1, x2) ≈ N (l|ˆ
l, pl|x) where
pl|x = p1 + p2 + pl,
. If x2 is strongly dis-
crepant with x1 and the prior (Fig. 1(e)), it would be inferred
that sensor 2 was occluded and its observation irrelevant. In
this case, the posterior over the location is again near Gaus-
ˆ
l = p1x1
sian but fusing only x1 and the prior; pl|x = p1+pl,
.
pl|x
If both x1 and x2 are strongly discrepant with each other and
the prior (Fig. 1(f)), both observations are likely to be back-
ground originated, in which case the posterior over the latent
ˆ
l = 0. Finally, if the corre-
state reverts to the prior pl|x = pl,
lation between the observations and the prior is only moderate
(Fig. 1(g)) such that the posterior over the structural visibility
variables is not near certain, then the posterior over the la-
tent state is a (potentially quad-modal) mixture of Gaussians
corresponding to the 4 possible models. For real world data,
occlusion, or other cause for meaningless observation is al-
most always possible, in which case assuming a typical pure
fusion model (equivalent to constraining M1 = M2 = 1) can
result in dramatically inappropriate inference (Fig. 1(box)).

t+1|l

t+1
t) and p(M
i

Incorporating Temporal Dependencies
Now we consider the case where the latent state of inter-
est and data association are correlated in time. A graphical
model to describe the generation of such data is illustrated
in Fig. 2(a). The state l and model variables Mi are each
connected through time, producing a factorial hidden Markov
model [Ghahramani and Jordan, 1997]. To generate from this
model, at each time t the location and model variables are
selected on the basis of their states at the previous time and
t
i). Con-
the transition probabilities p(l
ditional on these variables, each observation is then generated
in the same way as for the previous independently and iden-
tically distributed (IID) case. Inference may then consist of
computing the posterior over the latent variables at each time
) (i.e.,
given all T available observations, p(l
smoothing) if processing is off-line. If the processing must
be on-line, the posterior over the latent variables given all
1:t
2 ) (i.e., ﬁlter-
the data up to the current time p(l
ing) may be employed. Multi-modal source tracking is per-
formed by computing the posterior of l, marginalizing over
possible associations. We have seen previously that the pos-
terior distribution over location at a given time is potentially
non-Gaussian (Fig. 1(e)). To represent such general distribu-
tions, we can discretize the state space of l. In this simple

1:T
t|x
1

1:T
, x
2

|M

t

, M

t|x

1:t
1 , x

t

, M

IJCAI-07

2123

case, exact numerical inference on the discretized distribu-
t) and
tion is tractable. Given state transition matrices p(l
t), we can write down recursions for inference in
p(M
this factorial hidden Markov model in terms of the posteriors
α

1:t) and γ

t (cid:2) p(l

t (cid:2) p(l

1:T ) as

t+1|M

t
1,2|D

t+1|l

t

, M

t
1,2|D

t

, M

αt ∝

X

p(Dt|lt, M t

1,2)p(lt|lt−1)

lt−1 ,M t−1
1,2

γt ∝

X

P

lt+1,M t+1
1,2

lt,M t

1,2

2Y

i=1

p(M t

i |M t−1

i

)αt−1

(3)

(4)

Q2

Q2

p(lt+1|lt)

i=1 p(M t

i |M t−1

i

p(lt+1|lt)

i=1 p(M t

)αt
i |M t−1

i

γt+1

)αt

t

, p), D = {x
2}T
t
t
1, x

Filtering makes use of the forward α recursion in Eq. 3
and smoothing the backward γ recursion in Eq. 4, which are
analogues of the α and γ recursions in standard HMM in-
ference. The beneﬁts of temporal context for inference of
source state and data association are illustrated in Fig. 2(b-
g). Fig. 2(b) illustrates data from a series of T observations,
i ∼ N (l
t
t=1, in two independent modal-
x
ities, of a continuously varying latent source l. These data
include some occlusions/sensor failures, where the observa-
tion(s) are generated from a background distribution, and an
unexpected discontinuous jump of the source. The temporal
state evolution models for l and M are simple diffusion mod-
els. The robustness of source location inference by a pure
fusion model without temporal context (Fig. 2(e)) is very lim-
ited, as it must always averages over observations, which is
inappropriate when they are actually disassociated. A data
association model (Fig. 2(f)) is slightly more robust, inferring
that the generation structure was likely to have been differ-
ent when the observations are discrepant. However, without
temporal context, it cannot identify which observation was
discrepant, and hence produces a non-Gaussian, multi-modal
posterior for l. Including some temporal history, an on-line
ﬁltering data association model can infer which observations
are discrepant, and discount them, producing much smoother
inference (Fig. 2(g)). In this case, after the discontinuity in
state, the fully disassociated observation structure is inferred
and based on the temporal diffusion model, approximately
constant location is inferred until enough evidence is accumu-
lated to support the new location. Finally, an off-line smooth-
ing data association model (Fig. 2(c)) infers a robust, accurate
trajectory. For this case, the marginal posterior of the associa-
tion variables is shown in Fig. 2(d). The illustrative scenarios
discussed here generalize in the obvious way to more obser-
vations. With many sensors, the dis-association of a smaller
number of discrepant sensors can be inferred even without
prior information. In a pure fusion scheme, a single highly
discrepant sensor can throw off the others during averaging.

An Illustrative Example with Multiple Objects
There is another simple way in which two multi-modal point
observations can be generated, i.e., each could be gener-
ated by a separate source instead of a single source. The
choice of the multi-source versus the fused generating model

(a)

(b)

n
o

i
t

a
c
o
L

(c)

n
o

i
t

a
c
o
L

(d)

)

D
|
s
b
O
(
p

Actual Input

Pure Fusion Posterior

(e)

n
o

i
t

a
c
o
L

Smoothed Posterior

IID Posterior

(f)

n
o

i
t

a
c
o
L

Model Posterior

Filtered Posterior

(g)

n
o

i
t

a
c
o
L

Time

Time

Figure 2: (a) Graphical model to describe generation of ob-
servations xi with temporal dependency. (b) Synthetic input
dataset in modality x1 and x2. (c) Posterior probability of l
and (d) posterior probability of model structure for the tem-
poral data association model. Posterior probability of l in (e)
pure fusion model (f) IID data association model (g) ﬁltered
data association model.

(Fig. 1(b)) can also be expressed compactly as structure in-
ference as before by also using two latent state variables as in
the single source case, but requiring equality between them
if M = 1 and independence if M = 0 (Fig. 1(c)). It is pos-
sible to enumerate all ﬁve possible model structures and per-
form the Bayesian model selection given the data. However,
frequently the semantics of a given perceptual problem cor-
respond to a prior over models which either allows the four
discussed earlier (“occlusion semantic”) or a choice between
one or two sources (“multi-object semantic”). The occlu-
sion semantic arises for example, in audio-visual processing
where a source may independently be either visible or audi-
ble. The multi-object semantic arises, for example in some
psychophysics experiments[Shams et al., 2000] where both
sensors have deﬁnitely observed an interesting event, and the
task is to decide what they observed, which is conditionally
dependent on whether they observed the same source or not.

We will now illustrate the latter case with a toy but concrete
example of generating observations in two different modali-
ties x1, x2 which may both be due to a single latent source
(M = 1), or two separate sources (M = 0). Using vec-
tor notation, the likelihood of the observation x = [x1, x2]T
is N (x|l, Px) where
given the latent state l = [l1, l2]T

IJCAI-07

2124

Px = diag([p1, p2]). Let us assume the prior distribu-
tions over the latent locations are Gaussian but tend to un-
informativeness. In the multi-object model the prior over lis
p(l|M = 0) = N (l|0, P0) is uncorrelated, so P0 = p0I
and p0 → 0. In the single object model, the prior over lis
p(l|M = 1) = N (l|0, P1) requires the lis to be equal so P1
is chosen to be strongly correlated. The joint probability of
the whole model and the structure posterior are given in Eq. 5.

p(x, l, M ) = N (x|l, Px)N (l|0, P0)(1−M )N (l|0, P1)M p(M )

Z

p(M |x) ∝

N (x|l, Px)N (l|0, P0)(1−M )N (l|0, P1)M p(M )

l

p(M = 0|x) ∝ N (x|0, (P−1
p(M = 1|x) ∝ N (x|0, (P−1

x + P−1
x + P−1

0 )−1)p(M = 0)
1 )−1)p(M = 1)

(5)

A schematic of interesting behavior observed is illustrated
in Fig. 3. If x1 and x2 are only slightly discrepant (Fig. 3(a)),
then the single object model is inferred with high probabil-
ity. The posterior over l is also strongly correlated and Gaus-
sian about the point of the fused interpretation; p(l|x) ≈
N (l|ˆl, Pl|x) where
Pxx, Pl|x = Px+P1. The loca-
tion marginals for each li are therefore the same and aligned
ˆl. If x1 and x2 are highly discrepant (Fig. 3(b)), then the
at
two object model is inferred with high probability. In this
case the posterior p(l|x) is spherical and aligned with the ob-
servations themselves rather than a single fused estimate; i.e.
ˆl = P−1
l|x

Pxx ≈ x, Pl|x = Px + P0.

ˆl = P−1
l|x

The inferences discussed so far have been exact. There
are various potential approximations such as computing the
location posterior given the MAP model, which may be ac-
ceptable, but which crucially misrepresents the state poste-
rior for regions of input space with intermediate discrepancy
(c.f. Fig. 1(d)). Alternately, the model probability could be
approximated using a MAP or ML estimate of the state. The
agreement between the Bayesian and MAP solution depends
on how sharp the state posterior is, which depends on both
the agreement between the observations and the precision of
their likelihoods. Using the ML estimate of the state will not
work at all as the most complex model is always selected.

Previous probabilistic accounts of human multi-sensory
combination (e.g. [Ernst and Banks, 2002; Alais and Burr,
2004]) are special cases of our theory, having explicitly or im-
plicitly assumed a pure fusion structure. [Triesch and von der
Malsburg, 2001] describe a heuristic democratic adaptive cue
integration perceptual model, but again assume a pure fusion
structure. Hence these do not, for example, exhibit the ro-
bust discounting (sensory ﬁssion or segregation) of strongly
discrepant cues observed in humans [Ernst and Banks, 2002].
As we have seen, such ﬁssion is necessary for perception in
the real world as outliers can break pure fusion schemes. We
provide a principled probabilistic, adaptive theory of tempo-
ral sensor combination which can account for fusion, ﬁssion
and the spectrum in-between. The combination strategy is
handled by a Bayesian model selection without recourse to
heuristics, and the remaining parameters can be learned di-
rectly from the data with EM. A more challenging question is
that of realistic multi-dimensional observations which depend

Figure 3: Inference in multi-object semantic toy model. (a)
For correlated inputs, x1 ≈ x2, the presence of one objects is
inferred and its location posterior is the probabilistic fusion of
the observations. (b) For very discrepant inputs, x1 (cid:6)= x2, the
presence of two objects is inferred and the location posterior
for each is at the associated observation.

in complex ways on the latent state, a topic we will address
in the real world application discussed next.

3 Bayesian Multi-sensory Perception for

Audio-Visual Scene Understanding

To illustrate the application of these ideas to a real, large scale
machine perception problem, we consider a task inspired by
[Beal et al., 2003]; that of unsupervised learning and infer-
ence with audio-visual (AV) input. [Beal et al., 2003] demon-
strated inference of an AV source location and learning of
its auditory and visual templates based on correlations be-
tween the input from a camera and two microphones - use-
ful for example, in teleconferencing applications . The AV
localization part of this task is similar to the task required
in psychophysics experiments such as [Alais and Burr, 2004]
where humans are also reported to exhibit near Bayes optimal
sensor fusion. We now tackle the bigger scene understand-
ing problem of inferring how the AV data should be associ-
ated through time (pure fusion was previously assumed), i.e,
whether the source should be associated with both modalities,

IJCAI-07

2125

H = {a, v, t, l, W, Z}J

j=1 factorizes as

p(D, H) =

p(lj+1|lj )p(W j+1|W j)p(Z j+1|Z j )

p(x1|W, a)p(x2|W, a, t)p(a)p(t|l)p(v)p(y|Z, v, l)

 

=

j=1

J −1Y
JY
JY

j=1

·

J −1Y

N (x1|a, v1)W N (x1|0, σ1)(1−W )N (a|0, η)

j=1
· N (x2|Tta, v2)W N (x2|0, σ2)(1−W )N (τ |αl + β, ω)
· N (y|Tlv, ΨI)Z N (y|γ1, I)(1−Z)N (v|μ, φ)

”

·

Γlj ,lj+1 Θwj ,wj+1 Ωzj ,zj+1

(6)

j=1

3.2

Inference

Consider ﬁrst the inference for a single frame of data. The
posterior marginal of interest for this task is that of the dis-
crete location and visibility structure variables p(l, W, Z|D).
Because of the linear-Gaussian structure of the model, the
latent appearance variables a and v can be analytically in-
tegrated out, leaving only the inter-microphone delay t to
be summed out numerically when computing p(l, W, Z|D).
Conditioned on the fused model, and other discrete variables
(Z = 1, W = 1, t, l) the posteriors over the latent signals are
Gaussian, N (a|μa|x,t, νa) and N (v|μv|y,l, νv), with preci-
sion and mean given by μa|x,t = ν
t x2),
2
2
νa = η + λ
l Ψy),
2ν2, μv|y,l = ν
1ν1 + λ
νv = φ + Ψ. The marginal video likelihood is also Gaussian
with μy|l = Tlμ, νy|l = (Ψ−1 + Tlφ
. Expressions
for the likelihood of the fully fused model and the source lo-
cation (Eq. 7) and the likelihood of the fully ﬁssioned model
(Eq. 8) can be derived in terms of these statistics. Deﬁning
again zi ≡ (Zi = 1) and zi ≡ (Zi = 0) etc, we can write

(λ1ν1x1 + λ2ν2TT
(φμ + TT

l )−1

−1TT

−1
v

−1
a

p(D|w, z, l) ∝

v

p(y, v|l, z)

p(x1, x2, a, t|l, w)

∝ N (y|μy|l, νy|l)

t

a

p(t|l, D)exp(μT

a|t,xνaμa|t,x) (7)

t
p(D|w, z) ∝ p(x|w)p(y|z)

= N (x1|0, σ1I)N (x2|0, σ2I)N (y|γ1, I)

(8)

For a single observed modality, the likelihood is a mixture
of terms from Eqs. 7 and 8, along the lines of Eq. 2. To infer
the posterior over location and observability for IID frames,
these likelihoods can be combined with the discrete prior dis-
tributions over the location and observabilities. To infer the
probability of location and observability over time given the
data, the likelihoods are used in recursions exactly like those
in Eqs. 3 and 4.

the

3.3 Learning
=
All
{λ1,2, ν1,2, η, α, β, ω, πl, μ, φ, Ψ, Γ, Θ, Ω, πw, πz, γ, , σ1,2}
are jointly optimized by a standard EM procedure of al-
ternately inferring the posterior distribution q(H|D) over

this model

parameters

in

θ

Z

Z

X

X

Figure 4: Graphical model for AV data generation.

or only one, or if there is no source present at all.

3.1

Introduction

A graphical model to describe the generation of a single
frame of AV data D = {x1, x2, y} is illustrated in Fig. 4.
A discrete translation l representing the source state is se-
lected from its prior distribution πl and its observability in
each modality (W, Z) are selected from their binomial pri-
ors. For simplicity, we only consider source translation along
the azimuth. Consider ﬁrst the all visible case (W, Z = 1).
The video appearance v is sampled from a diagonal Gaus-
sian distribution N (v|μ, φ) with parameters deﬁning its soft
template. The observed video pixels are generated by sam-
pling from another Gaussian N (y|Tlv, ΨI) the mean of
which is the sampled appearance, translated by l using the
transformation matrix Tl. The latent audio signal a is sam-
pled from a zero mean, uniform covariance Gaussian i.e.,
N (a|0, ηI). The time delay between the signals at each mi-
crophone is drawn as a linear function of the translation of
the source N (t|αl + β, ω). Given the latent signal and the
delay, the observation xi at each microphone is generated by
sampling from a uniform diagonal Gaussian with the mean
a, with x2 shifted t samples relative to x1; N (x1|a, v1I),
N (x2|Tta, v2I). If the video modality is occluded (Z = 0),
the observed video pixels are drawn from a Gaussian back-
ground distribution N (y|γ1, I) independently of l and au-
dio data. If the audio modality is silent (W = 0), the sam-
ples at each speaker are drawn from background distributions
N (xi|0, σiI) independently of each other, l and the video.

To describe the generation of a series of correlated frames,
the IID observation model in Fig. 4 is replicated and a fac-
tored Markov model is deﬁned over the location and associ-
ation variables (l, W, Z) exactly as the toy model was devel-
oped previously (refer Fig. 2(a)). Using j to index time, the
state evolution distribution over the location shift is deﬁned in
j) = Γlj ,lj+1 , where the subscripts
the standard way p(l
pick out the appropriate element of the matrix Γ. The observ-
j) =
ability transitions are deﬁned similarly as p(W
j) = Ωzj ,zj+1 . Suppressing index-
Θwj ,wj+1 and p(Z
ing by j for clarity, the joint probability of the model in-
cluding visible D = {x1, x2, y}J
j=1 and hidden variables

j+1|W

j+1|Z

j+1|l

IJCAI-07

2126

(cid:2)
H q(H|D)lg

hidden variables H given the observed data D, and opti-
mizing the expected complete log likelihood or free energy
p(H,D)
∂
q(H|D) . As this is a complex model of
∂θ
many parameters, in the interest of space, we present just
two informative updates1. Eq. 9 gives the update for the
mean μ of the source visual appearance distribution. This
j
is deﬁned in terms of the posterior mean μ
v|y,l of the video
appearance given the data D for each frame j and translation
l, as inferred during the E step:

X

μ ←

q(lj , z

j|D)μj

v|y,l

/

q(z

j|D)

(9)

j,l

j

X

X

Intuitively, the result is a weighted sum of the appearance
inferences over all frames and transformations, where the
weighting is the posterior probability of transformation and
visibility in each frame. The scalar precision parameter of
background noise is given by Eq. 10, where Nf speciﬁes the
number of samples per audio frame.

X

σ−1
i ←

q(wj|D)(xj

i )T xj

i /Nf

q(wj|D)

(10)

j

j

Again, it is intuitive that the estimate of the background vari-
ance should be a weighted sum of square signals at each
frame where the weighting is the posterior that the source was
silent in that frame. In an IID context, the posterior marginals
j|D), are given visible variable,
to weight with, e.g. p(l
In the Markov model context, the posterior marginal
D
given the whole available data set D is used.

, z

.

j

j

3.4 Demonstration

Results for an AV sequence after 25 cycles of EM are illus-
trated in Fig. 5. In this sequence, the user is initially walk-
ing and talking, is then occluded behind another person while
continuing to speak, and then continues to walk while re-
maining silent. Fig. 5(a) illustrates three representative video
frames from each of these segments with the inferred data
association and location superimposed on each.

Tracking with the IID, pure fusion model

To illustrate tracking behavior, the MAP rather than full lo-
cation posterior is shown for clarity. In an IID pure fusion
model (constrained such that W, Z = 1 and with prior πl
instead of transition matrix Γ) the location inference is cor-
rect where the multi-modal observations are indeed associ-
ated (Fig. 5(c)). The video modality dominates the fusion as it
is much higher precision (i.e., the likelihood function is much
sharper), and the posterior is still therefore correct during the
visible but silent period where peaks in the audio likelihood
are spurious. However, while the person in the video fore-
ground is occluded but speaking, the next best match to the
learned dark foreground template usually happens to be the
ﬁling cabinet in the corner. With pure fusion, the incorrect
but still fairly sharp video likelihood still dominates the audio
likelihood, resulting in an incorrect posterior.

1Complete derivations, video clips & matlab code are available

at http://homepages.inf.ed.ac.uk/s0238587/

Figure 5: AV data association & inference results. (a) Video
samples and (b) audio data from a sequence where the user
is ﬁrst visibly walking and speaking, then occluded but still
speaking, and ﬁnally visible and walking but silent. (c) In-
ferred MAP location with IID pure fusion model, (d) IID
data association model and (e) full temporal data association
model. Inference based on audio observation alone is shown
in circles, video observation alone in triangles, and combined
inference by the dark line. (f) Posterior probability of visi-
bility (dark) and audibility (light) during the sequence. (g)
Initial and (h,i) ﬁnal video appearance after learning. (j) Fi-
nal location state transition matrix after learning.

Tracking with IID data association model
In an IID data association model (Fig. 5(d)) the video modal-
ity is correctly inferred with high conﬁdence to be disassoci-
ated during the occluded period. The ﬁnal posterior is there-
fore based mostly on the audio likelihood, and is generally
peaked around the correct central region of the azimuth. The
outlier points here have two causes. As speech data is in-
trinsically intermittent, both modalities occasionally have low
probability of association, during which times the ﬁnal es-
timate is still inappropriately attracted to that of the video
modality as in the pure fusion case. Others are simply due
to the lower inherent precision of the audio modality.

Tracking with smoothed data association model
The data association posterior (W, Z) (Fig. 5(f)) correctly
represents the visibility and audibility of the target at the
appropriate times, as with the IID case. This enables in-

IJCAI-07

2127

formation from the appropriate sensor(s) to be used at each
time. With the addition of temporal context, tracking based
on the noisy and intermittent audio modality is much more
reliable in the difﬁcult period of visual occlusion. The user
is now reliably and seamlessly tracked during all three do-
mains of the input sequence (Fig. 5(g)). The inferred data as-
sociation is used to label the frames in (Fig. 5(a)) with the
user’s speaking/visibility status. To cope with intermittent
cues, previous multi-modal machine perception systems in
this context have relied on observations of discrepant modal-
ities providing uninformative likelihoods [Perez et al., 2004;
Beal et al., 2003]. This may not always be the case, and
was not, for example in our video sequence where only the
data association models succeeded during the video occlu-
sion. This model retains properties of the inspiring formu-
lation [Beal et al., 2003] which allow most of the expensive
E and M step computations in the observation model to be
expressed in terms of FFTs. Using 120x100 pixel images
and 1000 sample audio frames, our matlab implementation
can perform on-line real time (ﬁltered) tracking at 50fps after
(smoothed) learning, which proceeds at 10fps.

4 Discussion

In this paper, we introduced a principled formulation of multi-
sensor perception and tracking in the framework of Bayesian
inference and model selection in probabilistic graphical mod-
els. Pure fusion multi-sensor models have previously been
applied in machine perception applications and understand-
ing human perception. However, for sensor combination with
real world data, extra inference in the form of data associa-
tion is necessary as most pairs of signals should not actually
be fused. In many cases, inferring observation association is
in itself an important goal for understanding structure in the
data. For example, a speech transcription model should not
associate nearby background speech of poorly matching tem-
plate and uncorrelated spatial location with the visible user
when he is silent. In our application the model “knows” if
observations arise from the source of interest by explicitly
inferring association, so it can for example, start recording
when the user enters the scene or begins speaking.

In radar tracking and association, some work[Stone et al.,
1999] uses similar techniques to ours, however popular meth-
ods [Bar-Shalom et al., 2005] tend to be more heuristic, use
stronger assumptions and approximations (e.g. Gaussian pos-
teriors) and use highly pre-processed point-input data. One
interesting contrast between these candidate detection based
approaches and our generative model approach is that we
avoid the expensive within-modality data association problem
typical of radar. This also enables use of signature or template
information in a uniﬁed way along with cross-modality cor-
relation during inference, which is exploited to good effect in
our AV application.

Investigations of human multi-sensory perception have re-
ported robustness to discrepant cues [Ernst and Banks, 2002]
but principled theory to explain this has been lacking. We
envisage that our theory can be used to understand a much
greater range of integrative and segregative perceptual phe-
nomena in a uniﬁed way. Performing psychophysical experi-

ments to investigate whether human perceptual association is
consistent with the optimal theory described here is a major
research theme which we are currently investigating.

In the context of machine perception, the type of model de-
scribed generalizes existing integrative models and provides
a principled solution to questions of sensor combination in-
cluding signature, fusion, ﬁssion and association. As our
AV application illustrates, computing the exact posterior over
source state and data association for real problems, even be-
fore applying approximations, is potentially even real-time.
The major complicating extension not considered in detail
here, is that of multiple sources. In this case, the computation
required for exhaustive reasoning grows exponentially in the
maximum number of objects; so for more than a few objects
the simple strategy employed here is not viable. For these
problems, we are investigating using approximate greedy in-
ference to identify the objects one at a time in order of best
correlation along the lines of [Williams and Titsias, 2004].

References
[Alais and Burr, 2004] David Alais and David Burr. The ventrilo-
quist effect results from near-optimal bimodal integration. Curr
Biol, 14(3):257–262, Feb 2004.

[Bar-Shalom et al., 2005] Y. Bar-Shalom, T. Kirubarajan, and
X. Lin. Probabilistic data association techniques for target track-
ing with applications to sonar, radar and eo sensors.
IEEE
Aerospace and Electronic Systems Magazine, 20(8):37–56, 2005.

[Beal et al., 2003] Matthew J. Beal, Nebojsa Jojic, and Hagai At-
tias. A graphical model for audiovisual object tracking. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
25(7):828–836, July 2003.

[Ernst and Banks, 2002] Marc O. Ernst and Martin S. Banks. Hu-
mans integrate visual and haptic information in a statistically op-
timal fashion. Nature, 415:429–433, 2002.

[Ghahramani and Jordan, 1997] Zoubin Ghahramani and Michael
Jordan. Factorial hidden markov models. Machine Learning,
29:245–273, 1997.

[MacKay, 2003] David MacKay.

Information Theory, Inference,

and Learning Algorithms. Cambridge University Press, 2003.

[Perez et al., 2004] Patrick Perez, Jaco Vermaak, and Andrew
Blake. Data fusion for visual tracking with particles. Proceedings
of the IEEE, 92(3):495–513, 2004.

[Shams et al., 2000] Ladan Shams, Yukiyasu Kamitani, and Shin-
suke Shimojo. Illusions: What you see is what you hear. Nature,
408:788, December 2000.

[Stone et al., 1999] Lawrence D. Stone, Carl A. Barlow, and
Thomas L. Corwin. Bayesian Multiple Target Tracking. Artech
House, 1999.

[Triesch and von der Malsburg, 2001] J. Triesch and C. von der
Malsburg. Democratic integration: self-organized integration of
adaptive cues. Neural Comput, 13(9):2049–2074, Sep 2001.

[Williams and Titsias, 2004] Christopher K I Williams

and
Michalis K Titsias. Greedy learning of multiple objects in
images using robust statistics and factorial learning. Neural
Comput, 16(5):1039–1062, May 2004.

IJCAI-07

2128


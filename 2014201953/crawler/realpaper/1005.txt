Speaker-Invariant Features for Automatic Speech Recognition

S. Umesh, D. R. Sanand and G. Praveen

Department of Electrical Engineering

Indian Institute of Technology

Kanpur 208 016, INDIA

{sumesh,drsanand,gpraveen}@iitk.ac.in

Abstract

In this paper, we consider the generation of features
for automatic speech recognition (ASR) that are ro-
bust to speaker-variations. One of the major causes
for the degradation in the performance of ASR sys-
tems is due to inter-speaker variations. These vari-
ations are commonly modeled by a pure scaling
relation between spectra of speakers enunciating
the same sound. Therefore, current state-of-the art
ASR systems overcome this problem of speaker-
variability by doing a brute-force search for the op-
timal scaling parameter. This procedure known as
vocal-tract length normalization (VTLN) is compu-
tationally intensive. We have recently used Scale-
Transform (a variation of Mellin transform) to gen-
erate features which are robust to speaker variations
without the need to search for the scaling param-
eter. However, these features have poorer perfor-
mance due to loss of phase information.
In this
paper, we propose to use the magnitude of Scale-
Transform and a pre-computed “phase”-vector for
each phoneme to generate speaker-invariant fea-
tures. We compare the performance of the proposed
features with conventional VTLN on a phoneme
recognition task.

1 Introduction
Inter-speaker variation is a major cause for the degradation
in the performance of Automatic Speech Recognition (ASR).
This is especially important in speaker-independent (SI) ASR
systems which are typically built to handle speech from any
arbitrary unknown speaker, e.g. in applications such as direc-
tory assistance. As a rule of thumb speaker-dependent (SD)
ASR systems have half the error rates when compared to SI
systems for the same task. This difference in performance
suggests that the performance of SI systems can be signif-
icantly improved if we can account for inter-speaker varia-
tions. Note that in practice, we cannot directly build SD sys-
tems for large vocabulary speech recognition tasks, since the
speech data required from that speaker will be too enormous
even for a co-operative user.
Instead, we build SI systems
and then “adapt” the system for a particular speaker to make
it speaker-dependent. Hence, irrespective of the application,

Smoothed Spectra for /aa/ 

Male Speaker
Female Speaker

x 108

6

5

4

3

2

1

0

0

50

100

150

200

250

300

Figure 1: Spectra of 2 speakers for one “frame” of /aa/

there is a lot of interest in improving the performance of SI
systems by accounting for inter-speaker variability.

In general, two speakers enunciating the same sound have
very different pressure waveforms and the resulting spectra
are very different. Fig. 1 shows the smoothed spectra (after
smoothing out pitch) for two speakers enunciating the vowel
/aa/. As seen from the ﬁgure the two spectra are very differ-
ent even though it is the same vowel spoken by two speak-
ers. Since ASR systems use features extracted from the spec-
tra, the features are themselves different for the same sound
enunciated by two speakers. Therefore for an automated sys-
tem to recognize these different features as belonging to the
same sound is difﬁcult leading to a degradation in perfor-
mance. Note that there will be a certain amount of variability
for the same sound spoken by the same person, i.e.
intra-
speaker variability, which is handled by the statistical model.
However, in general inter-speaker variability is substantially
larger than intra-speaker variability, leading to “coarse” sta-
tistical models and hence increased confusion between the
sound classes.

There is a lot of research that is being done in trying to
understand the mathematical relationship between spectra of
two speakers enunciating the same sound. This has impor-

IJCAI-07

1738

tant implications in other areas apart from speech recogni-
tion, such as in vowel perception, hearing-aid design and
speech pathology. One of the major causes for inter-speaker
variations is attributed to the physiological differences in the
vocal-tracts of the speakers. Based on this idea, in ASR a
commonly used model to describe the relation between spec-
tra of two speakers enunciating the same sound is given by

SA(f ) = SB(αf )

(1)
where SA(f ) and SB(f ) are the spectra of speakers A and B
respectively; and α denotes the uniform (constant) frequency-
warp factor. We refer to the above model as linear-scaling
model, since the two spectra are just scaled versions of each
other. The motivation for linear scaling comes from the fact
that to a first-order approximation, the vocal tract shape can
be assumed to be a tube of uniform cross-section and for this
simplifying approximation, the resonant frequencies (which
characterize different sounds) are inversely proportional to
vocal-tract length ([Wakita, 1977]). Therefore, differences in
vocal-tract lengths manifest in scaling of resonant frequencies
with the scaling being inversely proportional to the vocal-tract
length.

SpeechSignal

Preemphasis

and

Windowing

Subframing

and

Hamming
Windowing

Averaged

Autocorrelation

Estimation

Nonuniform

DFT

(Log spaced)

Logmagnitude

IDFT

Magnitude

STCC

In practical ASR systems, we have access only to the
acoustic data from the speaker and therefore do not have any
idea of the speaker’s vocal-tract length. Therefore, we do a
brute-force search for the optimal scaling-factor, α by trying
out different values of α. The optimality criterion is based
on maximizing the likelihood with respect to the SI Hid-
den Markov Model (HMM). Since in ASR we use features
derived from the spectra, for each value of α we scale the
frequency-spectra appropriately and then compute the fea-
tures. Therefore for each value of α, we recompute the fea-
ture of the ith utterance, X α
i , for that scale-factor. We then
ﬁnd the optimal scale-factor by
ˆαi = arg max

|λ, Wi)

(2)

Pr(X α
i

α

where α represents scale factor. λ is the HMM model and
Wi is the transcription of the utterance. Due to physiological
constraints of the human vocal-tract apparatus, α lies in the
range of 0.80 to 1.20. Therefore, in state-of-the-art ASR sys-
tems, we compute the features for different values of α from
0.80 to 1.20 in steps of 0.02 and ﬁnd the optimal α using
Eq. 2. This is a computationally expensive process since for
every frame the features have to be computed 21 times corre-
sponding to different α, before ﬁnding the optimal feature for
that frame after a maximum-likelihood (ML) search. Hence,
a lot of research is being done to have alternate normalization
schemes that are computationally efﬁcient.

Recently, an alternate method for generating features that
are invariant to speaker-variations due to the linear scaling
(see Eq. 1) was proposed [Umesh et al., 1999].
In this
method, the features are insensitive to the scale-factor α and
hence there is no need to search for the optimal α. We de-
scribe the method in the next section.

2 Review of Scale-Invariant Features using

Scale-Transform

In the linear-scaling model of Eq. 1, the speaker-dependent
warp-factor, α is a multiplicative constant and is independent

Figure 2: Block diagram for the computation of STCC

of frequency. For such a model, log-warping the frequency-
axis of the spectra of speakers separates the linear scaling fac-
tor as a translation factor in log-warped spectral domain, i.e.,

sa(λ) = SA(f = eλ) = SB(α eλ)

= SB(eλ+ln α) = sb(λ + ln α).

(3)

Hence, in the log-warped domain, λ, the speaker-dependent
scale-factor separates out as a translation factor ln α for the
linear scaling model of Eq. 1.

Let the Fourier-transform of the log-warped spectra sa(λ)
be denoted as Da(c) and that of sb(λ) be denoted as Db(c),
i.e.

sa(λ) DF T⇐⇒ Da(c)

and sb(λ) DF T⇐⇒ Db(c)

(4)

Using the property of the Fourier-transform that a translation
in one domain appears only as linear-phase in the other do-
main we have,

Da(c) = Db(c)e−jc ln α

(5)

Therefore, the speaker-dependent term α appears only in the
phase term. Hence the magnitudes of Da(c) and Db(c) are
identical and independent of the speaker-variations i.e.

|Da(c)| = |Db(c)|.

(6)

We can therefore use the magnitude as speaker-invariant fea-
tures for ASR.

We can show that the above procedure is equivalent to tak-
ing the Scale-Transform [Cohen, 1993] (a special case of
Mellin transform) of SA(f ) and SB(f ) and then using the
magnitude of Scale-Transform as features in ASR. In this pa-
per, we refer to these features as Scale-Transform Cepstral

IJCAI-07

1739

Original Spectrum 

SA(f)
SB(f)=SA(α

ABf)

Acc% 56.98

ST CC M F CC
62.14

0.06

0.04

0.02

0

0

0.06

0.04

0.02

0

0

10000

5000

0

0

10

20

30

40

50

60

70

80

90

f

Log Warped Spectrum 

SA(λ)
SB(λ)=SA(e

λ+lnα

AB)

50

100

150

200

250

STCC Features 

300
λ=log(f)
data1
data2

50

100

150

200

250

300

Table 1: Accuracy of the phoneme recognizer using STCC
and MFCC

out skips over states. Two mixture Gaussians with full co-
variance matrices per state were used. The features vec-
tors are of 39 dimensions comprising normalized log-energy,
c1. . . c12(excluding c0) and their ﬁrst and second order deriva-
tives. Conventional MFCC features are computed as de-
scribed in [Lee and Rose, 1998] while the STCC features are
computed as described in [Umesh et al., 1999] and a block
diagram is shown in Fig. 2.

The classiﬁcation performance between STCC and the
MFCC features are shown in Table 1. As seen from the Table,
STCC features have a lower classiﬁcation performance when
compared to MFCC features. Since STCC provides speaker-
invariant features, it should have provided improved normal-
ization performance when compared to MFCC. The reason
for the degradation is the complete loss of phase-information.
We discuss about the loss of phase information in Section 3
and propose new features to overcome this loss of information
by introducing an average phase vector which is discussed in
detail in Section 4.

3 Drawback of STCC – loss of phase

information

Theoretically, STCC provides speaker-normalization by ex-
ploiting the fact that the magnitude of STCC are identical as
seen in Eq. 6. However, note that Da(c) and Db(c) are com-
plex quantities with associated magnitude and phase. Let

Da(c) = |Da(c)|ejφ(c)

and

(7)

Db(c) = |Da(c)|ejφ(c)e−j ln αAB c.

(8)
Therefore |Da(c)| = |Db(c)|. However, we have completely
lost the phase information ejφ(c)
which is also important for
discrimination of phonemes. This loss in phase information
degrades the performance of the STCC and even though it is
insensitive to speaker-variations, the ﬁnal performance is in-
ferior to the MFCC. In the next section, we propose a method
to overcome this problem and help improve the performance
of the STCC.

4 Proposed Improvement over STCC features

As seen in the previous section, the loss in phase information
ejφ(c)
leads to degradation in performance when compared to
MFCC features even though STCC features are invariant to
speakers as seen in Eq. 6. As seen in Eq. 8, Da(c), Db(c)
contain both the magnitude and phase information and the
speaker-speciﬁc factor, α, appears only in the phase term.
Hence retaining the complete phase will result in no normal-
ization since the ln α will also be present. On the other hand,
taking only the magnitude will result in the loss of ejφ(c)
in-
formation which provides additional information for discrim-
ination between vowels.

Figure 3: Figure shows that linear-scaling of original spec-
tra, results in their being shifted versions in λ = log(f ) do-
main. Therefore, the magnitude of the subsequent Fourier-
transform (STCC) are identical.

Coefﬁcients (STCC). The steps to compute STCC are shown
in Fig. 2.

Fig. 3 illustrates the idea of using Scale-Transform to ob-
tain speaker-invariant features for the linear-scaling model of
Eq.1 using a synthetic example. Fig. 3(a) shows synthetic
spectra of two speakers enunciating the same sound that are
exactly scaled versions of each other corresponding to the
linear-scaling model of Eq.1. In Fig. 3(b) we show the same
spectra in the frequency-warped(i.e. log-warped) domain. As
seen from the ﬁgure, the frequency-warped spectra are ex-
actly shifted versions of each other as expected from Eq. 3.
Finally, we take the Fourier-transform of these frequency-
warped spectra and then take the magnitude of the Fourier-
transform to get the STCC features. These are shown in
Fig. 3(c). As seen from the ﬁgure, the STCC features are
identical, and are therefore invariant to speaker variations that
are present in the original spectra as seen in Fig. 3(a).

Since the linear scaling model of Eq. 1 is only a crude ap-
proximation, recently [Umesh et al., 2002] have proposed the
use of mel-scale for frequency-warping and not the log-scale.
With this mel-scale frequency-warping they claim better re-
sults [Sinha and Umesh, 2002]. Therefore, for the purposes
of this paper, we will use mel-scale warping to compute the
STCC features.

We now compare the performance of conventional MFCC
features with STCC features. TIMIT database was used for
the experiments and we considered the classiﬁcation of 42
monophones. The train and test set consist of 155015 and
56424 monophone occurrences for adult male and female
speakers respectively. The HMM models consisted of 3 emit-
ting and 2 non-emitting states, with left-to-right and with-

IJCAI-07

1740

In this section, we propose a method to incorporate the
phase information ejφ(c)
but not the speaker-speciﬁc ln α in
the STCC features. The basic idea of this approach is to esti-
mate an “average” phase vector for each phoneme using train-
ing data from all speakers.

As previously discussed, for the linear-scaling model, the
speaker differences manifest themselves as speaker-speciﬁc
shifts in the log-warped domain as seen in Fig. 3. The STCC
exploits this fact by considering the magnitude of the sub-
sequent Fourier-transform but in the process loses the phase
information completely. Our approach is to estimate the av-
erage phase for each phoneme from the training data and use
this same phase-vector for every occurrence of that phoneme
irrespective of the speaker. For example, for the phoneme
/ae/, our average-phase STCC features with acronym AP-
STCC will be of the type |Da(c)|ejφavg ae(c)
. We will ﬁrst
illustrate this idea through a synthetic example that is dis-
cussed below.

Consider that a particular phoneme is enunciated by
three different speakers and the corresponding spectra af-
ter frequency-warping are shifted versions of one another as
shown in Fig. 4(a). The three spectra in the Fourier-domain
will differ only in a linear-phase term and this can be seen
mathematically as:

sa(λ) DF T⇐⇒ Da(c) = |Da(c)|ejφ(c)

(9)

sa(λ − τ1) DF T⇐⇒ Da(c)e−jτ1c = |Da(c)|ejφ(c)e−jτ1c
sa(λ + τ2) DF T⇐⇒ Da(c)e+jτ2c = |Da(c)|ejφ(c)e+jτ2c

If we now compute the average phase of all the three

phases, i.e.

φavg(c) =

= φ(c) +

φ(c) + (φ(c) − τ1c) + (φ(c) + τ2c)

3
(τ2 − τ1)c

,

3

then the average-phase φavg(c) is same as the phase of the
phoneme, φ(c), with an additional linear-phase term that cor-
responds to the average of all the three shifts. Therefore, the
average phase φavg(c) preserves the phase of the phoneme
with an additional term corresponding to the average shift of
the spectra of the training speakers. If we now use this av-
erage phase for all speakers along with the magnitude of the
STCC feature, we get speaker-normalized features. This can
be easily seen by recalling the fact that |Da(c)| is constant for
all speakers; and when it is multiplied by the same phase term
ejφavg (c)
for all speakers, then the resulting features are same
for all speakers. This is shown in Fig. 4.

In practice, since the processing is done in the discrete-
domain, the phases are 2π wrapped, and hence we ﬁrst un-
wrap the phases before averaging. The other important practi-
cal limitation at this point is the fact that we need to know the
phoneme a priori so that we can add the appropriate average-
phase of that phone to the magnitude vector |Da(c)|. We
are now working on practical algorithms to ﬁnd the appro-
priate phase for a given frame without having the knowledge
of which phoneme the frame came from. However, as a ﬁrst

0.06

0.04

0.02

0

0

0.06

0.04

0.02

0

0

0.06

0.04

0.02

0

0

0.06

0.04

0.02

0

0

No Shift
Left Shift
Right Shift

50

100

150

200

250

300

Original Left Shift Vs Reconstructed 

50

100

150

200

250

300

Original Vs Reconstructed 

50

100

150

200

250

300

Original Right Shift Vs Reconstructed 

50

100

150

200

250

300

Figure 4: Figure shows our proposed method of normaliza-
tion using average phase. The top ﬁgure shows the shifted
spectra from different speakers in the frequency-warped do-
main. Using the average phase and the magnitude of the DFT
we then reconstruct the spectra (shown in solid line).

step, we will assume knowledge of appropriate phase vec-
tor and test the efﬁcacy of this method of normalization and
compare it with the conventional method of normalization.

Method

In this section, we will compare the performance of the pro-
posed normalization scheme with conventional normalization
scheme.

Here we considered the classiﬁcation of 8 “most confus-
able” vowels in contrast to the classiﬁcation of 42 mono-
phones used for MFCC and STCC feature vectors discussed
in section.2. The main reason was to study the effect of
our proposed phase-estimation procedure in more detail. The
vowels were extracted from TIMIT database, where the train
and test set consisted of 22686 and 8265 utterances respec-
tively. The HMM models consisted of 3 emitting and 2
non-emitting states, with left-to-right and without skips over
states. We used single mixture Gaussian with diagonal vari-
ance for each state. The feature vectors are of 26 dimen-
sions comprising normalized log-energy, c1. . . c12(excluding
c0) and their ﬁrst order derivatives.

The conventional normalization scheme is based on Eq. 2,
which involves ML estimation of the warp-factor α. This
method is also popularly known as Vocal-tract length normal-
ization (VTLN). As discussed in the introduction, we do a
brute-force search for the optimal α by computing the MFCC
feature for each α by appropriately scaling the mel ﬁlter-bank

(10)

5 Performance of the Proposed Normalization

IJCAI-07

1741

brute-force search for optimal features and are therefore
computationally very expensive. Recently a method has
been proposed that uses a special transform called Scale-
Transform to obtain speaker-invariant features. Since the
Scale-Transform uses only the magnitude of the features to
obtain speaker-invariance, there is a loss of discriminabil-
ity between phonemes due to the loss of phase information.
In our proposed method, we use the average-phase of the
corresponding phoneme in place of the lost phase informa-
tion of STCC. We are currently working on various distance
measures to ﬁnd the appropriate average-phase for normal-
ization. Using vowel classiﬁcation experiments and F-ratio
measures we show that if we have optimal average-phase in-
formation then the proposed method provides excellent nor-
malization performance when compared to the conventional
VTLN method.

7 Acknowledgments

This work was supported in part by funding from the Dept.
of Science & Technology, Government of India under project
No. SR/S3/EECE/0008/2006-SERC-Engg.

References
[Cohen, 1993] L. Cohen. The Scale Representation. IEEE

Trans. Signal Processing, 41:3275–3292, Dec. 1993.

[Lee and Rose, 1998] L. Lee and R. Rose. Frequency Warp-
IEEE Trans.

ing Approach to Speaker Normalization.
Speech Audio Processing, 6:49–59, Jan. 1998.

[Sinha and Umesh, 2002] Rohit Sinha and S. Umesh. Non-
Uniform Scaling Based Speaker Normalization. In Proc.
of Int. Conf. on Acoustics, Speech, and Signal Processing,
volume 1, pages 589–592, 2002.

[Umesh et al., 1999] S. Umesh, L. Cohen, N. Marinovic, and
IEEE

D. Nelson. Scale Transform in Speech Analysis.
Trans. Speech Audio Processing, Jan. 1999.

[Umesh et al., 2002] S. Umesh, L. Cohen, and D. Nelson.
Frequency Warping and the Mel Scale. IEEE Signal Pro-
cessing Letters, 9(3):104–107, March 2002.

[Wakita, 1977] H. Wakita. Normalization of Vowels by Vo-
cal Tract Length and its Application to Vowel Identiﬁ-
cation. IEEE Trans. Acoust., Speech, Signal Processing,
ASSP-25(2):183–192, Apr. 1977.

Condition

No Norm.
Baseline

Norm.

Norm.

Recog. Trans.

True Trans.

VTLN-MFCC

AP-STCC

60.99
60.42

61.17
36.58

69.69
99.36

Table 2: % of Accuracy for VTLN-MFCC and AP-STCC
speaker normalization methods for classiﬁcation of 8 vow-
els in TIMIT test set. First column is without normalization.
In the second column we have used recognition output of ﬁrst
column as transcription for the normalization, while the third
column shows the normalization performance when the true
transcription is known.

and choosing the feature that maximizes the likelihood with
respect to the statistical HMM model and the given transcrip-
tion Wi. Note that although the method needs transcription,
there is graceful degradation when there are errors in the tran-
scription.

In our proposed method, we are still working on methods
to ﬁnd the optimal phase-vector to multiply a given frame
of speech. As seen from Table 2 if we are given exact tran-
scription, then we can multiply by the correct average-phase
vectors and the performance is exceptional. On the other
hand, even with exact transcription the VTLN method is far
inferior to the proposed method of using AP-STCC features.
But, if we use the recognition output of the baseline recog-
nizer (i.e. without normalization) as the transcription then the
VTLN normalization method degrades gracefully while the
AP-STCC completely falls apart – it is worse than even base-
line. Note that we do not need to know the transcription for
AP-STCC, but a method of ﬁnding “optimal” phase-average
vector for each utterance. We are working on various distance
measures to obtain an “optimal” solution. However, it is im-
portant to note that at least under ideal conditions (when true
transcription is known) our method shows that it is possible to
remove almost all the speaker-variability which conventional
VTLN-MFCC is never able to achieve.

Another approach to measure the normalization perfor-
mance is to measure the separability of the vowel models
using the various normalization schemes. One good mea-
sure of separability is the F-ratio between models consid-
ered pair-wise. Tables 3,4,5,6 shows the F-ratio for the un-
normalized features, the VTLN features and proposed AP-
STCC features. These tables show F-ratio of models that are
built using training data for which true transcription is always
known. The F-ratio shows the separability between models
and the higher the number the better. Note that since MFCC
and STCC are computed slightly differently the features are
slightly different and hence there are small differences in per-
formance in the un-normalized case. From the Table, it can
be seen that the separability between the vowels models are
excellent for the AP-STCC features.

6 Conclusion & Discussion

In this paper, we have proposed a method of obtain-
ing speaker-invariant features for automatic speech recog-
nition. Our method is motivated by the fact that conven-
tional methods reduce inter-speaker variability by doing a

IJCAI-07

1742

aa
0

aa
10.6791
ae
3.3998
ah
10.0033
eh
13.6230
er
ih
20.2214
ow 8.6107
uh
9.7719

ae

10.6791

0

6.1473
2.7679
19.2485
5.5922
16.4265
9.3211

ah

3.3998
6.1473

0

4.7876
11.2611
10.4775
6.7118
4.3584

eh

10.0033
2.7679
4.7876

0

12.4709
3.8072
13.1436
5.0537

er

13.6230
19.2485
11.2611
12.4709

0

18.3380
18.3751
11.4699

ih

20.2214
5.5922
10.4775
3.8072
18.3380

0

16.0073
4.7921

ow

8.6107
16.4265
6.7118
13.1436
18.3751
16.0073

0

uh

9.7719
9.3211
4.3584
5.0537
11.4699
4.7921
6.9622

6.9622

0

Table 3: F-ratio test for Un-Normalized AP-STCC

aa
0

166.9895
45.6957
145.0410
64.4161
254.3571
73.0718
116.6179

ae

166.9895

0

84.6296
35.5867
162.3246
126.5625
171.1731
138.8447

ah

45.6957
84.6296

0

55.6417
48.9849
141.6923
57.2495
50.8937

eh

145.0410
35.5867
55.6417

0

114.7508
67.1734
118.8632
74.0480

er

64.4161
162.3246
48.9849
114.7508

0

180.8260
79.5003
76.1799

ih

254.3571
126.5625
141.6923
67.1734
180.8260

0

144.3595
61.7577

ow

73.0718
171.1731
57.2495
118.8632
79.5003
144.3595

0

uh

116.6179
138.8447
50.8937
74.0480
76.1799
61.7577
33.4184

33.4184

0

aa
ae
ah
eh
er
ih
ow
uh

Table 4: F-ratio test for Normalized AP-STCC

aa
0

aa
11.1627
ae
3.1279
ah
10.1707
eh
14.7092
er
ih
20.9919
ow 10.2234
10.1223
uh

ae

11.1627

0

8.3630
2.9611
19.4991
6.2119
19.2229
10.1541

ah

3.1279
8.3630

0

5.6553
12.6427
12.2763
8.0236
4.3762

eh

10.1707
2.9611
5.6553

0

12.5816
4.1006
14.8935
5.3250

er

14.7092
19.4991
12.6427
12.5816

0

17.6689
21.5590
12.1847

ih

20.9919
6.2119
12.2763
4.1006
17.6689

0

17.9766
5.0690

ow

10.2234
19.2229
8.0236
14.8935
21.5590
17.9766

0

uh

10.1223
10.1541
4.3762
5.3250
12.1847
5.0690
8.6819

8.6819

0

Table 5: F-ratio for Un-Normalized VTLN-MFCC

aa
0

aa
ae
16.7331
ah
4.4782
eh
13.7949
er
24.1999
ih
25.9021
ow 11.8576
uh
12.6169

ae

16.7331

0

13.3697
4.0665
29.5038
8.8052
27.0284
18.3779

ah

4.4782
13.3697

0

8.4281
20.4874
16.9298
11.2715
7.0433

eh

13.7949
4.0665
8.4281

0

18.2864
5.3348
20.8485
9.3426

er

24.1999
29.5038
20.4874
18.2864

0

23.7503
32.8729
18.3630

ih

25.9021
8.8052
16.9298
5.3348
23.7503

0

22.5615
8.2142

ow

11.8576
27.0284
11.2715
20.8485
32.8729
22.5615

0

uh

12.6169
18.3779
7.0433

9.34

18.3630
8.2142
10.0568

10.0568

0

Table 6: F-ratio for Normalized VTLN-MFCC. Note that the F-ratio is considerably smaller than that of Normalized AP-STCC

IJCAI-07

1743


Average-Reward Decentralized Markov Decision Processes

Marek Petrik

Department of Computer Science

University of Massachusetts

Amherst, MA 01003
petrik@cs.umass.edu

Shlomo Zilberstein

Department of Computer Science

University of Massachusetts

Amherst, MA 01003
shlomo@cs.umass.edu

Abstract

Formal analysis of decentralized decision making
has become a thriving research area in recent years,
producing a number of multi-agent extensions of
Markov decision processes. While much of the
work has focused on optimizing discounted cumu-
lative reward, optimizing average reward is some-
times a more suitable criterion. We formalize a
class of such problems and analyze its character-
istics, showing that it is NP complete and that opti-
mal policies are deterministic. Our analysis lays the
foundation for designing two optimal algorithms.
Experimental results with a standard problem from
the literature illustrate the applicability of these so-
lution techniques.

Introduction

1
Decentralized decision making under uncertainty is a grow-
ing area of artiﬁcial intelligence addressing the interaction
of multiple decision makers with different goals, capabili-
ties, or information sets. Markov decision processes (MDPs)
have been proved useful for centralized decision making in
stochastic environments, leading to the development of many
effective planning and learning algorithms. More recently,
extensions of MDPs to decentralized settings have been de-
veloped. Examples include stochastic games (SGs) or com-
petitive Markov decision processes [Filar and Vrieze, 1996],
and decentralized partially observable Markov decision pro-
cesses (DEC-POMDPs) [Bernstein et al., 2000]. SGs concen-
trate mainly on observable domains, capturing the competi-
tiveness among the players. DEC-POMDPs generalize par-
tially observable MDPs to multi-agent settings, modeling co-
operative players who may have different partial knowledge
of the overall situation.

This paper addresses the problem of average-reward de-
centralized MDPs. We analyze a class of DEC-POMDP prob-
lems, where the dependency and interaction among the agents
is deﬁned by a common reward function. The agents are oth-
erwise independent and they each have full observation of
their local, mutually-independent states. A similar model has
been previously studied by [Becker et al., 2004] with the ob-
jective of optimizing cumulative reward over a ﬁnite-horizon.
In contrast, we analyze the inﬁnite-horizon average reward

problem, which has not been previously studied in a decen-
tralized settings.

The need to optimize average reward has been demon-
strated in many applications, including ones in reinforce-
ment learning [Mahadevan, 1996], decentralized robot con-
trol [Tangamchit et al., 2002], decentralized monitoring,
sensor networks, and computer networks [Rosberg, 1983;
Hajek, 1984]. In these problems, the system operates over
an extended period of time and the main objective is to per-
form consistently well over the long run. The more common
discounted reward criterion usually leads to poor long-term
performance in such domains.

One motivation for using discounted reward models–even
when the average reward criterion seems more natural–is that
the problem is easier to analyze and solve [Puterman, 2005].
But it is not clear if the same argument prevails in decentral-
ized settings. In fact, in some problems the average reward
analysis may be simpler because the system may exhibit com-
plex behavior initially, but quickly settle into simple one. One
such example is the automaton used to prove that an optimal
policy for a POMDP may not be regular [Madani, 2000]. The
optimal discounted policy in this simple example is inﬁnite,
while the optimal average reward policy is very simple and
contains just a single action.

In the case of POMDPs, the analysis of the average re-
ward case is known to be much more involved than the dis-
counted reward criterion. A thorough account of these is-
sues and solutions may be found in [Arapostathis et al., 1993;
Puterman, 2005]. Since a DEC-POMDP is a generalization
of a POMDP, one would expect to face the same difﬁcul-
ties. However, we demonstrate a way to circumvent these
difﬁculties by focusing on a subclass of the problem–the
case of observation and transition independent decentralized
MDPs [Becker et al., 2004]. The results we obtain for this
class of problems are encouraging and they lay the foundation
for studying more complex models and competitive settings.
The main contribution of this paper is the formulation and
analysis of the DEC2-MDP problem with the average reward
criterion. Section 2 deﬁnes the problem and provides a mo-
tivating example. In Section 3, we show that calculating the
gain of a ﬁxed policy is easy, but ﬁnding an optimal policy
is NP complete. In Section 4, we propose two practical algo-
rithms to solve the problem.
In addition, these algorithms
show a connection with centralized average reward MDPs

IJCAI-07

1997

and thus help to analyze the properties of the optimal policies.
Both algorithms are based on mathematical programming for-
mulations. Finally, in Section 5 we demonstrate the algo-
rithms on a standard problem, in which the average-reward
objective is the most natural one. Some of the results we
provide are easily extensible to the competitive case. In the
interest of clarity, we only mention these extensions in this
paper.

2 Average Reward DEC-MDPs
This section provides a formal deﬁnition of the problem and
shows that under certain conditions, the average reward may
be expressed compactly, leading to a compact formulation of
the problem. The model we propose is similar to the one
proposed in [Becker et al., 2004]. We introduce a different
deﬁnition, however, because the original is not easily extensi-
ble to inﬁnite-horizon problems. To simplify the discussion,
the problem is deﬁned for two agents. However, the deﬁni-
tion as well as some of the results can be easily extended to
an arbitrary number of decision makers.
Deﬁnition 2.1. A DEC2-MDP is an n-tuple (S, A, P, R) such
that:
• A = A1 × A2 represents the actions of the players;
• S = S1 × S2 is the set of states;
• P = (P1, P2) is the transition probability. Each Pi is a
function: Si × Si × Ai → R. The transition probabilities
(s1, s2) → (s
(cid:2)
(cid:2)
2) given a joint action (a1, a2) is:
1, s
(cid:2)
(cid:2)
(cid:2)
(cid:2)
2) (s1, s2), (a1, a2) = P1(s
2 s2, a2)
1 s1, a1)P2(s
P (s
1, s
• R(s1, a1, s2, a2) ∈ R is the joint reward function; and
• (s1, s2) is a given initial state for both agents.

The process, as we deﬁned it, is both transition and obser-
vation independent as deﬁned in [Becker et al., 2004]. Tran-
sition independence means that the transition to the next state
of one agent is independent of the states of all other agents.
Observation independence means that the observations that
an agent receives are independent of the current states and
observations of other agents.

While transition independence is a plausible assumption
in many domains, observation independence is somewhat re-
strictive. It prevents any kind of communication among the
agents. This assumption is suitable when communication
is prohibitively expensive, risky, or when it does not add
much value to the process. When communication is use-
ful, the problem with no observations provides as an easily-
computable lower bound that could help to decide when to
communicate.

We refer to (S1, A1, P1) and (S2, A2, P2) as the local pro-
cesses. In fact, they represent MDPs with no reward. To sim-
plify the analysis, we assume that the state set S and action
set A are ﬁnite, and that the Markov chains for both agents,
induced by any policy, are aperiodic. A Markov chain is ape-
riodic when it converges to its stationary distribution in the
limit [Puterman, 2005]. We discuss how to relax these as-
sumptions in Section 6.

A policy π is a function that maps the history of the lo-
cal states into an action for each agent. To prevent the need
for introducing measure-theoretic concepts, we assume that
the policy may only depend on a ﬁnitely-bounded number of

previous states. If the action selection depends only on the
last state of both agents, the policy is stationary, or Markov.
We extend the standard terminology for the structure of av-
erage reward MDPs to the decentralized case. A DEC2-MDP
is said to be recurrent or unichain if the local decision pro-
cesses of all the agents are recurrent or unichain respectively.
Otherwise, it is said to be multichain [Puterman, 2005].
Given a ﬁxed policy, the history of the process can be repre-
sented by a sequence of random variables {(Xt, Yt)}∞
t=0. The
random variables Xt and Yt represent the state-action pairs of
the two agents at stage t. The corresponding sequence of re-
wards is deﬁned as {R(Xt, Yt)}∞
t=0. We seek to maximize
the expected average reward, or gain, deﬁned as follows.
Deﬁnition 2.2. The gain of a Markov policy is

(cid:4)

(cid:2)

N−1(cid:3)

t=0

g(s1, s2) = lim
N→∞

1
N

Es1,s2,u0

R(Xt, Yt)

.

The expectation subscript denotes the initial value of the ran-
dom variables, thus X0 = (s1, π(s1)) and Y0 = (s2, π(s2)).
Furthermore, the gain matrix G is deﬁned as G(s1, s2) =
g(s1, s2). The actual gain depends on the agents’ initial dis-
tributions α1, α2 and may be calculated as g = αT

1 Gα2.

One problem that ﬁts the average reward criterion is
the Multiple Access Broadcast Channel (MABC) [Rosberg,
1983; Ooi and Wornell, 1996]. In this problem, which has
been used widely in recent studies of decentralized control,
two communication devices share a single channel, and they
need to periodically transmit some data. However, the chan-
nel can transmit only a single message at the same time. Thus,
when several agents send messages at the same time inter-
val, this leads to a collision, and the transmission fails. The
memory of the devices is limited, thus they need to send the
messages sooner rather than later. We adapt the model from
[Rosberg, 1983], which is particularly suitable because it as-
sumes no sharing of local information among the devices.

The MABC problem may be represented as a DEC2-MDP
where each device is represented by an agent. The state space
of each agent represents the number of messages in the buffer.
There are two possible actions, send and do not send. The
arriving data is modeled by random transitions among the
buffer states. We provide more details on the formulation and
the optimal policy in Section 5.

3 Basic Properties
In this section, we derive some basic properties of DEC2-
MDPs, showing how to efﬁciently evaluate ﬁxed policies and
establishing the complexity of ﬁnding optimal policies. The
analysis is restricted to stationary policies. An extension to
non-stationary policies is mentioned in Section 6.

For average reward MDPs, the gain of a policy can be cal-
culated directly by solving a set of linear equations, which re-
semble value function equations in the discounted case [Put-
erman, 2005]. These value optimality functions cannot be
used in DEC2-MDPs, but as the following theorem states, the
gain can be expressed using the invariant or limiting distribu-
tion. The limiting distribution p represents the average proba-
bility of being in each state over the whole inﬁnite execution.

IJCAI-07

1998

It is deﬁned as:

p(si) = lim
N→∞

N−1(cid:3)

t=0

1
N

P [Xt = si] .

s

l11

l21

v1

v2

l12

l22

v3

l13

l23

v4

f

v5

The limiting distribution can be calculated from a the initial
distribution using a limiting matrix. Limiting matrix of tran-
sition matrix P is deﬁned as
∗ = lim
N→∞

N−1(cid:3)

P

P

.

t

1
N

Then, we have that p = αP∗, where α is the initial distribu-
tion.
Theorem 3.1. Suppose P1 and P2 are state-action transition
matrices of the two agents for ﬁxed policies, and R is the
reward matrix. Then, the gain matrix can be calculated as
follows:

t=0

G = P

∗
1 R(P

T

2 )∗

.

Calculating P

processes are aperiodic and from the deﬁnition of gain.

The theorem follows immediately from the fact that both
∗ is not practical, because it typically re-
quires calculating all eigenvectors of the matrix. However,
the same idea used for calculating the gain in the single agent
case can be used here as follows.
Theorem 3.2. For any initial distribution α and transition
matrix P , the limiting distribution p fulﬁlls:

T )p = 0
T )q = α.
Moreover, if p fulﬁlls (3.1) and (3.2) then:

(I − P
p + (I − P

(3.1)
(3.2)

p = (P

T )∗

α.

(3.3)
Note that I denotes the identity matrix of the appropriate
i=1 p(i) = 1. Due
size and that (3.2) also implies that
to space constraints, we do not include the proof, which is a
similar derivation to the one in [Puterman, 2005].

(cid:5)n

Next, we examine the complexity of ﬁnding an optimal
policy in this model. The complexity of ﬁnite-horizon DEC-
MDPs has been shown to be NEXP complete, while inﬁnite-
horizon problems are undecidable [Bernstein et al., 2000].
While the model we study here is not tractable, the follow-
ing theorem shows that it is easier than the general problem.
However, we focus only on stationary policies, while the truly
optimal policy may depend on a longer history. The complex-
ity further justiﬁes the use of the algorithms we present later,
since a polynomial algorithm is unlikely to exist.
Theorem 3.3. The problem of ﬁnding an optimal Markov
policy for the aperiodic DEC2-MDP is NP-complete.
Proof. To show that ﬁnding an optimal policy is in NP, we
note that Theorem 4.2 states the optimal Markov policies are
deterministic. Since calculating the gain, given ﬁxed policies
is possible in polynomial time, and the policy size is polyno-
mial, all policies can be checked in nondeterministic polyno-
mial time.

The NP hardness can be shown by reduction from satisﬁa-
bility (SAT). The main idea is that the ﬁrst agent may assign

Figure 1: A sketch of the DEC2-MDP created from a 3-SAT
problem with 2 clauses and 5 variables.

values to variable instances to satisfy the problem, and the
second one can assign values to variables. The agents are pe-
nalized for assigning a different value to the actual variable
and to the variable instance.

Let S = (V, C) be a 3-SAT problem in conjunctive nor-
mal form, where V is the set of variables and C is the set of
clauses. If the literal l is an instantiation of variable v, then
we write l ∼ v. We construct a corresponding DEC2-MDP
in the following way. The state and action sets for the agents
are:

S1 = {s, f} ∪ {lij i ∈ C, j = 1, 2, 3}
S2 = V
A1 = {t, f}
A2 = {t, f}

Each state lij ∈ S1 represents the j-th literal in the i-th
clause, which is an instantiation of a single variable. States
S2 represent the variables of the problem. The actions rep-
resent whether the variable value is true or false. The main
idea is to make the ﬁrst agent assign arbitrary values to vari-
able instances to satisfy the formula. The second agent then
determines whether the value assigned to multiple variable
instances is the same.

For lack of space, we omit the complete deﬁnition of transi-
tion matrices. A sketch of the reduction is shown in Figure 1.
The ﬁrst agent follows the transitions depicted when the ac-
tion is chosen such that the literal is not true. Otherwise, the
agent transitions to state s. For both agents, for each state,
there is a small probability (< 1) of remaining the in same
state. For the second agent, the transition probabilities are
independent of the actions taken. Notice that this process is
aperiodic.

The rewards are deﬁned such that a negative reward is re-
ceived when the value assigned by the ﬁrst agent to an in-
stance of a variable is different from the value assigned to the
variable by the second agent. In addition, a negative reward is
received when the ﬁrst agent transitions to state f. Notice that
this happens only when the assigned variable instance values

IJCAI-07

1999

do not satisfy at least one clause.

R((f, a1), (s, a2)) = −1 ∀a1 ∈ A1, a2 ∈ A2, s ∈ S2
R((lij, a1), (s, a2)) = −1 li,j ∼ v and a1 (cid:8)= a2.
In all other cases the reward is zero.

It is easy to show that the gain of the above DEC2-MDP is

0 if and only if the SAT problem is satisﬁable.

4 Optimal DEC2-MDP Algorithms
In this section we present two methods for ﬁnding optimal
policies for DEC2-MDPs. Both methods are based on for-
mulating the problem as a mathematical program. We also
discuss additional approaches to solving the problem.

In previous work, a similar problem was solved using an
iterative method that ﬁxes the policy of one agent, while
computing the optimal policy of the other agent [Nair et al.,
2003]. Such methods are unlikely to produce good results in
the average-reward case, because of the high number of local
minima. In contrast, the following MILP formulation facili-
tates an efﬁcient search through the set of all local minima.
4.1 Quadratic Programming Solution
In this section, we give a mathematical program that deﬁnes
the optimal solutions. The reward deﬁnition is based on The-
orem 3.1. We also discuss how to obtain the optimal policy
from the solution of the quadratic program.
maximize F = p
p1 ≥ 0
subject to
(cid:3)
p2 ≥ 0
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

a∈A1
p2(j, a) −
(cid:3)
p2(j, a) +

a∈A1
q1(j, a)−
(cid:3)

p1(j, a) −
(cid:3)
p1(j, a) +

q1(s, a)P1 (j s, a) = α1(j)

∀j ∈ S2
∀j ∈ S2

∀j ∈ S1
∀j ∈ S1

a∈A2
q2(j, a)−

(cid:3)
(cid:3)

(cid:3)

(cid:3)
(cid:3)

s∈S1

T
1 Rp2

a∈A1

a∈A1

a∈A2

a∈A1

a∈A2

a∈A2

s∈S2

s∈S1

p1(s, a)P1 (j s, a) = 0

p2(s, a)P2 (j s, a) = 0

q2(s, a)P2 (j s, a) = α2(j)

(4.1)

s∈S2

a∈A2

The beneﬁt of this formulation is not so much the compu-
tational efﬁciency of the solution technique, but that it helps
identify some key properties of optimal policies. Later, we
use these properties to develop a more efﬁcient formulation,
which is also shown to be optimal.
Theorem 4.1. An optimal solution of (4.1) has the optimal
gain.

We omit the proof due to lack of space. The correctness
of the constraints follows from the dual formulation of opti-
mal average reward, shown for example in [Puterman, 2005],
and from Theorem 3.2. The choice of the objective function
follows from Theorem 3.1.

The solution of (4.1) may also be used to determine an opti-
mal policy, given an optimal value of the program’s variables.
The procedure is the same as determining an optimal policy
from a solution to the dual of an MDP. That is, the optimal
(cid:5)
policy is randomized such that for each recurrent state:
(cid:2))
pi(s, a
a∈A pi(s, a) , i = 1, 2.

(cid:5)
This is well-deﬁned because the recurrent states are those for
a∈A pi(s, a) > 0. For transient states the policy can
which
be determined as

∗
i (s) = a

P [π

(cid:2)] =

P [π

∗
i (s) = a

(cid:2)] =

(cid:5)
(cid:2))
qi(s, a
a∈A qi(s, a) , i = 1, 2.

Proposition 4.1. The policy constructed according to the
above description is optimal.

The proof is only a small variation of the proof of opti-
mal policy construction for average reward MDPs. See, for
example, [Puterman, 2005].

It has also been shown that for any average-reward MDP
there exists an optimal deterministic policy. Notice, that when
p1 is ﬁxed, the formulation for p2 is equivalent to that of the
dual of average-reward linear program. Thus, for any optimal
∗
∗
2, we can construct a deterministic policy as follows.
1 and p
p
∗
1, we can ﬁnd ˜p2 with the same gain, by solving the
Fixing p
problem as an MDP. Following the same procedure, we can
obtain also a deterministic ˜p1. Hence the following theorem.
Theorem 4.2. There is always a deterministic Markov policy
for DEC2-MDP with optimal gain.
4.2 Mixed Integer Linear Program
This formulation offers a method that can be used to calcu-
late the optimal policies quite efﬁciently. The approach gen-
eralizes the approach of [Sandholm et al., 2005], which is
based on game-theoretic principles and works only for ma-
trix games. Our approach is based on Lagrange multipliers
analysis of the problem and thus may be extended to include
any additional linear constraints.

For clarity, we derive the algorithm only for the unichain
DEC2-MDP. The algorithm for the multi-chain case is sim-
ilar and is derived in the same way. To streamline the pre-
sentation, we reformulate the problem in terms of matrices as
follows:

T
1 Rp2

maximize F = p
subject to T1p1 = 0
1 p1 = 1
T
e
p1 ≥ 0

T2p2 = 0
2 p2 = 1
T
e
p2 ≥ 0

(4.2)

where e1 and e2 are vectors of ones of length n1 and n2
respectively, and T1 and T2 are matrix representations of the
3rd and the 5th constraint in (4.1).

The Mixed Integer Linear Program (MILP) formulation is
based on application of the Karush-Kuhn-Tucker (KKT) the-
orem [Bertsekas, 2003]. Thus, the Lagrangian for the formu-
lation (4.2) is

L(p1, p2, λ, ρ, μ) =

= p
+ ρ

T

1 Rp2 + λ1(e
1 T1p1 + ρ

1 p1 − 1) + λ2(e
1 p1 + μ

2 T2p2 + μ

T

T

T

T

T

2 p2 − 1) +
T
2 p2.

IJCAI-07

2000

Let A1 and A2 denote the sets of active constraints for in-
equalities p1 ≥ 0 and p2 ≥ 0 respectively. Following from
the KKT theorem, every local extremum must fulﬁll the fol-
lowing constraints:
Rp2 = −λ1e1 − T
2 ρ2 − μ2
p1(i) = 0 ∀i ∈ A1
μ1(i) = 0 ∀i /∈ A1
μ1(i) ≥ 0 ∀i ∈ A1

p1 = −λ2e2 − T
p2(i) = 0 ∀i ∈ A2
μ2(i) = 0 ∀i /∈ A2
μ2(i) ≥ 0 ∀i ∈ A2

1 ρ1 − μ1 R

T

T

T

The gain in a local extremum simpliﬁes to:
1 ρ1 − p
T

1 Rp2 = −λ1p

1 e1 − p

T
1 T

p

T

T

T

1 μ1 = −λ1,

1 T T

1 = 0 and for all i, we have p1(i) = 0∨μi = 0.
because P T
Though the above constraints are linear, the problem cannot
be directly solved since A1 and A2 are not known. They
can however be represented by binary variables b1(i), b2(i) ∈
{0, 1}, which determine for each constraint whether it is ac-
tive or not. Therefore, we can get the following MILP:
maximize −λ1
subject to T1p1 = 0
1 p1 = 1
T
e
Rp2 = −λ1e1 − T
p1 = −λ2e2 − T
R
μ1 ≥ 0
μ1 ≤ m1b1
p1 ≥ 0
p1 ≤ e1 − b1
b1(i) ∈ {0, 1}

μ2 ≥ 0
μ2 ≤ m2b2
p2 ≥ 0
p2 ≤ e2 − b2
b2(i) ∈ {0, 1}
(4.3)

1 ρ1 − μ1
2 ρ2 − μ2

T2p2 = 0
2 p2 = 1
T
e

T

T

T

In this program, m1 and m2 are sufﬁciently large constants.
It can be show that they may be bounded by the maximal
bias [Puterman, 2005] of any two policies. However, we
are not aware of a simple method to calculate these bounds.
In practice, it is sufﬁcient to choose a number that is close
the largest number representable by the computer. This only
eliminates solutions that are not representable using the lim-
ited precision of the computer.

The optimal policy can be in this case obtained in the same
way as in Subsection 4.1. Notice that λ1 = λ2, so unlike a
competitive situation, one of them may be eliminated. The
preceding analysis leads to the following proposition.
Proposition 4.2. The gain and optimal policy obtained from
(4.2), and (4.3) are equivalent.

Though we present this formulation only for cooperative
cases, it may be easily extended to the competitive variant of
the problem.

5 Application
This section presents an application of our average-reward
decentralized MDP framework. It is a simple instance of the

s2

s1

s3
a1 N N N
a2 N N
S

s4
S
S

s5
S
S

s6
S

s7
S

s8
S

Figure 2: Sample policy for agents a1 and a2. In each state,
action S represents sending and N, not sending. The states
are s1 – s8.

practical application introduced in Section 2. The experimen-
tal component of the paper is designed to illustrate the scope
of problems that can be solved quickly using our new algo-
rithm.

As mentioned earlier, the problem we use is an adaptation
of the Multiple Access Broadcast Channel problem solved in
[Rosberg, 1983]. In order to make the problem more interest-
ing, we assume that the following events may occur: a single
or two messages arrive, a single message in the buffer expires,
or nothing happens. These events occur independently with
some ﬁxed probabilities: (α, β, γ, δ).

The communication devices may have buffers of ﬁxed but
arbitrary length, unlike the original problem. In addition, the
possible actions are send and not send. For the action not
send, the probabilities are updated according to the list stated
above. For the action send, the buffer level is decreased by
1, and then the transition probabilities are the same as for the
action not send. Though [Rosberg, 1983] derives a simple
analytical solution of the problem, it is not applicable to our
extended problem.

The reward associated with overﬂowing either of the
buffers of the agents are r1 and r2 respectively. If both agents
decide to broadcast the message at the same time, a collision
occurs and the corresponding reward is r. The above rewards
are negative to model the cost of losing messages. Otherwise,
the reward is zero.

We ran the experiments with the following message arrival
probabilities (0.1, 0.3, 0.5, 0.1). An optimal policy for the
problem with rewards r1 = −1, r2 = −1.2, r = −4.5 and
buffer sizes of 8 and 5 is depicted in Figure 2. While the op-
timal policy has a simple structure in this case, the algorithm
does not rely on that. Note that choosing different thresholds
for sending messages leads to signiﬁcantly lower gains.

To asses the computational complexity of the approach and
its practicality, we ran experiments using a basic commer-
cially available MILP solver running on a PC. The solver
was a standard branch-and-bound algorithm, using linear pro-
gramming bounds. The linear programs to calculate the
bound were solved by the simplex algorithm. For all our runs,
we ﬁxed the transition probabilities and experimented with
two sets of rewards. The ﬁrst was r1 = −1, r2 = −1.2, r =
−4.5 and the second was r1 = −1, r2 = −2, r = −1.5.

The results are summarized in Figure 3. They show that
the time required to calculate an optimal solution is below 10
seconds for problems of this size. It is clear from the results
that the different reward sets have signiﬁcant impact on how
quickly the problem is solved. This is quite common among
branch-and-bound algorithms. For larger problems, it may be
beneﬁcial to use one of the branch-and-cut algorithms, which
often offer good performance even on large problems.

IJCAI-07

2001

n1
5
5
10
10
10
10
15
15

n2
5
5
5
5
10
10
5
5

rs
1
2
1
2
1
2
1
2

g

-0.76513
-0.31529
-0.76465
-0.28743
-0.72156
-0.24128
-1e-10
0

t (sec.)
2.4936
0.8312
7.7712
2.5236
8.4922
4.2261
7.9915
5.1274

Figure 3: Performance results: n1 and n2 represent the num-
ber of states of each agent, rs is the reward set, g is the gain,
and t is runtime in seconds.

6 Conclusion
We introduced an average-reward model for observation and
transition-independent decentralized MDPs. Analysis of the
model shows that ﬁnding optimal joint policies is easier com-
pared with the general DEC-POMDP framework, but it re-
mains intractable. Using standard mathematical program-
ming, we designed algorithms that can solve problem in-
stances of substantial size for decentralized problems. While
we expect performance to vary depending on the actual struc-
ture of the problem instance, the efﬁciency of the techniques
is encouraging.

The deﬁnitions and policy analysis that we presented gen-
eralize in a straightforward manner to problems with an arbi-
trary number of decision makers. The quadratic program for-
mulation may be generalized as well, but the objective func-
tion becomes a high-degree polynomial. This, unfortunately,
makes the analysis that leads to the MILP formulation inap-
plicable.

One issue that we have not addressed here is the use of non-
Markov policies. The algorithms we have presented can be
extended to ﬁnd the optimal policies of problems with depen-
dency on a longer history than a single state with increased
complexity. However, the question whether the optimal pol-
icy for any problem is ﬁnite or may be inﬁnitely long remains
open.

In future work, we plan to relax some of the limiting as-
sumptions, most importantly, allowing observations in the
model. This extension is not trivial, since such observa-
tions break the independence among the state-action visita-
tion probabilities.

This work lays the foundation for expanding models of de-
centralized decision making to address the practical average-
reward objective. Moreover, the MILP formulation we intro-
duce could be adapted to DEC-MDPs with other optimality
criteria. This opens up several new research avenues that may
help tackle more complex domains and competitive settings.

Acknowledgments
This work was supported in part by the Air Force Ofﬁce of
Scientiﬁc Research under Award No. FA9550-05-1-0254 and
by the National Science Foundation under Grants No. IIS-
0328601 and IIS-0535061.

References
[Arapostathis et al., 1993] A. Arapostathis, V. S. Borkar,
E. Fernandez-Gaucherand, M. K. Ghosh, and S. Markus.
Discrete time controlled Markov processes with average
cost criterion - a survey. SIAM Journal of Control and Op-
timization, 31:282–344, 1993.

[Becker et al., 2004] Raphen Becker, Shlomo Zilberstein,
Victor Lesser, and Claudia V. Goldman. Solving transition
independent decentralized Markov decision processes.
Journal of Artiﬁcial Intelligence Research, 22:423–455,
2004.

[Bernstein et al., 2000] Daniel S. Bernstein, Shlomo Zilber-
stein, and Neil Immerman. The complexity of decentral-
ized control of Markov decision processes. In Proceedings
of the Conference on Uncertainty in Artiﬁcial Intelligence,
pages 32–37, 2000.

[Bertsekas, 2003] Dimitri P. Bertsekas. Nonlinear program-

ming. Athena Scientiﬁc, 2003.

[Filar and Vrieze, 1996] Jerzy Filar and Koos Vrieze. Com-

petitive Markov decision processes. Springer, 1996.

[Hajek, 1984] Bruce Hajek. Optimal control of two interact-
ing service stations. In IEEE Transactions on Automatic
Control, volume 29, pages 491–499, 1984.

[Madani, 2000] Omid Madani.

Complexity results for
inﬁnite-horizon Markov decision processes. PhD thesis,
University of Washington, 2000.

[Mahadevan, 1996] Sridhar Mahadevan. Average reward re-
inforcement learning: Foundations, algorithms, and em-
pirical results. Machine Learning, 22(1-3):159–195, 1996.
[Nair et al., 2003] R. Nair, M. Tambe, M. Yokoo, D. Pyna-
dath, and S. Marsella. Taming decentralized POMDPs:
Towards efﬁcient policy computation for multiagent set-
tings. In Proceedings of the International Joint Conference
on Artiﬁcial Inteligence, pages 705–711, 2003.

[Ooi and Wornell, 1996] J. M. Ooi and G. W. Wornell. De-
centralized control of a multiple access broadcast channel:
performance bounds. In Proceeding of the IEEE Confer-
ence on Decision and Control, volume 1, pages 293–298,
1996.

[Puterman, 2005] Martin L. Puterman. Markov decision pro-
cesses: Discrete stochastic dynamic programming. John
Wiley & Sons, Inc., 2005.

[Rosberg, 1983] Z. Rosberg. Optimal decentralized control
IEEE

in a multiaccess channel with partial information.
Transactions on Automatic Control, 28:187–193, 1983.

[Sandholm et al., 2005] Tuomas Sandholm, Andrew Gilpin,
and Vincent Conitzer. Mixed-integer programming meth-
ods for ﬁnding Nash equilibria.
In Proceedings of the
National Conference on Aritiﬁcal Intelligence, pages 495–
501, 2005.

[Tangamchit et al., 2002] Poj Tangamchit, John M. Dolan,
and Pradeep K. Koshla. The necessity of average rewards
in cooperative multirobot learning. In Proceedings of the
IEEE International Conference on Robotics and Automa-
tion, pages 1296–1301, 2002.

IJCAI-07

2002


Word Sense Disambiguation through Sememe Labeling 

*Xiangyu Duan, *Jun ZHAO, **BO XU 

Institute of Automation, Chinese Academy of Sciences 

National Laboratory of Pattern Recognition 

Eastern Road of ZhongGuan Cun, 95#, 100080 

*{xyduan, jzhao}@nlpr.ia.ac.cn, **xubohitic.ia.ac.cn 

Abstract

Currently most word sense disambiguation (WSD) 
systems  are  relatively  individual  word  sense  ex-
perts.  Scarcely  do  these  systems  take  word  sense 
transitions  between  senses  of  linearly  consecutive 
words  or  syntactically  dependent  words  into  con-
sideration. Word  sense  transitions  are very impor-
tant. They embody the fluency of semantic expres-
sion and avoid sparse data problem effectively. In 
this paper, HowNet knowledge base is used to de-
compose  every  word  sense  into  several  sememes. 
Then  one  transition  between  two  words’  senses 
becomes  multiple  transitions  between  sememes. 
Sememe transitions are much easier to be captured 
than  word  sense  transitions  due  to  much  less  se-
memes. When sememes are labeled, WSD is done. 
In  this  paper,  multi-layered  conditional  random 
fields  (MLCRF)  is  proposed  to  model  sememe 
transitions.  The  experiments  show  that  MLCRF 
performs  better  than  a  base-line  system  and  a 
maximum entropy model. Syntactic and hypernym 
features can enhance the performance significantly. 

1  Introduction 
Word sense disambiguation (WSD) is one of the important 
tasks in natural language processing. Given the context of a 
word, a WSD system should automatically assign a correct 
sense to this word. WSD has been studied for many years. 
Current  word  sense  disambiguation  systems  are  mainly  in-
dividual word sense experts [Ide et al., 1998]. In these sys-
tems,  there  are  mainly  two  categories  of  the  target  word’s 
contexts. One category is the words in some  windows sur-
rounding the target word, the other category is the relational 
information,  such  as  syntactic  relation,  selectional  prefer-
ences, etc. But senses of contextual words, which may also 
need to be disambiguated, attract little attention. In this pa-
per,  senses  of  contextual  word  and  the  target  word  are  si-
multaneously considered. We try to incorporate sense tran-
sitions between  every  contextual  word  and the  target  word 
into  one  framework,  and  our  target  is  to  disambiguate  all 
words through determining word sense transitions. 
 
Some  scholars  have done  relative  researches  on glob-
ally modeling word sense assignments. Some methods have 

been  proposed,  including  GAMBL  (a  cascaded  mem-
ory-based  classifiers)  [Decadt  et  al.,  2004],  SenseLearner 
[Mihalcea and Csomai, 2005], Naïve Bayes methods [Yuret, 
2004].   
 
Most  of  these  methods  still  based  on  individual  word 
experts while our motivation is to model word sense transi-
tions.  This  is  a  different  view  of  globally  modeling  word 
sense assignments. In a sentence with no syntactic informa-
tion,  word  sense  transitions  take  the  form  of  linear  chain. 
That is, transitions are from left to right, word by word. In a 
sentence with dependency syntactic information, word sense 
transitions  are  from  all  children  to  their  parent.  Although 
linear  chain  word  sense  transitions  had  been  studied,  by 
using  simulated  annealing  [Cowie  et  al.,  1992]  and  unsu-
pervised graph-based algorithm [Mihalcea, 2005], the num-
ber of word sense transitions is so tremendous that they are 
not  easy  to  be  captured.  In  order  to  circumvent  this  short-
coming,  we  adopt  HowNet  (http://www.keenage.com) 
knowledge base to decompose every word sense into several 
sememes  (usually  no  more  than  6  sememes).  HowNet  de-
fines a closed set of sememes that are much less than word 
senses.  But  various  combinations  of  these  sememes  can 
describe various word senses. It is more practical to model 
sense transitions through sememes  than through pure word 
senses.
 
Then  one  transition  between  two  words’  senses  be-
comes multiple transitions between sememes. In this paper, 
we  propose  multi-layered  conditional 
fields 
(MLCRF) to model sememe transitions. As we know, con-
ditional  random  fields  (CRF)  can  label  nodes  of  structures 
like  sequences  and  trees.  Consecutive  labels  in  CRF  also 
constitute label transitions. Therefore, the problem of mod-
eling  sememe  transitions  can  also  be  viewed  as  sememe 
labeling problem. 
 
There  are  researches  on  HowNet-based  WSD.  Wong 
and Yang [2002] have done a similar work to ours. Just like 
POS tagging, they regarded WSD as word sense tagging and 
adopted a maximum entropy approach to tag senses. In sec-
tion  4.2,  we  made  some  comparisons.  The  experiments 
show that MLCRF performs better than a base-line system 
and Wong and Yang’s maximum entropy model. Syntactic 
and  hypernym  features  can  enhance  the  performance  sig-
nificantly.

random 

IJCAI-07

1594

2  An Introduction of HowNet 
HowNet  [Zhendong  Dong  et,  2006]  is  a  bilingual  com-
mon-sense  knowledge  base.  One  important  component  of 
HowNet  is  the  knowledge  dictionary,  which  covers  over 
6,500 words in Chinese and close to 7,500 English equiva-
lents. We use this knowledge  dictionary  to  generate  candi-
date senses of a word. In HowNet, each candidate sense of 
one word is a combination of several sememes. For example, 
a sense definition of the Chinese word “research institute” is 
as follows: 
DEF=InstitutePlace, *research, #knowledge 
 
 
where  word  sense  (DEF)  is  split  into  sememes  by 
commas. The sememes in this example are “InstitutePlace”, 
“research”, “knowledge”. Symbols preceding sememes rep-
resent relations between the entry word and the correspond-
ing  sememe.  In  this  example,  symbols  are  “*”  and  “#”. 
Symbol  “*”  represents  agent-event  relation,  “#”  represents 
co-relation. This word sense (DEF) can be glossed as: “re-
search  institute”  is  an  “InstitutePlace”,  which  acts  as  an 
agent  of  a  “researching”  activity,  and  it  has  a  co-relation 
with “knowledge”. 
 
Sememes  are  the  most  basic  semantic  units  that  are 
non-decomposable.  It  is  feasible  to  extract  a  close  set  of 
sememes  from  Chinese  characters.  This  is  because  each 
Chinese  character  is  monosyllabic  and  meaning  bearing. 
Using  this  closed  set  of  sememes,  HowNet  can  define  all 
words’  senses  through  various  combinations  of  these  se-
memes. 
 
A word  sense is  defined by sememes  according  to  an 
order.  The  first  sememe  in  a  word  sense  definition  repre-
sents  the  main  feature  of  the  sense  of  that  word,  and  this 
main  feature  is  always  the  category  of  that  sense.  In  the 
example  mentioned  above,  the  main  feature  of  the  word 
“research institute” is “InstitutePlace”. Other sememes in a 
word  sense  are  organized  by  an  order  of  importance  from 
the HowNet’s point of view. In our current system, symbols 
representing relations are omitted. 
 
Totally,  HowNet  contains  1,600  sememes,  which  are 
classified  into  7  main  classes,  including  “entity”,  “event”, 
“attribute”,  “attribute  value”,  “quantity”,  “quantity  value”, 
“secondary feature”. These 7 main classes are further classi-
fied  hierarchically.  In  the  end,  all  of  1,600  sememes  are 
organized as 7 trees. Each sememe besides the root sememe 
has a hypernym (A is a hypernym of B if B is a type of A). 
For  example,  “InstitutePlace”  has  a  hypernym  “organiza-
tion”, “organization” has a hypernym “thing”, “thing” has a 
hypernym “entity”. The hypernym feature shows usefulness 
in our word sense disambiguation system. 

3  System Description 

3.1  Task Definition 
Our  task  is  to  disambiguate  word  senses  through  sememe 
labeling. Here is the formal description of our task. In what 
follows, X denotes a sentence. The ith word of X is denoted 
i ,  and  the  sentence’s  length  is  n.  According  to 
by 
  is decomposed into m layers 
HowNet, the sense of each 

x

ix

y ,...
i1

y

im

, where m is an empirical 
of sememes denoted by 
parameter  depending  on  how  much  layers  of  sememes 
bound  together  can  differentiate  word  senses  from  each 
y ,...
imy
  are  ordered  by  decreas-
other.  Please  note  that 
i1
y
  is  the  first  sememe  in 
ing  importance.  For  example, 
1i
i . With some simplification of the 
the sense definition of 
HowNet knowledge dictionary, word senses can distinguish 
each other by 2 layers of sememes. If sememes are taken as 
i   has  2  layers  of  labels.  Word  sense 
labels,  every  word 
disambiguation  becomes  a  2-layered  labeling  problem.  Se-
meme labeling can be carried on flat sentences (sequences) 
or dependency syntactic trees. This is illustrated in Figure 1. 

x

x

2,1iy

1,1iy

1ix

2,jy

1,jy

jx

2,iy

1,iy

ix
(a)

2,iy

1,iy

ix

(b)

2,1iy

1,1iy

1ix

2,ky

1,ky

kx

Figure 1. Graphical representation of words and sememe layers. (a) 
is the sequence representation, and (b) is the tree representa-
tion. Solid lines show sememe transitions, dashed lines show 
related links. Please note that the direction of sememe transi-
tion is different for sequences and trees. For sequences, the 
direction is from left to right. For trees, the direction is from 
child node to its parent node. Although sememes can depend 
on  any  word,  higher  layer  sememes  can  depend  on  lower 
layer sememes, for clarity, we do not show these cross links. 
For  more  detailed  description,  see  the  feature  set  of  section 
3.5.

 
Sememes  per  layer  constitute  a  label  configuration  of 
the  sentence.  These  m  layer  configurations  of  the  sentence 
are denoted by 
ˆ
m

. Our aim is to find: 

,...
1
arg

...ˆ

)1(

...

)

(

1

m
max
...
YY
1
m

|

m

1

We propose a layer-by-layer strategy “MLCRF” to solve 
this equation. Current layer sememe labeling can use labeled 
pre-layer sememes as features. The order of the strategy is 
bottom up, from 1st layer to mth layer. To introduce MLCRF, 
we introduce conditional random fields in the following 
section. 

IJCAI-07

1595

3.2  Introduction of Conditional Random Fields 
Conditional random fields (CRF) [Lafferty et al., 2001] are 
undirected  graphical  models  used  to  calculate  the  condi-
tional probability of a set of labels given a set of input val-
ues. We cite the definitions of CRF in [McCallum, 2003]. It 
defines the conditional probability proportional to the prod-
uct of potential functions on cliques of the graph, 

(

|

)

1

(

,

c

)

c

)2(

c

(

,

c
)

,

(

where X is a set of input random variables and Y is a set of 
  is  the  clique  potential  on 
random  labels. 
clique c. C(Y,X) is the set of all cliques of the graph. In CRF, 
c   repre-
clique is often an edge with a node on each end. 
sents labels of the two end nodes. Clique potential is often 
defined as: 

)

c

c

c

(

c

,

c

)

c

exp(

R

r

1

f

r

(

r

,

c

))

c

)3(

rf
r

  is  an  arbitrary  feature  function  over  its  argu-
  is a learned weight for each feature function, R

where 
ments, 
is  the  total  number  of  feature  functions  of the  clique. 
is a normalization factor over all output values,   

(

'
c

,

)

c

)4(

'

c

(

,'

c
)

Two  dynamic  programming  algorithms  are  employed: 
Viterbi for decoding and a variant of the forward-backward 
algorithm [Sha and Pereira, 2003] for the computing of ex-
pectations and normalization. 
 
Two kinds of CRF are used in our system: sequential 
CRF and tree CRF. Sequential CRF is like traditional linear 
chain  CRF,  while  tree  CRF  is  slightly  different.  In  linear 
chain CRF, one clique is composed of current word and its 
pre-word.  In  tree  CRF,  one  clique  is  composed  of  current 
word and one child of it. Tree CRF adopts sum-product al-
gorithm  [Pearl,  1988],  which  is  an  inference  algorithm  in 
graphical models. Cohn and Blunsom [2005] have used tree 
CRF to solve semantic role labeling. But the algorithm was 
not presented. In this paper, we take sum-product algorithm 
into  a  forward-backward  frame.  Each  node  in  the  tree  has 
several  sub  forward  (backward)  vectors.  The  detailed  de-
scription  of  the  algorithm  is  not  included  in  this  paper  for 
the limit of the paper length. 

3.3  Multi-layered Conditional Random Fields 
In  multi-layered  conditional  random  fields,  the  conditional 
probability is as follows: 
()
[

,...,
1

),
1

),
1
]
2

m
]
mc

...
1

]
1

[

(

(

[

)

(

m

m

|

|

|

|

2

1

c

c

)5(

,

1

,

,...,

1

m

1

ic ]
[
layer.

  denotes  t

he  product  of  clique  potentials  of 
  etc  denotes normalization factor.  For  ex-

where 
the ith
ample, 

[

]
mc

exp(

R

r

1

c

f

r

(

,

,
mc

,...

,

c

1,

,
mc

1

r

))

)6(

,

1 ,...

m

1

[

m

]
mc

)7(

denotes the pair of mth layer labels of the words 

where 
mc,
of clique c.
|

1

|

|

m

,

(

)

(

)

1m

( 1

,...,

,…, 

ividually

rained ind

, using seque

) ,
  represent 1,
2,  …,  mth  la
yer  probabilities  respectively.  Each  layer  CRF 
ntial CRF or 
model can be t
tree CRF. 
 
Sutton et al. [2004] use dynamic CRF to perform mul-
tiple,  cascaded  labeling  tasks.  Rather  than  perform  ap-
proximate inference, we perform exact inference. To avoid 
exponential  increase,  we  propose  a  multi-layered  Viterbi 
algorithm for decoding. 
 
To illustrate multi-layered Viterbi, let us look back on 
single layer decoding. For single layer, the most probable Y
is as follows: 

^

arg

max

(

(

|

))

arg

max

(

(

,

))

)8(

w

here 

(

,

)

(

,

c

)

c

c

.  The  denominator

  is 

om

it-

ause 
is the sam
e  next  layer  decod

e for all Y. But in m ti-layered 
ing will  use  pre  layer  labels  as 

ted bec
case,  th
features, so the denominators are no longer the same. 
 
CRF of single layer. If the denominator 
for one layer, it can also be expressed as: 

For the ease of explanation, let us consider linear chain 
  is not omitted 

ul

(
sum

)

1

(
sum
(
sum

)
)

2

1

(
sum
(
sum

n

)
)

1

n

)9(

(

)

i
ain.

denotes  the  sum  of  the  vector

denotes the forward vector at the ith position of 
where 
the  ch
’s
sum
elements. In CRF, the sum of the forward vector 
i ’s ele-
ments is the sum of all possible paths up to the ith position. 
So
 

sum
Then the linear chain probability is as follows: 

  is equal to 

)

(

.

n

(

|

)
)
(
(

,
,

2

n

)
)

2

n

1

)
,(
1
{log(
{log(

lo
g(
(
sum
(
sum

(
sum
))
))

2

n

))
1
log(
log(

(
sum
(
sum

))}

1

...

))}

1n

)10(

Equation  8  shows  that  the  linear  chain  probability  can  be 
com
puted dynamically. At each time step, we can get prob-
ability up to that time. Based on this single layer equation, 
we can derive the multi-layered Viterbi algorithm. 
  With  some  simplification  of  the  HowNet  knowledge 
dictionary, word senses can distinguish each other by 2 lay-
ers of sememes. We present a 2-layer Viterbi for sequential 
CRF  in  Figure  2.  Multi-layered  Viterbi  can  be  extended 
from  2-layer  Viterbi.  In  Figure  2,  all  forward  vectors 
and  transition  matrices  M  are  only  about  second  layer. 
iS
denotes  a  combined  label  of  2  layers  at  ith  position,  S 2,i

IJCAI-07

1596

a

iS

ause 

st layer bec

1.  But  for  2nd  layer,  the  normalizing  factor  is 

denotes the second layer label at ith position. Suppose that 
the label set of each layer contains k elements, then 
has 
k*k kinds of 2-layer combinations. 
In  2-layer  Viterbi,  we  do not  need  transition  m trices 
 
and forward vectors of 1
is the same for 
,
al
l
1  is  the  1st  layer  sememes  that  are  related  to  1st
where 
layer  decoding.  So  forward  vectors  and  transition  matrices 
nd layer 
are nee d to calculate normalization factor for the 2
  is the forward vector of 2nd layer at ith
only. 
i
nd layer at 
  is the transition matrix of 2
position.
)
i
  and 
ith position. 
  are the logarithm value 
of 1st layer and 2
er’s ith clique potential respectively.   

2,is
1,]
i
nd lay

(
sM
[

de
(

2,]
i

2,1i

2,1

s

s

1,

[

)

2,

,

,

i

i

Initial value:

( 0sVal

)

0

,

0s is the initial state 

'

(
0 s

)

2,0

]0...1[

for every pos
 

iti n i:

o

for every candidate 2-layer label 

(

s

i

i

2,1

,

s

i

2,

)

'

i

(

s

1

)

(
sM

i

i

,

2,1

i

2,1

is :
)
is

2

,

Val

(

s
i

)

{max
s
i

1

(
sVal
i

)

1

pair

(

s
i

,

s
i

1

)}

(
,
s
pair
s
1
i
i
{log[
(
sum

)

[

(
s
i

i

,

s
i

2,1

]
i

2,

1,
))]

[

]
2,
i
(
log[
sum

'

i

(

s
i

1

))]}

2,1

'

s

i

1

{max

(
sVal

)

i

1

(
pair

s

,

s

i

)}

i

1

arg
s
i

1

'
i

2,

)

(
s
i

'
(
s
i
 the end node

i

till

,

s
i

2,

)

2,1

F
igure  2.  2-Layer  V

iterbi algorithm  for  Sequential  CRF. 

'
1is

record the optimal path. 

2

4

TKJ

), where J is t

The  time  complexity 
of  2-layer  Viterbi  for  one  sen-
he number of 1st layer labels, 
 is O(
tence
the  number  of  2nd  layer  labels,  T  is  the  length  of  the 
K  is 
sentence.  Figure  2  mainly  deals  with  sequence.  When  ap-
plying  it  to  trees,  sub 
s  for  every  tree  node  should  be 
introduced.  In  our  problem  for  WSD, 
iS   is  the  candidate 
sense at position i, whic is less than 2-layer combinations 
of entire labels. 

h

3.4  Reducing the Number of Candidate Labels 
In common CRF, every input variable 
i   has several ca
n-
didate labels. There are 1,600 sememes in HowNet. For 
our 
WSD problem, it is impractical to treat all these sememes as 
candidate labels for the training of single layer CRF. 
 
Using the HowNet knowledge dictionary we can gen-
erate candidate sememes much less. We propose two meth-
  is 
ods  to  reduce  the  number  of  candidate  sememes.  One

x

DefSpace and the other is ExDefSpace. In the following, we 
will introduce how to apply these two methods to first layer 
CRF in detail. 
DefSpace
For DefSpace, candidate first layer sememes of a word are 
emes  appeared  in  the  definitions  of  this 
the  first  layer  sem
word in Ho
wNet. DefSpace generates a real space that only 
includes the labels (sememes) listed in the definitions of the 
entry word. For example, Chinese word fazhan has 3 senses 
in  the  HowNet  dictionary:  “CauseToGrow”,  “grow”  and 
“include”. Since all these senses are single sememes, candi-
date  sememes  of  the  first  layer  are  these  3  sememes  for 
DefSpace.
ExDefSpace
ExDefSpace is the extended version of DefSpace. Suppose 
words a and b. According to the HowNet dic-
there are two 
tionary,  a has
  2  candidate  first  layer  sememes  sem_1  and 
sem_2,  b has  2  candidate  first  layer  sememes  sem_2  and 
sem_3.  Suppose  a  appears  in  the  training  corpus  while  b
does  not  appears.  Considering  training  examples  of a  with 
sem_1 or sem_2, for DefSpace, the training procedure only 
discriminates  sem_1  and  sem_2,  while  sem_2  and  sem_3 
are  not  discriminated  in  this  discriminative  training.  Then 
when  b  appears  in  test  data,  it  can’t  be  disambiguated.  To 
overcome  this  shortcoming  of  DefSpace,  candidate  se-
memes  of  a  word  are  extended.  For  every  sememe  in 
HowNet,  we  build  a  candidate  sememe  list.  Concretely 
speaking, for every sememe s, we search the whole diction-
ary entries to find sememes of first layer that together with s
define a common polysemous word. All such sememes are 
put into the candidate sememe list of s. Then during training, 
the candidate sememes of a word are generated according to 
the hand labeled sememe’s candidate list. For above exam-
ple,  sem_3  is  added  to  the  candidate  lists  of  sem_1  and 
sem_2. Then word a provides a positive example of sem_1 
or  sem_2,  but  a  negative  example  of  sem_3.  Also,  if  one 
sem_3 is tagged in the training corpus, it provides a negative 
example of sem_1 or sem_2. 

Finally, in a sememe’s candidate sememe list, there are 

average 11.76 candidate sememes. 

Candidate second layer 
sememes of a word are gener-
according to this word’s entries in HowNet whose first 
e  labeled  first  layer  se-

ated
layer  sememes  are  the  same  as  th
mem

e. This is like DefSpace used in first layer CRF. 
In the phase of decoding (multi-layered Viterbi), only 
multiple senses of a word are regarded as candidate senses 
of this word. No special measure like ExDefSpace is t

aken.

3.5  Feature Set 
In MLCRF, training is taken layer by layer. For first layer,
we use the following f
)

actored representation for features. 

(),
qc

p

)

,

(f
),
p
c
 clique c,

(

'
c

(

)11(

,
  is  a  binary  predicate  on  the  input  X  and 
'
(
cq

is a binary predicate on pairs 

)

,

c

c

where 

current

IJCAI-07

1597

ls of curre

nt clique c. For instance, 

of labe
be “ whether word at position i is de”.
 
Table  1  presents  the  categories  of 
'
(
,
cq
(
q

(
),
cXp

)

)

)

.

c

'
(
cq

,

c

tree

seq

,'
c

c

seq

s_i_0
s_i-1_0
s_i_0,s_i-1_0
^ s_i_0
^ s_i-1_0

s_i_0

s_n_0
s_c_0
s_c_0,s_n_0
^s_n_0
^s_c_0

s_n_0

true

uni_w
bi_w
uni_p
bi_p
tri_p
(from 
)
to i+2

i-2 

p

(

),
c

  might 

p

(

),
c

  and 

),
(
cXp

tree

true

uni_w
bi_w
uni_p
bi_p
tri_p
(from 
cl and

ff  to 
cr)

Table 1. featu

re set of sequ

ential CRF and tr

ee CRF 

st

For first layer, a label of the clique c is the first layer 
 
e.  In  table  1,  s_i_0  denotes  1   layer  sememe  o
f  ith
semem
n sequential CRF and s_n_0 denotes 1st layer sem
eme 
word i
rrent node n in tree CRF. “^” denotes hypernym, uni, bi
o
f cu
and tri denote unigrams, bigrams and trigrams respectively, 
w denotes word, p denotes POS. The subscript seq and tree
represent  sequential  CRF  and  tree  CRF  respectively.  For 
tree CRF, c denotes the child of n in current clique. cl and cr
are the left and right sibling of node c. f denotes the parent 
node of n, ff denotes grandfather of n.
 
For  second  layer,  s_i_0  (s_n_0)  is  replaced  by  s_i_1 
(s_n_1).  p(X,c)  excluding  true  are  appended  by  s_i_0 
(s_n_0).  For  true  value,  when  q  is  ab
out  s_i_1  (s_n_1)  or 
_1 (s_c_1), additional feature s_i_0 (s_n_0) is added to
s_
p(X,c).

i-1

4  Experiments and Results 

4.1  Experimental Settings 
Sememe (HowNet) based disambigua
tion is widely used in 
3  Chinese  lexical  sample 
Chinese  WSD  such  as  Senseval 
ion  at  sememe  level  is 
task.  But  full  text  disambiguat
scarcely considered. Furthermore, the full text corpus tagged 
with WordNet  like  thesaurus  is not publicly  available.  The 
sentences  used  in  our  experiments  are  from  Chinese  Lin-
guistic  Data  Consortium  (http://www.chineseldc.org)  that 
contains  phrase  structure  syntactic  trees.  We  can  generate 
gold  standard  dependency  trees  using  head  rules.  We  se-
lected  some  dependency  trees  and  hand  tagged  senses  on 
them  with  over  7,000  words  for  training  and  over  1,600 
words  for  testing.  There  are  two  taggers  to  tag  the  corpus 
and an adjudicator to decide the tagging results. The agree-
ment  figure  is  the  ratio  of  matches  to  the  total  number  of 

equ

annotations, and the figure is 0.88. This corpus is included 
in ChineseLDC. 
 
For  comparison,  we  build  a  base-line  system  and  du-
plicate Wong and Yang’s method [Wong et al., 2002]. The 
base-line system 
predicts word senses according to the most 
fr
ent sense of the word in the training corpus. For words 
that do not appear in the training corpus, we choose the first 
sense  entry  in  the  HowNet  dictionary.  We  also  imitate 
Wong’s method. Their method is at a word level. The sense 
of a word is the first sememe optionally combined with the 
second  sememe.  They  defined  this  kind  of  word  sense  as 
categorical  attribute  (and  also  a  semantic  tag).  Then  an 
off-the-shelf  POS  tagger  (MXPOST)  is  used  to  tag  these 
senses.
 
In  our  system,  named  entities  have  been  mapped  to 
predefined  senses.  For  example,  all  person  named  entities 
have  the
”.  For  those  words  that  do  not 
ha
 entries in dictionary, we define their sense by referring 
to their synonyms that have entries. In the end, polysemous 
words take up about 1/3 of the corpus, and there are average 
3.7 senses per polysemous word. 

  sense  “human|

ve

In  the  following  sections,  precisions  of  polysemous 
words are used to measure the performance. Since all words 
have entries in dictionary or can b
e mapped to some senses, 
recal
l is the same with precision. Note that for single layer 
sememe labeling, precision is the rate of correct sememes of 
the  corresponding  layer.  Sense  precision  is  the  rate of  cor-
rect senses, not sememes. 

4.2  Results of MLCRF in WSD 

1st layer
 
+h 
c 
1.0
8 
78.

8

2nd layer 
+h
c 
7.2
85.1

8

80.6

82.6

82.0

78.5 

- 

-

84.0

83.5

90.0

95.9

-

-

Sense

79.9

79.3 

85.9

85.1

Seq
Seq
Tree

Def

ExDef

Def

Tree

ExDef

rf

Table  2.  Pe orm

  WS   Nu
precisions 
resp
layer, second layer and whole word sense. 

CR
  w s  wi

of
  ML
lys
emous

ance 
of  po

F  in
ord

D.
th 

mbers  a
ect  to

re 
  first 

Def

  (

Seq

Seq

Table  2  shows  the  performance  of  MLCRF  in  WSD.  “c”
repre
sents common features that presented in table 1 except 
hyperny
plus hy-
pernym  features. 
)  denotes  sequential 

m  features.  “+h”  means  common  features 

ExDef
)
CRF  using  DefSpace  (ExDefSpace). 
Tree
ing DefSpace (ExD
denotes tree CRF us
efSpace). From table 
etter than sequentia
2, we can see that tree CRF performs b
l
s in trees are easie
CRF, which shows that sense transition
r
to  be  captured  than  in  sequences.  Hypernym  features  en-
hance  performance  of  tree  CRF  partly  because  it  avoids 
sparse data problem, while it adds noise to sequential CRF. 
ExDefSpace performs similarly as DefSpace, which maybe 
due to our relatively small corpus. 

Tree

  (

ExDef

Def

IJCAI-07

1598

 p

 
In  the  test  set,  there  are  203  different  types  of  502 
polysemous  words. Our  best  result  correctly  label 57  word 
senses that are labeled wrongly by baseline system. Of these 
57 olysemous words, there are 36 word types, of which 16 
word  types  never  occur  in  the  training  corpus.  Although 
baseline system also wins our method a few words, totally 
our method improves 6.9% over baseline. 
 
Our  dependency  trees  are  golden  standard.  We  insist 
that parsing and WSD should not be processed in a pipe line 
mode. WSD is not the ultimate object of NLP. It has inter-
plays with parsing, semantic role labeling etc. They can be 
integrated  together  if  WSD  can  be  fulfilled  by  globally 
modeling  word  senses.  There  are  some  methods  like 
re-ranking and approximate parameter estimation that have 
the potential to solve that integrating. 
 
Table  3  shows  the  comparisons  between  our  system 
and  (1)  base-line  system,  (2)  Wong  and  Yang’s  method. 
Both  sequential  CRF  and  tree  CRF
  perform  better  than 
base-line  system  and  Wong  and  Yang’s  method.  The  syn-
tactic  feature  enhances  performance  significantly.  Sequen-
tial CRF enhances little over base line partly because sense 
transitions  in  sequences  are  not  easy  to  be  captured.  The 
performance  of  Wong  and  Yang’s  method  is  below 
base-line  system,  which  maybe  due  to  its  simple  features 
and sparse representation of all possible paths. That is, even 
POS features are not included in MXPOST and no syntactic 
features are used. Moreover, in MXPOST, only features that 
appear more than 4 times in the training corpus are included 
in the feature set. But candidate labels are all sememes. No 
special measures like DefSpace and ExDefSpace are taken. 
Then the feature set is very sparse to represent all possible 
paths even we duplicate training corpus 3 times. 

our

Wong 

Base-line

Sense Acc. 

seq 
79.9
 

tree 
5.9 
8

77.1

79.0

Table 3. Comparison
ba

method and 

ee
sy

s betw n our s
se-line

stem.

ystem and Wong and Yang’s 

is paper, we probe into sense

 transition by decomposing 
hen  sense  transition  can  be  replaced 

5  Conclusion 
In th
sense  into  sememes.  T
.
by sememe transitions
  We  model  sememe  transitions  by  MLCRF.  For  each 
layer  sememe,  sequential  CRF  and  tree  CRF  are  used. 
Multi-layered Viterbi algorithm is proposed at the predicting 
asph e. Experiments show that MLCRF performs better than 
a base-line system and a max entropy model. Syntactic and 
hypernym  features  can  enhance  the  performance  signifi-
cantly.

Acknowledgments 
This work  was  supported by the  Natural  Sciences  Founda-
tion of China under grant No. 60372016, 60673042, and the 
Natural  Science  Foundation  of  Beijing  under  grant  No. 
4052027. Thanks for the helpful advices of Professor Zhen-
dong Dong, the founder of HowNet. 

References
[Cohn  and  Blunsom,  2005]  T.  Cohn  and  P.  Blunsom.  Semantic 
Role  Labeling  with  Tree  Conditional  Random  Fields.  In  Pro-
ceedings of Ninth Computational Natural Language Learning.

[Cowie et  al.,  1992]  J  Cowie,  J.  Guthrie  and  L.  Guthrie.  Lexical 
Disambiguation using Simulated Annealing. In Proceedings of
8th International Conference on Computational Linguistics.

[Decadt  et  al.,  2004]  B.  Decadt,  V.  Hoste,  W.  Daelemans 
and  A.  Bosch.  GAMBL,  Genetic  Algorithm  Optimiza-
tion  of  Memory-Based  WSD.  In  Proceedings  of 
ACL/SIGLEX Senseval-3, Barcelona, Spain, July.

[Ide et al., 1998] N. Ide, Jean V. Word Sense Disambiguation: The 

state of the art. Computational Linguistics.

[Lafferty  et  al.,  2001]  J.  Lafferty,  A.  McCallum,  and  F.  Pereira. 
Conditional random fields: Probabilistic models for segmenting 
and labeling sequence data. In Proceedings of 18th ICML.

[McCallum,  2003]  A.  McCallum. Efficiently inducing features  of 
conditional  ramdom  fields.  In  Proceedings  of  Conference  on 
Uncertainty in Artificial Intelligence (UAI).

[Mihalcea,  2005]  R.  Mihalcea.  Unsupervised  large-vocabulary 
word  sense  disambiguation  with  graph-based  algorithms  for 
sequence data labeling. In Proceedings of the Joint Conference 
on  Human  Language  Technology  /  Empirical  Methods  in 
Natural Language Processing (EMNLP), Vancouver, October. 
[Mihalcea  and  Csomai,  2005]  R.  Mihalcea,  A.  Csomai.  Sense 
learner: word sense disambiguation for all words in unrestricted 
text. In Proceedings of the 43th Annual Meeting of the Associa-
tion for Computational Linguistics  (ACL), companion volume,
Ann Arbor, MI, June. 

[Pearl, 1988] J. Pearl. Probabilistic reasoning in intelligent systems: 

Networks of plausible inference. Morgan Kaufmann.

[Sha  and  Pereira,  2003]  F.  Sha,  F.  Pereira,  Shallow  parsing  with 
conditional  random  fields.  In  Proceedings  of  the  Human  Lan-
guage Technology, NAACL Conference.

[Sutton et  al.,  2004]  C.  Sutton, K.  Rohanimanesh,  A.  McCallum. 
Dynamic  conditional  random  fields:  factorized  probabilistic 
models for labeling and segmenting sequence data. In Proceed-
ings of 21st ICML.

[Wong et  al.,  2002]  Ping-Wai  Wong  and  Yongsheng  Yang.  A 
Maximum Entropy Approach to HowNet-Based Chinese Word 
Sense Disambiguation. In Proceedings of SemaNet'02.

[Yuret, 2004] D. Yuret. Some experiments with a naive bayes wsd 
system. In Proceedings of ACL/SIGLEX Senseval-3, Barcelona, 
Spain, July 2004.

[Zhendong Dong et, 2006] Zhendong Dong, Qiang Dong. HowNet 

and the Computation of Meaning. World Scientific.

IJCAI-07

1599


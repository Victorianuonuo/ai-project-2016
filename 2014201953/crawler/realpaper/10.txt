Unsupervised Dimensionality Estimation and Manifold Learning in

high-dimensional Spaces by Tensor Voting

Philippos Mordohai and G´erard Medioni
email: {mordohai,medioni}@iris.usc.edu

University of Southern California

Abstract

We address dimensionality estimation and nonlin-
ear manifold inference starting from point inputs
in high dimensional spaces using tensor voting.
The proposed method operates locally in neigh-
borhoods and does not involve any global com-
putations. It is based on information propagation
among neighboring points implemented as a voting
process. Unlike other local approaches for man-
ifold learning, the quantity propagated from one
point to another is not a scalar, but is in the form
of a tensor that provides considerably richer infor-
mation. The accumulation of votes at each point
provides a reliable estimate of local dimensionality,
as well as of the orientation of a potential manifold
going through the point. Reliable dimensionality
estimation at the point level is a major advantage
over competing methods. Moreover, the absence of
global operations allows us to process signiﬁcantly
larger datasets. We demonstrate the effectiveness
of our method on a variety of challenging datasets.

Introduction

1
A number of algorithms for manifold learning from unor-
ganized points have been recently proposed in the machine
learning literature. These include kernel PCA [Sch¨olkopf
et al., 1998], locally linear embedding (LLE) [Roweis and
Saul, 2000], Isomap [Tenenbaum et al., 2000] and charting
[Brand, 2003], that are brieﬂy described in Section 2. They
aim at reducing the dimensionality of the input space in a way
that preserves some geometric or statistic properties of the
data. Isomap, for instance, attempts to preserve the geodesic
distances between all points as the manifold is “unfolded”
and mapped to a space of lower dimension. A common as-
sumption is that the desired manifold consists of locally lin-
ear patches. We relax this assumption by only requiring that
manifolds be smooth almost everywhere. Smoothness in this
context is the property of the manifold’s orientation to vary
gradually as one moves from point to point. This property is
encoded in the votes that each point casts to its neighbors.

The representation of each point is in the form of a second
order, symmetric, nonnegative deﬁnite tensor whose eigen-
values and eigenvectors fully describe the dimensionality and

orientation of each point [Medioni et al., 2000]. The votes are
also tensors and their accumulation is a process that is con-
siderably more powerful than the accumulation of scalars or
vectors in terms of both the types of structure that can be de-
scribed, as well as its robustness to noise. A point casts a vote
to a neighboring point that carries an estimate of the orienta-
tion of the receiver given the orientation of the voter. Even in
the case of unoriented inputs, meaningful votes can be cast as
a function of the relative position of the two points. The result
of voting is a new tensor at each point that encompasses the
weighted contribution of the point’s neighbors.

The dimensionality d of the manifold at each point in a D-
dimensional space is indicated by the maximum gap in the
eigenvalues of the tensor, while the top d eigenvectors span
the normal space of the manifold. The ﬁrst property makes di-
mensionality estimation a procedure with no parameters that
require tuning. These estimates are reliable without averag-
ing, as shown is Section 4, and allow us to handle data with
varying dimensionality. This is arguably one of the most im-
portant contributions of our approach. Moreover, the pres-
ence of outliers, boundaries, intersections or multiple mani-
folds pose no additional difﬁculties, due to the unsupervised
learning ability of our method. Furthermore, non-manifolds,
such as hyper-spheres, can be inferred since we do not at-
tempt to estimate a global “unfolding”.

We do not attempt to estimate a mapping from the input
space to the embedding space, as in [Brand, 2003] [Teh and
Roweis, 2003]. Even though certain advantages can be de-
rived from such a mapping, it is not always feasible, as in the
case of hyper-spheres or multiple manifolds, and it is not nec-
essary for many tasks, including interpolation. Valid paths on
the manifold can be followed using the estimated tangent sub-
spaces at the input points by projecting the desired direction
on them and moving on the projection.
Since all operations are local,

time complexity is
O(Dnlogn) where n is the number of points, enabling us to
handle datasets with orders of magnitude more points than the
current state of the art without incurring unreasonable com-
putational cost. In terms of storage, the requirements at each
point are O(D2). Our current implementation is probably
limited to D in the order of a few hundreds. As all manifold
learning methods, we operate under the assumption that the
data are randomly sampled from the manifold in a way that
is close to uniform, or at least does not favor certain parts or

directions of the manifold.

The paper is organized as follows: the next section brieﬂy
reviews related work; Section 3 describes the tensor voting
framework; Section 4 contains experimental results on classic
datasets; ﬁnally, Section 5 concludes the paper and discusses
the directions of our future work.

2 Related Work
In this section we brieﬂy present recent approaches for learn-
ing low dimensional embeddings from points in high dimen-
sional spaces. Most of them are extensions of linear tech-
niques, such as Principal Component Analysis (PCA) and
Multi-Dimensional Scaling (MDS), based on the assump-
tion that nonlinear manifolds can be approximated by locally
In contrast to other methods, Sch¨olkopf et
linear patches.
al. [1998] propose kernel PCA that attempts to ﬁnd linear
patches using PCA in a space of typically higher dimension-
ality than the input space. Correct kernel selection can reveal
the low dimensional structure of the input data. For instance a
second order polynomial kernel can “ﬂatten” quadratic man-
ifolds.

Roweis and Saul [2000] address the problem by Locally
Linear Embedding (LLE). Processing involves computing re-
construction weights for each point from its neighbors. The
same weights should also reconstruct the point from its neigh-
bors in the low dimensional embedding space, since the
neighborhoods are assumed to be linear. The dimensional-
ity of the embedding however has to be given as a parameter
since it cannot always be estimated from the data [Saul and
Roweis, 2003]. LLE assures that local neighborhood struc-
ture is preserved during dimensionality reduction.

Isomap Tenenbaum et al. [2000] approximates geodesic
distances between points by graph distances. Then, MDS
is applied on the geodesic instead of Euclidean distances to
compute an embedding that forces nearby points to remain
nearby and faraway points to remain that way. A variation of
Isomap that can be applied to data with intrinsic curvature,
but known distribution, and a faster alternative that only uses
a few landmark point for distance computations have been
proposed in [de Silva and Tenenbaum, 2003]. Isomap and its
variants are limited to convex datasets.

Belkin and Niyogi [2003] compute the graph Laplacian of
the adjacency graph of the input data, which is an approxima-
tion of the Laplace-Beltrami operator on the manifold, and
exploit its locality preserving properties that were ﬁrst ob-
served in the ﬁeld of clustering. The Laplacian eigenmaps
algorithm can be viewed as a generalization of LLE, since
the two become identical when the weights of the graph are
chosen according to the criteria of the latter. The dimension-
ality of the manifold however cannot be reliably estimated
from the data, as in most of the work reviewed here. Donoho
and Grimes [2003] propose a similar scheme which computes
the Hessian instead of the Laplacian. The authors claim that
the Hessian is better suited for detecting linear patches on the
manifold. The major contribution of this approach is that it
proposes a global method, which, unlike Isomap, can be ap-
plied to non-convex datasets.

Weinberger and Saul [2004] address the problem of man-

ifold learning by enforcing local isometry. The lengths of
the sides of triangles of neighboring points are preserved dur-
ing the embedding. These constraints can be expressed in
terms of pairwise distances and the optimal embedding can
be found by semi-deﬁnite programming. The method is more
computationally demanding than LLE and Isomap, but can
reliably estimate the underlying dimensionality of the inputs
by locating the largest gap between the eigenvalues of the
Gram matrix of the outputs. Similarly to our approach, this
estimate does not require a threshold, as do approaches that
estimate the residual error as a function of the dimensionality
of the ﬁtted model.

An algorithm that computes a mapping, and not just an
embedding, of the data is described in [Brand, 2003]. The
intrinsic dimensionality of the manifold is estimated by ex-
amining the rate of growth of the number of points contained
in hyper-spheres as a function of the radius. Linear patches,
areas of curvature and noise can be discriminated using the
proposed measure. Afﬁne transformations that align the co-
ordinate systems of the linear patches are computed at the
second stage. This deﬁnes a global coordinate system for the
embedding and thus a mapping between the input space and
the embedding space.

Finally, we brieﬂy review approaches that estimate intrin-
sic dimensionality without any attempts to learn local struc-
ture. One such approach was presented in [Bruske and Som-
mer, 1998]. An optimally topology preserving map (OTPM)
is constructed for a subset of the data, which is produced after
vector quantization. Then PCA is performed for each node of
the OTPM under the assumption that the data are locally lin-
ear. The average of the number of signiﬁcant singular values
at the nodes is the estimate of the intrinsic dimensionality.
K´egl [2005] estimates the capacity dimension of the mani-
fold, which does not depend on the distribution of the data and
is equal to the topological dimension. An efﬁcient approxi-
mation based on packing numbers is proposed. The algorithm
takes into account dimensionality variations with scale and is
based on a geometric property of the data, rather than succes-
sive projections to increasingly higher-dimensional subspaces
until a certain percentage of the data is explained. Costa and
Hero [2004] estimate the intrinsic dimension of manifold and
the entropy of the samples using geodesic-minimal-spanning
trees. The method is similar to Isomap and thus produces a
single global estimate. Levina and Bickel [2005] compute
maximum likelihood estimates of dimensionality by examin-
ing the number of neighbors included in spheres whose radius
is selected in such a way that the density of the data can be
assumed constant, and enough neighbors are included. These
requirements cause an underestimation of the dimensionality
when it is very high.

The difference between our approach and those of [Bruske
and Sommer, 1998][Brand, 2003] [K´egl, 2005] [Weinberger
and Saul, 2004] [Costa and Hero, 2004] and [Levina and
Bickel, 2005] is that it produces reliable dimensionality es-
timates at the point level, which do not have to be averaged
over the entire dataset. We are also able to estimate the local
orientation of the manifold without being restricted to simple
linear models.

3 Tensor Voting
In this section we ﬁrst review the tensor voting framework
[Medioni et al., 2000], which can infer structures from sparse
and noisy data via a local voting process. Results have been
published mostly in 2- and 3-D domains, but the framework
can be extended to higher dimensions, as in [Tang et al.,
2001]. We begin by describing the basics of the framework:
data representation and voting. Then, we present the contri-
bution of this paper to the methodology, which is an efﬁcient
implementation, in terms of both space and time, of tensor
voting in high-dimensional spaces. We are able to operate in
spaces where that seemed impractical or impossible based on
the formulation of [Medioni et al., 2000].

3.1 Data Representation
The representation at each point is in the form of a second
order, symmetric, nonnegative deﬁnite tensor. The tensor can
also be viewed as a matrix or an ellipsoid. It represents the
normal space at the point. For instance, a hyper-plane has
one normal ~n, which can be encoded in tensor form as ~n~nT .
A structure with a normal space of rank d has d normals and
is represented by a tensor of the form:
~nd~nT
d

T =X

(1)

d

A point without orientation information can be equivalently
viewed as one having all possible normals and is encoded as
the identity matrix. A tensor in this form represents an equal
preference for all orientations and is called a “ball tensor”,
since the corresponding ellipsoid is a sphere or hyper-sphere.
On the other hand, a tensor that contains only one orientation
is called a “stick tensor”. Stick tensors represent the hyper-
plane of a D-dimensional space, which has one normal and
D − 1 tangents.

Depending on the type of structure the point belongs to,
it has a different number of normals and tangents. The ten-
sor’s eigensystem encodes this information. Eigenvectors that
belong to the tangent space correspond to zero eigenvalues,
while those that belong to the normal space correspond to
nonzero eigenvalues (typically equal to 1). Therefore, points
with known orientation can be encoded in this representa-
tion by appropriately constructed tensors, as in (1). Points
with unknown orientation are represented by identity matri-
ces (ball tensors) See Fig. 1(a) for example tensors in 3-D.

On the other hand, given a second order, symmetric, non-
negative deﬁnite tensor, the type of structure encoded in it
can be found by examining its eigensystem. Any tensor that
has these properties can be decomposed as in the following
equation:

D

λdˆedˆeT

T =X
D−1[(λd− λd + 1)X
X

d =
= (λ1 − λ2)ˆe1ˆeT
+ .... + λD(ˆe1ˆeT

d=1

1 + (λ2 − λ3)(ˆe1ˆeT
1 + ˆe2ˆeT
dˆedˆeT

d ]+ λD(ˆe1ˆeT

2 + ... + ˆedˆeT

1 + ˆe2ˆeT
2 )
d ) =

1 + ...+ ˆedˆeT
d )

d=1

k=1

(2)

(a) Example tensors

(b) Vote generation

Figure 1: Tensor Voting. (a) The shape of the tensor indicates
the type of structure at the location, while its size indicates the
conﬁdence of this information. The top tensor has a strong
preference of orientation and has higher conﬁdence than the
bottom one, which has no preference for any orientation. (b)
Vote generation as a function of the relative position of a stick
voter and the receiver and the orientation of the voter.

where λd are the eigenvalues in descending order and ˆed are
the corresponding eigenvectors. The tensor simultaneously
encodes all possible types of structure. The conﬁdence we
have in the type that has d normals is encoded in the differ-
ence λd − λd+1.
3.2 The Voting Process
The tensor voting framework was designed to enforce con-
straints, such as proximity, co-linearity and co-curvilinearity
with human perception in 2- and 3-D in mind. These con-
straints still hold in high dimensional spaces as long as one is
looking for structures that are smooth almost everywhere. In
this section we describe how information is propagated from
one point, the voter, to another, the receiver.

During the voting process, each point casts votes to each of
its neighboring points. The votes are also second-order, sym-
metric, nonnegative deﬁnite tensors. They encode the orienta-
tion the receiver would have, if the voter and receiver were in
the same structure. We begin by examining the case of a voter
that is associated with a stick tensor of unit length. The mag-
nitude of a vote cast by the stick tensor decays with respect
to the length of the smooth circular path connecting the voter
and receiver. The circle was selected since it maintains con-
stant curvature. It degenerates to a straight line if the vector
connecting the voter and receiver is orthogonal to the normal
of the voter. The orientation of the vote is towards the center
of the circle deﬁned by the two points and the orientation of
the voter. The vote is generated according to the following
equation and then transformed to the appropriate coordinate
system.

S(s, l, θ) = e

−( s2+cκ2

)

σ2

(cid:21)

(cid:20) −sin(2θ)
cos(2θ)
s = θk~vk
sin(θ) ,

[−sin(2θ) cos(2θ)]

κ =

2sin(θ)
k~vk

(3)

s is the length of the arc between the voter and receiver, and
κ is its curvature (see Fig. 1(b)), σ is the scale of voting, and
c is a constant. No vote is generated if the angle θ is more
than 45o. Also, the ﬁeld is truncated to the extend where the
magnitude of the vote is more than 3% of the magnitude of
the voter.

Since tensor voting is a function of the position of the voter
and the receiver and the orientation of the voter, vote gener-
ation by a stick tensor occurs in a 2-D subspace deﬁned by
the vector ~v that connects the two points and the normal of
the voter. Therefore, stick voting is fully deﬁned by Fig. 1(b)
and Eq. 3, regardless of the dimension of the input space.
According to [Medioni et al., 2000], the votes cast by other
types of tensors were pre-computed and stored in look-up ta-
bles, called voting ﬁelds. The presence of a normal space
of rank more than one is simulated by a rotating stick ten-
sor that spans the space. Since no closed form solution ex-
ists, the integration is numerically approximated. This pro-
cess becomes impractical as the dimensionality of the space
increases. Space requirements for storing D D-dimensional
voting ﬁelds with k samples per axis are O(DkD). At the
same time, the advantage of re-using pre-computed votes van-
ishes as the dimensionality increases. We propose instead a
novel, efﬁcient way to directly compute votes. Since integra-
tion is infeasible, we seek an approximate solution.

3.3 Efﬁcient Voting Scheme
The new scheme, proposed here, generates votes that contain
the orientation information that should be communicated to
the receiver, but not the uncertainty. The latter, in the orig-
inal framework, was conveyed by the ball component of the
vote that was due to the integration of votes cast by the rotat-
ing stick tensor. For instance, according to [Medioni et al.,
2000], a voting tensor that encodes a curve in 3-D (that has
two normals and a tangent) casts a vote that has a dominant
curve component and also some uncertainty. Here, we pro-
pose to propagate only the curve orientation at the receiver
given the orientation of the voter. The uncertainty in the new
formulation comes only from the accumulation of votes that
are not perfectly aligned.

Let ~v be the vector connecting the voting and receiving
points.
It can be decomposed into ~vt in the tangent space
of the voter (null in the case of a ball voter) and ~vn in the
normal space. The new vote generation process is based on
the observation that curvature in Eq. 3 is not a factor when
θ is zero, or in other words, if the voting stick is orthogonal

Figure 2: New scheme for vote generation. The voter here
is a tensor with two normals in 3-D. The vector connecting
the voter and receiver is decomposed into ~vn and ~vt that lie
in the normal and tangent space of the voter respectively. A
new basis that includes ~vn is deﬁned for the normal space
and each basis component casts a stick vote. Only the vote
generated by the orientation parallel to ~vn is not parallel to
the normal space. Tensor addition of the stick votes produces
the combined vote.

to ~vn. We can exploit this by deﬁning a new basis for the
normal space of the voter that includes ~vn. Then, the vote is
constructed as the tensor addition of the votes cast by stick
tensors parallel to the new basis vectors. Among those votes,
only the one generated by the stick tensor parallel to ~vn is
not parallel to the normal space of the voter. See Fig. 2 for an
illustration in 3-D. This scheme is applicable without changes
in the case of ball voters. Since two unoriented points deﬁne
a straight line in any space, the vote from a ball tensor should
have D−1 normals and one tangent. Since ~vt = ~0, the tensor
parallel to ~vn does not cast a vote due to the 45o cut-off rule.
The remaining D − 1 tensors in the new basis cast votes that
are equal in magnitude and span the space orthogonal to ~v,
and thus represent a straight line connecting the two points.

Tensors with unequal eigenvalues are decomposed before
voting according to Eq. 2. Then, each component votes sep-
arately and the vote is weighted by λd − λd+1, except the
ball component whose vote is weighted by λD. This imple-
mentation of tensor voting is more efﬁcient in terms of space,
since there is no need to store voting ﬁelds, and in terms of
time since we have devised a direct method for computing
votes that replaces the numerical integration of [Medioni et
al., 2000]. The new computation requires at most D compu-
tations of a stick vote according to Eq. 3, followed by the
addition of all the votes, instead of integration over D dimen-
sions. Experiments in 2- and 3-D, that cannot be included
here due to space limitations, validate the accuracy of the new
vote generation scheme.

3.4 Vote Analysis
Votes are accumulated at each point by tensor addition, which
is equivalent to the addition of matrices. After voting is com-
pleted, the eigensystem of the new tensor is analyzed and the
tensor is decomposed as in Eq. 2. In 3-D, the inference of
the type of structure that a point belongs is accomplished as
follows. The difference between the two largest eigenvalues
encodes surface conﬁdence, with a surface normal given by
~e1. The difference between the second and third eigenvalue
encodes curve conﬁdence, with a the normals to the curve
being ~e1 and ~e2. The smallest eigenvalue encodes junction
conﬁdence. Outliers receive little and inconsistent support
from their neighborhood and can be identiﬁed by their low
eigenvalues. Generalization to higher dimensions is straight-
forward, considering that there are D manifold types in D
dimensions. The dimensionality d is estimated by ﬁnding the
maximum difference λd − λd+1.

4 Experimental Results
In this section we present experimental results in dimension-
ality estimation and local structure inference. All inputs con-
sist of un-oriented points and are encoded as ball tensors.

Swiss Roll The ﬁrst experiment is on the “Swiss Roll”
dataset, which is available at http://isomap.stanford.edu/. It
contains 20, 000 points on a 2-D manifold in 3-D (Fig. 3).
We perform a simple evaluation of the quality of the orien-
tation estimates by projecting the nearest neighbors of each

Figure 3: The “Swiss Roll” dataset in 3-D

(a) Input

(b) 1-D points

point on the estimated tangent space and measuring the per-
centage of the distance that has been recovered. The accuracy
of dimensionality estimation at each point, the percentage of
recovered distances for the 8 nearest neighbors and execution
times on a Pentium 4 at 2.8 GHz, as a function of σ, can
be seen in Table 1. Performance is the same at boundaries.
The number of votes cast by each point ranges from 187 for
σ2 = 50 to 5440 for σ2 = 1000. The accuracy is high for a
large range of σ.

σ2

50
100
200
300
500
700
1000

Correct
Dim. (%)
93.10
99.24
99.91
99.96
99.95
99.88
99.67

Dist.
Rec. (%)
91.75
93.07
93.21
93.20
93.18
93.13
93.02

Time
(sec)
11
22
50
81
144
202
307

Table 1: Rate of correct dimensionality estimation and exe-
cution times as functions of σ for the “Swiss Roll” dataset

Structures with varying dimensionality The second
dataset is in 4-D and contains points sampled from three
structures: a line, a 2-D cone and a 3-D hyper-sphere. The
hyper-sphere is a structure with three degrees of freedom. It
cannot be unfolded unless we remove a small part from it.
Figure 4(a) shows the ﬁrst three dimensions of the data. The
dataset contains a total 135, 864 points, voting is performed
with σ2 = 200 and takes 2 hours and 7 minutes. Figures 4(c-
d) show the points classiﬁed according to their dimensional-
ity. Performing the same analysis as above for the accuracy
of the tangent space estimation, 91.04% of the distances of
the 8 nearest neighbors of each point lie on the tangent space,
even though both the cone and the hyper-sphere have intrin-
sic curvature and cannot be accurately approximated by linear
models. All the methods presented in Sec. 2 would fail for
this dataset because of the presence of structures with differ-
ent dimensionalities and because the hyper-sphere cannot be
unfolded.

Data in high dimensions The datasets for this experiment
were generated by sampling a few thousand points with low
intrinsic dimensionality (3 or 4) and mapping them to a
medium dimensional space (14- to 16-D) using linear and
quadratic functions. The generated points were then rotated

(c) 2-D points

(d) 3-D points

Figure 4: Data of varying dimensionality in 4-D. The ﬁrst
three axes of the input and the classiﬁed points are shown.

and embedded in a 50- to 150-D space while uniform noise
was added to all dimensions. The accuracy of dimensionality
estimation after tensor voting can be seen in Table 2.

Intrinsic
Dim.
4
3
4
3

Linear
Quadratic Space
Mappings Mappings Dim.
10
8
10
8

50
100
100
150

6
6
6
6

Dim.
Est. (%)
93.6
97.4
93.9
97.3

Table 2: Rate of correct dimensionality estimation for high
dimensional data

5 Conclusions
We have presented an approach to manifold learning that of-
fers certain advantages over the state-of-the-art. First, we are
able to obtain accurate estimates of the intrinsic dimension-
ality at the point level. Moreover, since the dimensionality is
found as the maximum gap in the eigenvalues of the tensor at
the point, no thresholds are needed. In most other approaches
the dimensionality has to be provided, or, at best, an average
intrinsic dimensionality is estimated for the entire dataset, as
in [Bruske and Sommer, 1998] [Brand, 2003] [K´egl, 2005]
[Weinberger and Saul, 2004] [Costa and Hero, 2004]. Sec-
ond, even though tensor voting on the surface looks like a
local covariance estimation, the fact that the votes are tensors
and not scalars reveals more information for each point. The
properties of the tensor representation, which can handle the
simultaneous presence of multiple orientations, allow the reli-
able inference of the normal and tangent space at each point.
Due to its unsupervised nature, tensor voting is very robust
against outliers, as demonstrated in [Medioni et al., 2000] for
the 2- and 3-D case. This property holds in higher dimen-
sions, where random noise is even more scattered. Lack of
space did not allow us to include outlier rejection results here.

An important advantage of tensor voting is the ab-
sence of global computations, which makes time complex-
ity O(Dnlogn). This enables us to process datasets with
very large number of points. Computation time scales lin-
early with the number of points assuming that more points
are added to the dataset in a way that the density remains con-
stant. In this case, the number of votes cast per point remains
constant and time requirements grow linearly. Complexity
is adversely affected by the dimensionality of the space D,
since eigen-decompositions of D × D tensors have to be per-
formed. For most practical purposes, the number of points
has to be considerably larger than the dimensionality of the
space (n (cid:29) D) to allow structure inference. Computational
complexity, therefore, is reasonable with respect to the largest
parameter.

It should also be noted that the votes attenuate with dis-
tance and curvature. This is a more intuitive formulation than
using the N nearest neighbors with equal weights, since some
of them may be too far or belong to a different part of the
structure. For both the  distance and the N nearest neighbor
formulations, however, the distance metric in the input space
has to be meaningful. This is also the case for all the methods
presented in Sec. 2.

The representation by tensors allows us to deal with struc-
tures that are not necessarily manifolds. These include non-
manifolds, such as hyper-spheres, intersecting structures and
datasets containing structures of different dimensionality. A
mapping from the input to the embedding space cannot be
computed in any of these cases, but it is not necessary for
many tasks including interpolation, function approximation
and local orientation (gradient) estimation. One can also view
our algorithm as a preprocessing stage that facilitates further
processing. Its outputs can be used as inputs for tasks such as
dimensionality reduction and mapping to a low dimensional
space, if desired.

Two issues require further investigation. The ﬁrst is a more
thorough analysis of the required properties of the data dis-
tribution. In line with other methods for manifold learning
([Roweis and Saul, 2000][Tenenbaum et al., 2000][Brand,
2003]), we assume that the data are distributed regularly is
space. A uniform distribution is ideal but not necessary, as
shown in Sec. 4, especially in the last experiment where the
points are mapped to the high dimensional space nonlinearly.
The second issue is that of data sufﬁciency for structure in-
ference. As the dimensionality of the space and the intrinsic
dimensionality of the structure grow, the number of points
necessary to infer the latter correctly also grows. We intend
to investigate the minimum requirements in terms of the num-
ber of points for inferring a certain type of structure in a space
of high dimensions.

Acknowledgement
This research has been supported by the U.S. National Sci-
ence Foundation grant IIS 03 29247.

References
M. Belkin and P. Niyogi. Laplacian eigenmaps for dimen-
sionality reduction and data representation. Neural Com-

putation, 15(6):1373–1396, 2003.

M. Brand. Charting a manifold. In Advances in Neural Infor-
mation Processing Systems 15, pages 961–968. MIT Press,
Cambridge, MA, 2003.

J. Bruske and G. Sommer. Intrinsic dimensionality estima-
tion with optimally topology preserving maps. IEEE Trans.
on Pattern Analysis and Machine Intelligence, 20(5):572–
575, May 1998.

J. Costa and A.O. Hero. Geodesic entropic graphs for dimen-
IEEE

sion and entropy estimation in manifold learning.
Trans. on Signal Process., 52(8):2210–2221, Aug. 2004.

V. de Silva and J.B. Tenenbaum. Global versus local meth-
ods in nonlinear dimensionality reduction.
In Advances
in Neural Information Processing Systems 15, pages 705–
712. MIT Press, Cambridge, MA, 2003.

D. Donoho and C. Grimes. Hessian eigenmaps: new tools
for nonlinear dimensionality reduction. In Proceedings of
National Academy of Science, pages 5591–5596, 2003.

B. K´egl. Intrinsic dimension estimation using packing num-
bers. In Advances in Neural Information Processing Sys-
tems 15, pages 681–688. MIT Press, Cambridge, MA,
2005.

E. Levina and P. Bickel. Maximum likelihood estimation of
intrinsic dimension.
In Advances in Neural Information
Processing Systems 17, pages 777–784. MIT Press, Cam-
bridge, MA, 2005.

G. Medioni, M.S. Lee, and C.K. Tang. A Computational
Framework for Segmentation and Grouping. Elsevier, New
York, 2000.

S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduc-
tion by locally linear embedding. Science, 290:2323–2326,
2000.

L. K. Saul and S. T. Roweis. Think globally, ﬁt locally: unsu-
pervised learning of low dimensional manifolds. Journal
of Machine Learning Research, 4:119–155, 2003.

B. Sch¨olkopf, A.J. Smola, and K.-R. M¨uller. Nonlinear com-
ponent analysis as a kernel eigenvalue problem. Neural
Computation, 10(5):1299–1319, 1998.

C.K. Tang, G. Medioni, and M.S. Lee. N-dimensional ten-
sor voting and application to epipolar geometry estimation.
IEEE Trans. on Pattern Analysis and Machine Intelligence,
23(8):829–844, August 2001.

Y.W. Teh and S. Roweis. Automatic alignment of local repre-
sentations. In Advances in Neural Information Processing
Systems 15, pages 841–848, Cambridge, MA, 2003. MIT
Press.

J.B. Tenenbaum, V. de Silva, and J.C. Langford. A global ge-
ometric framework for nonlinear dimensionality reduction.
Science, 290:2319–2323, 2000.

K.Q. Weinberger and L.K. Saul. Unsupervised learning of
image manifolds by semideﬁnite programming. In Proc.
Int. Conf. on Computer Vision and Pattern Recognition,
pages II: 988–995, 2004.


Maximum Margin Coresets for Active and Noise Tolerant Learning

Sariel Har-Peled∗ and Dan Roth†
Department of Computer Science

University of Illinois at Urbana-Champaign

{sariel, danr}@uiuc.edu

Dav Zimak†‡
Yahoo! Inc.

Santa Clara, CA

davzimak@yahoo-inc.com

Abstract

We study the problem of learning large margin half-
spaces in various settings using coresets and show
that coresets are a widely applicable tool for large
margin learning. A large margin coreset is a sub-
set of the input data sufﬁcient for approximating
the true maximum margin solution. In this work,
we provide a direct algorithm and analysis for con-
structing large margin coresets1. We show various
applications including a novel coreset based analy-
sis of large margin active learning and a polynomial
time (in the number of input data and the amount of
noise) algorithm for agnostic learning in the pres-
ence of outlier noise. We also highlight a simple
extension to multi-class classiﬁcation problems and
structured output learning.

1 Introduction

Large margin techniques are the basis for both practical al-
gorithms and theoretic analysis in machine learning. Algo-
rithmically, the most notable example is the support vector
machine (SVM) [Vap95] that ﬁnds a maximum-margin sep-
aration of a given data set. The SVM has proven very suc-
cessful in practice and theoretically, a large margin separation
implies good generalization performance [KS94].

The SVM has a simple representation and a straightfor-
ward implementation — ﬁnd the set of support vectors that
uniquely deﬁne the maximum margin separation. Amazingly,
this approach simultaneously allows the classiﬁer to be rep-
resented with a (potentially small) subset of the input data
and, through the use of kernel functions, to utilize an arbi-
trarily powerful hypothesis space. If a small support set can
be found, then one can guarantee high performance on unseen

∗

Work on this paper was partially supported by a NSF CAREER

award CCR-0132901.

†

‡

Work on this paper was partially supported by NSF grants ITR-

IIS-0085980 and IIS-9984168.

Much of this work was done while at the University of Illinois.
1Throughout the paper, we often present only proof sketches
due to lack of space. The full proofs appear in Technical Report
UIUCDCS-R-2006-2784 available from the University of Illinois
Computer Science Department.

data when using a hypothesis class with unbounded complex-
ity. Unfortunately, there is no guarantee that the size of the
support set will be small. Additionally, the running time of
the SVM algorithm to ﬁnd an exact solution to the large mar-
gin problem is O(m3) time and O(m2) space using m exam-
ples and is infeasible for large datasets.

Most practical algorithms, such as chunking [Vap82], de-
composition [OFG97], and sequential minimal optimiza-
tion (SMO) [Pla98], reduce the problem to manageable sub-
tasks are heuristic solutions which may converge slowly.
Furthermore, the running time remains crucially dependent
on the number of support vectors in the solution. Alterna-
tively, recent work has focused on on-line approaches to ap-
proximate the maximum-margin algorithms [Kow00; Gen01;
LL02]. On-line algorithms are iterative solutions that add ex-
amples to the large margin solution based on various condi-
tions – all related to the relative margin of the example under
consideration. As a result, they can bound the number of ex-
amples necessary to guarantee a large margin separation.

, where ρ∗

In this paper we relax the requirement that we ﬁnd the
unique maximum margin separation. Speciﬁcally, we ﬁnd
an approximate maximum margin separation – a hyperplane
that separates all of the input data with margin larger than
(1 − )ρ∗
is best achievable. We use the coreset
method ﬁrst described in [BC03] and extended to the maxi-
mum margin setting in [TKC05]. A coreset for a maximum
margin separating hyperplane is a subset, C ∈ D of exam-
ples such that the maximum margin hyperplane on C is an
approximate maximum margin separating hyperplane on D.
In some sense, it captures all of the necessary information for
the approximation just as the set of support vectors does for
the true maximum margin separating hyperplane.

This paper studies the running time of a simple coreset al-
gorithm for binary and structured-output classiﬁcation and
the use of the coreset as an analysis tool for active learning
and noise-tolerant learning in the agnostic setting. In previ-
ous work, the coreset was constructed as a reduction to ﬁnd-
ing a coreset for a different problem – the minimum enclos-
ing ball [BC03]. In Section 3, we show a direct algorithm for
ﬁnding a coreset of size at most |C| = O((R/ρ∗)2/) in time
O(nd|C| + |C|T (|C|)) where R and ρ∗ measure the size of
the example set and the quality of the maximum-margin clas-
siﬁer and T (|C|) is the time to run an SVM black-box on |C|
examples. This improves previous bounds by a factor 1/ and

IJCAI-07

836

provides an explicit running time to the algorithm.

Algorithm CORESET SVM

δ

e

d|C|

(cid:3)

In Section 4 we analyze one of

(cid:2)
ln |C| + ln 1

the most effec-
tive active learning algorithms based on the maximum-
margin principal [TK02] and give a running time and a
(1 − )-approximation guarantee. We show that in time
+|C|T (|C|)) one can compute a core-
O(
set C of size at most |C| = O((R/ρ∗)2/2) such that with
high conﬁdence, 1 − δ, the classiﬁer produced will have large
margin and small, e, error. In Section 5, we analyze learn-
ing with outlier noise. Roughly speaking, a set of outliers is
a small subset of the input data such that, if removed would
yield the correct maximum-margin classiﬁer. Thus if we as-
sume there are k outliers, the best maximum margin classi-
ﬁer is well-deﬁned. We show a polynomial time algorithm
for learning the maximum margin separation in this setting.
Finally, in Section 6 we highlight an important connection
between the coreset algorithm and recent work for learning
SVM for structured output [THJA04]. Indeed, we can view
SVMstruct as a coreset algorithm.

2 Preliminaries
We are given D = {(x1, y1), . . . , (xM , yM )}, a labeled train-
ing set of cardinality M drawn from some distribution DX ,Y ,
where xm ∈ X are the examples in a inner-product space
and ym ∈ Y are labels. For most of the paper, we assume a
, and binary output, Y = {−1, 1}.
real valued input, X = R
However, in Section 6 we note an important extension to the
structured output domain.

d

In this paper, we use the maximum margin principle to
discover a hypothesis h ∈ H represented by a halfspace
h(x) = argmaxy∈{−1,1} y(w · x), where w ∈ R
. The bi-
nary margin (or geometric margin), ρ(w, x, y) = y(w · x),
for an example, x, is deﬁned as the distance from the exam-
ple to the discriminating hyperplane w · x = 0. Notice that a
negative margin is indicative of a misclassiﬁed example. The
margin of hypothesis h is ρ(h, D) = min(x,y)∈D ρ(w, x, y).
Therefore, given a sample, D, the maximum margin hy-

d

pothesis (hyperplane) is

L(D) = argmax

w∈RD ,||w||=1

min

(x,y)∈D

y(w · x)

is the uniform length hyperplane with maximum margin over
the data.

Deﬁnition 2.1 ((1 − )-Approximation) A hypothesis h is
a (1 − )-approximation to the optimal hypothesis h∗
if
ρ(h, D) ≥ (1 − )ρ(h∗, D).

A maximum
Deﬁnition 2.2 (Maximum margin coreset)
margin coreset is a set of examples C = C(, ρ) ⊂ D such
that h = L(C) is a (1 − )-approximation to h∗ = L(D).

3 Coreset Learning Algorithm
Large margin coresets were ﬁrst introduced in [TKC05] to
form the Core Vector Machine (CVM). In that work, they re-
duced ﬁnding a maximum margin hyperplane to ﬁnding the
minimum enclosing ball of a set of points around the origin.

INPUT:

S = ((x1, y1), . . . , (xm, ym)),
Approximation parameter  ∈ (0, 1)

(cid:4)

(cid:5)m

where S ∈

R

d × {−1, 1}

OUTPUT: A classiﬁer h ∈ H

begin

Set C = ((x1, y1))
For i = 1 . . . T

Set hi = SVM(C)
Set ρi = ρ(hi, C)
(xmin, ymin) = argmin(x,y)∈S\C ρ(hi, x, y)
if ρ(hi, xmin, ymin) < (1 − )ρi

(cid:6)
(xmin, ymin)

C = C

else

return hi

Return SVM(C)

end

Figure 1: Maximum-margin learning via coresets.

For the latter task, there exists a coreset algorithm that runs
in time linear in the number of points [BC03]. In this sec-
tion, we provide very similar results with a slight (factor 1/)
improvement by providing a direct algorithm and analysis.

In Figure 1, the simpliﬁed coreset algorithm is presented
for learning binary labeled data in a noise-free setting. The
coreset C is built iteratively. At each step, we construct the
true maximum margin classiﬁer, hi = SVM(C) and use it to
ﬁnd the example with the smallest (or negative) margin. This
example is then added to the coreset and the process repeats.
It is possible to tell that hi is a (1 − )-approximation by ob-
serving the ratio between the margin on the coreset, ρ(hi, C),
and the margin on the entire data set, ρ(hi, D). If this ratio
is small enough then the margin of hi on the entire data set is
sufﬁciently large and the algorithm halts.

Lemma 3.1 Let ρ∗ = ρ(L(D), D) be the optimal margin
for data set D ∈ {R
of size M . Given a
parameter,  ∈ (0, 1), one can compute a coreset C of size
|C| = O((R/ρ∗)2/) in time O(nd|C| + |C|T (|C|)), where
R = max(x,y)∈D ||x||.

d × {−1, 1}}M

Proof sketch: We show using a simple geometric argument
that each time an example is added to the coreset, the margin
on the next integration decreases by at least a constant factor.
That is,

(cid:8)

ρi+1 ≤

ρi,

(1)

(cid:7)
1 −

α2
i
8R2

where αi = ρi − ρ(hi, xi) measures how much the added
example violates the current margin guess using the current
hypothesis hi. Then, it can be shown that the decreasing se-
quence of margins will be smaller than ρi ≤ (1 + )ρ after at
64R2
most
ρ2 steps. Once the margin is small enough, we show
that it quickly decreases and outputs a (1 − )-approximation
to the maximum margin classiﬁer.

IJCAI-07

837

At each step, the algorithm in Figure 1 adds the example
with the smallest margin to the coreset. However, the algo-
rithm is easily modiﬁed such that any example with margin
small enough can sufﬁce. Speciﬁcally, if we know that each
example (x, y) added to the coreset is such that ρ(hi, x, y) <
(1−)ρi, but not necessarily the example with minimum mar-
gin, the algorithm still converges, but with a larger coreset.

Corollary 3.2 Let ρ∗ = ρ(L(D), D) be the optimal margin
for data set D ∈ {R
of size M . Given a
parameter,  ∈ (0, 1), one can compute a coreset C of size
|C| = O((R/ρ∗)2/2) in time O(nd|C| + |C|T (|C|)), where
R = max(x,y)∈D ||x||.

d × {−1, 1}}M

Proof sketch: If, rather than add the minimum margin ex-
ample at each step, we add any example with small enough
margin, ρ(hi, x, y) < ρi, then we can simplify the simple
geometric proof sketch for Lemma 3.1 and show that

(cid:7)
1 − ρi

(cid:8)

ρi+1 ≤

2
8R2

ρi.

Then, similarly to Lemma 3.1, we use this decreasing se-
quence to prove the result.

3.1 Related Work

Central to the coreset algorithms presented here, is the idea
of iteratively building a working set of examples by carefully
selecting examples to add at each step. In the online learning
literature, this idea has appeared in work related to the coreset
approach.

Indeed, even the perceptron algorithm can be viewed as
building a working set. At each iteration, an example,
(xi, yi), is added to the working set if yi(wi · xi) < 0. The
hypothesis is a linear sum of elements in the set.

2 log R

Various approximate algorithms for online learning have
In [Kow00], Kowalczyk proposed a
also been proposed.
perceptron-like update rule, with various criteria for choos-
ing which example to update. One of them is exactly the
minimum margin approach used in coresets. After only
O( R2
2 ) updates, the algorithm would converge to a
(1 − )-approximate classiﬁer — a result very similar to
ours, modulo 1/ and log R terms. At roughly the same
as Kowalczyk’s algorithm, two additional algorithms were
proposed:
the Relaxed Online Maximum Margin Algo-
rithm (ROMMA) and the Approximate Large Margin Algo-
rithm (ALMAp). ROMMA is an online algorithm that learns
a (1 − )-approximate maximum margin separation. Both
have similar selection criteria, and perform similarly in prac-
tice. ALMAp also provides a mistake bound of O( R2

2ρ2 ).

Recently a new algorithm, SVM-Perf, was introduced and
implemented [Joa06] with similar O( 1
2 ) bounds. SVM-Perf
is presented as a cutting-plane algorithm, where at each iter-
ation a cutting-plane is found that represents a ﬁxed subset
of the training examples. The coreset methods are a special
case of the cutting-plane algorithm where each cutting plane
is described by a single example. Indeed, this very clever al-
gorithm converges very fast. Many experimental results are
presented that show the fast convergence time in practice, and

we think this work helps support our claim that coreset-based
algorithms can be practical.

4 Maximum Margin Active Learning

In active learning, the learner is presented with a set of un-
labeled data, U = {x1, . . . , xm} and an oracle, ORACLE :
X → {−1, 1} that provides a label to any example x ∈ U
consistent with a large margin hypothesis. The goal is to
learn exactly this maximum margin separation using a lim-
ited number of oracle queries.

Recently,

iterative algorithms for active learning SVM
have been proposed [TK02; CCS00]. After presenting a
slight modiﬁcation of these algorithms using coresets and in-
troducing an explicit stopping criteria, we show that the al-
gorithm converges quickly to a (1 − )-approximation of the
true maximum margin hypothesis that exists given all labels
(with high probability).

4.1 Coreset Active Learning Algorithm
In Figure 2, the active learning algorithm from [TK02] is
adapted by adding a veriﬁcation stage. The algorithm runs
in iterations, where at each step, the unlabeled example that
is closest to the decision boundary is added to the coreset.
However, if there are no examples near the decision bound-
ary (i.e. they are further than the current large-margin guess),
we may think that all labels are classiﬁed correctly and thus
the algorithm can halt. Of course, since the labels are un-
known, it is possible that there are still a large number of
misclassiﬁed examples. At this point the algorithm enters a
veriﬁcation phase, VERIFY(), where examples are sampled
uniformly at random from the entire data set according to
UNIFORMRANDOM() and labeled using ORACLE().

It is important to note that the algorithm presented in Fig-
ure 2 repeatedly cycles over the unlabeled dataset to ﬁnd the
single example closest to the decision boundary. This is eas-
ily modiﬁed to two important cases when the number of ex-
amples is very large (i.e. m >> |C|) or when there is an
inﬁnite stream of examples (i.e. m = ∞). In these cases, any
example, x where |ρ(hi, x, y)| < (1 − )ρi can be added to
the coreset and the algorithm can halt after enough examples
are seen without making a mistake. Indeed Lemma 4.2 below
applies to these more general cases.

4.2 Analysis

Algorithm 2 seeks the true maximum margin hypothesis of
the data that would be found if all of the labels were known.
Here, we show that with high probability (1 − δ), it ﬁnds a
(1−a)-approximation to this hypothesis. Unfortunately, it is
impossible to guarantee error free learning (see Section 4.3),
so we must accept a small, e, prediction error.

The analysis follows from two facts. First, there exists a
coreset that can be constructed by adding examples that lie
close to the decision boundary. Any example with very small
margin (< (1 − a)ρi) will improve the approximation ir-
respective of the actual label. Second, the veriﬁcation stage
will halt either because it has found an example with very
small (i.e. negative) margin that will improve the coreset or
because enough examples have been seen with no mistakes.

IJCAI-07

838

Algorithm ACTIVE CORESET SVM

INPUT: Data U = (x1, . . . , xm) ∈
OUTPUT: A classiﬁer h(cid:4) : R

d → {−1, 1}

(cid:5)m

(cid:4)

d

R

Using Lemma 4.1 and Corollary 3.2, we bound the running
time of this algorithm to converge to an approximate solution.

begin

Set C = ∅
Repeat until Halt

Set hi = SVM(C)
Set ρi = ρ(hi, C)
xmin = argminx∈U\C ρ(hi, x, y)
if miny∈{−1,1} |ρ(hi, xmin, y)| < (1 − )ρi

(cid:6)
ymin = ORACLE(xmin)
(xmin, ymin)
C = C

else

(xv, yv) = VERIFY(U \ C, hi)
if xv = NULL

return hi and Halt

(cid:6)
(xv, yv)

else

C = C

else

Return SVM(C) and Halt

end

(a) ACTIVE CORESET SVM

Algorithm VERIFY

INPUT:

Data U = (x1, . . . , xm) ∈ {R
d → {−1, 1}
A classiﬁer h : R

d}m

OUTPUT:

(x, y) ∈ R

d × {−1, 1} or NULL

begin

for i = 1 . . . T do

x = UNIFORMRANDOM(U )
y = ORACLE(x)
if ρ(h, x, y) < 0
Return (x, y)

Return NULL

end

(b) VERIFY

Figure 2: (a) Active learning using coresets. Abusing nota-
tion, U \ C = (x ∈ U |(x, ORACLE(x)) (cid:9)∈ C). (b) Verify
procedure. UNIFORMRANDOM(U ) returns a random exam-
ple from the unlabeled set. ORACLE(x) returns the correct
label for x and UNIFORMRANDOM(U ) selects an example
from U uniformly at random.

In the latter case, we can apply the following lemma, adapted
from [KLPV87; Ang87] that shows learning from member-
ship queries can be used to give PAC-bounds.

Lemma 4.1 If L is a conservative on-line algorithm with
mistake bound M and access to an example oracle
ORACLE() drawing examples i.i.d. from distribution D, then
calls to ORACLE(), with con-
after at most
ﬁdence 1 − δ, L produces a hypothesis with expected error
less than  on examples drawn from D.

(cid:2)
ln M + ln 1

(cid:3)

M


δ

Lemma 4.2 Let D = ORACLE(U ), be the entire labeled
data set and ρ∗ = ρ(L(D), D) be the optimal margin for
data set D of size M . Given parameters, δ, e, a ∈ [0, 1],
(cid:2)
one can compute a coreset C of size at most O((R/ρ∗)2/2
a)
ln |C| + ln 1
+ |C|T (|C|)), where R =
in time O(
(cid:2)
(cid:3)
max(x,y)∈D ||x||. The total number of calls to ORACLE()
ln |C| + ln 1
is less than O(
), and T(m) is the running
δ
time of the SVM for m examples.

|C|
e

(cid:3)

d|C|

e

δ

Proof: The coreset will be at most |C| = O((R/ρ∗)2/2
a)
as a result of Corollary 3.2 by noticing that each time an ex-
ample, (x, y) is added to the coreset, ρ(hi, x, y) < (1 − )ρi
— either because an unlabeled example is added in Line (1)
in Algorithm 2 (a) or because a labeled example is added
in Line (2) in Algorithm 2 (b). In the former we know that
ρi − |ρ(hi, x, y)| > ρi, and in the latter we know that
ρ(hi, x, y) < 0.

Since we know that at most

|C| examples will be
added to the coreset, and at each iteration,
the margin
(cid:2)
of the current working hypothesis decreases we can use
Lemma 4.1 to bound the total number of Oracle queries in
1
ln |C| + ln 1
to ensure 1−δ conﬁdence that the classiﬁer
e
has at most e mistakes. Therefore, the total number of calls
(cid:3)
to ORACLE() is at most O(|C| + |C| 1
) =
e
O(|C| 1
). The running time follows since
e
SVM must be run each time an example is added to the core-
set.

(cid:2)
ln |C| + ln 1

(cid:2)
ln |C| + ln 1

(cid:3)

(cid:3)

δ

δ

δ

4.3 Related Work
Previously, the efﬁcacy of maximum margin active learning
algorithms were explained because by choosing the example
closest to the decision boundary, the version space will be ap-
proximately halved [TK02]2. More precisely, it was argued
that because at each iteration the version space is an intersec-
tion of half-spaces in the kernelized feature space. If we as-
sume that each example is of constant size (i.e. ||x|| = 1) then
the hypothesis with maximum margin separation is a point in
the version space at the center of the largest enclosed ball in
this polytope. Therefore, by choosing an example with small
margin, it is hoped that it comes close to bisecting the en-
closed ball and also the version space.

If one could guarantee that the version space was in-
deed halved at each iteration, then the algorithm would con-
verge quickly to the true maximum margin hypothesis [TK02;
FS97] Unfortunately, no such guarantee can be made, either
in practice or in theory, thus the “halving” argument falls
short to adequately explain the practical success of choosing
the minimum absolute margin example at each iteration.

In addition, as ﬁrst presented in [Das05] a zero-error active
learning algorithm is impossible without requesting the label

2In [TK02] it is assumed that ||x|| = 1 for all examples. While
relaxing this assumption does cause a different view of the versions
space and changes the motivation, it does not affect their results. In-
deed, here, we propose an alternate justiﬁcation that does not depend
on the version space.

IJCAI-07

839

Algorithm OUTLIER SVM

INPUT:

Data D = ((xm, ym))M

1 ∈

(cid:4)

R

d × {−1, 1}

(cid:5)M

OUTPUT:

A classiﬁer h : R

d → {−1, 1}

begin

Set R = max(x,y)∈D ||x||
For i = 0, 1, 2, . . .
(cid:10)

(cid:9)
ρ = R/2i
32 R2
Set c =
ρ2
For each subset Ds ∈

(cid:2)

hs = SVM(C)

(cid:3)

D
c

hmax = argmaxs ρk(hs, D)
if ρk(hmax, D) > ρ

Return hmax and Halt.

end

(a) OUTLIER SVM

Algorithm SIMPLE OUTLIER SVM

INPUT:

Data D = ((xm, ym))M
1

OUTPUT:

A classiﬁer h : R

d → {−1, 1}

begin

(cid:3)

Set R = max(x,y)∈D ||x||
For i = 0, 1, 2, . . .
(cid:2)

Set c = 2i
For each subset Ds ∈
hs = SVM(Ds)
(cid:11)
32 R2
c
Return hmax and Halt.

hmax = argmaxs ρk(hs, D)
if ρk(hmax, D) >

D
c

end

(b) SIMPLE OUTLIER SVM

Figure 4: (a) Approximate maximum-margin learning with
outlier noise. ρk(h, D) returns the margin of the example
in D that is smaller than the margin of all but k examples
(i.e. the k + 1-th smallest margin). (b) Simple algorithm. An
alternate description of the algorithm that simply doubles the
size of the sub-sample sets at each iteration.

expected coreset size. This provides an extremely simple
polynomial-time algorithm for learning with noise.

Lemma 5.2 Let ρk = ρ(L(D \ Vk), D \ Vk) be the op-
timal margin for data set D of size M with k outliers.
Given a parameter, , one can compute a separating hy-
perplane with margin (1 − )ρk on D \ Vk in polynomial
time O(dT (c)M c+1 log M ), where c = O((R/ρk)2/). and
R = max(x,y)∈D ||x||.

(a)

(b)

Impossibility of Zero-Error Approximation: (a)
Figure 3:
Active learning nightmare – all examples are positive, with a
single negative. (b) Nightmare in 3D – 2 negative points, one
below origin, a second on the plane of the circle.

2

of all examples. To see this, we consider a sample of exam-
ples spread at a constant interval on the surface of a circle in
R
. See Figure 3 for an illustration. The concept represented
by Figure 3(a) is one where a single example is negative and
the rest are positive. The maximum margin separation thus
separates a single example from the rest. Consider any algo-
rithm that computes the maximum margin separation of any
labeled subset of this data. Unless the single negative exam-
ple is included in the labeled subset, then there is no hope of
achieving an approximate large-margin separation that cor-
rectly classiﬁes all examples in the data set. Therefore, if an
adversary controls the oracle, by simply answering “+” to ev-
ery query until the ﬁnal query, the learner is forced to ask the
label of every example.

5 Agnostic Learning with Outlier Noise

Learning in the presence of noise is of great interest. While
there are many deﬁnitions of noise, such as attribute, label,
and malicious noise, we consider a very general model, out-
lier noise. One can think of outlier noise in the following
way: without the “noisy” examples, a “clean” function could
be learned. Thus if the noisy examples could be identiﬁed
a priori, we could learn the true maximum margin classiﬁer.
In some sense, many types of noise can be viewed as outlier
noise, so the analysis presented in this section can be widely
applied. We show a polynomial time algorithm for learning
an approximate maximum margin hyperplane in the presence
of outliers.

Deﬁnition 5.1 (Outlier Set) Consider a data set D =
{(xi, yi)}M
i=m of binary (y ∈ {−1, 1}) examples. For any
set of outliers, V ⊆ D, we can consider the maximum mar-
= L(D(cid:4)), on the examples D(cid:4) = D \ V .
gin hyperplane, hD(cid:2)
Then an outlier set of size k is a subset, Vk, of size k that
achieves maximum margin on the remaining data

Vk = argmax
V ∈D,|V |=k

ρ(L(D \ V ), D \ V ).

Because a coreset exists for the “clean” data and we know
that there are at most k (or a ﬁxed fraction) outliers, we
avoid exponential running time by subsampling based on the

Proof sketch:Because the clean data, D \Vk), contains a core-
set of size c = O((R/ρk)2/) from Lemma 3.1, it sufﬁces to
ﬁnd this set and observe that there are at most k outliers. It

IJCAI-07

840

(cid:2)

(cid:3)

M
c

is possible to examine all
subsets of the input data. For
each one, create the maximum margin hypothesis using an
SVM black-box and measure the margin obtained by remov-
ing the k examples with minimum margin (or negative mar-
gin). Then, one of these hypotheses will have the maximum
margin and have at most k outliers. Finally, since the margin
is unknown a-priori, we must repeat the above procedure for
exponentially decreasing guesses and stop once we see only
k outliers with margin as large as the guess.

6 Structured Output Learning
Structured output learning is one of the most important new
areas in machine learning. Structured output can be se-
quences, trees, rankings and general structures and are ubiq-
uitous in important applications in areas from NLP to web
search to biology. Machine learning have begun to address
these problems. Here, we show a common modeling ap-
proach for structured output learning and highlight the con-
nection to standard maximum margin learning.

Structured classiﬁers produce complex, structured output
, where Y l ∈ {1, . . . , K}. It is common

Y = Y 1 × . . . × Y L
to write the decision rule as

h(x) = argmax

y(cid:2)∈Y

w · Φ(x, y

(cid:4)

),

References

D. Angluin. Queries and concept learning. Machine Learn-

ing, 2(4):319–342, 1987.

M. Badoiu and K.L. Clarkson. Smaller core-sets for balls. In

SODA, pages 801–802, 2003.

C. Campbell, N. Cristianini, and A.J. Smola. Query learning
with large margin classiﬁers. In Proceedings of the Seven-
teeth ICML, pages 111–118, 2000.

S. Dasgupta. Analysis of a greedy active learning strategy. In

NIPS, volume 17, pages 337–344. MIT Press, 2005.

Y. Freund and R.E. Schapire. A decision-theoretic general-
ization of on-line learning and an application to boosting.
J. Comput. Syst. Sci., 55(1):119–139, 1997.

C. Gentile. A new approximate maximal margin classiﬁcation

algorithm. JMLR, 2:213–242, 2001.

T. Joachims. Training linear svms in linear time. In Proceed-

ings of the Twelfth ACM SIGKDD, New York, 2006.

M. Kearns, M. Li, L. Pitt, and L. Valiant. Recent results on
boolean concept learning.
In Proceedings of the Fourth
International Workshop on Machine Learning, pages 337–
352, 1987.

where Φ(x, y) represents features of each (example, label)
pair. Therefore, the maximum margin hypothesis (hyper-
plane) is

A. Kowalczyk. Maximal margin perceptron.

In Advances
in Large Margin Classiﬁers. MIT Press, Cambridge MA,
2000.

LS(D) = argmax

w∈RD ,||w||=1

min

(x,y)∈D

ρ(w, x, y),

where

ρ(w, x, y) = min
y(cid:2)(cid:5)=y

w · (Φ(x, y) − Φ(x, y

(cid:4)

))

As a result, the algorithms and analysis presented in this

paper extend to the structured output setting.

6.1 Related Work
Indeed, the SVM was recently extended to the structured out-
put domain. SVMstruct [THJA04] is an algorithm that runs in
(cid:4))-triples to the working
iterations, each time adding (x, y, y
(cid:4)) feature vec-
set. Indeed, they use exactly the same Φ(x, y, y
tor used here. Then, the working hypothesis is updated (in the
dual) by optimizing over the Lagrange multipliers, similar to
the Sequential Minimal Optimization (SMO) procedure intro-
duced by Platt [Pla98]. They also show a bound of O( R2
2ρ2 )
on the size of the working set.

7 Conclusions
In this paper, we give a simple coreset algorithm with an im-
proved bound on the coreset size. We are mainly concerned
with running time analysis of various algorithms. Using core-
sets, we give bounds for maximum margin active learning and
structured output learning. We also formulate a novel and
polynomial time algorithm for learning in the agnostic (noisy)
setting. Coresets are a very general tool in approximation al-
gorithms and we have shown that they have important uses
in maximum margin learning and analysis. We think that this
is the tip of the iceberg, and envision that coresets will ﬁnd
many more applications in machine learning.

M. Kearns and R. Schapire. Efﬁcient distribution-free learn-
ing of probabilistic concepts.
In Computational Learn-
ing Theory and Natural Learning Systems, Constraints and
Prospect, volume 1. The MIT Press, 1994.

Y. Li and P.M. Long. The relaxed online maximum margin

algorithm. Machine Learning, 46(1-3):361–387, 2002.

E. Osuna, R. Freund, and F. Girosi. Improved training algo-

rithm for support vector machines. NNSP, 1997.

J. Platt. Sequential minimal optimization: A fast algorithm
for training support vector machines. Technical Report 98-
14, Microsoft Research, April 1998.

I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
Support vector machine learning for interdependent and
structured output spaces. In Proceedings of the twenty-ﬁrst
ICML, page 104. ACM Press, 2004.

S. Tong and D. Koller. Support vector machine active learning
with applications to text classiﬁcation. JMLR, 2:45–66,
2002.

I.W. Tsang, J.T. Kwok, and P. Cheung. Core vector machines:
Fast svm training on very large data sets. JMLR, 6:363–
392, 2005.

V.N. Vapnik. Estimation of Dependences Based on Empirical
Data. Springer Series in Statistics. Springer, New York,
1982.

V.N. Vapnik.

The nature of statistical learning theory.

Springer Verlag, Heidelberg, DE, 1995.

IJCAI-07

841


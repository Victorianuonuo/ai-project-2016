AWA* - A Window Constrained Anytime Heuristic Search Algorithm

Sandip Aine

P. P. Chakrabarti

Rajeev Kumar

Department of Computer Science & Engineering
Indian Institute of Technology Kharagpur, India.

{sandip, ppchak, rkumar}@cse.iitkgp.ernet.in

Abstract

This work presents an iterative anytime heuris-
tic search algorithm called Anytime Window A*
(AWA*) where node expansion is localized within
a sliding window comprising of levels of the search
tree/graph. The search starts in depth-ﬁrst mode
and gradually proceeds towards A* by increment-
ing the window size. An analysis on a uniform tree
model provides some very useful properties of this
algorithm. A modiﬁcation of AWA* is presented
to guarantee bounded optimal solutions at each it-
eration. Experimental results on the 0/1 Knapsack
problem and TSP demonstrate the efﬁcacy of the
proposed techniques over some existing anytime
search methods.

1 Introduction

Development of optimization methods that can work within
time limitations is a major need for modern practitioners fac-
ing large-sized problems that are NP-hard in nature. This re-
quirement has spawned research in the direction of designing
anytime algorithms [Dean and Boddy, 1988], i.e., the algo-
rithms which can work under any time limitations. Anytime
algorithms are expected to regularly produce solutions of im-
proved quality and when suddenly terminated, return the best
solution produced so far as the result.

Among heuristic search techniques, the depth-ﬁrst branch
and bound (DFBB) approach and its variants [Sarkar et
al., 1991; Zhang, 1998] are by default anytime algorithms,
as they produce a stream of gradually improving solu-
tions. However, in general, the depth-ﬁrst techniques suf-
fer from excessive node expansions before they actually
reach the optimal solution.The best-ﬁrst anytime search algo-
rithms can be broadly classiﬁed into two categories, namely,
weighted heuristic search [Pohl, 1970; Hansen et al., 1997;
Zhou and Hansen, 2002; Likhachev et al., 2004] and beam
search [Zhou and Hansen, 2005; Furcy, 2006].

In weighted heuristic search, the heuristic estimate is mul-
tiplied by a weight, w (w ≥ 1), to yield f (n) = g(n) +
w ∗ h(n). In the anytime variants of weighted A*, a series of
solutions are produced either by iterating with the same cho-
sen weight [Hansen et al., 1997; Zhou and Hansen, 2002] or

by gradually decrementing the weight using a chosen Δ af-
ter each iteration [Likhachev et al., 2004]. Another algorithm
was proposed to modify the weighted A* technique with se-
lective commitments [Kitamura et al., 1998]. One of the ma-
jor disadvantages of using the weighted A* based techniques
is that weighing the heuristic introduces non-admissibility.
As the basic characteristics of A* change substantially with
non-admissible heuristics [Pearl, 1984], it becomes very dif-
ﬁcult to establish a relation between node expansions and the
weighing factors (w,Δ). Therefore, to use a weighted A*
technique, lengthy simulations are required to ﬁnd appropri-
ate parameter settings. Also, there are applications where
no speciﬁc heuristic for the remaining path is available, the
search may be guided only by the g(n) information [Pearl,
1984] or a lower bound f (n)-value may be used without dis-
tinct g(n) and h(n) components [Kumar et al., 1995]. In such
cases, the weighted A* techniques are not directly applicable.
On the other hand, in beam search, a chosen number of
most promising nodes at each level of the search tree/graph
are selected for further expansion, and the remaining nodes
are pruned. Several anytime variants of the basic beam
search technique [Zhang, 1998; Zhou and Hansen, 2005;
Furcy, 2006] have been applied for different problems with
reasonably good results. However, the lack of models, which
can provide the user an apriori estimate about the quality-time
trade-off, remains a problem for these algorithms as well.

In this work, we present an alternative anytime heuristic
search algorithm Anytime Window A* (AWA*), which local-
izes the global competition performed by A* within a ﬁxed-
size window comprising of levels of the search tree/graph.
The nodes which lie outside the active window are not con-
sidered for expansion at that point. The window is slid down-
wards to provide a depth-ﬁrst component to the best-ﬁrst
search that occurs within a window. The window size is set
to 0 to start with (depth-ﬁrst mode) and is then increased iter-
atively proceeding towards pure best-ﬁrst search algorithms
like A*. The solution quality is expected to improve for it-
erations with a larger window size. While in AWA*, we use
the basic anytime principles of depth guided pruning and it-
erative relaxation, the algorithm retains the admissibility of
the heuristic estimate while it continues to search in best-ﬁrst
mode within the given window. It may be noted that AWA*
does not need explicit decomposition of the evaluation func-
tion (f (n)) into g and h components. We analyze the char-

IJCAI-07

2250

acteristics of the presented algorithm using a uniform search
tree model. This reveals some very interesting properties of
the algorithm in terms of heuristic accuracy versus solution
quality and related search complexity.

We also present a bounded optimal version of the AWA*
algorithm, called Bounded Quality Anytime Window A*
(BQAWA*), which is guaranteed to produce solutions within
a pre-speciﬁed sub-optimality bound, at each iteration.
In
BQAWA*, the search backtracks and dynamically adjusts the
window size within an iteration, such that the bound restric-
tion is adhered to.

We demonstrate the efﬁcacy of the presented strategies on
two optimization problems, namely, the traveling salesman
problem (TSP) and the 0/1 knapsack problem. Compara-
tive results with the existing anytime techniques (like DFBB,
ARA*, Beam-stack search) show considerable improvement
in performance.

2 Anytime Window A* Algorithm

The algorithm A* considers each node to be equivalent in
terms of information content and performs a global compe-
tition among all the partially explored paths to select a new
node. In practice, the heuristic errors are usually distance de-
pendent [Pearl, 1984]. Therefore, the nodes lying in the same
locality are expected to have comparable errors, where as the
error may vary substantially for nodes which are distant to
each other. Thus, if the global competition performed by A*
is localized within some boundaries, tighter pruning can be
obtained. Also, as better nodes at a level are generally very
competitive even within small localities, we can expect good
quality solutions with localized expansions. This observation
forms the basis of AWA*, where we localize the node expan-
sions using a sliding window. The AWA* algorithm works in
two phases, in the inner loop it uses the Window A* (Algo. 1)
routine with a ﬁxed window size and computes the solution
under the current restriction. In the outer loop (Algo. 2) the
window restrictions are gradually relaxed to obtain iteratively
improving solutions.

The Window A* routine works with a pre-speciﬁed win-
dow size. This window size is used to restrict the active queue
of the A* algorithm. For a given window size ω, the ex-
pansions are localized in the following way. When the ﬁrst
node of any level (say l) is expanded, all nodes in the open
list which are from level (l − ω) or less will be suspended
for this iteration. The window slides in a depth-ﬁrst man-
ner when deeper nodes are explored. Window A* terminates
when the ﬁrst goal node is expanded (A* like termination)
or if the open list does not contain any node having estimated
cost less than the cost of the best solution obtained in previous
iterations.

The AWA* algorithm calls the Window A* routine multi-
ple times with gradual relaxation of the window bounds. At
the start, window size is set to 0 (depth-ﬁrst mode). Once
the Window A* routine terminates, AWA* checks whether
there are any nodes suspended in the previous iteration. If the
suspended list is empty, the algorithm is terminated return-
ing the optimal solution. Otherwise, the open list is emptied
and the suspended list is moved to the open list. The window

Return BestSol;

Return BestSol;

Remove n from ClosedList; Insert n into SuspendList;
goto line 2

Algorithm 1 Procedure Window A*
1: CurLevel ← −1;
2: if OpenList is empty then
3:
4: end if
5: Select node n which has the least f (n)-value node from OpenList;
6: Insert n into ClosedList;
7: if f (n) ≥ BestSol then
8:
9: else if Level(n) ≤ CurLevel − W indowSize then
10:
11:
12: else if Level(n) > CurLevel then
13: CurLevel ← Level(n)
14: end if
15: if IsGoal(n) then
16: BestSol ← f (n); Goal ← n;
17:
18: end if
19: for Each successor node n(cid:2) of n do
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
end if
34: end for
35: goto line 2

P arent(n(cid:2)) ← n; Level(n(cid:2)) ← Level(n) + 1;
Insert n(cid:2) to OpenList;

P arent(n(cid:2)) ← n; Update f (n(cid:2)), Level(n(cid:2));
Insert n(cid:2) to OpenList;

Calculate f (n(cid:2));
if n(cid:2) is not in OpenList or ClosedList or SuspendList then

else if n(cid:2) is in OpenList or SuspendList then

if f (n(cid:2)) < previously calculated path estimation then

P arent(n(cid:2)) ← n; Update f (n(cid:2)), Level(n(cid:2));

else if n(cid:2) is in ClosedList then

if f (n(cid:2)) < previously calculated path estimation then

end if

end if

Return BestSol;

size is increased in a pre-decided manner (we have used grad-
ual increment by 1), and Window A* is called again. If the
algorithm is terminated through external interrupts, the best
solution obtained thus far is returned.

Algorithm 2 Anytime Window A* (AWA*)
1: INPUT :: A search tree/graph and a start node s.
2: ClosedList ← φ; SuspendList ← φ; W indowSize ← 0; Calculate

f (s); Level(s) ← 0; Insert s to OpenList; BestSol ← ∞;

3: BestSol ← Window A*(); {Call Window A*}
4: if SuspendList is empty then
5:
6: else
7:

Terminate with BestSol as the Optimal Solution;

Add OpenList to ClosedList; Move SuspendList to OpenList;
W indowSize ← W indowSize + 1;
goto line 3

8:
9: end if

For an anytime algorithm, there are two desirable proper-
ties, namely, interruptibility and progress. The ﬁrst property
ensures the anytime nature, i.e., the algorithm should pro-
vide a solution at pre-mature terminations. The second one
states that the solution quality should improve with time, and
if enough time is allocated the algorithm should eventually
return the optimal solution. For ﬁnite search spaces, AWA*
adheres to both the properties mentioned. It starts in a depth-
ﬁrst mode (with no back-tracking) trying to obtain fast (possi-
bly sub-optimal) solutions, and then iteratively increases the
window size to produce better quality solutions. Eventually,
when the window size becomes such, that all the ’optimal-
path’ nodes lie within the active window, the optimal solution
is returned.

While iterative algorithms provide opportunities to attain

IJCAI-07

2251

different trade-offs through anytime terminations, the search
efforts can increase substantially if re-expansions are not con-
trolled. For AWA* with a consistent heuristic, a node is ex-
panded at most once within the Window A* routine, and it
can only be re-expanded in a later iteration if its g-value is
lowered. Thus AWA* requires minimal re-expansion.

It may be noted, that in the generalized case AWA* does
not guarantee a solution at each iteration (Window A* loop).
If this property is a requirement; an easy way to attain this
is through back-tracking (i.e., by moving the suspended list
to open list at line 2 of Algo. 1), when no solution is found.
However, we propose a better method for this in BQAWA*
(Sec. 4), which not only provides a solution but guarantees a
bounded quality solution, at each iteration.

3 Analyzing AWA* using a Search Tree Model

In this section, we analyze the quality-time response of Win-
dow A* algorithm in terms of a uniform cost tree model (as
suggested in [Pearl, 1984]). We model the search space as a
uniform m-ary tree T , with a unique start state s. The leaf
nodes are grouped into two parts. There is a unique optimal
goal node g, situated at a distance N from s, and all other
nodes at level N are sub-optimal terminal nodes. Window
A* terminates when either the optimal goal node or a sub-
optimal terminal node is reached. The optimal solution path
is given as (s, ng,1..ng,i..ng,N −1, g) where ng,i denotes the
solution path node at level i. The trees T1...Ti...TN are ’off-
course’ sub-trees of T , one level removed from the solution
path. An ’off-course’ node is labeled as ni,j , where j denotes
the level of that node and i denotes the root level of the ’off-
course’ sub-tree (Ti) to which the node belongs. Using this

1

T

1

S

1

i

T

i

i1

i

N1

G

T
N

Figure 1: Uniform binary tree (model)

model (Fig. 1), we try to quantify the expected node expan-
sions and the probability of reaching the optimal goal node,
for a speciﬁc window restriction. First we present few termi-
nologies,
Deﬁnition 1. qi,j,ω : qi,j,ω denotes the expansion probability
of an ’off-course’ node ni,j when it is in the open list and the
window size is ω,

qi,j,ω = P (f (ni,j) ≤ M in(Γ)) , Γ = min(l + ω, N )
and, M in(l) = min(f (n)), ∀n such that n ∈ open list
and level(n) = l.

(1)
Deﬁnition 2. rg,j,ω : rg,j,ω denotes the expansion probability
of a ’on-path’ node ng,j when it is already in the open list and

the window size is ω,

rg,j,ω = P (f (ng,j) ≤ M in(Γ)), Γ = min(l + ω, N )

(2)

Next, we use the search tree model to obtain the ex-
pressions for the expected node expansions and convergence
probability for a given ω. The expected node expansions and
convergence probabilities (for a given window size) are pre-
sented in the following lemmas,
Lemma 1. Expected number of node expansions EN (ω)
(window size = ω) for a m-ary uniform cost search tree T
(with depth N ) is given as,

(cid:2)

l=N
l=0 Pg(g, l, ω) + (m − 1)Σi=l

i=1ml−i ∗ Pn(i, l, ω)

EN (ω) =
Where, Pg(g, l, ω) = Πj=l
Pn(i, l, ω) = Πj=l

j=0rg,j,ω, and,
rg,k,ω

j=iqi,j,ω ∗ Πk=i−1

k=0

(3)

Lemma 2. Expected probability of reaching the optimal goal
node Ps(ω) (window size = ω) for a m-ary uniform cost
search tree T (with depth N ) is given as,

Ps(ω) = Πj=N

j=0 rg,j,ω

(4)

We try to estimate the expansion probability values
(qi,j,ω, rg,j,ω) from the heuristic information available. We
assume that the relative estimation errors (Eqn. 5), are inde-
pendent random variables Y (n) within (0, e) bound (0 ≤ e ≤
1) with an arbitrary distribution function.

Y (n) = [h∗(n) − h(n)]/h∗(n)

(5)

Now, for a node ni,j at depth j and belonging to ’off-course’
sub-tree Ti (Figure. 1), we have

g(ni,j) = j,
h∗(ni,j ) = N + j + 2(1 − i)

Similarly, for an ’on-path’ node ng,j , we have

g(ng,j) = j,
h∗(ng,j ) = N − j

(6)

(7)

Thus, with the chosen error model (Eqn. 5),

f (ni,j) = N + 2(j − i + 1) − Y (n)(N + j + 2(1 − i))
f (ng,j) = N − Y (g)(N − j)

(8)
Where Y (n), Y (g) are identically distributed random vari-
ables within (0, e) bounds. Combining Eqn. 8 with the earlier
deﬁnitions, we obtain

qi,j,ω = 1 − P (Y (n) ≤ (j + α − M in(Γ))/α)
where, α = N + j + 2(1 − i) and, Γ = min(l + ω, N )

(9)

and,

rg,j,ω = 1 − P (Y (n) ≤ (N − M in(Γ))/(N − j))

(10)
From Eqns. 1 and 2, we observe that apart from the f (n)
value, the node expansion probability also depends on the
M in(l + ω) value. Now, the M in(l) value at a given level is
bounded by the minimum and maximum possible f -values
for that level. Also, for a consistent heuristic the M in(l)

IJCAI-07

2252

function should be non-decreasing with l. Thus, we obtain
the following properties for M in(l).

N − e(N − l) ≤ M in(l) ≤ 2l + N
M in(l) ≥ M in(l − 1)

(11)

With these observations, we represent the M in(l) function as
follows,

 

e
c
n
e
g
r
e
v
n
o
c
 
f

o

 
y
t
i
l
i

b
a
b
o
r
p

 

d
e

t
c
e
p
x
E

 

 1

 0.8

 0.6

 0.4

 0.2

1. e = 0.5, c = 0.1
2. e = 0.5, c = 0.5
3. e = 1.0, c = 0.1
4. e = 1.0, c = 0.5

(1)

 0

 0

 2

(2)

(4)

(3)

 6
 4
 Window Size 

 8

 10

M in(l) = max(M in(l − 1),
N (1 − e(1 − s(l)) + l(e + (2 − e)s(l))))
where, s(l) is a function of l, 0 ≤ s(l) ≤ 1.

(12)

Figure 3: Convergence probability versus window size, for a
uniform binary tree of height 10 for AWA*

(13)

(14)

The M in(l) function denotes the minimum value of a set of
numbers. From statistics, we know that with the increase of
the set cardinality the probability of the minimum converging
to the lower limit increases exponentially. The implication
suggests that with increase in l, the probability of M in(l)
converging to the lower limit increases exponentially. Thus,
s(l) can be approximated using a decreasing function with l,
and most probably the decrease will be exponential.

Using the above presented results, we can estimate the ex-
pected node expansion and the optimal convergence proba-
bilities for a chosen error distribution and s(l) function. We
assume a simple linear distribution for Y (n) and approximate
s(l) as an exponential function of l, i.e., we assume,

(cid:3)

P (Y (n) ≤ x) =

0 if (x < 0)
x/e if (0 ≤ x ≤ e)
1 otherwise

and,

s(l) = cl, 0 ≤ c ≤ 1

Substituting the variables in Eqns. 3 and 4, we obtain the

i

 
s
n
o
s
n
a
p
x
e
 
e
d
o
n
 
d
e
t
c
e
p
x
E

 

 35

 30

 25

 20

 15

 10

1.  e = 0.5, c = 0.1
2.  e = 0.5, c = 0.5
3.  e = 1.0, c = 0.1
4.  e = 1.0, c = 0.5

(4)

(3)

(2)

 0

 2

(1)

 4
 6
 Window Size 

 8

 10

Figure 2: Expected node expansions versus window size, for
a uniform binary tree of height 10 for AWA*

expected values for node expansions as well as convergence
probabilities for different window constraints. In Figures 2
and 3, we present the curves for expected node expansions
and convergence probabilities with different window size, for
a uniform binary tree of height 10. The expected values are
computed using two error boundaries (e = 0.5 and e = 1.0),
and two chosen constants (c = 0.1 and c = 0.5). From the
curves, we observe that the node expansions and convergence
probabilities obtained are almost independent of the choice
of c. This can be explained by the exponential nature of
chosen s(l) function, which rapidly converges to the lower
limit. Investigating the trends with different heuristic errors,
we observe that the convergence probability versus window

 300

 250

 200

 150

 100

 50

i

 
s
n
o
s
n
a
p
x
e

 

 

e
d
o
n
e
g
a
r
e
v
A

 

1. Max density heuristic
2. Fitting heuristic

(1)

(2)

 0

 2

 4
 6
 Window Size 

 8

 10

Figure 4: Average node expansions versus window size, 0/1
Knapsack problem

size curves (Fig. 3) do not show much variability with dif-
ferent errors. While the convergence probability for a given
window size increases with heuristic accuracy, the improve-
ment is marginal. However, the expected node expansions
for a given window increases substantially with more error
(Fig. 2). This observation can be explained by the fact that
with more error, both the M in(l) and the f (n) values are
expected to decrease, causing relatively small changes in the
qi,j,ω and rg,j,ω probabilities. The difference is even less pro-
nounced for rg,j,ω as the f (n) values of the ’optimal-path’
nodes are expected to remain close to the M in(l) values. On
the other hand, the expected node expansions increase expo-
nentially with the increase in qi,j,ω values (Eqn. 3). Thus,
with increased error more ’off-course’ nodes are expanded,
causing appreciable variance in the node expansions.

We conclude this section by presenting the experimental
results for average node expansions and convergence prob-
abilities with changing window size, for the 0/1 Knapsack
problem using two different heuristics h1 (max. density) and
h2 (ﬁtting based), such that h2 dominates h1 (Fig. 4 and 5).

e
c
n
e
g
r
e
v
n
o
c
 
f

o
 
y
t
i
l
i

 

b
a
b
o
r
p
e
a
g
r
e
v
A

 1

1. Max density heuristic
2. Fitting heuristic

 0.8

 0.6

 0.4

 0.2

(1)

(2)

 0

 0

 2

 4
 6
 Window Size 

 8

 10

Figure 5: Average optimal convergence versus window size,
0/1 Knapsack problem

IJCAI-07

2253

The ﬁgures depict that the average node expansions and the
convergence probabilities with different heuristics show iden-
tical trends to that obtained from the analytical model.

4 BQAWA* - Anytime Window A* with

Provable Quality Bounds

In AWA*, we have an anytime algorithm which attempts to
produce a stream of gradually improving solutions. However,
no bound on the obtained solution is established. In this sec-
tion, we present a modiﬁed version of the AWA*, termed as
Bounded Quality Anytime Window A* (BQAWA*), where
each solution produced will be bounded within a pre-
speciﬁed factor () of the optimal solution, the bound is itera-
tively tightened to produce an iteratively improving stream of
solutions.

goto line 2

goto line21

Algorithm 3 Procedure Bounded Quality Window A*
1: CurLevel ← −1; M inSus ← ∞;
2: if OpenList is empty then
3:
4: end if
5: Select node n which has the least f (n)-value node from OpenList;
6: Insert n into ClosedList;
7: if f (n) ≥ BestSol or f (n) ≥ M inSus ∗  then
goto line 18 {Update window size and back-track}
8:
9: else if Level(n) ≤ CurLevel − W indowSize then
10: Move n from ClosedList to SuspendList; Update M inSus;
11:
12: else if Level(n) > CurLevel then
13: CurLevel ← Level(n)
14: end if
15: GOAL EVALUATION {Same as Window A*}
16: EXPANSION {Same as Window A*.}
17: goto line 2
18: if f (n) < BestSol then
19: Move n from ClosedList to OpenList;
20: end if
21: Merge SuspendList with OpenList; Increment W indowSize;
22: if OpenList is empty then
23:
24: end if
25: goto line 1

Return BestSol;

To achieve the pre-speciﬁed bounds at each iteration, we
modify the Window A* routine to obtain Bounded Quality
Window A* or BQWA* (Algo. 3).
In BQWA* routine, at
any intermediate point, the minimum f -value among the sus-
pended nodes (min(fsus)) is noted.
If the f -value of the
top node in the open list is greater than  ∗ min(fsus) (for
minimization problems), the open list and the suspended list
are merged, and the search back-tracks after incrementing the
window size. The ﬁrst solution produced by the algorithm is
published with  bound guarantee. The BQAWA* algorithm

Algorithm 4 Bounded Quality Anytime Window A*
1: INPUT :: A search tree/graph, a start node s and an initial bound .
2: ClosedList ← φ; SuspendList ← φ; W indowSize ← 0; Calculate

f (s); Level(s) ← 0; Insert s to OpenList; BestSol ← ∞;

Terminate with BestSol as the Optimal Solution;

3: BestSol ← Bounded Quality Window A*();
4: if SuspendList is empty then
5:
6: else
7:
8:
9:
10: end if

Add OpenList to ClosedList; Move SuspendList to OpenList;
 ←  − Δ;
goto line 3

(Algo. 4) works with the same principle of ARA* technique,
i.e., the algorithm starts with a high initial bound () and iter-
atively reduces the bound to get improved solutions. It uses
the BQWA* routine (Algo. 3) to obtain intermediate solutions
within the chosen sub-optimality bound. After each iteration,
the suspended list is moved to the open list, weight bound
is reduced and BQWA* is re-run to get improved solutions.
The algorithm terminates if the suspended list is empty after
an iteration.

5 Experimental Results
In this section, we present empirical results comparing the
performance of AWA* with some existing anytime tech-
niques (DFBB, ARA* and Beam-stack search). We per-
formed experiments with two optimization problems, namely
0/1 Knapsack and Euclidean Traveling Salesman (TSP) prob-
lem. Since time spent is usually proportional with the number
of nodes expanded, we present the results in terms of node ex-
pansions.

 226

 224

 222

 220

 218

 216

 
t
s
o
C
 
e
g
a
r
e
v
A

(4)

(3)

(2)

(1)

1. DFBB
2. ARA*
3. Beam-stack Search
4. AWA*

 0

 500  1000  1500  2000  2500  3000  3500  4000

 n (n = nodes expanded x 50)

Figure 6: Average cost versus node expansions - 0/1 Knap-
sack problem

For 0/1 Knapsack, we estimated the heuristic by ﬁtting the
remaining objects in decreasing order of their cost-densities.
We performed experiments on a set of 100 random instances
of the 50-object 0/1 Knapsack problem. Weight and costs of
individual objects were generated randomly, while the con-
straint was chosen within 0.4 − 0.6 of the sum of weights. In
Figure 6, we present the cost versus nodes expansion results
obtained for DFBB, ARA*, Beam-stack search and AWA*.
Figure 7 includes the percentage optimal convergence versus
node expansion curves. For ARA*, the weight limits were
initialized to 2.0, and decreased by 0.1 per iteration.

For Euclidean TSP, we used the minimum spanning tree
(MST) heuristic. We performed the experiments on a ran-
domly generated set of 100 25-city problem instances. The
intermediate tour length and percentage optimality results
comparing AWA* with DFBB, ARA* and Beam-stack search
are included in Figures 8 and 9, respectively.

From the results obtained for both 0/1 Knapsack and TSP,
we observe that the performance of AWA*, is superior to all,
DFBB, ARA* and Beam-stack search, both in terms of in-
termediate solution qualities as well as percentage optimal
convergences.

In Table 1, we present the average node expansions for
ARA* and BQAWA* applied to 0/1 Knapsack and TSP, with
different sub-optimality bounds. The results shows for all the

IJCAI-07

2254

e
c
n
e
g
r
e
v
n
o
c
 
l

a
m

i
t

p
O
%

 

 100

 80

 60

 40

 20

 0

(4)

(3)

(2)

(1)

1. DFBB
2. ARA*
3. Beam-stack Search
3. AWA*

 0

 500  1000  1500  2000  2500  3000  3500  4000

n (n = nodes expanded x 50)

Figure 7: Percentage optimal convergence versus node ex-
pansions - 0/1 Knapsack problem

 

t

h
g
n
e

l
 
r
u
o

t
 

e
g
a
r
e
v
A

 40

 39.5

 39

 38.5

 38

1. DFBB
2. ARA*
3. Beam-stack Search
4. AWA*

(2)

(3)

(1)

(4)

 50  100  150  200  250  300  350  400  450  500

 n (n = nodes expanded x 100)

Figure 8: Average tour length versus node expansions - Eu-
clidean TSP

 100

(4)

e
c
n
e
g
r
e
v
n
o
c
 
l
a
m

i
t
p
O
%

 

 80

 60

 40

 20

 0

(2)

(3)

(1)

1. DFBB
2. ARA*
3. Beam-stack Search
4. AWA*

 0

 50  100  150  200  250  300  350  400  450  500

 n (n = nodes expanded  x 100)

Figure 9: Percentage optimal convergence versus node ex-
pansions - Euclidean TSP

bounds, average node expansions required by BQAWA* is
less than that required by ARA*, reiterating the superiority
of the window based restrictions.

From the results presented, we can conclude that the AWA*
algorithm provides considerably better quality-time trade-off
than DFBB, ARA* or Beam-stack search. Intermediate so-
lution qualities are better and faster optimal convergence is
achieved.

6 Conclusions

In this paper, we proposed an anytime algorithm AWA*,
which localizes the global competition performed by A* us-
ing a sliding window. Experiments performed with 0/1 Knap-
sack and TSP demonstrate the efﬁcacy of AWA* over some
existing anytime techniques. It may be noted, that AWA*, in
its current form, is not designed to work under memory re-
strictions. It would be interesting to explore how the standard

Table 1: Average number of node expansions with different
sub-optimality bounds

Bounds

Knapsack

TSP

ARA*

BQAWA*

ARA*

BQAWA*

2.0
1.9
1.8
1.7
1.6
1.5
1.4
1.3
1.2
1.1
1.0

62
73
85
98
110
127
152
178
222
332

60
65
68
72
77
87
99
104
133.
158

59717

58581

61
65
74
80
92
106
141
211
587
3650
27952

25
37
52
67
81
91
111
135
579
3585
27905

memory bounded techniques can be adopted in AWA*.

References
[Dean and Boddy, 1988] T. Dean and M. Boddy. An analysis of
time-dependent planning. In Proceedings of 6th National Confer-
ence on Artiﬁcial Intelligence (AAAI 88), pages 49–54, St. Paul,
MN, 1988. AAAI Press.

[Furcy, 2006] D. Furcy. Itsa*: Iterative tunneling search with a*.
In Proceedings of AAAI Workshop on Heuristic Search, Memory-
Based Heuristics and Their Applications, pages 21–26, 2006.

[Hansen et al., 1997] E. A. Hansen, S. Zilberstein, and V. A.
Danilchenko. Anytime heuristic search: First results. Techni-
cal Report 50, Univ. of Massachusetts, 1997.

[Kitamura et al., 1998] Y. Kitamura, M. Yokoo, T. Miyaji, and
S. Tatsumi. Multi-state commitment search. In Proceedings of
10th IEEE International Conference on Tools with Artiﬁcial In-
telligence, pages 431–439, 1998.

[Kumar et al., 1995] A. Kumar, A. Kumar, and M. Balakrishnan.
Heuristic search based approach to scheduling, allocation and
binding in data path synthesis.
In Proceedings of 8th Interna-
tional Conference on VLSI Design, pages 75–80, 1995.

[Likhachev et al., 2004] M. Likhachev, G. J. Gordon, and S. Thrun.
Ara*: Anytime A* with provable bounds on sub-optimality. In
Advances in Neural Information Processing Systems 16. MIT
Press, Cambridge, MA, 2004.

[Pearl, 1984] J. Pearl. Heuristics: intelligent search strategies for
computer problem solving. Addison-Wesley Longman Publish-
ing Co., Inc., Boston, MA, USA, 1984.

[Pohl, 1970] I. Pohl. Heuristic search viewed as path ﬁnding in a

graph. Artif. Intell., 1(3):193–204, 1970.

[Sarkar et al., 1991] U. K. Sarkar, P. P. Chakrabarti, S. Ghose, and
S. C. De Sarkar. Multiple stack branch and bound. Inf. Process.
Lett., 37(1):43–48, 1991.

[Zhang, 1998] W. Zhang. Complete anytime beam search. In Pro-
ceedings of 14th National Conference of Artiﬁcial Intelligence
AAAI’98, pages 425–430. AAAI Press, 1998.

[Zhou and Hansen, 2002] R. Zhou and E. A. Hansen. Multiple se-
quence alignment using anytime a*. In Proceedings of 18th Na-
tional Conference on Artiﬁcial Intelligence AAAI’2002, pages
975–976, 2002.

[Zhou and Hansen, 2005] R. Zhou and E. A. Hansen. Beam-stack
search: Integrating backtracking with beam search. In Proceed-
ings of the 15th International Conference on Automated Planning
and Scheduling (ICAPS-05), pages 90–98, Monterey, CA, 2005.

IJCAI-07

2255


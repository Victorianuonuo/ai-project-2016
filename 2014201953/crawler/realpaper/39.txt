Cho-k-NN: A Method for Combining Interacting Pieces of Evidence

in Case-Based Learning

Otto-von-Guericke-Universit¤at Magdeburg, Fakult¤at f¤ur Informatik

Eyke H¤ullermeier

Universit¤atsplatz 2, 39106 Magdeburg, Germany

eyke.huellermeier@iti.cs.uni-magdeburg.de

Abstract

The case-based learning paradigm relies upon
memorizing cases in the form of successful prob-
lem solving experience, such as e.g. a pattern along
with its classi(cid:2)cation in pattern recognition or a
problem along with a solution in case-based rea-
soning. When it comes to solving a new problem,
each of these cases serves as an individual piece of
evidence that gives an indication of the solution to
that problem. In this paper, we elaborate on issues
concerning the proper combination (aggregation)
of such pieces of evidence. Particularly, we argue
that cases retrieved from a case library must not be
considered as independent information sources, as
implicitly done by most case-based learning meth-
ods. Focusing on the problem of prediction as a
performance task, we propose a new inference prin-
ciple that combines potentially interacting pieces
of evidence by means of the so-called (discrete)
Choquet-integral. Our method, called Cho-k-NN,
takes interdependencies between the stored cases
into account and can be seen as a generalization of
weighted nearest neighbor estimation.

1 Introduction
Case-based or instance-based learning algorithms have been
applied successfully in (cid:2)elds such as, e.g., machine learn-
ing and pattern recognition during the recent years [Aha et
al., 1991; Dasarathy, 1991]. The case-based learning (CBL)
paradigm is also of central importance in case-based reason-
ing (CBR), a problem solving methodology which goes be-
yond the standard prediction problems of classi(cid:2)cation and
regression [Riesbeck and Schank, 1989; Kolodner, 1993].

As the term suggests, in CBL special importance is at-
tached to the concept of a case. A case or an instance can
be thought of as a single experience, such as a pattern (along
with its classi(cid:2)cation) in pattern recognition or a problem
(along with a solution) in CBR.

Rather than inducing a global model (theory) from the
data and using this model for further reasoning, as inductive,
model-based machine learning methods typically do, CBL
systems simply store the data itself. The processing of the
data is deferred until a prediction (or some other type of

query) is actually requested, a property which quali(cid:2)es CBL
as a lazy learning method [Aha, 1997]. Predictions are then
derived by combining the information provided by the stored
cases, primarily by those which are similar to the new query.
In fact, the concept of similarity plays a central role in
CBL. The major assumption underlying CBL has already
been expressed, amongst others, by the philosopher DAVID
HUME ([Hume, 1999], page 116): (cid:147)In reality, all arguments
from experience are founded on the similarity ... among natu-
ral objects. ... From causes, which appear similar, we expect
similar effects.(cid:148) This commonsense principle, that we shall
occasionally call the (cid:147)similarity hypothesis(cid:148) [Rendell, 1986],
serves as a basic inference paradigm in various domains of
application. For example, in a classi(cid:2)cation context, it trans-
lates into the assertion that (cid:147)similar objects have similar class
labels(cid:148), and in CBR it suggests that (cid:147)similar problems have
similar solutions(cid:148).

The similarity hypothesis, which is apparently of a heuris-
tic nature, has been put into practice by means of different in-
ference schemes that combine similarity and frequency infor-
mation in one way or the other. For example, the well-known
nearest neighbor (NN) classi(cid:2)er (cid:2)rst selects a neighborhood
around the query, consisting of the k most similar cases, and
then counts the occurrence of the different class labels. De-
spite of their simplicity, inference methods of such kind have
proved to be quite successful in practical applications.

In this paper, we suggest a further improvement based
on the idea of taking interdependencies between neighbored
cases into account. In fact, in standard NN methods, these
cases are implicitly considered as independent information
sources. We argue that a corresponding assumption of in-
dependence is not always justi(cid:2)ed and propose a new infer-
ence principle that combines interacting pieces of evidence
in a more thorough way. This principle, which makes use
of non-additive measures for modeling interaction between
cases and employs the so-called (discrete) Choquet-integral
as an aggregation operator, can be seen as a generalization of
weighted NN estimation.

By way of background, section 2 gives a concise review
of the NN principle, which constitutes the core of the family
of CBL algorithms. In section 3, we discuss the problem of
interaction between cases in CBL. A new approach to CBL,
which takes such interactions into account, is then introduced
in section 4 and evaluated empirically in section 5.

2 Nearest Neighbor Estimation
The well-known nearest neighbor (NN) estimation principle
is applicable to both classi(cid:2)cation problems (prediction of
class labels) and regression (prediction of numeric values).

Consider a setting in which an instance space X is en-
dowed with a similarity measure sim : X (cid:2) X ! [0; 1]. An
instance corresponds to the description x of an object (usu-
ally in attribute(cid:150)value form). In the standard classi(cid:2)cation
framework, each instance x is assumed to have a (unique) la-
bel y 2 L. Here, L is a (cid:2)nite (typically small) set comprised
of m class labels f‘1 : : : ‘mg, and hx; yi 2 X (cid:2) L is a labeled
instance (case).

The NN principle originated in the (cid:2)eld of pattern recog-
nition [Dasarathy, 1991]. Given a sample S consisting of n
labeled instances hx{; y{i, 1 (cid:20) { (cid:20) n, and a novel instance
x0 2 X (a query), this principle prescribes to estimate the
label y0 of the yet unclassi(cid:2)ed query x0 by the label of the
nearest (most similar) sample instance. The k-nearest neigh-
bor (k-NN) approach is a slight generalization, which takes
the k (cid:21) 1 nearest neighbors of x0 into account. That is, an
of y0 is derived from the set Nk(x0) of the k
estimation yest
nearest neighbors of x0, usually by means of a majority vote.
Besides, further conceptual extensions of the (k-)NN princi-
ple have been devised, such as distance weighting [Dudani,
1976]:

0

yest
0 = arg max

‘2L Xhx;yi2Nk(x0)

!x (cid:1) I(y = ‘)

(1)

where !x is the weight of the instance x and I((cid:1)) the standard
ftrue; falseg ! f0; 1g mapping. (Throughout the paper, we
assume the weights to be given by !x = sim(x; x0).)

The NN principle can also be used for regression problems,
i.e., for realizing a (locally weighted) approximation of real-
valued target functions x 7! y = f (x) (in this case, L = R).
To this end, one reasonably computes the (weighted) mean of
the k nearest neighbors of a new query point:
0 = Phx;yi2Nk(x0) !x (cid:1) y
yest
Phx;yi2Nk(x0) !x

(2)

3 Interaction Between Cases in CBL
Some case-based approaches completely rely on the (suppos-
edly) most relevant piece of evidence, namely the observation
which is most similar to the query. In CBR, for example, it is
common practice to retrieve just a single case from the case
library, and to adapt the corresponding solution to the prob-
lem under consideration. On the one hand, ignoring all but
the most similar observation is conceptually simple and com-
putationally ef(cid:2)cient. On the other hand, this approach does
of course come along with a loss of information, because only
a very small part of the past experience is utilized.

If several instead of only a single case are retrieved, as e.g.
in k-NN, an important question arises: How should the pieces
of evidence coming from the different cases be combined? In
k-NN classi(cid:2)cation, the evidences in favor of a certain class
label are simply added up (see (1)). Likewise, in regression
the estimation is a simple linear combination of the observed

x2

x0

x1

x2

x0

x1

x3

x3

Figure 1: Different con(cid:2)gurations of locations in two-
dimensional space.

outcomes (see (2)). Thus, the neighbored cases are basically
considered as independent information sources.

This assumption of independence between case-based ev-
idence can thoroughly be called into question [H¤ullermeier,
2002]. Indeed, it is not even in agreement with the similarity
hypothesis itself! Namely, if this hypothesis is true, then two
neighbored cases that are not only similar to the query case
but also similar among each other will probably provide sim-
ilar information regarding the query. In other words, when
taking the similarity hypothesis for granted, the information
coming from the neighbored cases is at least not independent.
In particular, from a problem solving perspective, one
should realize that a set of cases can be complementary in
the sense that the experiences represented by the individual
cases complement or reinforce each other. On the other hand,
cases can also be redundant in the sense that much of the in-
formation is already represented by a smaller subset among
them. And indeed, as we said before, the similarity hypothe-
sis suggests that similar cases are likely to be redundant.

To illustrate this point by a simple example, consider the
problem of predicting student Peter’s grade in computer sci-
ence, knowing that he has an A in French. The latter infor-
mation (i.e. the case hFrench; Ai) clearly suggests that Peter
is an excellent student and, hence, supports predicting an A
or maybe a B. Yet, one cannot be sure that this prediction is
correct. In this situation, the additional information that Pe-
ter has a B in mathematics is probably more valuable than
the information that he has an A in Spanish as well. In fact,
the cases hFrench; Ai and hSpanish; Ai are partly redundant,
since the two subjects are quite similar by themselves. As op-
posed to this, the case hmathematics; Bi is complementary in
a sense, as it suggests that Peter is not only good in languages.
Now, suppose that you know all three grades (mathematics,
Spanish, French). Is A or B the more likely grade? Of course,
mathematics is more similar to computer science than Span-
ish and French. However, depending on the concrete spec-
i(cid:2)cation of similarity degrees between the various subjects,
it is quite possible that the weighted k-NN rule favors grade
A since the two moderately similar A’s compensate for the
more similar B. Of course, this result might be judged criti-
cally, and one might wonder whether the grade A should re-
ally count twice. In fact, one reasonable alternative is to con-
sider the two A’s as only a single piece of evidence, telling
something about Peter’s achievements in languages, instead
of two pieces of distinct information. In any case, the close
connection between the two A’s should not be ignored when
combining the three observations.

As a second example, consider the problem of predicting
the yearly rainfall at a certain location (city). For instance,

given the rainfall y{ at location x{ ({ = 1; 2; 3), what about the
rainfall at location x0 in the two scenarios shown in Fig. 1?
The important point to notice is that even though the indi-
vidual distances between x0 and the x{ are the same in both
scenarios, the y{ should not be combined in the same way. In
this example, this is due to the different arrangements of the
neighbors [Zhang et al., 1997]: Simply predicting the arith-
metic mean (y1 + y2 + y3)=3 seems to be reasonable in the
left scenario, while the same prediction appears questionable
in the scenario shown in the right picture. In fact, since x1 and
x2 are closely neighbored in the latter case, information about
the rainfall at these locations will be partly redundant. Con-
sequently, the weight of the (joint) evidence that comes from
the observations hx1; y1i and hx2; y2i should not be twice as
high as the weight of the evidence that comes from hx3; y3i.

The above examples show the need for taking interdepen-
dencies between observed cases into account and, hence, pro-
vide a motivation for the method that will be proposed in the
following section. Before proceeding, let us make two further
remarks: First, the above type of interaction between cases
seems to be less important if the sample size is large and even
becomes negligible in asymptotic analyses of NN principles.
In fact, strong results on the performance of NN estimation
can be derived [Dasarathy, 1991], but these are valid only
under idealized statistical assumptions and arbitrarily large
sample sizes. Roughly speaking, if the sample size n tends to
in(cid:2)nity, the distance between the query and its nearest neigh-
bors becomes arbitrarily small (with high probability). This
holds true even if the size k of the neighborhood is increased
too, as a function k(n) of n, provided that k(n)=n ! 0 for
n ! 1. Moreover, if the individual observations are inde-
pendent and identically distributed in a statistical sense, the
neighborhood becomes (cid:147)well distributed(cid:148). Under these as-
sumptions, it is intuitively clear that interdependencies be-
tween observations will hardly play any role. On the other
hand, it is also clear that statistical assumptions of such kind
will almost never be satis(cid:2)ed in practice.

The second remark concerns related work. In fact, there
are a few methods that (cid:2)t into the CBL framework and that
allow for taking certain types of interaction between observa-
tions into account. Particularly, these are methods that make
assumptions on the statistical correlation between observa-
tions, depending on their distance [Lindenbaum et al., 1999].
For example, in our rainfall example one could employ a
method called kriging, which is well-known in geostatistics
[Oliver and Webster, 1990]. Usually, however, such methods
are specialized on a particular type of application and, more-
over, make rather restrictive assumptions on the mathematical
(metric) structure of the instance space. Our approach, to be
detailed in the next section, is much more general in the sense
that it only requires a similarity measure sim((cid:1)) to be given.
We do not make any particular assumptions on this measure
(such as symmetry or any kind of transitivity), apart from the
fact that it should be normalized to the range [0; 1]. From an
application point of view, this seems to be an important point.
In CBR, for example, cases are typically complex objects that
cannot easily be embedded into a metric space.

4 The Cho-k-NN Method
4.1 Non-Additive Measures
Let X be a (cid:2)nite set and (cid:23)((cid:1)) a measure 2X ! [0; 1]. For any
A (cid:18) X, we interpret (cid:23)(A) as the weight or, say, the degree of
relevance of the set of elements A.

A standard assumption on the measure (cid:23)((cid:1)), which is made
e.g. in probability theory, is additivity: (cid:23)(A [ B) = (cid:23)(A) +
(cid:23)(B) for all A; B (cid:18) X s.t. A \ B = ;. Unfortunately, addi-
tive measures cannot model any kind of interaction between
elements: Extending a set of elements A by a set of elements
B always increases the weight (cid:23)(A) by the weight (cid:23)(B), re-
gardless of A and B.

Suppose, for example, that the elements of two sets A and
B are complementary in a certain sense. To illustrate, one
might think of X as a set of workers in a factory, and of a
weight (cid:23)(A) as the productivity of a team of workers A. In
that case, complementarity means that the output produced
by two teams A and B is higher if they cooperate. Formally,
this can be expressed as a positive interaction: (cid:23)(A [ B) >
(cid:23)(A) + (cid:23)(B). Likewise, elements can interact in a negative
way. For example, if two sets A and B are partly redundant
or competitive, then (cid:23)(A [ B) < (cid:23)(A) + (cid:23)(B).

The above considerations motivate the use of non-additive
measures, also called fuzzy measures, which are simply nor-
malized and monotone [Sugeno, 1974]:

(cid:15) (cid:23)(;) = 0, (cid:23)(X) = 1
(cid:15) (cid:23)(A) (cid:20) (cid:23)(B) for A (cid:18) B
In order to quantify the interaction between subsets of a set
X, as induced by a fuzzy measure (cid:23)((cid:1)), several indices have
been proposed in the literature. For two individual elements
x{; x| 2 X, the interaction index is de(cid:2)ned by

I(cid:23) (x{; x|) df= XA(cid:18)Xnfx{;x|g

(jXj (cid:0) jAj (cid:0) 1)! jAj!

(jXj (cid:0) 1)!

((cid:1){|(cid:23))(A);

where jAj denotes the cardinality of A and

((cid:1){|(cid:23))(A) df= (cid:23)(A [ fx{; x|g) (cid:0) (cid:23)(A [ fx{g) (cid:0)

(cid:0) (cid:23)(A [ fx|g) + (cid:23)(A):

The latter can be seen as the marginal interaction between
x{ and x| in the context A [Murofushi and Soneda, 1993].
The above index can be extended from pairs x{; x| to any set
U (cid:18) X of elements [Grabisch, 1997]:

I(cid:23) (U ) df= XA(cid:18)XnU

(jXj (cid:0) jAj (cid:0) jU j)! jAj!

(jXjjU j + 1)!

((cid:1)U (cid:23))(A);

where

((cid:1)U (cid:23))(A) df= XV (cid:18)U

((cid:0)1)jU j(cid:0)jV j(cid:23)(V [ A):

4.2 Modeling Interaction in Case-Based Learning
Now, recall our actual problem of combining evidence in
case-based learning. In this context, the set X of elements
corresponds to the neighbors of the query case x0:

X = Nk(x0) = fx1; x2 : : : xkg

(3)

Our comments so far have shown that fuzzy measures can
principally be used for modeling interaction between cases.
The basic question that we have to address in this connection
is the following: What is the evidence weight or simply the
weight, (cid:23)(A), of a subset A of the neighborhood (3)?

First, the boundary conditions (cid:23)(;) = 0 and (cid:23)(X) = 1
should of course be satis(cid:2)ed, expressing that the full evidence
is provided by the complete neighborhood X. Moreover, ac-
cording to our comments on the similarity hypothesis (sec-
tion 3), the evidence coming from a set of cases A (cid:18) X
should be discounted if these cases are similar among them-
selves. Likewise, the weight of A should be increased if
the cases are (cid:147)diverse(cid:148) (hence complementary) in a certain
sense.1 To express this idea in a more rigorous way, we de-
(cid:2)ne the diversity of a set of cases A by the average pairwise
dissimilarity:

div(A) df=

2

jAj2 (cid:0) jAj Xx{;x|2A

1 (cid:0) sim(x{; x|)

(By de(cid:2)nition, the diversity is 0 for singletons and the empty
set.) We furthermore de(cid:2)ne the relative diversity by

rdiv(A) df=

2 div(A)

1 (cid:0) min{;| sim(x{; x|)

(cid:0) 1 2 [(cid:0)1; 1]

Now, the idea is to modify the basic (additive) measure

(cid:22)(A) df= c(cid:0)1 Xx{2A

sim(x0; x{);

(4)

where c = Px{2X sim(x0; x{), by taking the diversity of A
into account. Of course, this can be done in different ways.
Here, we used the following approach:

(cid:22)(cid:23)(A) df= (cid:22)(A) (cid:1) (1 + (cid:11) rdiv(A))

(5)
As can be seen, the original measure (cid:22)(A) of a set of cases
A is increased if the diversity of A is relatively high, other-
wise it is decreased. The parameter (cid:11) (cid:21) 0 controls the extent
to which interactions between cases are taken into considera-
tion: (cid:22)(A) can be modi(cid:2)ed by at most (100 (cid:11))%. For (cid:11) = 0,
interactions are completely ignored and the original measure
(cid:22)((cid:1)) is recovered.

For the measure (cid:22)(cid:23)((cid:1)) as de(cid:2)ned in (5), the monotonicity
condition does not necessarily hold. To remedy this problem,
we simply enforce this property by setting

(cid:23)(A) df= max
B(cid:18)A

(cid:22)(cid:23)(B):

(6)

Finally, the boundary conditions are guaranteed by dividing
the measure thus obtained by (cid:23)(X).

To illustrate, consider again the rainfall example in the
right picture of Fig. 1 and suppose that sim(x{; x0) = :5 ({ =
1; 2; 3), sim(x1; x2) = :9, sim(x1; x3) = sim(x2; x3) = 0,
With (cid:11) = 1=2 in (5), we obtain the following weights:
x2; x3

x1; x3

x1; x2

A

(cid:23)(A)
1The idea of (cid:147)diversity(cid:148) of a set of cases plays also a role in
case-based retrieval [McSherry, 2002]. Here, however, the problem
is more to (cid:2)nd diverse cases, rather than to combine them.

:33

:83

:83

x3
:27

x1
:27

x2
:27

As can be seen, the joint weight of fx1; x2g is relatively

low, re(cid:3)ecting the partial redundance of the two cases.

Before going on, let us comment on the derivation of the
measure (cid:23)((cid:1)) from the similarity function sim((cid:1)). Firstly, even
though the measure (6) captures our intuitive idea of decreas-
ing (increasing) the evidence weight of cases that are (dis-
)similar by themselves, we admit that it remains ad hoc to
some extent, and by no means we exclude the existence of
better alternatives. For example, an interesting idea is to de-
rive the measure in an indirect way: First, the interaction in-
dices I(cid:23) ((cid:1)) from section 4.1 are de(cid:2)ned, again based on the
similarity between cases. These indices can be seen as con-
straints on the measure (cid:23)((cid:1)), and the idea is to (cid:2)nd a measure
that is maximally consistent with these constraints.

Secondly, even though the assumption that similar cases
provide redundant information is supported by the similarity
hypothesis, one might of course argue that the similarity be-
tween the predictive parts of two cases, x{ and x|, is not suf-
(cid:2)cient to call them redundant. Rather, the associated output
values y{ and y| should be similar as well. Indeed, if y1 differs
drastically from y2, the (cid:2)rst two measurements in our rainfall
example might better be considered as non-redundant. (In
that case, the two measurements in conjunction suggest that
there is something amiss ...) This conception of redundancy
can easily be represented by deriving (cid:23)((cid:1)) from an extended
similarity measure sim0((cid:1)) which is de(cid:2)ned over X (cid:2) L.2

Anyway, the important point of this section is not so much
the speci(cid:2)cation of a particular evidence measure, but rather
the insight that non-additive measures can in principle be
used for modeling the interaction between cases in CBL.

4.3 Aggregation of Interacting Pieces of Evidence
So far, we have a tool for modeling the interaction between
different pieces of evidence in case-based learning. The next
question that we have to address is how to combine these
pieces of evidence, i.e., how to aggregate them in agreement
with the evidence measure (cid:23)((cid:1)).

For the time being we focus on the problem of regression.
Recall that in the standard approach to NN estimation, an
aggregation of the output values f (x{) = y{ is realized by
means of a simple weighted average:

yest

0 = Xx{2X

(cid:22)(fx{g) (cid:1) f (x{);

(7)

where (cid:22)(fx{g) = sim(x0; x{)(cid:0)Px{2X sim(x0; x{)(cid:1)(cid:0)1. In-

terestingly, (7) is nothing else than the standard Lebesgue in-
tegral of the function f : X ! R with respect to the additive
measure (4):

yest

0 = ZX

f d(cid:22)

In order to generalize this estimation, an integral with respect
to the non-additive measure (cid:23)((cid:1)) is needed: the Choquet in-
tegral, a concept that originated in capacity theory [Choquet,
1954].

2We didn’t explore this alternative in detail so far.

Let (cid:23)((cid:1)) be a fuzzy measure and f ((cid:1)) a non-negative func-
tion.3 The Choquet integral of f ((cid:1)) with respect to (cid:23)((cid:1)) is then
de(cid:2)ned by

Z ch

f d(cid:23) df= Z 1

0

(cid:17)([f > t]) dt

where [f > t] = fx j f (x) > tg. The integral on the right-
hand side is the standard Lebesgue integral (with respect to
the Borel measure on [0; 1)). In our case, where X is a (cid:2)nite
set, we can refer to the discrete Choquet integral which can
be expressed in a rather simple form:

yest
0 =

k

X{=1

f (x(cid:25)({)) (cid:1)(cid:0)(cid:23)(A{) (cid:0) (cid:23)(A{(cid:0)1)(cid:1) ;

(8)

where (cid:25)((cid:1)) is a permutation of f1 : : : kg such that 0 (cid:20)
f (x(cid:25)(1)) (cid:20) : : : (cid:20) f (x(cid:25)(k)), and A{ = fx(cid:25)(1) : : : x(cid:25)({)g.

The discrete Choquet integral (8) can be seen as a special
type of aggregation operator, namely a generalized arithmetic
mean.
Indeed, (8) coincides with (7) if (cid:23)((cid:1)) is an additive
measure (i.e. if (cid:23)((cid:1)) = (cid:22)((cid:1))). Otherwise, it is a proper gener-
alization of the standard (weighted) NN estimation.

Coming back to our running example, suppose that we
have measured the following rainfalls: y1 = 100, y2 = 120,
y3 = 200. According to (8), we then obtain the estimation
0 (cid:25) 157. As the joint weight of the two locations with
yest
less rainfall, x1 and x2, has been decreased, this estimation is
higher (closer to y3) than the standard weighted NN estima-
tion given by yest

0 = 140.

So far, we have focused on the problem of regression. In
the case of classi(cid:2)cation, the Choquet integral cannot be ap-
plied immediately, since an averaging of class labels y{ does
not make sense.
Instead, the Choquet integral can be de-
rived for each of the indicator functions f‘ : y 7! I(y = ‘),
‘ = ‘1 : : : ‘m. As in (1), the evidence in favor of each class
label is thus accumulated separately. Now, however, the in-
teraction between cases is taken into account. As usual, the
estimation is then given by the label with the highest degree
of accumulated evidence.

5 Empirical Validation
In order to validate the extension of NN estimation as pro-
posed in the previous section, we have performed several ex-
perimental studies using benchmark data sets from the UCI
repository4 and the StatLib archive.5

Experiments were performed in the following way: A data
set is randomly split into a training and a test set of the same
size. For each example in the test set, a prediction is derived
using the training set in combination with weighted k-NN
resp. Cho-k-NN. In the case of regression, an estimation y est
is evaluated by the relative estimation error jyest
0 (cid:0)y0j(cid:1)jy0j(cid:0)1,
and the overall performance of a method by the mean of this
error over all test examples. In the case of classi(cid:2)cation, we
3The Choquet integral can be extended to any real-valued func-

0

tion through decomposition into a positive and negative part.

4http://www.ics.uci.edu/~mlearn
5http://stat.cmu.edu/

data set
auto-mpg

bolts

housing

detroit

echomonths

pollution

k weighted k-NN
12.21 (0.05)
5
7
12.18 (0.06)
47.07 (1.19)
5
51.36 (1.25)
7
14.83 (0.08)
5
7
14.99 (0.09)
16.02 (0.55)
5
15.93 (0.55)
7
97.77 (7.17)
5
99.03 (8.98)
7
5
4.12 (0.05)
4.22 (0.05)
7

Cho-k-NN
11.56 (0.05)
11.53 (0.05)
38.77 (0.71)
39.94 (0.81)
14.48 (0.08)
14.62 (0.08)
14.90 (0.50)
14.71 (0.54)
72.87 (3.87)
74.80 (7.55)
4.05 (0.05)
4.18 (0.05)

Table 1: Estimation of expected relative estimation error and
its standard deviation.

simply took the misclassi(cid:2)cation rate as a performance index.
Moreover, we derived statistical estimations of the expected
performance of a method by repeating each experiment 100
times.

For the purpose of similarity computation, all numeric at-
tributes have (cid:2)rst been normalized to the unit interval by lin-
ear scaling. The similarity was then de(cid:2)ned by 1(cid:0)distance
for numeric variables and by the standard 0/1-measure in the
case of categorical attributes. The overall similarity sim((cid:1))
was (cid:2)nally obtained by the average over all attributes. As
the purpose of our study was to compare (cid:150) under equal con-
ditions (cid:150) weighted k-NN with Cho-k-NN in order to verify
whether or not taking interactions into account is useful, we
refrained from tuning both methods, e.g. by including fea-
ture selection or feature weighting (even though it is well-
known that such techniques can greatly improve performance
[Wettschereck et al., 1997]). Results have been derived for
neighborhood sizes of k = 5 and k = 7; the parameter (cid:11) in
(5) has always been set to 1=2.

The application of Cho-k-NN for regression has shown
that it consistently improves weighted k-NN, sometimes only
slightly but often even considerably. Some results are shown
in table 1. In particular, it seems that the smaller the size of
the data set, the higher the gain in performance. This (cid:2)nding
is intuitively plausible, since for large data sets the neighbor-
hoods of a query tend to be more (cid:147)balanced(cid:148); as already said,
the neglect of interaction is likely to be less harmful under
such circumstances.

For classi(cid:2)cation problems, it is also true that Cho-k-NN
consistently outperforms weighted k-NN; see table 2. Usu-
ally, however, the gain in classi(cid:2)cation accuracy is only small,
in many cases not even statistically signi(cid:2)cant. Again, this is
especially true for large data sets, and all the more if the clas-
si(cid:2)cation error is already low for standard k-NN. Neverthe-
less, one should bear in mind that, in the case of classi(cid:2)cation,
the (cid:2)nal prediction is largely insensitive toward modi(cid:2)cations
of the estimated evidences in favor of the potential labels. In
fact, in this study we only checked whether the (cid:2)nal predic-
tion is correct or not and, hence, used a rather crude quality
measure. More subtle improvements of an estimation such
as, e.g., the enlargement of an example’s margin [Schapire et

data set
glass

wine

zoo

ecoli

balance

derma

k weighted k-NN
33.58 (0.34)
5
7
34.54 (0.37)
3.52 (0.20)
5
3.27 (0.20)
7
10.43 (0.60)
5
7
11.65 (0.51)
16.30 (0.25)
5
16.17 (0.24)
7
15.62 (0.14)
5
13.28 (0.16)
7
5
3.75 (0.13)
3.57 (0.12)
7

Cho-k-NN
33.23 (0.35)
33.77 (0.39)
3.48 (0.20)
3.25 (0.20)
10.08 (0.60)
11.35 (0.52)
16.29 (0.24)
16.11 (0.24)
15.23 (0.16)
13.15 (0.16)
3.68 (0.13)
3.46 (0.11)

Table 2: Estimation of expected classi(cid:2)cation error and its
standard deviation.

al., 1998], are not honored by this measure. For the future,
we therefore plan to complement our results by more sophis-
ticated experimental comparisons.

6 Concluding Remarks
This paper has motivated the consideration of mutual depen-
dencies between cases that represent past experience in case-
based learning. The basic idea is that a combination of two
or more cases can provide complementary but also redundant
evidence. In order to model this type of interaction in a formal
way, we have proposed the use of non-additive measures. The
aggregation of different pieces of evidence can then be ac-
complished by means of the Choquet integral. The inference
scheme thus obtained, referred to as CHO-k-NN, is a direct
extension of the standard weighted NN estimation (which is
recovered in the case of an additive measure).

Our experimental results have shown that CHO-k-NN con-
sistently outperforms standard (weighted) k-NN on publicly
available benchmark data. Sometimes, only marginal im-
provements can be achieved, particularly in the case of clas-
si(cid:2)cation and all the more for large, (cid:147)well-distributed(cid:148) data
sets, but for many problems CHO-k-NN is considerably bet-
ter than k-NN. All things considered, it can be said that taking
interaction between cases in CBL into account is worthwhile
by any means (mostly it helps, at worst it remains ineffective).
As already said, the derivation of a non-additive measure
(cid:23)((cid:1)) from the similarity function sim((cid:1)) as outlined in sec-
tion 4 is only a (cid:2)rst attempt which leaves scope for develop-
ment. In particular, the degree to which a set of cases is com-
plementary resp. redundant might not only depend on their
mutual similarity but also on other aspects.

We have explained how to use the Choquet integral as
an aggregation operator in both regression and classi(cid:2)cation
problems. An interesting question concerns the extension of
our approach to more general problem solving tasks as they
are typically found in case-based reasoning. Even though it
is obvious that the approach cannot be transferred immedi-
ately, the basic ideas and concepts might still be useful. In
any case, the concrete solution will strongly depend on the
particular type of application.

References
[Aha et al., 1991] D.W. Aha, D. Kibler, and M.K. Albert.
Instance-based learning algorithms. Machine Learning,
6(1):37(cid:150)66, 1991.

[Aha, 1997] D.W. Aha, editor. Lazy Learning. Kluwer Aca-

demic Publ., 1997.

[Choquet, 1954] G. Choquet. Theory of capacities. Annales

de l’Institut Fourier, 5:131(cid:150)295, 1954.

[Dasarathy, 1991] B.V. Dasarathy, editor. Nearest Neighbor
(NN) Norms: NN Pattern Classi(cid:2)cation Techniques. IEEE
Computer Society Press, Los Alamitos, California, 1991.
The distance-weighted k-
IEEE Transactions on Systems,

nearest-neighbor rule.
Man, and Cybernetics, SMC-6(4):325(cid:150)327, 1976.

[Dudani, 1976] S.A. Dudani.

[Grabisch, 1997] M. Grabisch.

k-order additive discrete
fuzzy measures and their representation. Fuzzy Sets and
Systems, 92:167(cid:150)189, 1997.

[H¤ullermeier, 2002] E. H¤ullermeier. On the representation
and combination of evidence in instance-based learning. In
Proc. ECAI(cid:150)2002, pages 360(cid:150)364, Lyon, France, 2002.
[Hume, 1999] D. Hume. An Enquiry concerning Human Un-

derstanding. Oxford Univ. Press Inc., New York, 1999.

[Kolodner, 1993] J.L. Kolodner. Case-based Reasoning.

Morgan Kaufmann, San Mateo, 1993.

[Lindenbaum et al., 1999] M. Lindenbaum, S. Marcovich,
and D. Rusakov. Selective sampling for nearest neighbor
classi(cid:2)ers. AAAI-99, pages 366(cid:150)371, Orlando, 1999.

[McSherry, 2002] D. McSherry.

Diversity-concious re-

trieval. ECCBR(cid:150)02, pages 219(cid:150)233. Springer, 2002.

[Murofushi and Soneda, 1993] T. Murofushi and S. Soneda.
Techniques for reading fuzzy measures (iii):
interaction
index. In 9th Fuzzy System Symposium, pages 693(cid:150)696.
Sapporo, Japan, 1993.

[Oliver and Webster, 1990] M. Oliver, R. Webster. Kriging:
a method of interpolation for geographical information
system. Int. J. Geogr. Inform. Syst., 4(3):313(cid:150)332, 1990.
[Rendell, 1986] L. Rendell. A general framework for induc-
tion and a study of selective induction. Machine Learning,
1:177(cid:150)226, 1986.

[Riesbeck and Schank, 1989] C.K. Riesbeck

and R.C.
Inside Case-based Reasoning. Hillsdale, New

Schank.
York, 1989.

[Schapire et al., 1998] RE. Schapire, Y. Freund, P. Bartlett,
and WS. Lee. Boosting the margin: a new explanation for
the evectiveness of voting methods. Annals of Statistics,
26:1651(cid:150)1686, 1998.

[Sugeno, 1974] M. Sugeno. Theory of Fuzzy Integrals and

its Application. PhD thesis, Tokyo Inst. of Techn., 1974.

[Wettschereck et al., 1997] D. Wettschereck, D.W. Aha, and
T. Mohri. A review and empirical comparison of feature
weighting methods for a class of lazy learning algorithms.
AI Review, 11:273(cid:150)314, 1997.

[Zhang et al., 1997] J. Zhang, Y. Yim, and J. Yang. Intelli-
gent selection of instances for prediction in lazy learning
algorithms. Art. Intell. Review, 11:175(cid:150)191, 1997.


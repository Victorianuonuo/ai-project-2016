Determining Expert Proﬁles

(With an Application to Expert Finding)

Krisztian Balog and Maarten de Rijke

ISLA, University of Amsterdam

Kruislaan 403, 1098 SJ Amsterdam
kbalog,mdr@science.uva.nl

Abstract

The proﬁle of an individual is a record of the types
and areas of skills of that individual (“topical pro-
ﬁle”) plus a description of her collaboration net-
work (“social proﬁle”).
In this paper we deﬁne
and formalize the task of automatically determin-
ing an expert proﬁle of a person from a heteroge-
neous corpus made up of a large organization’s in-
tranet. We propose multiple models for address-
ing the topical proﬁling task. Our main methods
build on ideas from information retrieval, while re-
ﬁnements bring in ﬁltering (allowing an area into a
person’s proﬁle only if she is among the top rank-
ing experts in the area). An evaluation based on the
W3C-corpus made available by TREC, shows sig-
niﬁcant improvements of the reﬁned methods over
the baseline. We apply our proﬁling algorithms to
signiﬁcantly enhance the performance of a state-of-
the-art expert ﬁnding algorithm and to help users of
an operational expert search system ﬁnd the person
they would contact, given a speciﬁc problem, topic
or information need. Finally, we address the task
of determining a social proﬁle for a given person,
using graph-based methods.

for experts, but not in isolation—the desired output should be
more than a ranked list of person names [Hawking, 2004].
Context and evidence are needed to help users of expertise
ﬁnding systems decide whom to contact when seeking ex-
pertise in some area. E.g., given an expert whose name is
returned in response to a query, what are her areas of exper-
tise? Who does she work with? What are her contact details?
Is she well-connected, just in case she is not able to help us
herself?

The main aim of this paper is to introduce the task of de-
termining an expert’s proﬁle—i.e., a concise description of
the areas in which she is an expert plus a description of her
collaboration environment—, and to devise and assess algo-
rithms that address this proﬁling task. To make matters more
concrete, let us look at an expert ﬁnding system that is cur-
rently being developed; it started out as “a ranked list of per-
son names” system and is now evolving so as to include the
type of context and evidence discussed above. Figure 1 pro-
vides a screen dump. In Figure 1 we see the information dis-

Introduction

1
Some of the most valuable knowledge in an enterprise re-
sides in the minds of its employees. Enterprises must com-
bine digital information with the knowledge and experience
of employees. Expert ﬁnding addresses the task of ﬁnding
the right person with the appropriate skills and knowledge:
“Who are the experts on topic X?” The task has recently re-
ceived increased attention, especially since the launch of an
expert ﬁnding task as part of the enterprise track at TREC in
2005 [TREC, 2005]. Given a query (describing the area in
which expertise is being sought), participating systems have
to return a ranked list of person names in response.

Like most tasks assessed at retrieval evaluation platforms
such as TREC, NTCIR, CLEF, and INEX, the expert ﬁnding
task is an abstraction of a real task. The abstractions are im-
portant when trying to set up experiments that will lead to
stable and re-usable test sets [Voorhees and Harman, 2005].
But when people search for expertise, they are often looking

Figure 1: Screen dump of the expert search interface.

played for one person (one of the hits produced in response to
a query on “authoring tools”). In the top row we see the per-
son’s name (optionally his internal id or username), and his
relevance for the given topical query. Below this, the contact
details (e-mail, web address, phone, fax number) are shown.
The keywords serve as a type of context, in the form of a
“tag cloud,” describing the general interests and activities of
the person; these keywords are extracted from documents that
the person is associated with. The candidate’s topical proﬁle
is presented as a list of knowledge areas, and the level of com-
petence in each (which is reﬂected by the size of the bars in

IJCAI-07

2657

our example). The relative ranking is shown when the person
is among the top experts given that ﬁeld. (As an aside the full
interface also includes links to documents associated with the
person, as well as a link to a ﬁgure depicting the candidate’s
social proﬁle; see Section 5 for more on the latter.)

Our main contribution in this paper is the introduction of
expert proﬁling as a task, together with algorithms for ad-
dressing the task. We ﬁrst ask whether existing expert ﬁnd-
ing algorithms can be used for topical proﬁling—effectively,
these algorithms build up a matrix with experts along one di-
mension, and topical areas along the other: given an area, de-
termine the candidate experts with the highest expertise lev-
els. Can these algorithms be “inverted”: given a candidate,
ﬁnd the areas for which her expertise levels are highest?

Our answer to this question is a clear “No”—implying that
expert ﬁnding and expert proﬁling are two distinct tasks. The
next question, then, becomes: How can we compute expert
proﬁles? We propose two models for extracting expert pro-
ﬁles; the ﬁrst uses information retrieval techniques to obtain
a set of relevant documents for a given knowledge area, and
aggregates the relevance of those documents that are associ-
ated with the given person. Our second model represents both
candidates and knowledge areas as a set of keywords, and the
skills of an individual are estimated based on the overlap be-
tween these sets. We demonstrate that both models perform
signiﬁcantly better than the “inverted” expert search results,
which serves as our baseline. Moreover, we introduce a ﬁl-
tering algorithm, which rejects areas to be part of the individ-
ual’s proﬁle, if there is not enough evidence found to support
that the person has reasonably high knowledge on that topic,
compared to others within the enterprise.

While proﬁling algorithms are important for feeding the
sort of interface described above, they are also helpful in
other ways within expertise ﬁnding scenarios: we show that
they can be used for re-ranking expert search results. Our
re-ranking method is very effective, improving upon state-of-
the-art expert ﬁnding methods in terms of mean average pre-
cision, mean reciprocal rank, as well as precision@5 scores.
Colleagues and collaborators play an important role in the
value of a person as an expert: an “isolated” expert might be
able to answer speciﬁc questions, but a well-connected ex-
pert might put us on track to explore new or additional ar-
eas [Cross et al., 2001]. A social proﬁle of a person contains
information about her collaborative network. We propose to
capture the social proﬁle of a person by examining the docu-
ments associated with her, and determining which other peo-
ple are also associated with those documents.

In Section 2 we provide background on expert ﬁnding and
proﬁling. Section 3 is devoted to topical proﬁling. In Sec-
tion 4 we put expert proﬁles to work to improve expert ﬁnd-
ing algorithms. Then, in Section 5 we turn to social proﬁles.
We conclude in Section 6.

2 Background
Initial approaches to expert ﬁnding employed a database
housing the skills and knowledge of each individual in the or-
ganization [Maron et al., 1986; Davenport and Prusak, 1998],
and were mainly focused on how to unify disparate and dis-

similar databases of the organization into one data warehouse
that can easily be mined. Most of this early work was per-
formed by the Knowledge Management and Computer Sup-
ported Cooperative Work community, usually called yellow
pages, people-ﬁnding systems or expertise-management [EC-
SCW, 1999]. Most of the tools rely on people to self-assess
their skill against a predeﬁned set of keywords, and there is a
need for intelligent technologies that could enhance the pro-
cess of updating proﬁles [Becerra-Fernandez, 2000].

More recently there has been a move to automatically
extract such representations from heterogeneous document
collections such as those found within a corporate in-
tranet [Craswell et al., 2001]. However, until recently much
of the work in this area has been performed in industry
with only sketchy solutions, tailored to speciﬁc organizational
needs, and without formal evaluations.

TREC 2005 [TREC, 2005] has introduced the expert ﬁnd-
ing task and provided a common platform with the Enterprise
Search Track for researchers to evaluate and assess methods
and techniques. The following scenario is presented: Given
a crawl of the World Wide Web Consortium’s web site, a list
of candidate experts and a set of topics, the task is to ﬁnd ex-
perts (provide a ranked list of candidates) for each of these
topics. This scenario is ideal for measuring the accuracy and
effectiveness of different methods, and for making a fair com-
parison between the performance of expert ﬁnder systems.

To the best of our knowledge, the task introduced in this
paper—expert proﬁling—is new. More precisely, topical ex-
pert proﬁling has not been previously identiﬁed as a task
amenable to computational approaches.
In contrast, social
proﬁling (ﬁnding collaboration networks around an expert) is
a task that has been gaining increasing attention, especially
from researchers with a background in social network analy-
sis. See, e.g., [Abrol et al., 2002] for an industrial example.

3 Topical Proﬁles
In this section we deﬁne what a topical proﬁle is, and propose
two methods for creating such proﬁles.

A topical proﬁle of an individual is a record of the types
and areas of skills and knowledge of that individual, to-
gether with an identiﬁcation of levels of ‘competency’ in each
(“What does expert Y know?”). We model the proﬁle of a
candidate as a vector, where each element of the vector corre-
sponds the person’s skills on the given knowledge area. This
skill is expressed by a score (not a probability), reﬂecting the
person’s knowledge on the given topic.
3.1 Algorithm
The output of a proﬁling algorithm is a ranked list of knowl-
edge areas, the estimated level of competency in each as well
as the evidence that supports these results (e.g., a list of doc-
uments). The task of determining expert proﬁles is naturally
decomposed into two stages.

1. Discovering and identifying possible knowledge areas.
2. Measuring the person’s competency in these areas.
We assume that a list of possible knowledge areas (KA =
{kai|i = 1, . . . , n}) is given, and restrict ourselves to

IJCAI-07

2658

stage 2). We represent the proﬁle of a person ca as a vector
of n values, where the ith element of the vector corresponds
to the knowledge area kai, and score(ca, kai) reﬂects the in-
dividual’s knowledge in the given area:
proﬁle(ca) =

(cid:2)score(ca, ka1), score(ca, ka2), . . . , score(ca, kan)(cid:3)

Baseline
As a baseline topical proﬁling method we use the results gen-
erated by an (existing) expert search method [Balog et al.,
2006]. The expert search method estimates pES(ca|q): what
is the probability of a candidate ca being an expert given
the query topic (knowledge area) q? Let rank ES(ca, q) =
1, . . . , m be the rank of candidate ca on topic q, where the
ranking of candidates is proportional to pES(ca|q):
⇒ rankES(cai, q) ≤ rankES(caj, q).

pES(cai|q) ≥ pES(caj|q)

We can use both the scores and the ranking generated by the
expert search method to estimate the level of competence (or
knowledge) of a given candidates. That is

• Baseline(probability): score(ca, ka) = pES(ca|ka)
• Baseline(rank): score(ca, ka) = 1/rank ES(ca)

A shortcoming of forming scores this way is that they are rel-
ative to the capabilities of other candidates, and do not reﬂect
the individual’s absolute knowledge. Hence, given a knowl-
edge area, we assume the candidate’s skill to be higher if she
is a higher ranked expert on the corresponding topic.
Method 1
We now describe the ﬁrst of two proﬁling methods that go
beyond the baseline. The intuition behind this ﬁrst method is
that a person’s skill can be represented as a score over docu-
ments that are relevant given a knowledge area.

For each knowledge area ka a query-biased subset of docu-
ments Dka is obtained by using the top n documents retrieved
for the query ka. We iterate over the relevant documents,
and sum up the relevance of those that are associated with the
given candidate. Formally, the score of an individual ca given
the knowledge area ka is:

(cid:2)

d∈Dka

score(ca, ka) =

relevance(d, ka)A(d, ca).

(1)
The association method A(d, ca) returns 1 if the name or
the e-mail address of person ca appears in the document d;
otherwise it returns 0. The recognition of candidates is ap-
proached as a (restricted and) specialized named entity recog-
nition task. We do not differentiate between the roles of the
person (author, contact person, mail recipient, etc.), or the
level of the contribution the person may have made to d.

To estimate the relevance of a document, we use stan-
techniques [Baeza-Yates
dard generative language model
and Ribeiro-Neto, 1999]. Speciﬁcally, relevance(d, ka) =
p(ka|θd) expresses how likely the document d would gener-
ate a certain knowledge area ka. The document model is con-
structed by taking a product of a linear combination of the
background model p(t) and the smoothed estimate for each
term t ∈ q:

(cid:4)
(cid:5)
(1 − λ)p(t|d) + λp(t)

p(q|θd) =

(cid:3)

(2)

t∈q

The speciﬁc choice of language modeling (LM) techniques
is pragmatic. For the topical proﬁling task, we could have
used any other information retrieval method for ranking docu-
ments. Our recent work on expert search used LM techniques,
and by re-using that work we can analyze, and obtain a bet-
ter understanding of, differences and similarities between the
expert ﬁnding and proﬁling tasks.

Conceptually, this method is similar to the expert ﬁnding
method Model 2, introduced by [Balog et al., 2006], but as-
sociations are not turned into probabilities, thus their strength
is not estimated—practically (and realistically) speaking, we
simply cannot capture the extent to which the candidate is re-
sponsible for a document’s content, compared to other indi-
viduals that may also be associated with the same document.
Method 2
Our next method takes a completely different approach,
where the proﬁling scores are estimated using keyword sim-
ilarity of candidates and knowledge areas. We ﬁrst tokenize
documents, and remove standard stopwords (no stemming is
applied), then extract the top 20 keywords for each docu-
ment, using the TF·IDF weighting formula [Baeza-Yates and
Ribeiro-Neto, 1999]. Let KW (d) be the set of keywords that
are extracted from document d.

We can obtain the set of keywords of a knowledge area ka
by looking at a query-biased subset of documents Dka, using
the top n documents retrieved using ka as the query and the
W3C corpus as the document collection. Then we use all
keywords from these documents to form a list of keywords of
the given area. Formally:
KWka =

(3)
Similarly, we can obtain keywords of individuals, by taking
keywords of documents that they are associated with:

KW (d)

d∈Dka

(cid:6)

KWca =

d∈D,A(d,ca)=1 KW (d)

(4)

(cid:6)

Having the set of keywords both for knowledge areas and for
candidates in hand, we estimate the proﬁle scores with the
ratio of co-occuring keywords:

score(ca, ka) = |KWka ∩ KWca|/|KWka|.

(5)

Filtering
We introduce a ﬁltering technique that is to be applied on top
of an existing proﬁling method. In the experimental section
we present its performance both on Method 1 and on Method
2. The intuition behind the method is to provide us with a
natural cut-off point that will restrict the number of people re-
turned in a proﬁling setting so as to only retain experts whose
topical proﬁle we are highly conﬁdent about: a knowledge
area can be part of an individual’s proﬁle if and only if the
person is among the top ranked experts on that ﬁeld. This
means that she has a reasonable knowledge on the given topic
both individually and compared to others.

We allow a given knowledge area to appear in a candidate’s
proﬁle only if the individual scores within the top f among
all candidates on that ﬁeld. Actually, we create a ranking of
experts, using the proﬁle scores, and hence solve an expert
ﬁnding task. To do that, we need a list of potential expert
candidates, which we assume to be given. Then we use the

IJCAI-07

2659

Method
Baseline(probability)
Baseline(rank)
Method 1
Method 2

MAP MRR
0.397
0.320
0.203
0.244
0.503
0.407
0.397
0.486

Table 1: Results of the topical proﬁling methods. The
columns are MAP: Mean Average Precision, and MRR: Mean
Reciprocal Rank. Best scores are in boldface.

results of the expert ﬁnding task to reﬁne the output of the
proﬁling method. Formally,
score(cid:3)(ca, ka) =

(cid:7)

score(ca, ka).
if |{ca(cid:3)|score(ca(cid:3), ka) < score(ca, ka)}| < f
0,

otherwise

3.2 Evaluation
We performed experiments to answer the following ques-
tions: Is “inverted expert ﬁnding” a viable solution to the
proﬁling problem? How well do Method 1 and Method 2
perform? And what is the impact of ﬁltering?
Experimental Setup
The document collection we use is the W3C corpus [W3C,
2005], a heterogenous document repository containing a mix-
ture of different document types crawled from the W3C web-
site. The corpus contains 330,037 documents, adding up to
5.7GB. A list of 1,092 candidate experts was made available,
where each candidate is described with a unique candidate id,
name(s) and one or more e-mail addresses. We took the top-
ics created within the expert ﬁnding task of the 2005 edition
of the TREC Enterprise track: 50 in total; these are the topics
for which experts have to be sought.
Proﬁling
In our evaluation methodology we utilize the fact that the
TREC 2005 topics are actually names of W3C working
groups, and experts on a given topic are members of the corre-
sponding working group. A person may be member of multi-
ple working groups, meaning that she is an expert on multiple
topics. We use the W3C working group names as knowledge
areas, and consider a knowledge area to be part of the individ-
ual’s proﬁle if the person is a member of the corresponding
working group. That is, we reverse the original TREC expert
search assessments.

Table 1 reports the results of our experiments. The answer
to our ﬁrst question (Can an expert search algorithm be “in-
verted” to obtain an effective topical proﬁling method?) is
a clear “No” since both proﬁling methods signiﬁcantly out-
perform both baselines; the probability baseline signiﬁcantly
outperforms the rank baseline.1 Hence, expert ﬁnding and
topical proﬁling are two different tasks. Their mathematical
foundation is common, since in both cases a candidate-topic

1To determine whether the observed differences are statistically
signiﬁcant, we use the two-tailed Wilcoxon Matchedpair Signed-
Ranks Test, and look for improvements at a signiﬁcance level of
0.05.

Method
Method 1
f=150
f=100
f=50
f=30
f=20
f=15
f=10
f= 5

Method 2
f=200
f=150
f=100
f=50
f=30
f=20
f=15
f=10
f=5

MAP MRR removed
0.407
–
0.330
0.403
0.508
0.395
0.737
0.395
0.846
0.392
0.388
0.900
0.408
0.926
0.951
0.385
0.978
0.350
0.397
–
0.625
0.355
0.383
0.718
0.813
0.372
0.907
0.303
0.266
0.944
0.962
0.247
0.972
0.249
0.981
0.229
0.286
0.990

0.503
0.509
0.511
0.535
0.558
0.584
0.649
0.677
0.654
0.486
0.463
0.511
0.511
0.476
0.450
0.461
0.466
0.438
0.463

retrieved

ﬁltered

error rate
–
0.100
0.116
0.156
0.193
0.232
0.277
0.309
0.392
–
0.058
0.065
0.075
0.082
0.094
0.108
0.118
0.132
0.176

Table 2: Results of ﬁltering on the topical proﬁling meth-
ods. The columns are MAP: Mean Average Precision, MRR:
Mean Reciprocal Rank, removed: fraction of the original re-
sults that is ﬁltered out, and error rate: fraction of incorrectly
ﬁltered out proﬁle elements. Best scores are in boldface.

matrix is calculated and then the values are sorted along one
of the dimensions. However, when one is developing algo-
rithms for ﬁlling that matrix, different aspects of the data may
need to be taken into account: documents on a topic are ex-
amined for expert ﬁnding, and documents associated with a
speciﬁc person are considered for expert proﬁling.

Finally, both Method 1 and Method 2 achieved fairly high
MAP and MRR scores on the proﬁling task, with Method 1
signiﬁcantly outperforming Method 2.
Filtering
Recall that ﬁltering (as deﬁned above) allows a knowledge
area to appear in the individual’s proﬁle only if the person is
among the top f ranked experts on that topic. Low f values
imply that the proﬁle will contain only the expertise areas of
a person. As a consequence of ﬁltering, an individual’s pro-
ﬁle may become empty, in which case we get a smaller, but
far more precise result set. The questions we should ask are:
how much do we gain for the experts that we retain, and how
many valid proﬁle elements do we lose (i.e., what is the er-
ror ratio)? See Table 2: for Method 1 and 2 we can obtain
signiﬁcant improvements (in terms of MRR scores) when ﬁl-
tering, where the original method (with no ﬁltering) acts as
a baseline.2 Thus, ﬁltering has an early precision enhancing
effect. However, in most cases the MAP scores drop, which
suggests that recall drops: we lose knowledge areas. This is
conﬁrmed by an inspection of the error ratios. The two proﬁl-
ing methods behave somewhat differently w.r.t. this measure,

2Here, we used the (non-paired) Mann-Whitney U test (two

tailed) to establish signiﬁcance.

IJCAI-07

2660

EF (baseline) 576 0.196

#rel MAP MRR P@5 P@10 P@20
0.332 0.269

0.531

0.336

+ Method 1:

+ Method 2:

(A)
(B) λ = 0.5 576 0.197

576 0.209* 0.659* 0.396* 0.326 0.267
0.584* 0.376* 0.324 0.267

(A)
576 0.181
(B) λ = 0.7 576 0.188

0.576* 0.340
0.559* 0.344

0.292 0.242
0.306 0.254

Table 3: Results of reranking expert ﬁnding results using in-
dividual’s proﬁle. The columns are: reranking method, num-
ber of relevant retrieved candidates, mean average precision,
mean reciprocal rank, precision after 5, 10 and 20 candidates
retrieved. Best results bold face; * denotes signiﬁcant im-
provements over the baseline.

when ﬁltering is applied. But in both cases ﬁltering substan-
tially reduces the size of the result set, and at the same time,
they keep the error ratio of the ﬁltering low. The drop in MAP
scores indicates that we lose appropriate knowledge areas that
were initially ranked highly, while the rise in MRR indicates
that the remaining appropriate ones are ranked higher.

4 An Application to Expert Finding
In this section we describe an application of the extracted
topical proﬁles to expert ﬁnding. Our approach takes the re-
sults of an existing expert ﬁnding method—treated as a black
box—as input, and adjust the results using the individuals’
proﬁles: if a knowledge area ranks low on a person’s proﬁle,
we push the candidate down on the list of experts returned to
the user. We expect this idea to have a precision enhancing ef-
fect, possibly hurting recall. Since we take the expert ﬁnding
method used to be a black box, we do not have any infor-
mation about the applied scoring method, which leaves us no
other option than to use the ranking of the expert ﬁnding re-
sults: we do not make any assumptions about the scores. We
combine the reciprocal of these ranks either in a multiplica-
tive or in an additive way: i.e., rank(cid:3)

EF (ca, ka) =

(A) =
(B) = λ +

1

1

rankEF (ca,ka)

rankP R(ca,ka)

rankEF (ca,ka) + (1 − λ)

1

1

rankP R(ca,ka)

Table 3 shows the results of our reranking methods; we only
include scores with the best performing λ parameter for (B).
Method 1 achieved the highest scores with equal weights on
both the expert ﬁnding and proﬁling rankings, while Method
2 operated best when less changes were allowed on the orig-
inal expert search results. All conﬁgurations improve MRR
and P@5 over the baseline, and this fact conﬁrms that proﬁl-
ing can be used for improving upon an existing expert ﬁnding
method in terms of early precision. Moreover, this special
application of the extracted proﬁles allows us to make further
comparisons between the proﬁling methods: Method 1 out-
performs Method 2, here and on the original proﬁling task.

The combination methods we presented here are fairly sim-
ple, but still proved to have a positive impact on application
of the proﬁles. Further improvements could be pursued us-
ing more sophisticated methods for combining the retrieval
results; consult [Kamps and de Rijke, 2004] for an overview.

5 Social Proﬁles
Collaborators play an important role when one is searching
for expertise. People that an individual is working with are
part of her personal workspace, and can serve as a back-
ground, or context, in which the system’s recommendations
should be interpreted. This collaboration network can also
help us to explore the roles of the individuals within an orga-
nization. We might have a speciﬁc, well-deﬁned need, where
we are looking for an expert, or even for a “specialist.” An-
other typical user scenario is to ﬁnd a “librarian,” who has
access to a wide range of knowledge, and can direct us to a
specialist, after the request has been narrowed down.

5.1 Collaboration network
We interpret the social proﬁle as a collaboration network, and
describe an algorithm for building such a network.

A collaboration network CN is a directed graph (V, E),
where the nodes correspond to people, and a weighted di-
rected edge (x, y) ∈ E indicates the strength of the collabora-
tion between x and y. Given a topic t, a topical collaboration
network CN(t) is a special CN where we restrict ourselves
to collaborations (between people) relevant for t.

Given our enterprise (personal workspace) setting we can
create a topical collaboration network CN(q) given the user’s
query q. The level of the collaboration between two people
x and y is estimated by the relevance of the documents Dxy
that are associated with both x and y. The weight of the edge
between the two individuals is expressed using

w(x, y) =

relevance(d, q)

d∈Dxy

(6)
where Dxy = {d ∈ Dq|A(x, d) ∧ A(y, d)}. A query-biased
subset of documents Dq is obtained by using the top n docu-
ments retrieved for the query q, and A(x, d) is a binary func-
tion denoting whether the person x is associated with the doc-
ument d (see Section 3 for details).

The topical collaboration network CN(q) is built using the

following algorithm:
Init Obtain a query-biased subset of documents Dq
(cid:2)
Step 1 Calculate the topical relevance of each individual
d∈Dq∧A(x,d) relevance(d, q)

∀x ∈ V : R(x, q) =

(7)

(cid:2)

Step 2 Calculate the level of collaboration w(x, y) between

individuals using Formula 6.

Step 3 Make the edges directed and normalize them using
the individuals’ topical relevance. We allow nodes to be
connected with themselves.

∀y ∈ Vx : w(cid:3)(x, y) = w(x, y)
w(cid:3)(x, x) = 1 −(cid:2)
R(x, q)

(8)
w(cid:3)(x, y) (9)
where Vx denotes the set of nodes connected to x: Vx =
{y ∈ V |w(x, y) > 0}.

y∈Vx

IJCAI-07

2661

tion. We described an algorithm for estimating the weights of
edges, based on “co-authored” documents.

Possible improvements include more sophisticated algo-
rithms for extracting topical proﬁles, e.g., taking similarities
and overlap between knowledge areas into account. We are
also developing methods to assess the quality and “useful-
ness” of social proﬁles.
Acknowledgments
Krisztian Balog was supported by the Netherlands Organi-
sation for Scientiﬁc Research (NWO) under project numbers
220-80-001, 600.065.120 and 612.000.106. Maarten de Rijke
was supported by NWO under project numbers 017.001.190,
220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-
001, 612.000.106, 612.066.302, 612.069.006, 640.001.501,
640.002.501, and by the E.U. IST programme of the 6th FP
for RTD under project MultiMATCH contract IST-033104.
References
[Abrol et al., 2002] M. Abrol, U. Mahadevan, K. McCracken,
R. Mukherjee, and P. Raghavan. Social networks for enterprise
webs. In Proceedings WWW 2002, 2002.

[Balog et al., 2006] K. Balog, L. Azzopardi, and M. de Rijke. For-
mal models for expert ﬁnding in enterprise corpora. In Proceed-
ings SIGIR 2006, 2006.

[Baeza-Yates and Ribeiro-Neto, 1999] R. Baeza-Yates and B. Ri-

beiro-Neto. Modern Information Retrieval. ACM Press, 1999.

[Becerra-Fernandez, 2000] I. Becerra-Fernandez. The role of arti-
ﬁcial intelligence technologies in the implementation of People-
Finder knowledge management systems. Knowledge-Based Sys-
tems, 13(5):315–320, 2000.

[Craswell et al., 2001] N. Craswell, D. Hawking, A. M. Vercoustre,
and P. Wilkins. P@noptic expert: Searching for experts not just
for documents. In Ausweb, 2001.

[Cross et al., 2001] R. Cross, A. Parker, L. Prusak, and S.P. Bor-
gatti. Knowing what we know: Supporting knowledge cre-
ation and sharing in social networks. Organizational Dynamics,
30(2):100–120, 2001.

[Davenport and Prusak, 1998] T. H. Davenport and L. Prusak.
Working Knowledge: How Organizations Manage What They
Know. Harvard Business School Press, Boston, MA, 1998.

[ECSCW, 1999] ECSCW’99 Workshop. ”Beyond Knowledge
Management: Managing Expertise”. 6th European Conference
on Computer Supported Cooperative Work, 1999.

[Hawking, 2004] D. Hawking. Challenges in enterprise search. In
Proceedings Fifteenth Australasian Database Conference, 2004.
[Kamps and de Rijke, 2004] J. Kamps and M. de Rijke. The effec-
tiveness of combining information retrieval strategies for Euro-
pean languages. In Proceedings 19th Annual ACM Symposium
on Applied Computing, pages 1073–1077, 2004.

[Maron et al., 1986] M. E. Maron, S. Curry, and P. Thompson. An
inductive search system: Theory, design and implementation.
IEEE Trans. Systems, Man and Cybernetics, 16(1):21–28, 1986.
[TREC, 2005] Enterprise track, 2005. URL: http://www.ins.

cwi.nl/projects/trec-ent/wiki/.

[Voorhees and Harman, 2005] E.M. Voorhees and D.K. Harman,
editors. TREC: Experiment and Evaluation in Information Re-
trieval. MIT Press, 2005.

[W3C, 2005] The W3C test

URL:
http://research.microsoft.com/users/nickcr/
w3c-summary.html.

collection,

2005.

Figure 2: Screen dump of a topical collaboration network dis-
played for the query “authoring tools.”

5.2 Visualization
Once a collaboration network has been built, we need to
present it to end-users of the search system. We opted to
visualize it centered around a selected candidate, where the
thickness of the directed edges reﬂects the strength of the re-
lations. Figure 2 presents a screen dump from our system,
where we search for experts on “authoring tools.” The thick-
ness of an edge corresponds to its weight. The full interface
also displays individual’s proﬁle on mouse-over.
5.3 Evaluation
Topical and social proﬁling go together and form a complete
picture. However, assessment of social proﬁles is an issue
that we are not addressing in this paper. An indirect way of
evaluation is to conduct user studies to measure how “use-
ful” a collaboration network is, when one is searching for ex-
pertise. A direct evaluation may be performed by using the
social collaboration network itself for inferring experts, by
using methods from graph-theory.

6 Conclusions
We presented methods for providing the context and the ev-
idence that supports the recommendations of an operational
expertise search system. We introduced the proﬁle of an in-
dividual, which is a record of the types and areas of skills
of that individual (“topical proﬁle”) plus a description of her
collaboration network (“social proﬁle”). We proposed mul-
tiple models for addressing the topical proﬁling task, and
showed that they signiﬁcantly outperform an “inverted” ex-
pert search baseline. Reﬁnements brought in ﬁltering, allow-
ing an area into a person’s proﬁle only if she is among the top
ranking experts in the area. Experimental evaluation showed
that more than 70% of the retrieved results could be ﬁltered
out, while keeping the error rate as low as 6.5%. We ap-
plied our proﬁling algorithms to signiﬁcantly enhance the per-
formance of a state-of-the-art expert ﬁnding algorithm. Our
reranking method is very effective in terms of early precision.
Finally, we addressed the task of determining a social pro-
ﬁle for a given person. We introduced a graph representation
of the collaboration network, where nodes represent people,
and (weighted, directed) edges reﬂect the level of collabora-

IJCAI-07

2662


State Abstraction Discovery from Irrelevant State Variables

Nicholas K. Jong

Department of Computer Sciences

University of Texas at Austin

Austin, Texas 78712

nkj@cs.utexas.edu

Peter Stone

Department of Computer Sciences

University of Texas at Austin

Austin, Texas 78712

pstone@cs.utexas.edu

Abstract

Abstraction is a powerful form of domain knowl-
edge that allows reinforcement-learning agents to
cope with complex environments, but in most cases
a human must supply this knowledge. In the ab-
sence of such prior knowledge or a given model,
we propose an algorithm for the automatic discov-
ery of state abstraction from policies learned in one
domain for use in other domains that have similar
structure. To this end, we introduce a novel condi-
tion for state abstraction in terms of the relevance
of state features to optimal behavior, and we ex-
hibit statistical methods that detect this condition
robustly. Finally, we show how to apply temporal
abstraction to beneﬁt safely from even partial state
abstraction in the presence of generalization error.

Introduction

1
Humans can cope with an unfathomably complex world due
to their ability to focus on pertinent information while ignor-
ing irrelevant detail.
In contrast, most of the research into
artiﬁcial intelligence relies on ﬁxed problem representations.
Typically, the researcher must engineer a feature space rich
enough to allow the algorithm to ﬁnd a solution but small
enough to achieve reasonable efﬁciency.
In this paper we
consider the reinforcement learning (RL) problem, in which
an agent must learn to maximize rewards in an initially un-
known, stochastic environment [Sutton and Barto, 1998]. The
agent must consider enough aspects of each situation to in-
form its choices without spending resources worrying about
minutiae. In practice, the complexity of this state represen-
tation is a key factor limiting the application of standard RL
algorithms to real-world problems.

One approach to adjusting problem representation is state
abstraction, which maps two distinct states in the original
formulation to a single abstract state if an agent should treat
the two states in exactly the same way. The agent can still
learn optimal behavior if the Markov decision process (MDP)
that formalizes the underlying domain obeys certain con-
ditions:
the relevant states must share the same local be-
havior in the abstract state space [Dean and Givan, 1997;
Ravindran and Barto, 2003]. However, this prior research
only applies in a planning context, in which the MDP model

is given, or if the user manually determines that the condi-
tions hold and supplies the corresponding state abstraction to
the RL algorithm.

We propose an alternative basis to state abstraction that is
more conducive to automatic discovery.
Intuitively, if it is
possible to behave optimally while ignoring a certain aspect
of the state representation, then an agent has reason to ig-
nore that aspect during learning. Recognizing that discover-
ing structure tends to be slower than learning an optimal be-
havior policy [Thrun and Schwartz, 1995], this approach sug-
gests a knowledge-transfer framework, in which we analyze
policies learned in one domain to discover abstractions that
might improve learning in similar domains. To test whether
abstraction is possible in a given region of the state space, we
give two statistical methods that trade off computational and
sample complexity.

We must take care when we apply our discovered ab-
stractions, since the criteria we use in discovery are strictly
weaker than those given in prior work on safe state abstrac-
tion. Transferring abstractions from one domain to another
may also introduce generalization error. To preserve conver-
gence to an optimal policy, we encapsulate our state abstrac-
tions in temporal abstractions, which construe sequences of
primitive actions as constituting a single abstract action [Sut-
ton et al., 1999]. In contrast to previous work with temporal
abstraction, we discover abstract actions intended just to sim-
plify the state representation, not to achieve a certain goal
state. RL agents equipped with these abstract actions thus
learn when to apply state abstraction the same way they learn
when to execute any other action.

In Section 2, we describe our ﬁrst contribution, an alterna-
tive condition for state abstraction and statistical mechanisms
for discovery. In Section 3, we describe our second contribu-
tion, an approach to discovering state abstractions and then
encapsulating them within temporal abstractions.
In Sec-
tion 4, we present an empirical validation of our approach.
In Section 5, we discuss related work, and in Section 6, we
conclude.

2 Policy irrelevance
2.1 Deﬁning irrelevance
First we recapitulate the standard MDP notation. An MDP
hS, A, P, Ri comprises a ﬁnite set of states S, a ﬁnite set

of actions A, a transition function P : S × A × S →
[0, 1], and a reward function R : S × A → R. Exe-
cuting an action a in a state s yields an expected immedi-
ate reward R(s, a) and causes a transition to state s0 with
probability P (s, a, s0). A policy π : S → A speciﬁes
(cid:80)
an action π(s) for every state s and induces a value func-
tion V π : S → R that satisﬁes the Bellman equations
s0∈S P (s, π(s), s0)V π(s0), where
V π(s) = R(s, π(s)) + γ
γ ∈ [0, 1] is a discount factor for future reward that may
be necessary to make the equations satisﬁable. For every
MDP at least one optimal policy π∗ exists that maximizes
the value function at every state simultaneously. We denote
the unique optimal value function V ∗. Many learning algo-
rithms converge to optimal policies by estimating the opti-
mal state-action value function Q∗ : S × A → R, with
Q∗(s, a) = R(s, a) + γ
Without loss of generality, assume that the state space is the
cartesian product of (the domains of) n state variables X =
{X1, . . . , Xn} and m state variables Y = {Y1, . . . , Ym}, so
S = X1 × ··· × Xn × Y1 × ··· × Ym. We write [s]X to de-
note the projection of s onto X and s0 |= [s]X to denote that
s0 agrees with s on every state variable in X . Our goal is to
determine when we can safely abstract away Y. In this work
we introduce a novel approach to state abstraction called pol-
icy irrelevance. Intuitively, if an agent can behave optimally
while ignoring a state variable, then we should abstract that
state variable away. More formally, we say that Y is policy ir-
relevant at s if some optimal policy speciﬁes the same action
for every s0 such that s0 |= [s]X :

s0∈S P (s, π(s), s0)V ∗(s0).

(cid:80)

∃a∀s0|=[s]X∀a0 Q∗(s0, a) ≥ Q∗(s0, a0).

(1)
If Y is policy irrelevant for every s, then Y is policy irrelevant
for the entire domain.

Consider the illustrative toy
domain shown in Figure 1. It has
just four nonterminal states de-
scribed by two state variables, X
and Y . It has two deterministic
actions, represented by the solid
and dashed arrows respectively.
When X = 1, both actions ter-
Figure 1: A domain with
minate the episode but determine
four nonterminal states and
the ﬁnal reward, as indicated in
two actions. When X = 1
the ﬁgure. This domain has two
both actions transition to an
absorbing state, not shown.
optimal policies, one of which
we can express without Y : take the solid arrow when X = 0
and the dashed arrow when X = 1. We thus say that Y is
policy irrelevant across the entire domain.

Note however that we cannot simply aggregate the four
states into two states. As McCallum pointed out, the state
distinctions sufﬁcient to represent the optimal policy are not
necessarily sufﬁcient to learn the optimal policy [McCallum,
1995]. In this example, observe that if we treat X = 1 as a
single abstract state, then in X = 0 we will learn to take the
dashed arrow, since it transitions to the same abstract state
as the solid arrow but earns a greater immediate reward. We
demonstrate how to circumvent this problem while still ben-
eﬁtting from the abstraction in Section 3.2.

(cid:84)

Figure 2: The domain of
Figure 1 with some learned
Q values.

2.2 Testing irrelevance
If we have access to the transition and reward functions,
we can evaluate the policy irrelevance of a candidate set of
state variables Y by solving the MDP using a method, such
as policy iteration, that can yield the set of optimal actions
π∗(s) ⊆ A at each state s. Then Y is policy irrelevant at s if
some action is in each of these sets for each assignment to Y:
s0|=[s]X π∗(s0) 6= ∅.
However, testing policy irrele-
vance in an RL context is trick-
ier if the domain has more than
one optimal policy, which is of-
ten the case for domains that con-
tain structure or symmetry. Most
current RL algorithms focus on
ﬁnding a single optimal action
at each state, not all the optimal
actions. For example, Figure 2
shows the Q values learned from
a run of Q-learning,1 a standard algorithm that employs
stochastic approximation to learn Q∗ [Watkins, 1989]. Even
though the state variable Y is actually policy irrelevant, from
this data we would conclude that an agent must know the
value of Y to behave optimally when X = 1. In this trial
we allowed the learning algorithm enough exploration to ﬁnd
an optimal policy but not enough to converge to accurate Q
values for every state-action pair. We argue that this phe-
nomenon is quite common in practical applications, but even
with sufﬁcient exploration the inherent stochasticity of the
domain may disguise state variable irrelevance. We the pro-
pose two methods for detecting policy irrelevance in a manner
robust to this variability.
Statistical hypothesis testing
Hypothesis testing is a method for drawing inferences about
the true distributions underlying sample data. In this section,
we describe how to apply this method to the problem of in-
ferring policy irrelevance. To this end, we interpret an RL al-
gorithm’s learned value Q(s, a) as a random variable, whose
distribution depends on both the learning algorithm and the
domain.
Ideally, we could then directly test that hypothe-
sis (1) holds, but we lack an appropriate test statistic. Instead,
we assume that for a reasonable RL algorithm, the means of
these distributions share the same relationships as the corre-
sponding true Q values: Q(s, a) ≥ Q(s, a0) ≡ Q∗(s, a) ≥
Q∗(s, a0). We then test propositions of the form

Q(s, a) ≥ Q(s, a0),

(2)
using a standard procedure such as a one-sided paired t-test
or Wilcoxon signed ranks test [Degroot, 1986]. These tests
output for each hypothesis (2) a signiﬁcance level ps,a,a0. If
Q(s, a) = Q(s, a0) then this value is a uniformly random
number from the interval (0, 1). Otherwise, ps,a,a0 will tend
towards 1 if hypothesis (2) is true and towards 0 if it is false.
We combine these values in a straightforward way to obtain a
conﬁdence measure for hypothesis (1):

p = max

a

min
s0|=[s]X

min
a06=a

ps0,a,a0 .

(3)

1No discounting, learning rate 0.25, Boltzmann exploration with

starting temperature 50, cooling rate 0.95, for 50 episodes

00113301X=0X=1Y=1Y=0Y=1Y=0X=0X=13.002.6000.991.922.811.482.93Figure 3 shows these p values for our toy domain. To ob-
tain the data necessary to run the test, we ran 25 independent
trials of Q-learning. We used the Wilcoxon signed-ranks test,
which unlike the t-test does not assume that Q(s, a) is Gaus-
sian. In Figure 3a we see “random” looking values, so we
accept that Y is policy irrelevant for both values of X. In
Figure 3b we see values very close to 0, so we must reject our
hypothesis that X is policy irrelevant for either value of Y .
In our work, we use 0.05 as a threshold for rejecting hypoth-
esis (1). If p exceeds 0.05 for every s, then Y is irrelevant
across the entire domain. In practice this number seems quite
conservative, since in those cases when the hypothesis is false
we consistently see p values orders of magnitude smaller.

(a)

(b)

Figure 3: The value of p for each of the two abstract states when
testing the policy irrelevance of (a) Y and (b) X.

Monte Carlo simulation
The hypothesis testing approach is computationally efﬁcient,
but it requires a large amount of data. We explored an alter-
native approach designed to conserve experience data when
interaction with the domain is expensive. We draw upon
work in Bayesian MDP models [Dearden et al., 1999] to
reason more directly about the distribution of each Q(s, a).
This technique regards the successor state for a given state-
action pair as a random variable with an unknown multino-
mial distribution. For each multinomial distribution, we per-
form Bayesian estimation, which maintains a probability dis-
tribution over multinomial parameters. After conditioning on
state transition data from a run of an arbitrary RL algorithm,
the joint distribution over the parameters of these multinomi-
als gives us a distribution over transition functions. The vari-
ance of this distribution goes to 0 and its mean converges on
the true transition function as the amount of data increases.2
Once we have a Bayesian model of the domain, we can ap-
ply Monte Carlo simulation to make probabilistic statements
about the Q values. We sample MDPs from the model and
solve them3 to obtain a sample for each Q value. Then we
can estimate the probability that Q∗(s, a) ≥ Q∗(s, a0) holds
as the fraction of the sample for which it holds. We use this
probability estimate in the same way that we used the signif-
icance levels in the hypothesis testing approach to obtain a
conﬁdence measure for the policy irrelevance of Y at some s:
(4)

Pr(Q∗(s0, a) ≥ Q∗(s0, a0)).

p = max

min
s0|=[s]X

min
a06=a

a

value of p = 0 for cases in which Y actually is relevant; we
obtain a value near 1 when only one action is optimal; we
obtain a uniformly random number in (0, 1) when more than
one action is optimal. Although it achieves similar results
using less data, this method incurs a higher computational
cost due to the need to solve multiple MDPs.4

3 Abstraction discovery
3.1 Discovering irrelevance
The techniques described in Section 2.2 both involve two
stages of computation. In the ﬁrst stage, they acquire sam-
ples of state-action values, either by solving the task repeat-
edly or by solving sampled MDPs repeatedly. In the second
stage, they use this data to test the relevance of arbitrary sets
of state variables at arbitrary states. Any one of these tests in
the second stage is very cheap relative to the cost of the ﬁrst
stage, but the number of possible tests is astronomical. We
must limit both the sets of state variables that we test and the
states at which we test them.
First consider the sets of state variables. It is straightfor-
ward to prove that if Y is policy irrelevant at s, then every
subset of Y is also policy irrelevant at s.5 A corollary is that
we only need to test the policy irrelevance of {Y1, . . . , Yk}
at s if both {Y1, . . . , Yk−1} and {Yk} are policy irrelevant
at s. This observation suggests an inductive procedure that
ﬁrst tests each individual state variable for policy irrelevance
and then tests increasingly larger sets only as necessary. This
inductive process will continue only so long as we ﬁnd in-
creasingly powerful abstractions.

We can afford to test each state variable at a given state,
since the number of variables is relatively small. In contrast,
the total number of states is quite large: exponential in the
number of variables. We hence adopt an heuristic approach,
which tests for policy irrelevance only at those states visited
on some small number of trajectories through the task. For
these states, we then determine what sets of state variables
are policy irrelevant, as described above. For each set of state
variables we can then construct a binary classiﬁcation prob-
lem with a training set comprising the visited states. An ap-
propriate classiﬁcation algorithm then allows us to generalize
the region over which each set of state variables is policy ir-
relevant. Note that in Section 3.2 we take steps to ensure that
the classiﬁers’ generalization errors do not lead to the appli-
cation of unsafe abstractions.

3.2 Exploiting irrelevance
Section 3.1 describes how to represent as a learned classi-
ﬁer the region of the state space where a given set of state
variables is policy irrelevant. A straightforward approach to
state abstraction would simply aggregate together all those

This method seems to yield qualitatively similar results to
the hypothesis testing method. We almost always obtain a

2It is also possible to build a Bayesian model of the reward func-
tion, but all the domains that we have studied use deterministic re-
wards.

3We use standard value iteration.

4We ameliorate this cost somewhat by initializing each MDP’s
value function with the value function for the maximum likelihood
MDP, as in [Strens, 2000].

5The converse is not necessarily true. Suppose we duplicate an
otherwise always relevant state variable. Then each copy of the state
variable is always policy irrelevant given the remainder of the state
representation, but the pair of them is not.

0.7310.367X=1X=0Y=1Y=00.0010.000X=1X=0Y=1Y=0states in this region that differ only on the irrelevant vari-
ables. However, this approach may prevent an RL algorithm
from learning the correct value function and therefore the op-
timal policy. In Section 2.1 we gave a simple example of such
an abstraction failure, even with perfect knowledge of policy
irrelevance. Generalizing the learned classiﬁer from visited
states in one domain to unvisited states in a similar domain
introduces another source of error. A solution to all of these
problems is to encapsulate each learned state abstraction in-
side a temporal abstraction. In particular, we apply each state
space aggregation only inside an option [Sutton et al., 1999],
which is an abstract action that may persist for multiple time
steps in the original MDP.
Formally, for a set of state variables Y that is policy irrele-
vant over some S0 ⊆ S, we construct an option o = hπ,I, βi,
comprising an option policy π : [S0]X → A, an initiation set
I ⊂ S, and a termination condition β : S → [0, 1]. Once
an agent executes an option o from a state in I, it always ex-
ecutes primitive action π(s) at each state s, until terminating
with probability β(s). We set I = S0 and β(s) = 0.01 for
s ∈ I and β(s) = 1 otherwise.6 Since Y is policy irrelevant
over S0, we may choose an option policy π equal to the pro-
jection onto [S0]X of an optimal policy for the original MDP.
An agent augmented with such options can behave optimally
in the original MDP by executing one of these options when-
ever possible.

Although we believe that the discovery of this structure is
interesting in its own right, its utility becomes most appar-
ent when we consider transferring the discovered options to
novel domains, for which we do not yet have access to an
optimal policy. To transfer an option to a new domain, we
simply copy the initiation set and termination condition. This
straightforward approach sufﬁces for domains that share pre-
cisely the same state space as the original domain. Even when
the state space changes, our representation of I and β as a
learned classiﬁer gives us hope for reasonable generalization.
We can also copy the option policy π, if we expect the opti-
mal behavior from the original domain to remain optimal in
the new domain.

In this paper we assume only that the policy irrelevance
remains the same. We thus relearn the option policy con-
currently with the learning of the high-level policy, which
chooses among the original primitive actions and the discov-
ered options. For each option, we establish an RL subproblem
with state space [I]X and the same action space A. Whenever
an option terminates in a state s, we augment the reward from
the environment with a pseudoreward equal to the current es-
timate of the optimal high-level value function evaluated at
s. We therefore think of the option not as learning to achieve
a subgoal but learning to behave while ignoring certain state
variables. In other words, the option adopts the goals of the
high-level agent, but learns in a reduced state space.

Since each option is just another action for the high-level
agent to select, RL algorithms will learn to disregard options
as suboptimal in those states where the corresponding ab-
stractions are unsafe. The options that correspond to safe
6The nonzero termination probability for s ∈ I serves as a prob-

abilistic timeout to escape from bad abstractions.

state abstractions join the set of optimal actions at each ap-
propriate state. The smaller state representation should allow
the option policies to converge quickly, so RL algorithms will
learn to exploit these optimal policy fragments instead of un-
covering the whole optimal policy the hard way. We illustrate
this process in the next section.

4 Results

Figure 4: The Taxi domain.

We use Dietterich’s Taxi do-
main [Dietterich, 2000],
illus-
trated in Figure 4, as the setting
for our work. This domain has
four state variables. The ﬁrst
two correspond to the taxi’s cur-
rent position in the grid world.
The third indicates the passen-
ger’s current location, at one of
the four labeled positions (Red,
Green, Blue, and Yellow) or in-
side the taxi. The fourth indicates the labeled position where
the passenger would like to go. The domain therefore has
5 × 5 × 5 × 4 = 500 possible states. At each time step, the
taxi may move north, move south, move east, move west, at-
tempt to pick up the passenger, or attempt to put down the
passenger. Actions that would move the taxi through a wall
or off the grid have no effect. Every action has a reward of -1,
except illegal attempts to pick up or put down the passenger,
which have reward -10. The agent receives a reward of +20
for achieving a goal state, in which the passenger is at the des-
tination (and not inside the taxi). In this paper, we consider
the stochastic version of the domain. Whenever the taxi at-
tempts to move, the resulting motion occurs in a random per-
pendicular direction with probability 0.2. Furthermore, once
the taxi picks up the passenger and begins to move, the desti-
nation changes with probability 0.3.

This domain’s representation requires all four of its state
variables in general, but it still affords opportunity for ab-
straction. In particular, note that the passenger’s destination
is only relevant once the agent has picked up the passenger.
We applied the methodology described in Sections 2 and 3
to the task of discovering this abstraction, as follows. First,
we ran 25 independent trials of Q-learning to obtain samples
of Q∗. For each trial, we set the learning rate α = 0.25 and
used -greedy exploration with  = 0.1. Learning to conver-
gence required about 75000 time steps for each trial. This
data allows us to compute the policy irrelevance of any state
variable at any state. For example, consider again the pas-
senger’s destination. To demonstrate the typical behavior of
the testing procedure, we show in Figure 5a the output for ev-
ery location in the domain, when the passenger is waiting at
the upper left corner (the Red landmark), using the Wilcoxon
signed-ranks test. The nonzero p values at every state im-
ply that the passenger’s destination is policy irrelevant in this
case. Note that the values are extremely close to 1 when-
ever the agent has only one optimal action to get to the upper
left corner, which the procedure can then identify conﬁdently.
The squares with intermediate values are precisely the states
in which more than one optimal action exists. Now consider

Figure 5b, which shows the output of the same test when the
passenger is inside the taxi. The p values are extremely close
to 0 in every state except for the four at the bottom middle,
where due to the layout of the domain the agent can always
behave optimally by moving north.

(a)

(b)

Figure 5: The results of the Wilcoxon signed-ranks test for deter-
mining the policy irrelevance of the passenger’s destination in the
Taxi domain. We show the result of the test for each possible taxi
location for (a) a case when the passenger is not yet in the taxi and
(b) the case when the passenger is inside the taxi.

Rather than compute the outcome of the test for every sub-
set of state variables at every state, we followed the approach
described in Section 3.1 and sampled 20 trajectories from the
domain using one of the learned policies. We tested each indi-
vidual state variable at each state visited, again using the hy-
pothesis testing approach. We created a binary classiﬁcation
problem for each variable, using the visited states as the train-
ing set. For the positive examples, we took each state at which
the hypothesis test returns a p value above the conservative
threshold of 0.05. Finally, we applied a simple rule-learning
classiﬁer to each problem:
the Incremental Reduced Error
Pruning (IREP) algorithm, as described in [Cohen, 1995]. A
typical set of induced rules follows:

1. Taxi’s x-coordinate:

(a) y = 1 ∧ passenger in taxi ∧ destination Red

⇒ policy irrelevant

(b) otherwise, policy relevant

2. Taxi’s y-coordinate:

(a) x = 4 ∧ passenger in taxi ⇒ policy irrelevant
(b) otherwise, policy relevant

3. Passenger’s destination:

(a) passenger in taxi ⇒ policy relevant
(b) otherwise, policy irrelevant

4. Passenger’s location and destination

(a) (x = 1 ∧ y = 2) ∨ (x = 1 ∧ y = 1)

⇒ policy irrelevant

(b) otherwise, policy relevant

relevant only when the passenger is in the taxi. The other
three rules classify state variables as usually relevant, except
in narrow cases. For example, rule 1a holds because the Red
destination is in the upper half of the map, y = 1 speciﬁes
that the taxi is in the lower half, and all the obstacles in this
particular map are vertical. Rule 2a is an example of an over-
generalization. When holding the passenger on the rightmost
column, it is usually optimal just to go left, unless the passen-
ger wants to go the Green landmark in the upper-right corner.
We tested the generalization performance of these learned
abstractions on 10 × 10 instances of the Taxi domain with
randomly generated obstacles, running both horizontally and
vertically. We placed one landmark near each corner and oth-
erwise gave these domains the same dynamics as the origi-
nal. Each abstraction was implemented as an option, as dis-
cussed in Section 3.2. Since the locations of the landmarks
moved, we could not have simply transferred option policies
from the original Taxi domain.
In all our experiments, we
used Q-learning with -greedy exploration7 to learn both the
option policies and the high-level policy that chose when to
apply each option and thus each state abstraction.8 To im-
prove learning efﬁciency, we added off-policy training [Sut-
ton et al., 1999] as follows. Whenever a primitive action a
was executed from a state s, we updated Q(s, a) for the high-
level agent as well as for every option that includes s in its
initiation set. Whenever an option o terminated, we updated
Q(s, o) for every state s visited during the execution of o.
Each state-action estimate in the system therefore received
exactly one update for each timestep the action executed in
the state. Figure 6 compares the learning performance of this
system to a Q-learner without abstraction. The abstractions
allowed the experimental Q-learner to converge much faster
to an optimal policy, despite estimating a strict superset of the
parameters of the baseline Q-learner.

5 Related work
Our approach to state abstraction discovery bears a strong re-
semblance to aspects of McCallum’s U-tree algorithm [Mc-
Callum, 1995], which uses statistical hypothesis testing to
determine what features to include in its state representation.
U-tree is an online instance-based algorithm that adds a state
variable to its representation if different values of the variable
predict different distributions of expected future reward. The
algorithm computes these distributions of values in part from
the current representation, resulting in a circularity that pre-
vents it from guaranteeing convergence on an optimal state
abstraction. In contrast, we seek explicitly to preserve opti-
mality.

Our encapsulation of partial state abstractions into options
is inspired by Ravindran and Barto’s work on MDP homo-
morphisms [Ravindran and Barto, 2003] and in particular
their discussion of partial homomorphisms and relativized
options. However, their work focuses on developing a more

The sets of state variables not mentioned either had no pos-
itive training examples or induced an empty rule set, which
classiﬁes the state variables as relevant at every state. Rule set
3 captures the abstraction that motivated our analysis of this
domain, specifying that the passenger’s destination is policy

7 = 0.1 and α = 0.25
8In general, SMDP Q-learning is necessary to learn the high-
level policy, since the actions may last for more than one time step.
However, this algorithm reduces to standard Q-learning in the ab-
sence of discounting, which the Taxi domain does not require.

0.99990.99990.99990.99990.99990.99990.99990.99990.99990.99990.99990.99990.99990.99990.31880.17660.99940.57990.79400.99190.47310.37840.57990.40950.77030.00000.00000.00000.00000.00000.00000.00000.00000.00000.00000.00000.00000.00000.00000.00000.00000.44120.49460.91710.65180.00010.00040.00020.00030.0003the state space. Finally, we showed that encapsulating these
learned state abstractions inside temporal abstractions allows
an RL algorithm to beneﬁt from the abstractions while pre-
serving convergence to an optimal policy.

Acknowledgments
We would like to thank Greg Kuhlmann for helpful comments and
suggestions. This research was supported in part by NSF CAREER
award IIS-0237699 and DARPA grant HR0011-04-1-0035.

References
[Cohen, 1995] William W. Cohen. Fast effective rule induction. In
Proceedings of the Twelfth International Conference on Machine
Learning, pages 115–123, 1995.

[Dean and Givan, 1997] Thomas Dean and Robert Givan. Model
minimization in Markov decision processes. In Proceedings of
the Fourteenth National Conference on Artiﬁcial Intelligence,
1997.

[Dearden et al., 1999] Richard Dearden, Nir Friedman, and David
Andre. Model based Bayesian exploration.
In Proceedings of
the Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence,
pages 150–159, 1999.

[Degroot, 1986] Morris H. Degroot. Probability and Statistics.

Addison-Wesley Pub Co, 2nd edition, 1986.

[Dietterich, 2000] Thomas G. Dietterich. Hierarchical reinforce-
ment learning with the MAXQ value function decomposition.
Journal of Artiﬁcial Intelligence Research, 13:227–303, 2000.

[Mannor et al., 2004] Shie Mannor, Ishai Menache, Amit Hoze,
and Uri Klein. Dynamic abstraction in reinforcement learning
via clustering. In Proceedings of the Twenty-First International
Conference on Machine Learning, pages 560–567, 2004.

[McCallum, 1995] Andrew Kachites McCallum. Reinforcement
Learning with Selective Perception and Hidden State. PhD thesis,
University of Rochester, 1995.
[ ¨Ozg¨ur S¸ims¸ek and Barto, 2004]

¨Ozg¨ur S¸ims¸ek and Andrew G.
Barto. Using relative novelty to identify useful temporal abstrac-
tions in reinforcement learning.
In Proceedings of the Twenty-
First International Conference on Machine Learning, pages 751–
758, 2004.

[Ravindran and Barto, 2003] Balaraman Ravindran and Andrew G.
Barto. SMDP homomorphisms: An algebraic approach to ab-
straction in semi-Markov decision processes. In Proceedings of
the Eighteenth International Joint Conference on Artiﬁcial Intel-
ligence, 2003.

[Strens, 2000] Malcolm Strens. A Bayesian framework for rein-
forcement learning. In Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning, pages 943–950, 2000.
[Sutton and Barto, 1998] Richard S. Sutton and Andrew G. Barto.
Reinforcement Learning: An Introduction. MIT Press, Cam-
bridge, MA, 1998.

[Sutton et al., 1999] Richard S. Sutton, Doina Precup, and Satinder
Singh. Between MDPs and semi-MDPs: A framework for tempo-
ral abstraction in reinforcement learning. Artiﬁcial Intelligence,
112(1–2):181–211, 1999.

[Thrun and Schwartz, 1995] Sebastian Thrun and Anton Schwartz.
Finding structure in reinforcement learning. In Advances in Neu-
ral Information Processing Systems 7, 1995.

[Watkins, 1989] Watkins. Learning From Delayed Rewards. PhD

thesis, University of Cambridge, England, 1989.

Figure 6: The average reward per episode earned by agents with
learned abstractions encapsulated as options and only primitive ac-
tions, respectively, on a 10 × 10 version of the Taxi domain. The
reward is averaged over 10000-step intervals. The results are the
average of 25 independent trials.

agile framework for MDP minimization, not on the discovery
of abstractions in an RL context.

Our work is also related to recent research into the auto-
matic discovery of temporal abstractions [ ¨Ozg¨ur S¸ims¸ek and
Barto, 2004; Mannor et al., 2004], usually in the options
framework. These techniques all seek to identify individ-
ual subgoal states that serve as chokepoints between well-
connected clusters of states or that otherwise facilitate better
exploration of environments. Our usage of options suggests
an alternative purpose for temporal abstractions: to enable the
safe application of state abstractions. Note that we can con-
strue our deﬁnition of policy irrelevance as a statement about
when a single reusable subtask could have contributed to sev-
eral parts of an optimal policy.

The connection to hierarchical RL suggests the recursive
application of our abstraction discovery technique to create
hierarchies of temporal abstractions that explicitly facilitate
state abstractions, as in MAXQ task decompositions [Diet-
terich, 2000]. This possibility highlights the need for ro-
bust testing of optimal actions, since each application of our
method adds new potentially optimal actions to the agent.
However, we leave the development of these ideas to future
work.

6 Conclusion
This paper addressed the problem of discovering state ab-
stractions automatically, given only prior experience in a sim-
ilar domain. We deﬁned a condition for abstraction in terms
of the relevance of state variables for expressing an optimal
policy. We described two statistical methods for testing this
condition for a given state and set of variables. One method
applies efﬁcient statistical hypothesis tests to Q-values ob-
tained from independent runs of an RL algorithm. The other
method applies Monte Carlo simulation to a learned Bayesian
model to conserve experience data. Then we exhibited an
efﬁcient algorithm to use one of these methods to discover
what sets of state variables are irrelevent over what regions of

-4500-4000-3500-3000-2500-2000-1500-1000-500 0 0 100000 200000 300000 400000 500000 600000 700000 800000 900000 1e+06Discovered abstractionsNo abstractions
Models of Searching and Browsing: Languages, Studies, and Applications

Doug Downey

Department of Computer Science and Engineering

University of Washington

Seattle, WA 98195

ddowney@cs.washington.edu

Susan Dumais & Eric Horvitz

Microsoft Research
Redmond, WA 98052

{sdumais,horvitz}@microsoft.com

Abstract

We describe the formulation, construction, and
evaluation of predictive models of human informa-
tion seeking from a large dataset of Web search
activities. We ﬁrst introduce an expressive lan-
guage for describing searching and browsing be-
havior, and use this language to characterize several
prior studies of search behavior. Then, we focus on
the construction of predictive models from the data.
We review several analyses, including an explo-
ration of the properties of users, queries, and search
sessions that are most predictive of future behavior.
We also investigate the inﬂuence of temporal de-
lay on user actions, and representational tradeoffs
with varying the number of steps of user activity
considered. Finally, we discuss applications of the
predictive models, and focus on the example of per-
forming principled prefetching of content.

Introduction

1
User interaction with Web search engines is a commonplace,
yet complex, information-seeking process. In the course of
a search session, users take various actions to accomplish
their goals. They formulate and issue queries, browse re-
sults, navigate to result pages or other content, reformu-
late their queries, request additional results, and often mix
these actions in an iterative and varying manner as they pur-
sue information. Each step in this process is informed by
both the user’s prior experiences and recent history with
search and retrieval, the user’s current goal, and the informa-
tion the user gathers as the session progresses [Bates, 1989;
Rose and Levinson, 2004]. Behavioral data capturing such
rich interaction is becoming available via the collection of
logs of Web searching and browsing activity. This newly
available data provides unprecedented opportunities for re-
search on applications of machine learning and reasoning to
better understand and support human information-seeking ac-
tivities.

As a concrete example, consider the log of search activity
shown in Table 1. Here, a computer user starts with a general
query (“drivers”), clicks on a result (“www.windrivers.com”),
and then returns to the results page (presumably unsatisﬁed).

User Action

Time
5:04:10 PM Queries for drivers
5:04:18 PM Clicks on result http://www.windrivers.com/
5:04:42 PM Returns to results for drivers
5:05:10 PM Requests second result page for drivers
5:06:30 PM Queries for ati 9550 drivers
5:06:35 PM Clicks on result http://support.ati.com/ics/...

End of Session

Table 1: Example of Web Search Activity

After spending some time browsing the second page of re-
sults for “drivers,” the user reformulates the query to be more
speciﬁc (“ati 9550 drivers”). The user then navigates to a de-
sired result page, and the search session ends. Activity logs
such as this can be used to build predictive models of search
behavior. A search engine capable of predicting users’ future
actions could streamline interaction sequences such as these,
potentially offering dramatic improvements in search perfor-
mance.

We report on an investigation of a large dataset of Web
searching and browsing activity, collected with permission
from hundreds of thousands of users. We focus on the use
of machine learning to study the challenges and opportunities
of building predictive models from this data.

Our contributions are as follows:
1. We introduce an expressive language for representing
searching and browsing behavior. This language pro-
vides a unifying framework for discussing and analyzing
models of search activity, regardless of the applications
for which the models may be intended. We employ the
language to describe our study of search activity in the
context of other studies.

2. We explore the construction of predictive models, and
measure the value of different observational variables
within these models. Our experiments demonstrate that
several variables have signiﬁcant effects on predictive
accuracy. We include an analysis of the inﬂuence of
different temporal delays on the likelihood of future ac-
tions, and also consider the tradeoff between complexity
and predictive power when assuming different degrees
of locality of evidence.

3. We formulate and experimentally evaluate a sample ap-

IJCAI-07

2740

plication of the predictive models, aimed at the princi-
pled prefetching of content.

Section 2 describes our language for search activity mod-
els. Section 3 gives the experimental results of our predictive
models, including an analysis of the inﬂuence of different ob-
servational variables. Section 4 discusses applications of the
predictive models and presents experiments on applying the
models to prefetch search engine results. We discuss related
work in Section 5 and conclude with a summary of the work.

2 A Model of Web Search Activity
We found it useful for reﬂection, comparative analysis, and
communication with colleagues to compose an expressive
graphical language for describing search activities.

2.1 State-Machine Representation
A model of search activity is cast in the form of a state
machine representation displayed in Figure 1.
In this state
model, nodes represent events that occur during a user’s in-
teraction with a search engine, and edges indicate the pos-
sible transitions between events. User actions,
indicated
by triangles in the diagram, are events that the user initi-
ates. Boxes within the state diagram represent states that
are machine generated. Lastly, the outer plates, marked by
U and S, indicate that the structures they contain are re-
peated once for each user (U) and search session (S), re-
spectively. Deﬁne the set of search activity elements as
E = {U, S, R, P, q, cr, c¬r, b, v, z}; the individual elements
of E are deﬁned as follows:

• U – User
• S – Session
• q – Query input to search engine
• R – Result page output by search engine
• P – Non-result page output by website
• cr – Click on search result hyperlink
• c¬r – Click on non-result hyperlink
• b – Browser-assisted navigation to previously viewed
pages (e.g., forward/back navigation, tabbed browsing,
etc.)
• v – Visit to Web page by means other than cr, c¬r, b (e.g.
by opening a new browser, typing into the address bar,
clicking a hyperlink from an external application, etc.)

• z – End of session
Doubled lines are used to indicate the session start action

q, and a circle indicates the end state z.

In Figure 1, each user U executes one or more sessions
S. Here a session is deﬁned as a sequence of search activity
directed toward a single user intent; all sessions begin with a
query q. Query actions are always followed by R (the presen-
tation of a search result page), and from the R state the user
can execute any action. As a concrete example, the event se-
quence from the activity log in Table 1 would be expressed as
qRcrP bRqRqRcrP z. We will refer to the state machine in
Figure 1 as the Search Activity Model (SAM).

q

R

b

P

cr

v

c¬r

z

S

U

Figure 1: Search Activity Model

2.2 A Language for Search Activity Models
The set of event sequences generated by the full SAM is in-
ﬁnite; a particular model will often consider only a subset of
these possible sequences. To specify the activity sequences
that a particular model considers, we adopt the following def-
inition:
Deﬁnition The event sequence space (ESS) of a model M is
a regular expression over the vocabulary E ∪ {(), τ} which
matches exactly those event sequences considered by M.
The added construct () indicates events that occur, but
that are not explicitly considered in the model. The added
symbol τ indicates points in the sequence at which the
model considers temporal duration. As an example, Lau and
Horvitz [1999] investigated the inﬂuence of temporal delay
on the type of query reformulation users will next perform
(e.g., specialization, generalization, etc.), given prior refor-
mulations. The event sequences that the Lau & Horvitz model
considers are pairs of queries separated by some number of
ignored non-query events. Additionally, the model considers
the duration between each pair of queries. Thus, the Lau &
Horvitz model has an ESS of qτ(.∗)q. We assume that wild-
cards in the regular expression execute a minimal match, so
a .∗ term (which matches zero or more events of any type) is
prohibited from including the event sequence following it (q
in this case).

The ESS language assists with comparing and contrasting
the intent and results of previous work in search activity mod-
eling. Table 2 lists the event sequence spaces for several re-
cent studies of search activity.1 Note that the ESS language
can be used to characterize a model regardless of the particu-
lar application for which the model may have been designed.
For example, the models in Table 2 were developed for tasks

1An ESS always speciﬁes a sequence of events as executed by a
single user— the element U is included for cases in which users are
explicitly modeled.

IJCAI-07

2741

qτ(.∗)q
[Lau and Horvitz, 1999]
[Beeferman and Berger, 2000]
q(R)cr
[Lee et al., 2005]
qR(crP (.∗))+
[Cui et al., 2002]
(qR(cr?(.∗))∗)+
[Rose and Levinson, 2004]
[Duame and Brill, 2004]
qR
[Davison et al., 2003]
q(.∗)q
[Jones and Fain, 2003]
[Jones et al., 2006]
U(qRcr(.∗))+
[Sun et al., 2005]
S(qR(cr?(.∗))∗)+ [Radlinski and Joachims, 2005]

Table 2: Previous search models. A summary of search ac-
tivity models studied previously, and their respective event
sequence spaces.

as wide ranging as query expansion, result ranking, and pre-
dicting different types of query reformulations, among others.
Importantly, all of the models in Table 2 are server-side
models, in that the user actions they consider – q and cr –
can be detected from a search engine Web server (although
cr events must be redirected through the search engine to be
logged). The full SAM is a client-side model, including event
types (e.g. browser actions b) that require instrumentation
installed on the user’s machine. Similar client-side activity
models have been investigated in recent work (e.g. [Agichtein
et al., 2006; Fox et al., 2005]). With the exception of the
scrolling events in [Fox et al., 2005], those models can be
fully expressed in our language.

A complete model of search activity is characterized by not
only an ESS, but also a parameterization, which maps events
to a vector of features (e.g., a query action can be parameter-
ized by properties like its length or average frequency). Be-
cause parameterizations can vary widely across models, we
do not attempt to express them in our language. Section 3
details the particular parameterizations we employ in our ex-
periments.
2.3 SAMlight
The full SAM considers all browsing activity following a
search action. Because our focus is on search activity in par-
ticular, we will ignore in our experiments the browsing ac-
tivity that is unlikely to be related to a search query. For ex-
ample, when the user types a new URL into the browser’s
address bar, it is unlikely that this URL is directly relevant
to a previous search action. Ignoring this browsing activity
allows us to dramatically reduce the size of the data sets we
analyze, while still retaining many of the events relevant to
search actions. We will focus on a subset of the SAM that we
call SAMlight. SAMlight has the following form:

SAMlight = U(S(q<res>(b<res>)∗ (.∗))+) +

<res> = Rτ(crPτ <path>τ v?(.∗))?
<path> = ((c¬r

|b)P)∗

SAMlight considers a set of users executing one or more
sessions, where each session consists of one or more queries
followed by activity surrounding a result set (<res>) gen-
erated by a search engine. Result set activity includes the

presentation of a results page, followed by an optional result
click. In the case of a result click, we explicitly distinguish
the subsequent path (<path>) of pages traversed via hyper-
link clicks or back actions,2 which tend to be particularly re-
lated to the search query (a similar approach was taken in
[Agichtein et al., 2006]). We consider temporal duration at
multiple points in the sequence, including the delay between
all search actions, the duration of the path, and the time be-
tween the end of the path and the next action.

3 Experiments
We now turn to experiments with building predictive models
of search behavior. We start by describing in detail our data
collection process and the parameterization of the events we
consider, as well as the machine learning methodology we
employ to construct models. We then present the results of
the experiments on predicting a search user’s next action, and
analyze which aspects of the SAM described in Section 2 are
most important for the task.

3.1 Data collection and parameterization
Our data set consists of three weeks of Web activity col-
lected via opt-in client-side instrumentation from a set of over
250,000 users. The data was collected from users who con-
sented to share their searching activities with Microsoft via
the Windows Live Toolbar download. The data consists of
triples of the form (user ID, time, URL visted), represent-
ing user activity. We extracted each user’s search actions
(all search engine queries and result clicks) for three popu-
lar search engines as well as ten Web page requests following
each search action.3

We partitioned the user actions into sessions, which, as de-
scribed in Section 2, are contiguous blocks of activity over
which we assume the user’s search goal remains relatively
constant. Detecting the boundaries of sessions is a challeng-
ing problem which could be informed by a variety of temporal
and behavioral factors. For these experiments, we employed
a simple heuristic that considers any search actions within 30
minutes of each other to be part of the same session, follow-
ing previous work [Radlinski and Joachims, 2005]. We have
explored the sensitivity of session numbers to this segmenta-
tion parameter. We found that the effect of using a shorter
temporal gap of 5 minutes would have been relatively small,
further dividing about 25% of the sessions.

In the models, each action is parameterized by a set of fea-
tures, which is detailed in Table 3. The features express prop-
erties of different elements of the SAM, including the user,
the session, query and click actions, time, and a summary
of information regarding non-search actions following search

2Although the SAM element b refers to browser-aided naviga-
tion in general, in our experiments, we consider only the browser’s
“back” action.

3As our instrumentation returns only which URLs the user re-
quests, we must infer when result clicks occur. We assume all non-
search URL loads originating from a link on a search page are result
clicks. Based on manual inspection, we estimate this heuristic to be
over 95% accurate.

IJCAI-07

2742

actions. Features describing the user (U) include statis-
tics such as how frequently the user searches (U(qPerDay))
or the average time the user takes to click a result page
(U(AvgSecTocr)). Properties of the session (S) include fea-
tures such as the number of queries issued in the current ses-
sion S(Numq). Features of query actions include attributes
of the query text (e.g. the number of words q(WordLen)), the
query words distributions in Web text (e.g. the frequency of
the query’s least frequent word, q(MinWordFreq)), and past
search behavior for the query (e.g. the average click position
for the query, q(AvgcrPos)). Properties of click actions in-
clude attributes like position in the result list (cr(Position)),
and whether the click was on an advertisement (cr(IsAd)).

We used the ﬁrst week of user data to estimate the “aggre-
gate” properties of users and queries, which are averaged over
multiple search sessions (e.g. U(AvgSSec), q(AvgcrPos)).
The following week was used as a training set, and the ﬁnal
week as a test set. Features related to query word distributions
on the Web (e.g. q(MinWordFreq)) were computed using a
400 million page sample of Web text.

3.2 Machine-learning methodology
The central challenge we address is that of predicting the
user’s next action, given a set of previous actions and in-
formation accumulated about the user and the session. For-
mally, we model
the conditional probability distribution
P (an|U, S, an−1, . . . , an−k), where ai represents the ith
search action in the session.4
In this distribution, actions
an−1, . . . , an−k and the U and S variables are parameterized
as in Table 3, and the target variable (the next action an) is
parameterized in different ways for different experiments, as
explained below.

To model the conditional probability distribution, we em-
ployed a learning method that performs Bayesian structure
search to ﬁnd predictive models that best explain the training
data [Chickering, 2002]. The resulting models are Bayesian
dependency networks in which the conditional distributions
at each node take the form of probabilistic decision trees.
Parameters for restricting the density of the dependency net-
works were estimated via cross validation on the training set.
Bayesian structure search to learn dependency networks
is one of several feasible learning procedures. We initially
selected the method because it scaled nicely to training sets
containing half a million instances with up to hundreds of fea-
tures. Also, the dependency models and trees output by the
method allowed us to inspect graphical relationships among
observations and predictions. A comparison of alternative
machine learning algorithms (e.g., logistic regression, Sup-
port Vector Machines, etc.) for this task is an item of future
work.

We parameterized the target variable representing the next
action at two different levels of granularity. At a coarse level
of granularity the variable includes ﬁve output values repre-
senting the nature of the next action. We model whether the
next action is a query (q), result click (cr), or end of session

Number of queries issued
Duration of the session
Ratio of queries to search actions
Maximum q(WordLen) (see below)
Minimum q(WordLen) (see below)

Avg. session length (in seconds)
Avg. search to result click interval
Avg. queries per second within session
Fraction of queries that are repeats
Avg. queries per day
Avg. rank of clicked results
Avg. query length
Ratio of result clicks to queries
Engine queried most frequently
Fraction of queries on preferred engine
Avg. ﬁrst result requested in queries

User Features (U)
U(AvgSSec)
U(AvgSecTocr)
U(qPerSecInS)
U(qRepeatRate)
U(qPerDay)
U(AvgcrPos)
U(AvgqWordLen)
U(crProb)
U(PrefEngine)
U(PrefEngineFreq)
U(AvgFirstResult)
Session Features (S)
S(Numq)
S(DurationSec)
S(qFrac)
S(MaxqWords)
S(MinqWords)
Query Features (q)
q(WordLen)
Length in words
q(CharLen)
Length in characters
q(FirstResult)
Rank of ﬁrst result requested
q(Freq)
Number of times query is issued
q(crProb)
Prob. some result is clicked
q(AvgcrPos)
Avg. result position clicked
q(AvgcrDelay)
Avg. time to click after query
q(AvgPathSec)
Avg. PathDwellSec (see below)
q(AvgPathPages)
Avg. PathPageLen (see below)
q(AvgAfterPathSec) Avg. AfterPathSec (see below)
q(DistinctU)
q(HasDeﬁnitive)
q(HasSuggestion)
q(AdImpressions)
q(AvgNumAds)
q(AdBid)
q(IsAdvanced)
q(MinWordFreq)
q(MaxWordFreq)
q(GeoMeanFreq)
q(AvgWordFreq)
q(MaxColloqQuot) Maximum bigram collocation quotient
q(ContainsName)
q(ContainsLoc)
Result Click Features (cr)
cr(Position)
cr(DwellSec)
cr(IsAd)
q|cr(Engine)
Non-search Action Features (v|c¬r|b)
PathPageLen

Number of distinct users issuing query
1 iff query has “deﬁnitive” result
1 iff query has a spelling suggestion
Number of times ads shown
Average number of ads shown
Average bid for ads on this query.
1 iff query has advanced features
Web frequency of least frequent word
Web frequency of most frequent word
Geo. mean of word Web frequencies
Average of word Web frequencies

Result rank for cr
Time in seconds on the target page.
1 iff the click is on an advertisement
Search engine for query or click

1 iff query contains a person name
1 iff query contains a location name

Length in pages of path after result click
traversed by links or “back” actions.
Time duration for the path
Time from end of path to next action.

PathDwellSec
AfterPathSec
Temporal/Transition Features
τ(SearchAct)
DayOfWeek
TimeOfDay
qq(WordDelta)

Time between two search actions
Day of the week
One of three 8-hour windows
Word length change between queries

4Properties of the session accumulate as the session progresses;
for example, S(DurationSec), when used to predict an, expresses
the duration the session up until action an−1.

Table 3: Events and parameterizations considered in
learning predictive models.

IJCAI-07

2743

 

d
o
o
h

i
l

e
k

i

L

 

g
o
L

 
.

g
v
A

e
c
n
a
m

r
o
f
r
e
P

-1.1
-1.2
-1.3
-1.4
-1.5
-1.6
-1.7
-1.8
-1.9

Log Likelihood
Fine Coarse
-1.497
-0.859

-1.881
-1.108

Accuracy
Fine Coarse
0.323
0.345
0.643
0.615

Marginal
SAMlight

500K
100K

Table 4: Next action prediction. At both the coarse and
ﬁne granularities, SAMlight offers substantial improvements
in likelihood and accuracy over a baseline marginal model.

marginal model, which simply predicts the distribution of
next actions observed in the training data (the marginal
model’s ESS is q|cr|b). Table 4 shows the average log like-
lihood of each model measured on the test set. Addition-
ally, we list each model’s accuracy, the fraction of test set
actions for which the model’s most likely prediction (that
Pmodel(an|U, S, an−1, . . . , an−k)) is correct.
is, arg maxan
SAMlight substantially outperforms the marginal model at
both action granularities and for both measures of perfor-
mance.
Predictive Value of Different SAM Elements
The SAM contains several different events, each of which can
be parameterized. Here, we investigate which of these ele-
ments and parameters are most important for predictive accu-
racy.

We ﬁrst examine the relative importance of SAM events,
and then consider individual features. The fact that the end of
session action z is a deterministic function of τ(SearchAct)
(that is, end of session occurs iff τ(SearchAct) > 30 min-
utes) has the potential to overstate the predictive value of tem-
poral delay. To eliminate this trivial dependency, for these
experiments we remove from the data all cases in which the
next action is z.

Our comparison of different SAM elements is summarized
in Table 5. The likelihood performance at both the coarse
and ﬁne granularities is shown. The coarse and ﬁne accuracy
differences tend to be small for these models (because the
event space is dominated by a few highly probable events).
Thus, in Table 5 we include accuracy on the speciﬁc problem
of predicting whether or not the next action is a result click
(cr), where differences between methods are more apparent.
Because the test set is large, all non-zero accuracy differences
in Table 5 are statistically signiﬁcant (p < 0.001, chi-squared
test).

0

1

2

3

4

Previous Actions k

Figure 2: Predictive performance as event history varies.
Considering search actions beyond the previous one does not
increase predictive performance for training sets of 100K and
500K search events.

(z). We distinguish three types of query actions (q-reform,
q-same, q-nextpage), capturing the relationship between the
next query and the most recently issued query. For the coarse
parameterization, b actions that navigate back to the previous
result page are treated as occurences of q-same. At the ﬁne
level of granularity, we consider a total of 13 output values.
These values extend the coarse granularity in that q-reform
actions are further parameterized by the type of textual mod-
iﬁcation (addition, deletion, or other) as well as whether or
not the user switched search engines. Also, the ﬁne granular-
ity distinguishes clicks on advertisements from other result
clicks, and distinguishes back navigation to a previous query
from a refresh of the same query. For both granularities, the
remaining actions (v, c¬r, and b actions that are not q-same)
are not predicted as output values, but are summarized as in-
put variables as in Table 3.
3.3 Locality of Evidence
The SAMlight model considers arbitrarily long event se-
quences; however,
the conditional probability
P (an|U, S, an−1, . . . , an−k), a machine learning algorithm
requires a ﬁnite choice of k. We measured how prediction
performance varied with k, the number of previous search
actions considered. As can be seen in Figure 2, the per-
formance improves dramatically when considering the pre-
vious action, but after k = 1 the effect of increasing k is
small. In fact, increasing the dimensionality of the parameter
space hinders performance slightly, a fact that remains true
even when we increase the size of training set ﬁve-fold, from
100,000 search events to 500,000. In the remaining experi-
ments, we set k = 1 and use the larger training set.
3.4 Prediction Results
In this subsection, we present results of our experiments on
predicting users next actions. We predict actions at both lev-
els of granularity (coarse and ﬁne) using a number of ele-
ments representing the user, query and session. We also ex-
amine the contribution of different elements and temporal de-
pendencies.

to model

The results of the prediction experiments are shown in
Table 4. We compare the accuracy of SAMlight with a

The Previous Action (PA) model uses a single input fea-
ture, whether the previous action was q (or, treated equiva-
lently, a back action to R) or cr, and ignores the other features
and SAM elements. PA has an ESS of (q|cr|b)(q|cr|b). PA,
like SAMlight, is a client-side model. As mentioned in Sec-
tion 2, many recent search models are server-side models. To
quantify the beneﬁts of client-side instrumentation, we com-
pare PA with a model that uses only the previous action ob-
servable by the search server (ESS of (q|cr)(q|cr)). The PA
model offers substantially improved performance over both
the marginal model and (in terms of likelihood) the server-
side model.
We now examine the combination of PA with additional
SAM elements. The action features {q, cr, b, v, c¬r} add the

IJCAI-07

2744

Marginal
Previous Server Action
Previous Action (PA)
PA + U
PA + S
PA + τ(SearchAct)
PA + {q, cr, b, v, c¬r}
SAMlight

Fine Coarse
-1.271
-1.087
-0.989
-0.985
-0.949
-0.931
-0.912
-0.840

-1.717
-1.398
-1.284
-1.281
-1.243
-1.218
-1.199
-1.128

cr Accuracy
0.601
0.688
0.688
0.695
0.737
0.725
0.733
0.772

Table 5: Importance of different SAM elements. The table
shows the average log likelihood performance on the coarse
and ﬁne prediction tasks, as well as the accuracy of predicting
cr events.

Variable
τ(SearchAct)
q(FirstResult)
q(HasSuggestion)
S(qFrac)
q(HasDeﬁnitive)
q(crProb)
S(Numq)
S(MaxqWords)

Effect on P (cr)

−
−
−
−
+
+
−
+

Table 6: Most predictive individual features. Features are
listed in order of discriminatory inﬂuence, along with an indi-
cation of whether each feature varies positively or negatively
with the probability that the next action is a result click.

most predictive power in terms of likelihood, while the ses-
sion features S offer the greatest accuracy improvements.
Further, the results show that the predictive power of the SAM
elements can be complementary. The SAM elements har-
nessed in concert (in SAMlight) provide better predictions
than models which use the elements in isolation. Somewhat
surprisingly, the user variables do not provide substantial im-
provements over PA. This is probably due to our focus on
predicting high-level events, rather than more detailed infor-
mation (such as individual URLs, where user data has been
valuable [Sun et al., 2005; Chevalier et al., 2003]). Lastly,
note that the relative likelihood differences between models
are similar for both the coarse and ﬁner-grained prediction
tasks, suggesting that the two tasks require similar features
for effective prediction.

The most predictive individual features are listed in Table
6, along with the direction of their inﬂuence on the proba-
bility of a result click. We found that the temporal feature
τ(SearchAct) is particularly important for improving predic-
tive accuracy. Figure 3 shows the temporal dynamics follow-
ing a search query in more detail. Many of the features’ re-
lationships with result click probability are intuitive—for ex-
ample, result clicks are less likely for queries with spelling
suggestions, but more likely for queries with “deﬁnitive” re-
sults (these are queries with a predictable destination such as
“amazon” or “hotmail”). More interestingly, queries request-
ing deeper portions of the result set are less likely to result

0.7

0.6

0.5

0.4

0.3

0.2

0.1

)
y
r
e
u
q

 

e
c
n

i

s

 

e
m

i
t
 
|
 

n
o
i
t
c
a
 
t
x
e
n
(

P

0

0

P(result click)
P(re-query)
P(end session)

25

50

75

100

Time since query (sec)

Figure 3: Effects of delay since previous action. Probabil-
ity distribution over the action following a query based on
τ(SearchAct), the time since the previous action.

in clicks (i.e., q(FirstResult) varies negatively with P (cr))
and result clicks become less likely as the number of queries
S(Numq) executed previously in the session increases.

An interesting ﬁnding highlighted in Figure 3 is that click
probability falls rapidly after a search.
In fact, 50% of re-
sult clicks occur within 15 seconds after a search. This quick
action implies that the latency involved in retrieving a result
page contributes substantially to the total duration of a search
session. Next, we show how we can decrease this latency
by prefetching results the user is likely to access, using spare
bandwidth available while the user examines search results.

4 Applications
Studies of user search activity have previously been applied
to improve the search experience by aiding with query ex-
pansion [Cui et al., 2002], generating reformulation sugges-
tions [Duame and Brill, 2004; Beeferman and Berger, 2000],
and improving result ranking [Agichtein et al., 2006]. A rich
activity model like SAMlight provides an opportunity for im-
proving the search experience in many ways. Below, we in-
vestigate applying the models to the principled prefetching of
content.
4.1 Sample Application: Result Prefetching
Prefetching uses idle bandwidth to proactively acquire con-
tent a user is likely to access, with the intent of reducing the
amount of latency the user experiences. In a prior study of
probabilistic policies for prefetching, it was postulated that
richer user modeling could enable wiser choices of which
content to prefetch, thus providing better bandwidth/latency
tradeoffs [Horvitz, 1998]. Here, we test this hypothesis using
the search activity models learned in Section 3.

Formally, we assume that the user incurs a cost when con-
suming bandwidth or experiencing latency, and that the costs

IJCAI-07

2745

First Result
PA Model
SAMlight

First Result
Prefetch All
PA
SAMlight

5

25

β/α = 1
50
13% 29% 31%
-29%
-576% -138% 35% 61%
0%
0% 40% 55%
0% 13% 43% 61%

y
c
n
e
t
a
L

100%

90%

80%

70%

60%

50%

40%

30%

20%

10%

0

2
8
Extra Pages Fetched per Click

4

6

10

Table 7: Cost reductions via prefetching. Shown is the
percentage cost reduction (versus the no-prefetch default) for
each method, as latency/bandwidth preferences vary. Nega-
tive reductions represent increased costs. PA and SAMlight
do not increase cost for any preference values, and SAMlight
offers the largest reductions overall.

lists the cost reduction due to prefetching using each method.
As preference values vary, cost-sensitive prefetching with PA
and SAMlight offers more consistent performance than the
baseline algorithms. In particular, PA and SAMlight never in-
crease cost. SAMlight offers the largest cost reductions over-
all.

5 Related Work
Several models of user search behavior have been proposed in
previous research efforts. We introduced a generalization and
formalization of search activities, and focused on modeling
the dynamics of search sessions. Section 2 describes in detail
the relationship between the general SAM model and several
previous models.

Recent work in query analysis (e.g.,

[Silverstein et al.,
1998; Broder, 2002; Beitzel et al., 2004]) has focused on
measuring and characterizing the global query distribution
(to determine, for example, the average number of words
per query, or the proportion of queries having a navigational
rather than informational purpose). While we compute global
statistics as well, our focus is on modeling the dynamics
of search sessions, including sequences of multiple queries
along with browsing actions.

In search personalization, models of user interest are con-
structed by analyzing the content a user retrieves while
searching and browsing, along with other data [Shen et al.,
2005; Teevan et al., 2005]. These models are then utilized
to provide search results tailored to the user’s interests. This
work is distinct from ours in that it focuses on modeling users,
rather than search activity, and is aimed at a speciﬁc applica-
tion. However, the content-focused user models employed in
search personalization could be used to augment the behav-
ioral user variables listed in Table 3, potentially improving the
predictive accuracy of our models. This is an item of future
work.

Lau and Horvitz [1999] developed a dynamic model of
search behavior but concentrated exclusively on query-to-
query transitions. As in our work, they found that the tem-
poral duration between queries was informative. Our work
builds on this by considering additional searching and brows-
ing events, and by providing experiments characterizing pre-
dictive accuracy.

Recently, search user interaction data has been applied to
improve search engine performance, as discussed in Section

Figure 4: Prefetching performance. SAMlight and PA of-
fer a smooth tradeoff between bandwidth (plotted in terms of
extra pages fetched per result click) and latency.

of both bandwidth and latency vary linearly with the amount
of data accessed. Let α denote the bandwidth cost of load-
ing one page, and let β denote the latency cost of loading
the same data. Let pi be the probability that a particular re-
sult page i is requested by the user. The expected increase in
bandwidth cost due to prefetching page i is then α(1 − pi),
and the expected reduction in latency cost due to prefetch-
ing i is βpi. To minimize expected cost, we prefetch a re-
sult page i iff doing so decreases expected cost, that is when
β/α > (1 − pi)/pi.
Our models are used to predict the click probability pi for
each result page, and we measure performance as the ratio
between α and β varies. We test each of the prefetching algo-
rithms using search and click-through events from the test set,
making the simplifying assumption that each page requires
four seconds to retrieve, and that pages are prefetched in par-
allel.

The tradeoff between bandwidth (in terms of extra pages
downloaded) and latency is shown in Figure 4. We compare
SAMlight and the previous action (PA) model with a base-
line algorithm (First Result, indicated with a box in Figure
4) which always fetches the top result (a feature currently
available for Google search results in Firefox and Mozilla
browsers). The graph shows that substantial latency reduc-
tions are possible beyond fetching the ﬁrst result for both the
PA and SAMlight models. The models allow a smooth trade-
off between bandwidth and latency, and using SAMlight pro-
vides a somewhat better tradeoff. At a latency reduction of
70%, SAMlight uses 20% less extra bandwidth than the PA
model.

The probabilistic models estimate a prefetching policy that
minimizes expected cost, given preferences in latency and
bandwidth. For different preference values, the probabilis-
tic models can offer cost reductions over algorithms that are
not cost-sensitive, like the First Result baseline from Figure
4. In Table 7, we show the cost performance of our models
as β/α varies (setting α + β = 1). We compare against the
First Result baseline as well as a second baseline (Prefetch
All) which prefetches all results, ignoring costs. The table

IJCAI-07

2746

4. Our work is complementary to this work. Instead of focus-
ing on a particular application (such as query expansion), we
explore the space of search activity models and the general
problem of predicting a user’s next action.

6 Summary and Conclusions
We investigated the application of machine learning to logs
of Web search activity to build predictive models of users
searching for information. We introduced an expressive lan-
guage for search activity models, and used it to character-
ize several prior studies. Using a large dataset of search
activity, we experimented with building predictive models,
and presented analyses showing how different attributes of
users, queries, sessions, and temporal delay inﬂuence predic-
tive performance. Then, we reviewed several applications of
search activity models, and presented an experimental inves-
tigation into the use of the predictive models to prefetch con-
tent.

We foresee an acceleration of research that couples ma-
chine learning with large-scale behavioral data to better un-
derstand and support human information-seeking behavior.
As part of efforts in this realm, we believe that developing
expressive languages for representing, modeling, and com-
municating about searching and browsing behavior will be
valuable for both constructing predictive models and for en-
hancing ongoing research and collaboration.

Acknowledgments
The ﬁrst author performed this research during an internship
at Microsoft Research and was supported by a Microsoft Re-
search Fellowship sponsored by Microsoft Live Labs. We
thank Eugene Agichtein, Andy Edmonds, Max Chickering,
and Robert Ragno for providing helpful components and ad-
vice for processing and analyzing user activity logs.

References
[Agichtein et al., 2006] E. Agichtein, E. Brill, S. Dumais,
and R. Ragno. Learning user interaction models for pre-
dicting web search result preferences. In Proc. of SIGIR,
2006.

[Bates, 1989] M. J. Bates. The design of browsing and
berrypicking techniques for the online search interface.
Online Review, 13(5), 1989.

[Beeferman and Berger, 2000] D. Beeferman and A. Berger.
Agglomerative clustering of a search engine query log. In
Proc. of KDD, 2000.

[Beitzel et al., 2004] S. M. Beitzel, E. C. Jensen, A. Chowd-
hury, D. Grossman, and O. Frieder. Hourly analysis of a
very large topically categorized web query log. In Proc. of
SIGIR, 2004.

[Broder, 2002] A. Broder. A taxonomy of web search. SIGIR

Forum, 36(2), 2002.

[Chevalier et al., 2003] K. Chevalier, C. Bothorel,

and
V. Corruble. Discovering rich navigation patterns on a web
site. In Discovery Science, 2003.

[Chickering, 2002] D. M. Chickering. The winmine toolkit.
Technical Report MSR-TR-2002-103, Microsoft, Red-
mond, WA, 2002.

[Cui et al., 2002] H. Cui, J. Wen, J. Nie, and W. Ma. Prob-
In Proc. of

abilistic query expansion using query logs.
WWW, 2002.

[Davison et al., 2003] B. D. Davison, D. G. Deschenes, and
D. B. Lewanda. Finding relevant website queries. In Proc.
of WWW, 2003.

[Duame and Brill, 2004] H. Duame and E. Brill. Web search
In

intent induction via automatic query reformulation.
Proc. of HLT, 2004.

[Fox et al., 2005] S. Fox, K. Karnawat, M. Mydland, S. Du-
mais, and T. White. Evaluating implicit measures to im-
prove web search. ACM Trans. Inf. Syst., 23(2), 2005.

[Horvitz, 1998] E. Horvitz. Continual computation policies

for utility-directed prefetching. In Proc. of CIKM, 1998.

[Jones and Fain, 2003] R. Jones and D. C. Fain. Query word

deletion prediction. In Proc. of SIGIR, 2003.

[Jones et al., 2006] R. Jones, B. Rey, O. Madani, and
In Proc. of

W. Greiner. Generating query substitutions.
WWW, 2006.

[Lau and Horvitz, 1999] T. Lau and E. Horvitz. Patterns of
search: Analyzing and modeling web query reﬁnement. In
Proc. of UM, 1999.

[Lee et al., 2005] U. Lee, Z. Liu, and J. Cho. Automatic
In Proc. of

identiﬁcation of user goals in web search.
WWW, 2005.

[Radlinski and Joachims, 2005] F.

and
T. Joachims. Query chains: Learning to rank from
implicit feedback. In Proc. of KDD, 2005.

Radlinski

[Rose and Levinson, 2004] D. E. Rose and D. Levinson. Un-
derstanding user goals in web search. In Proc. of WWW,
2004.

[Shen et al., 2005] X. Shen, B. Tan, and C. Zhai.

user modeling for personalized search.
2005.

Implicit
In CIKM 2005,

[Silverstein et al., 1998] C. Silverstein, M. Henzinger,
H. Marais, and M. Moricz. Analysis of a very large
altavista query log. Technical Report 1998-014, Digital
SRC, 1998.

[Sun et al., 2005] J. Sun, H. Zeng, H. Liu, Y. Lu, and
Z. Chen. Cubesvd: a novel approach to personalized web
search. In Proc. of WWW, 2005.

[Teevan et al., 2005] J. Teevan, S. T. Dumais, and E. Horvitz.
Personalizing search via automated analysis of interests
and activities. In Proc. of SIGIR, 2005.

IJCAI-07

2747


Semi-supervised Learning for Multi-component Data Classiﬁcation

Akinori Fujino, Naonori Ueda, and Kazumi Saito

NTT Communication Science Laboratories

NTT Corporation

2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0237

{a.fujino,ueda,saito}@cslab.kecl.ntt.co.jp

Abstract

This paper presents a method for designing a semi-
supervised classiﬁer for multi-component data such
as web pages consisting of text and link informa-
tion. The proposed method is based on a hybrid
of generative and discriminative approaches to take
advantage of both approaches. With our hybrid ap-
proach, for each component, we consider an indi-
vidual generative model trained on labeled samples
and a model introduced to reduce the effect of the
bias that results when there are few labeled sam-
ples. Then, we construct a hybrid classiﬁer by com-
bining all the models based on the maximum en-
tropy principle. In our experimental results using
three test collections such as web pages and techni-
cal papers, we conﬁrmed that our hybrid approach
was effective in improving the generalization per-
formance of multi-component data classiﬁcation.

1 Introduction
Data samples such as web pages and multimodal data usually
contain main and additional information. For example, web
pages consist of main text and additional components such
as hyperlinks and anchor text. Although the main content
plays an important role when designing a classiﬁer, additional
content may contain substantial information for classiﬁcation.
Recently, classiﬁers have been developed that deal with multi-
component data such as web pages [Chakrabarti et al., 1998;
Cohn and Hofmann, 2001; Sun et al., 2002; Lu and Getoor,
2003], technical papers containing text and citations [Cohn
and Hofmann, 2001; Lu and Getoor, 2003], and music data
with text titles [Brochu and Freitas, 2003].

In supervised learning cases, existing probabilistic ap-
proaches to classiﬁer design for arbitrary multi-component
data are generative, discriminative, and a hybrid of the
two. Generative classiﬁers learn the joint probability model,
p(x, y), of feature vector x and class label y, compute
P (y|x) by using the Bayes rule, and then take the most prob-
able class label y. To deal with multiple heterogeneous com-
ponents, under the assumption of the class conditional inde-
pendence of all components, the class conditional probabil-
ity density p(xj|y) for the jth component xj
is individually
modeled [Brochu and Freitas, 2003]. By contrast, discrim-
inative classiﬁers directly model class posterior probability

P (y|x) and learn mapping from x to y. Multinomial logistic
regression [Hastie et al., 2001] can be used for this purpose.
It has been shown that discriminative classiﬁers often
achieve better performance than generative classiﬁers, but
that the latter often provide better generalization performance
than the former when trained by few labeled samples [Ng and
Jordan, 2002]. Therefore, hybrid classiﬁers have been pro-
posed to take advantage of the generative and discriminative
approaches [Raina et al., 2004]. To construct hybrid classi-
ﬁers, the generative model p(xj|y) of the jth component is
ﬁrst designed individually, and all the component models are
combined with weight determined on the basis of a discrim-
inative approach. It has been shown experimentally that the
hybrid classiﬁer performed better than pure generative and
discriminative classiﬁers [Raina et al., 2004].

On the other hand, a large number of labeled samples are
often required if we wish to obtain better classiﬁers with gen-
eralization ability. However, in practice it is often fairly ex-
pensive to collect many labeled samples, because class labels
are manually assigned by experienced analysts. In contrast,
unlabeled samples can be easily collected. Therefore, effec-
tively utilizing unlabeled samples to improve the generaliza-
tion performance of classiﬁers is a major research issue in
the ﬁeld of machine learning, and semi-supervised learning
algorithms that use both labeled and unlabeled samples for
training classiﬁers have been developed (cf. [Joachims, 1999;
Nigam et al., 2000; Grandvalet and Bengio, 2005; Fujino et
al., 2005b], see [Seeger, 2001] for a comprehensive survey).
In this paper, we focus on Semi-Supervised Learning for
Multi-Component data classiﬁcation (SSL-MC) based on
probabilistic approach and present a hybrid classiﬁer for SSL-
MC problems. The hybrid classiﬁer is constructed by ex-
tending our previous work for semi-supervised learning [Fu-
jino et al., 2005b] to deal with multiple components. In our
formulation, an individual generative model for each compo-
nent is designed and trained on labeled samples. When there
are few labeled samples, the class boundary provided by the
trained generative models is often far from being the most
appropriate one. Namely, the trained generative models of-
ten have a high bias as a result of there being few labeled
samples. To mitigate the effect of the bias on classiﬁcation
performance, for each component, we introduce a bias cor-
rection model. Then, by discriminatively combining these
models based on the maximum entropy principle [Berger et

IJCAI-07

2754

al., 1996], we obtain our hybrid classiﬁer. The bias correc-
tion models are trained by using many unlabeled samples to
incorporate global data distribution into the classiﬁer design.
We can consider some straightforward applications of the
conventional generative and discriminative semi-supervised
learning algorithms [Nigam et al., 2000; Grandvalet and Ben-
gio, 2005] to train the above-mentioned classiﬁers for multi-
component data. Using three test collections such as web
pages and technical papers, we show experimentally that our
proposed method is effective in improving the generaliza-
tion performance of multi-component data classiﬁcation es-
pecially when the generative and discriminative classiﬁers
provide similar performance.

2 Straightforward Approaches to

Semi-supervised Learning

n=1 and unlabeled sample set Du = {xm}M

2.1 Multi-component Data Classiﬁcation
In multi-class (K classes) and single-labeled classiﬁcation
problems, one of the K classes y ∈ {1, . . . , k, . . . , K} is
assigned to a feature vector x by a classiﬁer. Here, each
feature vector consists of J separate components as x =
{x1, . . . , xj, . . . , xJ }. In semi-supervised learning settings,
the classiﬁer is trained on both labeled sample set Dl =
{xn, yn}N
m=1.
Usually, M is much greater than N . We require a framework
that will allow us to incorporate unlabeled samples without
class labels y into classiﬁers. First, we consider straight-
forward applications of conventional semi-supervised learn-
ing algorithms [Nigam et al., 2000; Grandvalet and Bengio,
2005] with a view to incorporating labeled and unlabeled
samples into generative, discriminative, and hybrid classiﬁers
proposed for multi-component data.

2.2 Generative Classiﬁers
Generative classiﬁers model a joint probability density
p(x, y). However, such direct modeling is hard for arbi-
trary components that consist of completely different types
of media. To deal with multi-component data in the genera-
tive approach, under the assumption that all components have
class conditional independence, the joint probability model
j=1 p(xj|k; θj
can be expressed as p(x, k; Θ) = P (k)
k)
(cf. [Brochu and Freitas, 2003]), where θj
k is the jth com-
ponent model parameter for the kth class and Θ = {θj
k}j,k
is a set of model parameters. Note that the class conditional
probability model p(xj|k; θj
k) should be selected according
to the features of the jth component. The class posteriors for
all classes are computed according to the Bayes rule, such as

(cid:2)J

(cid:2)J
(cid:2)J

(cid:3)K

P (k)

j=1 p(xj|k; ˆθ

j
k)

k(cid:2)=1 P (k(cid:2))

j=1 p(xj|k(cid:2); ˆθ

, (1)

j
k(cid:2) )

P (y = k|x, ˆΘ) =

j
where ˆΘ = {ˆθ
k}j,k is a parameter estimate set. The class
label y of x is determined as the k that maximizes P (k|x; ˆΘ).
For the semi-supervised learning of the generative classi-
ﬁers, unlabeled samples are dealt with as a missing class la-
bel problem, and are incorporated in a mixture of joint prob-
ability models [Nigam et al., 2000]. That is, xm ∈ Du is

(cid:3)K

drawn from the marginal generative distribution p(x; Θ) =
k=1 p(x, k; Θ). Model parameter set Θ is computed by
maximizing the posterior p(Θ|D) (MAP estimation). Ac-
cording to the Bayes rule, p(Θ|D) ∝ p(D|Θ)p(Θ), we can
provide the objective function of model parameter estimation
such as

J(cid:5)

N(cid:4)
M(cid:4)

n=1

F (Θ) =

p(xj

n|yn; θj

yn )

log P (yn)

K(cid:4)

j=1

J(cid:5)

+

log

P (k)

m=1

k=1

j=1

p(xj

m|k; θj
k)

+ log p(Θ).

(2)
Here, p(Θ) is a prior over Θ. We can obtain the Θ value that
maximizes F (Θ) using the Expectation-Maximization (EM)
algorithm [Dempster et al., 1977].

The estimation of Θ is affected by the number of unlabeled
samples used with labeled samples.
In other words, when
N (cid:4) M , model parameter Θ is estimated as almost unsuper-
vised clustering. Then, training the model by using unlabeled
samples might not be useful in terms of classiﬁcation accu-
racy if the mixture model assumptions are not true for actual
classiﬁcation tasks. To mitigate the problem, a weighting pa-
rameter λ ∈ [0, 1] that reduces the contribution of the un-
labeled samples to the parameter estimation was introduced
(EM-λ [Nigam et al., 2000]). The value of λ is determined
by cross-validation so that the leave-one-out labeled samples
are, as far as possible, correctly classiﬁed.

2.3 Discriminative Classiﬁers
Discriminative classiﬁers directly model class posterior prob-
abilities P (y|x) for all classes. We can design a discrimina-
tive model P (y|x) without the separation of components. In
Multinomial Logistic Regression (MLR) [Hastie et al., 2001],
the class posterior probabilities are modeled as
exp(wk · x)
k(cid:2)=1 exp(wk(cid:2) · x)

P (y = k|x; W ) =

(cid:3)K

(3)

,

where W = {w1, . . . , wk, . . . , wK} is a set of unknown
model parameters. wk · x represents the inner product of wk
and x.

Certain assumptions are required if we are to incorpo-
rate unlabeled samples into discriminative classiﬁers, because
p(x) is not modeled in the classiﬁers. A Minimum Entropy
Regularizer (MER) was introduced as one approach to the
semi-supervised learning of discriminative classiﬁers [Grand-
valet and Bengio, 2005]. This approach is based on the empir-
ical knowledge that classes should be well separated to take
advantage of unlabeled samples because the asymptotic in-
formation content of unlabeled samples decreases as classes
overlap. The conditional entropy is used as a measure of class
overlap. By minimizing the conditional entropy, the classiﬁer
is trained to separate unlabeled samples as well as possible.

Applying MER to MLR, we estimate W to maximize the

following conditional log-likelihood with the regularizer:

N(cid:4)

F (W ) =

n=1

log P (yn|xn; W )

IJCAI-07

2755

M(cid:4)

K(cid:4)

+ λ

m=1

k=1
+ log p(W ).

P (k|xm; W ) log P (k|xm; W )

objective function for Θ estimation as

(cid:6)

N(cid:4)

J(cid:4)

F1(Θ) =

(4)

j=1

n=1

log p(xj

n|yn; θj

yn) +

K(cid:4)

k=1

(cid:7)

log p(θj
k)

.

(6)

Here, p(θj

k) is a prior over θj
k.

When there are few labeled samples, the classiﬁer obtained
by using the trained component generative models often pro-
vides a class boundary that is far from the most appropriate
one. Namely, the trained component generative models often
have a high bias. To obtain a classiﬁer with a smaller bias, we
newly introduce another class conditional generative model
per component, called bias correction model. The generative
and bias correction models for each component belong to the
same model family, but the set of parameters Ψ = {ψj
k}j,k
of the bias correction models is different from Θ. We con-
struct our hybrid classiﬁer by combining the trained com-
ponent generative models with the bias correction models to
mitigate the effect of the bias.

3.2 Discriminative Class Posterior Design
We deﬁne our hybrid classiﬁer by using the class posterior
probability distribution derived from a discriminative combi-
nation of the component generative and bias correction mod-
els. The combination is provided based on the Maximum En-
tropy (ME) principle [Berger et al., 1996].

The ME principle is a framework for obtaining a proba-
bility distribution, which prefers the most uniform models
that satisfy any given constraints. Let R(k|x) be a target
distribution that we wish to specify using the ME princi-
ple. A constraint is that the expectation of log-likelihood,
j
log p(xj|k; ˆθ
k), with respect to the target distribution R(k|x)
is equal to the expectation of the log-likelihood with respect
n=1 δ(x − xn, k −
to the empirical distribution ˜p(x, k) =
yn)/N of the training samples as

(cid:3)N

˜p(x, k) log p(xj|k; ˆθ

j
k)

=

˜p(x)R(k|x) log p(xj|k; ˆθ

j
k), ∀j,

(7)

n=1 δ(x − xn)/N is the empirical distribu-
where ˜p(x) =
tion of x. The equation of the constraint for log p(xj|k; ψj
k)
can be represented in the same form as Eq. (7). We also re-
strict R(k|x) so that it has the same class probability as seen
in the labeled samples, such that

(cid:4)

(cid:4)

˜p(x, k) =

x

x

˜p(x)R(k|x), ∀k.

(8)

By maximizing the
−

=
x,k ˜p(x)R(k|x) log R(k|x) under these constraints, we

entropy H(R)

conditional

(cid:3)

can obtain the target distribution:

R(y = k|x; ˆΘ, Ψ, Γ)

(cid:3)K

=

eμk

j=1 p(xj|k; ˆθ

k(cid:2)=1 eμk(cid:2)

j=1 p(xj|k(cid:2); ˆθ

j

k)γ1j p(xj|k; ψj

k)γ2j
k(cid:2) )γ1j p(xj|k(cid:2); ψj

j

k(cid:2) )γ2j

, (9)

(cid:2)J
(cid:2)J

x,k

(cid:4)
(cid:4)
(cid:3)N

x,k

Here, λ is a weighting parameter, and p(W ) is a prior over
W . A Gaussian prior [Chen and Rosenfeld, 1999] is often
employed as p(W ).

2.4 Hybrid Classiﬁers
Hybrid classiﬁers learn an individual class conditional prob-
ability model p(xj|y; θj
y) for the jth component and directly
model the class posterior probability by using the trained
component models [Raina et al., 2004; Fujino et al., 2005a].
Namely, each component model is estimated on the basis of a
generative approach, while the classiﬁer is constructed on the
basis of a discriminative approach. For the hybrid classiﬁers,
the class posteriors are provided as

(cid:2)J
(cid:2)J

R(y = k|x; ˆΘ, Γ)

(cid:3)K

eμk

k(cid:2)=1 eμk(cid:2)

j=1 p(xj|k; ˆθ

j
k)γj
j=1 p(xj|k(cid:2); ˆθ

=

,

j
k(cid:2) )γj

(5)

where ˆΘ = {ˆθj,k}j,k is a parameter estimate set of
component generative models {p(xj|k; θj
k)}j,k, and Γ =
{{γj}J
k=1} is a parameter set that provides the com-
bination weights of components and class biases. In super-
vised learning cases, Γ is estimated to maximize the cross-
validation conditional log-likelihood of labeled samples.

j=1, {μk}K

To incorporate unlabeled samples in the hybrid classiﬁers,
we can consider a simple approach in which we train the com-
ponent generative models using EM-λ as mentioned in Sec-
tion 2.2. With this approach, we incorporate unlabeled sam-
ples into the component generative models and then discrimi-
natively combine these models. For convenience, we call this
approach Cascade Hybrid (CH).

3 Proposed Method

As mentioned in the introduction, we propose a semi-
supervised hybrid classiﬁer for multi-component data, where
bias correction models are introduced to use unlabeled sam-
ples effectively. For convenience, we call this classiﬁer Hy-
brid classiﬁer with Bias Correction Models (H-BCM classi-
ﬁer). In this section, we present the formulation of the H-
BCM classiﬁer and its parameter estimation method.

3.1 Component Generative Models and Bias

Correction

We ﬁrst design an individual class conditional probability
model (component generative model) p(xj|k; θj
k) for the jth
component xj
of data samples that belong to the kth class,
where Θ = {θj
k}j,k denotes a set of model parameters over
all components and classes.
In our formulation, the com-
ponent generative models are trained by using a set of la-
beled samples, Dl. Θ is computed using MAP estimation:
ˆΘ = maxΘ{log p(Dl|Θ) + log p(Θ)}. Assuming Θ is in-
dependent of class probability P (y = k), we can derive the

IJCAI-07

2756

j=1, {μk}K

where Γ = {{γ1j, γ2j}J
k=1} is a set of Lagrange
multipliers. γ1j and γ2j represent the combination weights
of the generative and bias correction models for the jth com-
ponent, and μk is the bias parameter for the kth class. The
distribution R(k|x, ˆΘ, Ψ, Γ) gives us the formulation of the
discriminative classiﬁer, which consists of the trained com-
ponent generative models and the bias correction models.

According to the ME principle, the solution of Γ in Eq. (9)
is the same as the Γ that maximizes the log-likelihood for
R(k|x; ˆΘ, Ψ, Γ) of labeled samples (xn, yn) ∈ Dl [Berger
et al., 1996; Nigam et al., 1999]. However, Dl is also used
to estimate Θ. Using the same labeled samples for both Γ
and Θ may lead to a bias estimation of Γ. Thus, a leave-one-
out cross-validation of the labeled samples is used to estimate
Γ [Raina et al., 2004]. Let ˆΘ(−n)
be a generative model pa-
rameter set estimated by using all the labeled samples except
(xn, yn). The objective function of Γ then becomes

N(cid:4)

n=1

F2(Γ|Ψ) =

log R(yn|xn; ˆΘ

(−n)

, Ψ, Γ) + log p(Γ), (10)

(cid:2)

(cid:2)

k exp(−μ2

where p(Γ) is a prior over Γ. We used the Gaus-
[Chen and Rosenfeld, 1999] as p(Γ) ∝
sian prior
k/2ρ2)
l,j exp{−(γlj −1)2/2σ2}. Global con-
vergence is guaranteed when Γ is estimated with ﬁxed ˆΘ(−n)
and Ψ, since F2(Γ|Ψ) is an upper convex function of Γ.

3.3 Training Bias Correction Models
In our formulation, the parameter set Ψ of the bias correction
models is trained with unlabeled samples to reduce the bias
that results when there are few labeled samples. According
to R(k|x; ˆΘ, Ψ, Γ) as shown in Eq. (9), the class label y of
a feature vector x is determined as the k that maximizes the
discriminative function:

gk(x; Ψ) = eμk

p(xj|k; ˆθ

j

k)γ1 p(xj|k; ψj

k)γ2 . (11)

J(cid:5)

j=1

Here, when the values of gk(x; Ψ) for all classes are small,
the classiﬁcation result for x is not reliable, because gk(x; Ψ)
is almost the same for all classes. Thus, we expect our clas-
siﬁer to provide a large gk(x; Ψ) difference between classes
for unseen samples, by estimating Ψ that maximizes the sum
of the discriminative function of unlabeled samples:

M(cid:4)

K(cid:4)

F3(Ψ|Γ) =

log

gk(xm; Ψ) + log p(Ψ),

(12)

m=1

k=1

where p(Ψ) is a prior over Ψ. We incorporate unlabeled
samples into our hybrid classiﬁer directly by maximizing
F3(Ψ|Γ) in the same way as for a mixture model in genera-
tive approaches, where joint probability models are regarded
as discriminative functions.

If Γ is known, we can estimate the Ψ that provides the lo-
cal maximum of F3(Ψ|Γ) around the initialized value of Ψ,
with the help of the EM algorithm [Dempster et al., 1977].
However, Γ is not a known value but an unknown parameter
that should be estimated using Eq. (10) with Θ and Ψ ﬁxed.
Therefore, we estimate Ψ and Γ iteratively and alternatively.
Ψ and Γ are updated until some convergence criterion is met.

4 Experiments

4.1 Test Collections
Empirical evaluations were performed on three test collec-
tions: WebKB 1, Cora 2, and 20newsgroups (20news) 3
datasets, which have often been used as benchmark tests for
classiﬁers in text classiﬁcation tasks [Nigam et al., 1999].

WebKB contains web pages from universities. This dataset
consists of seven categories, and each page belongs to one
of them. Following the setup in [Nigam et al., 1999], we
used only four categories: course, faculty, project, and stu-
dent. The categories contained 4199 pages. We extracted
four components, Main Text (MT), Out-Links (OL), In-Links
(IL), and Anchor-Text (AT), from each page. Here, MT is the
text description except for tags and links. The OL for a page
consists of web page URLs linked by the page. The IL for
a page is the set of web page URLs linking the page. AT
is the anchor text set for each page, which consists of text
descriptions expressing the link to the page found on other
web pages. We collected IL and AT from the links within the
dataset. For the MT and AT components, we removed stop
words and vocabulary words included in only one web page.
We also removed URLs included in only one page for the OL
and IL components. There were 18525 and 496 vocabulary
words in the MT and AT of the dataset, respectively. OL and
IL contained 4131 and 500 different URLs, respectively.

Cora contains more than 30000 summaries of technical pa-
pers, and each paper belongs to one of 70 groups. For our
evaluation, we used 4240 papers included in 7 groups: /Ar-
tiﬁcial Intelligence/Machine Learning/*. We extracted three
components, Main Text (MT), authors (AU), and citations
(CI) from each paper. Here, MT consists of the text distri-
bution included in the papers, and AU is the set of authors.
CI consists of citations to other papers. We removed vocab-
ulary words, authors, and cited papers for each component
in the same way as for WebKB. There were 9190 vocabulary
words, 1495 authors, and 13282 cited papers in the dataset.

20news consists of 20 different UseNet discussion groups.
Each article belongs to one of the 20 groups. The test col-
lection has been used to evaluate a supervised hybrid classi-
ﬁer for multi-component data [Raina et al., 2004]. Following
the setup in [Nigam et al., 1999], we used only ﬁve groups:
comp.*. There were 4881 articles in the groups. We extracted
two components, Main (M) and Title (T), from each article,
where T is the text description following “Subject:” and M
is the main content of each article except for the title. We
removed stop words and vocabulary words included in only
one article. There were 19273 and 1775 vocabulary words in
components M and T of the dataset, respectively.

4.2 Experimental Settings
We used a naive Bayes (NB) model [Nigam et al., 2000] as a
generative model and a bias correction model for each com-
ponent, assuming that different features (works, links, cita-

1http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-

20/www/data/webkb-data.gtar.gz

2http://www.cs.umass.edu/˜mccallum/data/cora-classify.tar.gz
3http://people.csail.mit.edu/jrennie/20Newsgroups/20news-

18828.tar.gz

IJCAI-07

2757

tions, or authors) included in the component are independent.
xj
was provided as the feature-frequency vector of the jth
component. For the MAP estimation in Eqs (2), (6), and (12),
we used Dirichlet priors as the priors over NB model param-
eters.

For our experiments, we randomly selected labeled, un-
labeled, and test samples from each dataset. We made ten
different evaluation sets for each dataset by this random se-
lection. 1000 data samples from each dataset were used as the
test samples in each experiment. 2500, 2000, and 2500 un-
labeled samples were used with labeled samples for training
these classiﬁers in WebKB, Cora, and 20news, respectively.
The average classiﬁcation accuracy over the ten evaluation
sets was used to evaluate the methods.

We compared our H-BCM classiﬁer with three semi-
supervised classiﬁers for multi-component data as mentioned
in Section 2: an NB based classiﬁer with EM-λ [Nigam et
al., 2000], an NLR classiﬁer with MER (MLR/MER) [Grand-
valet and Bengio, 2005], and a CH classiﬁer. We also exam-
ined three supervised classiﬁers: NB, MLR, and hybrid (HY)
classiﬁers. NB, MLR, and HY classiﬁers were trained only
on labeled samples.

In our experiments, for EM-λ, the value of weighting pa-
rameter λ was set in the manner mentioned in Section 2.2
Note that in our experiments we selected the value from four-
teen candidate values of {0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4,
0.5, 0.6, 0.7, 0.75, 0.8, 0.9, 1} to save computational time, but
we carefully selected these candidate values via preliminary
experiments.

For MLR/MER, the value of weighting parameter λ in
Eq. (4) was selected from sixteen candidate values of {{0.1 ×
10−n, 0.2 × 10−n, 0.5 × 10−n}4
n=0, 1} that were carefully
selected via the preliminary experiments. For a fair compar-
ison of the methods, the value of λ should be determined us-
ing training samples, for example, using a cross-validation
of labeled samples [Grandvalet and Bengio, 2005]. We de-
termined the value of λ that gave the best classiﬁcation per-
formance for test samples to examine the potential ability of
MLR/MER because the computation cost of tuning λ was
very high.

4.3 Results and Discussion

Table 1 shows the average classiﬁcation accuracies over the
ten different evaluation sets for WebKB, Cora, and 20news
for different numbers of labeled samples. Each number in
parentheses in the table denotes the standard deviation of the
ten evaluation sets. |Dl| (|Du|) represents the number of la-
beled (unlabeled) samples.

As reported in [Raina et al., 2004; Fujino et al., 2005a], in
supervised cases, the hybrid classiﬁer was useful for achiev-
ing better generalization performance for multi-component
data classiﬁcation. In our experiments, the average classiﬁ-
cation accuracy of HY was similar to or better than that of
NB and MLR.

We examined EM-λ, MLR/MER, CH, and H-BCM classi-
ﬁers for semi-supervised cases. The H-BCM classiﬁer out-
performed the other semi-supervised classiﬁers except when
the generative classiﬁer performed very differently from the
discriminative classiﬁer for WebKB. We conﬁrmed experi-

mentally that the H-BCM classiﬁer was useful for improving
the generalization performance of multi-component data clas-
siﬁcation with both labeled and unlabeled samples.

More speciﬁcally, the H-BCM classiﬁer outperformed the
MLR/MER classiﬁer except when there were many labeled
samples for WebKB. This result is because MLR/MER tends
to be overﬁtted to few labeled samples. In contrast, H-BCM
inherently has the characteristic of the generative models,
whereby such an overﬁtting problem is mitigated. When
many labeled samples are available such that the overﬁtting
problem can be solved, it would be natural for the discrimi-
native classiﬁer to be better than the H-BCM classiﬁer.

The classiﬁcation performance with H-BCM was better
that with EM-λ except when there were few labeled samples
for WebKB. It is known that discriminative approaches often
provide better classiﬁcation performance than generative ap-
proaches when there are many labeled samples [Ng and Jor-
dan, 2002]. Our experimental result indicates that there might
be an intrinsic limitation preventing EM-λ from achieving a
high level of performance because only weighting parameter
λ is trained discriminatively.

H-BCM provided better classiﬁcation performance than
CH except when there were few labeled samples for WebKB.
The H-BCM and CH classiﬁers are constructed based on a
hybrid of generative and discriminative approaches, but CH
differs from H-BCM, in that the former does not contain bias
correction models. This experimental result indicates that in-
troducing bias correction models was effective in incorporat-
ing unlabeled samples into the hybrid classiﬁer and thus im-
proves its generalization ability.

5 Conclusion
We proposed a semi-supervised classiﬁer design method for
multi-component data, based on a hybrid generative and dis-
criminative approach, called Hybrid classiﬁer with Bias Cor-
rection Models (H-BCM classiﬁer). The main idea is to de-
sign an individual generative model for each component and
to introduce models to reduce the bias that results when there
are few labeled samples. For evaluation, we also consid-
ered straightforward applications of conventional generative
and discriminative semi-supervised learning algorithms to the
classiﬁers proposed for multi-component data. We conﬁrmed
experimentally that H-BCM was more effective in improv-
ing the generalization performance of multi-component data
classiﬁcation than the straightforward approaches. Our exper-
imental results using three test collections suggest that the H-
BCM classiﬁer is useful especially when the generative and
discriminative classiﬁers provide similar performance. Fu-
ture work will involve applying H-BCM to multimodal data
in which different generative models are employed.

References
[Berger et al., 1996] A. L. Berger, S. A. Della Pietra, and V. J.
Della Pietra. A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39–71, 1996.

[Brochu and Freitas, 2003] E. Brochu and N. Freitas. “name that
song!”: A probabilistic approach to querying on music and text.
In Advances in Neural Information Processing Systems 15, pages
1505–1512. MIT Press, Cambridge, MA, 2003.

IJCAI-07

2758

Table 1: Classiﬁcation accuracies (%) with classiﬁers trained on various numbers of labeled samples.

(a) WebKB (|Du| = 2500, K = 4)

Training set

|Dl|/|Du| H-BCM

Semi-supervised
CH

69.9 (5.1) 73.2 (3.7) 73.5 (3.1)
76.2 (2.8) 77.1 (2.5) 76.0 (2.5)
81.8 (1.7) 79.7 (2.2) 77.7 (2.4)
85.0 (1.3) 83.3 (1.1) 80.6 (1.7)
87.2 (1.1) 85.0 (1.1) 82.7 (1.2)

EM-λ MLR/MER
63.5 (5.0)
72.2 (2.7)
78.5 (2.2)
84.3 (1.7)
88.7 (1.3)

Supervised

HY

NB

MLR

68.6 (3.7) 68.8 (3.3) 63.3 (4.5)
74.8 (1.1) 73.4 (1.4) 71.9 (2.2)
80.7 (1.9) 77.5 (2.0) 78.3 (2.0)
84.5 (1.3) 80.9 (1.5) 83.9 (1.7)
87.4 (1.1) 83.5 (1.4) 87.9 (1.3)

0.0128
0.0256
0.0512
0.1024
0.2048

Training set

|Dl|/|Du| H-BCM

(b) Cora (|Du| = 2000, K = 7)
Semi-supervised
CH

76.4 (3.6) 74.1 (5.0) 71.6 (4.2)
83.0 (2.1) 80.7 (1.2) 77.9 (1.6)
85.7 (1.2) 83.6 (0.9) 81.6 (1.1)
87.4 (1.3) 85.6 (1.4) 83.8 (1.0)
89.1 (0.7) 88.1 (1.1) 86.8 (1.1)

EM-λ MLR/MER
55.7 (4.2)
63.8 (2.3)
72.9 (1.1)
78.6 (1.5)
82.7 (1.2)

Supervised

HY

NB

MLR

63.2 (3.6) 57.0 (2.9) 55.2 (3.8)
72.1 (3.4) 64.5 (3.2) 63.5 (2.1)
80.4 (1.1) 75.2 (1.5) 72.3 (1.1)
84.4 (1.2) 79.8 (1.2) 77.3 (1.2)
87.9 (1.0) 84.6 (0.8) 82.1 (1.2)

(c) 20news (|Du| = 2500, K = 5)

Training set

|Dl|/|Du| H-BCM

Semi-supervised
CH

64.8 (3.0) 55.5 (6.2) 53.6 (6.6)
71.6 (1.8) 62.5 (4.2) 59.4 (6.4)
75.7 (1.5) 70.1 (2.4) 66.3 (4.6)
78.5 (1.4) 75.3 (1.8) 71.3 (3.2)
82.0 (1.2) 79.4 (1.4) 76.0 (1.5)
85.0 (1.0) 83.4 (1.4) 80.1 (1.8)

EM-λ MLR/MER
50.2 (5.3)
57.2 (3.3)
65.3 (3.2)
72.3 (1.2)
77.7 (1.1)
81.0 (1.2)

Supervised

HY

NB

MLR

47.7 (3.7) 45.6 (3.6) 48.8 (3.9)
56.3 (2.2) 51.8 (3.6) 56.4 (3.0)
65.4 (1.4) 60.1 (3.1) 63.7 (2.3)
72.8 (1.8) 67.8 (2.4) 71.0 (1.0)
79.6 (1.2) 74.9 (1.6) 76.4 (0.9)
83.4 (0.9) 78.8 (1.5) 80.2 (0.9)

|Dl|
32
64
128
256
512

|Dl|
56
112
224
448
896

|Dl|
40
80
160
320
640
1280

0.028
0.056
0.112
0.224
0.448

0.016
0.032
0.064
0.128
0.256
0.512

[Chakrabarti et al., 1998] S. Chakrabarti, B. Dom, and P. Indyk.
Enhanced hypertext categorization using hyperlinks. In Proceed-
ings of ACM International Conference on Management of Data
(SIGMOD-98), pages 307–318, 1998.

[Joachims, 1999] T. Joachims. Transductive inference for text clas-
siﬁcation using support vector machines. In Proceedings of the
16th International Conference on Machine Learning (ICML-99),
pages 200–209, 1999.

[Chen and Rosenfeld, 1999] S. F. Chen and R. Rosenfeld. A Gaus-
sian prior for smoothing maximum entropy models. Technical
report, Carnegie Mellon University, 1999.

[Cohn and Hofmann, 2001] D. Cohn and T. Hofmann. The missing
link - a probabilistic model of document content and hypertext
connectivity. In Advances in Neural Information Processing Sys-
tems 13, pages 430–436. MIT Press, Cambridge, MA, 2001.

[Dempster et al., 1977] A. P. Dempster, N. M. Laird, and D. B. Ru-
bin. Maximum likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Series B, 39:1–38,
1977.

[Fujino et al., 2005a] A. Fujino, N. Ueda, and K. Saito. A classi-
ﬁer design based on combining multiple components by max-
imum entropy principle.
In Information Retrieval Technology
(AIRS2005 Proceedings), LNCS, volume 3689, pages 423–438.
Springer-Verlag, Berlin, Heidelberg, 2005.

[Fujino et al., 2005b] A. Fujino, N. Ueda, and K. Saito. A hybrid
generative/discriminative approach to semi-supervised classiﬁer
design. In Proceedings of the 20th National Conference on Arti-
ﬁcial Intelligence (AAAI-05), pages 764–769, 2005.

[Grandvalet and Bengio, 2005] Y. Grandvalet and Y. Bengio. Semi-
supervised learning by entropy minimization.
In Advances in
Neural Information Processing Systems 17, pages 529–536. MIT
Press, Cambridge, MA, 2005.

[Hastie et al., 2001] T. Hastie, R. Tibshirani, and J. Friedman. The
Elements of Statistical Learning: Data Mining, Inference, and
Prediction. Springer-Verlag, New York Berlin Heidelberg, 2001.

[Lu and Getoor, 2003] Q. Lu and L. Getoor. Link-based text clas-
siﬁcation. In IJCAI Workshop on Text-Mining & Link-Analysis
(TextLink 2003), 2003.

[Ng and Jordan, 2002] A. Y. Ng and M. I. Jordan. On discrimina-
tive vs. generative classiﬁers: A comparison of logistic regression
and naive Bayes. In Advances in Neural Information Processing
Systems 14, pages 841–848. MIT Press, Cambridge, MA, 2002.

[Nigam et al., 1999] K. Nigam, J. Lafferty, and A. McCallum. Us-
ing maximum entropy for text classiﬁcation. In IJCAI-99 Work-
shop on Machine Learning for Information Filtering, pages 61–
67, 1999.

[Nigam et al., 2000] K. Nigam, A. McCallum, S. Thrun, and
T. Mitchell. Text classiﬁcation from labeled and unlabeled docu-
ments using EM. Machine Learning, 39:103–134, 2000.

[Raina et al., 2004] R. Raina, Y. Shen, A. Y. Ng, and A. McCal-
lum. Classiﬁcation with hybrid generative/discriminative mod-
els. In Advances in Neural Information Processing Systems 16.
MIT Press, Cambridge, MA, 2004.

[Seeger, 2001] M. Seeger. Learning with labeled and unlabeled

data. Technical report, University of Edinburgh, 2001.

[Sun et al., 2002] A. Sun, E. P. Lim, and W. K. Ng. Web classiﬁ-
cation using support vector machine. In Proceedings of 4th Int.
Workshop on Web Information and Data Management (WIDM
2002) held in conj. with CIKM 2002, pages 96–99, 2002.

IJCAI-07

2759


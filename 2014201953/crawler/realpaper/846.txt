Case Base Mining for Adaptation Knowledge Acquisition

M. d’Aquin1, 2, F. Badra1, S. Lafrogne1,
J. Lieber1, A. Napoli1, L. Szathmary1

1 LORIA (CNRS, INRIA, Nancy Universities) BP 239, 54506 Vandœuvre-l`es-Nancy, France,

{daquin, badra, lafrogne, lieber, napoli, szathmar}@loria.fr

2 Knowledge Media Institute (KMi), the Open University, Milton Keynes, United Kingdom,

m.daquin@open.ac.uk

Abstract

In case-based reasoning, the adaptation of a source
case in order to solve the target problem is at the
same time crucial and difﬁcult to implement. The
reason for this difﬁculty is that, in general, adapta-
tion strongly depends on domain-dependent knowl-
edge. This fact motivates research on adaptation
knowledge acquisition (AKA). This paper presents
an approach to AKA based on the principles and
techniques of knowledge discovery from databases
and data-mining. It is implemented in CABAMA-
KA, a system that explores the variations within the
case base to elicit adaptation knowledge. This sys-
tem has been successfully tested in an application
of case-based reasoning to decision support in the
domain of breast cancer treatment.

Introduction

1
Case-based reasoning (CBR [Riesbeck and Schank, 1989])
aims at solving a target problem thanks to a case base. A
case represents a previously solved problem and may be seen
as a pair (problem, solution). A CBR system selects a case
from the case base and then adapts the associated solution,
requiring domain-dependent knowledge for adaptation. The
goal of adaptation knowledge acquisition (AKA) is to detect
and extract this knowledge. This is the function of the semi-
automatic system CABAMAKA, which applies principles of
knowledge discovery from databases (KDD) to AKA, in par-
ticular frequent itemset extraction. This paper presents the
system CABAMAKA: its principles, its implementation and
an example of adaptation rule discovered in the framework
of an application to breast cancer treatment. The original-
ity of CABAMAKA lies essentially in the approach of AKA
that uses a powerful learning technique that is guided by a
domain expert, according to the spirit of KDD. This paper
proposes an original and working approach to AKA, based
on KDD techniques.
In addition, the KDD process is per-
formed on a knowledge base itself, leading to the extraction
of meta-knowledge, i.e. knowledge units for manipulating
other knowledge units. This is also one of the rare papers try-
ing to build an effective bridge between knowledge discovery
and case-based reasoning.

The paper is organized as follows. Section 2 presents ba-
sic notions about CBR and adaptation. Section 3 summarizes
researches on AKA. Section 4 describes the system CABA-
MAKA: its main principles, its implementation and examples
of adaptation knowledge acquired from it. Finally, section 5
draws some conclusions and points out future work.

2 CBR and Adaptation
A case in a given CBR application encodes a problem-solving
episode that is represented by a problem statement pb and
an associated solution Sol(pb). The case is denoted by
the pair (pb, Sol(pb)) in the following. Let Problems and
Solutions be the set of problems and the set of solutions
of the application domain, and “is a solution of” be a binary
relation on Problems × Solutions. In general, this rela-
tion is not known in the whole but at least a ﬁnite number of
its instances (pb, Sol(pb)) is known and constitutes the case
base CB. An element of CB is called a source case and is de-
noted by srce-case = (srce, Sol(srce)), where srce is a
source problem. In a particular CBR session, the problem to
be solved is called target problem, denoted by tgt.

A case-based inference associates to tgt a solution
Sol(tgt), with respect to the case base CB and to additional
knowledge bases, in particular O, the domain ontology (also
known as domain theory or domain knowledge) that usually
introduces the concepts and terms used to represent the cases.
It can be noticed that the research work presented in this paper
is based on the assumption that there exists a domain ontol-
ogy associated with the case base, in the spirit of knowledge-
intensive CBR [Aamodt, 1990].

retrieval and adaptation.

A classical decomposition of CBR relies on the
selects
steps of
(srce, Sol(srce)) ∈ CB such that srce is similar to
tgt according to some similarity criterion. The goal of adap-
tation is to solve tgt by modifying Sol(srce) accordingly.
Thus, the proﬁle of the adaptation function is

Retrieval

Adaptation : ((srce, Sol(srce)), tgt) (cid:3)→ Sol(tgt)
The work presented hereafter is based on the follow-
ing model of adaptation, similar to transformational anal-
ogy [Carbonell, 1983]:
 (srce, tgt) (cid:3)→ Δpb, where Δpb encodes the similari-
ties and dissimilarities of the problems srce and tgt.

IJCAI-07

750

 (Δpb, AK)

(cid:3)→ Δsol, where AK is the adaptation
knowledge and where Δsol encodes the similarities
and dissimilarities of Sol(srce) and the forthcoming
Sol(tgt).
 (Sol(srce), Δsol) (cid:3)→ Sol(tgt), Sol(srce) is modi-

ﬁed into Sol(tgt) according to Δsol.

Adaptation is generally supposed to be domain-dependent
in the sense that
it relies on domain-speciﬁc adaptation
knowledge. Therefore, this knowledge has to be acquired.
This is the purpose of adaptation knowledge acquisition
(AKA).

3 Related Work in AKA
The notion of adaptation case is introduced in [Leake et al.,
1996]. The system DIAL is a case-based planner in the do-
main of disaster response planning. Disaster response plan-
ning is the initial strategic planning used to determine how to
assess damage, evacuate victims, etc. in response to natural
and man-made disasters such as earthquakes and chemical
spills. To adapt a case, the DIAL system performs either a
case-based adaptation or a rule-based adaptation. The case-
based adaptation attempts to retrieve an adaptation case de-
scribing the successful adaptation of a similar previous adap-
tation problem. An adaptation case represents an adaptation
as the combination of transformations (e.g. addition, deletion,
substitution) plus memory search for the knowledge needed
to operationalize the transformation (e.g. to ﬁnd what to add
or substitute), thus reifying the principle: adaptation = trans-
formations + memory search. An adaptation case in DIAL
packages information about the context of an adaptation, the
derivation of its solution, and the effort involved in the deriva-
tion process. The context information includes characteristics
of the problem for which adaptation was generated, such as
the type of problem, the value being adapted, and the roles
that value ﬁlls in the response plan. The derivation records
the operations needed to ﬁnd appropriate values in memory,
e.g. operations to extract role-ﬁllers or other information to
guide the memory search process. Finally, the effort records
the actual effort expended to ﬁnd the solution path. It can be
noticed that the core idea of “transformation” is also present
in our own adaptation knowledge extraction.

In [Jarmulak et al., 2001], an approach to AKA is presented
that produces a set of adaptation cases, where an adaptation
case is the representation of a particular adaptation process.
The adaptation case base, CBA, is then used for further adap-
tation steps: an adaptation step itself is based on CBR, reusing
the adaptation cases of CBA. CBA is built as follows. For
each (srce1, Sol(srce1)) ∈ CB, the retrieval step of the CBR
system using the case base CB without (srce1, Sol(srce1))
returns a case (srce2, Sol(srce2)). Then, an adaptation
case is built based on both source cases and is added to CBA.
This adaptation case encodes srce1, Sol(srce1), the dif-
ference between srce1 and srce2 (Δpb, with the notations
of this paper) and the difference between Sol(srce1) and
Sol(srce2) (Δsol). This approach to AKA and CBR has
been successfully tested for an application to the design of
tablet formulation.

The idea of the research presented in [Hanney and Keane,
1996; Hanney, 1997] is to exploit the variations between
source cases to learn adaptation rules. These rules compute
variations on solutions from variations on problems. More
precisely, ordered pairs (srce-case1, srce-case2) of simi-
lar source cases are formed. Then, for each of these pairs, the
variations between the problems srce1 and srce2 and the
solutions Sol(srce1) and Sol(srce2) are represented (Δpb
and Δsol). Finally, the adaptation rules are learned, using
as training set the set of the input-output pairs (Δpb, Δsol).
This approach has been tested in two domains: the estima-
tion of the price of ﬂats and houses, and the prediction of
the rise time of a servo mechanism. The experiments have
shown that the CBR system using the adaptation knowledge
acquired from the automatic system of AKA shows a better
performance compared to the CBR system working without
adaptation. This research has inﬂuenced our work that is
globally based on similar ideas.

[Shiu et al., 2001] proposes a method for case base main-
tenance that reduces the case base to a set of representative
cases together with a set of general adaptation rules. These
rules handle the perturbation between representative cases
and the other ones. They are generated by a fuzzy decision
tree algorithm using the pairs of similar source cases as a
training set.

In [Wiratunga et al., 2002], the idea of [Hanney and Keane,
1996] is reused to extend the approach of [Jarmulak et al.,
2001]: some learning algorithms (in particular, C4.5) are ap-
plied to the adaptation cases of CBA, to induce general adap-
tation knowledge.

These approaches to AKA share the idea of exploiting adap-
tation cases. For some of them ([Jarmulak et al., 2001;
Leake et al., 1996]), the adaptation cases themselves con-
stitute the adaptation knowledge (and adaptation is itself a
CBR process). For the other ones ([Hanney and Keane, 1996;
Shiu et al., 2001; Wiratunga et al., 2002]), as for the approach
presented in this paper, the adaptation cases are the input of a
learning process.
4 CABAMAKA
We now present the CABAMAKA system, for acquiring adap-
tation knowledge. The CABAMAKA system is at present
working in the medical domain of cancer treatment, but it
may be reused in other application domains where there exist
problems to be solved by a CBR system.
4.1 Principles
CABAMAKA deals with case base mining for AKA. Although
the main ideas underlying CABAMAKA are shared with those
presented in [Hanney and Keane, 1996], the followings are
original ones. The adaptation knowledge that is mined has
to be validated by experts and has to be associated with ex-
planations making it understandable by the user. In this way,
CABAMAKA may be considered as a semi-automated (or in-
teractive) learning system. This is a necessary requirement
for the medical domain for which CABAMAKA has been ini-
tially designed.
the system takes into account every or-
dered pair (srce-case1, srce-case2) with srce-case1 (cid:5)=

Moreover,

IJCAI-07

751

srce-case2, leading to examine n(n − 1) pairs of cases for
a case base CB where |CB| = n.
In practice, this number
may be rather large since in the present application n (cid:6) 650
(n(n − 1) (cid:6) 4 · 105). This is one reason for choosing for this
system efﬁcient KDD techniques such as CHARM [Zaki and
Hsiao, 2002]. This is different from the approach of [Hanney
and Keane, 1996], where only pairs of similar source cases
are considered, according to a ﬁxed criterion. In CABAMA-
KA, there is no similarity criterion on which a selection of
pairs of cases to be compared could be carried out. Indeed,
the CBR process in CABAMAKA relies on the adaptation-
guided retrieval principle [Smyth and Keane, 1996], where
only adaptable cases are retrieved. Thus, every pair of cases
may be of interest, and two cases may appear to be similar
w.r.t. a given point of view, and dissimilar w.r.t. another one.

Principles of KDD. The goal of KDD is to discover knowl-
edge from databases, under the supervision of an analyst (ex-
pert of the domain). A KDD session usually relies on three
main steps: data preparation, data-mining, and interpretation
of the extracted pieces of information.

Data preparation is mainly based on formatting and ﬁlter-
ing operations. The formatting operations are used to trans-
form the data into a form allowing the application of the cho-
sen data-mining operations. The ﬁltering operations are used
for removing noisy data and for focusing the data-mining op-
eration on special subsets of objects and/or attributes.

Data-mining algorithms are applied for extracting from
data information units showing some regularities [Hand et al.,
2001]. In the present experiment, the CHARM data-mining
algorithm that efﬁciently performs the extraction of frequent
closed itemsets (FCIs) has been used [Zaki and Hsiao, 2002].
CHARM inputs a formal database, i.e. a set of binary trans-
actions, where each transaction T is a set of binary items. An
itemset I is a set of items, and the support of I, support(I),
is the proportion of transactions T of the database possess-
ing I (I ⊆ T ). I is frequent, with respect to a threshold
σ ∈ [0; 1], whenever support(I) ≥ σ. I is closed if it has
no proper superset J (I (cid:2) J) with the same support.

The interpretation step aims at interpreting the extracted
pieces of information, i.e. the FCIs in the present case, with
the help of an analyst.
In this way, the interpretation step
produces new knowledge units (e.g. rules).

The CABAMAKA system relies on these main KDD steps

as explained below.

Formatting. The formatting step of CABAMAKA inputs
the case base CB and outputs a set of transactions obtained
from the pairs (srce-case1, srce-case2). It is composed
of two substeps. During the ﬁrst substep, each srce-case =
(srce, Sol(srce)) ∈ CB is formatted in two sets of boolean
properties: Φ(srce) and Φ(Sol(srce)). The computation of
Φ(srce) consists in translating srce from the problem rep-
resentation formalism to 2P, P being a set of boolean prop-
erties. Some information may be lost during this translation,
for example, when translating a continuous property into a
set of boolean properties, but this loss has to be minimized.
Now, this translation formats an expression srce expressed

in the framework of the domain ontology O to an expression
Φ(srce) that will be manipulated as data, i.e. without the use
of a reasoning process. Therefore, in order to minimize the
translation loss, it is assumed that
if p ∈ Φ(srce) and p (cid:2)O q

(1)
for each p, q ∈ P (where p (cid:2)O q stands for “q is a conse-
quence of p in the ontology O”). In other words, Φ(srce) is
assumed to be deductively closed given O in the set P. The
same assumption is made for Φ(Sol(srce)). How this ﬁrst
substep of formatting is computed in practice depends heavily
on the representation formalism of the cases and is presented,
for our application, in section 4.2.

then q ∈ Φ(srce)

The second substep of formatting produces a transaction
T = Φ((srce-case1, srce-case2)) for each ordered pair
of distinct source cases, based on the sets of items Φ(srce1),
Φ(srce2), Φ(Sol(srce1)) and Φ(Sol(srce2)). Following
the model of adaptation presented in section 2 (items , 
and ), T has to encode the properties of Δpb and Δsol.
Δpb encodes the similarities and dissimilarities of srce1 and
srce2, i.e.:
• The properties common to srce1 and srce2 (marked by
• The properties of srce1 that srce2 does not share (“-”),
• The properties of srce2 that srce1 does not share (“+”).
All these properties are related to problems and thus are
marked by pb. Δsol is computed in a similar way and
Φ(T ) = Δpb ∪ Δsol. For example,

“=”),

and

pb

pb

pb

T = {p-
∪ {p=
∪ {p+
∪ {p-
∪ {p=
∪ {p+

sol

sol

sol

| p ∈ Φ(srce1)\Φ(srce2)}
| p ∈ Φ(srce1) ∩ Φ(srce2)}
| p ∈ Φ(srce2)\Φ(srce1)}
| p ∈ Φ(Sol(srce1))\Φ(Sol(srce2))}
| p ∈ Φ(Sol(srce1)) ∩ Φ(Sol(srce2))}
| p ∈ Φ(Sol(srce2))\Φ(Sol(srce1))}

Filtering. The ﬁltering operations may take place before,
between and after the formatting substeps, and also after the
mining step. They are guided by the analyst.

Mining. The extraction of FCIs is computed thanks to
CHARM (in fact, thanks to a tool based on a CHARM-like
algorithm) from the set of transactions. A transaction T =
Φ((srce-case1, srce-case2)) encodes a speciﬁc adapta-
tion ((srce1, Sol(srce1)), srce2) (cid:3)→ Sol(srce2). For ex-
ample, consider the following FCI:
, A-

I =

, B=

, C +

(cid:3)

(cid:4)

a-
pb

, c=
pb

, d+
pb

sol

sol

sol

(3)

if

(cid:2)
Φ(srce1) = {a, b, c}
Φ(srce2) = {b, c, d}
, d+
pb

, c=
pb

, b=
pb

(cid:3)

then T =
a-
pb
More generally:

Φ(Sol(srce1)) = {A, B}
Φ(Sol(srce2)) = {B, C}
, B=
(2)

, C +

(cid:4)

sol

sol

sol

, A-

IJCAI-07

752

I can be considered as a generalization of a subset of the
transactions including the transaction T of equation (2): I ⊆
T . The interpretation of this FCI as an adaptation rule is ex-
plained below.

Interpretation. The interpretation step is supervised by the
analyst. The CABAMAKA system provides the analyst with
the extracted FCIs and facilities for navigating among them.
The analyst may select an FCI, say I, and interpret I as an
adaptation rule. For example, the FCI in equation (3) may be
interpreted in the following terms:

if a is a property of srce but is not a property of tgt,

c is a property of both srce and tgt,
d is not a property of srce but is a property of tgt,
A and B are properties of Sol(srce), and
C is not a property of Sol(srce)
then the properties of Sol(tgt) are
Φ(Sol(tgt)) = (Φ(Sol(srce)) \ {A}) ∪ {C}.

This rule has to be translated from the formalism 2P (sets of
boolean properties) to the formalism of the adaptation rules of
the CBR system. The result is an adaptation rule, i.e. a rule
whose left part represents conditions on srce, Sol(srce)
and tgt and whose right part represents a way to compute
Sol(tgt). The role of the analyst is to correct and to validate
this adaptation rule and to associate an explanation with it.
The analyst is helped in this task by the domain ontology O
that is useful for organizing the FCIs and by the already avail-
able adaptation knowledge that is useful for pruning from the
FCIs the ones that are already known adaptation knowledge.
4.2
The CABAMAKA discovery process relies on the steps de-
scribed in the previous section: (s1) input the case base, (s2)
select a subset of it (or take the whole case base): ﬁrst ﬁlter-
ing step, (s3) ﬁrst formatting substep, (s4) second ﬁltering
step, (s5) second formatting substep, (s6) third ﬁltering step,
(s7) data-mining (CHARM), (s8) last ﬁltering step and (s9)
interpretation. This process is interactive and iterative: the
analyst runs each of the (si) (and can interrupt it), and can go
back to a previous step at each moment.

Implementation

Among these steps, only the ﬁrst ones ((s1) to (s3)) and
the last one are dependent on the representation formalism.
In the following, the step (s3) is illustrated in the context of
an application. First, some elements on the application itself
and the associated knowledge representation formalism are
introduced.

Application domain. The application domain of the CBR
system we are developing is breast cancer treatment: in this
application, a problem pb describes a class of patients with a
set of attributes and associated constraints (holding on the age
of the patient, the size and the localization of the tumor, etc.).
A solution Sol(pb) of pb is a set of therapeutic decisions (in
surgery, chemotherapy, radiotherapy, etc.).

Two features of this application must be pointed out. First,
the source cases are general cases (or ossiﬁed cases according
to the terminology of [Riesbeck and Schank, 1989]): a source

case corresponds to a class of patients and not to a single
one. These source cases are obtained from statistical studies
in the cancer domain. Second, the requested behavior of the
CBR system is to provide a treatment and explanations on this
treatment proposal. This is why the analyst is required to
associate an explanation to a discovered adaptation rule.

Representation of cases and of the domain ontology
O. The problems, the solutions, and the domain ontology
of the application are represented in a light extension of
OWL DL (the Web Ontology Language recommended by the
W3C [Staab and Studer, 2004]). The parts of the underlying
description logic that are useful for this paper are presented
below (other elements on description logics, DLs, may be
found in [Staab and Studer, 2004]).
Let us consider the following example:
srce ≡ Patient (cid:12) ∃age.≥45 (cid:12) ∃age.<70

(cid:12) ∃tumor.(∃size.≥4

R. For example, intervals such as ≥R

(cid:12) ∃localization.Left-Breast)
(4)
srce represents the class of patients with an age a ∈ [45; 70[,
and a tumor of size S ≥ 4 centimeters localized in the left
breast.
The DL representation entities used here are atomic and
deﬁned concepts (e.g. srce, Patient and ∃age.≥45), roles
(e.g. tumor and localization) concrete roles (e.g. age and
size) and constraints (e.g. ≥45 and <70). A concept C is an
expression representing a class of objects. A role r is a name
representing a binary relation between objects. A concrete
role g is a name representing a function associating a real
number to an object (for this simpliﬁed presentation, the only
concrete domain that is considered is (IR,≤), the ordered set
of real numbers). A constraint c represents a subset of IR de-
45= [45; +∞[
noted by c
70=]−∞; 70[ introduce constraints that are used in the
and <R
application.
A concept is either atomic (a concept name) or deﬁned.
A deﬁned concept is an expression of the following form:
C (cid:12) D, ∃r.C or ∃g.c, where C and D are concepts, r is a role,
g is a concrete role and c is a constraint (many other con-
structions exist in the DL, but only these three constructions
are used here). Following classical DL presentations [Staab
and Studer, 2004], an ontology O is a set of axioms, where an
axiom is a formula of the form C (cid:16) D (general concept inclu-
sion) or of the form C ≡ D, where C and D are two concepts.
The semantics of the DL expressions used hereafter can
be read as follows. An interpretation is a pair I = (ΔI,·I)
where ΔI is a non empty set (the interpretation domain) and
·I is the interpretation function, which maps a concept C to a
I ⊆ ΔI × ΔI,
set C
I : ΔI −→ IR.
In
and a concrete role g to a function g
·I
the following, all roles r are assumed to be functional:
I : ΔI −→ ΔI. The interpretation
maps r to a function r
of the deﬁned concepts, for an interpretation I, is as follows:
I, (∃r.C)I is the set of objects x ∈ ΔI
(C (cid:12) D)I = C
I and (∃g.c)I is the set of objects x ∈
such that r
R. An interpretation I is a model
ΔI such that g
of an axiom C (cid:16) D (resp. C ≡ D) if C
I =

I ⊆ ΔI, a role r to a binary relation r

I ∩ D
I(x) ∈ C
I(x) ∈ c

I (resp. C

I ⊆ D

IJCAI-07

753

I). I is a model of an ontology O if it is a model of each
D
axiom of O. The inference associated with this representation
formalism that is used below is the subsumption test: given an
ontology O, a concept C is subsumed by a concept D, denoted
by (cid:2)O C (cid:16) D, if for every model I of O, C

I ⊆ D
I.

More practically, the problems of the CBR application are
represented by concepts (as srce in (4)). A therapeutic de-
cision dec is also represented by a concept. A solution is
a ﬁnite set {dec1, dec2, . . . deck} of decisions. The de-
cisions of the system are represented by atomic concepts.
The knowledge associated with atomic concepts (and hence,
with therapeutic decisions) is given by axioms of the do-
main ontology O. For example, the decision in surgery
dec = Partial-Mastectomy represents a partial ablation
of the breast:

Partial-Mastectomy (cid:16) Mastectomy

Mastectomy (cid:16) Surgery

Surgery (cid:16) Therapeutic-Decision

(5)

Implementation of the ﬁrst formatting substep (s3).
Both problems and decisions constituting solutions are rep-
Thus, computing Φ(srce) and
resented by concepts.
Φ(Sol(srce)) amounts to the computation of Φ(C), C be-
ing a concept. A property p is an element of the ﬁnite
set P (see section 4.1).
In the DL formalism, p is repre-
sented by a concept P. A concept C has the property p if
(cid:2)O C (cid:16) P. The set of boolean properties and the set of the
corresponding concepts are both denoted by P in the follow-
ing. Given P, Φ(C) is simply deﬁned as the set of properties
P ∈ P that C has:

Φ(C) = {P ∈ P | (cid:2)O C (cid:16) P}
(6)
As a consequence, if P ∈ Φ(C), Q ∈ P and (cid:2)O P (cid:16) Q then
Q ∈ Φ(C). Thus, the implication (1) is satisﬁed.
The algorithm of the ﬁrst formatting substep that has been
implemented ﬁrst computes the Φ(C)’s for C: the source prob-
lems and the decisions occurring in their solutions, and then
computes P as the union of the Φ(C)’s. This algorithm relies
on the following set of equations1:

(cid:5)(cid:5)(cid:5)(cid:5) B is an atomic concept
occurring in KB and (cid:2)O A (cid:16) B
(cid:5)(cid:5)(cid:5) d ∈ Cstraintsg and c

Φ(C (cid:12) D) = Φ(C) ∪ Φ(D)
Φ(∃r.C) = {∃r.P | P ∈ Φ(C)}
Φ(∃g.c) =

(cid:7)
∃g.d

R ⊆ d
Cstraintsg = {c | the expression ∃g.c occurs in KB}
where A is an atomic concept, C and D are (either atomic
or deﬁned) concepts, r is a role, g is a concrete role, c is a
constraint and KB, the knowledge base, is the union of the
case base and of the domain ontology.

R

Φ(A) =

B

(cid:2)

(cid:6)

(cid:8)

1This set of equations itself can be seen as a recursive algorithm,
but is not very efﬁcient since it computes several times the same
things. The implemented algorithm avoids these recalculations by
the use of a cache.

It can be proven that the algorithm for the ﬁrst formatting
substep (computing the Φ(C)’s and the set of properties P)
respects (6) under the following hypotheses. First, the con-
structions used in the DL are the ones that have been intro-
duced above (C (cid:12) D, ∃r.C and ∃g.c, where r is functional).
Second, no deﬁned concept may strictly subsume an atomic
concept (for every atomic concept A, there is no deﬁned con-
cept C such that (cid:2)O A (cid:16) C and (cid:5)(cid:2)O A ≡ C). Under these
hypotheses, (6) can be proven by a recursion on the size of
C (this size is the number of constructions that C contains).
These hypotheses hold for our application. However, an on-
going study aims at ﬁnding an algorithm for computing the
Φ(C)’s and P in a more expressive DL, including in particu-
lar negation and disjunction of concepts.

For example, let srce be the problem introduced by the ax-
iom (4). It is assumed that the constraints associated with the
concrete role age in KB are <30, ≥30, <45, ≥45, <70 and ≥70,
that the constraints associated with the concrete role size in
KB are <4 and ≥4, that there is no concept A (cid:5)= Patient in
KB such that (cid:2)O Patient (cid:16) A, and that the only concept
A (cid:5)= Left-Breast of KB such that (cid:2)O Left-Breast (cid:16) A is
A = Breast. Then, the implemented algorithm returns:
Φ(srce) = {Patient,∃age.≥30,∃age.≥45,∃age.<70,

∃tumor.∃size.≥4,
∃tumor.∃localization.Left-Breast
∃tumor.∃localization.Breast}

And the 7 elements of Φ(srce) are added to P.
Another example, based on the set of axioms (5) is:
Φ(Partial-Mastectomy) = {Partial-Mastectomy,
Mastectomy, Surgery, Therapeutic-Decision}

4.3 Results
The CABAMAKA process piloted by the analyst produces a
set of FCIs. With n = 647 cases and σ = 10%, CABAMAKA
has given 2344 FCIs in about 2 minutes (on a current PC).
Only the FCIs with at least a + or a - in both problem prop-
erties and solution properties were kept, which corresponds
to 208 FCIs. Each of these FCIs I is presented for inter-
pretation to the analyst under a simpliﬁed form by removing
some of the items that can be deduced from the ontology. In
pb is
particular if P=
removed from I. For example, if P = (∃age ≥45) ∈ P,
pb
∈ I, then, nec-
Q = (∃age ≥30) ∈ P and (∃age ≥45)=
essarily, (∃age ≥30)=
∈ I, which is a redundant piece of
information.
I = {(∃age. <70)=

The following FCI has been extracted from CABAMAKA:

∈ I and (cid:2)O P (cid:16) Q then Q=

∈ I, Q=

pb

pb

pb

,

, (∃tumor.∃size. ≥4)+

,

pb

pb

(∃tumor.∃size. <4)-
Curettage=
Partial-Mastectomy-

sol

pb

, Mastectomy=

,

sol

}
sol
It has been interpreted in the following way: if srce and tgt
both represent classes of patients of less than 70 years old, if
the difference between srce and tgt lies in the tumor size of

, Radical-Mastectomy+

sol

IJCAI-07

754

the patients—less than 4 cm for the ones of srce and more
than 4 cm for the ones of tgt—and if a partial mastectomy
and a curettage of the lymph nodes are proposed for the srce,
then Sol(tgt) is obtained by substituting in Sol(srce) the
partial mastectomy by a radical one.

It must be noticed that this example has been chosen for its
simplicity: other adaptation rules have been extracted that are
less easy to understand. More substantial experiments have
to be carried out for an effective evaluation.

The choice of considering every pairs of distinct source
cases can be discussed. Another version of CABAMAKA
has been tested that considers only similar source cases, as
in [Hanney and Keane, 1996]: only the pairs of source cases
such that |Φ(srce1) ∩ Φ(srce2)| ≥ k were considered (ex-
perimented with k = 1 to k = 10). The ﬁrst experiments have
not shown yet any improvements in the results, compared to
the version without this constraint (k = +∞), and involves
the necessity to have the threshold k ﬁxed.

5 Conclusion and Future Work
The CABAMAKA system presented in this paper is inspired
by the research of Kathleen Hanney and Mark T. Keane [Han-
ney and Keane, 1996] and by the principles of KDD for the
purpose of semi-automatic adaptation knowledge acquisition.
It reuses an FCI extraction tool developed in our team and
based on a CHARM-like algorithm. Although implemented
for a speciﬁc application to breast cancer treatment decision
support, it has been designed to be reusable for other CBR ap-
plications: only a few modules of CABAMAKA are dependent
on the formalism of the cases and of the domain ontology, and
this formalism, OWL DL, is a well-known standard.

One element of future work consists in searching for ways
of simplifying the presentation of the numerous extracted
FCIs to the analyst. This involves an organization of these
FCIs for the purpose of navigation among them. Such an
organization can be a hierarchy of FCIs according to their
speciﬁcities or a clustering of the FCIs in themes.

A second piece of future work, still for the purpose of
helping the analyst, is to study the algebraic structure of all
the possible adaptation rules associated with the operation of
composition: r is a composition of r1 and r2 if adapting
(srce, Sol(srce)) to solve tgt thanks to r gives the same
solution Sol(tgt) as (1) solving a problem pb by adaptation
of (srce, Sol(srce)) thanks to r1 and (2) solving tgt by
adaptation of (pb, Sol(pb)) thanks to r2. The idea is to ﬁnd a
smallest family of adaptation rules, F , such that the closure of
F under composition contains the sets of the extracted adap-
tation rules expressed in the form of FCIs. It is hoped that F
is much smaller than S and so requires less effort from the an-
alyst while corresponding to the same adaptation knowledge.
Another study on AKA for our CBR system was AKA from
experts (based on the analysis of the adaptations performed
by the experts). This AKA has led to a few adaptation rules
and also to adaptation patterns, i.e. general strategies for
case-based decision support that are associated with explana-
tions but that need to be instantiated to become operational.
A third future work is mixed AKA, that is a combined use of
the adaptation patterns and of the adaptation rules extracted

from CABAMAKA: the idea is to try to instantiate the former
by the latter in order to obtain a set of human-understandable
and operational adaptation rules.

References
[Aamodt, 1990] A. Aamodt. Knowledge-Intensive Case-
Based Reasoning and Sustained Learning. In L. C. Aiello,
editor, Proc. of the 9th European Conference on Artiﬁcial
Intelligence (ECAI’90), August 1990.

[Carbonell, 1983] J. G. Carbonell. Learning by analogy:
Formulating and generalizing plans from past experience.
In R. S. Michalski and J. G. Carbonell and T. M. Mitchell,
editor, Machine Learning, An Artiﬁcial Intelligence Ap-
proach, chapter 5, pages 137–161. Morgan Kaufmann,
Inc., 1983.

[Hand et al., 2001] D. Hand, H. Mannila, and P. Smyth.
Principles of Data Mining. The MIT Press, Cambridge
(MA), 2001.

[Hanney and Keane, 1996] K. Hanney and M. T. Keane.
Learning Adaptation Rules From a Case-Base. In I. Smith
and B. Faltings, editors, Advances in Case-Based Reason-
ing – Third European Workshop, EWCBR’96, LNAI 1168,
pages 179–192. Springer Verlag, Berlin, 1996.

[Hanney, 1997] K. Hanney. Learning Adaptation Rules from

Cases. Master’s thesis, Trinity College, Dublin, 1997.

[Jarmulak et al., 2001] J. Jarmulak, S. Craw, and R. Rowe.
Using Case-Base Data to Learn Adaptation Knowledge for
Design.
In Proceedings of the 17th International Joint
Conference on Artiﬁcial Intelligence (IJCAI’01), pages
1011–1016. Morgan Kaufmann, Inc., 2001.

[Leake et al., 1996] D. B. Leake, A. Kinley, and D. C. Wil-
son. Acquiring Case Adaptation Knowledge: A Hybrid
Approach. In AAAI/IAAI, volume 1, pages 684–689, 1996.
[Riesbeck and Schank, 1989] C. K. Riesbeck and R. C.
Inside Case-Based Reasoning. Lawrence Erl-

Schank.
baum Associates, Inc., Hillsdale, New Jersey, 1989.

[Shiu et al., 2001] S. C. K. Shiu, Daniel S. Yeung, C. Hung
Sun, and X. Wang. Transferring Case Knowledge to Adap-
tation Knowledge: An Approach for Case-Base Mainte-
nance. Computational Intelligence, 17(2):295–314, 2001.
[Smyth and Keane, 1996] B. Smyth and M. T. Keane. Using
adaptation knowledge to retrieve and adapt design cases.
Knowledge-Based Systems, 9(2):127–135, 1996.

[Staab and Studer, 2004] S. Staab and R. Studer, editors.

Handbook on Ontologies. Springer, Berlin, 2004.

[Wiratunga et al., 2002] N. Wiratunga,

and
R. Rowe. Learning to Adapt for Case-Based Design.
In S. Craw and A. Preece, editors, Proceedings of the
6th European Conference on in Case-Based Reasoning
(ECCBR-02), LNAI 2416, pages 421–435, 2002.

S. Craw,

[Zaki and Hsiao, 2002] M. J. Zaki and C.-J. Hsiao. CHARM:
An Efﬁcient Algorithm for Closed Itemset Mining.
In
SIAM International Conference on Data Mining SDM’02,
pages 33–43, Apr 2002.

IJCAI-07

755


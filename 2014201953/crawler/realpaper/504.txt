When Discriminative Learning of Bayesian Network Parameters Is Easy 

Hannes Wettig*, Peter Griinwald0, Teemu Roos*, Petri Myllymaki*, and Henry Tirri* 

*  Complex Systems Computation Group (CoSCo) 
Helsinki Institute for Information Technology (HUT) 

°  Centrum voor Wiskunde en Informatica (CWI) 

University of Helsinki & Helsinki University of Technology 

NL-1090 GB Amsterdam, The Netherlands. 

P.O. Box 9800, FIN-02015 HUT, Finland 

{Firstname}. {Lastname} @hiit.fi 

P.O. Box 94079 

Peter.Grunwald®cwi.nl 

Abstract 

Bayesian network models are  widely used for dis(cid:173)
criminative prediction tasks such as classification. 
Usually their parameters are determined using  'un(cid:173)
supervised'  methods  such  as  maximization  of the 
joint likelihood.  The  reason  is  often  that it  is un(cid:173)
clear  how  to  find  the  parameters  maximizing  the 
conditional (supervised) likelihood.  We show how 
the discriminative  learning problem can be solved 
efficiently  for  a  large  class  of  Bayesian  network 
models, including the Naive Bayes (NB) and tree-
augmented Naive Bayes (TAN) models. We do this 
by  showing  that  under a certain  general condition 
on the network structure, the discriminative learn(cid:173)
ing problem is exactly equivalent to logistic regres(cid:173)
sion  with  unconstrained convex parameter spaces. 
Hitherto this was known only for Naive Bayes mod(cid:173)
els.  Since  logistic  regression  models  have  a con(cid:173)
cave  log-likelihood  surface,  the  global  maximum 
can be easily found by local optimization methods. 

Introduction 

1 
In recent years it has been recognized that for discriminative 
prediction tasks such as classification, we should use a  'su(cid:173)
pervised*  learning  algorithm  such  as  conditional  likelihood 
maximization [Friedman et  al.,  1997; Ng and Jordan, 2001; 
Kontkanen et al., 2001; Greiner and Zhou, 2002].  Neverthe(cid:173)
less,  for Bayesian  network  models  the  parameters  are  cus(cid:173)
tomarily determined using ordinary  methods such as maxi(cid:173)
mization of the joint (unsupervised) likelihood.  One of the 
main  reasons  for this  discrepancy  is  the  difficulty  in  finding 
the global maximum of the conditional likelihood. In this pa(cid:173)
per, we show that this problem can be solved, as long as the 
underlying  Bayesian  network  meets  a  particular  additional 
condition,  which  is  satisfied  for  many  existing  Bayesian-
network  based  models  including  Naive  Bayes  (NB),  TAN 
(tree-augmented NB) and  'diagnostic' models [Kontkanen et 
al., 2001]. 

We consider domains of discrete-valued random variables. 
We  find  the maximum conditional  likelihood parameters by 
logarithmic reparametrization.  In this way, each conditional 
Bayesian  network  model  is  mapped  to  a  logistic  regression 

model, for which the likelihood surface is known to be con(cid:173)
cave.  However,  in  some  cases  the  parameters of this  logis(cid:173)
tic regression model are not allowed to vary freely.  In other 
words,  the Bayesian network model corresponds to a subset 
of a logistic regression model rather than the full model. 

Our main result (Thm. 3 below) provides a general condi(cid:173)
tion on the network structure under which, as we prove, the 
Bayesian network model is mapped to a full logistic regres(cid:173)
sion model with freely varying parameters.  Therefore, in the 
new  parametrization the conditional  log-likelihood becomes 
a  concave  function  of the  parameters  that  under  our  condi(cid:173)
tion  are  allowed to  vary  freely over the convex set Rk.  Now 
we can find the global maximum in the conditional likelihood 
surface by simple local optimization  techniques such as hill 
climbing. 

The result still leaves open the possibility that there are no 
network  structures  for which  the  conditional  likelihood  sur(cid:173)
face has local, non-global maxima. This would make our con(cid:173)
dition superfluous.  Our second result (Thm. 4 below) shows 
that this is not the case:  there are very simple network struc(cid:173)
tures that do not satisfy our condition, and for which the con(cid:173)
ditional likelihood can exhibit local, non-global maxima. 

Viewing  Bayesian  network (BN)  models as  subsets of lo(cid:173)
gistic  regression  models  is  not  new;  it  was  done  earlier  in 
papers  such  as  [Heckerman  and Meek,  1997a;  Ng  and Jor(cid:173)
dan,  2001;  Greiner and  Zhou,  2002].  Also,  the  concavity 
of the log-likelihood surface for logistic regression is known. 
Our main contribution is to supply the condition under which 
Bayesian  network  models  correspond  to  logistic  regression 
with  completely freely  varying  parameters.  Only  then  can 
we guarantee that there are no local maxima in the likelihood 
surface.  As  a direct consequence of our result,  we show for 
the  first  time  that  the  supervised likelihood of,  for instance, 
the  tree-augmented Naive Bayes  (TAN)  model  has  no  local 
maxima. 

This  paper is  organized  as  follows.  In  Section  2  we  in(cid:173)
troduce  Bayesian  networks  and  an  alternative,  so-called  L-
parametrization. 
In  Section  3  we  show  that  this  allows  us 
to  consider Bayesian  network  models  as  logistic  regression 
models.  Based  on  earlier results  in  logistic  regression,  we 
conclude  that  in  the  L-parametrization  the  supervised  log-
likelihood  is  a  concave  function. 
In  Section  4  we  present 
our  main  results,  giving  conditions  under  which  the  two 
parametrizations correspond to exactly the  same conditional 

LEARNING 

491 

distributions.  Conclusions  are  summarized  in  Section  5; 
proofs of the main results are given in Appendix A. 

2  Bayesian Networks and the L-model 
We  assume that the  reader is  familiar with  the  basics  of the 
theory of Bayesian networks, see, e.g., iPearl,  988]. 

Consider  a  random  vector 

where each variable  Xi  takes values in { 1 , . . ., ni}. Let B be 
a  Bayesian  network  structure  over  A",  that  factorizes  P(X) 
into 

(1) 

is the parent set of variable Xt 

where 
inB. 

We  are  interested  in  predicting  some  class  variable  Xm 

conditioned on  all 

for some  m 
Without  loss  of generality  we  may assume that  m  =  0  (i.e., 
Xo  is  the  class  variable)  and  that  the  children  of A'0  in  B 
are 
  For instance, in the 
so-called Naive Bayes model  we have M  —  M' and the chil(cid:173)
dren of the class variable Ao are independent given the value 
of XQ.  The Bayesian  network  model  corresponding to  B  is 
the set of all distributions satisfying the conditional indepen(cid:173)
dencies  encoded in  B.  It  is usually  parametrized by  vectors 

for  s o m e.

with components of the form 

defined by 

(2) 
where  Pai  is  any  configuration  (set  of  values)  for  the  par(cid:173)
ents 
Whenever  we  want  to  emphasize  that 
each  pai  is  determined  by  the  complete  data  vector  x  = 
,  we  write  pat(x)  to  denote  the  configuration 
given  by  the  vector  x.  For  a  given  data  vec(cid:173)
of 
tor 
,  we sometimes need to consider 
a modified vector where  X0  is replaced by x'0  and the other 
entries  remain  the  same.  We  then  write 
for  the 
configuration  of 

given by 
the  set  of  conditional  distributions 
corresponding  to  distributions 
satisfying  the  conditional  indepen(cid:173)

We 

let  MB  be 

dencies encoded in  B.  The conditional distributions  in 
can be written as 

(3) 

extended to TV outcomes by indenendence. 

Given a complete data-matrix 

ditional 
given by 

log-likelihood, 

with  parameters 

, the con(cid:173)
is 

(4) 

where 

Note  that  in  (3),  and  hence  also  in  (4),  all 

(5) 
with 
(standing for nodes that are neither the class variable 

3  The L-model Viewed as Logistic Regression 
Although L-models are closely related to and in some cases 
formally identical to Bayesian network models, we can also 
think of them as predictors that combine the information of 
the attributes using the  so-called softmax rule  [Heckerman 
and Meek,  1997b; Ng and Jordan, 2001].  In statistics, such 
models have been extensively studied  under the name of lo(cid:173)
gistic regression models, see, e.g. [McLachlan, 1992, p.2551. 

492 

LEARNING 

More  precisely,  let 
be  real-valued  random  variables.  The  multiple  logistic  re(cid:173)
gression  model  with  dependent  variable  XQ  and  covariates 
Y\,..., Yk  is defined as the set of conditional distributions 

and  let 

(7) 

where  the 
defines a conditional model parameterized in  
i 
configurations  of  Xt,  let 

are  allowed  to  take  on  all  values  in  R  This 
.  Now, for 
and pax  in  the set of parent 

(8) 

The  indicator  random  variables  
thus  obtained  can 
be  lexicographically  ordered  and  renamed  l , . . . , f c,  which 
shows  that  each  L-model  corresponding  to  a  Bayesian  net(cid:173)
work  structure  B  as  in  (6)  is  formally  identical  to  the  logistic 
model  (7)  with  dependent  variable  A'o  and  covariates  given 
by  (8).  So,  for all  network structures B,  the  corresponding L-
model  ML  is  the  standard  multiple  logistic  model,  where  the 
input  variables  for  the  logistic  model  are  transformations  of 
the  input  variables  to  the  L-model,  the  transformation  being 
determined  by  the  network  structure  B. 

It  turns  out  that  the  conditional  log-likelihood  in  the  L-

parametrization  is  a  concave  function  of the  parameters: 
Theorem  2.  The  parameter  set 
ditional 
strictly 

is  convex,  and  the  con(cid:173)
is  concave, 
though  not 

log-likelihood 

concave. 

in  

Concavity 

Proof.  The  first  part is obvious since each parameter can take 
values 
 a direct con(cid:173)
sequence  of the  fact  that  multiple  logistic  regression  models 
are exponential families; see, e.g., [McLachlan,  1992, p.2601. 
For an example showing that the conditional  log-likelihood is 
not  strictly  concave,  see  [Wettig  et at, 20021. 

is

o

f

Remark.  Non-strictness  of the  proven  concavity  may  pose 
a technical  problem  in  optimization.  This  can  be  avoided  by 
assigning  a  strictly  concave  prior  on  the  model  parameters 
and  then  maximizing  the  'conditional  posterior'  [Grunwald 
et  «/.,  2002;  Wettig  et  a/.,  2002]  instead  of  the  likelihood. 
One  may  also  prune  the  model  of weakly  supported  parame(cid:173)
ters  and/or add constraints to  arrive  at  a  strictly  concave con(cid:173)
ditional  likelihood  surface.  Our  experiments  [Wettig  et  al, 
2002]  suggest that for small  data samples this  should be done 
in  any case,  in  order to  avoid over-fitting;  see  also  Section  5. 
Any  constraint  added  should  of  course  leave  the  parameter 
space  a convex  set,  e.g.  a  subspace  of the full  0L. 
Corollary  1.  There  are  no  local,  non-global,  maxima  in  the 
likelihood  surface  of an  L-model. 

The  conditions  under  which  a  global  maximum  exists  are 
discussed in, e.g.,  [McLachlan,  1992] and references therein. 
A  possible  solution  in  cases  where  no  maximum  exists  is  to 
introduce a strictly concave prior as discusssed above. 

The  global  conditional  maximum  likelihood  parameters 
obtained  from  training  data  can  be  used  for  prediction  of 

future  data. 
In  addition,  as  discussed  in  [Heckerman  and 
Meek,  1997a],  they  can  be  used  to  perform  model  selec(cid:173)
tion  among  several  competing  model  structures  using,  e.g., 
the  BIC  or  (approximate)  M DL  criteria.  In  [Heckerman  and 
Meek,  1997a] it is stated that for general conditional Bayesian 
network  models  MB,  "although  it  may  be  difficult  to  deter(cid:173)
mine a global  maximum, gradient-based methods  [...]  can  be 
used  to  locate  local  maxima".  Theorem  2  shows  that  if  the 
network  structure  B  is  such  that  the  two  models  are  equiva(cid:173)
,  we  can  find  even  the  global  maximum  of 
lent, 
the  conditional  likelihood  by  reparametrizing  MB  to  the  L-
model, and using some  local  optimization  method.  Thus,  the 
question  under  which  condition  .  
becomes  cru(cid:173)
cial.  It  is this question  we address in  the next section. 

Remark.  Because  the  log-transformation  is  continuous,  it 
follows  (with  some  calculus)  that,  if .  
,  then  all 
maxima  of  the  (concave)  conditional  likelihood  in  the  L-
parameterization  are  global  (and  connected)  maxima  also  in 
the original parametrization.  Nevertheless, the likelihood sur(cid:173)
face as a function  of  
has  some  unpleasant  proper(cid:173)
ties  (see  [Wettig  et  al.,  2002]): 
it  is  not  concave  in  general 
and,  worse,  it  can  have  'wrinkles':  by  these  we  mean  con(cid:173)
vex  subsets 
such  that,  under  the  constraint  that 
the  likelihood  surface  does  exhibit  local,  non-
global  maxima.  This  suggests  that  it  is  computationally  pre-
ferrable  to  optimize  over  
.  Empirical evi(cid:173)
dence for this  is  reported in  [Greiner and  Zhou,  2002]. 
4  Main Result 
By  setting 
,  it  follows  that  each  distribu(cid:173)
tion  in  MB  is  also  in  ML  (Thm.  1).  This  suggests  that,  by 
doing  the  reverse  transformation 

rather  than 

(9) 

for  some  i 

.  However, 

'sum-up-to-one constraint',  i.e.,  for  some  

we  could  also  show  that  distributions  in 

are  also  in 
contains  distributions  that  violate  the 
we  have 
and  pai. 
.  But,  since  the  L-
index  the 
it  may  still 

Then  the  corresponding  
parameterization  is  redundant  (many  different  
same  conditional  distribution  
be  the  case  that  the  distribution 
Indeed,  it  turns  out  that  for  some  network  struc(cid:173)
is  in  , -. 
tures  B,  the  corresponding  M L  is  such  that  each  distribution 
in  , 
such that 
and/Mi.  In 
that  case,  by  (9),  we  do  have  MB  —  ML.  Our  main  result  is 
that  this  is  the case  if B  satisfies  the  following condition: 

can  be  expressed  by  a  parameter  vector   _ 

indexed  by 

is  not  in 

. 

Condition  1.  For  all  

such  that  ~ 

~ 

there  exists 

Remark.  Condition  1  implies  that  the  class  
must  be  a 
'moral node',  i.e.,  it cannot have a common child with  a node 
it  is  not  directly  connected  with.  But  Condition  1  demands 
more than that; see Figures  1  and 2. 

LEARNING 

493 

Figure  1:  A  simple  Bayesian  network  (the  class  variable  is 
denoted by  Xo)  satisfying Condition  1  (left); and a network 
that does not satisfy the condition (right). 

Figure 2:  A tree-augmented Naive Bayes (TAN) model satis(cid:173)
fying Condition 1 (left); and a network that is not TAN (right). 
Even though  in  both cases  the class  variable  X0  is  a moral 
node, the network on the right does not satisfy Condition  1. 

Example 1.  Consider the Bayesian networks depicted in Fig(cid:173)
ure  1.  The  leftmost network,  B1,  satisfies Condition  1,  the 
rightmost network,  B2,  does not.  Theorem  shows that the 
conditional  likelihood  surface  of 
can have local max(cid:173)
ima, implying that in this case 

Examples of network structures that satisfy Condition 1  are 
the  Naive Bayes  (NB)  and  the tree-augmented Naive Bayes 
(TAN)  models  [Friedman et a/.,  1997].  The  latter is  a gen(cid:173)
eralization  of the  former  in  which  the  children  of the  class 
variable are allowed to form tree-structures; see Figure 2. 

Proposition  1.  Condition  J  is satisfied by the Naive  Bayes 
and the tree-augmented Naive Bayes structures. 

Proof.  For Naive Bayes,  we have 

for  all 

For TAN  models, all children of the class vari(cid:173)
able have either one or two parents.  For children  with  only 
one parent (the class variable) we can use the same argument 
as in the NB  case.  For any child Xj  with two parents,  let Xi 
be the parent that is not the class variable. Because  Xi  is also 
a child of the class variable, we have 
I 

Condition  1  is  also  automatically satisfied  if  Xo  only has 
incoming  arcs1  ('diagnostic'  models,  see  [Kontkanen  et  ai, 
2001 ]). For Bayesian network structures B for which the con(cid:173)
dition does not hold, we can always add some arcs to arrive 
at  a  structure  B'  for  which  the  condition  does  hold  (for  in(cid:173)
stance, add an arc from 
in  the  rightmost  network 
in  Figure 2).  Therefore,  Mb  is  always  a subset of a  larger 
model 
for which the condition holds. We are now ready 
to present our main result (for proof see Appendix A): 
Theorem 3.  if  B satisfies Condition 1, then 

Together with Corollary  1, Theorem 3  shows that Condi(cid:173)
tion  1  suffices to  ensure that  the conditional  likelihood sur(cid:173)
face  of 
has no local (non-global) maxima. Proposition 1 

now implies that, for example, the conditional likelihood sur(cid:173)
face of TAN models has no local maxima. Therefore, a global 
maximum can be found by local optimization techniques. 

But  what  about  the  case  in  which  Condition  1  does  not 
hold? Our second result, Theorem 4 (proven in Appendix A) 
says that in this case, there can be local maxima: 
Theorem  4. 
be the network 
structure depicted in  Figure  J  (right).  There exist data sam(cid:173)
ples such  that the conditional likelihood has local,  non-global 
maxima  over, 

The theorem implies that 

Thus, Condition 1 
is not superfluous.  We  may now  ask  whether our condition 
is necessary for having . 
MB  for all  network  structures  that  violate  the  condition.  We 
plan to address this intriguing open question in future work. 

that is, whether 

5  Concluding Remarks 
We showed that one can effectively find the parameters max(cid:173)
imizing  the  conditional  (supervised)  likelihood  of NB,  TAN 
and  many  other Bayesian  network  models.  We  did  this  by 
showing  that  the  network  structure  of these  models  satisfies 
our  'Condition  1',  which ensures that the conditional distri(cid:173)
butions corresponding to such models are equivalent to a par(cid:173)
ticular multiple logistic  regression  model with unconstrained 
parameters.  An  arbitrary  network  structure  can  always  be 
made  to  satisfy  Condition  1  by  adding  arcs.  Thus,  we  can 
embed any Bayesian  network model  in a larger model  (with 
less independence assumptions) that satisfies Condition  1. 

Test runs for the Naive Bayes case in  [Wettig et  al,  2002] 
have  shown  that  maximizing  the  conditional  likelihood  in 
contrast to the usual  practice of maximizing the joint  (unsu(cid:173)
pervised)  likelihood  is  feasible  and  yields  greatly  improved 
classification.  Similar  results  are  reported  in  [Greiner  and 
Zhou,  2002].  Our conclusions  are  also  supported  by  theo(cid:173)
retical analysis in [Ng and Jordan, 2001]. Only on very small 
data sets we sometimes see that joint likelihood optimization 
outperforms conditional likelihood, the reason apparently be(cid:173)
ing  that  the  conditional  method  is  more  inclined  to  over-
fitting. We conjecture that in such cases, rather than resorting 
to maximizing the joint instead of the conditional likelihood, 
it may be preferable to use a simpler model or simplify (i.e. 
prune or restrict) the model at hand and still choose its param(cid:173)
eters  in  a discriminative fashion.  In  our setting,  this  would 
amount to model selection using the L-parametrization. This 
is a subject of our future research. 

References 
[Friedman et al, 1997]  N.  Friedman,  D.  Gcigcr,  and  M.  Gold-
szmidt.  Bayesian  network  classifiers.  Machine  Learning, 
29:131-163, 1997. 

[Greiner and Zhou, 2002]  R. Greiner and W. Zhou.  Structural ex(cid:173)
tension to logistic regression:  Discriminant  parameter learning 
of belief net classifiers. In Proceedings of the Eighteenth Annual 
National Conference on Artificial Intelligence (AAA1-02), pages 
167-173, Edmonton, 2002. 

'it is easy to see that in that case the maximum conditional like(cid:173)

lihood parameters may even be determined analytically. 

[Griinwald et al, 2002]  P. Griinwald, P. Kontkanen, P. Myllymaki, 
T. Roos, H. Tirri, and H. Wettig.  Supervised posterior distri-

494 

LEARNING 

butions,  2002.  Presented  at the  Seventh  Valencia  International 
Meeting on Baycsian Statistics, Tenerife, Spain. 

[Hcckcrman and Meek,  1997a]  D.  Heckcrman  and C.  Meek.  Em(cid:173)
bedded bayesian network classifiers. Technical Report MSR-TR-
97-06, Microsoft Research,  1997. 

[Heckcrman and Meek,  1997b)  D. Hcckerman and C. Meek.  Mod(cid:173)
els  and  selection  criteria  for  regression  and  classification. 
In 
D.  Geiger  and  P.  Shenoy,  editors,  Uncertainty  in Arificial  Intel­
ligence  13,  pages  223-228.  Morgan  Kaufmann  Publishers,  San 
Mateo, CA,  1997. 

[Kontkanen et al, 2001]  P. Kontkanen, P. Myllymaki, and H. Tirri. 
In 
Classifier  learning  with  supervised  marginal  likelihood. 
J.  Breese  and  D.  Koller,  editors,  Proceedings  of the  17th  In­
ternational  Conference  on  Uncertainty  in  Artificial  Intelligence 
(UAI'01). Morgan  Kaufmann  Publishers, 2001. 

[McLachlan,  1992]  G.J.  McLachlan.  Discriminant  Analysis  and 
Statistical Pattern  Recognition.  John  Wiley  &  Sons,  New  York, 
1992. 

[Ng and Jordan,  2001]  A.Y. Ng and M.I. Jordan.  On discriminative 
vs. generative classifiers:  A comparison of logistic regression and 
naive  Bayes.  Advances  in  Neural  Information  Processing  Sys­
tems, 14:605-610,2001. 

[Pearl,  19881  J.  Pearl.  Probabilistic  Reasoning  in  Intelligent  Sys­
tems:  Networks  of  Plausible  Inference.  Morgan  Kaufmann  Pub(cid:173)
lishers, San Mateo, CA, 1988. 

lWettig et al.,2002] H.  Wettig,  P.  Grunwald,  T.  Roos,  P.  Myl-
lymaki,  and  H.  Tirri.  On  supervised  learning  of Bayesian  net(cid:173)
work  parameters.  Technical  Report  HIIT-2002-1,  Helsinki  In(cid:173)
stitute  for  Information  Technology  (HUT),  2002.  Available  at 
http://cosco.hiit.fi/Articles/hiit2002- 

.ps. 

A  Proofs 
Proof of Theorem  3.  We  introduce  some  more  notation.  For 
the  maximum  number  in 

, 
{ ( ) , . . ., A /}  such  that 
Such  an  mj  exists  by  Condition  1.  To  see  this,  note  that 
t h e m e n t
i o n ed  in  Condition  1  must  lie  in  the  set 

let  mj  be 

(otherwise 
,  contradiction). 

,  so 

is  completely  determined  by 

Condition  1  implies  that  
r We  can 

a

i

the  p
therefore  introduce  functions 
Qj  m a p p i n g)  to  the  corresponding  paj.  Hence, 
for  all  

  have 

we

a

n

d

We  introduce,  for  all  

figuration 

,  a  constant 

and  for  each  con(cid:173)
and  define,  for  any 

(10) 

( ID 

constructed  this  way  are  combined to 

The  parameters 
a  vector 

which is clearly  a  member  of 

Having  introduced this  notation, we  now show that no mat(cid:173)
ter how  we  choose the constants  Ci|p a,,  for all  0L  and corre(cid:173)
sponding 

we  have 

We  first  show that,  for all  possible  vectors x  and  the corre(cid:173)

sponding  parent  configurations,  no  matter  how  the  
chosen,  it  holds that 

are 

(12) 

n

e w i

by  their 
, there is ex(cid:173)
that appears  in the  sum with 
there  exists 

To derive (12) we substitute all terms of 
definition  (11).  Clearly,  for all 
actly one term  of the  form  
a  positive  sign.  Since  for  each  ■  
exactly  o
that  for  all  
appears exactly once in  the  sum with  a negative sign.  By  (10) 
we  have 
that 
appear once  with  a positive sign  also appear once with  a neg(cid:173)
ative  sign.  It  follows  that,  except  for  
cancel.  This establishes  (12).  By  plugging in  (12)  into  (6),  it 
follows  that 

th  mj  =  i,  it  must  be  the  case 

.  Therefore all terms  

,  a  term  of the  form 

all  terms 

Now  set,  for  all  x,  and  pai 

We  show  that we can  determine the  constants  
for  all  
is satisfied,  i.e.,  we have 

such that 
and pa,, the  'sum up to one'  constraint 

(14) 

We  achieve  this  by  sequentially  determining  values  for 
in  a  particular order. 

of 

We  need  some  additional  terminology:  we  say  'ci,  is deter­
mined'  if  for  all  configurations  pai  of  Pai  we  have  already 
t.  We  say  V,  is  undetermined'  if we  have  de(cid:173)
determined 
for no  configuration  
termined 
.  We say  ci,  is 
ready  to  be  determined'  if  Ci  is  undetermined  and  at  the  same 
time  all  

with  mj  =  i  have  been  determined. 

We  note  that  as  long  as  some  ct  with  

{ 0 , . . ., A /}  arc 
that  are  ready  to  be  de(cid:173)
undetermined,  there  must  exist  ci 
termined.  To  see  this,  first  take  any  
{ 0 , . . .,  A /}  with 
ct  undetermined.  Either  ci  itself  is  ready  to  be  determined 
{ 1 , . ..  M) 
(in  which  case  we  are  done),  or  there  exists  
such  that  cj  is  undeter(cid:173)
with  mj  =  i  (and  hence  
If Cj  is  ready  to  be  determined,  we  are  done.  Oth(cid:173)
mined. 
erwise  we  repeat  the  argument,  move  forward  in  B  restricted 
and  (because  B  is  acyclic)  within  M  steps 
to 
surely  rind a C. that is  ready to be determined. 

We  now  describe  an  algorithm  that  sequentially  assigns 
such that (14) is satisfied.  We  start with  all  c, 

values  to 
undetermined and  repeat the  following steps: 

WHILE there exists 
DO 

, that is undetermined 

1.  Pick the largest i such that ci is ready to be determined. 
2.  Set,  for  all  configurations 
such  that 

of 

DONE 

LEARNING 

495 

The algorithm loops M + 1 times and then halts.  Step 2 does 
not affect the values  of 
has already been determined.  Therefore, after the algorithm 
halts, (14) holds. 

such that 

for any 

.  Each  choice of constants 

such  that  (14)  holds.  This  is  the  choice  of 

determines 
with components given by (11). 
a corresponding vector 
with com(cid:173)
This in turn determines a corresponding vector 
ponents given by (13). In Stage 2 we showed that we can take 
the 
which  we  adopt.  With  this  particular choice, 
indexes 
a  distribution  in  MB.  By  applying  the  log-transformation 
we  find  that  for  any  D  of  any 
to  the  components  of 
length, 
de(cid:173)
notes the conditional log-likelihood  of 
as given by sum(cid:173)
ming  the  logarithm  of (3).  The  result  of Stage  1  now  im(cid:173)
plies  that 
indexes  the  same  conditional  distribution  as 
was chosen arbitrarily, this shows that 
Together with Theorem  1  this concludes the 

, where 

proof. 

I 

Proof  (sketch)  of  Theorem  4.  Use  the  rightmost  network  in 
.  Let the data be 
Figure  1  with  structure 
. Note that X0  and 
always have the same value.  We first show that with this 
data, there are four local, non-connected suprema of the con(cid:173)
ditional likelihood. 

We  are  interested in  predicting the  value of  X0  given  X1, 
and  X2.  The parameter defining the distribution of X1i has no 
effect on conditional predictions and we can ignore it.  For the 
remaining five parameters we use the following notation: 

Therefore  setting  either 
to  0.5  results  in 
a  smaller  supremum  of  the  log-likelihood  than  the  above 
choices. Consequently, the four suprema are separated by ar(cid:173)
eas where the log-likelihood is smaller, i.e., the suprema are 
local and not connected. 

To  conclude  the  proof  we  still  need  to  address  two  is(cid:173)
sues:  (a)  the  four  local  suprema  give  the  same  conditional 
log-likelihood  -2 In 2,  and  (b),  they  are  suprema,  not  max(cid:173)
ima  (not  achieved  by  any 
.  To  deal  with  (a), 
consider  data  D'  consisting  of  n1  repetitions  of  (1,1,1), 
Ti2  repetitions  of  (1,1,2),  n3  repetitions  of  (2,2,1)  and 
n4  repetitions  of  (2,2,2).  By  doing  a  slightly  more  in(cid:173)
volved  analysis,  one  can  show  that,  for  some  choices  of 
n1,n2,n3,n4, 
the  supervised  log-likelihood  still  has  four 
suprema,  but they have different likelihood  values.  To deal 
with (b),  let  D"  be equal  to  D'  but with four extra data vec(cid:173)
tors  (1,2,1), (2,1,1), (1,2,2), (2,1,2).  If n i , n2, n3  and n4 
are chosen large enough, the supervised likelihood for D" has 
four maxima (rather than  suprema), not all of which achieve 
the same supervised likelihood.  We omit further details. 
I 

Lemma  1.  W i
between 0 and 1,  the supremum  of g(x, y, z)  defined by  (17) 
is given by 
(20) 

i x ed and y  and z  both  varying 

t h f

The conditional log-likelihood can be written as 

(15) 

Proof  Differentiating twice  wrt.  z gives 

Figure  3  illustrates  functions  g(x,y,z)  at  x  =  0.5.  In  (16) 
each parameter except 
appears only once. Thus, for a fixed 
we can maximize each term separately.  From Lemma 1 
below it follows that the supremum of the log-likelihood with 

value  -2 In 2 at  » = 0.5. Furthermore, the lemma shows that 
the  log-likelihood  approaches  its  supremum  when 

fixed is In 

which achieves its maximum 

X) 

Since in the  first  case the derivative is always negative, and 
in the second case the derivative is always positive, g(x, y, 0) 
increases  monotonically  as  y  -->  0,  and  g(x,y, 1)  increases 
monotonically as y  ->  1.  In both cases the limiting value is 
ln(x). 

I 

496 

LEARNING 


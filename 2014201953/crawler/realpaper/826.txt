A Convergent Solution to Tensor Subspace Learning

Huan Wang1

1

IE, Chinese University

of Hong Kong, Hong Kong

hwang5@ie.cuhk.edu.hk

Shuicheng Yan2,Thomas Huang2

2

ECE, University of Illinois

at Urbana Champaign, USA
{scyan,huang}@ifp.uiuc.edu

Xiaoou Tang 1,3

3

Microsoft Research Asia

Beijing, China

xitang@microsoft.com

Abstract

Recently, substantial efforts have been devoted to
the subspace learning techniques based on tensor
representation, such as 2DLDA [Ye et al., 2004],
DATER [Yan et al., 2005] and Tensor Subspace
Analysis (TSA) [He et al., 2005]. In this context,
a vital yet unsolved problem is that the computa-
tional convergency of these iterative algorithms is
not guaranteed. In this work, we present a novel so-
lution procedure for general tensor-based subspace
learning, followed by a detailed convergency proof
of the solution projection matrices and the objec-
tive function value. Extensive experiments on real-
world databases verify the high convergence speed
of the proposed procedure, as well as its superiority
in classiﬁcation capability over traditional solution
procedures.

1 Introduction
Subspace learning algorithms [Brand, 2003] such as Princi-
pal Component Analysis (PCA) [Turk and Pentland, 1991]
and Linear Discriminant Analysis (LDA) [Belhumeur et al.,
1997] traditionally express the input data as vectors and of-
ten in a high-dimensional feature space. In real applications,
the extracted features are usually in the form of a multidi-
mensional union, i.e. a tensor, and the vectorization process
destroys this intrinsic structure of the original tensor form.
Another drawback brought by the vectorization process is the
curse of dimensionality which may greatly degrade the algo-
rithmic learnability especially in the small sample size cases.
Recently substantial efforts have been devoted to the em-
ployment of tensor representation for improving algorith-
mic learnability [Vasilescu and Terzopoulos, 2003]. Among
them, 2DLDA [Ye et al., 2004] and DATER [Yan et al.,
2005] are tensorized from the popular vector-based LDA
algorithm. Although the initial objectives of these algo-
rithms are different, they all end up with solving a higher-
order optimization problem, and commonly iterative pro-
cedures were used to search for the solution. A collec-
tive problem encountered by their solution procedures is
that
the iterative procedures are not guaranteed to con-
verge, since in each iteration,
the optimization problem
is approximately simpliﬁed from the Trace Ratio form

p

p

S

S

kU k)/T r(U k T

arg maxU k T r(U kT
SkU k) 1 to the Ratio
Trace form arg maxU k T r[ (U k T
kU k) ] in
order to obtain a closed-form solution for each iteration. Con-
sequently, the derived projection matrices are unnecessary to
converge, which greatly limits the application of these algo-
rithms since it is unclear how to select the iteration number
and the solution is not optimal even in the local sense.

SkU k)−1(U k T

In this work, by following the graph embedding formula-
tion for general dimensionality reduction proposed by [Yan
et al., 2007], we present a new solution procedure for sub-
space learning based on tensor representation.
In each it-
eration, instead of transforming the objective function into
the ratio trace form, we transform the trace ratio optimiza-
tion problem into a trace difference optimization problem
maxU k T r[U k T
k −λSk)U k] where λ is the objective func-
tion value computed from the solution (U k|n
k=1) of the pre-
vious iteration. Then, each iteration is efﬁciently solved with
the eigenvalue decomposition method [Fukunaga, 1991]. A
detailed proof is presented to justify that λ, namely the value
of the objective function, will increase monotonously, and
also we prove that the projection matrix U k
will converge to
a ﬁxed point based on the point-to-set map theories [Hogan,
1973].

(S

p

It is worthwhile to highlight some aspects of our solution
procedure to general subspace learning based on tensor rep-
resentation here:

1. The value of the objective function is guaranteed to
monotonously increase; and the multiple projection ma-
trices are proved to converge. These two properties en-
sure the algorithmic effectiveness and applicability.

2. Only eigenvalue decomposition method is applied for
iterative optimization, which makes the algorithm ex-
tremely efﬁcient; and the whole algorithm does not suf-
fer from the singularity problem that is often encoun-
tered by the traditional generalized eigenvalue decompo-
sition method used to solve the ratio trace optimization
problem.

3. The consequent advantage brought by the sound theo-
retical foundation is the enhanced potential classiﬁcation

1Matrices S

p

k and Sk

are both positive semideﬁnite and more

detailed deﬁnitions are described afterward.

IJCAI-07

629

capability of the derived low-dimensional representation
from the subspace learning algorithms.

The rest of this paper is organized as follows. Section II
reviews the general subspace learning based on tensor rep-
resentation, and then we introduce our new solution proce-
dure along with the theoretical convergency proof in section
III. By taking the Marginal Fisher Analysis (MFA) algorithm
proposed in [Yan et al., 2007] as an example, we verify the
convergency properties of the new proposed solution pro-
cedure and the classiﬁcation capability of the derived low-
dimensional representation is examined with a set of experi-
ments on the real-world databases in Section IV.

2 Subspace Learning with Tensor Data

In this section, we present a general subspace learning frame-
work by encoding data as tensors of arbitrary order, extended
from the one proposed by [Yan et al., 2007] and taking the
data inputs as vectors. The concepts of tensor inner produc-
tion, mode-k production with matrix, and mode-k unfolding
are referred to the work of [Yan et al., 2005].

2.1 Graph Embedding with Tensor Representation
Denote the sample set as X = [X1, X2, . . . , XN ], Xi ∈
Rm1×m2×...×mn , i = 1, . . . N , with N as the total number of
samples. Let G = {X, S} be an undirected similarity graph,
called an intrinsic graph, with vertex set X and similarity ma-
trix S ∈ RN ×N
. The corresponding diagonal matrix D and
the Laplacian matrix L of the graph G are deﬁned as

(cid:2)

L = D − S, Dii =

Sij ∀ i.

(1)

j(cid:2)=i

The task of graph embedding is to determine a low-
dimensional representation of the vertex set X that preserves
the similarities between pairs of data in the original high-
dimensional feature space. Denote the low-dimensional em-
bedding of the vertices as Y = [Y1, Y2, . . . , YN ], where
Yi ∈ Rm(cid:2)
n is the embedding for the vertex Xi,
with the assumption that Yi is the mode-k production of Xi
with a series of column orthogonal matrices U k ∈ Rmk×m(cid:2)
k ,

×...×m(cid:2)

×m(cid:2)

1

2

Yi = Xi ×1 U

1 ×2 U

2

. . . ×n U n, U k T

U k = Im(cid:2)
k ,

(2)

k is an m(cid:3)

k-by-m(cid:3)

where Im(cid:2)
k identity matrix. To maintain sim-
ilarities among vertex pairs according to the graph preserving
criterion [Yan et al., 2007], we have

(cid:3)

(U k)

∗|n

i(cid:2)=j (cid:4) Yi − Yj (cid:4)2 Sij

k=1 = arg min
(cid:3)

f (U k|n
i(cid:2)=j (cid:4) (Xi − Xj) ×k U k|n

U k|n

k=1

k=1)
k=1 (cid:4)2 Sij

= arg min

U k|n

k=1

f (U k|n

k=1)

(3)

,

(4)

where f (U k|n
k=1) is a function that poses extra constraint for
the graph similarity preserving criterion. Here U k|n
k=1 means
the sequence U 1
and so for the other similar rep-
resentations in the following parts of this work. Commonly,

to U n

, U 2

f (Uk|n
normalization, that is,

k=1) may have two kinds of deﬁnitions. One is for scale

f (Uk|n

k=1) =

N(cid:2)

i=1

(cid:4)Xi ×k U k|n

k=1(cid:4)2

Bii,

(5)

where B is a diagonal matrix with non-negative elements.
The other is a more general constraint which relies on a new
graph, referred to as penalty graph with similarity matrix Sp
,
and is deﬁned as

f (U k|n

k=1) =

(cid:4)(Xi − Xj) ×k U k|n

k=1(cid:4)2

p
ij.

S

(6)

(cid:2)

i(cid:2)=j

Without losing generality, we assume that the constraint
function is deﬁned with penalty matrix for simplicity; and for
scale normalization constraint, we can easily have the similar
deduction for our new solution procedure. Then, the gen-
eral formulation of the tensor-based subspace learning is ex-
pressed as

(cid:3)
(cid:3)

arg max
U k|n

k=1

i(cid:2)=j (cid:4) (Xi − Xj) ×k U k|n
i(cid:2)=j (cid:4) (Xi − Xj) ×k U k|n

k=1 (cid:4)2 S
k=1 (cid:4)2 Sij

p
ij

.

(7)

Recent studies [Shashua and Levin, 2001] [Ye, 2005] [Ye
et al., 2004] [Yan et al., 2005] have shown that dimensional-
ity reduction algorithms with data encoded as high-order ten-
sors usually outperform those with data represented as vec-
tors, especially when the number of training samples is small.
Representing images as 2D matrices instead of vectors allows
correlations between both rows and columns to be exploited
for subspace learning.

Generally, no closed-form solution exists for (7). Previ-
ous works [Ye et al., 2004] [Yan et al., 2005] utilized iter-
ative procedures to search for approximate solutions. First,
the projection matrices U 1, . . . , U n
are initialized arbitrarily;
then each projection matrix U k
is reﬁned by ﬁxing the other
projection matrices U 1, . . . , U k−1, U k+1, . . . , U n
and solv-
ing the optimization problem:

U k∗

= arg max
U k

= arg max
U k

(cid:3)
(cid:3)

p
ij

i(cid:2)=j (cid:4)U kT
i − U kT
j (cid:4)2S
Y k
Y k
i(cid:2)=j (cid:4)U kT Y k
j (cid:4)2Sij
i − U kT Y k
kU k)
S
SkU k)

T r(U k T
T r(U k T

p

(8)

(9)

j )(Y k

i − Y k

where Y k
i
(cid:3)
Xi ×1 U 1 . . . ×k−1 U k−1 ×k+1 U k+1 . . . ×n U n

is the mode-k unfolding matrix of the tensor ˜Yi =
and Sk =
p
i −
ij (Y k

i − Y k
j )T

i(cid:2)=j Sij(Y k
i − Y k
Y k
j )(Y k
The optimization problem in (9) is still intractable, and tra-
ditionally its solution is approximated by transforming the
objective function in (9) into a more tractable approximate
form, namely, Ratio Trace form,

j )T , S

p
k =

i(cid:2)=j S

(cid:3)

.

U k∗

= arg max
U k

T r((U k T

SkU k)

−1

(U k T

p

kU k))

S

(10)

which can be directly solved with the generalized eigenvalue
decomposition method. However, this distortion of the objec-
tive function leads to the computational issues as detailed in
the following subsection.

IJCAI-07

630

2.2 Computational Issues

As the objective function in each iteration is changed from the
trace ratio form (9) to the ratio trace form (10), the deduced
solution can satisfy neither of the two aspects: 1) the objective
function value in (7) can monotonously increase; and 2) the
solution (U 1, U 2, . . . , U n
) can converge to a ﬁxed point. In
this work, we present a convergent solution procedure to the
optimization problem deﬁned in (7).

3 Solution Procedure and Convergency Proof

In this section, we ﬁrst introduce our new solution procedure
to the tensor-based subspace learning problems, and then give
the convergency proof to the two aspects mentioned above.

As described above, there does not exist closed-form solu-
tion for the optimization problem (7), and we solve the op-
timization problem also in an iterative manner. For each it-
eration, we reﬁne one projection matrix by ﬁxing the others
and an efﬁcient method is proposed for this reﬁnement. In-
stead of solving a ratio trace optimization problem (10) for an
approximate solution, we transform the trace ratio optimiza-
tion problem (9) into a trace difference optimization problem
deﬁned as

U k∗

= arg max
U k

T r(U k T

(S

p

k − λSk)U k),

(11)

where λ is the value of objective function (7) computed from
the projection matrices of the previous iteration.

Though the iterative procedure may converge to a local op-
timum for the optimization problem (7), it can monotonously
increase the objective function value as proved later, which
directly leads to its superiority over the ratio trace based opti-
mization procedure, since the step-wise solution of the latter
is unnecessarily optimal for (9).

We iteratively reﬁne the projection matrices, and the de-
tailed solution procedure to solve the tensor-based general
subspace learning problem is listed in Algorithm 1.

3.1 Analysis of Monotonous Increase Property

Rewrite the objective function of (7) as

(cid:3)

G(U k|n

k=1) =

i(cid:2)=j

(cid:3)

i(cid:2)=j

(cid:4)(Xi − Xj) ×k U k|n

k=1

(cid:4)(Xi − Xj) ×k U k|n

k=1

(cid:4)2S

p
ij

,

(cid:4)2Sij

(13)

and then we have the theory as below:

Theorem-1. By following the terms in Algorithm-1 and

Eqn. (13), we have
t , . . . , U k−1
1
G(U

G(U

t

t−1, U k+1
, U k
t , . . . , U k−1
1

t

t−1 . . . , U n

t−1) ≤
t−1 . . . , U n

t , U k+1

, U k

t−1). (14)

Proof. Denote g(U ) = T r(U T (S

p

k − λSk)U ) where

λ = G(U

t , . . . , U k−1
1

t

, U k

t−1, U k+1

t−1 . . . , U n

t−1),

then we have

g(U k

t−1) = 0.

Algorithm 1 . Procedure to Tensor-based Subspace Learning
1: Initialization. Initialize U 1

0 as arbitrary col-

0 , . . . , U n

0 , U 2

umn orthogonal matrices.

2: Iterative optimization.

For t=1, 2, . . . , Tmax, Do

For k=1, 2, . . . , n, Do

1. Set λ =

(cid:3)
(cid:3)

i(cid:4)=j

i(cid:4)=j

(cid:5)(Xi−Xj )×oU o

(cid:5)(Xi−Xj )×oU o
t

t |k-1
o=1
|k-1
o=1

×oU o

t-1

×oU o

t-1

|n
o=k
|n
o=k

(cid:5)2Sp
ij

(cid:5)2Sij

.

2. Compute Sk
matrices U 1

and S

p
k as in (9) based on the projection

t , . . . , U k−1

t

and U k+1

t−1 , . . . , U n

t−1.

3. Conduct Eigenvalue Decomposition:

(S

p

(cid:3)
k − λSk)vj = λj vj, j = 1, . . . , m
k,

where vj is the eigenvector corresponding to the j-
th largest eigenvalue λj .

4. Reshape the projection directions for the sake of or-

thogonal transformation invariance:

(a) Set V = [v1, v2, . . . , vm(cid:2)

(cid:3)

k ];
i X k
i X k
i

T

(b) Let Sv = V V T (

, where
i is the mode-k unfolding of the tensor Xi;

X k

)V V T

(c) Conduct Eigenvalue Decomposition as

Svui = γi ui.

(12)

5. Set the column vectors of matrix U k

t as the leading

eigenvectors, namely, U k

t = [u1, u2, . . . , um(cid:2)

k].

(cid:4)

End
If (cid:4)U k
to 10−4

t − U k

t−1(cid:4) <

mkm(cid:3)

k ε, k = 1, 2, . . . , n (ε is set

in this work), then break.

End

3: Output the projection matrices U k

=U k

t , k=1, 2, . . . , n.

Moreover, from U T U = Im(cid:2)

k , it is easy to prove that

k(cid:2)
m(cid:2)

sup g(U ) =

λj .

j=1

From Algorithm 1, we have g(U k

t ) =

(cid:3)m(cid:2)

k

j=1 λj , and hence

g(U k

t ) ≥ g(U k

t−1) = 0.

Then, T r(U k
t

T

(S

p

k − λSk)U k

t ) ≥ 0. As matrix Sk

is pos-

itive semideﬁnite 2, we have
T
T r(U k
t
T r(U k
t

T

p

kU k
t )
S
SkU k
t )

≥ λ,

that is,

G(U o

t |k−1

o=1 , U o

t−1|n

o=k) ≤ G(U o

t |k

o=1, U o

t−1|n

o=k+1)

2Though Sk

positive when m(cid:2)

may have zero eigenvalues, T r(U k
t ) will be
k is larger than the number of the zero eigenvalues.

SkU k

t

T

IJCAI-07

631

 

l

e
u
a
V
n
o
i
t
c
n
u
F
e
v
i
t
c
e
b
O

j

 

1.4
1.3
1.2
1.1
1
0.9
0.8
0.7

0

5

Ours
Traditional

15

10
30
Iteration Number

20

25

35

40

 

l

e
u
a
V
n
o
i
t
c
n
u
F
e
v
i
t
c
e
b
O

j

 

9.5

9

8.5

8

7.5

7

6.5
0

5

Ours
Traditional

15

10
30
Iteration Number

20

25

35

40

 

l

e
u
a
V
n
o
i
t
c
n
u
F
e
v
i
t
c
e
b
O

j

 

13

12

11

10

9

8

7

6

0

5

Ours
Traditional

15

10
30
Iteration Number

20

25

35

40

(a)

(b)

(c)

Figure 1: The value of the objective function (7) vs. iteration number. (a) USPS database (b) ORL database, and (c) CMU PIE
database. Here the traditional method means the solution procedure based ratio trace optimization.

From theorem-1, we can conclude that the value of the ob-

jective function monotonously increases.

3.2 Proof of Convergency

the projection matrices
To prove the convergency of
U 1, U 2, . . . , U n
, we need the concept of point-to-set map.
The power set ℘(χ) of a set χ is the collection of all sub-
sets of χ. A point-to-set map Ω is a function: χ → ℘(χ).
In our solution procedure to tensor-based subspace learning,
the map from (U k
k=1) to (U k
t |n
k=1) can be considered as
a point-to-set map, since each U k
is invariant under any or-
t
thogonal transformation.

t−1|n

Strict Monotony. An algorithm is a point-to-set map
Ω:χ → ℘(χ). Given an initial point x0, an algorithm gen-
erates a sequence of points via the rule that xt ∈ Ω(xt−1).
Suppose J : χ → R+ is continuous, non-negative function,
an algorithm is called strict monotony if 1) y ∈ Ω(x) implies
that J(y) ≥ J(x), and 2) y ∈ Ω(x) and J(y) = J(x) imply
that y = x.

(cid:5)

Let set χ be the direct sum of the orthogonal ma-
trix space Omk×m(cid:2)
(cid:5)
the data space χ =
k ,
Om1×m(cid:2)
Om2×m(cid:2)
then the Algo-
rithm 1 produces a point-to-set algorithm with respect to
J(x) = G(U k|n
k=1), and it can be proved to be strictly mono-
tonic as follows.

is,
Omn×m(cid:2)
n ,

that
. . .

(cid:5)

2

1

Theorem-2. The point-to-set map from Algorithm 1 is

strictly monotonic.

t |n

t−1|n

t−1|n

Proof.
t |n

k=1) = G(U k

From theorem-1, we have G(U k

k=1) ≤
G(U k
k=1), and hence the ﬁrst condition for strict monotony
is satisﬁed. For the second condition, we take U 1
as an
If
example to prove that this condition is also satisﬁed.
G(U k
k=1), then from the proof of theorem-
1, we have g(U 1
k=1)
and Sk, S
k=1). From the proof of
theorem-1, we can have that there only exists one orthogo-
nal transformation3 between U 1
t . As shown in Al-
gorithm 1, this kind of orthogonal transformation has been
normalized by the reshaping step, hence we have U 1
t−1=U 1
t .
Similarly, we can prove that U k
t−1 for k = 1, 2, . . . , n,

t−1) = g(U 1
k computed from (U k

t ) with λ = G(U k
t−1|n

t−1 and U 1

t = U k

t−1|n

p

hence the second condition is also satisﬁed and the Algo-
rithm 1 is strictly monotonic.

Theorem-3 [Meyer, 1976]. Assume that the algorithm Ω
is strictly monotonic with respect to J and it generates a se-
quence {xt} which lies in a compact set. If χ is normed, then
(cid:4)xt − xt−1(cid:4) → 0.

From theorem-3, we can have the conclusion that the ob-
k=1) will converge to a local optimum, since the

tained (U k
χ is compact and with norm deﬁnition.

t |n

4 Experiments

In this section, we systematically examine the convergency
properties of our proposed solution procedure to tensor-based
subspace learning. We take the Marginal Fisher Analysis
(MFA) as an instance of general subspace learning, since
MFA has shown to be superior to many traditional subspace
learning algorithms such as Linear Discriminant Analysis
(LDA); more details on the MFA algorithm is referred to [Yan
et al., 2007]. Then, we evaluate the classiﬁcation capability of
the derived low-dimensional representation from our solution
procedure compared with the traditional procedure proposed
in [Ye et al., 2004] and [Yan et al., 2005]. For tensor-based
algorithm, the image matrix, 2rd tensor, is used as input, and
the image matrix is transformed into the corresponding vector
as the input of vector-based algorithms.

4.1 Data Sets

Three real-world data sets are used. One is the USPS hand-
written dataset 4 of 16-by-16 images of handwritten digits
with pixel values ranging between -1 and 1. The other two
are the benchmark face databases, ORL and CMU PIE 5. For
the face databases, afﬁne transform is performed on all the
samples to ﬁx the positions of the two eyes and the mouth
center. The ORL database contains 400 images of 40 per-
sons, where each image is normalized to the size of 56-by-46
pixels. The CMU PIE (Pose, Illumination, and Expression)
database contains more than 40,000 facial images of 68 peo-
ple.
In our experiment, a subset of ﬁve near frontal poses

4Available at: http://www-stat-class.stanford.edu/ tibs/ElemStat-

3This claim is based on the assumption that there do not exist

Learn/data.html

duplicated eigenvalues in (11).

5Available at http://www.face-rec.org/databases/.

IJCAI-07

632

i

e
c
n
e
r
e
ff
D
n
o
i
t
c
e
o
r
P

j

 

i

e
c
n
e
r
e
ff
D
n
o
i
t
c
e
o
r
P

j

 

6

5

4

3

2

1

0
0

7

6

5

4

3

2

1

0
0

Ours
Traditional

10

5
15
Iteration Number
t−1(cid:2)

t − U 1

(a) (cid:2)U 1

Ours
Traditional

10

5
15
Iteration Number
t−1(cid:2)

t − U 2

(d) (cid:2)U 2

20

20

i

e
c
n
e
r
e
ff
D
n
o
i
t
c
e
o
r
P

j

 

25

20

15

10

5

0

0

i

e
c
n
e
r
e
ff
D
n
o
i
t
c
e
o
r
P

j

 

10
9
8
7
6
5
4
3
2
1
0
0

Ours
Traditional

10

5
15
Iteration Number
t−1(cid:2)

t − U 1

(b) (cid:2)U 1

Ours
Traditional

10

5
15
Iteration Number
t−1(cid:2)

t − U 2

(e) (cid:2)U 2

20

20

i

e
c
n
e
r
e
ff
D
n
o
i
t
c
e
o
r
P

j

 

i

e
c
n
e
r
e
ff
D
n
o
i
t
c
e
o
r
P

j

 

9
8
7
6
5
4
3
2
1
0
0

12

10

8

6

4

2

0

0

Ours
Traditional

10

5
15
Iteration Number
t−1(cid:2)

t − U 1

(c) (cid:2)U 1

Ours
Traditional

10

5
15
Iteration Number
t−1(cid:2)

t − U 2

(f) (cid:2)U 2

20

20

Figure 2: The step difference of the projection matrices vs. iteration number. (a,d) USPS database, (b,e) ORL database, and
(c,f) CMU PIE database.

(C27, C05, C29, C09 and C07) and illuminations indexed as
08 and 11 are used and normalized to the size of 32-by-32.

Table 1: Recognition error rates (%) on the ORL database.

4.2 Monotony of Objective Function Value

In this subsection, we examine the monotony property of the
objective function value from our solution procedure com-
pared with the optimization procedure that step-wisely trans-
forms the objective function into the ratio trace form. The
USPS, ORL and PIE databases are used for this evalua-
tion. The detailed results are shown in Figure 1.
It is ob-
served that the traditional ratio trace based procedure does
not converge, while our new solution procedure guarantees
the monotonous increase of the objective function value and
commonly our new procedure will converge after about 4-10
iterations. Moreover, the ﬁnal converged value of the objec-
tive function from our new procedure is much larger than the
value of the objective function for any iteration of the ratio
trace based procedure.

4.3 Convergency of the Projection Matrices

To evaluate the solution convergency property compared with
the traditional ratio trace based optimization procedure, we
calculate the difference norm of the projection matrices from
two successive iterations and the detailed results are dis-
played in Figure 2. It demonstrates that the projection matri-
ces converge after 4-10 iterations for our new solution pro-
cedure; while for the traditional procedure, heavy oscilla-
tions exist and the solution does not converge. As shown in
Figure 3, the recognition rate is sensitive to the oscillations
caused by the unconvergent projection matrices and the clas-
siﬁcation accuracy is degraded dramatically.

Method

G3P7 G4P6 G5P5

w/o DR.

LDA

MFA RT
MFA TR
TMFA RT
TMFA TR

28.57
17.86
17.50
13.93
12.14
11.07

24.17
17.08
16.25
10.00
11.67
6.67

21.5
11.00
10.50
6.50
5.00
4.00

Table 2: Recognition error rates (%) on the PIE database.

Method

G3P7 G4P6 G5P5

w/o DR.

LDA

MFA RT
MFA TR
TMFA RT
TMFA TR

49.89
18.82
16.55
14.97
14.74
13.61

31.75
19.84
15.61
13.49
14.29
12.17

30.16
18.10
13.65
9.52
3.81
9.52

4.4 Face Recognition

In this subsection, we conduct classiﬁcation experiments on
the benchmark face databases. The Tensor Marginal Fisher
Analysis algorithm based on our new solution procedure
(TMFA TR) is compared with the traditional ratio trace based
Tensor Marginal Fisher Analysis (TMFA RT), LDA, Ratio
Trace based MFA (MFA RT) and Trace Ratio based MFA
(MFA TR), where MFA TR means to conduct tensor-based
MFA by assuming n=1. To speed up model training, PCA is
conducted as a preprocess step for vector-based algorithms.
The PCA dimension is set as N -Nc (N is the sample number

IJCAI-07

633

)

%

 

(
 
e
t
a
R
 
r
o
r
r
E
n
o
i
t
i
n
g
o
c
e
R

15
14
13
12
11
10
9
8
7
6

0

in

Ours
Traditional

5
15
Iteration Number

10

20

(a)

)

%

 

(
 
e
t
a
R
 
r
o
r
r
E
n
o
i
t
i
n
g
o
c
e
R

21
20
19
18
17
16
15
14
13
12
0

Ours
Traditional

5
15
Iteration Number

10

20

(b)

Figure 3: Recognition error rate (%) vs. iteration number. (a) ORL database(G4P6), and (b) CMU PIE database(G4P6).

and Nc is the class number), which is equivalent to the case
for Fisherface algorithm [Belhumeur et al., 1997]. The same
graph conﬁguration with nearest neighbor k = 3 for the in-
trinsic graph and kp = 40 for the penalty graph is adopted
for all the MFA based algorithms. Since the traditional tensor
subspace learning algorithms do not converge, we terminate
the process after 3 iterations.

For comparison, the classiﬁcation result on the original
gray-level features without dimensionality reduction is also
reported as the baseline, denoted as ’w/o DR.’ in the result
tables. In all the experiments, the Nearest Neighbor method
is used for ﬁnal classiﬁcation. All possible dimensions of the
ﬁnal low-dimensional representation are evaluated, and the
best results are reported. For each database, we test various
conﬁgurations of training and testing sets for the sake of sta-
for which x images of
tistical conﬁdence, denoted as
each subject are randomly selected for model training and the
remaining y images of each subject are used for testing. The
detailed results are listed in Table 1 and 2. From these results,
we can have the following observations:

(cid:3)GxP y(cid:3)

1. TMFA TR mostly outperforms all the other methods
concerned in this work, with only one exception for the
case G5P 5 on the CMU PIE database.

2. For vector-based algorithms, the trace ratio based for-
mulation (MFA TR) is consistently superior to the ratio
trace based one (MFA RT) for subspace learning.

3. Tensor representation has the potential to improve the
classiﬁcation performance for both trace ratio and ratio
trace formulations of subspace learning.

5 Conclusions

In this paper, a novel iterative procedure was proposed to
directly optimize the objective function of general subspace
learning based on tensor representation. The convergence of
the projection matrices and the monotony property of the ob-
jective function value were proven. To the best of our knowl-
edge, it is the ﬁrst work to give a convergent solution for gen-
eral tensor-based subspace learning.

6 Acknowledgement
The work described in this paper was supported by grants
from the Research Grants Council of the Hong Kong Special
Administrative Region and DTO Contract NBCHC060160 of
USA.

References
[Belhumeur et al., 1997] P. Belhumeur, J. Hespanha, and D. Krieg-
man. Eigenfaces vs. ﬁsherfaces: Recognition using class spe-
ciﬁc linear projection. IEEE TPAMI, pages 711–720, 1997.

[Brand, 2003] M. Brand. Continuous nonlinear dimensionality re-
duction by kernel eigenmaps. International Join Conference
on Artiﬁcial Intelligence (IJCAI), 2003.

[Fukunaga, 1991] K. Fukunaga.

Introduction to statistical pattern

recognition. Academic Press, second edition, 1991.

[He et al., 2005] X. He, D.Cai, and P. Niyogi. Tensor subspace
analysis. Advances in Neural Information Processing Systems,
2005.

[Hogan, 1973] W. Hogan. Point-to-set maps in mathematical pro-

gramming. SIAM Rev., 15(3):591–603, 1973.

[Meyer, 1976] R. Meyer.

Sufﬁcient conditions for the conver-
gence of monotonic mathematical programming algorithms. J.
Comp. Sys. Sci., 12:108–121, 1976.

[Shashua and Levin, 2001] A. Shashua and A. Levin. Linear image
coding for regression and classiﬁcation using the tensor-rank
principle. Proceedings of CVPR, 2001.

[Turk and Pentland, 1991] M. Turk and A. Pentland. Face recogni-

tion using eigenfaces. Proceedings of CVPR, 1991.

[Vasilescu and Terzopoulos, 2003] M. Vasilescu and D. Terzopou-
los. Multilinear subspace analysis for image ensembles. Pro-
ceedings of CVPR, 2003.

[Yan et al., 2005] S. Yan, D. Xu, Q. Yang, L. Zhang, X. Tang, and
H. Zhang. Discriminant analysis with tensor representation.
Proceedings of CVPR, 2005.

[Yan et al., 2007] S. Yan, D. Xu, B. Zhang, H. Zhang, Q. Yang, and
S. Lin. Graph embedding and extension: A general framework
for dimensionality reduction. IEEE TPAMI, 2007.

[Ye et al., 2004] J. Ye, R. Janardan, and Q. Li. Two-dimensional
linear discriminant analysis. Advances in Neural Information
Processing Systems, 2004.

[Ye, 2005] J. Ye. Generalized low rank approximations of matrices.

Machine Learning Journal, 2005.

IJCAI-07

634


Augmented Experiment:  

Participatory Design with Multiagent Simulation 

Toru Ishida†, Yuu Nakajima†, Yohei Murakami‡, Hideyuki Nakanishi§ 

†Department of Social Informatics, Kyoto University 

‡The National Institute of Information and Communications Technology 

§Department of Adaptive Machine Systems, Osaka University 

ishida@i.kyoto-u.ac.jp, nkjm@ai.soc.i.kyoto-u.ac.jp, yohei@nict.go.jp, nakanishi@ams.eng.osaka-u.ac.jp 

Abstract 

To test large scale socially embedded systems, this paper 
proposes  a  multiagent-based  participatory  design  that 
consists of two steps; 1) participatory simulation, where 
scenario-guided  agents  and  human-controlled  avatars 
coexist  in  a  shared  virtual  space  and  jointly  perform 
simulations,  and  the  extension  of  the  participatory 
simulation into the 2) augmented experiment, where an 
experiment is performed in real space by human subjects 
enhanced  by  a  large  scale  multiagent  simulation.  The 
augmented experiment, proposed in this paper, consist of 
1) various sensors to collect the real world activities of 
human subjects and project them into the virtual space, 2) 
multiagent  simulations  to  simulate  human  activities  in 
the virtual space, and 3) communication channels to in-
form  simulation  status  to  human  subjects  in  the  real 
space.  To  create  agent  and  interaction  models  incre-
mentally from the participatory design process, we pro-
pose  the  participatory  design  loop  that  uses  deductive 
machine  learning  technologies.  Indoor  and  outdoor 
augmented experiments have been actually conducted in 
the city of Kyoto. Both experiments were intended to test 
new  disaster  evacuation  systems  based  on  mobile 
phones.  

1  Introduction 
Ubiquitous/pervasive computing systems in the public space 
often interact  with anonymous users.  We propose to apply 
multiagent technologies to the design of those systems that 
are to be embedded in society.  

Participatory simulations have been already studied inten-
sively for modeling human societies [Drogoul et al., 2002, 
Guyot  et  al.,  2005].  For  designing  socially  embedded  sys-
tems, however, simulations in virtual space cannot reproduce 
the reality of the actual application environment. The goal of 
this  paper  is  to  introduce  real  world  experiments  to  the 
process of participatory design, so as to bridge the gap be-
tween  participatory  simulations  and  services  in  operation. 
We propose multiagent-based participatory design method-
ologies (participatory design hereafter) to test socially em-
bedded systems in the following steps. 

1.  Describe interactions between human users and socially 
embedded systems so as to define the expected user be-
haviors  with  socially  embedded  systems  (interaction 
model hereafter).  
2.  Perform a multiagent-based simulation by modeling users 
under the given interaction scenarios (agent model here-
after).  The  simulation  takes  place  in  virtual  space  and 
involves a large number of simulated users. Results of the 
simulation can predict how the entire system would work 
in society.  
3.  Replace some of the simulated users by human-controlled 
avatars  to  perform  a  multiagent-based  participatory 
simulation  (participatory  simulation  hereafter).  The 
simulation is performed in virtual space, and the avatars 
are controlled by human subjects sitting in front of their 
personal computers.  
4.  Perform  experiments  in  real  space  to  try  out  the  entire 
system  with  human  subjects.  Since  the  number  of  sub-
jects  is  often  limited,  the  experiment  should  be  aug-
mented by a large number of simulated users in virtual 
space.  We  called  this  the  multiagent-based  augmented 
experiment (augmented experiment hereafter). 

To learn from the participatory design process, this paper 
separates agent models from interaction models: the former 
covers the beliefs, desires, intentions, and emotions of human 
users, and the latter covers protocols, methods, rules, or laws 
that guide users when interacting with the socially embedded 
systems. We use extended finite state automata for describing 
interaction  models,  while  production  rules  are  used  to  de-
scribe  agent  models  for  approximating  users.  We  view  in-
teraction  models  as  behavioral  guidelines  of  human  users 
playing  with  socially  embedded  systems;  users  keep 
autonomy  within  the  given  guidelines.  The  question  is 
whether or not users will follow the guidelines in an actual 
service  environment.  In  other  words,  the  effectiveness  of 
interaction models depends on the agent models, which in-
clude user personalities such as deliberative and reactive. 
 
In this paper, we call the descriptions of interaction models 
scenarios.  We  use  scenario  description  language  Q,  which 
can simultaneously interpret scenarios for a large number of 
agents  [Ishida,  2002b].  Q  has  been  already  connected  to 
FreeWalk [Nakanishi et al., 2004b]  for  virtual city experi-

IJCAI-07

1341

ments [Ishida, 2002a], Cormas for a natural resource simu-
lation [Bousquet et al., 1998], and Caribbean [Yamamoto et 
al., 1999] for massively multiagent simulations [Ishida et al., 
2005]. Q, FreeWalk, and Caribbean have been used to con-
duct  indoor  and  outdoor  disaster  evacuation  experiments. 
Those  real-scale  experiments  confirm  the  feasibility  and 
usefulness of augmented experiments. 

2  Participatory Simulation 
In  our  participatory  design  methodology,  we  first  conduct 
multiagent-based  simulations.  Figure  1(a)  illustrates  how  a 
multiagent-based simulation is realized. The scenario proc-
essor  interprets  interaction  models  and  requests  agents  in 
virtual space to perform sensing and acting functions. Note 
that, since agents are autonomous and have their own agent 
models,  though  agents  receive  instruction  according  to  the 
scenarios, there is no guarantee that they will behave as re-
quested.  
  We  can  easily  extend  multiagent-based  simulations  to 
yield  participatory  simulations  by  replacing  some  of  the 
scenario-guided agents with human-controlled avatars. Fig-
ure 1(b) illustrates how human subjects and agents can co-
operatively perform a simulation. Just as with video games, 
human subjects can join the simulation by controlling avatars 
via  joy  sticks,  mice,  or  other  input  devices.  To  analyze 
simulation  results,  we  monitor  the  entire  process  of  the 
simulation  by  visualizing  the  virtual  space.  In  addition  to 
video  taping  the  virtual  space,  we  record  how  the  human 
subjects control their avatars. Recording human behavior is 
useful for analyzing the simulation results and for improving 
the agent and interaction models. 
 

Scenario
Scenario

Real Space
Real Space

Scenario
Scenario

Agent
Agent

Virtual Space
Virtual Space

Virtual Space
Virtual Space

Human
Human
Subject
Subject

Monitor
Monitor

Avatar
Avatar

b) Multiagent-Based
b) Multiagent-Based

Participatory Simulation
Participatory Simulation

 

Figure 1. Participatory Simulation 

a) Multiagent-Based
a) Multiagent-Based

Simulation
Simulation

 

In  summary,  a  participatory  simulation  consists  of  1) 
agents  for  modeling  users,  2)  avatars  to  represent  human 
subjects,  3)  scenarios  for  modeling  interactions,  4)  human 
subjects to control avatars, 5) virtual space to represent real 
space, and 6) a monitor to visualize simulations ongoing in 
the virtual space. 

We conducted a multiagent-based simulation in the disaster 
evacuation domain. To establish the process of refining agent 
models  and  interaction  models,  we  simulated  the  precisely 
controlled experiments conducted by Sugiman [Sugiman et 
al., 1998]. He developed a simple environment with human 
subjects  to  determine  the  effectiveness  of  two  evacuation 
methods  (i.e.,  interaction  models):  the  Follow-direction 

method and the Follow-me method. In the former, the leader 
shouts  out  evacuation  instructions  and  eventually  moves 
toward  the  exit.  In  the  latter,  the  leader  tells  a  few  of  the 
nearest evacuees to follow him and actually proceeds to the 
exit without verbalizing the direction of the exit.  

A participatory simulation was performed in which we re-
placed twelve out of twenty scenario-guided agents by hu-
man-controlled  avatars.  The  virtual  space  was  designed  so 
that  the  human  subjects  could  not  distinguish  agents  from 
avatars. After the simulation, we applied deductive machine 
learning  technology  with  domain  knowledge  to  refine  the 
agent models from the logs of the participatory simulation, 
and  successfully  modified  the  agent  models  and  original 
evacuation scenarios (i.e., interaction models). 

Once  an  accurate  model  is  acquired  from  participatory 
simulations, it becomes possible to simulate experiments that 
cannot be conducted in real space. For example, Sugiman’s 
experiment was performed using two or four leaders, but we 
can  vary  this  number  to  clarify  the  relation  between  the 
number  of  leaders  and  the  time  required  for  evacuation. 
Moreover,  we  can  explore  an  effective  combination  of  the 
Follow-me  and  Follow-direction  methods.  Given  accurate 
and explainable agent models, even though the models can 
only capture a part of the variations possible in human be-
havior,  participatory  simulations  must  be  seen  as  useful  in 
educating or training people. People can be provided with a 
learning environment wherein evacuations or other drills can 
be experienced.   

3  Augmented Experiment 
Participatory  simulations  are  particularly  useful  when  con-
ducting controlled experiments: they make it easy to prepare 
the application environments for testing, and user behavior 
can be recorded for later analysis. However, it is sometimes 
fails to provide enough reality to allow the testing of ubiq-
uitous/pervasive computing environments. 

To understand how users behave with socially embedded 
systems,  real-world  experiments  are  often  required.  In  the 
case  of  video  phones,  for  example,  since  user  behavior  in 
everyday life is not easy to simulate, it is essential to observe 
how individual users employ their video phones in real space. 
In ubiquitous/pervasive computing, however, because a large 
number  of  electronic  devices  will  be  embedded  in  public 
spaces like railway stations and are in continuous service, it 
is desired but often impossible to conduct experiments with a 
large  number  of  human  subjects.  Augmented  experiments 
offer the ability to use just a small number of human subjects 
to perform an experiment that can be enhanced by large scale 
multiagent-based simulations.  

Figure 2 illustrates how augmented experiments are real-
ized. Figure 2(a) represents a real world experiment, where 
human subjects communicate and perform an experiment in 
real  space.  Figure  2(b)  shows  how  we  introduce  a  virtual 
space into the real world experiment. The sensors in the real 
space captured the behavior of human subjects for reproduc-
tion in the virtual space. The sensors can be cameras, RFIDs, 
or GPS depending on the environment. By using the virtual 
space,  we  can  monitor  the  entire  experiment  from  various 

IJCAI-07

1342

viewpoints.  Furthermore,  we  can  communicate  with  the 
human subjects in the real space through their avatars in the 
virtual space. Transcendent communication is a new moni-
toring  interface,  where  a  visually  simulated  public  space 
provides  a  more  flexible  and  consistent  view  than  regular 
surveillance systems [Nakanishi et al., 2004a].  

Figure 2(c) illustrates an augmented experiment. In parallel 
with a real world experiment, a large scale multiagent-based 
simulation is conducted in  virtual space:  the experiment is 
augmented by the simulation. To provide social reality to the 
human subjects, scenario-guided extras are placed around the 
subjects.  In  contrast  to  the  participatory  simulations,  the 
human extras do not control avatars: the human extras in the 
real space are controlled by the scenario-guided agents. For 
example,  we  can  use  human  extras  acting  as  evacuation 
leaders  to  conduct  disaster  evacuation  drills.  In  contrast  to 
participatory simulations where human subjects sense and act 
in  virtual  space,  augmented  experiments  allow  subjects  to 
sense  and  act  in  real  space.  Conducting  an  augmented  ex-
periment is possible if the real space is equipped with enough 
sensors.  

Avatar
Avatar

Human
Human
Subject
Subject

Virtual Space
Virtual Space

Real Space
Real Space
a) Real-World Experiment
a) Real-World Experiment

Real Space
Real Space
b) Virtual Space to Monitor Experiment
b) Virtual Space to Monitor Experiment

Monitor
Monitor

Scenario
Scenario

Virtual Space
Virtual Space

Monitor
Monitor

Avatar
Avatar

Agent
Agent

Human
Human
Extra
Extra

must receive sensing information through the communi-
cation channels or from the corresponding human extras. 
To ensure the subjects naturally behave in real space, the 
selection  of  sensor  devices  and  communication  media 
becomes crucial. Visual, auditory and textual interactions 
are to be organized appropriately. 
2.  Learning from data obtained in virtual and real spaces: 
Learning  issues  are  similar  to  those  in  participatory 
simulations, but their evaluation is more difficult, since 
the behaviors of human subjects in real space are harder 
to  analyze  than  those  in  participatory  simulations. 
Though we can monitor the experiment via virtual space, 
video capture of the human subjects in real space is also 
indispensable for analyzing the entire experiment. 

To confirm the feasibility and usefulness of augmented ex-
periments and to determine their future issues, we conducted 
indoor  and  outdoor  evacuation  experiments  in  real  space 
augmented by a large scale multiagent-based simulation. 

4  Indoor Experiment 
Everyday, more than 300,000 passengers pass through Kyoto 
station,  the  main  railway  station  in  Kyoto.  In  this  station, 
using  Q  [Ishida,  2002b]  and  FreeWalk  [Nakanishi  et  al., 
2004b], we installed a disaster evacuation system that tracks 
passengers  to  help  their  navigation  based  on  their  current 
positions.  Beyond  conventional  navigation  systems,  which 
announce route information using public loudspeakers, our 
system sends instructions to individuals using mobile phones. 
Augmented experiments are required for testing the evacua-
tion  system,  because  there  is  no  other  way  to  conduct  ex-
periments with enough reality. 
 

Camera
Camera

Real Space
Real Space
c) Multiagent-Based Augmented Experiment
c) Multiagent-Based Augmented Experiment
Figure 2. Augmented Experiment 

 

In  summary,  an  augmented  experiment  consists  of  1) 
agents  for  modeling  users,  2)  avatars  to  represent  human 
subjects,  3)  scenarios  for  modeling  interactions,  4)  human 
subjects to act experiments, 5) virtual space to represent real 
space, 6) a monitor to visualize the experiment in real space 
enhanced by simulations in virtual space, 7) sensors to re-
produce human subjects in virtual space as avatars, 8) com-
munication channels between real and virtual spaces, and 9) 
human extras to represent agents in virtual space for inter-
acting with human subjects. 
The issues on augmented experiments are as follows.  
1.  Seamless  connections  between  virtual  and  real  spaces: 
The difficulty with augmented experiments is to provide 
the human subjects with a sufficient level of reality. To 
guide  human  subjects  in  a  timely  fashion,  the  subjects 

3D Virtual Space
3D Virtual Space

Indoor Real Space
Indoor Real Space

Figure 3. Indoor Experiment 

 

The augmented experiment was designed as follows. As the 
sensors,  we  placed  twenty  eight  cameras  in  Kyoto  station, 
and captured the movements of passengers in real time. The 
cameras are specially designed to detect passengers' behavior 
but not personal features. Figure 3 shows the omnidirectional 
cameras placed in the station. As the virtual space, we used 
FreeWalk, a three dimensional virtual city system and used it 
to  reproduce  the  passengers'  behavior.  We  implemented  a 
monitor based on transcendent communication [Nakanishi et 
al., 2004a]. Figure 3 includes a snapshot of the monitoring 
system; human subjects on the station platform are projected 

 

 

IJCAI-07

1343

onto avatars in virtual space. A bird’s-eye view of the real 
space is reproduced on the screen of the control center so that 
evacuation leaders in the center can easily monitor the ex-
periment.  As  the  communication  channels,  the  leader  can 
point at particular passengers on the screen, and talk to them 
through  their  mobile  phones.  When  the  monitor  detects 
pointing  operations,  a  wireless  connection  is  immediately 
activated between the control center and the indicated pas-
sengers.  

A  multiagent-based  simulation  with  a  large  number  of 
agents controlled by evacuation scenarios was performed in 
parallel  with  the  experiment  in  real  space.  See-through 
head-mounted  displays  are  not  suitable  for  presenting  the 
simulation  of  augmented  experiments,  since  it  is  unsafe  to 
mask the views of passengers. As described above, since we 
used  mobile  phones,  small  and  low-resolution  images  of 
three dimensional virtual spaces are difficult to understand. 
Instead of displaying visual simulations, the mobile phones 
displayed symbols that directly represent what human sub-
jects need to understand. We did not use human extras in this 
experiment. 

We  conducted  the  indoor  augmented  experiment  in  Feb-
ruary  2005.  From  this  experience,  we  learned  that  insuffi-
cient visual reality prevented the human subjects from rec-
ognizing the crowd of agents around the staircase. It appears 
that the usability of an augmented experiment depends sig-
nificantly on the user interface for interacting with the agents. 

5  Outdoor Experiment 
One  year  after  the  indoor  experiment,  we  implemented  a 
large-scale outdoor evacuation system for emergency situa-
tions using Q [Ishida, 2002b] and Caribbean [Yamamoto et 
al.,  1999],  and  conducted  an  augmented  experiment  in 
January 2006. This system architecture is close to that of our 
indoor experiment, but the sensors were GPS devices instead 
of omnidirectional cameras, and as the virtual space, we used 
a two dimensional map instead of a three dimensional virtual 
city  system.  The  virtual  space  is  displayed  on  the  monitor 
screen of the control center in a birds-eye view, so that the 
leader could grasp how evacuees were behaving in the ex-
periment. Evacuees on a screen consisted of a small number 
of avatars representing the human subjects in real space, and 
a large number of agents controlled by evacuation scenarios 
in  virtual  space.  In  the  actual  experiment,  ten  to  thirteen 
humans and three thousand agents per each time undertook 
the augmented experiment.  

The location of human subjects in real space was projected 
into virtual space based on their positions acquired by GPS. 
This map showed fires, blocks routes, and safe areas in real 
time. The human subjects could always get the latest map by 
sending  their  location.  The  evacuation  leaders  assigned 
evacuation  destinations  and  directions  to  each  evacuee 
through  the  monitor  screen  shown  in  Figure  4.  The  leader 
issued high level instructions to the evacuees using the map, 
and precise navigation instructions were automatically gen-
erated  for  each  evacuee.  Dragging  operations  indicating  a 
rectangular  area  enabled  the  leader  to  broadcast  an  an-
nouncement to a group of evacuees. 

As in the indoor experiments, the evacuation leader acti-
vated  communication  channels  to  particular  evacuees  by 
pointing at the evacuees on the screen. The leader could talk 
to the particular evacuees and provide customized navigation. 
We used human extras as evacuation leaders in this experi-
ment to check the evacuation routes taken by human subjects 
in real space. 

GPS
GPS

 

2D Virtual Space
2D Virtual Space

Outdoor Real Space
Outdoor Real Space

Figure 4. Outdoor Experiment 

 
The human subjects could grasp the experiment from the 
screen of their mobile phones. The map around the place the 
subject was standing was displayed together with informa-
tion  of  fires, blocked routes,  and safe areas, just  as on the 
monitor in the control center. The moves of other evacuees 
were also displayed on their mobile phones. From this ex-
perience, we learned that a map can be an excellent interface 
between  human  subjects  and  agents  as  well  as  evacuation 
leaders. Unlike the indoor experiment, since route selection 
is the main issue in the outdoor experiment, the human sub-
jects did not have much difficulty in imagining the disaster 
situation. However, if the number of agents was high in the 
outdoor experiment, then virtual crowding would become a 
problem.  Human  extras  playing  evacuation  leaders  can  in-
crease the reality of the experiment. 

6  Learning from Experiments 
Multiagent-based  simulations  use  production  rules  to  de-
scribe  agent  models  for  approximating  human  users,  and 
extended finite state automata to describe interaction models 
among users and socially embedded systems. By analyzing 
the  logs  of  participatory  simulations  or  augmented  experi-
ments, we can refine both agent and interaction models. The 
learning  process  of  multiagent-based  participatory  design 
consists of two phases: the normal design loop and the par-
ticipatory design loop.  

At first, scenario writers create agent and interaction mod-
els and verify their design by multiagent-based simulations. 
This process is called the normal design loop and its detailed 
steps  can  be  found  in  a  previous  work  [Murakami  et  al., 
2003].  After  verifying  the  agent  and  interaction  models, 
participatory  simulations  and  augmented  experiments  are 
performed. The results of which may be different from those 

IJCAI-07

1344

of  multiagent-based  simulations,  because  human  behaviors 
are  often  more  variable  than  those  of  agents  modeled  by 
scenario writers. If this is the case, we first refine the agent 
models to better express the variation seen in human behavior. 
We  then  perform  the  multiagent-based  simulations  again 
with the new agent models. If the new simulations succeed to 
express  various  human  behaviors  but  fail  to  produce  the 
expected results, we then refine the interaction models; i.e. 
modify  protocols,  methods,  rules,  and/or  laws.  After  the 
improved 
is  confirmed  by  multi-
agent-based  simulations,  participatory  simulation  and  aug-
mented  experiment  take  place  again.  This  loop,  called  the 
participatory design loop, is illustrated in Figure 5. 
 

interaction  model 

Start
Start

Normal
Normal
Design
Design
Loop
Loop

Refine agent and interaction models
Refine agent and interaction models

no
no

Multiagent-based simulation
Multiagent-based simulation

Simulation data = Expected results?
Simulation data = Expected results?

yes
yes

Participatory simulation / Augmented experiment
Participatory simulation / Augmented experiment

Participatory
Participatory
Design
Design
Loop
Loop

no
no

no
no

Results of human participation
Results of human participation

= Expected results?
= Expected results?

no
no

Refine agent models
Refine agent models

Multiagent-based simulation
Multiagent-based simulation

yes
yes

End
End

Simulation data =
Simulation data =

Results of human participation?
Results of human participation?

yes
yes

Refine interaction models
Refine interaction models

Multiagent-based simulation
Multiagent-based simulation

Simulation data = Expected results?
Simulation data = Expected results?

yes
yes

 

Figure 5. Participatory Design Loop 

 
It would seem, at first glance, that inductive machine learning 
technologies could be applied to refine agent and interaction 
models.  However,  it  is  extremely  rare  to  have  a  sufficient 
number of human subjects to gather enough data for induc-
tive learning: deductive methods with domain knowledge are 
more  feasible  [Torii  et  al.,  2006].  To  refine  agent  models, 
given  that  the  training  examples  will  be  limited,  explana-
tion-based learning is a reasonable approach. Though varia-
tions exist in human behavioral rules such as “escape from 
crowds”  and  “follow  crowds,”  observing  the  behavior  of 
human-controlled avatars can create a plausible explanation 
that selects one of the two rules. 

By combining machine learning technologies with inter-
views of human subjects, agent models have been success-
fully refined in several domains [Murakami et al., 2005]. To 
refine interaction models, if we have enough candidate pro-
tocols, we can try them, and select the one that best fits. If we 
need to invent a new model, however, there is no available 
technology  that  can  create  such  an  interaction  model.  The 
visualization of simulations and experiments can, however, 
support scenario writers in inventing new models. 

7  Related Work 
There are two types of multiagent-based simulations and they 
have  different  goals;  a)  analytic  multiagent-based  simula-
tions often with simple agent and interaction models and b) 
synthetic  multiagent-based  simulations  sometimes  with 
complex agent and interaction models. Analytic simulations 
have been used to model social systems [Epstein and Axtel, 
1996]. Here, the KISS principle (Keep It Simple, Stupid) is 
often applied [Axelrod, 1997]. The KISS principle states that 
modeling should be simple even though the observed phe-
nomenon is complex, and that complexity should be a result 
of repeated interaction among agents and their environment. 
Hence,  agents  are  often  modeled  with  limited  parameters. 
This  approach  is  effective  in  the  analysis  of  macro-micro 
links,  the  relationship  between  the  macro  properties  of  the 
entire system and the micro properties of the agents consti-
tuting the system.  

On the other hand, synthetic simulations are used to pro-
duce realistic situations. Agent and interaction models can be 
complex reflecting the real world to make the simulation as 
realistic as possible. This approach is used in an early stage of 
system development [Noda and Stone, 2003], and in educa-
tion or training [Rickel and Johnson, 2000]. Since our pur-
pose is to design socially embedded systems, our simulation 
is synthetic: our agent and interaction models can be complex 
to reproduce realistic situations; after a participatory simula-
tion,  an  augmented  experiment  is  performed  to  understand 
user behavior in a more realistic environment.  

Our  work  appears  to  follow  various  multiagent-based 
simulation  papers  including  the  RoboCup  soccer  simulator 
[Noda and Stone, 2003], but is clearly unique due to its focus 
on  participatory  design  methodology.  Drogoul  proposed  a 
methodological  process  for  developing  multiagent-based 
simulations,  and  introduced  the  idea  of  the  participatory 
design of simulations  [Drogoul et al., 2002], while we are 
proposing  the  participatory  design  for  socially  embedded 
systems.  Bousquet  applied  role  games  to  the  modeling  of 
social systems [Bousquet et al., 2002]. This paper, however, 
applies multiagent-based simulation to software system de-
sign, not social system analysis.  

We propose the use of participatory methodology includ-
ing  simulations  and  experiments  for  designing  large-scale 
socially embedded systems. This methodology differs from 
the  Gaia  methodology  [Zambonelli  et  al.,  2003]  or  OperA 
[Vázquez-Salceda et al., 2005], which focus on an early stage 
of  designing  multiagent  systems  mainly  for  enterprise  ap-
plications [Jennings, 2000]. Our methodology can be applied 
to  designing  ubiquitous/pervasive  computing  systems,  but 
the  resulting  systems  are  not  necessarily  multiagent-based, 
though  multiagent  systems  are  often  natural  solutions  of 
socially embedded systems. 

8  Conclusion 
The waterfall model has been used in software development 
for a long time. Given the increase in human-computer in-
teraction,  however,  it  has  become  essential  to  use  the 
user-centered  design  approach  when  creating  usable  and 

IJCAI-07

1345

accessible interactive systems. In future, however, millions 
of electronic devices with computing facilities will be con-
nected with each other in ad hoc ways, and should behave 
coherently. It is natural to say we need a new methodology 
for designing ubiquitous/pervasive computing systems. 

In  this  paper,  we  proposed  the  multiagent-based  partici-
patory  design  methodology  for  ubiquitous/pervasive  com-
puting systems, which are to be embedded in human society. 
Though the concept of multiagent-based participatory design 
is open-ended, we include the following two components.  
We call multiagent-based simulations participatory simu-
lations  when  they  include  human-controlled  avatars  and 
scenario-guided agents. We can monitor the entire process of 
a  participatory  simulation  by  visualizing  the  virtual  space. 
We  call  real-world  experiments  as  augmented  experiments 
when  they  are  augmented  by  large  scale  multiagent-based 
simulations.  In  ubiquitous/pervasive  computing,  because  a 
large  number  of  electronic  devices  will  be  embedded  in 
public spaces and in continuous use, it is often impossible to 
conduct experiments with a large number of human subjects. 
Augmented experiments enable us to perform such trials with 
a small number of human subjects.  

By introducing augmented experiments and actually trying 
out indoor and outdoor experiments in the city of Kyoto, we 
learned the following lessons. Interaction among the human 
subjects  in  the  real  space  and  agents  in  the  virtual  space 
should be designed carefully. If there exists a medium like 
geographical map, which human subjects use daily to grasp 
situations,  it  is  a  good  idea  to  adopt  them  as  the  interface 
between the real and virtual spaces. Human extras controlled 
by scenario-guided agents are also effective to increasing the 
reality, if they have a clear role such as evacuation leaders. 

References 
[Axelrod, 1997]  R.  Axelrod.  The  Complexity  of  Coopera-
tion: Agent-Based Models of Competition and Collabo-
ration. Princeton University Press, pp. 4-5, 1997. 

[Bousquet et al., 1998]  F.  Bousquet,  I.  Bakam,  H.  Proton 
and C. Le Page. Cormas: Common-Pool Resources and 
Multiagent Systems. LNAI 1416, pp. 826-838, 1998. 

[Bousquet et al., 2002]  F.  Bousquet,  O.  Barreteau,  P. 
Aquino, M. Etienne, S. Boissau, S. Aubert, C. Le Page, D. 
Babin, and J. C. Castella. Multi-Agent Systems and Role 
Games:  Collective  Learning  Processes  for  Ecosystem 
Management. M. Janssen Ed. Complexity and Ecosystem 
Management, Edward Elgar Publishers, 2002.  

[Drogoul et al., 2002] A.  Drogoul,  D.  Vanbergue  and  T. 
Meurisse.  Multiagent-Based  Simulation:  Where  are  the 
Agents? LNAI 2581, pp. 1-15, 2002. 

[Epstein and Axtel, 1996]  J.  M.  Epstein  and  R.  L.  Axtell. 
Growing  Artificial  Societies:  Social  Science  from  the 
Bottom Up. MIT Press, 1996. 

 [Guyot et al., 2005]  P. Guyot, A. Drogoul and C. Lemaître. 
Using Emergence in Participatory Simulations to Design 
Multi-Agent Systems. AAMAS-05, pp. 199-203, 2005. 

[Ishida, 2002a] T.  Ishida.  Digital  City  Kyoto:  Social  Infor-
mation Infrastructure for Everyday Life. Communications 
of the ACM (CACM), Vol. 45, No. 7, pp. 76-81, 2002. 

[Ishida, 2002b]  T.  Ishida.  Q:  A  Scenario  Description 
Language  for  Interactive  Agents. IEEE  Computer,  Vol. 
35, No. 11, pp. 54-59, 2002. 

[Ishida et al., 2005] T. Ishida, L. Gasser and H. Nakashima 
Eds.  Massively  Multi-Agent  Systems  I.  LNAI,  3446, 
Springer-Verlag, 2005. 

[Jennings, 2000]  N. R. Jennings. On Agent-Based Software 
Engineering. Artificial Intelligence, Vol. 177, No. 2. pp. 
277-296, 2000. 

[Murakami et al., 2003]  Y. Murakami, T. Ishida, T. Kawa-
soe  and  R.  Hishiyama.  Scenario  Description  for 
Multi-Agent Simulation. AAMAS-03, pp. 369-376, 2003. 
[Murakami et al., 2005]  Y. Murakami, Y. Sugimoto and T. 
Ishida.  Modeling  Human  Behavior  for  Virtual  Training 
Systems. AAAI-05, pp. 127-132, 2005. 

[Nakanishi et al., 2004a] H. Nakanishi, S. Koizumi, T. Ishida 
and  H. 
Ito.  Transcendent  Communication:  Loca-
tion-Based  Guidance  for  Large-Scale  Public  Spaces. 
CHI-04, pp. 655-662, 2004. 

[Nakanishi et al., 2004b] H.  Nakanishi  and  T.  Ishida.  Free-
Walk/Q:  Social  Interaction  Platform  in  Virtual  Space. 
VRST-04, pp. 97-104, 2004. 

[Noda and Stone, 2003]  I. Noda and P. Stone. The RoboCup 
Soccer  Server  and  CMUnited  Clients:  Implemented  In-
frastructure for MAS Research. Autonomous Agents and 
Multi-Agent Systems, Vol. 7, No. 1-2, pp. 101-120, 2003. 
[Rickel and Johnson, 2000] J.  Rickel  and  W.  L.  Johnson. 
Task-Oriented  Collaboration  with  Embodied  Agents  in 
Virtual Worlds. I J. Cassell, J. Sullivan, S. Prevost, and E. 
Churchill  Eds.  Embodied  Conversational  Agents,  pp. 
95-122, MIT Press, 2000. 

 [Sugiman et al., 1998] 

 T.  Sugiman  and  J.  Misumi.  De-
velopment of a New Evacuation Method for Emergencies. 
Journal of Applied Psychology, Vol. 73, No. 1, pp. 3-10, 
1988. 

[Torii et al., 2006]  D.  Torii,  T.  Ishida  and  F.  Bousquet. 
Modeling  Agents  and  Interactions  in  Agricultural  Eco-
nomics. AAMAS-06, pp. 81-88, 2006. 

[Vázquez-Salceda et al., 2005]  J.  Vázquez-Salceda,  V. 
Dignum and F. Dignum. Organizing Multiagent Systems. 
Autonomous  Agents  and  Multi-Agent  Systems,  Vol.  11, 
No. 3, pp. 307-360, 2005 

[Yamamoto et al., 1999] G.  Yamamoto  and  Y.  Nakamura. 
Architecture  and  Performance  Evaluation  of  a  Massive 
Multi-Agent System. Agents-99, pp. 319-325, 1999. 

[Zambonelli et al., 2003] F. Zambonelli, N. R. Jennings and 
M.  Wooldridge.  Developing  Multiagent  Systems:  The 
Gaia Methodology. ACM Transactions on Software En-
gineering and Methodology. Vol. 12, No. 3, pp. 317-370, 
2003. 

IJCAI-07

1346


Bayesian Inverse Reinforcement Learning

Deepak Ramachandran
Computer Science Dept.

Eyal Amir

Computer Science Dept.

University of Illinois at Urbana-Champaign

University of Illinois at Urbana-Champaign

Urbana, IL 61801

Urbana, IL 61801

Abstract

Inverse Reinforcement Learning (IRL) is the prob-
lem of learning the reward function underlying a
Markov Decision Process given the dynamics of
the system and the behaviour of an expert.
IRL
is motivated by situations where knowledge of the
rewards is a goal by itself (as in preference elici-
tation) and by the task of apprenticeship learning
(learning policies from an expert).
In this paper
we show how to combine prior knowledge and evi-
dence from the expert’s actions to derive a probabil-
ity distribution over the space of reward functions.
We present efﬁcient algorithms that ﬁnd solutions
for the reward learning and apprenticeship learn-
ing tasks that generalize well over these distribu-
tions. Experimental results show strong improve-
ment for our methods over previous heuristic-based
approaches.

1 Introduction
The Inverse Reinforcement Learning (IRL) problem is de-
ﬁned in [Russell, 1998] as follows: Determine The reward
function that an agent is optimizing. Given 1) Measurement
of the agent’s behaviour over time, in a variety of circum-
stances 2) Measurements of the sensory inputs to that agent;
3) a model of the environment. In the context of Markov De-
cision Processes, this translates into determining the reward
function of the agent from knowledge of the policy it executes
and the dynamics of the state-space.

There are two tasks that IRL accomplishes. The ﬁrst, re-
ward learning, is estimating the unknown reward function as
accurately as possible. It is useful in situations where the re-
ward function is of interest by itself, for example when con-
structing models of animal and human learning or modelling
opponent in competitive games. Pokerbots can improve per-
formance against suboptimal human opponents by learning
reward functions that account for the utility of money, prefer-
ences for certain hands or situations and other idiosyncrasies
[Billings et al., 1998]. There are also connections to various
preference elicitation problems in economics [Sargent, 1994].
The second task is apprenticeship learning - using obser-
vations of an expert’s actions to decide one’s own behaviour.
It is possible in this situation to directly learn the policy from

the expert [Atkeson and Schaal, 1997]. However the reward
function is generally the most succint, robust and transferable
representation of the task, and completely determines the op-
timal policy (or set of policies). In addition, knowledge of
the reward function allows the agent to generalize better i.e. a
new policy can be computed when the environment changes.
IRL is thus likely to be the most effective method here.

In this paper we model the IRL problem from a Bayesian
perspective. We consider the actions of the expert as evidence
that we use to update a prior on reward functions. We solve
reward learning and apprenticeship learning using this poste-
rior. We perform inference for these tasks using a modiﬁed
Markov Chain Monte Carlo (MCMC) algorithm. We show
that the Markov Chain for our distribution with a uniform
prior mixes rapidly, and that the algorithm converges to the
correct answer in polynomial time. We also show that the
original IRL is a special case of Bayesian IRL (BIRL) with a
Laplacian prior.

There are a number of advantages of our technique over
previous work: We do not need a completely speciﬁed op-
timal policy as input to the IRL agent, nor do we need to
assume that the expert is infallible. Also, we can incorpo-
rate external information about speciﬁc IRL problems into
the prior of the model, or use evidence from multiple experts.
IRL was ﬁrst studied in the machine learning setting by
[Ng and Russell, 2000] who described algorithms that found
optimal rewards for MDPs having both ﬁnite and inﬁnite
states. Experimental results show improved performance by
our techniques in the ﬁnite case.

The rest of this paper is organised as follows: In section
2 we deﬁne our terms and notation. Section 3 presents our
Bayesian model of the IRL process. Section 4 discusses how
to use this model to do reward learning and apprenticeship
learning while section 5 discusses the sampling procedure.
Sections 6, 7 and 8 then present experimental results, related
work and our conclusions respectively.

2 Preliminaries
We recall some basic deﬁnitions and theorems relating to
Markov Decision Processes and Reinforcement Learning.

A (ﬁnite) Markov Decision Problem is

(S, A, T, γ, R) where

• S is a ﬁnite set of N states.
• A = {a1, . . . , ak} is a set of k actions.

a

tuple

IJCAI-07

2586

• T : S × A × S (cid:2)→ [0, 1] is a transition probability func-
• γ ∈ [0, 1) is the discount factor.
• R : S (cid:2)→ R is a reward function, with absolute value

tion.

bounded by Rmax.

The rewards are functions of state alone because IRL prob-
lems typically have limited information about the value of
action and we want to avoid overﬁtting.

A Markov Decision Process (MDP) is a tuple (S, A, T, γ),
with the terms deﬁned as before but without a reward func-
tion. To avoid confusion we will use the abbreviation MDP
only for Markov Decision Processes and not Problems.

, R) = R(st1 )+Est1

We adopt the following compact notation from [Ng and
Russell, 2000] for ﬁnite MDPs : Fix an enumeration s1 . . . sN
of the ﬁnite state space S. The reward function (or any other
function on the state-space) can then be represented as an N -
dimensional vector R, whose ith element is R(si).
A (stationary) policy is a map π : S (cid:2)→ A and the (dis-
counted, inﬁnite-horizon) value of a policy π for reward func-
tion R at state s ∈ S,denoted V π(s, R) is given by:
,...[γR(st2 )+γ2R(st3 )+. . .|π]
V π(st1
, π(sti ), sti+1). The goal of
where P r(sti+1
standard Reinforcement Learning is to ﬁnd an optimal policy
such that V π(s, R) is maximized for all s ∈ S by π = π∗
π∗
.
Indeed, it can be shown (see for example [Sutton and Barto,
1998]) that at least one such policy always exists for ergodic
MDPs. For the solution of Markov Decision Problems, it is
useful to deﬁne the following auxilliary Q-function:
We also deﬁne the optimal Q-function Q∗(·,·, R) as the Q-
function of the optimal policy π∗

Qπ(s, a, R) = R(s) + γEs(cid:2)∼T (s,a,·)[V π(s(cid:4), R)]

,st2
, π) = T (sti

for reward function R.

|sti

Finally, we state the following result concerning Markov

Decision Problems (see [Sutton and Barto, 1998]) :
Theorem 1 (Bellman Equations). Let a Markov Decision

Problem M = (S, A, T, γ, R) and a policy π : S (cid:2)→ A be
1. For all s ∈ S, a ∈ A, V π
V π(s) = R(s) + γ

T (s, π(s), s(cid:4)

and Qπ
(cid:2)

given. Then,

)V π(s(cid:4)

satisfy

)

(1)

s(cid:2)

(cid:2)

Qπ(s, a) = R(s) + γ

)V π(s(cid:4)
2. π is an optimal policy for M iff, for all s ∈ S,

T (s, a, s(cid:4)

s(cid:2)

)

π(s) ∈ argmax

a∈A

Qπ(s, a)

(2)

3 Bayesian IRL
IRL is currently viewed as a problem of infering a single re-
ward function that explains an agent’s behaviour. However,
there is too little information in a typical IRL problem to get
only one answer. For example, consider the MDP shown in
Figure 1. There are at least three reasonable kinds of reward

elsewhere) which explains why the policy tries to return to

functions: R1(·) has high positive value at s1 (and low values
this state, while R2(·) and R3(·) have high values at s2 and
s3 respectively. Thus, a probability distribution is needed to
represent the uncertainty.

a2

a2

S0

0.4

a1

S1

a1

0.3
0.3

a2

a2

S2

a1

a1

S3

Figure 1: An example IRL problem. Bold lines represent the
optimal action a1 for each state and broken lines represent
some other action a2. Action a1 in s1 has probabilities 0.4,0.3
and 0.3 of going to states s1, s2, s3 respectively, and all other
actions are deterministic.

3.1 Evidence from the Expert
Now we present the details of our Bayesian IRL model (Fig.
2). We derive a posterior distribution for the rewards from
a prior distribution and a probabilistic model of the expert’s
actions given the reward function.

(the expert) operating in this MDP. We assume that a reward

Consider an MDP M = (S, A, T, γ) and an agent X
function R for X is chosen from a (known) prior distribu-
tion PR. The IRL agent receives a series of observations of
the expert’s behaviour OX = {(s1, a1), (s2, a2) . . . (sk, ak)}
which means that X was in state si and took action ai at time
step i. For generality, we will not specify the algorithm that
X uses to determine his (possibly stochastic) policy, but we
1. X is attempting to maximize the total accumulated re-
ward according to R. For example, X is not using an
2. X executes a stationary policy, i.e. it is invariant w.r.t.

make the following assumptions about his behaviour:

epsilon greedy policy to explore his environment.

time and does not change depending on the actions and
observations made in previous time steps.

For example, X could be an agent that learned a policy for
(M, R) using a reinforcement learning algorithm. Because
the expert’s policy is stationary, we can make the following
independence assumption:

P rX (OX|R) = P rX ((s1, a1)|R)P rX ((s2, a2)|R)

. . . P rX ((sk, ak)|R)

The expert’s goal of maximizing accumulated reward is
equivalent to ﬁnding the action for which the Q∗
value at each
state is maximum. Therefore the larger Q∗(s, a) is, the more
likely it is that X would choose action a at state s. This like-
lihood increases the more conﬁdent we are in X ’s ability to
select a good action. We model this by an exponential dis-
tribution for the likelihood of (si, ai), with Q∗
as a potential
function:

P rX ((si, ai)|R) =

eαX Q∗(si,ai,R)

1
Zi

where αX is a parameter1 representing the degree of con-
ﬁdence we have in X ’s ability to choose actions with high
1Note that the probabilities of the evidence should be conditioned

IJCAI-07

2587

X

αX

R

PLaplace(R(s) = r) =

2σ ,∀s ∈ S

e− |r|

1
2σ

3. If the underlying MDP represented a planning-type
problem, we expect most states to have low (or negative)
rewards but a few states to have high rewards (corre-
sponding to the goal); this can be modeled by a Beta dis-
tribution for the reward at each state, which has modes
at high and low ends of the reward space:

(s1, a1)

(s2, a2)

(sk, ak)

Figure 2: The BIRL model

value. This distribution satisﬁes our assumptions and is easy
to reason with. The likelihood of the entire evidence is :

P rX (OX|R) =

eαX E(OX ,R)

1
Z

P
i

Q∗(si, ai, R) and Z is the appropriate
where E(OX , R) =
normalizing constant. We can think of this likelihood func-
tion as a Boltzmann-type distribution with energy E(OX , R)
and temperature

1
αX .

Now, we compute the posterior probability of reward func-

tion R by applying Bayes theorem,

P rX (R|OX ) =

P rX (OX|R)PR(R)

P r(OX )

=

1
Z (cid:4)

eαX E(OX ,R)PR(R)

(3)

Computing the normalizing constant Z (cid:4)

is hard. However
the sampling algorithms we will use for inference only need
the ratios of the densities at two points, so this is not a prob-
lem.

3.2 Priors
When no other information is given, we may assume that the
rewards are independently identically distributed (i.i.d.) by
the principle of maximum entropy. Most of the prior func-
tions considered in this paper will be of this form. The exact
prior to use however, depends on the characteristics of the
problem:

1. If we are completely agnostic about the prior, we can

use the uniform distribution over the space −Rmax ≤
R(s) ≤ Rmax for each s ∈ S. If we do not want to spec-
ify any Rmax we can try the improper prior P (R) = 1
for all R ∈ R

n

.

2. Many real world Markov decision problems have parsi-
monious reward structures, with most states having neg-
ligible rewards. In such situations, it would be better to
assume a Gaussian or Laplacian prior:

PGaussian(R(s) = r) =

1√
2πσ

2

2σ2 ,∀s ∈ S

e− r

on αX as well (Fig 2). But it will be simpler to treat αX as just a
parameter of the distribution.

PBeta(R(s) = r) =

(

r

Rmax

)

1

1

2 (1 − r

Rmax

,∀s ∈ S

1
2

)

In section 6.1, we give an example of how more informa-

tive priors can be constructed for particular IRL problems.

4 Inference
We now use the model of section 3 to carry out the two tasks
described in the introduction: reward learning and appren-
ticeship learning. Our general procedure is to derive minimal
solutions for appropriate loss functions over the posterior (Eq.
3). Some proofs are omitted for lack of space.

4.1 Reward Learning
Reward learning is an estimation task. The most common loss
functions for estimation problems are the linear and squared
error loss functions:

Llinear(R, ˆR) = (cid:8) R − ˆR (cid:8)1
LSE(R, ˆR) = (cid:8) R − ˆR (cid:8)2

where R and ˆR are the actual and estimated rewards, respec-
tively. If R is drawn from the posterior distribution (3), it can
be shown that the expected value of LSE(R, ˆR) is minimized
by setting ˆR to the mean of the posterior (see [Berger, 1993]).
Similarily, the expected linear loss is minimized by setting ˆR
to the median of the distribution. We discuss how to compute
these statistics for our posterior in section 5.

It is also common in Bayesian estimation problems to use
the maximum a posteriori (MAP) value as the estimator. In
fact we have the following result:

Theorem 2. When the expert’s policy is optimal and fully
speciﬁed, the IRL algorithm of [Ng and Russell, 2000] is
equivalent to returning the MAP estimator for the model of
(3) with a Laplacian prior.

However in IRL problems where the posterior distribution
is typically multimodal, a MAP estimator will not be as rep-
resentative as measures of central tendency like the mean.

4.2 Apprenticeship Learning
For the apprenticeship learning task, the situation is more in-
teresting. Since we are attempting to learn a policy π, we can
formally deﬁne the following class of policy loss functions:

policy(R, π) =(cid:8) V
Lp

∗

(R) − V

π

(R) (cid:8)p

∗

(R) is the vector of optimal values for each state
where V
acheived by the optimal policy for R and p is some norm.
We wish to ﬁnd the π that minimizes the expected policy loss
over the posterior distribution for R. The following theorem
accomplishes this:

IJCAI-07

2588

Theorem 3. Given a distribution P (R) over reward
functions R for an MDP (S, A, T, γ),
the loss function
Lp
policy(R, π) is minimized for all p by π∗
M , the optimal policy
for the Markov Decision Problem M = (S, A, T, γ, EP [R]).

Proof. From the Bellman equations (1) we can derive the fol-
lowing:

π

V

(R) = (I − γT

π

−1
)

R

is the |S|×|S| transition matrix for policy π. Thus,
where T π
for a state s ∈ S and ﬁxed π, the value function is a linear

(4)

function of the rewards:

V π(s, R) = w(s, π) · R

π

π

max

)−1

E[V π(s, R)] = max

where w(s, π) is the s’th row of the coefﬁcient matrix (I −
in (4). Suppose we wish to maximize E[V π(s, R)]
γT
alone. Then,
w(s, π)·E[R]
M (s), the optimum value
function for M , and the maximizing policy π is π∗
M , the op-
timal policy for M . Thus for all states s ∈ S, E[V π(s, R)] is
maximum at π = π∗
But V ∗(s, R) ≥ V π(s, R) for all s ∈ S, reward functions

By deﬁnition this is equal to V ∗

E[w(s, π)·R] = max

M .

π

π

R, and policies π. Therefore

E[Lp

policy(π)] = E((cid:8) V
is minimized for all p by π = π∗

M .

∗

(R) − V

π

(R) (cid:8)p)

So, instead of trying a difﬁcult direct minimization of the
expected policy loss, we can ﬁnd the optimal policy for the
mean reward function, which gives the same answer.

5 Sampling and Rapid Convergence
We have seen that both reward learning and apprenticeship
learning require computing the mean of the posterior distribu-
tion. However the posterior is complex and analytical deriva-
tion of the mean is hard, even for the simplest case of the uni-
form prior. Instead, we generate samples from these distribu-
tions and then return the sample mean as our estimate of the
true mean of the distribution. The sampling technique we use
is an MCMC algorithm GridWalk (see [Vempala, 2005])
that generates a Markov chain on the intersection points of a
grid of length δ in the region R

(denoted R

|S|/δ).

|S|

However, computing the posterior distribution at a partic-
ular point R requires calculation of the optimal Q-function
for R, an expensive operation. Therefore, we use a modi-
ﬁed version of GridWalk called PolicyWalk (Figure 3)
that is more efﬁcient: While moving along a Markov chain,
the sampler also keeps track of the optimal policy π for the
current reward vector R. Observe that when π is known,
the Q function can be reduced to a linear function of the
reward variables, similar to equation 4. Thus step 3b can
be performed efﬁciently. A change in the optimal policy
can easily be detected when moving to the next reward vec-

tor in the chain ˜R, because then for some (s, a) ∈ (S, A),
Qπ(s, π(s), ˜R) < Qπ(s, a, ˜R) by Theorem 1. When this
happens, the new optimal policy is usually only slightly dif-
ferent from the old one and can be computed by just a few

Algorithm PolicyWalk(Distribution P , MDP M , Step Size δ )

1. Pick a random reward vector R ∈ R
2. π := PolicyIteration(M, R)
3. Repeat

|S|/δ.

(a) Pick a reward vector ˜R uniformly at random from the

neighbours of R in R

|S|/δ.

(b) Compute Qπ(s, a, ˜R) for all (s, a) ∈ S, A.
(c) If ∃(s, a) ∈ (S, A), Qπ(s, π(s), ˜R) < Qπ(s, a, ˜R)

i. ˜π := PolicyIteration(M, ˜R, π)
ii. Set R := ˜R and π := ˜π with probability

min{1, P ( ˜R,˜π)
P (R,π)

}

Else
i. Set R := ˜R with probability min{1, P ( ˜R,π)

P (R,π)

}

4. Return R

Figure 3: PolicyWalk Sampling Algorithm

steps of policy iteration (see [Sutton and Barto, 1998]) start-
ing from the old policy π. Hence, PolicyWalk is a correct
and efﬁcient sampling procedure. Note that the asymptotic
memory complexity is the same as for GridWalk.

The second concern for the MCMC algorithm is the speed
of convergence of the Markov chain to the equilibrium dis-
tribution. The ideal Markov chain is rapidly mixing (i.e. the
number of steps taken to reach equilibrium is polynomially
bounded), but theoretical proofs of rapid mixing are rare. We
will show that in the special case of the uniform prior, the
Markov chain for our posterior (3) is rapidly mixing using
the following result from [Applegate and Kannan, 1993] that
bounds the mixing time of Markov chains for pseudo-log-
concave functions.

Lemma 1. Let F (·) be a positive real valued function deﬁned
n| − d ≤ xi ≤ d} for some positive d,
on the cube {x ∈ R
satisfying for all λ ∈ [0, 1] and some α, β

|f (x) − f (y)| ≤ α (cid:8) x − y (cid:8)∞

and

f (λx + (1 − λ)y) ≥ λf (x) + (1 − λ)f (y) − β

where f (x) = log F (x). Then the Markov chain induced by
GridWalk (and hence PolicyWalk) on F rapidly mixes
to within  of F in O(n2d2α2e2β log 1

 ) steps.

Proof. See [Applegate and Kannan, 1993].

Theorem 4. Given an MDP M = (S, A, T, γ) with |S| =
N , and a distribution over rewards P (R) = P rX (R|OX )
deﬁned by (3) with uniform prior PR over C = {R ∈ R
n| −
Rmax ≤ Ri ≤ Rmax}. If Rmax = O(1/N ) then P can be
efﬁciently sampled (within error ) in O(N 2 log 1/) steps by
algorithm PolicyWalk.

Proof. Since the uniform prior is the same for all points R,
we can ignore it for sampling purposes along with the normal-
izing constant. Therefore, let f (R) = αX E(OX , R). Now
choose some arbitrary policy π and let

fπ(R) = αX

Qπ(s, ai, R)

(cid:2)

i

IJCAI-07

2589

Reward Loss

Policy Loss

QL/BIRL
k-greedy/BIRL
QL/IRL
k-greedy/IRL

L

 80

 70

 60

 50

 40

 30

 20

 10

 0

 10

 100

N

 1000

QL/BIRL
k-greedy/BIRL
QL/IRL
k-greedy/IRL

 30

 25

 20

L

 15

 10

 5

 0

 10

 100

N

 1000

Figure 4: Reward Loss.

Figure 5: Policy Loss.

Note that fπ is a linear function of R and f (R) ≥ fπ(R),
for all R ∈ C. Also we have,

Q∗

(s, a) = max
s,a,π

Qπ(s, a) = max
max
s,a
Similarly, mins,a Q∗(s, a) ≥ − Rmax
and fπ(R) ≥ − αX N Rmax

αX N Rmax

1−γ

1−γ

s,π

max(s) ≤ Rmax
V π
1 − γ
1−γ . Therefore, f (R) ≤

and hence

fπ(R) ≥ f (R) − 2αX N Rmax

1 − γ

So for all R1, R2 ∈ C and λ ∈ [0, 1],
f (λR1 + (1 − λ)R2) ≥ fπ(λR1 + (1 − λ)R2)
≥ λfπ(R1) + (1 − λ)fπ(R2)
≥ λf (R1) + (1 − λ)f (R2)

− 2αX N Rmax

1 − γ

Therefore, f satisﬁes the conditions of Lemma 1 with β =

2αX N Rmax

1−γ

1−γ = O(1) and

N

)

= 2N · O( 1
|f (R1) − f (R2)|
(cid:8) R1 − R2 (cid:8)∞

α =

≤ 2αX N Rmax
(1 − γ)O( 1
N )

= O(N )

Hence the Markov chain induced by the GridWalk al-
gorithm (and the PolicyWalk algorithm) on P mixes
rapidly to within  of P in a number of steps equal to
O(N 2 1

N 2 N 2eO(1) log 1/) = O(N 2 log 1/).

Note that having Rmax = O(1/N ) is not really a restric-
tion because we can rescale the rewards by a constant factor k
after computing the mean without changing the optimal pol-
icy and all the value functions and Q functions get scaled by
k as well.

6 Experiments

We compared the performance of our BIRL approach to the
IRL algorithm of [Ng and Russell, 2000] experimentally.
First, we generated random MDPs with N states (with N
varying from 10 to 1000) and rewards drawn from i.i.d. Gaus-
sian priors. Then, we simulated two kinds of agents on these
MDPs and used their trajectories as input: The ﬁrst learned

1

0.5

0

)
2
s
(
R

−0.5

−1
−1

1

0.5

0

)
2
s
(
R

−0.5

−1
−1

−0.8

−0.6

−0.4

−0.2

−0.8

−0.6

−0.4

−0.2

Posterior Samples

0

R(s1)

0.2

True Rewards

0

R(s1)

0.2

0.4

0.6

0.8

1

0.4

0.6

0.8

1

Figure 6: Scatter diagrams of sampled rewards of two arbi-
trary states for a given MDP and expert trajectory. Our com-
puted posterior is shown to be close to the true distribution.

a policy by Q-learning on the MDP + reward function. The
learning rate was controlled so that the agent was not allowed
to converge to the optimal policy but came reasonably close.
The second agent executed a policy that maximized the ex-
pected total reward over the next k steps (k was chosen to be
slightly below the horizon time).

For BIRL, we used PolicyWalk to sample the posterior
distribution (3) with a uniform prior. We compared the results
of the two methods by their average (cid:10)2 distance from the true
reward function (Figure 4) and the policy loss with (cid:10)1 norm
(Figure 5) of the learned policy under the true reward. Both
measures show substantial improvement. Note that we have
used a logarithmic scale on the x-axis.

We also measured the accuracy of our posterior distribu-
tion for small N by comparing it with the true distribution
of rewards i.e. the set of generated rewards that gave rise to
the same trajectory by the expert. In Figure 6, we show scat-
ter plots of some rewards sampled from the posterior and the
true distribution for a 16-state MDP. These ﬁgures show that
the posterior is very close to the true distribution.

6.1 From Domain Knowledge to Prior
To show how domain knowledge about a problem can be in-
corporated into the IRL formulation as an informative prior,

IJCAI-07

2590

L

 18
 16
 14
 12
 10
 8
 6
 4
 2
 0

 10

Reward Loss

Uniform
Gaussian
Ising

 100

N

 1000

Figure 7:
Games

Ising versus Uninformed Priors for Adventure

we applied our methods to learning reward functions in ad-
venture games. There, an agent explores a dungeon, seek-
ing to collect various items of treasure and avoid obstacles
such as guards or traps. The state space is represented by
an m-dimensional binary feature vector indicating the po-
sition of the agent and the value of various ﬂuents such as
hasKey and doorLocked. If we view the state-space as
an m-dimensional lattice LS, we see that neighbouring states
in LS are likely to have correlated rewards (e.g.
the value
of doorLocked does not matter when the treasure chest is
picked up). To model this, we use an Ising prior (see [Cipra,
1987]):

(cid:2)

R(s)R(s(cid:4)

) − H

(cid:2)

R(s))

PR(R) =

1

Z exp(−J

(s(cid:2),s)∈N

s

where N is the set of neighbouring pairs of states in LS and
J and H are the coupling and magnetization parameters.

We tested our hypothesis by generating some adventure
games (by populating dungeons with objects from a common
sense knowledge base) and testing the performance of BIRL
with the Ising prior versus the baseline uninformed priors.
The results are in ﬁgure 7 and show that the Ising prior does
signiﬁcantly better.

7 Related Work
The initial work on IRL was done by [Ng and Russell, 2000]
while [Abbeel and Ng, 2004] studied the special case where
rewards can be represented as linear combinations of features
of the state space and gave a max-margin characterization
of the optimal reward function. [Price and Boutilier, 2003]
discusses a Bayesian approach to imitating the actions of a
mentor during reinforcement learning whereas the traditional
literature on apprenticeship learning tries to mimic the be-
haviour of the expert directly [Atkeson and Schaal, 1997].

Outside of computer science, IRL-related problems have
been studied in various guises. In the physical sciences, there
is a body of work on inverse problem theory, i.e.
infering
values of model parameters from observations of a physical
system [Tarantola, 2005].
In control theory, [Boyd et al.,
1994] solved the problem, posed by Kalman, of recovering
the objective function for a deterministic linear system with
quadratic costs.

8 Conclusions and Future Work
Our work shows that improved solutions can be found for
IRL by posing the problem as a Bayesian learning task. We
provided a theoretical framework and tractable algorithms for
Bayesian IRL and our solutions contain more information
about the reward structure than other methods. Our experi-
ments verify that our solutions are close to the true reward
functions and yield good policies for apprenticeship learning.
There are a few open questions remaining:

1. Are there more informative priors that we can construct
for speciﬁc IRL problems using background knowledge?
2. How well does IRL generalize? Suppose the transition
function of the actor and the learner differed, how robust
would the reward function or policy learned from the
actor be, w.r.t the learner’s state space?

Acknowledgements
The authors would like to thank Gerald Dejong and Alexan-
der Sorokin for useful discussions, and Nitish Korula for help
with preparing this paper. This research was supported by
NSF grant IIS 05-46663 and CERL grant DACA42-01-D-
0004-0014.

References
[Abbeel and Ng, 2004] P. Abbeel and A. Y. Ng. Apprenticeship

learning via inverse reinforcement learning. In ICML, 2004.

[Applegate and Kannan, 1993] D. Applegate and R. Kannan. Sam-
In STOC,

pling and integration of near log-concave functions.
1993.

[Atkeson and Schaal, 1997] C. Atkeson and S. Schaal. Robot learn-

ing from demonstration. In ICML, 1997.

[Berger, 1993] J. O. Berger.

Statistical Decision Theory and

Bayesian Analysis. Springer, 2nd edition, 1993.

[Billings et al., 1998] D. Billings, D. Papp, J. Schaeffer, and
D. Szafron. Opponent modeling in Poker. In AAAI, pages 493–
498, Madison, WI, 1998. AAAI Press.

[Boyd et al., 1994] S. Boyd, L. E. Ghaoui, E. Feron, and V. Balakr-
ishnan. Linear matrix inequalities in system and control theory.
SIAM, 1994.

[Cipra, 1987] B. A. Cipra. An introduction to the Ising model. Am.

Math. Monthly, 94(10):937–959, 1987.

[Ng and Russell, 2000] A. Y. Ng and S. Russell. Algorithms for in-
verse reinforcement learning. In ICML, pages 663–670. Morgan
Kaufmann, San Francisco, CA, 2000.

[Price and Boutilier, 2003] B. Price and C. Boutilier. A Bayesian
approach to imitation in reinforcement learning. In IJCAI, 2003.
[Russell, 1998] S. Russell. Learning agents for uncertain environ-

ments (extended abstract). In COLT. ACM Press, 1998.

[Sargent, 1994] T J Sargent. Do people behave according to Bell-

man’s principle of optimality? JEP, 1994.

[Sutton and Barto, 1998] R.S. Sutton and A.G. Barto. Reinforce-

ment Learning. MIT Press, 1998.

[Tarantola, 2005] A. Tarantola. Inverse Problem Theory and Meth-
ods for Model Parameter Estimation. SIAM, 2nd edition, 2005.
[Vempala, 2005] S. Vempala. Geometric random walks: A survey.
MSRI volume on Combinatorial and Computational Geometry,
2005.

IJCAI-07

2591


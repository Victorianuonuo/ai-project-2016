Computing Semantic Relatedness using

Wikipedia-based Explicit Semantic Analysis

Evgeniy Gabrilovich and Shaul Markovitch

Department of Computer Science

Technion—Israel Institute of Technology, 32000 Haifa, Israel

{gabr,shaulm}@cs.technion.ac.il

Abstract

Computing semantic relatedness of natural lan-
guage texts requires access to vast amounts of
common-sense and domain-speciﬁc world knowl-
edge. We propose Explicit Semantic Analysis
(ESA), a novel method that represents the mean-
ing of texts in a high-dimensional space of concepts
derived from Wikipedia. We use machine learning
techniques to explicitly represent the meaning of
any text as a weighted vector of Wikipedia-based
concepts. Assessing the relatedness of texts in
this space amounts to comparing the corresponding
vectors using conventional metrics (e.g., cosine).
Compared with the previous state of the art, using
ESA results in substantial improvements in corre-
lation of computed relatedness scores with human
judgments: from r = 0.56 to 0.75 for individual
words and from r = 0.60 to 0.72 for texts. Impor-
tantly, due to the use of natural concepts, the ESA
model is easy to explain to human users.

1 Introduction
How related are “cat” and “mouse”? And what about “prepar-
ing a manuscript” and “writing an article”? Reasoning about
semantic relatedness of natural language utterances is rou-
tinely performed by humans but remains an unsurmountable
obstacle for computers. Humans do not judge text relatedness
merely at the level of text words. Words trigger reasoning
at a much deeper level that manipulates concepts—the ba-
sic units of meaning that serve humans to organize and share
their knowledge. Thus, humans interpret the speciﬁc wording
of a document in the much larger context of their background
knowledge and experience.

It has long been recognized that in order to process nat-
ural language, computers require access to vast amounts
of common-sense and domain-speciﬁc world knowledge
[Buchanan and Feigenbaum, 1982; Lenat and Guha, 1990].
However, prior work on semantic relatedness was based on
purely statistical techniques that did not make use of back-
ground knowledge [Baeza-Yates and Ribeiro-Neto, 1999;
Deerwester et al., 1990], or on lexical resources that incor-
porate very limited knowledge about the world [Budanitsky
and Hirst, 2006; Jarmasz, 2003].

We propose a novel method, called Explicit Semantic
Analysis (ESA), for ﬁne-grained semantic representation of
unrestricted natural language texts. Our method represents
meaning in a high-dimensional space of natural concepts de-
rived from Wikipedia (http://en.wikipedia.org), the
largest encyclopedia in existence. We employ text classi-
ﬁcation techniques that allow us to explicitly represent the
meaning of any text in terms of Wikipedia-based concepts.
We evaluate the effectiveness of our method on automatically
computing the degree of semantic relatedness between frag-
ments of natural language text.

The contributions of this paper are threefold. First, we
present Explicit Semantic Analysis, a new approach to rep-
resenting semantics of natural language texts using natural
concepts. Second, we propose a uniform way for computing
relatedness of both individual words and arbitrarily long text
fragments. Finally, the results of using ESA for computing
semantic relatedness of texts are superior to the existing state
of the art. Moreover, using Wikipedia-based concepts makes
our model easy to interpret, as we illustrate with a number of
examples in what follows.

2 Explicit Semantic Analysis

Our approach is inspired by the desire to augment text rep-
resentation with massive amounts of world knowledge. We
represent texts as a weighted mixture of a predetermined set
of natural concepts, which are deﬁned by humans themselves
and can be easily explained. To achieve this aim, we use con-
cepts deﬁned by Wikipedia articles, e.g., COMPUTER SCI-
ENCE, INDIA, or LANGUAGE. An important advantage of
our approach is thus the use of vast amounts of highly orga-
nized human knowledge encoded in Wikipedia. Furthermore,
Wikipedia undergoes constant development so its breadth and
depth steadily increase over time.

We opted to use Wikipedia because it is currently the
largest knowledge repository on the Web. Wikipedia is avail-
able in dozens of languages, while its English version is the
largest of all with 400+ million words in over one million
articles (compared to 44 million words in 65,000 articles in
Encyclopaedia Britannica1).
Interestingly, the open editing
approach yields remarkable quality—a recent study [Giles,
2005] found Wikipedia accuracy to rival that of Britannica.

1

http://store.britannica.com (visited on May 12, 2006).

IJCAI-07

1606

We use machine learning techniques to build a semantic
interpreter that maps fragments of natural language text into
a weighted sequence of Wikipedia concepts ordered by their
relevance to the input. This way, input texts are represented
as weighted vectors of concepts, called interpretation vectors.
The meaning of a text fragment is thus interpreted in terms
of its afﬁnity with a host of Wikipedia concepts. Comput-
ing semantic relatedness of texts then amounts to comparing
their vectors in the space deﬁned by the concepts, for exam-
ple, using the cosine metric [Zobel and Moffat, 1998]. Our
semantic analysis is explicit in the sense that we manipulate
manifest concepts grounded in human cognition, rather than
“latent concepts” used by Latent Semantic Analysis.

Observe that input texts are given in the same form as
Wikipedia articles, that is, as plain text. Therefore, we can use
conventional text classiﬁcation algorithms [Sebastiani, 2002]
to rank the concepts represented by these articles according
to their relevance to the given text fragment. It is this key ob-
servation that allows us to use encyclopedia directly, without
the need for deep language understanding or pre-cataloged
common-sense knowledge. The choice of encyclopedia arti-
cles as concepts is quite natural, as each article is focused on
a single issue, which it discusses in detail.

Each Wikipedia concept is represented as an attribute vec-
tor of words that occur in the corresponding article. Entries
of these vectors are assigned weights using TFIDF scheme
[Salton and McGill, 1983]. These weights quantify the
strength of association between words and concepts.

To speed up semantic interpretation, we build an inverted
index, which maps each word into a list of concepts in which
it appears. We also use the inverted index to discard insignif-
icant associations between words and concepts by removing
those concepts whose weights for a given word are too low.

We implemented the semantic interpreter as a centroid-
based classiﬁer [Han and Karypis, 2000], which, given a text
fragment, ranks all the Wikipedia concepts by their relevance
to the fragment. Given a text fragment, we ﬁrst represent it as
a vector using TFIDF scheme. The semantic interpreter iter-
ates over the text words, retrieves corresponding entries from
the inverted index, and merges them into a weighted vector
of concepts that represents the given text. Let T = {wi}
be input text, and let (cid:2)vi(cid:3) be its TFIDF vector, where vi is
the weight of word wi. Let (cid:2)kj (cid:3) be an inverted index entry
for word wi, where kj quantiﬁes the strength of association
of word wi with Wikipedia concept cj , {cj ∈ c1, . . . , cN }
(where N is the total number of Wikipedia concepts). Then,
the semantic interpretation vector V for text T is a vector of
length N , in which the weight of each concept cj is deﬁned
wi ∈T vi · kj . Entries of this vector reﬂect the relevance
as
of the corresponding concepts to text T . To compute seman-
tic relatedness of a pair of text fragments we compare their
vectors using the cosine metric.

(cid:2)

Figure 1 illustrates the process of Wikipedia-based seman-
tic interpretation. Further implementation details are avail-
able in [Gabrilovich, In preparation].

In our earlier work [Gabrilovich and Markovitch, 2006],
we used a similar method for generating features for text cat-
egorization. Since text categorization is a supervised learning
task, words occurring in the training documents serve as valu-

B uilding Semantic I nterpreter

B uilding weighted
inverted index

W ikipedia

Using Semantic I nterpreter

Semantic
I nterpreter

T ext1

T ext2

word1

wordi

wordn

W eighted list
of concepts
(= W ikipedia
articles)

W eighted

inverted index

V ector
comparison

R elatedness
estimation

W eighted
vector of
W ikipedia
concepts

Figure 1: Semantic interpreter

#
1
2
3
4
5
6
7
8
9
10

Input: “equipment”
Tool
Digital Equipment Corporation
Military technology and equipment
Camping
Engineering vehicle
Weapon
Original equipment manufacturer
French Army
Electronic test equipment
Distance Measuring Equipment

Input: “investor”
Investment
Angel investor
Stock trader
Mutual fund
Margin (ﬁnance)
Modern portfolio theory
Equity investment
Exchange-traded fund
Hedge fund
Ponzi scheme

Table 1: First ten concepts in sample interpretation vectors.

able features; consequently, in that work we used Wikipedia
concepts to augment the bag of words. On the other hand,
computing semantic relatedness of a pair of texts is essen-
tially a “one-off” task, therefore, we replace the bag of words
representation with the one based on concepts.

To illustrate our approach, we show the ten highest-scoring
Wikipedia concepts in the interpretation vectors for sample
text fragments. When concepts in each vector are sorted in the
decreasing order of their score, the top ten concepts are the
most relevant ones for the input text. Table 1 shows the most
relevant Wikipedia concepts for individual words (“equip-
ment” and “investor”, respectively), while Table 2 uses longer
passages as examples.
It is particularly interesting to jux-
tapose the interpretation vectors for fragments that contain
ambiguous words. Table 3 shows the ﬁrst entries in the vec-
tors for phrases that contain ambiguous words “bank” and
”jaguar”. As can be readily seen, our semantic interpreta-
tion methodology is capable of performing word sense dis-
ambiguation, by considering ambiguous words in the context
of their neighbors.

3 Empirical Evaluation

We implemented our ESA approach using a Wikipedia snap-
shot as of March 26, 2006. After parsing the Wikipedia XML
dump, we obtained 2.9 Gb of text in 1,187,839 articles. Upon

IJCAI-07

1607

#

1
2
3
4
5
6

Input: “U.S. intelligence cannot say conclu-
sively that Saddam Hussein has weapons of
mass destruction, an information gap that is
complicating White House efforts to build sup-
port for an attack on Saddam’s Iraqi regime.
The CIA has advised top administration ofﬁ-
cials to assume that Iraq has some weapons of
mass destruction. But the agency has not given
President Bush a “smoking gun,” according to
U.S. intelligence and administration ofﬁcials.”
Iraq disarmament crisis
Yellowcake forgery
Senate Report of Pre-war Intelligence on Iraq
Iraq and weapons of mass destruction
Iraq Survey Group
September Dossier

Iraq War
Scott Ritter
Iraq War- Rationale

7
8
9
10 Operation Desert Fox

Input: “The development of T-cell leukaemia following the oth-
erwise successful treatment of three patients with X-linked se-
vere combined immune deﬁciency (X-SCID) in gene-therapy tri-
als using haematopoietic stem cells has led to a re-evaluation
of this approach. Using a mouse model for gene therapy of X-
SCID, we ﬁnd that the corrective therapeutic gene IL2RG itself
can act as a contributor to the genesis of T-cell lymphomas, with
one-third of animals being affected. Gene-therapy trials for X-
SCID, which have been based on the assumption that IL2RG is
minimally oncogenic, may therefore pose some risk to patients.”
Leukemia
Severe combined immunodeﬁciency
Cancer
Non-Hodgkin lymphoma
AIDS
ICD-10 Chapter II: Neoplasms; Chapter III: Diseases of the blood
and blood-forming organs, and certain disorders involving the
immune mechanism
Bone marrow transplant
Immunosuppressive drug
Acute lymphoblastic leukemia
Multiple sclerosis

Table 2: First ten concepts of the interpretation vectors for sample text fragments.

#

1
2
3
4
5
6
7

8
9
10

Ambiguous word: “Bank”

Ambiguous word: “Jaguar”

“Bank of America”
Bank
Bank of America
Bank of America Plaza (Atlanta)
Bank of America Plaza (Dallas)
MBNA
VISA (credit card)
Bank of America Tower,
New York City
NASDAQ
MasterCard
Bank of America Corporate Center

“Bank of Amazon”
Amazon River
Amazon Basin
Amazon Rainforest
Amazon.com
Rainforest
Atlantic Ocean
Brazil

Loreto Region
River
Economy of Brazil

“Jaguar car models”
Jaguar (car)
Jaguar S-Type
Jaguar X-type
Jaguar E-Type
Jaguar XJ
Daimler
British Leyland Motor
Corporation
Luxury vehicles
V8 engine
Jaguar Racing

“Jaguar (Panthera onca)”
Jaguar
Felidae
Black panther
Leopard
Puma
Tiger
Panthera hybrid

Cave lion
American lion
Kinkajou

Table 3: First ten concepts of the interpretation vectors for texts with ambiguous words.

removing small and overly speciﬁc concepts (those having
fewer than 100 words and fewer than 5 incoming or outgo-
ing links), 241,393 articles were left. We processed the text
of these articles by removing stop words and rare words, and
stemming the remaining words; this yielded 389,202 distinct
terms, which served for representing Wikipedia concepts as
attribute vectors.

To better evaluate Wikipedia-based semantic interpreta-
tion, we also implemented a semantic interpreter based
on another large-scale knowledge repository—the Open
Directory Project (ODP, http://www.dmoz.org). The
ODP is the largest Web directory to date, where con-
the directory, e.g.,
cepts correspond to categories of
TOP/COMPUTERS/ARTIFICIAL INTELLIGENCE.
In this
case, interpretation of a text fragment amounts to computing
a weighted vector of ODP concepts, ordered by their afﬁnity
to the input text.

We built the ODP-based semantic interpreter using an ODP
snapshot as of April 2004. After pruning the Top/World
branch that contains non-English material, we obtained a
hierarchy of over 400,000 concepts and 2,800,000 URLs.

Textual descriptions of the concepts and URLs amounted to
436 Mb of text.
In order to increase the amount of train-
ing information, we further populated the ODP hierarchy by
crawling all of its URLs, and taking the ﬁrst 10 pages en-
countered at each site. After eliminating HTML markup and
truncating overly long ﬁles, we ended up with 70 Gb of ad-
ditional textual data. After removing stop words and rare
words, we obtained 20,700,000 distinct terms that were used
to represent ODP nodes as attribute vectors. Up to 1000 most
informative attributes were selected for each ODP node us-
ing the document frequency criterion [Sebastiani, 2002]. A
centroid classiﬁer was then trained, whereas the training set
for each concept was combined by concatenating the crawled
content of all the URLs cataloged under this concept. Fur-
ther implementation details are available in [Gabrilovich and
Markovitch, 2005].

Using world knowledge requires additional computation.
This extra computation includes the (one-time) preprocess-
ing step where the semantic interpreter is built, as well as the
actual mapping of input texts into interpretation vectors, per-
formed online. On a standard workstation, the throughput of

IJCAI-07

1608

the semantic interpreter is several hundred words per second.

Algorithm

WordNet [Jarmasz, 2003]
Roget’s Thesaurus [Jarmasz, 2003]
LSA [Finkelstein et al., 2002]
WikiRelate! [Strube and Ponzetto, 2006]
ESA-Wikipedia
ESA-ODP

Correlation
with humans

0.33–0.35

0.55
0.56

0.19 – 0.48

0.75
0.65

3.1 Datasets and Evaluation Procedure
Humans have an innate ability to judge semantic relatedness
of texts. Human judgements on a reference set of text pairs
can thus be considered correct by deﬁnition, a kind of “gold
standard” against which computer algorithms are evaluated.
Several studies measured inter-judge correlations and found
them to be consistently high [Budanitsky and Hirst, 2006;
Jarmasz, 2003; Finkelstein et al., 2002], r = 0.88 − 0.95.
These ﬁndings are to be expected—after all, it is this consen-
sus that allows people to understand each other.

In this work, we use two such datasets, which are to the
best of our knowledge the largest publicly available collec-
tions of their kind. To assess word relatedness, we use
the WordSimilarity-353 collection2 [Finkelstein et al., 2002],
which contains 353 word pairs.3 Each pair has 13–16 human
judgements, which were averaged for each pair to produce
a single relatedness score. Spearman rank-order correlation
coefﬁcient was used to compare computed relatedness scores
with human judgements.

For document similarity, we used a collection of 50 docu-
ments from the Australian Broadcasting Corporation’s news
mail service [Lee et al., 2005]. These documents were paired
in all possible ways, and each of the 1,225 pairs has 8–12
human judgements. When human judgements have been av-
eraged for each pair, the collection of 1,225 relatedness scores
have only 67 distinct values. Spearman correlation is not ap-
propriate in this case, and therefore we used Pearson’s linear
correlation coefﬁcient.

3.2 Results
Table 4 shows the results of applying our methodology to
estimating relatedness of individual words. As we can see,
both ESA techniques yield substantial improvements over
prior studies. ESA also achieves much better results than the
other Wikipedia-based method recently introduced [Strube
and Ponzetto, 2006]. Table 5 shows the results for computing
relatedness of entire documents.

On both test collections, Wikipedia-based semantic inter-
pretation is superior to that of the ODP-based one. Two fac-
tors contribute to this phenomenon. First, axes of a multi-
dimensional interpretation space should ideally be as orthog-
onal as possible. However, the hierarchical organization of
the ODP deﬁnes the generalization relation between con-
cepts and obviously violates this orthogonality requirement.
Second, to increase the amount of training data for build-
ing the ODP-based semantic interpreter, we crawled all the
URLs cataloged in the ODP. This allowed us to increase the
amount of textual data by several orders of magnitude, but
also brought about a non-negligible amount of noise, which

2 http://www.cs.technion.ac.il/˜gabr/

resources/data/wordsim353

3Despite its name, this test collection is designed for testing
word relatedness and not merely similarity, as instructions for hu-
man judges speciﬁcally directed the participants to assess the degree
of relatedness of the words. For example, in the case of antonyms,
judges were instructed to consider them as “similar” rather than “dis-
similar”.

Table 4: Computing word relatedness

Algorithm

Bag of words [Lee et al., 2005]
LSA [Lee et al., 2005]
ESA-Wikipedia
ESA-ODP

Correlation
with humans

0.1–0.5

0.60
0.72
0.69

Table 5: Computing text relatedness

is common in Web pages. On the other hand, Wikipedia ar-
ticles are virtually noise-free, and mostly qualify as Standard
Written English.

4 Related Work

The ability to quantify semantic relatedness of texts under-
lies many fundamental tasks in computational linguistics,
including word sense disambiguation, information retrieval,
word and text clustering, and error correction [Budanitsky
and Hirst, 2006]. Prior work in the ﬁeld pursued three main
directions: comparing text fragments as bags of words in vec-
tor space [Baeza-Yates and Ribeiro-Neto, 1999], using lexical
resources, and using Latent Semantic Analysis (LSA) [Deer-
wester et al., 1990]. The former technique is the simplest,
but performs sub-optimally when the texts to be compared
share few words, for instance, when the texts use synonyms
to convey similar messages. This technique is also trivially
inappropriate for comparing individual words. The latter two
techniques attempt to circumvent this limitation.

Lexical databases such as WordNet [Fellbaum, 1998] or
Roget’s Thesaurus [Roget, 1852] encode relations between
words such as synonymy, hypernymy. Quite a few met-
rics have been deﬁned that compute relatedness using vari-
ous properties of the underlying graph structure of these re-
sources [Budanitsky and Hirst, 2006; Jarmasz, 2003; Baner-
jee and Pedersen, 2003; Resnik, 1999; Lin, 1998; Jiang and
Conrath, 1997; Grefenstette, 1992]. The obvious drawback
of this approach is that creation of lexical resources requires
lexicographic expertise as well as a lot of time and effort, and
consequently such resources cover only a small fragment of
the language lexicon. Speciﬁcally, such resources contain few
proper names, neologisms, slang, and domain-speciﬁc tech-
nical terms. Furthermore, these resources have strong lexical
orientation and mainly contain information about individual
words but little world knowledge in general.

WordNet-based techniques are similar to ESA in that both
approaches manipulate a collection of concepts. There are,
however, several important differences. First, WordNet-based
methods are inherently limited to individual words, and their

IJCAI-07

1609

adaptation for comparing longer texts requires an extra level
of sophistication [Mihalcea et al., 2006].
In contrast, our
method treats both words and texts in essentially the same
way. Second, considering words in context allows our ap-
proach to perform word sense disambiguation (see Table 3).
Using WordNet cannot achieve disambiguation, since infor-
mation about synsets is limited to a few words (gloss); in
both ODP and Wikipedia concept are associated with huge
amounts of text. Finally, even for individual words, ESA
provides much more sophisticated mapping of words to con-
cepts, through the analysis of the large bodies of texts associ-
ated with concepts. This allows us to represent the meaning
of words (or texts) as a weighted combination of concepts,
while mapping a word in WordNet amounts to simple lookup,
without any weights. Furthermore, in WordNet, the senses of
each word are mutually exclusive. In our approach, concepts
reﬂect different aspects of the input (see Tables 1–3), thus
yielding weighted multi-faceted representation of the text.

On the other hand, LSA [Deerwester et al., 1990] is a
purely statistical technique, which leverages word cooccur-
rence information from a large unlabeled corpus of text. LSA
does not rely on any human-organized knowledge; rather, it
“learns” its representation by applying Singular Value De-
composition (SVD) to the words-by-documents cooccurrence
matrix. LSA is essentially a dimensionality reduction tech-
nique that identiﬁes a number of most prominent dimensions
in the data, which are assumed to correspond to “latent con-
cepts”. Meanings of words and documents are then com-
pared in the space deﬁned by these concepts. Latent semantic
models are notoriously difﬁcult to interpret, since the com-
puted concepts cannot be readily mapped into natural con-
cepts manipulated by humans. The Explicit Semantic Analy-
sis method we proposed circumvents this problem, as it rep-
resents meanings of text fragments using natural concepts de-
ﬁned by humans.

Our approach to estimating semantic relatedness of words
is somewhat reminiscent of distributional similarity [Lee,
1999; Dagan et al., 1999]. Indeed, we compare the mean-
ings of words by comparing the occurrence patterns across a
large collection of natural language documents. However, the
compilation of these documents is not arbitrary, rather, the
documents are aligned with encyclopedia articles, while each
of them is focused on a single topic.

In this paper we deal with “semantic relatedness” rather
than “semantic similarity” or “semantic distance”, which are
also often used in the literature.
In their extensive survey
of relatedness measures, Budanitsky and Hirst [2006] argued
that the notion of relatedness is more general than that of sim-
ilarity, as the former subsumes many different kind of speciﬁc
relations, including meronymy, antonymy, functional associ-
ation, and others. They further maintained that computational
linguistics applications often require measures of relatedness
rather than the more narrowly deﬁned measures of similarity.
For example, word sense disambiguation can use any related
words from the context, and not merely similar words. Bu-
danitsky and Hirst [2006] also argued that the notion of se-
mantic distance might be confusing due to the different ways
it has been used in the literature.

larity of words, using R&G [Rubenstein and Goodenough,
1965] list of 65 word pairs and M&C [Miller and Charles,
1991] list of 30 word pairs. When only the similarity re-
lation is considered, using lexical resources was often suc-
cessful enough, reaching the correlation of 0.70–0.85 with
human judgements [Budanitsky and Hirst, 2006; Jarmasz,
2003]. In this case, lexical techniques even have a slight edge
over ESA, whose correlation with human scores is 0.723 on
M&C and 0.816 on R&G.4 However, when the entire lan-
guage wealth is considered in an attempt to capture more
general semantic relatedness, lexical techniques yield sub-
stantially inferior results (see Table 1). WordNet-based tech-
niques, which only consider the generalization (“is-a”) rela-
tion between words, achieve correlation of only 0.33–0.35
with human judgements [Budanitsky and Hirst, 2006]. Jar-
masz & Szpakowicz’s ELKB system [Jarmasz, 2003] based
on Roget’s Thesaurus achieves a higher correlation of 0.55
due to its use of a richer set if relations.

Sahami and Heilman [2006] proposed to use the Web as
a source of additional knowledge for measuring similarity of
short text snippets. A major limitation of this technique is that
it is only applicable to short texts, because sending a long text
as a query to a search engine is likely to return few or even no
results at all. On the other hand, our approach is applicable to
text fragments of arbitrary length.

Strube and Ponzetto [2006] also used Wikipedia for com-
puting semantic relatedness. However, their method, called
WikiRelate!, is radically different from ours. Given a pair of
words w1 and w2, WikiRelate! searches for Wikipedia arti-
cles, p1 and p2, that respectively contain w1 and w2 in their
titles. Semantic relatedness is then computed using various
distance measures between p1 and p2. These measures ei-
ther rely on the texts of the pages, or path distances within
the category hierarchy of Wikipedia. On the other hand,
our approach represents each word as a weighted vector of
Wikipedia concepts, and semantic relatedness is then com-
puted by comparing the two concept vectors.

Thus, the differences between ESA and WikiRelate! are:

1. WikiRelate! can only process words that actually occur
in titles of Wikipedia articles. ESA only requires that the
word appears within the text of Wikipedia articles.

2. WikiRelate! is limited to single words while ESA can

compare texts of any length.

3. WikiRelate! represents the semantics of a word by either
the text of the article associated with it, or by the node
in the category hierarchy. ESA has a much more so-
phisticated semantic representation based on a weighted
vector of Wikipedia concepts.

Indeed, as we have shown in the previous section, the richer
representation of ESA yields much better results.

5 Conclusions

We proposed a novel approach to computing semantic relat-
edness of natural language texts with the aid of very large

4WikiRelate!

[Strube and Ponzetto, 2006] achieved relatively

Prior work in the ﬁeld mostly focused on semantic simi-

low scores of 0.31–0.54 on these domains.

IJCAI-07

1610

scale knowledge repositories. We use Wikipedia and the
ODP, the largest knowledge repositories of their kind, which
contain hundreds of thousands of human-deﬁned concepts
and provide a cornucopia of information about each concept.
Our approach is called Explicit Semantic Analysis, since it
uses concepts explicitly deﬁned and described by humans.

Compared to LSA, which only uses statistical cooc-
currence information, our methodology explicitly uses the
knowledge collected and organized by humans. Compared to
lexical resources such as WordNet, our methodology lever-
ages knowledge bases that are orders of magnitude larger and
more comprehensive.

Empirical evaluation conﬁrms that using ESA leads to sub-
stantial improvements in computing word and text related-
ness. Compared with the previous state of the art, using ESA
results in notable improvements in correlation of computed
relatedness scores with human judgements: from r = 0.56
to 0.75 for individual words and from r = 0.60 to 0.72 for
texts. Furthermore, due to the use of natural concepts, the
ESA model is easy to explain to human users.

6 Acknowledgments

We thank Michael D. Lee and Brandon Pincombe for making
available their document similarity data. This work was par-
tially supported by funding from the EC-sponsored MUSCLE
Network of Excellence.

References

[Baeza-Yates and Ribeiro-Neto, 1999] Ricardo Baeza-Yates and
Berthier Ribeiro-Neto. Modern Information Retrieval. Addison
Wesley, New York, NY, 1999.

[Banerjee and Pedersen, 2003] Satanjeev Banerjee and Ted Peder-
sen. Extended gloss overlaps as a measure of semantic related-
ness. In IJCAI, pages 805–810, 2003.

[Buchanan and Feigenbaum, 1982] B. G. Buchanan and E. A.
Feigenbaum. Forward.
In R. Davis and D. B. Lenat, editors,
Knowledge-Based Systems in Artiﬁcial Intelligence. McGraw-
Hill, 1982.

[Budanitsky and Hirst, 2006] Alexander Budanitsky and Graeme
Hirst. Evaluating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13–47, 2006.

[Dagan et al., 1999] Ido Dagan, Lillian Lee, and Fernando C. N.
Pereira. Similarity-based models of word cooccurrence probabil-
ities. Machine Learning, 34(1–3):43–69, 1999.

[Deerwester et al., 1990] S. Deerwester, S. Dumais, G. Furnas,
T. Landauer, and R. Harshman. Indexing by latent semantic anal-
ysis. JASIS, 41(6):391–407, 1990.

[Fellbaum, 1998] Christiane Fellbaum, editor. WordNet: An Elec-

tronic Lexical Database. MIT Press, Cambridge, MA, 1998.

[Finkelstein et al., 2002] Lev Finkelstein, Evgeniy Gabrilovich,
Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. Placing search in context: The concept revisited.
ACM TOIS, 20(1):116–131, January 2002.

[Gabrilovich and Markovitch, 2005] Evgeniy Gabrilovich

and
Shaul Markovitch. Feature generation for text categorization
using world knowledge. In IJCAI’05, pages 1048–1053, 2005.

[Gabrilovich and Markovitch, 2006] Evgeniy Gabrilovich

and
Shaul Markovitch. Overcoming the brittleness bottleneck using
Wikipedia: Enhancing text categorization with encyclopedic
knowledge. In AAAI’06, pages 1301–1306, July 2006.

[Gabrilovich, In preparation] Evgeniy Gabrilovich. Feature Gener-
ation for Textual Information Retrieval Using World Knowledge.
PhD thesis, Department of Computer Science, Technion—Israel
Institute of Technology, Haifa, Israel, In preparation.

[Giles, 2005] Jim Giles. Internet encyclopaedias go head to head.

Nature, 438:900–901, 2005.

[Grefenstette, 1992] Gregory Grefenstette. SEXTANT: Exploring
unexplored contexts for semantic extraction from syntactic anal-
ysis. In ACL’92, pages 324–326, 1992.

[Han and Karypis, 2000] Eui-Hong

and George
Karypis. Centroid-based document classiﬁcation: Analysis and
experimental results. In PKDD’00, September 2000.

(Sam) Han

[Jarmasz, 2003] Mario Jarmasz. Roget’s thesaurus as a lexical re-
source for natural language processing. Master’s thesis, Univer-
sity of Ottawa, 2003.

[Jiang and Conrath, 1997] Jay J. Jiang and David W. Conrath. Se-
mantic similarity based on corpus statistics and lexical taxonomy.
In ROCLING’97, 1997.

[Lee et al., 2005] Michael D. Lee, Brandon Pincombe,

and
Matthew Welsh. An empirical evaluation of models of text doc-
ument similarity. In CogSci2005, pages 1254–1259, 2005.

[Lee, 1999] Lillian Lee. Measures of distributional similarity. In

Proceedings of the 37th Annual Meeting of the ACL, 1999.

[Lenat and Guha, 1990] D. Lenat and R. Guha. Building Large

Knowledge Based Systems. Addison Wesley, 1990.

[Lin, 1998] Dekang Lin. An information-theoretic deﬁnition of

word similarity. In ICML’98, 1998.

[Mihalcea et al., 2006] Rada Mihalcea, Courtney Corley, and Carlo
Strapparava. Corpus-based and knowledge-based measures of
text semantic similarity. In AAAI’06, July 2006.

[Miller and Charles, 1991] George A. Miller and Walter G.
Charles. Contextual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1–28, 1991.

[Resnik, 1999] Philip Resnik. Semantic similarity in a taxonomy:
An information-based measure and its application to problems of
ambiguity in natural language. JAIR, 11:95–130, 1999.

[Roget, 1852] Peter Roget. Roget’s Thesaurus of English Words

and Phrases. Longman Group Ltd., 1852.

[Rubenstein and Goodenough, 1965] Herbert

John B. Goodenough.
Communications of the ACM, 8(10):627–633, 1965.

and
Contextual correlates of synonymy.

Rubenstein

[Sahami and Heilman, 2006] Mehran Sahami and Timothy Heil-
man. A web-based kernel function for measuring the similarity
of short text snippets. In WWW’06. ACM Press, May 2006.

[Salton and McGill, 1983] G. Salton and M.J. McGill. An Introduc-

tion to Modern Information Retrieval. McGraw-Hill, 1983.

[Sebastiani, 2002] Fabrizio Sebastiani. Machine learning in auto-
mated text categorization. ACM Comp. Surv., 34(1):1–47, 2002.

[Strube and Ponzetto, 2006] Michael Strube and Simon Paolo
Ponzetto. WikiRelate! Computing semantic relatedness using
Wikipedia. In AAAI’06, Boston, MA, 2006.

[Zobel and Moffat, 1998] Justin Zobel and Alistair Moffat. Explor-
ing the similarity space. ACM SIGIR Forum, 32(1):18–34, 1998.

IJCAI-07

1611


r-grams: Relational Grams

Niels Landwehr and Luc De Raedt

Machine Learning Lab, Department of Computer Science

Albert-Ludwigs-University Freiburg

Georges-K¨ohler-Allee 79, 79110 Freiburg, Germany

{landwehr,deraedt}@informatik.uni-freiburg.de

Abstract

We introduce relational grams (r-grams). They up-
grade n-grams for modeling relational sequences of
atoms. As n-grams, r-grams are based on smoothed
n-th order Markov chains. Smoothed distributions
can be obtained by decreasing the order of the
Markov chain as well as by relational generaliza-
tion of the r-gram. To avoid sampling object iden-
tiﬁers in sequences, r-grams are generative models
at the level of variablized sequences with local ob-
ject identity constraints. These sequences deﬁne
equivalence classes of ground sequences, in which
elements are identical up to local identiﬁer renam-
ing. The proposed technique is evaluated in several
domains, including mobile phone communication
logs, Unix shell user modeling, and protein fold
prediction based on secondary protein structure.

1 Introduction
Probabilistic sequence models occupy an important position
within the ﬁeld of machine learning. They are not only
theoretically appealing but have also proven to provide ef-
fective learning algorithms across many application areas,
ranging from natural language processing to bioinformatics
and robotics. In traditional sequence models, sequences are
strings s = w1 . . . wk over a (ﬁnite) alphabet Σ. Typically,
the goal is to estimate the joint distribution P(s) over all pos-
sible strings. Given P(s) several tasks can be solved, includ-
ing sequence classiﬁcation, sampling, or predicting the next
event in a sequence given a history of preceding events.

Recent technological advances and progress in artiﬁcial
intelligence has led to the generation of structured data se-
quences. One example is that of the smart phone, where
communication events of many phone users have been logged
during extended periods of time [Raento et al., 2006]. Other
ones are concerned with logging the activities and locations
of persons [Liao et al., 2005], or visits to websites [Ander-
son et al., 2002]. Finally, in bioinformatics, sequences of-
ten contain also structural information [Durbin et al., 1998].
These developments together with the interest in statistical
relational learning (see [De Raedt and Kersting, 2003] for
an overview) have motivated the development of probabilis-
tic models for relational sequences. These are sequences of

tuples or atoms such as that indicated in Example 1. The Re-
lational Markov Model (RMM) approach by [Anderson et al.,
2002] extends traditional Markov models to this domain, and
the Logical Hidden Markov Models (LOHMMs) by [Kerst-
ing et al., 2006] upgrade traditional HMMs towards relational
sequences and also allow for logical variables and uniﬁca-
tion. Motivated by these models and the success and simplic-
ity of n-gram models, we introduce relational grams, which
upgrade the traditional n-grams towards relational sequences.
There are two distinguished features of the r-grams. First, as
LOHMMs and RMMs they allow one to work with general-
ized, i.e. abstract atoms in the sequences. For instance, in the
bioinformatics example the atom helix(right,alpha,short)
can be generalized to helix(X,alpha,Y) to describe an alpha-
helix of any orientation and length. Secondly, the use of
variables and uniﬁcation allows one to make abstraction of
object identiﬁers and to share information between events.
For example, the (abstract) sub-sequence outcall(X,fail),
outtxt(X) describes that a user, after failing to reach a per-
son, writes a text message to the same person, without stating
the identity of the person. This is especially important when
generalizing the patterns across phones or users, as the ob-
jects referred to will typically be different, and the precise
identiﬁers do not matter but the relationships and events they
occur in do.

The paper is structured as follows: in Section 2, we intro-
duce relational sequences and deﬁne the notion of a genera-
tive model that takes into account the nature of identiﬁers and
object identity; in Section 3, we then start from n-grams to
derive r-grams; in Section 4, we report on some experiments,
and ﬁnally, in Section 5, we conclude and touch upon related
work.

2 Relational Sequences
An atom p(t1, . . . , tn) is a relation p of arity n that is fol-
lowed by n terms ti. We will work with three kinds of terms:
constants (in slanted font), identiﬁers (in italic) and vari-
ables (starting with an upper case character). Furthermore,
for each argument position i
the relations are typed, i.e.
of the relation p, one can either have constants or identi-
ﬁers. Variables may appear at any position. We shall also
distinguish constant-variables from identiﬁer-variables; the
former can be instantiated to constants, the latter to identi-
ﬁers. They will be written in italic or slanted font respec-

IJCAI-07

907

Example 1. The following sequences over atoms are examples for relational sequences from different domains:

outcall(116513 ,fail), outcall(116513 ,fail), incall(116513 ,succ), outtxt(213446 ), intxt(213446 ), . . .
mkdir(rgrams), ls(rgrams), emacs(rgrams.tex ), latex(rgrams.tex ), . . .
strand(sa,plus,short), helix(right,alpha,medium), strand(blb,plus,short), helix(right,f3to10,short),. . .

The ﬁrst domain describes incoming and outgoing calls and text messages for a mobile phone user. In the second domain, Unix
shell commands executed by a user are described in terms of command name and arguments. In the third domain, helices and
strands as protein secondary structure elements are deﬁned in terms of their orientation, type, and length.

tively. For instance, the relation outcall has identiﬁers at
the ﬁrst position, and constants at the second one. Therefore,
outcall(111365 ,fail) is type-conform. The types, constants,
identiﬁers, variables and relations together specify the alpha-
¯Σ ⊆ Σ is the set of
bet Σ, i.e. the set of type-conform atoms.
atoms that do not contain identiﬁers. A relational sequence is
then a string s = w1, . . . , wm in Σ∗. An expression is ground
if it does not contain any variable. Example sequences will
typically be ground, cf. Example 1.

Notice that constants typically serve as attributes describ-
ing properties of the relations, and identiﬁers identify particu-
lar objects in the domain. Identiﬁers have no special meaning
and they are only used for sharing object identity between
events. Moreover, there is no ﬁxed vocabulary for identiﬁers
which is known a priori, rather, new identiﬁers will keep ap-
pearing when applying a model to unseen data. Therefore, it
is desirable to distinguish ground sequences only up to iden-
tiﬁer renaming, which motivates the following deﬁnition.

q1 . . . qm are n-congruent

Deﬁnition 1 (Sequence Congruence). Two relational se-
quences p1 . . . pm,
for
i ∈ {1, . . . , m − n} the subsequences pi . . . pi+n−1,
all
ri . . . ri+n−1 are identical up to identiﬁer renaming. Two se-
quences s1, s2 are identical up to identiﬁer renaming if there
exist a one-to-one mapping ϕ of the identiﬁers in s1 to those
in s2 such that s1 equals s2 after replacing the identiﬁers ac-
cording to ϕ.

if

r(x)p(a,y)r(y)p(b,x) and
Example 2. The sequences
r(z)p(a,w)r(w)p(b,u) are 3-congruent (but not 4-congruent).
m-congruent sequences are identical up to identiﬁer re-
naming. For n < m, the deﬁnition takes into account a
limited history: two sequences are congruent if their object
identity patterns are locally identical. Finally we note that n-
congruence deﬁnes an equivalence relation, i.e. it is reﬂexive,
symmetric and transitive.

Let us now deﬁne a generative model for relational se-
quences.
It should not sample actual identiﬁer values, but
rather equivalence classes of congruent sequences. This
yields the following deﬁnition:
Deﬁnition 2 (Generative Model). Let Σ be a relational al-
phabet. Let S(Σ) be the set of all ground sequences of length
m over Σ, and let Sn(Σ) be the set of equivalence classes
induced on S(Σ) by n-congruence. Then a generative model
of order n over Σ deﬁnes a distribution over Sn(Σ).

Such a generative model can be learned from a set of
ground sequences and the alphabet Σ. In its simplest form,
the learning problem can be stated as maximum likelihood
estimation:

Given

• a relational alphabet Σ
• a set S of ground sequences over Σ
• n ∈ N
• a family Λ of generative models of order n over Σ,

Find a model λ ∈ Λ that maximizes

(cid:2)

P (S | λ) :=

P ([s] | λ)

s∈S

where [s] denotes the equivalence class of s with regard to n-
congruence. A simple instance of such a family of generative
models will be introduced in the next section.

3 r-gram Models for Relational Sequences

Markov Chains are amongst the simplest yet most success-
ful approaches for sequence modeling. n-grams are based on
higher-order Markov chains, and employ certain smoothing
techniques to avoid overﬁtting the sample distribution. In this
section, we will brieﬂy review n-grams, and then propose a
simple extension of n-grams for relational sequences.

3.1 n-grams: Smoothed Markov Chains

n-grams deﬁne a distribution over sequences w1...wm of
length m by an order n − 1 Markov assumption:

m(cid:2)

P (w1...wm) =

P (wi | wi−n+1...wi−1)

i=1

(where wi−n+1 is a shorthand for wmax(1,i−n+1)).
In the
most basic case, the conditional probabilities are estimated
from a set S of training sequences in terms of ”gram” counts:

Pn(wi | wi−n+1 . . . wi−1) = C(wi−n+1 . . . wi)
C(wi−n+1 . . . wi−1)

(1)

where C(w1...wk) is the number of times w1...wk appeared
as a subsequence in any s ∈ S. Note that this is indeed the
estimate maximizing the likelihood.

The gram order n deﬁnes the tradeoff between reliability of
probability estimates and discriminatory power of the model.
Rather than selecting a ﬁxed order n, performance can be in-
creased by combining models of different order which are
discriminative but can still be estimated reliably [Manning
and Sch¨utze, 1999]. The two most popular approaches are
back-off and interpolation estimates. In this paper, we will

IJCAI-07

908

n(cid:3)

k=1

focus on the latter approach, which deﬁnes conditional distri-
butions as linear combinations of models of different order:

P (wi | wi−n+1 . . . wi−1) =

αkPk(wi | wi−k+1 . . . wi−1)

(cid:4)

(2)
k=1 αk = 1,
where the α1, ..., αn are suitable weights with
and the lower-order distributions Pk(wi | wi−k+1 . . . wi−1)
are estimated according to maximum likelihood (Equation 1).
Several more advanced smoothing techniques have been pro-
posed (cf. [Manning and Sch¨utze, 1999]), but are beyond the
scope of this paper.

n

r-grams: Smoothed Relational Markov Chains
form
The key idea behind r-grams is a

ground relational

sequences

the

of

3.2
Consider
g1 . . . gm ∈ S(Σ).
Markov assumption

P (g1...gm) =

m(cid:2)

i=1

P (gi | gi−n+1...gi−1).

(3)

However, deﬁning conditional probabilities at the level of
ground grams does not make sense in the presence of object
identiﬁers. Thus, ground grams will be replaced by general-
ized ones. Generality is deﬁned by a notion of subsumption:
Deﬁnition 3 (Subsumption). A relational sequence l1, . . . , lk
subsumes another sequence k1, . . . , kn with substitution θ,
notation l1, . . . , lk (cid:4)θ k1, . . . , kn, if and only if k ≤ n
liθ = ki. A substitution is a set
and ∀i, 1 ≤ i ≤ k :
{V1/t1, . . . , Vl/tl} where the Vi are different variables and
the ti are terms such that no identiﬁer or identiﬁer variable
occurs twice in {t1, ..., tl}.

The restriction on the allowed substitutions implements the
object identity subsumption of [Semeraro et al., 1995]. The
notion of sequence subsumption is due to [Lee and De Raedt,
2003]. It can be tested in linear time.
Example 3. Let Y, Z, U, V be identiﬁer variables.

r(X )p(a, Y )r(Y ) (cid:4)θ1
r(X )p(a, Y )r(Z ) (cid:4)θ2

r(w )p(a, u)r(u)
r(W )p(a, U )r(V )

but

r(X )p(a, Y )r(Z ) (cid:7)(cid:4) r(W )p(a, U )r(U ).

with θ1 = {X/w, Y /u} and θ2 = {X/W, Y /U, Z/V }.

We can now reﬁne equation 3 to take into account general-

ized sequences. This will be realized by deﬁning

m(cid:2)

P (g1...gm) =

P (li | li−n+1...li−1)

i=1

where li−n+1...li (cid:4)θ gi−n+1...gi. This generalization ab-
stracts from identiﬁer values and at the same time yields
smoothed probability estimates, with the degree and charac-
teristic of the smoothing depending on the particular choice
of li−n+1...li. This is formalized in the following deﬁnition.
Deﬁnition 4 (r-gram model). An r-gram model R of order n
over an alphabet Σ is a set of relational grams

1
n

l

∨ ... ∨ ld
n

← l1...ln−1

where

∈ ¯Σ∗;

n

1. ∀i : l1...ln−1li
2. ∀i : li
3. ∀i : li
Pr(li

n

n contains no constant-variables;
n is annotated with the probability values
| l1...ln−1) such that

(cid:4)

d

i=1 Pr(li
n
n; i.e.

| l1...ln−1) = 1
the heads are

(cid:7)(cid:4) l1...ln−1lj

4. ∀i (cid:7)= j : l1...ln−1li
mutually exclusive.

n

Example 4. The following is an example of an order 2 rela-
tional gram in the mobile phone domain (see Example 1).

0.3 outtxt(X)
0.05 outtxt(Y )
0.2 outcall(X, fail)
...

0.05 intxt(Y )

⎫⎪⎪⎪⎬
⎪⎪⎪⎭

← outcall(X, fail)

It states that after not reaching a person a user is more likely
to write a text message to this person than to somebody else.
We still need to show that an r-gram model R deﬁnes a dis-
tribution over relational sequences. We ﬁrst discuss a basic
model by analogy to an unsmoothed n-gram, before extend-
ing it to a smoothed one in analogy to Equation 2.

n

∨ ... ∨ ld
n

A Basic Model
In the basic r-gram model, for any ground sequence g1...gn−1
there is exactly one gram l1
← l1...ln−1 with
l1...ln−1 (cid:4)θ g1...gn−1. Its body l1...ln−1 is the most speciﬁc
¯Σ∗ subsuming g1...gn−1. According to Equa-
sequence in
tion 3, we start by deﬁning a probability PR(g | g1...gn−1)
for any ground atom g given a sequence g1...gn−1 of ground
literals. Let g be a ground literal and consider the above gram
r subsuming g1...gn−1. If there is an i ∈ {1, ..., d} such that
l1...ln−1li
n
PR(g | g1...gn−1) := Pr(g | g1...gn−1) := Pr(li
| l1...ln−1)
Otherwise, PR(g | g1...gn−1) = 0. From PR(g | g1...gn−1),
a probability value PR(g1...gm) can be derived according to
Equation 3. Note that this is not a distribution over all ground
sequences of length m, as the model does not distinguish be-
tween n-congruent sequences. Instead, the following holds:
Lemma 1. Let R be an order n r-gram over Σ, and
s, s(cid:4) ∈ S(Σ) be relational sequences with s n-congruent to
s(cid:4). Then PR(s) = PR(s(cid:4)) .

(cid:4)θ g1...gn−1g it is unique and we deﬁne

n

Let us therefore deﬁne PR([s])

:= PR(s) for any
[s]∈Sn(Σ) PR([s]) = 1. There-

[s] ∈ Sn(Q). Furthermore,
fore,
Theorem 1. An order n r-gram over Σ is a generative model
over Σ.
Example 5. Consider the r-gram model R with grams

(cid:4)

p(a, X ) ∨ p(b, X ) ← r(X )

r(X ) ← p(b, X )
r(X ) ∨ r(Y ) ← p(a, X )

r(X ) ← 

 is an
and uniform distributions over head literals.
that only matches at
artiﬁcial start symbol
the begin-
The ground sequence g1...g5 =
ning of the sequence.
r(u)p(a,u)r(v)p(b,v)r(v) has probability PR(g1...g5) = 1 ·
0.5 · 0.5 · 0.5 · 1 = 0.125.

IJCAI-07

909

Smoothing r-grams
In the basic model, there was exactly one gram r ∈ R sub-
suming a ground subsequence g1...gn−1, namely the most
speciﬁc one. As for n-grams, the problem with this approach
is that there is a large number of such grams and the amount
of training data needed to reliably estimate all of their fre-
quencies is prohibitive unless n is very small. For n-grams,
grams are therefore generalized by shortening their bodies,
i.e., smoothing with k-gram estimates for k < n (Equation 2).
The basic idea behind smoothing in r-grams is to generalize

grams logically, and mix the resulting distributions:
Pr(g | g1...gn−1)

PR(g | g1...gn−1) =

(cid:3)

r∈ ˆ
R

αr
α

r∈ ˆ

(cid:4)

where Pr(g | g1...gn−1) is the probability deﬁned by r
ˆR is the subset of grams in R sub-
as explained above,
suming g1...gn−1, and α is a normalization constant, i.e.
α =
R αr. The more general r, the more smooth the
probability estimate Pr(g | g1...gn−1) will be. The actual de-
gree and characteristic of the smoothing is deﬁned by the set
of matching r-grams together with their relative weights αr.
By analogy with n-grams, additional smoothing can be
obtained by also considering relational grams r ∈ R with
shorter bodies l1...lk−1, k < n. However, there is a sub-
tle problem with this approach: Relational grams of order
k < n deﬁne a probability distribution over Sk(Σ) rather than
Sn(Σ), i.e. the sequences are partitioned into a smaller num-
ber of equivalence classes. However, this can be taken care
of by a straightforward normalization, which distributes the
probability mass assigned to an equivalence class modulo k
equally among all subclasses modulo n.
Example 6. Consider the r-gram model R with grams r,q
given by

r :
q :

r(X ) ∨ r(Y ) ← r(X )

r(X ) ←

uniform distribution over head literals and αr = αq = 0.5.
We expect PR(r(u) | r(v)) + PR(r(v) | r(v)) = 1. However,
when directly mixing distributions
PR(r(u) | r(v )) = αrPr(r(Y ) | r(X ))+αqPq(r(X )) = 0.75
PR(r(v ) | r(v )) = αrPr(r(X ) | r(X ))+αqPq(r(X )) = 0.75,
as the r-gram q does not distinguish between the sequences
r(x )r(x ) and r(x )r(y). Instead, we mix by

PR(r(x ) | r(y)) = αrPr(r(X ) | r(X )) +

1

γ

αqPq(r(X ))

where γ = 2 is the number of subclasses modulo
2-congruence of the class [r(X)].

The ultimate level of smoothing can be obtained by a rela-
tional gram r of the form l1
n are fully
variablized (also for non-identiﬁer arguments). For this gram

∨ ... ∨ ld
n

n

← where the li
a(cid:2)

Pr(g | g1...gn−1) = Pr(li

n

)

P (Xj = xj)

3.3 Building r-grams From Data
To learn an r-gram from a given set S of training sequences,
we need to 1) choose the set R of relational grams; 2) es-
timate their corresponding conditional probabilities; and 3)
deﬁne the weights αr for every r ∈ R. Before specifying our
algorithm, we need to deﬁne counts in the relational setting:
C(l1 . . . lk) = |{i|s1 . . . sm ∈ S and l1 . . . lk (cid:4)θ si . . . si+k}|
(4)

Our algorithm to learn r-grams is speciﬁed below:
r-grams(input: sequences S; alphabet: Σ; parameters: γ, n)
1 B := {l1 . . . lk−1 ∈ ¯Σ∗|C(l1 . . . lk−1) > 0 and k ≤ n}
2
3

for each l1 . . . lk−1 ∈ B

do let {l1

} contain all maximally speciﬁc
k . . . ld
k
∈ ¯Σ such that C(l1 . . . lk−1li
literals li
k
add r = l1
∨ · · · ∨ ld
k
i
(cid:9)
k)
|l1 . . . lk−1) = C(l1...lk−1l
C(l1...lk−1)

← l1 . . . lk−1 to R with

k

) > 0

k

g1...gn−1g∈S(r) Pr(g | g1...gn−1).

4

5
6

7
8

Pr(li
k
L(r) :=
t := |S(r)|
αr := L(r)

γ
t

return R

In Line 1 of the algorithm, one computes all r-grams that
occur in the data. Notice that no identiﬁers occur in these
¯Σ∗). This can be realized using either a fre-
r-grams (cf.
quent relational sequence miner, such as MineSeqLog [Lee
and De Raedt, 2003], or using an on-the-ﬂy approach.
In
the latter case, a relational gram with body l1...lk is only
built and added to R when and if it is needed to evaluate
PR(g | g1...gn−1) with l1...lk (cid:4) g1...gk on unseen data. In
Line 3, all possible literals li
k are sought that occur also in
the data. They are maximally speciﬁc, which means that they
do not contain constant-variables (cf. condition 2 of Deﬁni-
tion 4). Line 4 then computes the maximum likelihood esti-
mates and Lines 5–7 the weight αr. Here S(r) denotes the set
of all ground subsequences g1...gn−1g appearing in the data
which are subsumed by r. The likelihood L(r) of r deﬁned
in Line 5 is a measure for how well the distribution deﬁned
by r matches the sample distribution. The αr as in Line 7 is
then deﬁned in terms of |S(r)| and the parameter γ, which
controls the tradeoff between smoothness and discrimination.
Highly discriminative (speciﬁc) rules have higher likelihood
than more general ones as they are able to ﬁt the sample dis-
tribution better, and thus receive more weight if γ > 0.

4 Experiments

This section reports on an empirical evaluation of the pro-
posed method in several real-world domains. More speciﬁ-
cally, we seek to answer the following questions:

(Q1) Are r-grams competitive with other state-of-the-art ap-

proaches for relational sequence classiﬁcation?

j=1

(Q2) Is relational abstraction, especially of identiﬁers, use-

where X1, ..., Xa are the non-identiﬁer arguments of li
n and
x1, ..., xa their instantiations in g. This case corresponds to
an out-of-vocabulary event (observing an event that was not
part of the training vocabulary) for n-grams.

ful?

Experiments were carried out on real-world sequence clas-
siﬁcation problems from three domains. In the Unix Shell
domain [Greenberg, 1988; Jacobs and Blockeel, 2003],

IJCAI-07

910

LOHMM LOHMM + FK

Domain
Protein
Domain
Unix-50
Unix-1000

r-grams

83.3

r-grams
93.8 ± 2.7
97.2 ± 0.4

74.0
kNN
91.0
95.3

82.7
C4.5
88.8
94.7

Domain
Protein
Unix-50
Unix-1000
Phone I
Phone II

r-grams

83.3

93.8 ± 2.7
97.2 ± 0.4
95.0 ± 2.0
93.3 ± 14.9

n-grams

79.7
74.4
76.3
30.0
33.3

n-grams w/o IDs

83.3

95.6 ± 2.7
97.1 ± 0.4
86.8 ± 3.3
86.7 ± 18.3

Table 1: Comparison of classiﬁcation accuracy of r-grams to
Logical Hidden Markov models and Fisher kernels in the Pro-
tein Fold domain, and to k-nearest neighbor and C4.5 in the
Unix Shell domain. For protein fold prediction, a single split
into training and test set is used. In the Unix Shell domain, 10
subsets of 50/1000 examples each are randomly sampled, ac-
curacy determined by 10-fold cross-validation and averaged.

Table 2: Accuracy comparison of r-grams to n-grams, and
to n-grams w/o IDs. For Protein/Unix domains settings are
as before. For the Phone I domain, 5 subsets of size 100
have been sampled from the data, a 5-fold cross-validation
is performed on each set and results are averaged. For Phone
II, results are based on one 5-fold cross-validation. Results
for n-grams are based on one sample only.

the task is to classify users as novice programmers or
non-programmers based on logs of 3773 shell sessions con-
taining 94537 commands (constants) and their arguments
(identiﬁers). To reproduce the setting used in [Jacobs and
Blockeel, 2003], we sampled 10 subsets of 50/1000 instances
each from the data, measured classiﬁcation accuracy on these
using 10-fold cross-validation, and averaged the results. In
the Protein fold classiﬁcation domain, the task is to classify
proteins as belonging to one of ﬁve folds of the SCOP hi-
erarchy [Hubbard et al., 1997]. Strand names are treated as
identiﬁers, all other ground terms as constants. This problem
has been used as a benchmark before [Kersting et al., 2006;
Kersting and G¨artner, 2004], and we reproduce the experi-
mental setting used in this earlier work:
the same 200 ex-
amples per fold are used for training, and the remaining ex-
amples as the test set. In the Context Phone domain, data
about user communication behavior has been gathered using
a software running on Nokia Smartphones that automatically
logs communication and context data. In our study, we only
use information about incoming and outgoing calls and text
messages. Phone numbers are identiﬁers, other ground terms
constants. The task in Phone I is to discriminate between
real sequences of events and ”corrupted” ones, which con-
tain the same sequence elements but in random order. For
k ∈ {20, 40, 60, 80, 100}, 5 subsets of size k were sampled
randomly, 5-fold cross-validation performed and averaged for
each k. In Phone II, the task is to classify communication
logs as belonging to one of three users, based only on their
communication patterns but without referring to actual phone
numbers in the event sequence.

In all domains sequence classiﬁcation is performed by
building an r-gram model RC for each class C and la-
beling unseen sequences s with the class that maximizes
PC (s)P (C). We used bigram models in the Phone II domain
and trigram models for all other domains, and the smoothing
parameter γ was set to 1 in all experiments. Learning the r-
gram model was done on-the-ﬂy as explained in Section 3.3.
Table 1 compares the classiﬁcation accuracy of r-grams
with accuracy results from the literature in the Protein Fold
and Unix Shell domains. In the Protein Fold domain, a hand-
crafted Logical Hidden Markov Model achieves 74% accu-
racy [Kersting et al., 2006]. This has been improved to 82.4%
by a ﬁsher kernel approach, in which the gradient of the like-
lihood function of the Logical Hidden Markov Model is used

y
c
a
r
u
c
c
A

 1

 0.95

 0.9

 0.85

 0.8

 0.75

 20

r-grams

n-grams w/o IDs

 30

 40

 50

 60

 70

 80

 90

 100

Number of examples

Figure 1: Accuracy for different training set sizes ranging
from 20 to 100 examples in the Phone I domain. For each
size, 5 sets are sampled, a 5-fold cross-validation is per-
formed and the result averaged.

as input in a support vector machine [Kersting and G¨artner,
2004]. The Unix Shell log classiﬁcation problem was orig-
inally tackled using a k-nearest neighbor method based on
customized sequence similarity [Jacobs and Blockeel, 2003].
In the same paper, the authors present results for a C4.5 de-
cision tree learner using a bag-of-words representation.
In
both cases, r-grams yield competitive classiﬁcation accuracy,
which is a positive answer to question (Q1). Furthermore, we
note that even using a na¨ıve implementation r-grams are com-
putationally efﬁcient. Times for building an r-gram model
ranged from 3 to 240 seconds in the presented experiments1.
In a second set of experiments, the effect of using rela-
tional abstraction was examined in more detail. More pre-
cisely, r-grams were compared to n-grams which implement
non-relational smoothing as outlined in Section 3.1, treating
the atoms in Σ as ﬂat symbols. For these experiments, we
tried keeping identiﬁers in the events (n-grams) or removing
them from the data (n-grams w/o IDs). Accuracy results for
the Protein Fold, Unix Shell and Context Phone domains are

1All experiments were run on standard PC hardware with

3.2GHz processor and 2GB of main memory.

IJCAI-07

911

given in Table 2. If identiﬁers are treated as normal constants,
accuracy is reduced in all cases, especially for the identiﬁer-
rich Unix and Context Phone domains. This is not surpris-
ing, as most identiﬁers appearing in the test data have never
been observed in the training data and thus the correspond-
ing event has probability zero. When identiﬁers are removed
from the data, performance is similar as for r-grams in the
Protein Fold and Unix Shell domain, but worse in the Con-
text Phone domains. On Phone I, r-grams signiﬁcantly out-
perform n-grams w/o IDs (unpaired sampled t-test, p = 0.01).
Figure 1 shows more detailed results on Phone I for differ-
ent numbers of training examples.
It can be observed that
relational abstraction is particularly helpful for small num-
bers of training examples. To summarize, there are domains
where it is possible to ignore identiﬁers in the data, but in
other domains relational abstraction is essential for good per-
formance. Therefore, question (Q2) can be answered afﬁr-
matively as well.

5 Conclusions and Related Work

We have presented the ﬁrst approach to upgrading n-grams to
relational sequences involving identiﬁers. The formalism em-
ploys variables, uniﬁcation and distinguishes constants from
identiﬁers. It also implements smoothing by relationally gen-
eralizing the r-grams. The approach was experimentally eval-
uated and shown to be promising for applications involving
the analysis of various types of logs.

The work that we have presented is related to other work
on analyzing relational and logical sequences. This in-
cludes most notably, the RMMs by [Anderson et al., 2002]
who presented a relational Markov model approach, and the
LOHMMs by [Kersting et al., 2006] who introduced Logical
Hidden Markov Models. RMMs deﬁne a probability distribu-
tion over ground relational sequences without identiﬁers. Fur-
thermore, they do not employ uniﬁcation (or variable propa-
gation) but realize shrinkage through the use of taxonomies
over the constant values appearing in the atoms and decision
trees to encode the probability values. RMMs only consider
ﬁrst order Markov models, i.e.
the next state only depends
on the previous one, so RMMs do not smooth over sequences
of variable length. RMMs have been applied to challenging
applications in web-log analysis. Whereas RMMs upgrade
Markov models, LOHMMs upgrade HMMs to work with log-
ical sequences. As r-grams, they allow for uniﬁcation and
offer also the ability to work with identiﬁers.
In addition,
they allow one to work with structured terms (and functors).
However, as RMMs, they only consider ﬁrst order Markov
models. Furthermore, they do not smooth distributions us-
ing models of different speciﬁcity. Finally, MineSeqLog [Lee
and De Raedt, 2003] is a frequent relational sequences miner
that employs subsumption to test whether a pattern matches
a relational sequence. One might consider employing it to
tackle the ﬁrst step in the r-gram Algorithm.

There are several directions for further work. These in-
clude: extending the framework to work with structured
terms, considering back-off smoothing instead of interpola-
tion, and, perhaps most importantly, applying the work to
challenging artiﬁcial intelligence applications.

Acknowledgments
The authors would like to thank the anonymous reviewers,
Kristian Kersting and Hannu Toivonen for valuable com-
ments, and Mika Raento for making the Context Phone data
available. The research was supported by the European Union
IST programme, contract no. FP6-508861, Application of
Probabilistic Inductive Logic Programming II.

References
[Anderson et al., 2002] C. R. Anderson, P. Domingos, and
D. S. Weld. Relational Markov Models and their Applica-
tion to Adaptive Web Navigation. In Proceedings of the
8th KDD, pages 143–152. ACM, 2002.

[De Raedt and Kersting, 2003] L. De Raedt and K. Kerst-
ing. Probabilistic Logic Learning. SIGKDD Explorations,
5(1):31–48, 2003.

[Durbin et al., 1998] R. Durbin, S. Eddy, A. Krogh, and
G. Mitchison. Biological Sequence Analysis: Probabilistic
Models of Proteins and Nucleic Acids. Cambridge Univer-
sity Press, 1998.

[Greenberg, 1988] S. Greenberg. Using Unix: Collected
Traces of 168 Users. Technical report, Department of
Computer Science, University of Calgary, Alberta, 1988.

[Hubbard et al., 1997] T. Hubbard, A. Murzin, S. Brenner,
and C. Chotia. SCOP: a Structural Classiﬁcation of Pro-
teins Database. Nucleic Acids Res., 27(1):236–239, 1997.
[Jacobs and Blockeel, 2003] N. Jacobs and H. Blockeel.
User Modeling with Sequential Data. In Proceedings of
the 10th HCI, pages 557–561, 2003.

[Kersting and G¨artner, 2004] K. Kersting and T. G¨artner.
Fisher Kernels for Logical Sequences. In Proceedings of
the 15th ECML, volume 3201 of Lecture Notes in Com-
puter Science, pages 205–216. Springer, 2004.

[Kersting et al., 2006] K. Kersting, L. De Raedt,

and
T. Raiko. Logical Hidden Markov Models. Journal of
AI Research, 25:425–456, 2006.

[Lee and De Raedt, 2003] S. D. Lee and L. De Raedt. Min-
ing Logical Sequences Using SeqLog. In Database Tech-
nology for Data Mining, volume 2682 of Lecture Notes in
Computer Science. Springer, 2003.

[Liao et al., 2005] L. Liao, D. Fox, and H. A. Kautz.
Location-based Activity Recognition using Relational
Markov Networks.
In Proceedings of the 19th IJCAI,
pages 773–778. Professional Book Center, 2005.

[Manning and Sch¨utze, 1999] C.

H. Sch¨utze.
guage Processing. The MIT Press, 1999.

and
Foundations of Statistical Natural Lan-

H. Manning

[Raento et al., 2006] M. Raento, A. Oulasvirta, R. Petit, and
H. Toivonen. ContextPhone - a Prototyping Platform
for Context-aware Mobile Applications. IEEE Pervasive
Computing, 4(2):51–59, 2006.

[Semeraro et al., 1995] G. Semeraro, F. Esposito,

and
Ideal Reﬁnement of Datalog Programs. In
D. Malerba.
Proceedings of the 5th LOPSTR, volume 1048 of Lecture
Notes in Computer Science. Springer, 1995.

IJCAI-07

912


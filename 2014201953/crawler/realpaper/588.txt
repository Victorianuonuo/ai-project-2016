Definition and Complexity of Some Basic Metareasoning Problems* 

Vincent Conitzer  and  Tuomas Sandholm 

Carnegie Mellon University 
Computer Science Department 

5000 Forbes Avenue 

Pittsburgh, PA 15213, USA 

{conitzer,sandholm} @cs.cmu.edu 

Abstract 

In  most real-world settings,  due to limited time or 
other resources, an agent cannot perform all poten(cid:173)
tially useful deliberation and information gathering 
actions.  This  leads  to  the metareasoning  problem 
of selecting such actions.  Decision-theoretic meth(cid:173)
ods for metareasoning have been studied in AI, but 
there are few theoretical results on the complexity 
of metareasoning.  We  derive  hardness  results  for 
three  settings  which  most  real  metareasoning  sys(cid:173)
tems  would  have  to  encompass  as  special  cases. 
In  the  first,  the  agent  has  to  decide  how  to  allo(cid:173)
cate its deliberation time across anytime algorithms 
running on  different  problem instances.  We  show 
this to be ATP-complete.  In the second,  the agent 
has to (dynamically) allocate its deliberation or in(cid:173)
formation  gathering  resources  across  multiple  ac(cid:173)
tions  that  it  has  to  choose  among.  We  show  this 
to be AfP-hard even when evaluating each individ(cid:173)
ual  action  is  extremely  simple. 
In  the  third,  the 
agent  has  to  (dynamically) choose  a limited  num(cid:173)
ber of deliberation or information gathering actions 
to  disambiguate  the  state  of the  world.  We  show 
that this is AfP-hard under a natural restriction, and 

hard in general. 

Introduction 

1 
In most real-world settings, due to limited time, an agent can(cid:173)
not  perform  all  potentially  useful  deliberation  actions.  As 
a  result  it  will  generally  be  unable  to  act  rationally  in  the 
world. This phenomenon, known as bounded rationality, has 
been  a long-standing  research  topic  (e.g.,  [3,17]).  Most of 
that research has been descriptive:  the goal has been to char(cid:173)
acterize  how  agents—in  particular,  humans—deal  with  this 
constraint.  Another  strand  of  bounded  rationality  research 
has  the  normative  (prescriptive)  goal  of characterizing  how 
agents  should deal  with  this  constraint.  This  is  particularly 
important when building artificial agents. 

Characterizing how an agent should deal with bounded ra(cid:173)
tionality entails determining how the agent should deliberate. 
*The material in this paper is based upon work supported by the 
National Science Foundation under CAREER Award IRI-9703122, 
Grant IIS-9800994, ITR IIS-0081246, and ITR nS-0121678. 

Because  limited  time  (or other  resources)  prevent  the  agent 
from  performing  all  potentially useful  deliberation  (or infor(cid:173)
mation gathering) actions, it has to select among such actions. 
Reasoning about which deliberation actions to take is called 
metareasoning.  Decision  theory  [7,10]  provides  a  norma(cid:173)
tive basis for metareasoning under uncertainty, and decision-
theoretic  deliberation  control  has  been  widely  studied  in  AI 
(e.g., [2,4-6,8,9,12-15,18-20]). 

However,  the  approach  of using  metareasoning  to  control 
reasoning  is  impractical  if the  metareasoning  problem  itself 
is prohibitively complex.  While this issue is widely acknowl(cid:173)
edged  (e.g.,  [8,12-14]),  there  are  few  theoretical  results  on 
the complexity of metareasoning. 

We  derive  hardness  results  for  three  central  metareason(cid:173)
ing  problems.  In  the  first  (Section  2),  the  agent  has  to  de(cid:173)
cide  how to allocate  its deliberation  time  across  anytime  al(cid:173)
gorithms  running  on  different  problem  instances.  We  show 
this to be  NP-complcte.  In the second metareasoning prob(cid:173)
lem (Section 3), the agent has to (dynamically) allocate its de(cid:173)
liberation or information gathering resources across multiple 
actions that it has to choose among.  We show this to be MV-
hard even when evaluating each individual action is extremely 
simple.  In the third metareasoning problem (Section 4),  the 
agent has to (dynamically) choose a limited number of delib(cid:173)
eration or information gathering actions  to disambiguate  the 
state  of the  world.  We  show  that  this  is  NP-hard  under  a 
natural restriction, and  PSPACE-hard  in general. 

These results have general applicability in that most metar(cid:173)
easoning  systems  must  somehow  deal  with  one  or  more  of 
these problems (in addition to dealing with other issues).  We 
also believe that these  results give a good basic  overview of 
the space of high-complexity issues in metareasoning. 
2  Allocating anytime algorithm time across 

problems 

In  this  section  we  study  the  setting  where  an  agent  has  to 
allocate its deliberation time across different problems—each 
of which the agent can solve using an anytime algorithm. We 
show that this  is hard even  if the agent can  perfectly predict 
the performance of the anytime algorithms. 

2.1  Motivating example 
Consider  a  newspaper  company  that  has,  by  midnight,  re(cid:173)
ceived the next day's orders from newspaper stands in the 3 

RESOURCE-BOUNDED  REASONING 

1099 

problem 
that  is,  whether  there  exists  a  vector 

total  performance  of  at  least  K; 
with 

instances 

to  get  a 

i 

A  reasonable  approach  to  representing  the  performance 
profiles  is  to  use  piecewise  linear performance  profiles.  They 
can  model  any  performance  profile  arbitrarily  closely,  and 
have  been  used  in  the  resource-bounded  reasoning  litera(cid:173)
ture  to  characterize  the  performance  of  anytime  algorithms 
(e.g.  [2]).  We  now  show  that  the  metareasoning  problem  is 
AfP-complete  even  under  this  restriction.  We  will  reduce 
from  the  KNAPSACK  problem.2 

Theorem  1 
even 
linear.3. 

if each  performance  profile 

PERFORMANCE-PROFILES 

is  NP-complete 
is  continuous  and  piecewise 

Proof:  The  problem  is  in  MV  because  we  can  nondctermin-
istically  generate  the 
in  polynomial  time  (since  we  do  not 
need  to  bother  trying  numbers  greater  than  N),  and  given  the 
we  can  verify  if  the  target  value  is  reached  in  polyno(cid:173)
mial  time.  To  show  NP-hardness,  we  reduce  an  arbitrary 
K N A P S A CK  instance  to  the  following  PERFORMANCE-

cities  where  the  newpaper is  read.  The company owns  a  fleet 
of  delivery  trucks  in  each  of  the  cities.  Each  fleet  needs  its 
vehicle  routing  solution  by  5am.  The  company  has  a default 
routing  solution  for each  fleet,  but  can  save  costs  by  improv(cid:173)
ing  (tailoring  to  the  day's  particular orders)  the  routing  solu(cid:173)
tion of any  individual fleet using an anytime algorithm.  In this 
setting,  the  "solution quality"  that the  anytime algorithm pro(cid:173)
vides  on  a  fleet's  problem  instance  is  the  amount  of savings 
compared  to  the  default  routing  solution. 

We  assume  that  the company can  perfectly  predict  the  sav(cid:173)
ings made on  a given  fleet's  problem  instance as a function of 
deliberation  time spent on  it (we  will  prove hardness of metar-
easoning  even  in  this  deterministic  variant).  Such  functions 
are  called 
[2,6,8,9,20]. 
Each  fleet's  problem  instance  has  its  own  performance  pro(cid:173)
file.1  Suppose the performance profiles are as shown in Fig.  1. 

(deterministic)  performance  profiles 

Figure  1:  Performance  profiles  for the routing problems. 

Then  the  maximum  savings  we  can  obtain  with  5  hours  of 
deliberation  time  is  2.5,  for  instance  by  spending  3  hours  on 
instance  1  and  2  on  instance  2.  On  the  other  hand,  if we  had 
until  6am  to  deliberate  (6  hours),  we  could  obtain  a  savings 
of 4  by  spending  6  hours  on  instance  3. 

D e f i n i t i o ns  a nd  results 

2.2 
We  now  define  the  metareasoning  problem  of allocating  de(cid:173)
liberation  across  problems  according  to performance  profiles. 

Definition  1  ( P E R F O R M A N C E - P R O F I L E S)  We 
given  a  list  of performance  profiles  ( f i,  f2,  •  •  •, 
fm) 
each 
f, 
mapping 
liberation  steps  N,  and  a 
whether  we  can  distribute 

are 
(where 
time, 
real  numbers),  a  number  of  de(cid:173)
target  value  K.  We  are  asked 
the  deliberation  steps  across 
the 

is  a  nondecreasing  function  of  deliberation 
to  nonnegative 

'Because the anytime algorithm's performance differs across in(cid:173)
stances,  each  instance  has  its  own  performance  profile  (in  the  set(cid:173)
ting  of deterministic  performance  profiles).  In  reality,  an  anytime 
algorithm's  performance  on  an  instance  cannot  be  predicted  per(cid:173)
fectly.  Rather,  usually  statistical  performance profiles  are kept that 
aggregate across instances.  In that  light one might question the  as(cid:173)
sumption  that  different  instances  have  different  performance  pro(cid:173)
files. However,  sophisticated  deliberation control  systems can con(cid:173)
dition  the  performance  prediction  on  features  of the  instance—and 
this is necessary  if the deliberation control  is to be fully normative. 
(Research has already been conducted on conditioning performance 
profiles  on  instance  features  [8,9,15]  or  results  of deliberation  on 
the instance so far [4,8,9,15,18-20].) 

2This  only  demonstrates  weak  NP-completencss,  as  KNAP(cid:173)
SACK  is  weakly  NP-complete;  thus,  perhaps  pseudopolynomial 
time algorithms exist. 

3If  one  additionally  assumes  that  each  performance  profile  is 
concave, then the metareasoning problem is solvable in polynomial 
time [2].  While returns to deliberation indeed tend to be diminish(cid:173)
ing,  usually this is not the case throughout the performance profile. 
Algorithms often have a setup phase in the beginning during which 
there  is  no  improvement.  Also,  iterative  improvement  algorithms 
can switch to using different local search operators once progress has 
ceased using one operator (for example, once 2-swap has reached a 
local  optimum  in  TSP,  one  can  switch  to  3-swap and  obtain  gains 
from deliberation again) [16]. 

1100 

RESOURCE-BOUNDED  REASONING 

So  we  have  found  a  solution  to 

the  PERFORMANCE-PROFILES  instance.  On  the  other 
hand,  suppose there  is  a  solution  to the  PERFORMANCE-

is no silver, the test will be positive with probability 0.  This 
test takes  3  units  of time.  (3)  Test  for copper at  C.  If there 
is copper,  the  test will  be  positive with probability  l;if there 
is no copper, the test will be positive with probability 0. This 
test takes 2 units of time. 

Given the probabilities of the tests turning out positive un(cid:173)
der various circumstances, one can use Bayes'  rule to com(cid:173)
pute  the  expected  utility  of each  digging  option  given  any 
(lack  of) test result.  For instance,  letting  be the event that 

The  PERFORMANCE-PROFILES  problem  occurs  natu(cid:173)
rally as a subproblem within many metareasoning problems, 
and  thus  its  complexity  leads  to  significant  difficulties  for 
metareasoning.  This  is the case even under the (unrealistic) 
assumption of perfect predictability of the efficacy of deliber(cid:173)
ation. On the other hand, in the remaining two metareasoning 
problems that we analyze, the complexity stems from uncer(cid:173)
tainty about the results that deliberation will provide. 

3  Dynamically allocating evaluation effort 

across options (actions) 

In  this  section  we study the  setting where  an  agent  is  faced 
with  multiple  options  (actions)  from  which  it eventually  has 
to choose one. The agent can use deliberation (or information 
gathering) to evaluate each action.  Given limited time, it has 
to decide which ones to evaluate.  We show that this is  hard 
even in very restricted cases. 

3.1  Motivating example 
Consider an autonomous robot looking for precious metals.  It 
can choose between three sites for digging (it can dig at most 
one site).  At site A  it may find gold;  at site  B,  silver; at site 
C,  copper.  If the robot  chooses  not to dig  anywhere,  it gets 
utility 1  (for saving digging costs).  If the robot chooses to dig 
somewhere,  the  utility of  finding  nothing  is 0;  finding  gold, 
5; finding silver, 3; finding copper, 2. The prior probability of 
there being gold at site A is 1/8, that of finding silver at site B 
is 1/2, and that of finding copper at site C is 1/2. 

In  general,  the  robot could  perform  deliberation  or  infor(cid:173)
mation gathering actions to evaluate the alternative (digging) 
actions.  The metareasoning problem would be the same for 
both,  so  for  simplicity  of exposition,  we  will  focus  on  in(cid:173)
formation gathering only.  Specifically, the robot can perform 
tests to better evaluate the likelihood of there being a precious 
metal at each site, but it has only limited time for such tests. 
The  tests  are  the  following:  (1)  Test  for gold  at  A.  If there 
is gold, the test will be positive with  probability  
if  there 
is no gold, the test will be positive with probability 1/15. This 
test  takes  2  units  of time.  (2)  Test  for  silver  at  B.  If there 
is  silver,  the test will be positive with probability  1;  if there 

utility is 
Doing a similar analysis everywhere, we can rep(cid:173)
resent the problem by trees shown in Fig. 2. In these trees, be-

Figure  2:  Tree  representation  of  the  action  evaluation  in(cid:173)
stance. 

ing at the root represents not having done a test yet, whereas 
being  at  a  left  (right)  leaf represents  the  test  having  turned 
out positive (negative); the value at each node is the expected 
value of digging at this site given the information correspond(cid:173)
ing to that node. The values on the edges are the probabilities 
of the  test  turning  out  positive  or  negative.  We  can  subse(cid:173)
quently  use  these  trees  for analyzing  how  we  should gather 
information.  For instance, if we have 5 units of time, the op(cid:173)
timal  information gathering policy  is to test at  B  first;  if the 
result is positive, test at A; otherwise test at C.  (We omit the 
proof because of space constraint.) 

3.2  Definitions 
In  the example,  there  were  four actions  that we could eval(cid:173)
uate:  digging for a precious metal  at one of three  locations, 
or  not  digging  at  all.  Given  the  results  of all  the  tests  that 
we might undertake on a given action, executing it has some 
expected value.  If,  on the other hand,  we do not (yet) know 
all the results of these tests, we can still associate an expected 
value with the action by taking an additional expectation over 
the outcomes of the tests.  In  what follows,  we will drop the 
word  "expected"  in  its  former meaning  (that  is,  when  talk(cid:173)
ing  about  the  expected  value  given  the  outcomes  of all  the 
tests), because the probabilistic process regarding this expec(cid:173)
tation has no relevance to how the agent should choose to test. 
Hence, all expectations are over the outcomes of the tests. 

While we  have presented this as a model  for information 
gathering planning, we can use this as a model for planning 

RESOURCE-BOUNDED  REASONING 

1101 

(computational) deliberation over multiple actions as well. In 
this case, we regard the tests as computational steps that the 
agent can take toward evaluating an action.4 

To  proceed,  we  need  a  formal  model  of how  evaluation 
effort  (information  gathering  or  deliberation)  invested  on  a 
given  action  changes  the  agent's  beliefs  about  that  action. 
For this model, we generalize the above example to the case 
where we can take multiple evaluation steps on a certain ac(cid:173)
tion (although we will later show hardness even when we can 
take at most one evaluation step per action). 
Definition 3  An action evaluation tree is a tree with 

•  A root r, representing the start of the evaluation; 
•  For each nonleafnode w,  a cost kwfor investing another 

step of evaluation effort at this point; 

•  For each edge e between parent node p and child node 
of transitioning from  p  to  c 

c, a  probability 
upon taking a step of evaluation effort at p; 

value 

•  For each leaf node 
According  to  this  definition,  at  each  point  in  the  evalua(cid:173)
tion of a single action,  the agent's onlv  choice is whether to 
invest  further evaluation  effort,  but  not  how  to  continue  the 
evaluation.  This is a reasonable model when the agent does 
evaluation through deliberation  and has one algorithm at its 
disposal.  However,  in general  the  agent  may  have different 
information gathering actions to choose from at a given point 
in the evaluation, or may be able to choose from among sev(cid:173)
eral deliberation actions (e.g.,  via search control  [1,141).  In 
Section 4, we will discuss how being able to choose between 
tests may introduce drastic complexity even when evaluating 
a single thing.  In  this  section,  however,  our focus is on  the 
complexities introduced by having to choose between differ(cid:173)
ent actions on which to invest evaluation effort next. 

The  agent can  determine  its  expected  value  of an  action, 
given  its  evaluation  so  far,  using  the  subtree  of the  action 
evaluation tree that is rooted at the node where evaluation has 
brought us so far.  This value can be determined in that sub(cid:173)
tree by propagating upwards from the leafs:  for parent p with 
a set of children C, we have 

We  now  present  the  metareasoning  problem.  In  general, 
the agent could use an online evaluation control policy where 
the choices  of how  to invest  future evaluation  effort can  de(cid:173)
pend on evaluation results obtained so far. However, to avoid 
trivial  complexity  issues  introduced  by  the  fact  that  such  a 
contingency strategy for evaluation can be exponential in size, 
we  merely  ask  what  action  the  agent  should  invest  its first 
evaluation step on. 
Definition 4 (ACTION-EVALUATION)  We are given I ac(cid:173)
tion evaluation  trees,  indexed 1  through  I,  corresponding to 
I  different actions.  (The  transition processes of the  trees are 
independent.)  Additionally,  we are  given an  integer N.  We 
are asked whether,  among the online evaluation control poli(cid:173)
cies that spend at most N units of effort, there exists one that 

4For this to be a useful model, it is necessary that updating be(cid:173)
liefs about the value of an action (after taking a deliberation step) is 
computationally easy relative to the evaluation problem itself. 

takes its first evaluation step on action  J,  and gives maximal 
expected utility among online  evaluation  control policies that 
spend at most N units of effort. (If at the end of the deliber(cid:173)
ation process,  we are at node 
then  our utility  is 
max 
because  we will choose the action with  the 
highest expected value.) 

for  tree 

3.3  Results 
We  now show  that even  a severely  restricted  version  of this 
problem  is NP-hard.5 

Theorem 2  ACTION-EVALUATION is  NP-hard,  even when 
all trees have depth  either 0 or  I,  branching factor 2,  and all 
leaf values are  -I,  0,  or  I. 

vations  about  the  constructed  ACTION-EVALUATION  in(cid:173)
stance.  First,  once  we  determine  the  value  of  a  action  to 
be  1,  choosing this  action  is certainly optimal  regardless  of 
the rest of the deliberation  process.  Second,  if at the end of 
the deliberation process we have not discovered the value of 
any action  to  be  1,  then  for any  of the  trees of depth  1,  ei(cid:173)
ther we  have discovered the corresponding action's value to 
be  - 1,  or we  have done no deliberation  on  it at all.  In  the 
latter case,  the expected  value of the action  is always below 
0 
is  carefully  set  to  achieve  this).  Hence,  we  will  pick 
action  3  for value 0.  It follows  that an  optimal  deliberation 
policy  is  one  that  maximizes  the  probability  of discovering 
that a action has value  1.  Now, consider the test set of a pol(cid:173)
icy, which is the set of actions that the policy would evaluate 
if no action turned out to have value  1.  Then,  the probabil(cid:173)
ity  of discovering  that a action  has  value  1  is  simply  equal 
to  the  probability  that  at  least  one  of the  actions  in  this  set 
has  value  1.  So,  in  this case,  the  quality of a policy  is de(cid:173)
termined  by  its  test  set.  Now  we  observe  that  any  optimal 
action is either the one that only evaluates action 2 (and then 
runs out of deliberation time),  or one that has action  1  in  its 

5ACTION-EVALUATION  is  trivial  for /  =  1:  the  answer is 
"yes" if it is possible to take a step of evaluation. The same is true if 
there is no uncertainty with regard to the value of any action; in that 
case any evaluation is irrelevant. 

6Note that using m in the exponent does not make the reduction 
exponential in size, because the length of the binary representation 
of numbers with 

in the exponent is linear in 

1102 

RESOURCE-BOUNDED REASONING 

test set.  (For consider any other policy;  since evaluating ac(cid:173)
tion  1  has minimal cost, and gives strictly higher probability 
of discovering a action with value  1  than evaluating on any 
other action  besides  2,  simply  replacing  any  other action  in 
the test set with  action  1  is possible and  improves the pol(cid:173)
icy.)  Now  suppose  there  is  a  solution  to  the  KNAPSACK 
instance,  that  is,  a  set 
and 

S  such  that 

Then  we can  construct a policy  which  has  as 

(Evaluating  all  these 
test set 
actions costs at most 
deliberation  units.)  The proba(cid:173)
bility of at least one of these actions having value  1  is at least 
the probability that exactly one of them has value  1, which is 

Using our previous observation we 
can conclude that there is an optimal action that has action  1 
in  its  test set,  and since  the order in  which  we evaluate ac(cid:173)
tions in the test set does not matter, there is an optimal policy 
which evaluates  action  1  first.  On  the other hand,  suppose 
there is no solution to the KNAPSACK instance.  Consider a 
policy which has  1  in  its test set,  that is, the test set can be 
expressed as 
Then 
and since there is no solution to 
we  must have 

for some set 

the KNAPSACK instance, it follows that 

But 

the  probability  that at  least one of the  actions  in  the test set 
has value 1 is at most 

On the other hand, 

If we now observe that 
it  follows 

that the policy of just evaluating action 2 is strictly better. So, 
there is no optimal policy which evaluates action 1 first. 
We  have no  proof that the general  problem  is  in 

It 
is an interesting open question whether stronger hardness re(cid:173)
sults can be obtained for it.  For instance, perhaps the general 
problem is 

-complete. 

4  Dynamically choosing how to disambiguate 

state 

We  now  move  to  the  setting  where  the  agent  has  only  one 
thing to evaluate, but can choose the order of deliberation (or 
information  gathering) actions for doing so.  In other words, 
the  agent  has  to  decide  how  to  disambiguate  its  state.  We 
show that this is hard.  (We consider this to be the most sig(cid:173)
nificant result in the paper.) 

4.1  Motivating example 
Consider an autonomous robot that has discovered it is on the 
edge of the floor; there is a gap in front of it. It knows this gap 
can only be one of three things: a staircase (5), a hole (i/), or 
a canyon (C) (assume a uniform prior distribution over these). 
The  robot would  like to continue its exploration  beyond the 
gap.  There  are three courses of physical  action  available  to 
the robot: attempt a descent down a staircase, attempt to jump 

over a hole,  or simply walk away.  If the gap turns out to be 
a staircase and the robot descends down  it,  this gives utility 
2.  If it turns out to be a hole and the robot jumps over it, this 
gives utility  1  (discovering new floors is more interesting).  If 
the robot walks away,  this gives  utility 0 no matter what the 
gap was.  Unfortunately,  attempting to jump over a staircase 
or canyon, or trying to descend into a hole or canyon, has the 
disastrous consequence of destroying the robot  (utility 
). 
It  follows  that  if the  agent  cannot  determine  with  certainty 
what the gap is, it should walk away. 

In order to determine the nature of the  gap,  the  robot  can 
conduct  various  tests  (or queries).  The  tests  can  determine 
the  answers  to  the  following  questions:  (1)  Am  I  inside  a 
building? A yes answer is consistent only with S\ a no answer 
is consistent with 5, H,  C.  (2) If I drop a small  item into the 
gap,  do I hear it  hit the ground?  A yes answer is consistent 
with  S,  H\  a no  answer is consistent with  H,  C.  (3) Can  1 
walk around the gap?  A yes answer is consistent with  S,  H; 
a no answer is consistent with  S, H, C. 

Assume  that if multiple  answers  to  a query  are consistent 
with the true state of the gap, the distributions over such an(cid:173)
swers  are  uniform  and  independent.  Note  that  after  a  few 
queries,  the  set  of  states  consistent  with  all  the  answers  is 
the  intersection of the sets consistent with the individual  an(cid:173)
swers;  once  this  set  has  been  reduced  to  one  element,  the 
robot knows the state of the gap. 

Suppose  the  agent  only  has  time  to  run  one  test.  Then, 
to maximize expected utility, the robot should run test 1, be(cid:173)
cause the other tests give it no chance of learning the state of 
the gap for certain.  Now suppose that the agent has time for 
two tests.  Then the optimal test policy is as follows:  run test 
2 first; if the answer is yes, run test  1  second; otherwise, run 
test 3  second.  (If the true  state  is  5,  this  is discovered  with 
probability 
so total expected utility is 
only give expected  utility 

if it is  H,  this is discovered with  probability 

Starting with test 1 or test 3 can 

4.2  Definitions 
We now define the metareasoning problem of how the agent 
should dynamically choose queries to ask (deliberation or in(cid:173)
formation gathering actions to take) so as to disambiguate the 
state of the world.  While the illustrative example above was 
for information gathering actions, the same model applies to 
deliberation actions for state disambiguation  (such as image 
processing, auditory scene analysis, sensor fusing, etc.). 

Definition 5 (STATE-DISAMBIGUATION)  We are given 

of possible world states;1 

A  probability function  p over 

7If there  are  two  situations  that are  equivalent  from the  agent's 
point of view  (the  agent's optimal  course  of action  is  the  same  and 
the utility is the same), then we consider those situations to be one 
state.  Note  that  two  such  situations  may  lead  to  different  answers 
to the  queries.  For example,  one  situation  may  be  that the  gap  is 
an indoor staircase, and another situation may be that the gap is an 
outdoor staircase.  These  situations  are  considered  to  be  the  same 
state, but will give different answers to the query "Am I inside?". 

RESOURCE-BOUNDED  REASONING 

1103 

where 

gives the 

where each 

A  utility  function 
utility of knowing for certain that the world is in state 
at the  end of the  metareasoning process;  (not knowing 
the state of the world for certain always gives utility 
is  a  list  of subsets 
A  query set 
of 
Each  such  subset corresponds  to an  answer to 
the  query,  and indicates  the  states  that are  consistent 
with that answer. We require that for each state, at least 
one of the answers is consistent with  it:  that is,  for any 
When 
a query is asked,  the answer is chosen (uniformly) ran(cid:173)
domly by nature from the answers to that query that are 
consistent with the world's true state (these drawings are 
independent); 
An integer N;  A  target value G. 

we have 

We are asked whether there exists a policy for asking at most 
N queries that gives expected utility at least  G 
be  the  probability  of identifying  the  state  when  it  is 
expected utility is given by 

the 

4.3  Results 
hardness result, we will first 
Before presenting our 
present  a  relatively  straightforward 
hardness  result  for 
the case where for each query, only one answer is consistent 
with  the  state  of the  world.  This  situation  occurs  when  the 
states are so specific as to provide enough information to an(cid:173)
swer every query.  Our reduction is from 

a  collec(cid:173)
and a positive integer M.  We 
that  is, 

Definition 6 (SET-COVER)  We are given a set 
tion of subsets 
are asked whether any  M  of these  subsets  cover 
whether there is a  subcollection 
such that 
and 
Theorem  3  STATE-DISAMBIGUATION  is 
hard,  even 
when for  each  state-query pair  there  is  only  one  consistent 
answer. 

Proof:  We reduce an  arbitrary SET-COVER  instance  to the 
following  STATE-DISAMBIGUATION  instance.  Let 
and for any 

Let  be uniform.  Let 

let 0. Q 

We claim the instances are equivalent. 

8Therc arc several natural generalizations of this metareasoning 
problem, each of which is at least as hard as the basic variant. One 
allows for positive utilities even if there remains some uncertainty 
about the state at the end of the disambiguation process. In this more 
general case, the utility function would have subsets of  as its do(cid:173)
main (or perhaps even probability distributions over such subsets). 
In general, specifying such utility functions would require space ex(cid:173)
ponential in the number of states, so some restriction of the utility 
function is likely to be necessary; nevertheless, there are utility func(cid:173)
tions specifiable in polynomial space that are more general than the 
one given here. Another generalization is to allow for different dis(cid:173)
tributions for the query answers given. One could also attribute dif(cid:173)
ferent execution costs to different queries.  Finally, it is possible to 
drop the assumption that queries completely rule out certain states, 
and rather take a probabilistic approach. 

First  suppose  there  is  a  solution  to  the  SET-COVER  in(cid:173)

such  that 

stance,  that  is,  a  subcollection 
M  and 
Then  our  policy  for  the  STATE-
DISAMBIGUATION  instance  is  simply  to  ask  the  queries 
corresponding to the elements  of 
in  whichever order and 
unconditionally on  the answers of the query.  If the true state 
is in  5,  we will get  utility  
each  query will  eliminate the  elements  of the  corresponding 
is a set cover, it follows that 
after all  the queries have been asked,  all elements of S have 
been eliminated, and we know that the true state of the world 
is 
so there 
is a solution to the STATE-DISAMBIGUATION instance. 

to get utility  1.  So the expected utility is 

regardless.  If the true  state is 

from consideration.  Since 

On  the  other  hand,  suppose  there  is  a  solution  to  the 
STATE-DISAMBIGUATION  instance,  that  is,  a  policy  for 
asking  at  most  N  queries  that gives  expected  utility  at  least 
G.  Because  given  the  true  state  of the  world,  there  is  only 
one  answer consistent  with  it  for each  query,  it  follows  that 
the queries that will be asked, and the answers given, follow 
deterministically  from  the  true  state of the  world.  Since  we 
cannot derive any utility from cases where the true state of the 
world is not 6, it follows that when it is b, we must be able to 
conclude that this is so in order to get positive expected utility. 
Consider the queries that the policy will ask in this latter case. 
Each of these queries will eliminate precisely the correspond(cid:173)
ing 
Since at the end of the deliberation, all the elements of 
in  fact 
5 must have  been eliminated,  it  follows that these 
cover S.  Hence, if we let 
this 
is a solution to the SET-COVER instance. 

be the collection of these 

We  are now  ready  to present  our 

hardness re(cid:173)
sult.  The reduction  is from  stochastic satisfiability,  which is 

complete  [11]. 

Definition 7 (STOCHASTIC-SAT (SSAT))  We are given a 
Boolean formula  in  conjunctive  normal form  (with  a  set  of 
clauses _  over variables 
We 
play the following game  with  nature:  we pick a  value  for 
subsequently  nature  (randomly) picks  a  value for y1, where(cid:173)
upon we pick a value  for 
after which  nature picks a value 
for 
etc.,  until all variables have  a  value.  We are asked 
whether  there  is  a policy  (contingency plan) for playing  this 
game  such  that  the probability  of the formula  being  eventu(cid:173)
ally satisfied is at least 

Now we can present our 

hardness result. 

Theorem  4  STATE-DISAMBIGUATION is 
Proof:  Let 
elements  of  an  upper  triangular  matrix, 

where 

hard. 
consists  of  the 
that  is, 

.  p  is 
for 

uniform over this  set.  u  is defined as follows: 
a

or 

f

l

l

all 
swers to 
there is a query 
each variable 

where  Nans(q)  is  the  number  of  possible  an(cid:173)

The queries are as follows.  For every 

Additionally,  for 
there are the following two queries:  letting 
(that is, row i in the matrix), and letting 

we have 

1104 

RESOURCE-BOUNDED REASONING 

We  have  n  steps  of  deliberation.  Finally,  the  goal  is  G  = 
First  suppose  there  is  a  solution  to  the  SSAT  in(cid:173)
stance,  that  is,  there  exists  a  contingency  plan  for setting  the 
xt  such  that  the  probability  that  the  formula  is  eventually  sat(cid:173)
we  say 
isfied  is  at  least  
this  corresponds  to  us  selecting  
if  the  answer  to 
query 
we  say 
this  corresponds  to  nature  selecting  
Then,  consider 
the  following  contingency  plan  for asking  queries: 

.  Now,  if we  ask  query  

is 

•  Start  by  asking  the  query  corresponding  to  how  the  first 
is 

variable  is  set  in  the  SSAT  instance  (that  is,  
set to  true, 

is  set  to false)', 

if 

if 

•  So  long  as  all  the  queries  and  answers  correspond  to 
variables  being  selected,  we  follow  the  SSAT  contin(cid:173)
gency  plan;  that  is,  whenever we  have to ask a query,  we 
ask  the query  that  corresponds to the  variable  that  would 
be  selected  in  the  SSAT contingency  plan  if variables  so 
far  had  been  selected  in  a  manner  corresponding  to  the 
queries  and answers  we  have  seen; 

• 

If, on the other hand,  we get  
to  ask 

as an answer, we proceed 
in  that  order; 

•  Finally,  if we  get  C  as  an  answer,  we  simply  stop. 

We  make  two  observations  about  this  policy.  First,  if  the 
we  will  certainly 

true  state  of  the  world  is  one  of  the  
discover  this. 
we  will  receive  answer V1 and  switch  to  qk  queries;  then  if 

(Upon  asking  query  

which  is 

or 

we  may  assume  that  the  policy  that  achieves  the  target 
value  asks  one  of the  former  two  in  this  case  as  well.  It  fol(cid:173)
lows  that  the  part  of this  policy  that  handles  the  cases  where 
no  answers  have  been  either  one  of the  
corresponds 
exactly  to  a  valid  SSAT  policy,  according  to  the  correspon(cid:173)
dence  between  queries/answers  and  variable  selections  out(cid:173)
lined  earlier  in  the  proof.  But  now  we  observe,  as  before, 
that  if  the  true  state  is  
the  probability  that  we  discover 
this  with  the  STATE-DISAMBIGUATION  policy  is  precisely 
the  probability  that  this  SSAT  policy  satisfies  all  the  clauses. 
This  probability  must  be  at  least  
in  order  for  the  STATE-
D I S A M B I G U A T I ON  policy  to  reach  the  target  expected  util(cid:173)
ity value.  So there  is a solution  to the  SSAT  instance. 

The  following  theorem  allows  us  to  make  any  hardness  re(cid:173)
sult  on  STATE-DISAMBIGUATION  go  through  even  when 
restricting  ourselves  to  a  uniform  prior  over  states,  or  to  a 
constant  utility  function  over  the  states. 

T h e o r e ms  Every  STATE-DISAMBIGUATION 
equivalent 
to 
with  a  uniform 
ity 
equivalent  instances  can  be  constructed  in 

is 
instance 
and  to  another  with  a  constant  util(cid:173)
these 

another  STATE-DISAMBIGUATION 
prior 

Moreover, 
linear  time. 

instance 

function 

query 

will  be  

we  will  receive  answer 

and  know  the  state;  whereas  if  
the  other  elements  of  
1  through 
know  the  state.)  Second,  if the  true  state  is  b,  for  any 

we  will  eliminate  all 
and 

with  queries  

query  will  be  either  

This  will  certainly 
eliminate  all  the  
,  so  we  will  know  the  state  at  the  end  if 
and  only  if  we  also  manage  to  eliminate  all  the  clauses.  But 
now  notice  that each  query-answer pair eliminates  exactly  the 
same  clauses  as  the corresponding  variable  selections  satisfy. 
It  follows  that  we  will  know  the  state  in  the  end  if and  only  if 
these corresponding  variable  selections  satisfy  all  the  literals. 
But the process by which the queries and answers are selected 
is  exactly  the  same  as  in  the  SSAT  instance  with  the  solution 
policy.  It  follows  we  discover  the  true  state  with  probability 
at least 1/2. Hence, our total expected utility is at least 

G.  So  there  is  a policy  that achieves  the  goal. 

Now  suppose  there  is  a  policy  that  achieves  the  goal.  We 
first  claim  that  such  a  policy  will  always  discover  the  true 
For  if  a  policy  does  not  manage 
state  if  it  is  one  of  the  
such  that  for  some  combination 
this,  then  there  is  some  
of  answers  consistent  with  
,  the  policy  will  not  discover 
the  state.  Suppose  this  is  indeed  the  true  state.  Since  each 
occurs  with  probability  at  least 
consistent  answer  to  query  
it  follows  that  the  unfavorable  combination  of  an(cid:173)
It  follows 

swers  occurs  with  probability  at  least  

that even  if we  discover the  true  state  in  every  other  scenario, 

5  Conclusion and future research 
In  most  real-world  settings,  due  to  limited  time  or  other  re(cid:173)
sources,  an  agent  cannot  perform  all  potentially  useful  de(cid:173)
liberation  and  information  gathering  actions.  This  leads  to 
the  metareasoning  problem  of  selecting  such  actions  care(cid:173)
fully.  Decision-theoretic  methods  for  metareasoning  have 
been  studied  in  AI  for  the  last  15  years,  but  there  are  few 
theoretical  results  on  the  complexity  of metareasoning. 

We  derived  hardness  results  for  three  metareasoning  prob(cid:173)
lems. 
In  the  first,  the  agent  has  to  decide  how  to  allocate 
its  deliberation  time  across  anytime  algorithms  running  on 
different  problem  instances.  We  showed  this  to  be 
complete.  In  the  second,  the  agent has  to (dynamically)  allo(cid:173)
cate  its deliberation  or information  gathering resources  across 

RESOURCE-BOUNDED  REASONING 

1105 

I ll J  C  Papadimitriou.  Games  against  nature.  Journal  of 

Computer and System Sciences, 31:288-301, 1985. 

112]  David  Parkes  and  Lloyd  Greenwald.  Approximate 
and  compensate:  A  method  for  risk-sensitive  meta-
deliberation  and  continual  computation. 
In AAAI Fall 
Symposium  on  Using  Uncertainty within  Computation, 
2001. 

[13]  S  Russell  and  D  Subramanian.  Provably  bounded-
optimal  agents.  Journal  of Artificial  Intelligence  Re(cid:173)
search,  1:1-36,  1995. 

[14]  S Russell and E Wefald.  Do the right thing:  Studies in 

Limited Rationality.  1991. 

[15]  Tuomas Sandholm and Victor Lesser.  Utility-based ter(cid:173)
mination of anytime algorithms. ECAI Workshop on De(cid:173)
cision Theory for DAI Applications, pp. 88-99, Amster(cid:173)
dam,  1994.  Extended  version:  UMass CS  tech report 
94-54. 

[16]  Tuomas Sandholm and Victor Lesser.  Coalitions among 
computationally bounded agents. Artificial Intelligence, 
94:99-137,  1997.  Early version:  IJCAl-95,662-669. 

[17]  Herbert  A  Simon.  Models of bounded rationality,  vol(cid:173)

ume 2.  MIT Press,  1982. 

[18]  Shlomo  Zilberstein,  Francois  Charpillet,  and  Philippe 
Chassaing.  Real-time problem solving with contract al(cid:173)
gorithms. IJCAI, pp.  1008-1013,  1999. 

[191  Shlomo Zilbcrstein  and  Abdel-Illah  Mouaddib.  Reac(cid:173)
tive control of dynamic  progressive processing.  IJCAI, 
pp. 1268-1273, 1999. 

[20]  Shlomo Zilberstein and Stuart Russell. Optimal compo(cid:173)
sition of real-time systems. Artificial Intelligence, 82(1-
2):181-213, 1996. 

multiple  actions  that  it  has  to  choose  among.  We  showed 
this to be 
hard even when evaluating each individual ac(cid:173)
tion  is  very  simple.  In  the third,  the agent has  to  (dynami(cid:173)
cally) choose a limited number of deliberation or information 
gathering actions to disambiguate the state of the world.  We 
showed that this is 
hard under a natural restriction, and 

hard in general. 

Our results  have  general  applicability  in  that  most metar-
easoning  systems  must  somehow  deal  with  one  or  more  of 
these problems (in addition to dealing with other issues). The 
results are not intended as an argument against metareason-
ing or decision-theoretic deliberation control.  However, they 
do  show  that  the  metareasoning  policies  directly  suggested 
by  decision theory are not always feasible.  This leaves  sev(cid:173)
eral  interesting avenues for future research:  1) investigating 
the complexity of metareasoning when deliberation (and  in(cid:173)
formation  gathering)  is  costly  rather than  limited,  2)  devel(cid:173)
oping optimal metareasoning algorithms that usually run fast 
(albeit,  per  our  results,  not  always),  3)  developing  fast  op(cid:173)
timal  metareasoning  algorithms  for  special  cases,  4)  devel(cid:173)
oping  approximately  optimal  metareasoning  algorithms  that 
are always fast, and 5) developing meta-mctareasoning algo(cid:173)
rithms to control the meta-reasoning, etc. 

References 
[1J  Eric  B  Baum  and  Warren  D  Smith.  A  Bayesian  ap(cid:173)
proach  to  relevance  in  game  playing.  Artificial Intel(cid:173)
ligence, 97(1 -2): 195-242, 1997. 

[3] 

[2]  Mark Boddy and Thomas Dean.  Deliberation schedul(cid:173)
ing  for  problem  solving  in  time-constrained  environ(cid:173)
ments. Artificial Intelligence, 67:245-285,  1994. 
Irving Good.  Twenty-seven principles of rationality.  In 
V Godambe and D Sprott, eds, Foundations of Statisti(cid:173)
cal Inference. Toronto: Holt, Rinehart, Winston, 197). 
[4]  E  Hansen  and  S  Zilbcrstein.  Monitoring  and  control 
of  anytime  algorithms:  A  dynamic  programming  ap(cid:173)
proach. Artificial Intelligence, 126:139-157,2001. 

[5]  Eric  Horvitz.  Principles  and  applications  of  contin(cid:173)
ual  computation.  Artificial Intelligence,  126:159-196, 
2001. 

[6]  Eric J. Horvitz.  Reasoning about beliefs and actions un(cid:173)
der  computational  resource  constraints.  Workshop  on 
Uncertainty  in AI,  pp.  429-444,  Seattle,  Washington, 
1987.  American  Assoc,  for  AI.  Also  in  L.  Kanal,  T. 
Levitt,  and J.  Lemmer,  eds,  Uncertainty in AI 3,  Else(cid:173)
vier, 1989, pp. 301-324. 

[7]  Ronald Howard.  Information value theory. IEEE Trans(cid:173)
actions on  Systems Science and Cybernetics,  2(1):22-
26, 1966. 

[8]  Kate  Larson  and Tuomas  Sandholm.  Bargaining  with 
limited computation:  Deliberation  equilibrium.  Artifi(cid:173)
cial Intelligence, 132(2): 183-217, 2001.  Early version: 
AAAI, pp. 48-55, Austin, TX, 2000. 

[9]  Kate  Larson  and Tuomas  Sandholm.  Costly  valuation 
computation in auctions. Theoretical Aspects of Ratio(cid:173)
nality and Knowledge (TARK), pp. 169-182, 2001. 

[10]  James E Matheson.  The economic value of analysis and 
IEEE  Transactions  on  Systems  Science 

computation. 
and Cybernetics, 4(3):325-332, 1968. 

1106 

RESOURCE-BOUNDED  REASONING 


Monte Carlo Theory as an Explanation of Bagging and Boosting 

Roberto  Esposito 
Universita  di  Torino 

C.so Svizzera  185, Torino, Italy 

csposito@di.unito.it 

Lorenza  Saitta 

Universita del  Piemonte  Orientale 

Spalto Marengo 33, Alessandria,  Italy 

saitta@mfn.unipmn.it 

Abstract 
this  paper  we  propose 

the 

learning. 

In 
framework  of 
Monte  Carlo  algorithms  as  a  useful  one  to  ana(cid:173)
lyze  ensemble 
this 
framework  allows  one  to  guess  when  bagging 
will  be  useful,  explains  why  increasing  the  mar(cid:173)
gin  improves  performances,  and  suggests  a  new 
way  of  performing  ensemble  learning  and  error 
estimation. 

In  particular, 

1  Introduction 
Ensemble  Learning  aims  at  combining  learned  hypothe(cid:173)
ses  for  classification  tasks  instead  of choosing  one.  It  has 
proved  to  be  a  valuable  learning  approach,  and  it  is  ac(cid:173)
tively  investigated  in  the  Machine  Learning  community 
[Freund  1995;  Freund  &  Schapire  1997;  Breiman,  1996; 
Wolpert,  I992|. 

In  addition  to  the  advantages  pointed  out  by  Dietterich 
[2000],  what  really  attracts  the  great  interest  to  ensemble 
learning  is  its  amazing  effectiveness  and  robustness  to 
overfitting.  In  fact,  ensemble  classifiers,  and  specifically 
AdaBoost,  often  show  a  drop  in  generalization  error  far 
beyond  the point  in  which  the training error reaches zero. 

In  this  paper  wc  concentrate  on  two  approaches,  namely 
Bagging  [Breiman,  1996],  and  Boosting,  as  implemented 
in  the  AdaBoost  algorithm  [Freund  &  Schapire,  1997], 
and  we  propose  an  analysis  of  these  two  approaches  by 
exploiting  links  with  Monte  Carlo  theory.  This  parallel 
allows  a  deeper  understanding  of  the  basic  process  un(cid:173)
derlying hypothesis  combination.  In  particular: 

• 

• 

• 

it  suggests  the  conditions  under  which  Bagging  is 
likely to be better than single hypothesis learning, 
it  explains  why  reducing  the  margin  [Schapire  et  ai, 
1998] increases the performances, 
it allows some reasons for robustness to overfitting to 
be conjectured. 

The  article  is  organized  as  follows:  in  Section  2  we 
briefly  review  Bagging  and  AdaBoost  algorithms,  and  in 
Section  3  we  introduce  basic  terminology  and  results 

from  Monte  Carlo  theory.  Section  4  is  the  core  of  the 
paper:  it  introduces  the  link  between  Monte  Carlo  theory 
and  Ensemble  Learning  algorithms,  and  analyzes  the  two 
algorithms  in  the  ideal  situation  of  having  access  to  the 
whole  example  and  hypothesis  spaces.  Section  5  relaxes 
the  hypotheses  of  Section  4.  In  particular,  it  focuses  on 
AdaBoost's  case  offering  an  intuitive  accounting  of  the 
generalization  performances  of  the  algorithm.  Finally, 
Section  6 concludes the paper and proposes  future work. 

2  Bagging  and  AdaBoost 
Bagging  [Breiman,  1996]  is  one  among  the  most  studied 
Ensemble  Learning  algorithms.  It 
takes  in  input  a  learn(cid:173)
ing  set  Lo,  an  integer  number  T,  and  a  (possibly  weak) 
induction  algorithm  A.  At  each  iteration  step  t  the  algo(cid:173)
rithm  creates  a  bootstrap  replicate  L  [Efron  &  Tibshirani, 
1993]  of the  original  learning  set,  and  uses  it  in  conjunc(cid:173)
tion  with  A  to  form  a  new  hypothesis  ht.  When  T  hy(cid:173)
potheses  have  been  learned,  they  are  combined  using  a 
simple  majority  voting  rule. 

Breiman  [1996]  found  a  significant  increase  of  classifi(cid:173)
cation  accuracy,  when  the  hypotheses  are  bagged,  on  dif(cid:173)
ferent  datascts  from  the  UCI  repository,  for  both  regres(cid:173)
sion  and  classification  tasks.  Moreover,  in  the  same  pa(cid:173)
per,  a  theoretical  analysis  is  carried  on.  The  notion  of 
order  correct  classifier  is  introduced,  and  it  is  shown 
that  when  A  is  order correct,  then  Bagging  is  expected  to 
behave  almost  optimally  (i.e.,  it  behaves  like  the  Bayes 
classifier).  Throughout  the  paper  it  is  claimed  that  "bag(cid:173)
ging  stable  classifiers  is  not  a  good  idea".  A  "stable" 
classifier  is  qualitatively  defined  as  one  that  does  not 
show  large  variations  in  the  learned  hypotheses  when  the 
training  set  undergoes  small  variations.  Experimental 
analysis  of Bagging's  performances  is  provided  by  Bauer 
and  Kohavi 
[1996],  and  Dietterich 
[2000|. 

[1999],  Quinlan 

The  simplest  version  of  AdaBoost  [Freund  &  Schapire, 
1997]  deals  with  binary  classification  problems,  and  this 
is  the  one  we  refer  to.  AdaBoost  maintains  a  weight  dis(cid:173)
tribution  over  the  training  examples,  and  adjusts  it  at 
each  iteration.  The  adjusted  weights  are  chosen  so  that 

LEARNING 

499 

higher  values  are  associated 
to  previously  misclassified 
examples.  As  a  main  effect,  the  weighting  scheme  forces 
A  to  focus  on  "more  difficult"  examples,  ensuring  thus 
that  all  the  examples  will  be,  sooner  or  later,  taken  into 
account  by  the  weak  learner.  The  hypothesis  combination 
rule  is  a  weighted  majority  vote  that  puts  more  weight  on 
successful  hypotheses  (i.e.,  hypotheses  that  incur  a  low 
weighted  training  error). 

Despite  its  apparent  simplicity  AdaBoost  solves  a  diffi(cid:173)
cult  problem:  choosing  example  and  hypothesis  weights 
in  such  a  way  to  ensure  an  exponential  decrease  in  the 
training  error  committed  by 
the  boosted  committee. 
Moreover,  the  choice  is  made  without  any  knowledge  or 
the 
assumption  on 
random 
weak 
learner 
guessing). 
"be(cid:173)
nign"  weak 
the  other  hand, 
AdaBoost  seems  to  be  more  sensitive  to  noise  than  Bag(cid:173)
ging  [Quinlan,  1996]. 

In  contrast,  Bagging  only  works  when 

the  weak 
is  always  able 

learners  are  used.  On 

itself  (except  that 

to  outperform 

learner 

As  mentioned  earlier,  AdaBoost  shows  the  characteristic 
of  being  robust  with  respect  to  ovcrfitti.ig 
[Schapire  et 
al.,  1998;  Drucker  &  Cortes,  1996;  Quinlan,  1996;  Bre-
iman,  1998;  Freund  &  Schapire,  1998].  A  rather  con(cid:173)
vincing  account  of  this  behavior 
to  AdaBoost 
increasing  the  final 
the  ability  to  be  very  successful 
classifier's  margin  over  the  training  examples 
[Schapire 
e t a l,  1998]. 

imputes 
in 

the 

effort 

and/or 

of  understanding 

In 
explaining 
AdaBoosfs  behavior,  boosting  has  been  related  to  other 
existing  theories,  such  as  Game  Theory  [Schapire,  2001], 
logistic  regression  [Friedman  et  al.,  1998;  Collins  et  al., 
2000),  optimization  [Ratsch  et  al.,  2002],  and  Brownian 
motion  [Freund,  1995]. 

theory. 

3  Monte  Carlo  Algorithms 
In  this  section  we  recall  some  basic  notions  from  Monte 
Carlo  algorithms 
the  name 
Monte  Carlo  often  denotes  a  generic  stochastic  algo(cid:173)
rithm;  in  this  paper  we  will  refer  to  the  more  restrictive 
definition  given  by  Brassard  and  Bratley  [1988].  Let 
be  a  class  of  problems,  Y  a  finite  set  of  answers  for  the 
problems  in  
be  a  function  that  maps 
each  problem  into  the  set  of its  correct  answers. 

literature, 

and 

the 

In 

/  -  A  stochastic  algorithm  A,  mapping  

Definition 
into 
Y,  is  said  to  be  a  Monte  Carlo  algorithm,  if,  when  applied 
to  an  instance  
it  always  terminates,  providing  an 
answer  to 

,  which  may  be  occasionally  incorrect. 

of 

Two  properties  of  Monte  Carlo  algorithms  turn  out  to  be 
relevant: 

Definition  2  -  A  Monte  Carlo  algorithm 
is  consistent  if  it 
never  outputs  two  distinct  correct  answers  when  run  two 
times  on  the  same  problem  instance 

-  A  Monte  Carlo  algorithm 

Definition  3 
if 
the  probability  that  it  gives  a  correct  answer  to  
is  at 
least  p,  independently  of  the  specific  problem  instance  n 
considered. 

is  p-correct 

instance 

The  great  interest  in  Monte  Carlo  algorithms  resides  in 
their  amplification  ability:  given  a  problem 
and  a  Monte  Carlo  algorithm  A/C,  if  MC  is  run  multiple 
times  on 
and  the  majority  answer  is  taken  as  the  result, 
then,  under  mild  assumptions,  the  probability  of  a  correct 
increases.  The  conditions  arc  that 
answer  exponentially 
that  p  must  be  greater 
MC  must  be  consistent 
than  random  guess.  We  note  that,  whenever  
reduces 
),  the  algorithm 
to  a  mapping  from 
the  algorithm  ob(cid:173)
is  consistent. 
tained  by  combining  with  a  majority  vote 
the  answers 
given  by  multiple  runs  of  a  Monte  Carlo  MC. 

Let   us  denote  by  

to  Y  (instead  of  2 

and 

for 

in 

the  case  we  have  complete  information  on 

4  Complete  I n f o r m a t i on  Case 
In  this  section  we  provide  a  theoretical  account  of  the 
links  between  Monte  Carlo  theory  and  bagging  or  boost(cid:173)
ing, 
the 
learning/classification  problem.  More  precisely,  let  X  be 
a  target  concept  (binary  clas(cid:173)
a  finite  set  of  examples1,  
and  w  be  a 
sification),  A  a  (weak)  learner.  
probability  distribution  over  X.  Let  moreover  
be  the 
power  set  of  X.  Any  intensional  concept  description,  be(cid:173)
longing  to  a  given 
will  be  mapped  to  an 
element  i)»  of  .  ,  i.e., 
is  the  set  of  examples  classified 
by  that  description  as  belonging  to  co.  In  this  way,  we  can 
reduce  ourselves  to  a  finite  set  of  (extensional)  hypothe(cid:173)
ses.  Let 
.A  prob(cid:173)
ability  distribution  q 
the  elements  of 
'can  be  thought  of  as  the  collection  of  the 
hypotheses  generated  by  a  learning  algorithm  A  applied 
to  a  number  of  training  sets,  or  a  set  of  hypotheses  pro(cid:173)
vided  by  an  oracle.  Learning  can  then  be  simulated  by 
according 
extracting  with  replacement  elements 
to  q 
extraction 
run 
of 
A  on  some  training  set). 

be  any  subset  of  

is  associated 

corresponds 

.  The  set 

from 
to 

language 

(each 

to 

a 

correctly 

Let  us  represent  the  described  situation  by  means  of  Ta(cid:173)
ble  1.  Here,  let  
be  the  probability  that  hy(cid:173)
classifies  e x a m p l e be 
pothesis 
the  average  of  such  probabilities 
be  the  ac(cid:173)
curacy  of  hypothesis  
By  using 
compute  the  average  accuracy  for  the  whole  table: 

the  given  probability  distributions,  we  can 

over  the  whole  X. 

for  

(1) 

The finiteness of X is not essential; it is only assumed for 
the sake of simplicity. 
2 This assumption implies that the Bayes error is zero. 

500 

LEARNING 

Table  1  -  Theoretical  setting 

Condition  (2)  is  true  as  long  as  the  Bayes  error  is  zero. 
Condition  (3)  may  or  may  not  be  true,  depending  on  the 
set 

be  the  set  of  examples  for  which  

Let 
Ac(cid:173)
cording  to  the  Monte  Carlo  theory,  all  these  examples 
will  be  "amplified",  i.e.,  they  w i ll  be  correctly  classified 
in  the  limit  
to  all 
the  rows  yields  an  asymptotic  accuracy: 

.  The  application  of 

By  denoting  by  pT 
the  expected  Monte  Carlo  accuracy 
when  only  a  finite  number  T  of  extractions  is  performed, 
we  obtain: 

(2) 

(3) 

Let  now  
greater  than 

be  the  set  of  hypotheses  whose  accuracy  is 

The  value 

(4) 

the  probability 

It  is  clear  that, 

that  a  hypothesis  "better" 

is 
than  bag(cid:173)
ging  is  obtained  in  a  single  extraction.  Then,  in  an  infi(cid:173)
nite  number  of trials,  such  a  hypothesis  w i ll  occur  with  a 
frequency 
is 
surely  convenient  only  when  when 
because,  in 
this  case,  there  is  no  single  hypothesis  better  than  Bag(cid:173)
ging.  When 
a  better  hypothesis  w i ll  surely  ap(cid:173)
pear,  soon  or  later.  The  interesting  case,  however,  occurs 
when  only  a  finite  number  T  of  extractions  is  performed. 
Before  approaching  this  case,  let  us  make  some  consid(cid:173)
erations  on  what  has  been  described  so  far. 

the  limit,  Bagging 

in 

rj 

Let  us  first  notice  that  ensemble  learning  claims  that  am(cid:173)
plification  occurs  when  the 
(accuracy  of  each  of  the 
combined  hypotheses)  are  greater  than  1/2  (condition  on 
the  columns).  Actually  it  may  not  be  the  case.  What  is 
really  required  is  that,  for  each  example  
,  more  than  1/2 
of  the  learned  hypotheses  are  correct  on  it  (condition  on 
the  rows).  Then,  Monte  Carlo  Theory  suggests  that  Bag(cid:173)
it  is  possible  to  find  a 
ging  would  be  successful  when 
.  Before  looking  with  more 
subset  of 
details 
the 

such  that 
the  relationship  between 

into 
the 
let  us  consider  some  extreme  cases. 

rjs  and 

the 

regardless  of 

training  set.  The 

Let  A  be  a  weak  learner  that  always  returns  the  same  hy(cid:173)
pothesis 
lucky 
is  quite  accu(cid:173)
situation  in  which  that  single  hypothesis 
rate  over  the  training  set  docs  not  imply  anything  about 
the  occurrence  of  Monte  Carlo  amplification.  In  fact,  all 
are  equal  to  1  on  those  examples  which  arc  cor(cid:173)
the 
and  0  otherwise.  In  other  words, 
rectly  classified  by 
whatever  the  accuracy  of  
no  amplification  at  all  oc(cid:173)
curs.  This  is  an  example  of a  perfectly  stable  learner. 

Let  us  consider  a  situation  in  which  the  global  average  r 
of  the  accuracy  is  a  little  more  than  1/2.  This  value  is  at 
the  same  time  the  average  of  the  last  column  and  on  the 
last  row  in  Table  1,  according  to  (1).  For  the  sake  of  i l(cid:173)
lustration,  let  q  and  w  be  uniform  distributions.  Let  us 
consider 
three  extreme  situations  (a  de-

following 

the 

Given  a  subset  Z  of  X,  
i.e.: 

the  measure  of  Z  w.r.t.  w, 

Analogously,  if  

,  we  define: 

Clearly  the  measures  

and 

reduce  to 

and 

,  respectively,  if q  and  w  are  uniform  distributions. 

4.1  M o n te  C a r lo  a nd  B a g g i ng 
We  w i ll  now  establish  a  link  between  Monte  Carlo  algo(cid:173)
rithms  and  Bagging.  Let  L0  be  a  training  set,  and  T  an 
let  I  be  a  generic  bootstrap  replicate  of  L0.  In 
integer; 
is  the  set  of  all  the  different  hypotheses  that 
this  case, 
can  be  obtained  by  applying  A 
replicates.  The 
frequencies  with  which  each 

is  generated  determine  q. 

to  .  

If  we  look  at  Table  1,  Bagging  proceeds,  so  to  say,  by 
columns,  by  extracting  T  columns  from  
and  using  them 
to  obtain  a  majority  vote  on  each  
the 
Considering 
global  average  accuracy  r,  or  even  the  exact  distribution 
of the  rj 
w i ll  be 
correctly  classified  by 
the  majority  voting  procedure, 
and,  then,  whether  to  perform  Bagging  w i ll  be  rewarding. 
It  is  generally  said  [Breiman,  1996]  that  bagging  is  useful 
when  A  is  "unstable".  We  w i ll  show  later  that  this  notion 
of "instability"  can  be  made  more  precise. 

it  is  not  easy  to  say  how  many  of  the  

proceeds  by  rows: 

Let  us  consider  now  a  Monte  Carlo  interpretation  of  Ta(cid:173)
ble  1.  To  the  contrary  of  Bagging,  a  Monte  Carlo  algo(cid:173)
for  e a c h ( p r o b l em   to  be 
rithm  , 
solved),  it  extracts  T  hypotheses  
and  classifies 
xk  by  majority  voting.  This  process  is  repeated  for  each 
row  .  This  procedure  is  necessary,  because  the  compo(cid:173)
nents  of the  Monte  Carlo  voting  must  be  independent,  but 
the  entries  in  each  column  are  not.  Monte  Carlo  theory 
tells  us  that  the  correctness  of  
can  be  amplified 
if: 

from 

1.  The  T trials  are  independent 
2.  MC  is  consistent 
3. 

Condition  (1)  is  true  by  construction  of  the  process  (ex(cid:173)
traction  with  replacement  of  T  hypotheses  for  each  row). 

3 Let us set apart, for the moment, the computational issues. 

LEARNING 

501 

tailed  analysis  of  several  other  interesting  cases  is  pro(cid:173)
vided by Esposito  [2003])  : 
(a)  The 

in  the  last column  are distributed in  such 
a  way  that  one  half of the  examples  have 
val(cid:173)
ues  close  to  one,  and  one  half of the  examples  have 
values  close  to  zero.  The  rjs  in  the  last  row, 
on  the  contrary,  have  all  almost  the  same  value 
around  1/2.  (All  hypotheses  show  the  same  per(cid:173)
formances  and  they  cover  almost  the  same  exam(cid:173)
ples) 
(b)  The 

in  the  last  column  have  all  almost  the 
same  value  around  1/2,  whereas  the  rjs  in  the  last 
row  have,  for  one  half,  values  close  to  one,  and  for 
the  other  half  close  to  zero.  (Some  hypotheses  are 
very  good and  other are very bad, but  their coverage 
is uniformly distributed on the examples). 

(c)  Both  distributions  are  uniformly  valued  around  1/2 

(All the hypotheses are uncertain on all examples). 

In  case  (a)  half of the  examples  arc  amplifiable,  but  Bag(cid:173)
ging  is  not  useful,  because  any  single  hypothesis  classi(cid:173)
fies  correctly  half of the  examples,  so  that nothing  can  be 
gained by the Bagging process. 
In  case  (b),  the  number  of amplifiable  examples  depends 
on  how  many  of them  have  actually  values  a  little  over 
1/2.  But, even  in  the best case 
, Bag(cid:173)
ging  is  again  not  convenient.  In  fact,  half  of  the  single 
hypotheses  have  r,  =  1;  then,  in  one  over two  extractions 
(on average) a perfect hypothesis is extracted. 
In  case  (c),  if the 
values  in  the  last columns  arc all  a 
little  greater  than  1/2,  then 
This  is  the  case  in 
which  Bagging  is  convenient,  because  the  single  hy(cid:173)
pothesis  have  all  accuracy  close  to  1/2.  Actually,  case  (c) 
is  the  ideal  case  for  Bagging,  because  amplification  can 
be  obtained  with  the  minimum  effort  (with  "bad"  hy(cid:173)
potheses we may obtain a large amplification). 

The  following  theorem  summarizes  the  above  consid(cid:173)
erations  and  establishes  a  link  between  Bagging  and 
Monte  Carlo  amplification  (proof is  provided  by  Esposito 
[2003]): 
Theorem  1  -  The  amplified  accuracy  p 
cannot  be 
greater  than  2r,  where  r  is  the  average  accuracy  of  the 
bagged hypotheses. 

Then,  even  by  bagging  single  hypotheses  with  accuracy 
less  then  1/2  it  is  possible  to  obtain  higher  accuracies  by 
Monte  Carlo  amplification.  However,  in  order to  reach  p^ 
=  1,  the  value  of r must  be at  least  1/2.  Even  in  this  case, 
some  of the  combined  hypotheses  are  allowed  to  have  a 
accuracy  less  than  1/2,  without  hindering  amplification, 
provided that 

and that r does not go below  112. 

4.2  Monte  Carlo  and  AdaBoost 
Let  us  notice  that,  if  the  set 
of  hypotheses  coincides 
with  the  power  set  2X  (all  possible  hypotheses  are  con(cid:173)
sidered),  we  are  in  the  situation  in  which  all 

502 

r =  1/2,  and 
,  so that no amplification  is possible. 
This  means  that  the  learning  procedure  was  not  able  to 
make  any  useful  selection  inside  the  hypothesis  space. 
On  the  contrary,  we  would  like  that  the  weak  learner  has 
the  nice  property  of  focusing  on  the  best  hypotheses.  In 
the  previous  section  we  showed  that  the  basic  Monte 
Carlo  approach,  implemented  by  Bagging,  may  fail  due 
to  a  number  of reasons;  this  happens,  for  instance,  in  the 
cases  (a)  and  (b)  of  Section  4.1.  Even  if  not  explicitly 
stated,  the  underling  problem  is  that  the  weak  learner 
may  not  be  suited  enough  for  this  kind  of procedure.  The 
question  we  arc  now  trying  to  investigate  is  what  to  do 
when  such  a  situation  occurs,  in  particular  when  we  do 
not  want  or  can  modify  the  weak  learner.  Then,  is  it  pos(cid:173)
sible  to  convert  case  (a)  or  (b)  into  case  (c)?  Or,  said  in 
other  words,  is  it  possible  to  convert  a  learning  problem 
in  which  the  Monte  Carlo  assumptions  for  amplification 
do not hold  into one in  which they do? 

The  solution  to  this  problem  can  be  found  by  thinking 
again  of the  role  of the  weak  learner  in  the  Monte  Carlo 
process.  As  mentioned  earlier,  the  learner  has  the  duty  to 
modify  the  distribution  from  which  the  hypotheses  are 
(i.e.,  the  qj  of  the  columns)  in  such  a  way  that 
drawn 
fctbctter" hypotheses  are  more  likely  to  be  drawn.  Moreo(cid:173)
ver,  we  can  actually  change  the  behavior  of  the  weak 
learner  through  its  input,  i.e.,  by  modifying  the  training 
set.  In  particular,  we  have  to  force  the  weak  learner  to 
extract  hypotheses  that  make  the  minimum  value  of  the 
p(xij 's  grow  larger  than  1/2  (in  order  to  satisfy  the  Monte 
Carlo  conditions  of  amplifiability  for  all  the  examples). 
Since  the  weak  learner  is  influenced  by  the  training  set, 
the  solution  is  to  modify  the  training  set  in  such  a  way 
that  examples  with  low 
values  arc  more  represented, 
in  the  attempt  to  make  the  weak  learner  try  harder  to  be 
successful  on  them.  What  we  obtain  with  this  reasoning 
is  an  AdaBoost-like  algorithm.  In  other words,  when  the 
weak  learner  is  unable  to  satisfy  Monte  Carlo  conditions 
for  amplification,  AdaBoost  forces  it  to  satisfy  them  by 
weighting  the  examples.  More  precisely,  AdaBost  tries  to 
increase  the  minimum 
with  the  unconscious  goal  of 
extending 
.  In  fact,  when  the  weak  learner  satisfies 
relatively  mild  conditions,  it  has  been  shown  by  Schapire 
et  al.  [1998]  that  AdaBoost  is  actually  effective  in  in(cid:173)
creasing  the  minimal  margin: 
Definition  4  -  For  any  particular  example 
the classifi(cid:173)
cation  margin 
is  the  difference  between  the 
weight  assigned  to  the  correct  label  and  the  maximal 
weight assigned to any  single  incorrect label. 
In  the  case  of binary  classification,  the  margin  is  the  dif(cid:173)
ference  between  the  weight  assigned  to  the  correct  label 
and the weight assigned  to the incorrect one. 
Theorem  2  -  In  the  case  of binary  classification,  the  fol(cid:173)
lowing  relation  holds  between  the  margin  and  the  Monte 
Carlo 

~ 

(7) 

LEARNING 

the  margin 

immediately  from 

the 
The  proof  of  Theorem  2  derives 
.  Formula  (7)  explains  why 
definition  of  margin  and  
increasing 
the  performances  of 
boosting:  in  fact,  increasing  the  minimum  margin  lets  the 
increase,  augmenting  the  number 
cardinality  of  the  set  
of  examples 
the  Monte  Carlo  conditions 
for 
amplifiability  hold. 

for  which 

improves 

that  a  hypothesis  "better" 

4.3  N o n - A s y m p t o t ic  Case 
Let  us  consider  a  finite  integer  T.  Let  
denote  the  prob(cid:173)
ability 
than  Bagging  is  ob(cid:173)
tained  in  R  extractions.  In  order  to  compare  Bagging  with 
hypothesis  selection,  let  us  consider  a  classification  accu(cid:173)
Given  Table  1,  and  given  a  number  
racy 
close  to  0, 
let 
be  the  minimum  number  of  components  of  the 
bagged  hypothesis  such  that: 

(8) 

then  probability  (8)  is  zero. 

If 
Analogously,  let 
be  the  set  of  hypotheses  with  rj  >  a 
and  R(a)  be  the  minimum  number  of  hypothesis  extrac(cid:173)
tions  such  that: 

(9) 

Again,  if  

probability  (9)  is  zero. 

The  graph 

We  can  draw  a  graph  of  T  (ordinate)  vs.  R  (abscissa), 
parameterized  by 
is  problem  dependent. 
What  we  can  say  in  general  is  that  points  in  the  graph 
above  the  diagonal  correspond  to  situation  for  which  se(cid:173)
lection  is  better  than  Bagging  (for  the  same  accuracy  ex, 
Bagging  requires  more  trials),  points  below  the  diagonal 
correspond  to  situation  for  which  selection  is  worse  than 
Bagging  (for  the  same  accuracy  
Bagging  requires  less 
trials),  and  points  on  the  diagonal  correspond  to  situa(cid:173)
tions  of  indifference. 

is  an  unbiased  estimate  of  the  Bernoulli  probability  p(x/). 
Even  though  the  T  hypotheses  generated  are  only  used  to 
classify  X1,  and  could  then  be  discarded,  nothing  hinders 
from  recording  them  in  the  first  T  (or  less,  if  there  is  du(cid:173)
plication)  columns  of  Table  1.  When  we  consider  jr?,  we 
repeat  the  whole  procedure,  running  again  A  on  T  new 
replicates  of  L0  and  classifying 
x2  Again  we  record  the 
new  generated  hypotheses  (or  update  the  probability  dis(cid:173)
tribution  q  in  case  of  repetitions).  When  we  finish  exam(cid:173)
ining  L0,  we  have  also  filled  the  part  of  the  table  corre(cid:173)
for 
sponding  to  the  test  set,  which  allows  estimates  
the  examples  in  the  test  set  to  be  computed.  Each  
is 
based  on  T  M  experimental  values,  which  is  the  number 
of  runs  necessary  to  estimate  the  generalization  accuracy 
using  the  leave-one-out  method  on  a  number  M  of  train(cid:173)
ing  examples. 

is  convenient  or  not  to  exploit  Bagging 

What  is  interesting  is  that,  during  learning,  we  also  learn 
whether  it 
for 
classifying  future  unseen  examples.  It  also  turns  out,  ex(cid:173)
perimentally, 
than 
T-M  is  very  often  sufficient  to  obtain  a  stable  estimate  of 
the  sought  probabilities. 

lower  number  of  runs 

that  a  much 

in 

their 

interpretation 

5.1  AdaBoost's  generalization  behavior 
In  this  section  we  discuss  AdaBoost's  generalization  ca(cid:173)
pabilities  and 
the  Monte  Carlo 
framework.  Having  related  the  margin  of  an  example 
with  the  average  probability  that  it  is  correctly  classified 
by  the  hypotheses  learned  via  the  weak  learner  allows  us 
to  transfer  results  provided  by  Schapire  et  al.  [1998]  to 
the  Monte  Carlo 
the 
link  may  be  used  to  give  a  new,  more  intuitive,  interpre(cid:173)
tation  of  the  margin  explanation  of  AdaBoost's  generali(cid:173)
zation  capabilities.  Moreover  a  nice  explanation  of  why 
the  test  error  is  often  observed  to  decrease  even  after  the 
training  error reaches  zero  can  be  suggested. 

Interestingly  enough, 

framework. 

the 

is  useful 

functioning  of  ensemble 

5  Problems  with  Partial  Information 
The  setting  discussed  in  Section  4 
for  under(cid:173)
standing 
learning,  but  we 
need  to  extend  it  to  the  case  of real  learning  problems.  In 
concrete  situation,  neither 
the  example 
is  available  a  priori.  What  we  have  is  a  training 
space  X 
and  a  test  set  To,  both  subsets  of X,  and  a 
set 
learning  algorithm  A  for  generating  hypotheses.  We  learn 
hypotheses,  possibly  consistent  with  L(),  and  estimate  the 
generalization  error  on 

the  set 

nor 

If  we  consider  the  problem 
from  the  Monte  Carlo  per(cid:173)
spective  and  we  go  back  to  Table  1,  we  can  think  to  pro(cid:173)
ceed  as  follows.  At  the  beginning  we  have,  on  the  rows, 
the  training  and  the  test  examples,  but  the  columns  are 
Let  A 
empty.  Let  us  consider  the  first  example  
run  on  T  replicates  of  L0  and  let  us  compute  
which 

the  distribution  of 
the  two  major  parameters  of  the  ensemble  algo(cid:173)

To  understand  how  generalization  error  can  be 
inter(cid:173)
preted  in  the  Monte  Carlo  framework  it  is  necessary  to 
deepen  a  little  the  discussion  about  the  link  between  en(cid:173)
learning  and  Monte  Carlo  algorithms.  Let  us 
semble 
choose 
the  randomization  procedure  (whatever 
is). 
Once  this  is  done,  the  choice  of  the  weak  learner  deter(cid:173)
mines 
when 
rithm  are  fixed,  we  can 
property  of  the  examples. 
Once  the  ensemble  process  starts,  we  compute  the  esti(cid:173)
mations 
The  accuracy  of  the  estimates  increases 
with 
the 
final  rule  w i ll  classify  correctly  or  not  any  example,  is  a 
matter  of  two  things:  the  value  of  the  underlying  exact 

the  number  of  iterations  performed.  Whether 

In  other  words, 

as  a  theoretical 

think  of  

the 

it 

. 

4 

In this paper we do not consider to use cross-validation or 
leavc-onc-out. 

5 The datasets used, as well as part of the experimental results, 
can be found at the URL: 

http://www.di. unito.it/~csposito/mcandboost 

LEARNING 

503 

the  point 

for  AdaBoost,  beyond 

and  how  lucky  we  were  in  the  choice  of 

probability 
the rules used to perform the actual estimation. 
The  above  argument  might  explain  the  finding  of the  test 
error  decreasing, 
in 
which  training  error  reaches  zero.  In  fact,  as  an  effect  of 
the  learning  bias  introduced  by  the  weak  algorithm,  it  is 
likely  that  the 
corresponding  to  examples  in  the 
training  set would  be greater  than  the ones  corresponding 
to  the  other  examples.  It  is  then  likely  that  those  exam(cid:173)
ples  would  be  correctly  classified  with  high  probability 
after  a  moderate  number  of  iterations.  On  the  contrary, 
other  examples  would  require  a  greater  number  of  itera(cid:173)
tions,  and  thus  they  may  start  to  be  correctly  labelled 
well  beyond  the  point  where  the  training  error  reaches 
zero.  In  fact,  even  though  any  example  with 
will  eventually  be  correctly  classified,  the  number  of  it(cid:173)
erations  necessary  to  see  this  effect  to  emerge  strongly 
depends  on  T.  The  observed  effect  of  the  test  error  de(cid:173)
is 
creasing is  hence due  to  those examples for which 
only  slightly  greater  than  1/2.  Initially,  their 
is pos(cid:173)
sibly  underestimated  by  the  observed 
.  For  small 
values  of T, 
and 
greater  values  of  T  are  necessary  to  let  the  choice  of hy(cid:173)
potheses  converge  toward  a  set  for  which 
and,  hence,  to  obtain  a  stable  correct  classification  of 
in the test set. 

can  be  quite  different  from 

6  Conclusions  and  Further  W o rk 
In  this  paper  we  have  proposed  to  analyze  ensemble 
learning  (in  particular  Bagging  and,  partly,  AdaBoost) 
from  the  perspective  of  Monte  Carlo  algorithm  theory. 
We  have  shown  that  this  theory  can  shed  light  on  some 
basic  aspects  of Bagging  and  boosting,  and,  also  offers  a 
new  way  to  actually  perform  ensemble  learning  and  error 
estimation.  More  research 
to 
deepen  our  understanding  of the  presented  framework.  In 
particular,  it  would  be  very  interesting  to  investigate  the 
relationships  between  the  Monte  Carlo  explanation  of 
Ensemble  Learning  and  other  well  known  ones  (for  in(cid:173)
stance,  the  bias-variance  decomposition).  Moreover,  it 
would  be  very  interesting  to  investigate  how  a  particu(cid:173)
larly  aggressive  class  of Monte  Carlo  algorithms,  namely 
the  one  of  biased  Monte  Carlos,  relates  to  ensemble 
learning. 

is  nevertheless  needed 

References 
[Bauer  and Kohavi,  1999].  E.  Bauer and R.  Kohavi.  An 
empirical  comparison  of  voting  classification  algorithms: 
Bagging,  boosting,  and  variants.  Machine  Learning, 
56:105-139. 
[Brassard and  Bratley,  1988]  G.  Brassard  and  P.  Bratley. 
Algorithmics: 
Inc., 
1988. 
[Brciman,  1996]  L.  Breiman.  Bagging  predictors.  Ma(cid:173)
chine  Learning,  24(2):123-140,  1996. 

theory  and  practice.  Prentice-Hall, 

to 

in 

Learing 

Theory, 

introduction 

In  Computational 

[Breiman,  1998]  L.  Breiman.  Arcing  classifiers.  The 
Annals of Statistics, 26(3):801- 849,  1998. 
[Collins  et  al.,  2000]  M.  Collins,  R.  Schapire,  and  Y. 
Singer.  Logistic  regression,  adaboost  and  bregman  dis(cid:173)
tances. 
pages 
158-169,2000. 
[Dicttcrich,  2000]  T.  Diettcrich.  Ensemble  methods  in 
machine  learning.  Lecture  Notes  in  Computer  Science, 
1857:1-15,2000. 
[Drucker and Cortes,  1996]  Drucker and  Cortes, Boosting 
decision  trees.  In  David  S.  Touretzky,  Michael  C.  Mozer, 
and  Michael  E.  Hasselmo  (Eds.),  Advances  in  Neural 
Information  Processing  Systems, Vol.  8,  (pp.  479-485), 
The MIT Press,  1996. 
[Efron  and  Tibshirani,  1993]  B.  Efron  and  R.  Tibshirani. 
An 
the  Bootstrap.  Chapman  and  Hall, 
1993. 
[Esposito,  2003]  R.  Esposito.  Analyzing  Ensemble 
Learning 
the  Framework  of  Monte  Carlo  Theory. 
Ph.D.  Thesis,  University  of Torino,  Dept.  of Informatica, 
Italy. 
[Freund,  1995].  Y.  Freund.  Boosting  a  weak  learning 
algorithm  by  majority. 
Information  and  Computation, 
2(121):256-2%5,  1995. 
[Freund  and  Shapirc,  1997]  Y.  Freund and  R.  Schapire, A 
decision-theoretic  generalization  of  on-line  learning  and 
to  boosting.  Journal  of  Computer  and 
an  application 
System Sciences, 55(1):119-139,  1997.  * 
[Freund  and  Shapire,  1998]  Y.  Freund  and  R.  Schapire. 
Discussion  of  the  paper  "arcing  classifiers"  by  Leo 
Breiman. The Annals of Statistics,  26(3):824-832,  1998. 
[Friedman  et  al.,  1998]  J.  Friedman,  T.  Hastic,  and  R. 
Tibshirani.  Additive  logistic  regression:  a  statistical  view 
of  boosting.  Technical  report,  Department  of  Statistics, 
Sequoia Hall, Stanford Univerity, July  1998. 
[Quinlan,  1996]  R.  Quinlan.  Bagging,  boosting,  and 
C4.5.  In  Proceedings  of the  Thirteenth  National  Confer(cid:173)
ence  on  Artificial  Intelligence  and  the  Eighth  Innovative 
Applications  of  Artificial 
Intelligence  Conference,  pages 
725-730,  1996. AAAI  Press / MIT Press 
[Ratsch  et al.,  2002]  G.  Ratsch,  M.  Warmuth,  S.  Mika,  T. 
Onoda,  S.  Lemm_  and  K.  Muller.  Barrier  boosting.  In 
13th  Conference  on  Computational  Learning  Theory, 
2000. 
[Schapire,  2001]  R.  Schapire.  Drifting  games.  Machine 
Learning 43(3):  265-291,2001. 
[Schapire  et  al.,  1998]  R.  Schapire,  Y.  Freund,  P. 
Bartlett,  and  W.  Lee.  Boosting  the  margin:  A  new  expla(cid:173)
nation  for  the  effectiveness  of  voting  methods.  Annals  of 
Statistic,  26(5): 1651-1686,  1998. 
[Wolpert,  1992]  D.  Wolpert.  Stacked  Generalization, 
Neural Networks,  5,  241-259,  1992. 

504 

LEARNING 


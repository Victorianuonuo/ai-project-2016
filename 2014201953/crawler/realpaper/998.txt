Subtree Mining for Question Classiﬁcation Problem

Minh Le Nguyen

Thanh Tri Nguyen

Akira Shimazu

Japan Advanced Institute of

Japan Advanced Institute of

Japan Advanced Institute of

Science and Technology

nguyenml@jaist.ac.jp

Science and Technology

Science and Technology

t-thanh@jaist.ac.jp

shimazu@jaist.ac.jp

Abstract

Question Classiﬁcation, i.e., putting the questions
into several semantic categories, is very important
for question answering. This paper introduces a
new application of using subtree mining for ques-
tion classiﬁcation problem. First, we formulate
this problem as classifying a tree to a certain la-
bel among a set of labels. We then present a use of
subtrees in the forest created by the training data to
the tree classiﬁcation problem in which maximum
entropy and a boosting model are used as classi-
ﬁers. Experiments on standard question classiﬁ-
cation data show that the uses of subtrees along
with either maximum entropy or boosting models
are promising. The results indicate that our method
achieves a comparable or even better performance
than kernel methods and also improves testing efﬁ-
ciency.

1 Introduction
What a current information retrieval system can do is just
”document retrieval”, i.e., given some keywords it only re-
turns the relevant documents that contain the keywords.

However, what a user really wants is often a precise an-
swer to a question. For instance, given the question ”Who
was the ﬁrst American in space?”, what a user really wants
is the answer ”Alan Shepard”, but not to read through lots
of documents that contain the words ”ﬁrst”, ”American” and
”space” etc.

In order to correctly answer a question, usually one needs
to understand what the question was asked. Question Classi-
ﬁcation can not only impose some constraints on the plausible
answers but also suggest different processing strategies. For
instance, if the system understands that the question ”Who
was the ﬁrst American in space?” asks for a person name,
the search space of plausible answers will be signiﬁcantly re-
duced. In fact, almost all the open-domain question answer-
ing systems include a question classiﬁcation module. The
accuracy of question classiﬁcation is very important to the
overall performance of the question answering system.

Question classiﬁcation is a little different from document
classiﬁcation, because a question contains a small number of
words, while a document can have a large number of words.

Thus, common words like ”what”, ”else”, etc. are considered
to be ”stop-words” and omitted as a dimension reduction step
in the process of creating features in document. However,
these words are very important for question classiﬁcation.
Also, word frequencies play an important role in document
classiﬁcation while those frequencies are usually equal to 1
in a question, thus, they do not signiﬁcantly contribute to the
performance of classiﬁcation. Furthermore, the shortness of
question makes the feature space sparse, and thus, makes the
feature selection problem more difﬁcult and different from
document classiﬁcation.

There are two main approaches for question classiﬁca-
tion. The ﬁrst approach does question classiﬁcation us-
ing handcrafted rules which are reported in [Voorhees,
1999][Voorhees, 2000][Voorhees, 2001]. Although it is pos-
sible to manually write some heuristic rules for the task, it
usually requires tremendous amount of tedious work. One
has to correctly ﬁgure out various forms of each speciﬁc type
of questions in order to achieve reasonable accuracy for a
manually constructed classiﬁcation program.

The second approach applies machine learning to ques-
tion classiﬁcation problems such as in [Radev et al, 2002],
the machine learning tool Ripper has been utilized for this
problem. The author deﬁned 17 question categories, trained
and tested their question classiﬁcation on TREC dataset.
The reported accuracy of their system is 70%, which is
not high enough for question classiﬁcation. Li [Li and
Roth, 2002] presented the use of SNoW method in which
the good performance of their system depends on the fea-
ture called ”RelWords” (related word) which are constructed
semi-automatically but not automatically. The author also re-
ported several useful features for the question classiﬁcation
task including words, parts of speech, chunking labels, and
named entity labels. In addition, they indicated that hierar-
chical classiﬁcation methods is natural to the question classi-
ﬁcation problem. A question is classiﬁed by two classiﬁers,
the ﬁrst one classiﬁes it into a coarse category, the second
classiﬁes it into a ﬁne category.

Zhang and Lee[Zhang and Lee, 2003] employed tree ker-
nel with a SVM classiﬁer and indicated that the syntactic in-
formation as well as tree kernel is suitable for this problem.
However, the authors showed the performance only for the
coarse category, and there was no report on classifying ques-
tions to ﬁne categories.

IJCAI-07

1695

On the other hand, the connection of data mining tech-
niques with machine learning problem recently shows that
data mining can help improve the performance of classiﬁer.
Morhishita [Morhishita, 2002] showed that the use of associ-
ation rule mining can be helpful for enhancing the accuracy
of the boosting algorithm. Berzal et al also [F. Berzal et al ,
2004] presented a family of decision list based on association
rules mined from training data and reported a good result in
the UCI data set [Li and Roth, 2002].

Mining all subtrees can be helpful for re-ranking in the
syntactic parsing problem [Kudo et al, 2005] as well as en-
semble learning in semantic parsing [Nguyen et al, 2006].
Along with the success, the natural question is whether or not
subtree mining can be helpful for question classiﬁcation prob-
lem? This paper investigates the connection of subtree mining
to the maximum entropy and the boosting model. We formu-
late the question classiﬁcation as a tree classiﬁcation prob-
lem. We then present an efﬁcient method for utilizing sub-
trees from the forests created by the training data in the max-
imum entropy model (subtree-mem) and a boosting model
(subtree-boost) for the tree classiﬁcation problem.

The remainder of this paper is organized as follows: Sec-
tion 2 formulates the tree classiﬁcation problem in which a
maximum entropy model and a boosting model using subtree
features are presented. Section 3 discusses efﬁcient subtree
mining methods. Section 4 shows experimental results and
Section 5 gives some conclusions and plans for future work.

2 Classiﬁer for Trees

Assume that each sentence in a question is parsed into a tree.
A tree can be a sequence of word, a dependency tree, and a
syntactic tree.

The problem of question classiﬁcation is equivalent to clas-

sifying a syntactic tree into a set of given categories.

2.1 Deﬁnition
The tree classiﬁcation problem is to induce a mapping f (x) :
X → {1, 2, ..., K}, from given training examples T =
{(cid:3)xi, yi(cid:4)}L
i=1, where xi ∈ X is a labeled ordered tree and
yi ∈ {1, 2, ..., K} is a class label associated with each train-
ing data. The important characteristic is that the input exam-
ple xi is represented not as a numerical feature vector (bag-of-
words) but a labeled ordered tree.

A labeled ordered tree is a tree where each node is asso-
ciated with a label and is ordered among its siblings, that is,
there are a ﬁrst child, second child, third child, etc.

Let t and u be labeled ordered trees. We say that t matches
u, or t is a subtree of u (t ⊆ u ), if there exists a one-to-one
function ψ from nodes in t to u, satisfying the conditions:
(1)ψ preserves the parent-daughter relation, (2)ψ preserves
the sibling relation, (3)ψ preserves the labels.

Let t and x be labeled ordered trees, and y be a class label
(yi ∈ {1, 2, ..., K}), a decision stump classiﬁer for trees is
given by:

(cid:2)

h<t,y>(x) =

1 t ⊆ x
0 otherwise

Figure 1: Labeled ordered tree and subtree relation

Decision stump functions are observed on the training data
(the set of trees and their labels), then these functions are
incorporated to a maximum entropy model and a boosting
model.

2.2 Boosting with subtree features
The decision stumps classiﬁers for trees are too inaccurate
to be applied to real applications, since the ﬁnal decision
relies on the existence of a single tree. However, accu-
racies can be boosted by the Boosting algorithm[Schapire,
1999] Boosting repeatedly calls a given weak learner to ﬁ-
nally produce hypothesis f, which is a linear combination
of K hypotheses produced by the prior weak learners, i,e.:
f (x) = sgn(

k=1 αkh<tk,yk>(x))

(cid:3)K

N(cid:3)

ent distributions or weights d

A weak learner is built at each iteration k with differ-
(k)
L ) , (where
(k)
i ≥ 0). The weights are calculated in such a

(k)
i = 1, d
d

(k) = (d

(k)
i

i=1
way that hard examples are focused on more than easier ex-
amples.

, ..., d

L(cid:4)

gain(< t, y >) =

yidih<t.y>(xi)

i=1

There exist many Boosting algorithm variants, how-
ever,
the original and the best known algorithm is
AdaBoost[Schapire, 1999]. For this reason we used the Ad-
aboost algorithm with the decision stumps serving as weak
functions.

2.3 Maximum Entropy Model with subtree

features

Maximum entropy models [Berger
et al, 1996], do not
make unnecessary independence assumptions. Therefore, we
are able to optimally integrate together whatever sources of
knowledge we believe to be potentially useful to the transfor-
mation task within the maximum entropy model. The max-
imum entropy model will be able to exploit those features
which are beneﬁcial and effectively ignore those that are ir-
relevant. We model the probability of a class c given a vector
of features x according to the ME formulation:

Figure 1 shows an example of a labeled ordered tree and its

subtree and non-subtree.

p(c|x) =

IJCAI-07

1696

n(cid:3)

i=0

exp[

λifi(c, x)]

Zx

(1)

Here Zx is the normalization constant, fi(c, x) is a fea-
ture function which maps each class and vector element to
a binary feature, n is the total number of features, and λi is
a weight for a given feature function. These weights λi for
features fi(c, x) are estimated by using an optimization tech-
nique such as the L-BFGs algorithm [Liu and Nocedal, 1989].
In the current version of maximum entropy model using
subtrees, we deﬁne each feature function fi(c, x) using sub-
trees x and a class label c. The value of a feature function
is received as +1 if (c, x) is observed in the training data.
Otherwise, it is received 0. Under the current framework of
maximum entropy model, all subtrees mined are incorporated
to the MEM as these feature functions mentioned above.

3 Efﬁcient subtree mining

Our goal is to use subtrees efﬁciently in the MEM or Boosting
models. Since the number of subtrees within the corpus is
large a method for generating all subtrees in the corpus seems
intractable.

Fortunately, Zaki [Zaki, 2002] proposed an efﬁcient
method, rightmost-extension, to enumerate all subtrees from
a given tree. First, the algorithm starts with a set of trees con-
sisting of single nodes, and then expands a given tree of size
(k .. 1) by attaching a new node to this tree to obtain trees
of size k. However, it would be inefﬁcient to expand nodes
at arbitrary positions of the tree, as duplicated enumeration is
inevitable. The algorithm, rightmost extension, avoids such
duplicated enumerations by restricting the position of attach-
ment. To mine subtrees we used the parameters bellow:

• minsup: the minimum frequency of a subtree in the data
• maxpt: the maximum depth of a subtree
• minpt: the minimum depth of a subtree

Table 1 shows an example of mining results.

Table 1: Subtrees mined from the corpus

Frequency
4
7
2
2
5

Subtree

(VP(VBZfeatures)(PP))

(VP(VP(VBNconsidered)))

(VP(VP(VBNcredited)))

(VP(VP(VBNdealt)))

(VP(VP(VBNinvented)))

The subtree mining using the right most extension is used
in both maximum entropy model and boosting model. The
following subsection will describe how these subtrees can be
used in MEM and Boosting.

3.1 Subtree selection for Boosting

The subtree selection algorithm for boosting is based on the
decision stump function. We will eliminate a subtree if its
gain value is not appropriate for the boosting process. We
borrow the deﬁnition from [Kudo et al , 2004][Kudo et al,
2005] as shown bellow:

Let T = {< x1, y1, d1 >, ..., < xL, yL, dL >} be training
data, where xi is a tree and yi is a labeled associated with xi

and di is a normalized weight assigned to xi. Given T ﬁnd
0
the optimal rule < t

> that maximizes the gain value.

, y

0

The most naive and exhaustive method, in which we ﬁrst
enumerate all subtrees F and then calculate the gains for all
subtrees, is usually impractical, since the number of subtrees
is exponential to its size. The method to ﬁnd the optimal rule
can be modeled as a variant of the branch-and-bound algo-
rithm, and is summarized in the following strategies:

• Deﬁne a canonical search space in which a whole set of

subtrees of a set of trees can be enumerated.

• Find the optimal rule by traversing this search space.
• Prune search space by proposing a criterion with respect

to the upper bound of the gain.

In order to deﬁne a canonical search space, we applied the
efﬁcient mining subtree method as described in [Zaki, 2002].
After that, we used the branch-and-bound algorithm in which
we deﬁned the upper bound for each gain function in the pro-
cess of adding a new subtree. To deﬁne a bound for each
subtree, we based our calculation on the following theorem
[Morhishita, 2002]

For any t(cid:2) ⊇ t and y ∈ {1, 2, ..., K}, the gain of < t(cid:2), y >

is bounded by μ(t)

(cid:5)

(cid:4)

L(cid:4)

(cid:4)

L(cid:4)

(cid:6)

μ(t) = max

2

di −

yidi, 2

di +

yidi

i|yi =+1,t∈xi

i=1

i|yi =−1,t∈xi

i=1

(2)

Note that this equation is only used for binary classiﬁca-
tion. To adapt it to the multi classiﬁcation problem one can
use some common strategies such as one-vs-all, one-vs-one,
and error correcting code to consider the multi classiﬁcation
problem as the combination of several binary classiﬁcations.
In this paper we focus on the use of one-vs-all for multi-class
problems. Hence the subtrees can be selected by using the
bound mentioned above along with binary classiﬁcation us-
ing boosting.

We can efﬁciently prune the search space spanned by right
most extension using the upper bound of gain μ(t). Dur-
ing the traverse of the subtree lattice built by the recursive
process of rightmost extension, we always maintain the tem-
porally suboptimal gain δ among all gains calculated previ-
ously. If μ(t) < delta , the gain of any super-tree t(cid:2) ∈ t is
no greater than delta , and therefore we can safely prune the
search space spanned from the subtree t. Otherwise, we can
not prune this branch. The algorithm for selecting subtrees is
listed in Algorithm 1.

Note that |t| means the length of a given subtree t. In addi-
tion, Algorithm 1 is presented as in an algorithm for K classes
but in the current work we applied it to the binary class prob-
lem.

After training the model using subtrees, we used the one-

vs-all strategy to obtain a label for a given question.

3.2 Subtree selection for MEM
Count cut-off method
The subtree selection for maximum entropy model is con-
ducted based on a right most extension way. Each subtree is

IJCAI-07

1697

Input: Let T = {< x1, y1, d1 >, ..., < xL, yL, dL >} be
training data, where xi is a tree and yi is a labeled associated
with xi and di is a normalized weight assigned to xi.
Output: Optimal rule < t
Begin
Procedure project(t)
Begin
if μ(t) ≤ δ then

, y

>

0

0

return 1

= arg maxy∈{1,2,...,K} gain(< t, y >) if

(cid:2)

end
y
gain(< t, y
0
, y

0

(cid:2)

>) > δ then
>=< t, y >

< t
δ = gain(< t, y >)

end
for each t

(cid:2) ∈ {set of trees that are rightmost extension of t } do

s = single node added by right most extension
if μ(t) ≤ δ continue
project(t

);

(cid:2)

(cid:7)

end
End (project(t)
for t

(cid:2) ∈
project(t

);

(cid:2)

(cid:9)

end

return
End

0

0

, y

t

(cid:10)

(cid:8)

t|t ∈ ∪L

i=1 {t|t ⊆ xi} , |t| = 1

do

Algorithm 1: Find Optimal Rule

mined using the algorithm described in [Zaki, 2002] in which
a frequency, a maximum length, and minimum length of a
subtree is used to select subtrees. The frequency of a sub-
tree can suitably be used for the count cut-off feature selec-
tion method in maximum entropy models. Under the cur-
rent framework of maximum entropy model we used only the
simple feature selection method using count cut off method.
However, the results we gained in question classiﬁcation are
promising.

Using boosting
In order to ﬁnd effective subtree features for maximum en-
tropy model, we applied the subtree boosting selection as
presented in the previous section. We obtained all subtree
features after the running of the subtrees boosting algorithm
for performing on data set of one vs all strategy. In fact we
obtained 50 data set for 50 class labels in the question classi-
ﬁcation data. These subtree features after applying the boost-
ing tree algorithm in each data set will be combined to our
set features for maximum entropy models. The advantage of
using the boosting subtrees with maximum entropy models
is that we can ﬁnd an optimization combination of subtree
features under the maximum entropy principle using the ef-
ﬁcient optimization algorithm such as the L-BFGS algorithm
[Liu and Nocedal, 1989]. In my opinion, the combination of
subtree features using maximu entropy models might better
than the linear combination as the mechanism of Adaboost.

4 Experimental results

The experiments were conducted on a pentium IV at 4.3 GHz.
We tested the proposed models on the standard data similarly

experiments in previous work which was collected from four
sources: 5,500 English questions published by USC, about
500 manually constructed questions for a few rare classes,
TREC 8 and TREC 9 questions and also 500 questions from
TREC 10. The standard data is used by almost previous
work on question classiﬁcation [Zhang and Lee, 2003][Li and
Roth, 2002]. The labels of each question is followed the two-
layered question taxonomy proposed by [Li and Roth, 2002],
which contains 6 coarse grained category and 50 ﬁne grained
categories, as shown bellow. Each coarse grained category
contains a non-overlapping set of ﬁne grained categories.

• ABBRS : abbreviation, expansion
• DESC : deﬁnition, description, manner, reason
• ENTY : animal, body, color, creation, currency, dis-
ease/medical, event, food, instrument, language, letter,
other, plant, product, religion, sport, substance, symbol,
technique, term, vehicle, word

• HUM: description, group, individual, title
• LOC: city, country, mountain, other, state
• NUM: code, count, date, distance, money, order, other,

percent, period, speed, temperature, size, weight

In order to obtain subtrees for our proposed methods, we
initially used the Chaniak parser [E. Charniak, 2001] to parse
all sentences within the training and testing data. We obtained
a new training data consisting of a set of trees along with their
labels. We then mined subtrees using the right most extension
algorithm [Zaki, 2002]. Table 2 shows a running example
of using maximum entropy models with subtrees. To train
our maximum entropy model we used the L-BFGS algorithm
[Liu and Nocedal, 1989] with the gaussian for smoothing.

Table 2: A running example of using maximum entropy
model with subtrees

Features
ENTY (ADJP(ADVP))
ENTY (ADJP(INof))
ENTY (ADJP(JJcelebrated))
HUM (ADJP(JJcommon))
HUM (ADJP(JJdead))
ENTY:substance (NP(ADJP(RBSmost)(JJcommon)))
NUM:count (ADJP(PP(INfor)))
NUM:count (NP(DTthe)(NNnumber))

Weight
0.074701
0.433091
0.084209
0.003351
0.117682
0.354677
0.157310
0.810490

To evaluate the proposed methods in question classiﬁca-
tion, we used the evaluation method presented in [Li and
Roth, 2002][Zhang and Lee, 2003] as the formula below.

accuracy =

#of correct predictions

#of predictions

We conducted the following experiments to conﬁrm our ad-
vantage of using subtree mining for classiﬁcation with the
maximum entropy model and boosting model. The experi-
ments were done using signiﬁcant test with 95% conﬁdent
interval. Table 3 shows the accuracy of our subtree maxi-
mum entropy model (ST-Mem), subtree boosting model (ST-
Boost), the combination of subtree boosting and maximum

IJCAI-07

1698

entropy(ST-MB), and the tree kernel SVM (SVM-TK) for the
coarse grained category, respectively. It shows that the boost-
ing model achieved competitive results to that of SVM tree
kernel. The ST-MB outperforms the SVM-TK and achieve
the best accuracy.

Table 3: Question classiﬁcation accuracy using subtrees, un-
der the coarse grained category deﬁnition (total 6 labels)

ST-Mem ST-Boost
87.6

90.6

ST-MB SVM-TK

91.2

90.0

Table 4: Question classiﬁcation accuracy using subtrees, un-
der the ﬁne grained category deﬁnition (total 50 labels).
ST-MB Mem SVM
80.2

ST-Mem ST-Boost
80.5

82.6

83.6

77.6

Table 4 shows the performance of subtree-mem, subtree-
boost, and subtree-MB for question classiﬁcation.
It also
shows the result of using word features for maximum entropy
models (MEM) and support vector machine models (SVM).
The result of using SVM tree kernel is not reported in the
original paper [Zhang and Lee, 2003]. The author did not
report the performance of SVM using tree kernel on the ﬁne
grained category. Now on our paper we reported the perfor-
mance of ﬁne grained category using subtrees under the max-
imum entropy and the boosting model. We see that without
using semantic features such as relation words we can obtain
a better result in comparison with previous work. Table 4 in-
dicates that the subtree-MB obtained the highest accuracy in
comparison with other methods since it utilizes both the ad-
vantage of subtree features selection using boosting and the
maximum entropy principle.

Since the subtree information and other semantic features
such as the named entity types and the relation words do not
overlap, mixing these features together might improve the
performance of question classiﬁcation task.

The results also indicate that boosting model outperformed
maximum model using subtrees and the computational time
of MEM is faster than that of boosting model.

The training time of maximum entropy models is approx-
imately 12 times faster than the boosting model. We needed
approximately 6 hours to ﬁnish all the training process of
boosting models for 5,500 training data examples. While us-
ing the maximum entropy model we needed only approxi-
mately 0.5 hours. The computational times of combination
subtree boosting features and maximum entropy models is
comparable to the subtree boosting model.

The testing using MEM is also approximately 5 times
faster in comparison with that of boosting model. We guess
that the reason is the boosting model had to ﬁnd optimal rules
during the training process. In addition, the larger number
of classes (50 labels) might affect to the computational time
because of the strategy of one-vs-all method.

Table 5 depicted the results of boosting model and maxi-
mum model using subtrees in which the minsup and maxpt
mean the frequency and the maximum depth of a subtree,
respectively. This table reported that there is a relation be-
tween the subtree mined parameters and the performance of

Table 5: Question classiﬁcation accuracy using subtrees, un-
der the ﬁne grained category deﬁnition (total 50 labels).

Parameters
maxpt=6 minsup=3
maxpt=4 minsup=2
maxpt=5 minsup=2

Subtree-boost

Subtree-mem

0.796
0.826
0.812

78.3
80.50
79.76

the boosting and maximum entropy model. So, the mathe-
matical formulation for this relation is worth investigating.

The subtree mining approach to question classiﬁcation
relies on a parser to get the syntactic trees. However,
most parsers, including the Charniak’s parser we used in
the above experiments, are not targeted to parse questions.
Those parsers are usually trained on the Penn Treebank
corpus, which is composed of manually labeled newswire
text. Therefore it is not surprising that those parsers can not
achieve a high accuracy in parsing questions, because there
are only very few questions in the training corpus. In [Herm-
jakob, 2001], the accuracy of question parsing dramatically
improves when complementing the Penn Treebank corpus
[Marcus et al, 1993] with an additional 1153 labeled ques-
tions for training. We believe that a better parser is beneﬁcial
to our approach.

Summarily, all experiments show that the subtree mined
from the corpus is very useful for the task of question classi-
ﬁcation. It also indicated that subtrees can be used as feature
functions in maximum entropy model and weak function as
in a boosting model. In addition, in the paper we give an al-
ternative method for using subtrees in question classiﬁcation.
As the results showed in our experiment, we can conclude
that mining subtrees is signiﬁcantly contribution to the per-
formance of question classiﬁcation.

In the paper, we investigate the use of syntactic tree repre-
sentation which do not overlap the named entity types and the
relation words, so other tree structure representation with the
combination of syntactic and semantic information be bene-
ﬁcial to our approach. In future, we would like to investigate
the use of semantic parsing such as semantic role labelling to
the task.

5 Conclusions

This paper proposes a method which allows incorporating
subtrees mined from training data to the question classiﬁ-
cation problem. We formulate the question classiﬁcation as
the problem of classifying a tree into a preﬁxed categories.
We then proposed the use of maximum entropy and booting
model with subtrees as feature and weak functions by consid-
ering subtree mining as subtree feature selection process.

Experimental

results show that our boosting model
achieved a high accuracy in comparison to previous work
without using semantic features. The boosting model out-
performed the maximum entropy model using subtrees but
its computational times is slower more than 12 times in com-
parison with that of the proposed maximum model. In ad-
dition, the combination of using boosting and maximum en-
tropy models with subtree features achieved substantially bet-

IJCAI-07

1699

ter results in comparison with other methods in term of either
accuracy or computational times performance.

Future work will also be focused on extending our method
to a version of using semi-supervised learning that can efﬁ-
ciently be learnt by using labeled and unlabeled data. We also
plan to exploit the use of hierarchical classiﬁcation model and
WordNet semantic information as described in[Li and Roth,
2002][Li and Roth, 2006].

Furthermore, we hope to extend this work to support inter-
active question answering. In this task, the question answer-
ing system could be able to interact with users to lead to more
variations of questions but with more contextual information.
Our subtree mining method does not depend on the task of
question classiﬁcation. So, we believe that our method is suit-
able for other tasks (i.e text classiﬁcation, text summarization,
etc) where its data can be represented as a tree structure.

Acknowledgments

We would like to thank to anonymous reviewers for help-
ful discussions and comments on the manuscript. Thanks
to the Cognitive Computation Group at UIUC for opening
their datasets. Thank to Mary Ann M at JAIST for correcting
grammatical errors in the paper.

The work on this paper was supported by the JAIST 21
century COE program ”Veriﬁable and Evolvable e-Society”.

References
[Berger et al, 1996] Adam Berger, Stephen Della Pietra, and
Vincent Della Pietra, A maximum entropy approach to nat-
ural language processing, Computational Linguistics, Vol.
22, No. 1, (1996).

[F. Berzal et al , 2004] F. Berzal,

Juan-Carlos Cubero,
Daniel Sanchez, Jose Maria Serrano. ART: A Hybrid
Classiﬁcation Model. Machine learning. Pages: 67 - 92

[E. Charniak, 2001] E. Charniak. A Maximum-Entropy In-

spired Parser. In Proc ACL 2001.

[Carlson, 1999] Andrew Carlson, Chad Cumby and Dan
Roth, The SNoW learning architecture, Technical report
UIUC-DCS-R-99-2101, UIUC Computer Science Depart-
ment (1999).

[Cortes and Vapnik, 1995] Corinna Cortes and Vladimir
Vapnik, Support vector networks, Machine Learning, Vol.
20, No. 3, pp. 273-297, (1995).

[Hacioglu and Ward, 2003] Hacioglu Kadri

and Ward
Wayne, Question classiﬁcation with Support vector
machines and error correcting codes, proceedings of
NAACL/Human Language Technology Conference, pp.
28-30, (2003).

[Hermjakob, 2001] U. Hermjakob. Parsing and Question
Classiﬁcation for Question Answering. Proceedings of the
ACL Workshop on Open-Domain Question Answering,
Toulouse, France, 2001.

[Kudo et al, 2005] Taku Kudo, Jun Suzuki, Hideki Isozaki.
Boosting-based parse reranking with subtree features, Pro-
ceedings ACL 2005.

[Kudo et al , 2004] Taku Kudo, Eisaku Maeda, Yuji Mat-
sumoto: An Application of Boosting to Graph Classiﬁca-
tion. Proceedings NIPS 2004.

[Liu and Nocedal, 1989] D. Liu and J. Nocedal. On the lim-
ited memory BFGS method for large-scale optimization.
Mathematical Programming, pp.503-528, Vol.45, 1989.

[Li and Roth, 2002] Xin Li and Dan Roth, Learning question
classiﬁers, Proceedings of the 19th International Confer-
ence on Computational Linguistics, pp. 556-562, (2002).

[Li and Roth, 2006] Learning question classiﬁers:

the role
of semantic information. Natural Language Engineering,
Volume 12, Issue 03, September 2006, pp 229-249.

[Marcus et al, 1993] M. Marcus, B. Santorini, and M.
Marcinkiewicz. 1993. Building a large annotated corpus
of english: the penn treebank. Computational Linguistics.

[Morhishita, 2002] Shinichi Morhishita. 2002. Computing
optimal hypotheses efﬁciently for boosting. In Progress in
Discovery Science, pages 471.481. Springer.

[Nguyen et al, 2006] Le-Minh Nguyen and Akira Shimazu,
and Xuan-Hieu Phan. Semantic Parsing with Structured
SVM Ensemble Classiﬁcation Models. Proceedings of the
COLING/ACL 2006 Main Conference Poster Sessions, pp.
619–626.

[Zhang and Lee, 2003] Dell Zhang and Wee Sun Lee, Ques-
tion classiﬁcation using Support vector machine, proceed-
ings of the 26th Annual International ACM SIGIR Confer-
ence, pp. 26-32, (2003).

[Zaki, 2002] Mohammed J. Zaki. 2002. Efﬁciently Min-
ing Frequent Trees in a Forest. In proceedings 8th ACM
SIGKDD 2002.

[Schapire, 1999] Schapire, 1999. A brief introduction to

boosting. Proceedings of IJCAI 99

[Radev et al, 2002] D. R. Radev, W. Fan, H. Qi, H. Wu
and A. Grewal. Probabilistic Question Answering from the
Web. In Proceedings of the 11th World Wide Web Confer-
ence (WWW2002), Hawaii, 2002.

[Voorhees, 1999] E. Voorhees. The TREC-8 Question An-
swering Track Report. In Proceedings of the 8th Text Re-
trieval Conference (TREC8), pp. 77-82, NIST, Gaithers-
burg, MD, 1999.

[Voorhees, 2000] E. Voorhees. Overview of the TREC-9
Question Answering Track. In Proceedings of the 9th Text
Retrieval Conference (TREC9), pp. 71-80, NIST, Gaithers-
burg, MD, 2000.

[Voorhees, 2001] E. Voorhees. Overview of the TREC 2001
Question Answering Track. In Proceedings of the 10th
Text Retrieval Conference (TREC10), pp. 157-165, NIST,
Gaithersburg, MD, 2001

IJCAI-07

1700


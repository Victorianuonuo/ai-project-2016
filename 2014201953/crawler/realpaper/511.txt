Multi-prototype Support Vector Machine 

Fabio Aiolli 

Dept. of Computer Science 

University of Pisa, Italy 

aiolli@di.unipi.it 

Alessandro Sperduti 

Dept. of Pure and Applied Mathematics 

University of Padova, Italy 
sperduti@math.unipd.it 

Abstract 

1.1 

Preliminaries  and  notation 

We  extend multiclass  SVM  to  multiple prototypes 
per class.  For this framework,  we give a compact 
constrained  quadratic  problem  and  we  suggest  an 
efficient algorithm for its optimization that guaran(cid:173)
tees a local minimum of the objective function.  An 
annealed process is also proposed that helps to es(cid:173)
cape from local minima.  Finally, we report experi(cid:173)
ments where the performance obtained using linear 
models  is  almost  comparable  to  that  obtained  by 
state-of-art kernel-based methods but with a signif(cid:173)
icant  reduction  (of one  or two  orders)  in  response 
time. 

Introduction 

1 
Automatic multiclass classification,  i.e.  the process to auto(cid:173)
matically assign exactly one from a prefixed set of labels to a 
stream of input instances, is a central task for many real world 
problems  like  speech  recognition,  OCR,  text  categorization 
etc.  Many  supervised  learning  methods  have  been  studied 
that help on these tasks.  Recently, kernel-based methods, like 
SVM,  have been  well  studied  especially  for binary settings, 
and  they  yielded  state-of-art  performance  in  many  different 
tasks.  SVM  searches  for a  high  margin  linear discriminant 
model in a very high dimensional space (feature space) where 
examples are  implicitly  mapped via a  function 
.  Since 
kernel-based algorithms need only of dot products in  feature 
space,  it  is  possible  to  resort  to  the  so  called  kernel  trick  if 
these dot products can be computed efficiently with a kernel 
function  K(x, y)  =  
•.  When the number of exam(cid:173)
ples (or support vectors) is large, these approaches tend to be 
less efficient w.r.t.  the time spent for classifying new vectors 
since  they  require to  work  with  the  implicit characterization 
of the discriminant model. 

One  of  the  most  effective  SVM  extension  able  to  deal 
with general multiclass problems has been recently provided 
[Crammer and Singer, 2000] and it has shown very good re(cid:173)
sults. 
In  this  paper,  we  extend  the  multiclass  SVM  to  the 
multi-prototype  setting  where  for  each  class  there  can  be 
more than one prototype vector.  This allowed us to get very 
expressive decision functions by using simple models without 
necessarily requiring the use of kernels. 

be a set of n  training exam(cid:173)
,A 

single-label 

l

s

e

p
multi-class classifier  is  a  f u n c t i o n t h at  maps  in(cid:173)
stances 
winneMake-all  linear  classifiers  with  form: 

We  focus  on  the  class  of 

to  labels 

(i) 

where  Mr  is  the r-th  prototype vector,  R  is the  set  of proto(cid:173)
type indexes and C(r) the function returning the class associ(cid:173)
ated to the prototype indexed r.  Given a classifier  hM(x)  and 
a training example 
misclassi-
fies 

I we will say that 

In the following, we denote by 

the constant that is equal 

f o t h e r w i s e.  For a given example x1,, 

to 1  i
we  denote by 
the  set  of "positive" 
prototypes  indexes  and  by 
'> the 
set of "negative" prototypes  indexes.  Finally,  the  real  value 
•  x  is  referred to  as similarity score (or simply 

score) of the r-th prototype on the  instance x. 

2  Single-prototype multi-class SVM 
An  effective multi-class extension to  SVM has been already 
proposed  in  [Crammer  and  Singer,  2000].  The  resulting 
classifier  is  of the  same  form  of (1)  where  each  class  has 
associated exactly one prototype,  i.e. 
The  solution  is  obtained  through  the  minimization  of  a 
convex  quadratic  constrained  problem. 
In  this  section,  we 
present a simpler derivation of an equivalent formulation that 
will  be  extended  in  the  next  section  to  the  multi-prototype 
framework. 

In the multiclass setting,  in order to have a correct classi(cid:173)
fication  of the  instance 
the  prototype of the  correct class 
should have a score that is greater than the maximum among 
the  scores  of the  prototypes  associated  to  incorrect  classes. 
We can formally write the constraints for a correct classifica(cid:173)
tion of the  example  with a margin greater or equal to 1 by 
requiring: 

where yi; =  d  is the  index of the prototype vector associated 
to the correct class  for the example x,.  To  allow  for margin 

LEARNING 

541 

violations, for each example, we introduce soft margin slack 
variables 

denotes  the  hinge  loss  equal  to  x  if x  >  0  and 
where 
0  otherwise.  Notice  that  the value 
is  an  upper bound to 
the 0 -  1  loss for the example Xi,  and consequently its aver(cid:173)
age value over the training set will be an upper bound for the 
empirical error. 

Motivated by the structural risk minimization (SRM) prin(cid:173)
ciple  in  [Vapnik,  1998],  we  want to search for a set of pro(cid:173)
totype  vectors  M  =  { M i , . ., M\c\}  with  small  norm  such 
to minimize the empirical error on the training set.  For this, 
we  can  formulate the  problem  in  a  SVM-style by requiring 
a  set of prototypes of minimal norm that fulfill  the soft con(cid:173)
straints  given  by the  classification  requirements.  Thus,  the 
single-prototype multiclass SVM (SProtSVM) will result in: 

(2) 

Notice that, as desired, the optimal solution for  will be the 
maximum value  among the  negative  scores for the  instance 
Xi.  This problem is convex and it can be solved in the stan(cid:173)
dard  way by resorting to  the optimization of the  Wolfe  dual 
problem. In this case, the lagrangian is: 

(3) 
subject to the constraints 
.  By differentiating the 
lagrangian  with  respect  to  the  primal  variables  and  impos(cid:173)
ing the optimality conditions we obtain the set of constraints 
(KKT conditions) that the variables must fulfill in order to be 
an optimal solution: 

3  Multi-prototype  multi-class SVM 
In this section, the SProtSVM model previously defined will 
be extended to the case of multiple prototypes per class.  The 
basic idea is that, given multiple prototype vectors, we have a 
correct classification iff at least one of the prototypes associ(cid:173)
ated to the correct class has a score higher than the maximum 
of the scores of the prototypes associated to incorrect classes. 
In this case, we write the constraints for a correct classifi(cid:173)
cation of the example  Xi  with a margin greater or equal to  1 
requiring: 

To  allow  for  margin  violations,  for  each  example  xi,  we 
0,  one  for each 

introduce  soft  margin  slack  variables 
positive prototype, s.t. 

Given a pattern Xi, we arrange the soft margin slack variables 
Let  us  now  introduce,  for  each 
f J* in a vector 
example Xi  a new vector 
, whose components 
are all zero except one component that is  1.  In the following, 
as the assignment of the pattern xt.  Notice that 
we refer to 
the dot product 
is always an upper bound to the 0—1 
loss for the example X;  independently from its assignment. 
Now, we are ready to formulate the multi-prototype prob(cid:173)
lem  by  requiring  a  set  of prototypes  of small  norm  and  the 
best assignment  for the examples able to  fulfill  the soft con(cid:173)
straints given by the classification requirements. 

Thus, the MProtSVM formulation will result in: 

(6) 

Unfortunately,  this  is  a  mixed  integer  problem  that  is  not 
convex and it is a difficult problem in general but, as we will 
see  in  the  following,  it  is  prone to  an  efficient  optimization 
procedure  that  approximates  the  global  optimum.  At  this 
point, it is worth noticing that, since the effective formulation 
is  itself  an  (heuristic)  approximation  to  the  structural  risk 
minimization principle,  a  good solution,  although  not  opti(cid:173)
mal, can anyway give good results. As we see after, this claim 
is confirmed by the results obtained in our experimental work. 

By  using the  fact 

(4) 
and substituting condi(cid:173)
tions (4) in (3) and omitting constants that do not change the 
solution, the problem can be restated as: 

Let suppose now that the assignments are kept fixed. In this 
case the reduced problem becomes convex and can be solved 
as  above by resorting to  the  optimization  of the  Wolfe  dual 
problem. In this case, the lagrangian is: 

(5) 

The  next  section  includes  an  efficient  optimization  pro(cid:173)
cedure for the more  general  multi-prototype setting that  in(cid:173)
cludes the single-prototype case as a particular instance. 

subject to the constraints 
.  As above, by differen(cid:173)
tiating the  lagrangian of the  reduced problem and  imposing 

542 

LEARNING 

the optimality conditions, we obtain: 

Notice that the second condition requires the dual variables 
associated to positive prototypes and not assigned to the as(cid:173)
sociated pattern to be 0.  By denoting now as yi  the unique 
index 
, once using the conditions (8) 
in (7) and omitting constants that do not change the obtained 
solution, the reduced problem can be restated as: 

such that 

(9) 

It can be trivially shown that this formulation is consistent 

with the formulation for SProtSVM's given above. 

3.1  Optimization with  static  assignments 
When  patterns  are  statically  assigned  to  the  prototypes 
via  constant  vectors 
the  convexity  of  the  associated 
MProtSVM problem implies that the optimal solution for the 
primal problem in (6) can be found through the maximization 
of the lagrangian in (9). 

Assuming an equal number q of prototypes per class, the 
dual involves n x m x q variables which lead to a very large 
scale  problem.  Anyway,  the  independence  of constraints 
among the different patterns allows for the separation of the 
variables in n disjoint sets of m x q variables. 

The algorithm we propose for the optimization of the prob(cid:173)
lem in (9) is inspired by the ones already presented in [Cram(cid:173)
mer and Singer, 2000; Aiolli and Sperduti, 2002a] and con(cid:173)
sists in iteratively selecting patterns from the training set and 
optimizing with respect to the variables associated to that pat(cid:173)
tern.  From the convexity, each iteration leads to an increase 
of the lagrangian until no more improvement is possible since 
the optimal solution for the lagrangian has been found. 

After  selecting  a  pattern  Xi,  to  enforce  the  constraint 
two  elements from the  set 
of variables 
will be optimized in pair 
while keeping the solution inside the feasible region.  In par(cid:173)
ticular, let wi and w2 be the two selected variables, we restrict 
the updates to the form 
with 
optimal choices for p.  Iterating the application of this basic 
step over different pairs and then doing the same for different 
patterns will guarantee to reach the optimum for the overall 
problem. 

Let now compute the optimal value for p.  First of all, we 
observe that the  update will  affect the  squared norm of the 
prototype vector Mr  of an  amount 

Now,  we  show  how  to  analytically  solve  the  associated 
problem with respect to an update involving a single variable 
does not influence 

and the variable 

Since 

If the values of a7

p and aft, after being updated, would turn 
out to be not feasible for the constraints ar
p  >  0 and aft  < 7, 
we select the value for p such to fulfill the violated constraint 
bounds at the limit. 

Now,  we  show  how  to  analytically  solve  the  associated 
problem with  respect to an  update  involving a pair of vari(cid:173)
ables a^1, ap
2  such that r\,  r2  E  Ni  and  =  ^  r-i.  Since,  in 
this case, the update must have zero sum, we have: 

and 
2, after being updated, would turn out to be not feasible for 
p
we select the value for 

Similarly  to  the previous case,  if the  values  of the 
ar
the constraints 
p such to fulfill the violated constraint bounds at the limit. 

Iterating multiple times the basic step described above on 
pairs  of variables  chosen  among  that  associated  to  a  given 
pattern  it  is  guaranteed to  find  the  optimality condition for 
the  pattern.  This  has  been  exploited  in  [Aiolli  and  Sper-
duti, 2002a] to devise an incremental algorithm that uses the 
method on the reduced problem of a single example and then 
iterates  over  different  examples.  This  optimization  proce(cid:173)
dure can be considered incremental in the sense that the so(cid:173)
lution previously found for a given pattern forms the initial 
condition  when  the  pattern  is  selected  again  for  optimiza(cid:173)
tion. 
In  this  version,  the  basic  step  of optimization of the 
reduced problem  can  require  the  optimization  over all  the 
pairs  of variables  not 
constrained to 0 associated with the selected pattern.  Thus 
the complexity of the optimization of the reduced problem is 

where / is the number of iterations. 

Now, we perform a further step by giving an algorithm that 
at each iteration has a complexity o(m x q). For this, we ob(cid:173)
serve that for the variables 
associated to the pattern 

LEARNING 

543 

xp  to be optimal, the  feasible  analytic  solution p must be 0 
for each pair. 

In particular, for the first case above, in order to be able to 
apply  the  step,  it  is  required that,  one  of the  following two 
conditions are verified, i.e.: 

while for the second case we must have: 

0) 

Notice that these facts can be checked in linear time.  It is 
easy to show that the solution obtained at a certain step can 
be improved iff one of these facts are verified.  This suggests 
an efficient procedure, shown  in Figure  l(Top),  that tries to 
greedily  fulfill  the previous condition of optimality. 
3.2  Optimization of general MProtSVM 
In this section, we present an efficient procedure that guaran(cid:173)
tees to reach a local minimum of the objective function of the 
problem in (6) associated to MProtSVM. This procedure will 
try to optimize also with respect to the assignments 

Let suppose to start with a given assignment 

I  for the 
patterns. In this case, as we have already seen, the associated 
problem is convex and can be efficiently solved by using the 
algorithm given in Section 3.1.  Once that the optimal value 
for the primal 
has been reached, we observe that the so(cid:173)
lution can be  further improved by  updating the assignments 
in such a way to assign each pattern 
to a positive prototype 
with  minimal  slack  value,  i.e.  by  setting  the  vector 
(2) 
such to have the unique 1 corresponding to the best perform(cid:173)
ing positive prototype.  However,  with  this  new  assignment 
7r(2), the variables  might not fulfill  the second KKT con(cid:173)
dition in  eq.  (8) anymore.  If it is the case,  it simply means 
that the current solution 
is not optimal for the new assign(cid:173)
ment.  Thus,  a  lagrangian  optimization  done  by  satisfying 
constraints  dictated by  KKT  conditions  for the  new  assign(cid:173)
ment is guaranteed to obtain a new  with  a better optimal 
primal value 
.  For the optimization algorithm to  suc(cid:173)
ceed, however, the KKT conditions on the  must be restored 
in order to return back to a feasible solution and then finally 
resuming  the  lagrangian  optimization  with  the  new  assign(cid:173)
ment 7r(2).  We restore the KKT conditions by setting 
whenever there exists 

such that 

Performing the same procedure over different assignments, 
each  one  obtained  from  the  previous  one by  the  procedure 
just mentioned, implies the convergence of the algorithm to a 
local solution for the primal problem when no improvements 
are  possible and the  KKT conditions are  all  fulfilled by  the 
current solution. This is due to the fact that every step induces 
an improvement on the primal value. 

The first problem with this procedure is that it results oner(cid:173)
ous when  dealing with many prototypes since  we must per(cid:173)
form  many  lagrangian  optimizations.  For this,  we  observe 
that for the procedure to work, at each step, it is sufficient to 
stop the optimization of the lagrangian when we find a value 

Figure 1: Top: algorithm for the optimization of the variables 
associated with a given pattern xp and a tolerance e.  Bottom: 
algorithm for the optimization of MProtSVM. 

544 

LEARNING 

for the primal  better than the last found and this is going to 
happen for sure since the last solution has been found not op(cid:173)
timal. This requires only a periodic check of the primal value 
when optimizing the lagrangian. 

The second and more stringent problem is that the proce(cid:173)
dure will  lead to a local minimum that can be very far from 
the best possible.  For this, we suggest to update the assign(cid:173)
ments on the basis of a  stochastic  annealed function instead 
of the hard decision function described above. 

Specifically, let us view the value of the primal as an energy 

function 

Let  suppose  to  have  a  pattern  Xi  having  slack  variables 

and suppose that the probability for the assignment to 
be in the state s (i.e. with the 5-th component set to 1) follows 
the law 

I 

where T is the temperature of the system and 

the  variation of the  system  energy  when  the pattern  Xi 
is assigned to the s-th prototype.  By multiplying every term 

by the normalization term exp 

and considering that probabilities over alternative 

states must sum to one, i.e. 

, we obtain 

with 

the partition function. 
Thus,  when assigning the pattern Xi,  each positive proto(cid:173)
type  s  will  be  selected  with  probability Pi{s).  Notice  that, 
when  the  temperature  of the  system  is  low,  the  probability 
for a pattern to be assigned to a prototype different from the 
one having minimal  slack  value tends to 0 and we obtain a 
behavior similar to the not annealed version of the algorithm. 
The simulated annealing is typically implemented by decreas(cid:173)
ing the temperature as the number of iterations increases by a 
monotonic  decreasing  function  T  =  T(t,T0).  The  full  algo(cid:173)
rithm is depicted in Figure  1 (Bottom). 

4  Experimental Results 
We tested our model against three datasets that we briefly de(cid:173)
scribe in the following: 
N1ST: it consists of a 10-class task of 10705 digits randomly 
taken  from  the  NIST-3  dataset.  The  training  set  con(cid:173)
sists of 5000 randomly chosen digits, while the remain(cid:173)
ing 5705 digits are used in the test set. 

USPS: it consists of a 10-class OCR task (digits from 0 to 9) 
whose input are the pixels of a scaled digit image. There 
are 7291 training examples and 2007 test examples. 

LETTER:  it  consists  of a  26-class  task  (alphabetic  letters 
A-Z). Inputs are measures of the printed font glyph. The 
first 16000  examples  are  used  for training  and  the  last 
4000 for testing. 

Even if any kernel function can be in principle used, for the 
following preliminary experiments, we used the linear kernel 
K(x, y)  =  (x - y +  1).  This allowed us to write an optimized 

code  for training  and  classification  that  works  directly  with 
the explicit (compact) version of the model  M. 

Since  we  are  primarily  interested  into  the  evaluation  of 
MProtSVM w.r.t.  SProtSVM, for each dataset, we performed 
validation on the parameter 
for the  SProtSVM  model  and 
we re-used the obtained value for training every MProtSVM 
generated for the same dataset.  This approach seems us any(cid:173)
way a pessimistic estimate of the performance of MProtSVM. 
The  annealing process  required  by  MProtSVM  has  been 
implemented  by  decreasing  the  temperature  of  the  system 
with the exponential law: 

where 0  <  r  <  1  and T0  >  0 are external parameters.  We 
used To  =  10 for all the following experiments. 

Table  1:  Comparison  of  generalization  performances  be(cid:173)
tween  LVQ and MProtSVM  increasing the  number of code-
books/prototypes on the NIST dataset 

Table  2:  (a)  Test  error  of MProtSVM  on  the  USPS  dataset 
,  with  an  increasing 
number  of prototypes;  (b)  Test  error  of MProtSVM  on  the 
LETTER dataset 
with an 
increasing  number of prototypes. 

A set of experiments have been performed to compare the 
generalization performance of our (linear) model versus LVQ 
[Kohonen et al.,  1996] which seemed to us the most compa(cid:173)
rable model.  For this, we have reported the best results that 
have  been  obtained by  LVQ  on  the  NIST  dataset.  Specifi(cid:173)
cally,  they  have  been  obtained  with  the  LVQ2.1  version  of 
the algorithm (see [Sona and Sperduti, 2000]).  As it is possi(cid:173)
ble to see in Table 1, MProtSVM performs significantly better 
when few prototypes per class are used, while the difference 
gets lower when the number of prototypes per class increases. 
This can be due to the more effective control of the margin for 
SVM w.r.t.  LVQ models.  On the same dataset, the tangent-
distance based TVQ algorithm  [Aiolli  and  Sperduti,  2002b] 
has  obtained  the  best  result,  a  remarkable  2.1%  test  error, 
and polynomial SVM's have obtained a 2.82% test error.  In 

LEARNING 

545 

Table 3:  Primal values and generalization error obtained with different configurations varying the parameter r (USPS dataset). 

addition, we tested the MProtSVM on the UCI  Irvine USPS 
and LETTER datasets.  As it is possible to see in Table 2, by 
combining a reasonably high number of linear prototypes, we 
have obtained performances almost comparable with the ones 
obtained in literature by using non-linear models.  In fact, on 
the  USPS  dataset,  we have  been able to get a  4.63%  error, 
using  a  SProtSVM  with  polynomial  kernel  of degree  3  and 
without preprocessing  the  data,  while  for  LETTER,  we  re(cid:173)
fer to the  1.95% obtained in  [Crammer and Singer,  2001]  by 
SProtSVM  with exponential kernel.  Although obtained with 
a slightly different split of the LETTER dataset (15000 exam(cid:173)
ples for training and 5000 for test), we would like to mention 
the  results  reported in  [Michie et al.,  1994]  where LVQ and 
k-NN yielded a 7.9% and 6.8% error, respectively. 

=  100  x  (m  x  q)/n,  i.e. 

Notice  that  MProtSVM  returns  far more  compact models 
with  respect  to  state  of the  art  non-linear  kernel  methods, 
thus  allowing  a  (one  or  two  order)  reduced  response  time 
in  classification.  Defining  a  sort  of model  complexity  fac(cid:173)
tor 
the  number  of prototypes 
produced  as  a  fraction  of the  cardinality  of the  training  set, 
the above experiments have shown very low value for 
(e.g. 
15  x  26 prototypes in the LETTER dataset gives  =  0.65% 
and 20  x  10 prototypes for USPS gives  =  2.74%).  Notice 
that 
can be directly compared with the  fraction of support 
vectors in kernel machines. Thus, MProtSVMs also give us a 
way to decide (before training) the complexity of the model. 
Finally,  in Table 3  we have reported the values of the ob(cid:173)
jective function of the primal problem in (6) along with their 
corresponding test errors obtained with the USPS dataset us(cid:173)
ing different configurations and lowering the parameter r.  As 
expected,  fixed  a raw  in  the  table,  better values  for the pri(cid:173)
mal  can  be  obtained  with  lower  values  of r.  Moreover,  as 
the  number  of prototypes  per  class  increases,  the  choice  of 
small r tends to be more crucial.  Anyway, higher values for 
r, and thus not optimal values for the primal, can neverthless 
lead to good generalization performances. This can be due to 
the fact that the primal value is just a way to approximate the 
theorethical SRM principle. 

5  Conclusion 
We  have  proposed  an  extension  of multiclass  SVM  able  to 
deal with several prototypes per class. This extension defines 
a non-convex problem.  We suggested to solve this problem 
by  using  a novel  efficient  optimization procedure within  an 
annealing framework where the energy function corresponds 
to the primal  of the problem.  Experimental results on some 
popular benchmarks demonstrated that it is possible to reach 
very  competitive  performances  by  using  few  linear  models 

per class instead of a single model per class with kernel. This 
allows the  user to  get very  compact models  which  are  very 
fast in classifying new patterns.  Thus, according to the com(cid:173)
putational constraints, the user may decide how to balance the 
trade-off between better accuracy and speed of classification. 
Finally,  it should be noted that the proposed approach com(cid:173)
pares favorably versus LVQ, a learning procedure that, simi(cid:173)
larly to the proposed approach, returns a set of linear models 
per class. 

References 
[Aiolli and Sperduti, 2002a]  Fabio  Aiolli  and  Alessandro 
Sperduti.  An  efficient  smo-like  algorithm  for  multiclass 
In  Proceedings  of IEEE  workshop  on  Neural  Net(cid:173)
svm. 
works/or  Signal Processing,  2002. 

[Aiolli and Sperduti, 2002b]  Fabio  Aiolli  and  Alessandro 
Sperduti.  A re-weighting strategy for improving margins. 
Artificial  Intelligence  Journal,  137/1-2:197-216,2002. 

[Crammer and Singer, 2000]  Koby  Crammer  and  Yoram 
Singer.  On  the  learnability  and  design  of output  codes 
for multiclass problems. In Proceedings of the Thirteenth 
Annual  Conference  on  Computational  Learning  Theory, 
pages 35-46,2000. 

[Crammer and Singer, 2001]  Koby  Crammer  and  Yoram 
Singer.  On  the  algorithmic  implementation  of multiclass 
kernel-based machines. Journal of Machine Learning Re(cid:173)
search, 2(Dec):265-292,2001. 

[Kohonen et al.,  1996]  Teuvo Kohonen, Jussi Hynninen, Jari 
Kangas, Jorma Laaksonen, and Kari Torkkola.  Lvq_pak: 
The learning vector quantization program package.  Tech(cid:173)
nical Report A30, Helsinki University of Technology, Lab(cid:173)
oratory  of Computer  and  Information  Science,  Rakenta-
janaukio  2  C,  SF-02150  Espoo,  Finland,  January  1996. 
http://www.cis.hut.fi/nnrc/nnrc-programs.html. 

[Michie etal,  1994]  D.  Michie,  D.  Speigelhalter,  and 
C.  Taylor.  Machine Learning,  Neural and Statistical Clas(cid:173)
sification.  Ellis Horwood,  1994. 

[Sona and Sperduti, 2000]  Diego  Sona  and  Alessandro 
Sperduti. 
recognition  using 
transformation-invariant  neurons.  Neural  Computation, 
12:1355-1370,2000. 

Discriminant  pattern 

[Vapnik,  1998]  V.  Vapnik.  Statistical Learning Theory.  Wi(cid:173)

ley, 1998. 

546 

LEARNING 


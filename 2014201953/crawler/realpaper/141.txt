Learning Web Page Scores by Error Back-Propagation

Michelangelo Diligenti, Marco Gori, Marco Maggini

Dipartimento di Ingegneria dell’Informazione

Universit`a di Siena

Via Roma 56, I-53100 Siena (Italy)
{michi, marco, maggini}@dii.unisi.it

Abstract

In this paper we present a novel algorithm to learn a
score distribution over the nodes of a labeled graph
(directed or undirected). Markov Chain theory is
used to deﬁne the model of a random walker that
converges to a score distribution which depends
both on the graph connectivity and on the node la-
bels. A supervised learning task is deﬁned on the
given graph by assigning a target score for some
nodes and a training algorithm based on error back-
propagation through the graph is devised to learn
the model parameters. The trained model can as-
sign scores to the graph nodes generalizing the cri-
teria provided by the supervisor in the examples.
The proposed algorithm has been applied to learn a
ranking function for Web pages. The experimental
results show the effectiveness of the proposed tech-
nique in reorganizing the rank accordingly to the
examples provided in the training set.

1

Introduction

In this paper we propose an algorithm able to learn a
generic distribution of values over the nodes of a graph with
symbolic labels. In many applications, the input data is nat-
urally represented by a labeled graph. However, the actual
lack of adequate algorithms to directly process this structured
data commonly forces to convert the input patterns to vectors
of real numbers, loosing signiﬁcant information. In particu-
lar, “Web like” data structures are common in many pattern
recognition tasks and possible applications range from object
recognition in segmented images to molecular analysis in bio-
informatics, and so on.

Like in the PageRank algorithm [Page et al., 1998], the
score of a node is assumed to correspond to the probability
that a random walker will visit that node at any given time.
The score distribution computed by the random walker de-
pends both on the connectivity graph and the labels assigned
to each node. Markov Chain theory allows to deﬁne a set
of constrains on the parameters of the random walker in or-
der to guarantee some desired features like convergence and
independence of the ﬁnal distribution on the initial state. A
learning algorithm which is based on error back-propagation
through the graph is used to optimize the parameters of the

random walker minimizing the distance of the scores assigned
by the walker from the target values provided by a supervisor
for some nodes in the graph.

Previous work in the same ﬁeld achieved very limited re-
sults. For example, in [Kondor and Lafferty, 2002] the au-
thors present an algorithm that processes only non labeled
graphs, while the technique presented in [Chang et al., 2000]
is very application speciﬁc and features very limited general-
ization capabilities. In [Tsoi et al., 2003] a method to modify
PageRank scores computed over a collection of pages on the
basis of a supervisor’s feedback is proposed. This algorithm
has limited approximation capabilities, since it controls only
a term of the PageRank equation and it does not optimize how
the scores ﬂow though the connectivity graph.

The algorithm presented in this paper allows to learn target
scores assigned as real numbers, and this makes it more gen-
eral than the one proposed in [Diligenti et al., 2003] which
allows only to specify a set of nodes whose scores should be
increased or decreased.

The paper is organized as follows. In the next section the
web surfer model is introduced. Then, in section 3 the learn-
ing algorithm based on error back-propagation is described,
while the experimental results are presented in section 4. Fi-
nally, in section 5 the conclusions are drawn.

2 Random Walks

We consider the model of a graph walker that can perform
one out of two atomic actions while visiting the Web graph:
jumping to a node of the graph (action J) and following a
hyperlink from the current node (action l).

In general, the action taken by the surfer will depend on
the label and outlinks from the node where it is located. We
can model the user’s behavior by a set of probabilities which
depend on the current node: x(l|q) is the probability of fol-
lowing one hyperlink from node q, and x(J|q) = 1 − x(l|q)
is the probability of jumping from node q.
The two actions need to specify their targets: x(p|q, J)
is the probability of jumping from node q to node p, and
x(p|q, l) is the probability of selecting a hyperlink from node
q to node p. x(p|q, l) is not null only for the nodes p linked
directly by node q, i.e. p ∈ ch(q), being ch(q) the set
of the children of node q in the graph G. These sets of
values must satisfy the probability normalization constraints
p∈ch(q) x(p|q, l) = 1.

p∈G x(p|q, J) = 1 andP

∀q ∈ G P

The model considers a temporal sequence of actions per-
formed by the surfer and it can be used to compute the prob-
ability xp(t) that the surfer is located in node p at time t. The
probability distribution on the nodes of the Web is updated by
taking into account the actions at time t + 1 using the follow-
ing equation

xp(t + 1) = X
+ X

q∈G

q∈pa(p)

x(p|q, J) · x(J|q) · xq(t) + (1)

x(p|q, l) · x(l|q) · xq(t) ,

where pa(p) is the set of the parents of node p. The score
xp of a node p is computed using the stationary distri-
bution of the Markov Chain deﬁned by equation (1) (i.e.
xp = limt→∞ xp(t)). The probability distribution over all
the nodes at time t is represented by the vector x(t) =
[x1(t), . . . , xN (t)]0, being N the total number of nodes.
An interesting property of this model is that, under the as-
sumption that x(J|q) 6= 0 and x(p|q, J) 6= 0, ∀p, q ∈ G, the
stationary distribution x? does not depend on the initial state
vector x(0) (see [Diligenti et al., 2002]). For example this is
true for the PageRank equation for which x(J|q) = 1 − d,
being 0 < d < 1, and x(p|q, J) = 1
N .

The random walker model of equation (1) is a simpliﬁed
version of the model proposed in [Diligenti et al., 2002],
where two additional actions are considered by allowing the
surfer to follow backlinks or to remain in the same node. The
learning algorithm proposed in the next section can be easily
extended to this more general model.

3 Learning the score distribution
The random surfer assigns the scores according to the prob-
abilities x(p|q, J), x(J|q), x(p|q, l), and x(l|q). In most of
the approaches proposed in the literature, these parameters
are predeﬁned or computed from some features describing
the node p and/or the node q. For example, setting the param-
eters to x(l|q) = d, x(J|q) = 1 − d, x(p|q, J) = 1
N , and
x(p|q, l) = 1
ch(q) , we obtain the popular PageRank random
surfer [Page et al., 1998].

In the general case, the model has a number of parame-
ters quadratic in the number of nodes and it is infeasible to
estimate these parameters without any assumption to reduce
their number. In particular, we assume that the labels of nodes
can assume n distinct values C = {c1, . . . , cn} and that the
surfer’s behavior for any node p is dependent only on its label
cp ∈ C and the labels of the nodes pointed by p. Therefore,
the parameters of the model are rewritten as:

P
k∈ch(q) x(ck|cq, l)

x(cp|cq, l)

x(p|q, l) =
x(p|q, J) = x(cp|cq, J)

|cp|
x(l|p) = x(l|cp)

x(J|p) = x(J|cp)

where cq ∈ C is the label of node q and |cp| is the number of
nodes with label cp. The parameter x(cp|cq, l) represents the

tendency of the surfer to follow a hyperlink from a node with
label cq to a node with label cp, the parameter x(cp|cq, J) is
the probability of the surfer to jump from a node with label cq
to a node with label cp and x(l|ci) = 1 − x(J|ci) represents
the probability that the surfer decides to follow a link out of
a node with label ci. In the following, we indicate as P the
set of model parameters. T , J , B indicate transition, jump
and behavior parameters, respectively. In particular, it holds
that: P = {T ,J ,B}, T = {x(ci|cj, l) i, j = 1, . . . , n},
J = {x(ci|cj, J) i, j = 1, . . . , n}, and B = {x(l|ci) i =
1, . . . , n}. Under these assumptions, the random surfer equa-
tion becomes,

xp(t + 1) = X
+ X

q∈G

q∈pa(p)

x(cp|cq, J)

|cp|
x(cp|cq, l)

P
k∈ch(q) x(ck|cq, l)

· x(J|cq) · xq(t) +

(2)

· x(l|cq)·xq(t) ,

Typically, the values x(cp|cq, l) are initialized to 1, such that
all links are equally likely to be followed. Because of the
model assumptions, the stronger is the tendency of the surfer
to move to a node with a speciﬁc label, the higher are the
scores associated to the nodes with that label. These assump-
tions reduce the number of free parameters to 2·(n2−n)+n =
2 · n2 − n, being n the number of distinct labels.
3.1 Learning the parameters
We assume that a supervisor provides a set of examples E
in the graph G, where each example s ∈ E is a node associ-
ated to a target score xd
s.
puted by a surfer model using the set of parameters P,

Consider the following cost function for the scores com-

EP =

1
|E|

eP
s =

1
|E|

· (xP

s − xd

s)2 ,

(3)

1
2

X

s∈E

X

s∈E

where eP
s is called instant error for the node s. Such error
takes into account the distance between the target score and
the value returned by the ranking function.
scent with respect to a generic parameter w ∈ P, i.e.,

We want to minimize the cost function (3) by gradient de-

w0 = w − η

∂EP
∂w

= w − η
|E|

∂eP
s
∂w

.

(4)

X

s∈E

Using a procedure similar to the back-propagation through
time algorithm (BPTT) [Haykin, 1994], commonly used in
training recurrent neural networks, for each node s in the
training set we unfold the graph starting from s at time T
and moving backward in time through the path followed by
the random surfer. The graph unfolding is basically a back-
ward breadth-ﬁrst visit of the graph, starting from the target
node at time T , where each new unfolded level corresponds
to moving backward of one time step. Each unfolded node is
associated to the variable xq(T−t) where q is the correspond-
ing graph node, t is the number of backward steps from the
target node. Similarly, the parameters of the model are split
into an independent set of parameters corresponding to the

= X
= X

·

δz(t + 1) ·

z∈ch(p)
x(l|cp)(t) =

P
k∈ch(p) x(ck|cp, l)(t)

x(cz|cp, l)(t)

·

δz(t + 1) · P (p, t → z, t + 1) =

z∈ch(p)
s − xd

= (xP

s) · P (p, t → s, T ) ,

where ch(p) is the set of children of node p in the unfolding
and P (p, t1 → z, t2) indicates the probability of the surfer to
follow a path in the unfolding connecting the node p at time
t1 to the node z at time t2. Only the inﬂuence of transition
parameters on the score of node p was taken into account in
the propagation of the delta values.

Neglecting the inﬂuence of the jump parameters (i.e. tak-
ing into account only the inﬂuence due to link following) is
an accurate approximation when the probability of following
a link is much higher than the probability of jumping (like it is
commonly assumed in PageRank). However, the experimen-
tal results show that such an approximation provides good
results in the general case.
A generic node p at time t has inﬂuence over a supervised
node s at time T proportional to the probability P (p, t →
s, T ) that the surfer will follow the path from the node p at
time t to the central node s at time T . An implication of this
result is that farther nodes will have less inﬂuence, given the
exponential decrease of the probability of following a path
∂eP
with its length. In particular, a node p inﬂuences
p (t) only
∂xP
if P (p, t → s, T ) 6= 0. Since Us(T − t) is the set of nodes
from which it is possible to reach node s at time T starting
at time t, only for the nodes in this set it holds that P (p, t →
s, T ) 6= 0. Thus, in equation (8) not all the score variables at
time t + 1 must be considered in the application of the chain
rule. Inserting equation (8) into (6), and limiting the chain
rule only to the nodes with a non-zero inﬂuence, yields,

s

T−1X

X

t=−∞

p∈Us(t+1)

P (p, t → s, T ) ·

∂eP
s
∂w

= (xP

s − xd
s)
∂xP

p (t + 1)
∂w(t)

.

Since the stable point of the system does not depend on the
initial state, without loss of generality, the unfolding can be
interrupted at time 1 (the graph is unfolded T times) assuming
that T is large enough so that the state before time 1 has no
effect on the ﬁnal state (this is also expressed by the fact that
the probability of following a path in the unfolding P (p, t →
s, T ) converges to 0 when the path length T − t increases),

T−1X

X

t=1

p∈Us(t+1)

P (p, t → s, T ) ·

∂eP
s
∂w

= (xP

s − xd
s)
∂xP

p (t + 1)
∂w(t)

·

.

(9)

∂xP

In the following sections, we compute the derivative
p (t+1)
to be inserted into equation (9) for a transition, jump
∂w(t)

and behavior parameter.

T−1X

X

t=−∞

p∈G

∂eP
s
∂w

=

∂eP
p (t + 1)

s

∂xP

· ∂xP

p (t + 1)
∂w(t)

.

(6)

·

Figure 1:
(a) A graph composed by three nodes. (b) The ﬁrst four
levels of the unfolding for the graph, starting from node b at time
T . U (t) indicates the set of nodes at the level t of the unfolding
(corresponding to the time step t). In the example, U (T ) = {b},
U (T − 1) = {a, c}, U(T − 2) = {a} and U (T − 3) = {a}.

different time steps. We indicate with P(t) the model param-
eters at time t and with w(t) ∈ P(t) the value of any given
parameter at time t. An inﬁnite unfolding is perfectly equiv-
alent to the original graph with respect to the computation of
the score xP
s (T ) for a target node s. However, the unfolding
takes into account only the score propagation due to the for-
ward links, whereas the contribution of jumps (coming from
any node in the graph) is neglected.
In the following, we indicate as Us(t) the set of nodes con-
tained in the unfolding centered on node s for the time step t.
See ﬁgure 1 for an example of graph unfolding.

Thanks to the parameter decoupling in the unfolding, a pa-
rameter w(t) inﬂuences only the scores at the following time
step. Thus, the derivative of the instant error with respect to a
parameter w(t) can be written as,
∂eP
p (t + 1)

= X

p (t + 1)
∂w(t)

∂eP
∂w(t)

· ∂xP

∂xP

(5)

.

s

s

p∈G

Assuming that the parameters of the surfer are stationary
(i.e. they do not depend on time: w(t) = w ∀t), the derivative
with respect to a single parameter can be obtained summing
all the contributions for the single time steps,

The term ∂eP

s

s

∂xP

= xP

s (T ) − xd
s .

p (t+1) in equation (6) can not be directly com-
puted, unless when t = T − 1, s = p. In this latter case, it
holds that,

∂eP
∂xP
s (T )
∂eP
Let δp(t) indicate
p (t) . If p is not a target node, then
∂xP
∂eP
p (t) can be rewritten as,
∂xP
∂eP
∂xP
p (t)

· ∂xP
z (t + 1)
∂xP

∂xP

p (t)

≈

(7)

s

s

s

= δp(t) = X
≈ X

z∈G

z∈ch(p)

∂eP
z (t + 1)

s

∂xP

s

∂eP
z (t + 1)
· ∂xP
z (t + 1)
∂xP

p (t)

=

(8)

acabcx   (T−1)ax   (T−3)  P(t−1)P(t−1)P(t−2)P(t−2)P(t−3)x   (T−1)  x   (T)bax   (T−2)  (a)(b)U(T)={b}U(T−1)={a,c}U(T−2)={a}U(T−3)={a}3.2 Learning the transition parameters

For a transition parameter w = x(ci|cj, l), equation (2)
∂xP
∂x(ci|cj ,l)(t) = 0 if cp 6= ci. On the other hand if

p (t+1)
shows that
cp = ci, it holds that,
∂xP
p (t + 1)
∂x(ci|cj, l)(t)

x(l|cq)(t)xP

= X
h
P
x(cp|cq,l)(t)
k∈ch(q) x(ck|cq,l)(t)
∂x(ci|cj, l)(t)

q (t) ·
i

q∈pa(p)

∂

·

The application of the derivative rule for a fraction of func-

tions, yields that when cp = ci,

∂xP

p (t+1)

∂x(ci|cj ,l)(t) =P
P
[P

·

q (t) ·
k∈ch(q) x(ck|cq,l)(t)−x(cp|cq,l)(t)·|k∈ch(q):ck=ci|

x(l|cq)(t) · xP

q∈pa(p)
q:cq =cj

(10)

k∈ch(q) x(ck|cq,l)(t)]2

where |k ∈ ch(q) : ck = ci| is the number of children nodes
of q having label ci.

Without loss of generality, it can be assumed that the sys-
tem has already converged at time 0 (i.e. xq = xq(t), t =
0, . . . , T, ∀q ∈ G). Inserting equation (10) into (9) and, after
removing all temporal dependencies from the parameters and
from the node scores under the assumption of model station-
arity and convergence at time 0, the derivative of the instant
error for the considered transition parameter is obtained.

Averaging over all the examples, the model parameters can
be updated by gradient descent according to equation (4). The
new parameters can be used to compute a new score distri-
bution over the nodes xP
p . Function optimization and score
estimation can be iterated through multiple epochs, till a ter-
mination criterion is satisﬁed.
3.3 Learning the jump parameters

For a jump parameter w = x(ci|cj, J), equation (2) shows

that it holds that,

=

|ci|

q∈G
q:cq =cj

x(J|cj )(t)

∂xP
p (t + 1)
xP
∂x(ci|cj, J)(t)
q (t) if cp = ci
where |ci| indicates the number of nodes in the graph G with
label ci. The previous equation can be inserted into (9) and,
removing all temporal dependencies under the assumption of
model convergence and stationarity, yields the derivative of
the instant error with respect to a jump parameter.
3.4 Learning the surfer action bias

p (t+1)

Following a procedure similar to that shown for the transi-
tion and jump parameters, we start computing the derivative
∂xP
∂x(l|ci)(t) for a single parameter.
In particular, equation (2) shows that,
∂xP
p (t + 1)
∂x(l|ci)(t)

P
k∈ch(q) x(ck|ci, l)(t)

x(cp|ci, l)(t)

q −

· xP

( 0 if cp 6= ci
·P

= X
− X

q∈pa(p)
q:cq =ci

q∈G
q:cq =ci

x(cp|ci, J)(t)

|cp|

· xP

q (t) ,

(11)

where the surfer model is assumed to respect the probabilistic
constrains at every time step, i.e. x(J|ci)(t) = 1− x(l|ci)(t).
Merging equation (9) and (11) under the assumption of
model stationarity and convergence at time 0, yields the
derivative of the instant error with respect to a behavior pa-
rameter. All parameters can be simultaneously updated as
described in section 3.2, 3.3 and 3.4 to train the model. Af-
ter the updating, the jump and behavior parameters must be
normalized to respect the probabilistic constrains.

Finally, using an EM-style algorithm [Dempster et al.,
1977] the adapted surfer model can re-surf the Web graph
yielding a new estimate of the scores. The procedure can be
repeated till a stop criterion is satisﬁed. Since the parame-
ters are estimated from a few levels deep unfoldings, only a
small set of nodes must be considered at each iteration. The
main overhead of the proposed learning algorithm is the full
PageRank computation that must be completed at each step.
Our experiments show that usually only very few iterations
are needed to converge.

4 Experimental results

The experiments were performed on two datasets, which
were collected by focus crawling the Web [Diligenti et al.,
2000], on the topics “wine” and “Playstation”, respectively.
The “wine” dataset contains 1.000.000 documents, while the
“Playstation” one 500.000. The documents were clustered
using a hierarchical version of the k-means algorithm [Duda
and Hart, 1973]. In particular, in two different runs, we clus-
tered the pages from the dataset on topic “Playstation” into 25
sets, whereas the dataset on topic “wine” was clustered into
25 and 100 sets.

Two topic-speciﬁc search engines were built containing the
downloaded documents. The rank of each page in the dataset
was computed using a random walker producing the standard
PageRank scores. We tested the quality of the ranking func-
tion by submitting a set of queries to the search engine whose
results were sorted by the scores assigned by the random
surfer. By browsing the results, we found pages which were
authorities on the speciﬁc topic but were (incorrectly) not in-
serted in the top positions of the result list. Such pages were
selected as examples of pages which should get a higher rank
(positive examples). A target score equal to one (the maxi-
mum reachable score according to the model) was assigned
to the positive examples. On the other hand, we found pages
having a high rank which were not authorities for the topic.
Such pages were selected as negative examples and a target
score equal to zero was assigned to those pages. The learn-
ing algorithm, as previously described was applied to these
datasets, using the selected examples. Only a small set of ex-
amples (3-15) was typically labeled as positive or negative in
any single experiment.
4.1 Analysis of the effects of learning

This ﬁrst group of experiments aims at demonstrating that
the surfer model as learned by the proposed training algo-
rithm, could not be generated by a non-adaptive schema as
the focused versions of PageRank proposed in the literature.
In ﬁgure 2, we show the values of the transition, jump
and behavior parameters for each cluster, resulting from the

(a)

(b)

Figure 3: Plots of the normalized probability of the surfer of being
located in a page of a given cluster before and after learning. (a)
results on the crawl for the topic “wine” with 100 clusters. (b) results
on a random graph with 200.000 nodes and 25 clusters.

Figure 2: Values of the transition, jump and behavior parameters
after learning for the “Playstation” dataset. For each cluster (x axis),
all the n parameters to the other clusters are sequentially plotted.

training sessions for a surfer on the topic “Playstation”. The
learned parameters represent a complex interaction of the
resulting surfer with the page clusters where a few clus-
ters are strongly boosted and a few other strongly demoted.
This behavior (expecially targeted demotions) can not be ex-
pressed by the focused versions of PageRank [Richardson and
Domingos, 2002; Diligenti et al., 2002], which simply boost
all the parameters leading to the clusters of relevant pages
(according to the simplistic assumption that relevant pages
are more likely to point to other relevant pages). This demon-
strates that the trained surfer is able to exploit hierarchies of
topics to model the complex path leading to good/bad pages.
In ﬁgure 3-(a) and 3-(b), there are plotted the probabilities
that the surfer is located in a page of a speciﬁc cluster. In
particular we show the results for the dataset on topic “wine”
clustered into 100 groups and a random graph with 200.000
nodes clustered into 25 groups. The learning algorithm is able
to increase the likelihood of the random surfer to visit pages
belonging to a set of clusters, while decreasing its probability
of visiting pages belonging to other clusters.
Figure 4 shows the value of the cost function EP at each
iteration. Each plot reports the cost function when a speciﬁc
set of parameters is optimized during training. The behavior
parameters are less effective than the jump parameters in op-
timizing the surfer model. However, neither the behavior or
the jump parameters are as effective as the transition parame-
ters. As expected, when all parameters considered during the

Figure 4: Values of cost function as a function of the iteration
number. Each plot reports the cost function when a speciﬁc set of
parameters is considered during training.

learning process, the cost function reaches its minimum.

In ﬁgure 5, there are shown the scores of the pages of clus-
ter 8 of the dataset “Playstation” before and after learning.
This cluster was manually detected as strongly on the topic
“Playstation” and 3 pages belonging this cluster were explic-
itly inserted in learning set as pages that should get an higher
score. As expected the rank of the other pages in this cluster
was boosted up thanks to the capability of the algorithm to
generalize from a small number of training examples.

4.2 Qualitative results

This experiment compares the quality of the page ranks
before and after training.
In particular, some authoritative
pages on the topic “wine” were provided as positive exam-
ples, while some authoritative pages on other topics were pro-
vided as negative examples. Figure 6 reports the pages which
obtained the largest variation in their rank. The pages that ob-

0123456789101112131415161718192021222324252627282930Epoch00.10.20.30.40.50.60.70.80.91Cost functionTransitionBehaviorTransition and BehaviorJumpTransition and JumpBehavior and JumpAllwith many nodes. The learning algorithm was applied to the
problem of learning a generic ranking function on the portion
of the Web graph.

References
[Chang et al., 2000] H. Chang, D. Cohn, and A. K. McCal-
lum. Learning to create customized authority lists. In Pro-
ceedings of the 17th International Conference on Machine
Learning, pages 127–134. Morgan Kaufmann, 2000.

[Dempster et al., 1977] A.P. Dempster, N.M. Laird, and
D.B. Rubin. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical So-
ciety, 39:185–197, 1977.
[Diligenti et al., 2000] M.

Coetzee,
S. Lawrence, L. Giles, and M. Gori. Focus crawling
by context graphs.
In Proceedings of the 26th Interna-
tional Conference on Very Large Databases (VLDB 2000),
pages 527–534, 2000.

Diligenti,

F.

[Diligenti et al., 2002] M. Diligenti, M. Gori, and M. Mag-
gini. Web page scoring systems for horizontal and vertical
search. In Proceedings of the World Wide Web Conference
(WWW2002), pages 508–516, 2002.

[Diligenti et al., 2003] M. Diligenti, M. Gori, and M. Mag-
gini. A learning algorithm for web page scoring systems.
In Proceedings of International Joint Conference on Arti-
ﬁcial Intelligence (IJCAI2003), pages 575–580, 2003.

[Duda and Hart, 1973] R. O. Duda and P. E. Hart. Pattern
Classiﬁcation and Scene Analysis. John Wiley and Sons,
1973.

[Haykin, 1994] S. Haykin. Neural networks. Macmillan Col-

lege Publishing Company, 1994.

[Kondor and Lafferty, 2002] R. I. Kondor and J. Lafferty.
Diffusion kernels on graphs and other discrete input
spaces.
In Proceedings of the International Conference
on Machine Learning (ICML2002), pages 315–322, 2002.
[Page et al., 1998] L. Page, S. Brin, R. Motwani, and
T. Winograd. The PageRank citation ranking: Bringing
order to the web. Technical report, Computer Science De-
partment, Stanford University, 1998.
[Richardson and Domingos, 2002] M.

and
P. Domingos. The intelligent surfer: Probabilistic combi-
nation of link and content information in PageRank.
In
Advances in Neural Information Processing Systems 14,
pages 1441–1448, Cambridge, MA, 2002.

Richardson

[Tsoi et al., 2003] A. C. Tsoi, G. Morini, F. Scarselli, M. Ha-
genbuchner, and M. Maggini. Adaptive ranking of web
pages.
In Proceedings of the 12th International World
Wide Web Conference (WWW12), pages 356–365, 2003.

Figure 5:
The scores of the pages of cluster 8 of the dataset
“Playstation” before and after learning. In the learning process only
3 pages belonging to the cluster 8 were explicitly inserted in learning
set as pages that should get an higher score.

Most descending pages on topic “wine”
http://www.macromedia.com/go/getﬂashplayer
http://www.netscape.com
http://www.macromedia.com/downloads
http://www.yahoo.com
http://www.adobe.com/products/acrobat/readstep.html
http://help.yahoo.com
http://www.nl.placestostay.com/index.html
http://home.netscape.com
http://www.forgottensoldier.com
http://www.inntravels.com

Most ascending pages on topic “wine”
http://www.winecountry.com
http://webwinery.com
http://www.ny-wine.com/wineinfo.asp
http://www.wine-searcher.com
http://www.insidewine.com
http://webwinery.com
http://www.viniesaporidipuglia.com/en/index.html
http://www.vinitaly.com/home en.asp
http://www.wineinstitute.org
http://www.winespectator.com

Figure 6: URLs of the pages that yielded either the largest negative
or positive rank changes among the pages that were initially in the
top 1000 scoring documents.

tained a negative rank variation were either topic-generic au-
thoritative pages (e.g. www.netscape.com) or pages relevant
for different topics than “wine” (e.g. “www.forgottensoldier-
.org”). Among the pages that got a negative rank varia-
tion, only the page “www.yahoo.com” was speciﬁed as a
negative example. On the other hand, the pages that ob-
tained a positive rank variation were effectively authoritative
pages on the topic “wine” (e.g. “www.wine-searcher.com” or
“webwinery.com”). Among the most ascending documents,
only the page “www.winespectator.com” was explicitly pro-
vided to the system as a positive example, whereas other
pages were pushed up thanks to the capability of the system
to generalize from a small set of training examples.

5 Conclusions

In this paper we introduced a novel algorithm to learn a
score distribution over the nodes of a labeled graph. The
learning algorithm adapts the parameters of a random surfer
in order to match the target scores assigned on a small sub-
set of the nodes. Since the parameters are estimated from the
unfoldings of the graph centered on the example nodes, the
learning algorithm is efﬁcient and can be applied to graphs


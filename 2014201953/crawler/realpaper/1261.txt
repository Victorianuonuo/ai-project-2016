EXPERIMENTS  WITH  A  SEARCH  ALGORITHM FOR  THE  DATA  BASE 

OF  A  HUMAN  BELIEF  STRUCTURE 

Kenneth  Mark  Colby-
Lawrence  Tesler 

Horace  Enea 

Summary 

A  large  data  base  was  collected  from  a  human 
informant. 
The  data  consisted  of  b e l i e fs  regard(cid:173)
ing  p a r e n t - c h i ld  r e l a t i o n s.  A  v a r i e ty  of  factors 
in  searching  the  data  base  were  manipulated  in  an 
attempt  to  discover  which  were  the  more 
important 
in  c o n t r i b u t i ng  to  estimates  of  c r e d i b i l i t y. 
Problems  of  data  c o l l e c t i o n,  data  representation 
and  a  searching  algorithm  are  discussed  in  d e t a i l. 

I n t r o d u c t i on 

It 

is  clear  that  people  hold  b e l i e f s.  What 

is  not  so  clear  is  how  these  b e l i e fs  are  processed 
to  judge  the  c r e d i b i l i ty  of  an  input  p r o p o s i t i o n. 
As  an  aid  in  understanding  c r e d i b i l i ty  processes, 
we  constructed  a  computer  model  which  was  intended 
to  simulate 
human  informant.  We  s h a ll  begin  w i th  a  c l a r i f i c a(cid:173)
t i on  of  terminology  used 
in  describing  the  model. 

the  b e l i ef  processes  of  a  p a r t i c u l ar 

Terminology 

A  model  consists  of  a  set  of  i n t e r a c t i ng  com(cid:173)
ponents. 
The  major  components  of  the  model  to  be 
described  are  state-descriptions  (data  base),  pro(cid:173)
cess-descriptions  (procedures)  and  an  i n t e r p r e t er 
of  process-
whose  l o g ic  governs  the  applications 
descriptions 
in  accordance 
w i th  the  aim  or  task  of  judging  c r e d i b i l i t y. 
the  model  is  two  l e v e l l ed  w i th  the  i n t e r p r e t er  at 
the  top  supervising  i n t e r a c t i o ns  between  procedures 
and  data  base. 

to  state-descriptions 

Thus 

The  data  base  is  made  up  of  a  conceptual  graph 

whose  basic  structures  consists  of  conceptualiza(cid:173)
tions 
in  t u rn  composed  of  elementary  conceptions. 
Conceptualizations 
in  the  model  are  held  proposi(cid:173)
tions  which  symbolically  represent  states  of  a f f(cid:173)
a i rs  or  s i t u a t i o n s.  Conceptualizations  can  be 
represented  in  both  n a t u r al  and  computer  languages 
by  an  ordered  set  of  name-tokens.  Thus  a  given  con(cid:173)
c e p t u a l i z a t i on  might  be  described  in  English  as 
' B i ll  l i k es  Mary'  and  in  a  programming  language  as 
the  l i st 
(object  Mary)). 
The  p a r t i c u l ar  conceptualizations  we  focussed  on 
were  those  which  semantically  involved  c e r t a in 
r e l a t i o ns  between  humans.  Since  people  conceptual(cid:173)
ize  t h e ir  experience  w i th  persons 
in  terms  of  human 
a c t i o n, 
the  elementary  conceptions  of  the  data  base 
involve  agents,  a c t i o n s,  objects  and  ( o p t i o n a l l y) 
s e t t i n g s,  modalities  and  r a t i o n a l e s. 

((agent  B i l l) 

(action  l i k e) 

The  term 

in  human  b e l i ef  structures 
refers  to  (a)  an  a f f e c t i ve  a t t i t u de  of  acceptance, 
r e j e c t i on  or  n e u t r al  judgment  towards, 
(b)  a  held 

fb e l i e f' 

'True'  here  means 

conceptualization.  Each  conceptualization  held  or 
prehended  by  a  b e l i ef  structure 
is  either  accepted 
to  some  degree  as  t r u e,  rejected  to  some  degree  as 
f a l se  or  held  in  suspension  as  a  n e u t r al  candidate 
f or  b e l i e f. 
that  the  s i t u a t i on 
conceptualized  is  accepted  as  being  c e r t a i n l y, 
' f a l s e' 
probably,  or  possibly  being  the  case  while 
f or  the  opposite  of  these  three  modal  quan(cid:173)
stands 
important  to  note  that  the  a t t i t u de 
t i f i e r s. 
is 
i n c r e d i b i l i ty  is  towards  a  concep(cid:173)
of  r e j e c t i on  or 
t u a l i z a t i on  prehended  w i t h in  a  s t r u c t u r e.  A  d i s(cid:173)
believed  conceptualization  is  not  expelled  from  the 
structure  but  is  prehended  w i th  an  a t t i t u de  of 
r e j e c t i o n.  A  conception  is  thus  judged  to  be 
c r e d i b l e, 
incredible  or  somewhere  in  between. 

It 

We  postulate  c r e d i b i l i ty  to  be  a  function  of 
foundation  and  consistency.  The  foundation  of  a 
given  b e l i ef  is  a  measure  of  those  b e l i e fs  which 
imply  it  as  opposed  to  i ts  negation.  Consistency 
refers  to  a  degree  of  consonance  and  dissonance 
found  in  those  b e l i e fs  a  given  b e l i ef  implies. 
The  term 
t i on  but  to  psychological  i m p l i c a t i on  which  involves 
rules  of  expectancy.  We  also  assume  a  weight  which 
determines  the  r e l a t i ve 
consistency  have  f or  one  another  in  a  p a r t i c u l ar 
domain  of  i n t e r e s t. 

'imply'  does  not  r e f er  to  l o g i c al  implica(cid:173)

importance  foundation  and 

Conceptualizations  w i th  t h e ir  associated  cred-

in 

' if  x  l i k es  y  then  x  helps  y', 

The  components  of  t h is  r e l a t i on 

b i l i t i es  make  up  one  component  of  the  data  base. 
We  term  these  conceptualizations 
' f a c t s'  since 
they  stand  f or  that  which  is  or  is  not  the  case 
the  s t r u c t u r e.  A  second  component  of  the  data  base 
consists  of  r u l e s.  By  the  term  ' r u l e'  we  mean  a 
connectivity  r e l a t i on  holding  between  two  or  more 
conceptualizations. 
contains  variables  as  w e ll  as  name-tokens.  Hence, 
a  rule  might  read 
where  both  x  and  y  are  variables  to  which  the  name-
tokens  of  persons  can  be  bound.  As  mentioned,  t h is 
i f - t h en  r e l a t i on  represents  a  type  of  psychological 
i m p l i c a t i o n.  Our  i n t e r p r e t a t i on  of  psychological 
i m p l i c a t i on  is  that  given  conceptualization  A  to  be 
the  case,  conceptualization  B  is  expected  to  be  the 
case  by  the  s t r u c t u r e.  For  example,  humans  commonly 
expect  that 
f i r st  person  w i ll  help  the  second  person.  Such 
general  expectancy  rules  allow  a  v a r i e ty  of 
i n f e r(cid:173)
ence  processes  to  be  c a r r i ed  out.  A  f u r t h er  d i s(cid:173)
cussion  of  psychological  i m p l i c a t i on  can  be  found 
in  Abelson2. 

if  a  person  l i k es  another  person, 

the 

The  remaining  components  of  the  data  base  con(cid:173)

s i st  of  d e f i n i t i o ns  and  c l a s s i f i c a t i o n s. 

For 

-649-

example,  the  name-token  'love1  is  defined  as  sim-
i l a r - to  and  stronger-then  ' l i k e '. 
definable  as  negation-of  ' l i k e '.  These  definitions 
are  used  in  finding  similarities  and  contrasts  be(cid:173)
tween  conceptualizations.  Classifications  consist 
of  set  memberships  and  set  inclusions. 

'Dislike1  is 

In  summary,  the  data  base  represents  informa(cid:173)
tion  in  the  form  of  various  kinds  of  state  descrip(cid:173)
tions.  When  the  model  runs,  this  information  is 
subjected  to  procedures 
governed  by  a  top-level  interpreter.  The  procedures 
are  called  into  operation  by  the  interpreter  in 
accordance  with  the  task  involved.  The  main  task  we 
were  interested  in  involves  estimating  the  credibil(cid:173)
i ty  of  a  given  proposition  describing  some  actual  or 
hypothetical  situations.  Given  such  a  proposition, 
how  might  a  person  judge  i ts  credibility  using  the 
information-processing  capacities  he  has  available? 

We  approached  this  problem  by  selecting  an 

informant,  collecting  certain  beliefs,  and  represent(cid:173)
ing  them  in  a  data  base.  We  planned  f i r st  to  con(cid:173)
duct  certain  information-processing  experiments  on 
the  data  base  and  second  to  attempt  a  validation  of 
the  simulation.  This  report  w i ll  be  concerned  with 
the  f i r st  phase  of  experimentation. 

Data  Collection 

To  find  an  informant  for  this  research,  we 

advertised  in  a  college  newspaper  for  persons  who 
might  be  interested.  Out  of  65  applicants,  we 
interviewed  26  and  then  selected  a  30  year  old 
married  woman  on  grounds  that  she  was  intelligent, 
articulate,  interested  in  the  research  and  serious 
of  purpose.* 

Several  times  a  week  she  would  write  down  in 

natural  language  beliefs  which  occurred  to  her 
about  events  in  her  l i f e.  Each  week  we  would  t ry 
to  reduce  these  natural  language  statements  to  a 
simpler  form  suitable  for  the  model's  data  base  and 
processing.  At  intervals  we  would  show  the  data 
base  to  the  informant  for  her  corroboration  or 
correction  of  our  paraphrasings. 
I n i t i a l ly  we 
planned  to  obtain  her  beliefs  regarding  a ll  the 
important  people  in  her  l i fe  space.  Preliminary 
experience  showed  that  while  collection  of  such 
data  is  possible,  the  labor  required  to  organize 
and  represent  this  amount  of  data  in  a  computer 
model  makes  the  task  extremely  d i f f i c u lt  with 
currently  available  methods. 

We  found  two  main  disadvantages  to  this  method 

of  data-collection.  F i r s t,  it  is  cumbersome  and 
time-consuming,  requiring  hundreds  of  man-hours  to 
obtain  a  data  base  of  700  facts,  rules,  definitions 
and  classifications.  A  better  method  should  be 
developed  whereby  an  informant  could  type  informa(cid:173)
tion  directly  into  a  data-base  by  means  of  a  man-
machine  dialogue.  This  input  might  be  in  an  a r t i f i(cid:173)
c i al  and  simplified  language  which  an  informant 

*For  obvious  reasons  we  cannot  give  her  name,  but 
we  would  l i ke  to  use  this  footnote  to  express  our 
gratitude  for  her  help. 

could  learn.  However  an  a r t i f i c i al  language  is 
distracting  and  constrains  expressiveness. 
be  better  to  allow  the  rich  freedom  of  natural  lang(cid:173)
uage  but  there  are  great  problems  involved  in  the 
machine  handling  of  this  sort  of  input.  We  have  had 
some  experience  along  these  lines3  and  we  are  con(cid:173)
tinuing  an  attempt  to  make  a  conceptual  parsing  of 
natural  language  sentences  in  order  to  translate  them 
into  conceptualizations  suitable  for  the  data  base  of 
a  belief  structure  • 

It  would 

A  second  drawback  to  our  i n i t i al  data  collection 

method  involved  the  problem  of  extensiveness  versus 
intensiveness.  An  extensive  data  base  is  one  in 
which  there  are  a  great  variety  of  conceptualizations 
but  not  a  great  number  around  any  one  theme.  While 
containing  a  large  amount  of  information,  this  type 
of  data  is  too  sparse  to  permit  the  model  to  come  to 
very  many  conclusions  regarding  credibility.  Few 
beliefs  of  relevance  can  be  found  for  a  given  propo(cid:173)
sition  unless  it  is  of  a  very  general  and  hence 
t r i v i al  nature. 

We  then  attempted  to  concentrate  on  a  particular 

theme  in  order  to  make  the  data  base  dense  around 
selected  conceptions.  Since  our  informant  was  the 
mother  of  a  three  year  old  child  and  interested  in 
the  problem  of  child  raising,  we  concentrated  on  her 
beliefs  in  this  area.  For  each  of  her  beliefs  in 
this  domain,  we  obtained  a  weighting  of  a  degree  of 
credibility.  We  used  crude  categories  of  strong, 
medium  and  weak  for  these  weightings.  To  obtain 
data  rules  we  would  ask  the  informant  for  reasons 
for  each  belief.  For  example,  if  a  belief  were  'a 
child  ought  not  h it  another  child'  and  the  reason 
given  by  the  informant  was  'because  if  a  child  hits 
another  child  then  the  second  child  gets  hurt',  a 
general  if-then  expectation  rule  can  be  constructed 
about  the  relation  between  hitting  and  hurting. 

If  a  person's 

One  d i f f i c u l ty  to  be  anticipated  in  simulating 
a  human  belief  structure  involves  keeping  the  model 
updated  along  with  the  informant. 
beliefs  are  continually  changing,  one  cannot  keep  a 
model  in  close  enough  correspondence  to  test  out 
comparisons  between  the  person's  and  the  model's 
performances  in  estimating  credibility.  However,  we 
found  that  with  our  informant,  these  particular 
beliefs  about  parent-child  behavior  changed  very 
l i t t le  over  a  period  of  several  months. 
of  only  two  beliefs  did  she  change  a  credibility 
weight  from  strong  to  medium.  Hence  the  structure 
appeared  quite  stable  over  this  period  of  time. 
It 
should  also  be  remarked  that  there  occurred  no  major 
environmental  event  in  this  domain  of  interest  which 
might  be  expected  to  have  great  impact  on  a  belief 
structure. 

In  the  case 

We  attempted  to  model  the  credibility  processes 

of  a  single  individual.  This  approach  is  in  the 
research  tradition  of  an  intensive  design  in  contrast 
to  an  extensive  design.  An  extensive  design  might 
make  one  observation  on  1000  persons  while  an  inten(cid:173)
sive  design  would  make  1000  observations  on  one 
person.  Both  designs  attempt  to  account  for  varia(cid:173)
tion  in  the  phenomena  observed. 
design,  the  unit  of  v a r i a b i l i ty  is  an  individual  and 
variation  between  individuals  is  studied  whereas  in 

In  an  extensive 

-650-

This 

i n q u i r y. 

involved 

The  c r i t e r ia  f or 

to  discover  how  general  the 

an  intensive  design  we  are  studying  the  v a r i a b i l i ty 
w i t h in  an  i n d i v i d u a l. 
In  modelling  a  single  case  we 
are  t r y i ng  to  understand  the  mechanisms 
in  i n t r a - i n d i v i d u al  processes.  An  intensive  design 
attempts  to  show  what  can  and  does  happen.  The 
frequency  of  t h is  sort  of  happening  in  a  population 
and  which  population  is  another  matter.  After  l e a r n(cid:173)
ing  how  to  model  one  person  we  can  model  another  and 
so  b u i ld  up  a  series  of  cases.  The  inductive  problem 
of  generalizing  then  becomes  one  of  sampling  and  of 
s t a t i s t i c al  measures 
informant's  b e l i e fs  might  be  in  a  population. 
was  not  our  problem  at  t h is  state  of  the 
Our  problem  was  how  to  construct  a  good  model  of  the 
informant's  b e l i ef  processes. 
'good'  can  be  v a r i e d.  And  are  we  g e t t i ng  at  what 
the  informant 
means  here  is  obscure  but 
that  people  have 
b e l i e fs  at  a  given  moment.  Even  worse, 
they  have 
the  capacity  to  deceive  themselves  to  r a t i o n a l i z e, 
and  to  d i s t o rt  t h e ir  own  b e l i e f s.  Over  time  we 
hoped  to  increase  a c c e s s i b i l i t y,  r e a l i z i ng  there  are 
always  l i m i t s. 
' r e a l l y' 
believed  we  found  it  useful  to  keep  in  mind  that  we 
were  constructing  a  model  of  a  model.  A  b e l i ef  struc(cid:173)
ture  is  a  representation  and  in  giving  information 
about  himself,  an  informant  t e l ls  us  what  he  believes 
he  believes.  He  simulates  himself  and  it  is  his 
accessible  model  of  himself  that  becomes  the  data 
base  of  a  computer  model.  Humans'  a b i l i ty  to 
simulate  themselves  and  to  make  models  of  other 
models 
is  of  course  a  most 
a  symbolic  system  to  have. 

l i m i t ed  a c c e s s i b i l i ty  to  t h e ir 

In  worrying  about  what  is 

' r e a l l y'  believes?  What 

i n t e r e s t i ng  property  f or 

is  common  knowledge 

' r e a l l y' 

it 

Data  Representation 

In  b u i l d i ng  a  data  base  f or  the  model  we 

thought  of  the  c o l l e c t ed  f a c t s,  r u l e s,  d e f i n i t i o ns 
and  c l a s s i f i c a t i o ns  as  a  graph.  Physically  in  the 
model  they  were  l i s ts 
in  the  programming  language 
MLISP5 , 6.  M L I SP  is  a  high  l e v el  l i st  processing 
language  which  t r a n s l a te  ALGOL-like  meta-statements 
(M-expressions) 
expressions)  of  LISP  1.5.  The  program  runs  on  the 
PDP-6/10  time-sharing  system  of  the  Stanford  A r t i(cid:173)
f i c i al 

i n to  the  symbolic  statements 

I n t e l l i g e n ce  P r o j e c t. 

(S-

'Terminology', 

Each  conceptualization  was  represented  as  a 
l i st  of  elements  consisting  of  E n g l i s h - l i ke  name 
tokens  or  atoms,  of  l i s ts  of  atoms,  and  of  l i s ts  of 
l i s ts  which  contained  semantic  and  numerical  i n f o r-
mationt  As  mention  in  the  section 
the  conceptualizations  r e f l e ct  a  human  action 
interpersonal  world. 
model  of  s i t u a t i o ns 
From  t h is  perspective,  agents  carry  out  actions 
towards  objects  which  in  t u rn  can  be  agents  or  other 
In  the  data  base  each  agent,  a c t i o n, 
s i t u a t i o n s. 
object,  e t c ., 
is  i d e n t i f i ed  by  an  atom.  On  the 
to 
property  l i st  of  each  atom  is  a  l i st  of  pointers 
a ll  conceptions 
in  the  data  base  in  which  t h at  par(cid:173)
t i c u l ar  atom  occurs.  A  hash  coding  scheme  is  used 
f or  r a p id  look-ups  and  r e t r i e v al  of  relevant  con(cid:173)
ceptions. 

in  the 

(action  l i k e) 

(object  children) 

(F(agent  Barb) 

i n d i c a t i ng 

For  example, 

the  n a t u r al 
'parents  spank  c h i l d r e n'  has  a 

( c r e d i b i l i ty  0.9))  w i th  the  symbol  F 
t h is  is  a  f a c t.  More  complicated  is  the  representa(cid:173)
t i on  of  a  rule  because  of  the  problem  of  binding 
variables  unambiguously. 
language  statement 
number  of  possible  semantic 
necessary  to  check  c a r e f u l ly  w i th  the 
informant 
order  to  be  clear  about  which  i n t e r p r e t a t i on  she 
intended. 
'parents'  spank  the  set 
each  member  of  the  set 
the  set 
children'  she  meant  that  a  given  parent  spanks  his 
c h i l d r e n.  More  f o r m a l l y, 
if  x  is  a  parent  and  y  is 
a  c h i ld  and  x  is  a  parent  of  y, 

' c h i l d r e n'  nor  d id  she  mean 
'parents'  spank  each  member  of 
' c h i l d r e n '.  By  the  expression  'parents  spank 

In  t h is  case  she  did  not  mean  that  the  set 

then  x  spanks  y. 

i n t e r p r e t a t i o n s. 

It  was 

in 

The  r e l a t i on 

' is  a  parent  of  must 

f i r st  be 

the  variable  P  . is  defined  as  a 

defined  in  terms  of  certain  constrained  variables. 
For  instance, 
parent  who  is  a  parent  of  ~ 
a  c h i ld  who  is  a  c h i ld  of  
made  to  such  variables  only  those  name  tokens  which 
q u a l i fy  can  be  substituted. 
•parent  l i k es  his  c h i l d, 
would  be  represented  as  the  l i s t: 

' if  a 
then  his  c h i ld  is  happy' 

Thus  the  rule 

is  defined  as 

When  assignments  are 

and 

the  fact 

i n d i c a t i ng  that  it  is  an  i m p l i(cid:173)
w i th  the  symbol  R 
c a t i o n s!  r u l e.  When  facts  are  search  to  match  the 
components  of  a  r u l e, 
'John  l i k es  Mary' 
would  f it  t h is  r u le  only  if  John  is  held  to  be  a 
'John'  can  be  substituted 
parent  of  Mary  so  that 
f or  
f or 
These   constrained 
variables  are  global  in  the  program. 
the  binding  of  variables  to  be  unambiguous  and  allow 
rules  to  be  a r b i t r a r i ly  complex  since  the  q u a l i f i c a(cid:173)
tions  required  for  the  variable  may  involve  m u l t i(cid:173)
ple  conditions. 

They  permit 

'Mary' 

and 

Representation  of  d e f i n i t i o ns 

is 

in  the  form  of 

a  simple  l i s t.  To  indicate  conceptual  r e l a t i o ns 
between 

' l i k e ',  the  l i st  appears  as 

' l o v e'  and 

(love  similar  l i ke  S) 

where  the  symbol  S 
i n t e n s i ty  than 
tions  represented  in  d e f i n i t i o n s,  S  meaning 
stronger,  W  weaker,  and  E  equal. 

is  stronger  in 
' l i k e '.  The  following  are  some  r e l a(cid:173)

indicates 

' l o v e' 

s i m i l ar 
d i f f e r e nt 
negative 
opposite 
kindof 

(love  s i m i l ar  l i ke  S) 
(men  d i f f e r e nt  women) 
( n o t l i ke  negative  l i ke  E) 
(love  opposite  hate  E) 
(spank  kindof  aggression) 

C l a s s i f i c a t i o ns  take  the  simple  form, 

The  representation  of  a  f a ct  such  as 

c h i l d r e n1  appears  on  the  l i s t: 

'Barb  l i k es 

(F(agent  matches) 

(action  is  a) 

(object  t h i n g s )) 

I n i t i al  experience  w i th  a  data  base  of  700  f a c t s, 

r u l e s,  d e f i n i t i o ns  and  c l a s s i f i c a t i o ns  not  only 

-651-

is 

in  the  data  as  w e ll  as  to 

taught  us  about  the  density  requirements  of  data 
but  also  brought  to  l i g ht  an  implementation  problem. 
When  output  from  a  running  model  is  not  s a t i s f a c t o r y, 
it  may  be  due  to  errors 
inadequacies  of  the  procedures.■  A  small  data  e r r o r, 
(such  as  the  term  ' n o t l i k e '.  in  a  conceptualization 
instead  of 
' l i k e ' ),  o r i g i n a t i ng  from  human  mistakes 
in  i n p u t t i ng  data  i n to  the  data  base,  can  give  r i se 
to  an  i n c o r r e ct  c r e d i b i l i ty  estimate.  When  a  data 
base 
trace  e n t i r e ly  by  hand  what  happened  in  a  given  run 
of  the  model.  We  t r i ed  f r e q u e n t ly  to  check  the  data 
f or  errors  and  the  informant  repeatedly  studied  a 
l i s t i ng  of  the  data-base  searching  f or  mistakes. 
In 
s p i te  of  t h is  labor  of  s c r u t i n y,  disconcerting  data 
errors  would  s t i ll  crop  up.  To  make  sure  the  pro­
cedures  were  operating  as  p o s t u l a t e d,  we  f i r st 
selected  a  very  r e s t r i c t ed  subset  of  the  data  base 
and  then  gradually  added  to  it  as  the  program  be­
came  debugged. 

it  becomes  extremely  d i f f i c u lt  to 

l a r g e, 

Procedures 

The  modelling  program  scales  variables  such  as 

c r e d i b i l i t y, 
range  0  to  100. 
is  as  f o l l o w s: 

foundation,  and  consistency  i n to  the 

The  i n t e r p r e t a t i on  of  these  numbers 

90-100 
6O-89 
41-59 
11-40 
0-10 

Strongly  p o s i t i ve 
Weakly  to  moderately  p o s i t i ve 
Undecided 
Moderately  to  weakly  negative 
Strongly  negative. 

C r e d i b i l i ty  is  a  f u n c t i on  of  two  components: 
dation  and  consistency. 

foun-

The  foundation  of  a  p r o p o s i t i on  is  a  measure  of 
the  model's  evidence  f or  and  against  the  p r o p o s i t i o n. 
If  the  p o s i t i ve  evidence  outweighs 
then  the  foundation  is  h i g h; 
dence 

the  negative, 
if  the  negative  e v i­

foundation  is  low. 

is  stronger, 

the 

C r e d i b i l i ty  is  a  f u n c t i on  mainly  of  foundation. 

Thus, 

When  foundation  is  moderate,  consistency  has  more 
influence  on  c r e d i b i l i ty  than  when  foundation  is 
extreme. 
s i t i on  is  not  dominantly  pro  or  con, 
gives  e x t ra  weight 
i ts  c r e d i b i l i t y. 
incorporates 

if  the  evidence  concerning  a  propo­
then  the  model 
to  i ts  consistency  in  determining 

A  formula  f or  c r e d i b i l i ty  which 
factor 

is  given  below. 

t h is 

c r e d i b i l i ty  =  foundation  +  (consistency-50)  x 
(50  - | f o u n d a t i o n - 5 0 |)  x  weight 

the  r e l a t i ve 

The  "weight"  is  a  number  between  0  and  0.02  which 
importance  of  consistency  in 
indicates 
t h is  computation. 
the 
weight 
0.02. 

i r r e l e v a n t, 
is 

If  consistency  is 

is  dominant, 

the  weight 

is  zero. 

If  it 

EXAMPLES 

CREDIBILITY  as  a  f u n c t i on  of  Foundation  and  Consis­
tency 

Consistency 

100 

80 

50 

20 

0 

Foundation 

80 
50 
20 

90 
75 
30 

86 
65 
26 

80 
50 
20 

74 
35 
14 

70 
25 
10 

The  consistency  of  a  p r o p o s i t i on  P 

is  com­
puted  by  f i n d i ng  a  few  h i g h ly  relevant  b e l i e fs  and 
measuring  the  consonance  of  P  w i th  these  b e l i e f s. 
is  defined  o b j e c t i v e l y . If  P  is  a  pro­
Relevance 
p o s i t i on  of  the  predicate 
and  if  there 
is  a  r u le  in  the  model  t h at  says: 

form 

f (p) 

if 

f ( x) 

then  g(x) 

and  if  g(p) 
Qi, 
then  Qi 
model  already  disbelieves  Qi.  — 
-1  Qi.  —  then  P 
model  neither  believes  or  disbelieves  Qi. 
consistency  of  P  is  not  a f f e c t e d. 

is  the  predicate  form  of  a  b e l i ef 
is  h i g h ly  relevant  to  P. 
If  the 
or  believes 
If  the 
then  the 

is  dissonant  w i th  i t. 

The  computation  of  consistancy  consists  of 
determining  the  percentage  of  Qi.'s  w i th  which  P 
is  consonant.  More  weight 
is  given  to  consonance 
w i th  more  credible  Qi.'s 
less  credible  Qi.'s.  A  set  of  formulas  that  i n c o r­
porate  t h is  weighting 

than  to  consonance  w i th 

is  given  below. 

sc  =  Z  c r e d i b i l i ty  (Qi.),  where  P  Qi.  and 

c r e d i b i l i ty 

(Qi)  >  50 

cc= 

count  of  Qi's  c o n t r i b u t i ng  to  sc 

cn  = 

count  of  Qi  's  where  P  o  -1  Qi.  and 

c r e d i b i l i ty 

consistency  = 

(qi)  >  50 
sc  +  1 

cc  +  en  +  .02 

The  foundation  of  a  p r o p o s i t i on  P 

is  computed 

is  or  is  not  the  case. 

by  f i n d i ng  relevant  b e l i e fs  and  seeing  whether  they 
imply  t h at  P 
In  the  search 
f or  relevant  b e l i e f s,  graph  paths  through  b e l i e fs 
consonant  w i th  P  are  searched  harder  than  paths 
through  b e l i e fs  dissonant  w i th  P 
seems  to 
be  h i g h ly  consistent. 
The  reverse  strategy  is  used 
This  is  done  so 
if  P 
t h at  model  can  attempt  to  l i m it 
i ts  search  f or  e v i­
dence  in  such  a  way  as  to  maintain  the  consistency 
of 

i ts  e n t i re  b e l i ef  s t r u c t u r e. 

seems  to  be  i n c o n s i s t e n t. 

if  P 

Formulas 

f or  foundation  in  terms  of  evidence 

f or  and  against  P  are: 

-652-

The  paths  with  highest  proportions  are  searched 
f i r st  and  receive  a  proportionately  greater  work 
allotment• 

If  not  a ll  the  work  along  a  path  is  exhausted, 
the  remainder  is  divided  among  the  remaining  paths. 
I f,  after  searching  any  path,  enough  relevant  beliefs 
have  been  found  to  compute  a  credibility exceeding 
60  or  below  4O,  then  the  search  of  the  rest  of  the 
paths  is  cancelled. 

Experiments 

The  program  performs  two  major  experiments. 

The  f i r st  experiment  assumes  that  the  belief  struc(cid:173)
ture  is  unchanging.  One  proposition  at  a  time  is 
presented  to  the  structure  and  its  credibility  is 
judged. 
ture  does  change.  After  each  proposition's  credi(cid:173)
b i l i ty  has  been  evaluated,  it  becomes  incorporated 
into  the  structure  as  a  belief. 

In  the  second  experiment,  the  belief  struc(cid:173)

The  f i r st  experiment  is  run  by  presenting  each 
belief  in  the  structure  to  a ll  the  other  beliefs  and 
judging  i ts  credibility.  The  result  can  be  compared 
with  the  prestipulated  credibility  of  the  belief. 
Then,  a  l i st  of  new  propositions  is  presented  to  the 
structure  for  evaluation. 
In  both  experiments,  many 
factors  of  the  evaluation  are  varied. 

One  factor  to  vary  is  the  means  of  finding  rele(cid:173)

vant  beliefs.  There  are  four  variations: 

1)  Use  only  rules  -  no  definitions. 

2)  Use  (l)  plus  rules  to  find  supersets. 

3)  Use  (l)  and  (2)  plus  "similar"  and 

"opposite"  rules. 

4)  Use  ( l ),  (2),  and  (3)  plus  rules  to  find 

instances. 

Another  factor  to  vary  is  the  use  of  consistency. 

There  are  two  variations: 

1)  Use  foundation  and  not  consistency. 

2)  Use  also  consistency. 

Other  factors  varied  are: 

The  search  for  relevant  beliefs  is  controlled 
by  a  "work"  factor.  A  consistency  search  w i ll  do, 
say,  200  units  of  work  while  a  foundation  search 
w i ll  do,  say,  1000  units  of  work.  This  work  allot(cid:173)
ment  is  apportioned  among  the  possible  graph  paths 
that  lead  from  the  proposition  in  question  to 
relevant  areas  of  the  graph. 

The  algorithm  for  searching  is  as  follows. 
The  directly  relevant  beliefs  in  the  graph  are 
found.  A  directly  relevant  belief  is  one  which  can 
be  derived  from  P 
in  one  step  by  any  one  of  these 
methods: 

1)  Replace  the  verb  by  a  similar  (or  opposite) 

verb. 

2)  Replace  the  subject  or  object  by  an  analogous 

(or  complementary)  noun. 

3)  Replace  the  predicate  adjective  by  a  similar 

(or  opposite)  adjective. 

4)  Generate  a  belief  which  implies  P  (or  P 

implies)  according  to  any  one  rule. 

These  beliefs  are  the  heads  of  paths  to  be 

searched.  A  certain  amount  of  work  is  used  up  just 
in  finding  them;  say,  2  units  for  each  relevant 
rule  used,  2  for  each  step  of  an  analogy  that  is 
drawn,  and  3  for  each  similar  verb  that  is  found, 
plus  6  units  overhead  even  if  nothing  relevant  is 
found. 

If  there  is  any  work  that  remains  unused  after 
finding  the  heads  of  these  paths,  it  is  divided  up 
among  the  paths  for  further  searching. 

In  the  consistency  search,  a ll  paths  receive 

equal  treatment.  However,  in  the  foundation  search, 
the  division  among  paths  is  affected  by  consistency. 
To  compute  the  consistency  of  P  with  these  paths, 
a  recursive  short-depth  search  is  performed  along 
each  path;  these  searches  are  alotted,  say,  l/3  of 
the  remaining  work.  From  the  resulting  consistencies, 
proportions  are  computed  according  to  the  following 
formulas: 

eg  =  consistency  of  P  with  whole  system 

cp  =  consistency  of  P  with  this  path 

1)  Weight  of  consistency  relative  to  founda(cid:173)

tion  in  computing  credibility. 

2) 

3) 

Amount  of  work  expended  in  search. 

The  i n i t i al  credibilities  assigned  each 
belief. 

A ll  values  of  these  factors  are  combined  with 

every  meaningful  combination  of  other  factors. 

-653-

Experimental  Results 

For  the  data  base  used  in  the  experiments  so 

far,  a  few  interesting  results  were  obtained. 

The  search  for  relevant  beliefs  was  effective 
when  both  rules  and  supersets  were  utilized  in  the 
search.  Without  supersets,  many  relevant  beliefs 
were  missed.  The  addition  of  similar  and  opposite 
rules  expanded  the  search  enough  to  discover  nearly 
a ll  beliefs  considered  relevant  by  the  experimenters. 
In  only  a  handful  of  cases  did  the  application  of 
instance  rules  improve  the  relevance  search. 

The  use  or  disuse  of  consistency  made  no  notice-

able  difference  in  the  c r e d i b i l i ty  computation. 
It 
is  planned  to  see  whether  consistency  w i ll  make  a 
difference  with  different  data  or  with  work  a l l o t(cid:173)
ments  that  have  not  yet  been  t r i e d. 

The  amount  of  work  alloted  made  a  difference 
It  is 

in  the  success  of  finding  relevant  beliefs. 
intended  to  measure  this  difference  quantitatively, 
but  techniques  for  this  have  not  yet  been  developed. 

Scaling  the  credibilities  of  a ll  the  beliefs  in 

the  system  by  a  factor  x  seemed  to  affect  the 
c r e d i b i l i ty  computed  for  an  input  proposition  by 
that  same  multiple,  x.  This  showed  that  the 
complex  search  combined  with  the  quotient  formulas 
for  credibility  s t i ll  preserved  linearity. 

Instead  we  explored  a  variety  of 

search  algorithm. 
procedures  in  an  effort  to  learn  more  about  their 
respective  merits  in  processing  the  same  data  base. 
We  intend  to  discuss  the  validation  problem 
future  report. 

in  a 

References 

( l)  Abelson,  R.P.  and  Carroll,  J .,  Computer 
simulation  of  individual  belief  systems. 
American  Behavioral  Scientist,  8,  24-30  (1965). 

(2) 

(3) 

00 

Abelson,  R.P.,  Psychological  implication. 
In 
Theories  of  Cognitive  Consistency  (Abelson,R., 
Aronson,  E.,  McGuire,  W.,  Newcomb,  T.,  Rosen(cid:173)
berg,  M.,  Tannenbaum,  P.  Edc.  Rand-McNally, 
New  York,  1969) 

Colby,  K.M.  and  Enea,  H.,  Heuristic  methods  for 
computer  understanding  of  natural  language  in 
context  restricted  on-line  dialogues.  Mathemat(cid:173)
ical  Biosciences,  1,  1-25  (1967)-

Schank,  R.C  and  Tesler,  L.G.  A  conceptual 
parser  for  natural  language.  Stanford  A r t i f i c ia 
Intelligence  Project  Memo  No.  AI-76,  1969  (See 
also  this  volume). 

(5)  Enea,  H.,  MLISP.  Stanford  Computer  Science 

Department  Technical  Report,  No.  92,  March  14, 
1965: 

Further  values  of  the  variable  f  .ctors  are  in 

the  process  of  being  tested,  as  well  as  improved 
searching  algorithms. 

(6) 

Smith,  D.C.  MLISP  Users  Manual.  Stanford  A r t i fi 
cial  Intelligence  Memo,  No. 
(In  prep-
aration. 

1969. 

Acknowledgements 

This  research  is  supported  by  Grant  PHS  MH  06645-07 
from  the  National  Institute  of  Mental  Health,  by  (in 
part)  Research  Scientist  Award  (No.  K-l4,433) 
from 
the  National  Institute  of  Mental  Health  to  the  senior 
author  and  (in  part)  by  the  Advanced  Research  Project 
Agency  of  the  Office  of  the  Secretary  of  Defense 
(SD-183). 

Discussion 

In  the  Abelson  and  Carroll  program 

The  only  other  program  we  know  of  which  judges 
credibility  is  that  of  Abelson  and  Carroll1.  There 
are  a  number  of  similarities  and  differences  be(cid:173)
tween  the  two  programs.  Perhaps  the  most  important 
difference  lies  in  the  way  the  search  algorithm  is 
controlled. 
searches  through  a  large  data  base  are  cut  off 
probabilistically,  depending  on  a  random  number 
exceeding  some  fixed  value. 
In  our  model,  three 
factors  govern  the  search,  consistency,  work  and 
firmness.  Search  along  consistent  paths  is 
preferred  to  search  along  inconsistent  paths.  Also 
search  along  a  path  is  cut  off  if  (a)  alloted  work 
runs  out,  (b)  a  firm  credibility  of  >60  or  <k0  is 
reached  or  (c)  the  path  is  exhausted. 

Another  interesting  difference  lies  in  the  way 

the  two  programs  treat  instances  and  supersets. 
The  Abelson  and  Carroll  program  looks  "down"  at 
instances  and  "up"  at  supersets  to  an  equal  degree. 
We  found  that  with  this  data  base  searching  for 
instances  contributed  to  credibility  less  reliably 
than  supersets.  Therefore  we  alloted  more  work  to 
searching  supersets  than  to  searching  instances. 

Our  experiments  with  this  data  base  collected 

from  an  informant  constituted  an  attempt  to  discover 
what  search  factors  made  a  significant  difference  in 
estimating  the  c r e d i b i l i ty  of  input  propositions. 
We  were  not  attempting  to  validate  a  particular 

-654-


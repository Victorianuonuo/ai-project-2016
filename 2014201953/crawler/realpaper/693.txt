Using Available  Memory  to  Transform  Graphplan's  Search 

Terry  Zimmerman  &  Subbarao  Kambhampati 
Department of Computer Science and Engineering 
Arizona State University, Tempe AZ 85287-5406 

{zim,rao| @asu.cdu 

Abstract 

We  present  a  major  variant  of the  Graphplan  algo(cid:173)
rithm  that  employs  available  memory  to  transform 
the  depth-first  nature  of Graphplan's  search  into  an 
iterative  state  space  view  in  which  heuristics  can 
be  used  to  traverse  the  search  space.  When  the 
planner,  PEGG, 
to  conduct  exhaustive 
search, 
it  produces  guaranteed  optimal  parallel 
plans  2  to  90  times  faster than  a  version  of Graph-
plan  enhanced  with  CSP  speedup  methods.  By 
heuristically  pruning  this  search  space  PEGG  pro(cid:173)
duces  plans  comparable  to  Graphplan's  in  make-
span,  at  speeds  approaching  state-of-the-art  heuris(cid:173)
tic  serial  planners. 

is  set 

1  Motivation  and  Approach 
Despite  the  recent  dominance  of  heuristic  state-search 
planners  over  Graphplan-style  planners,  the  Graphplan 
approach  [Blum  and  Furst  1997]  is  still  one  of the  most 
effective  ways  to  generate  so-called  "optimal  parallel 
plans".  While  state-space  planners  are  drowned  by  the 
exponential  branching  factors  of the  search  space  of par(cid:173)
allel  plans,  Graphplan  excels  due  to  the  way  it  combines 
an  IDA*  style  iterative  search  [Bonet  and  Geffner,  1999] 
with  a  highly efficient  CSP-based,  incremental  generation 
of valid  action  subsets.  We  present  here  a  system  called 
PEGG,  that  addresses  weaknesses  in  Graphplan's  ap(cid:173)
proach  by  employing  available  memory  to:  1)  reduce  the 
redundant  search  Graphplan  conducts  in  consecutive  it(cid:173)
erations,  and  2)  more  importantly,  to  transform  Graph-
plan's  IDA*  search  into  iterative  expansion  of a  select  set 
of states  that  can be  traversed any order. 

A  shortfall  of the  IDA*'s  approach  to  search  is  the  fact 
that  it  regenerates  many  of the  same  nodes  in  each  of its 
iterations.  This  can  be  traced  to  using  too  little  memory 
in  many  cases;  the  only  information  carried  over  from 
one  iteration  to  the  next  is  the  upper  bound  on  the  f-
value.  Given  that  consecutive  iterations  of search  over(cid:173)
lap  significantly,  we  investigated  using  additional  mem(cid:173)
ory  to  store  a  trace  of the  explored  search  tree  to  avoid 
repeated  re-generation  of search  nodes.  With  a  represen(cid:173)
tation  of the  explored  search  space,  we  can  transform  the 
way  this  space  is  extended  during  the  next  iteration.  In 
particular,  we  can  (a)  expand  search  trace  nodes  in  the 
order  of  their  heuristic  merit  and  (b)  we  may  also  con(cid:173)
sider  iteratively  expanding  a  select  set  of  states.  This 
strategy  is  too  costly  for normal  IDA*  search,  but  Graph-
plan's  type  of IDA*  search  is  particularly  well-suited  to 

these  changes  as  the kth  level  planning  graph  provides  a 
compact  way  of  representing  the  search  space  traversed 
by  the  corresponding  IDA*  search  in  its  kth  iteration. 
The  state  space  view  provided  by  the  search  trace  allows 
us  to  transform  Graphplan's  search  from  its  depth-first 
default to  a more  informed traversal  of the  space. 

2  Design  and  Experiments 

As  would  be  expected  for  IDA*  search  there  is  great 
similarity  (redundancy)  in  the  search  space  for successive 
search episodes as the plan graph  is extended.  In  fact, the 
search  conducted  at  any  level  k+1  of the  graph  is  essen(cid:173)
tially  a  replay  of  the  search  conducted  at  the  previous 
level  k  with  certain  well-defined  extensions.  Specifi(cid:173)
cally,  every  set  of  subgoals  reached  in  the  backward 
search  of episode  n,  starting  at  level  k,  will  be  generated 
again  by  Graphplan  in  episode  n+1  starting  at  level  k+1. 

Figure  J.  State  space  view  of Graphplan's  search  space: 

3  consecutive  search  episodes  leading  to  a  solution 

Figure  1  depicts the state space tree structure correspond(cid:173)
ing  to  Graphplan's  search  over  three  consecutive  search 
iterations  on  a  hypothetical  problem.  The  dark  shaded 
states  are  first  produced  during  Graphplan's  attempt  to 
satisfy  the XYZ goal  at  level  7.  They are generated again 
in  the  next  episode,  each  at  one  level  higher,  along  with 
some  new  states  (shown  in  lighter  shade).  Finally,  in  the 
third  episode  Graphplan  regenerates  the  dark  and  lighter 
shaded  states,  attempting  to  satisfy  XYZ  at  level  9,  and 
finds a solution. 

1526 

POSTER  PAPERS 

12.2 
~ 
~ 

~ 

Graphplan 

Problem 
Problem 

47.5 

GP-e 

(511/511)  118 
(12/12)  7.2 

190  (19/29) 

13.4(18/18) 
(36/36) 
~ 
~ 
- 
(36/45) 
~  (40/59) 
~ 
~ 

cpu sec (steps/acts) 
Stnd. 
194.8 
~ 
-
-
~ 
~ 
~ 

bw-large-B 
bw-large-D 
att-log-b 
gripper-15 
gripper-20 
tower-9 
TSP-12 
AIPS '98, '00, '02  Competition Problems 
gripper-x-4 
gripper-x-5 
log-y-4 
blocks-10-1 
blocks-16-2 
logistics-10-0 
logistics-12-1 
freecell-2-1 
depot-6512 
depot-1212 
driverlog-2-3-6 
driverlog-4-4-8 
ztravel-3-7a 
ztravel-3-8a 

239  5.1 
~ 
- 
-
~ 
-
-

(22/55) 
27.5  (7/20) 

~ 
~ 
-
-
~ 
-
-

73.9 
-
512 
(11/60)  366 
470 
95.4  (34/34)  18.7 
-  (54/54) 

30.0  (15/56)  21 

98.0  (6/10) 

972  (7/25) 

~ 
~ 

-

5oPEGG 
5oPEGG 
cpu  sec 
cpu  sec 
(steps/acts) 
(steps/acts) 

PEGG 
PEGG 
cpu sec 
cpu sec 

(steps/acts) 
(steps/acts) 

Speedup  I 
Speedup  I 
(PEGG vs. 
(PEGG vs. 

GP-e) 
GP-e) 

4.3x 
3.1  (18/18) 
>5x 
340 (38 / 38) 
120  (11/79)  >15x 
16.7  (36/45)  >107x 
44.8  (40/59) 
>40x 
23.6(511/511)  >76x 
>277x 
6.5  (12/12) 

\ 

~ 

30.9  (19/29) 
110  (23/35) 
330  (11/58) 
11.0 
(34/34) 
58.7  (54/54) 
8.9  (15/55) 
1101 (15/75)  119  (15/75) 
80.1 
5.0 

62.9 
2.1 
127  (22/56) 
(7/20) 
1.9 
589 
(10/24) 
1434 (10/23)  222  (10/21) 
11.2 
(7/25) 

~ 
1.9 
~ 

2.3 

6.1x 
>16x 
1.4x 
8.7x 
>31x 
3.4x 
>15.1x 
1.5x 
2.4x 
>14x 
14.5x 
>3x 
>8x 
423x 

EGBG 

[Zimmerman,  Kambhampati,  1999] 
used  memory  to  aggressively  record  the  experi-
ence  in  each  search  episode,  essentially  avoid(cid:173)
ing  all  redundant  effort. 
That  approach  con(cid:173)
fronted  memory  constraints  on  larger  problems, 
but  it  suggests  a  more  powerful  use  for  a  pared-
down  search  trace:  exploiting  the  snapshot  view 
of  the  entire  search  space  of  a  Graphplan  itera(cid:173)
tion  to  focus  on  the  most  promising  areas.  This 
transformation  frees  us  from  the  depth-first  na(cid:173)
ture  of  Graphplan's  search,  permitting  move(cid:173)
ment  about  the  search  space  to  focus  on  its  most 
promising  sections  first  -or  even  exclusively. 

level 

indexed 

transposed  up  one  planning  graph 

A  summary  of  PEGG  (for  details; 

initiates  Graphplan's  CSP-style  search 

[Zimmer(cid:173)
man  and  Kambhampati,  2003])  relies  on  these 
definitions:  Search  segment:  a  state  generated 
during  Graphplan's  regression  search  from  the 
goal  state, 
to  a  specific  plan  graph 
level.  It  holds  the  state's  goal  list,  a  pointer  to 
the  parent  search  segment,  and  the  actions  as(cid:173)
signed  in  satisfying  the  parent's  goals.  Search 
trace  (ST): 
the  linked  set  of  search  segments 
(states)  representing  the  search  space  visited  in 
a  Graphplan  backward  search  episode.  The  sets 
of  states  in  each  of  the  three  shadings  of Figure 
1  can  be  seen  as  the  ST  after  each  of the  three 
episodes.  T r a n s p o s i t i o n:  The  extant  trace  of 
search  segments  (states)  after  search  episode  n 
is 
for 
episode  n+1  as 
follows:  For  each  ST  search 
segment  associated  with  graph  level  j  associate  it 
with  level  j+1  for  episode  n+1.  V i s i t i ng  a  search 
the  goals  of  segment  S p  are  memo 
segment: 
checked  at 
their  associated 
if  valid, 
PEGG 
to 
satisfy  them.  The  process  is  enhanced  by  a  cadre  of 
efficiency  techniques  such  as  a  bi-level  plan  graph, 
domain  preprocessing,  explanation  based 
learning 
(EBL),  dependency  directed  backtracking  (DDB),  and 
goal  &  action  ordering.  Whenever  the  goals  of  S p  are  val(cid:173)
idly  assigned,  a  child  segment  is  created  containing  S p's 
goals  regressed  over  the  assigned  actions,  and  linked  to 
SP  (thus  extending  the  ST). 
The  PEGG  algorithm:  the  graph  is  built  until  the  prob(cid:173)
lem  goals  appear  non-mutex,  and 
regression 
search  episode  is  conducted  ala  Graphplan  fashion.  Dur(cid:173)
ing  this  search,  the  initial  trace  is  constructed,  concisely 
capturing  all  'states'  generated  during  the  search  process. 
If no  solution  is  found,  the  ST  directs  the  search  process 
for  future  iterations.  This  search  is  2-phased:  select  a 
promising  ST  state,  then  Graphplan's  depth-first,  CSP-
type  search  on  the  state's  subgoals  is  conducted.  Another 
ST  search  segment  is  heuristically  selected  if search  fails. 
Since  the  ST  provides  a  state  space  view,  PEGG  can  use 
'distance  based'  heuristics  (c.f.  HSP-R  [Bonet  and  Geff-
[Nguyen  and  Kambhampati, 
ner,  1999]  and  A l t A lt 
2000]). 
'adjusted  sum' 
heuristic  from  the  latter  is  used. 

For  results  reported  here, 

level  and 

first 

the 

the 

Table  1  compares  PEGG  against  standard  Graphplan 
and  a  highly  enhanced  version  (GP-e),  which  has  been 
augmented  with  the  efficiency  methods  mentioned  above. 
T wo  PEGG  modes  of operation  are  reported;  1)  so-PEGG: 
make-span  optimal,  ST  search  segments  ordered  accord(cid:173)
ing  to  a  state  space  heuristic,  all  segments  visited,  2) 
PEGG:  Ordering  the  ST  search  segments  as  for  1,  beam 

Table  J.  PEGG  vs.  Graphplan  and  enhanced  Graphplan  (GP-e) 
GP-e: enhanced  Graphplan (sec text)  so-PEGG  step-optimal,  search via 
the ST.  PEGG:  beam search on best 20% of search segments in ST 
- indicates failure in  30 min  limit, cpu time 
Parentheses next to cpu time give # of steps/ # of actions in solution. 
Allegro Lisp, runtimes (cxcl. gc time) on Pentium 900 mhz, 384 MB RAM 

search  on  only  the  heuristically  'best'  fraction.  The  first 
approach  maintains  Graphplan's  guarantee  of  step  opti-
mality  while  the  latter  sacrifices  the  guarantee  of optimal(cid:173)
ly  in  favor  of  pruning  search  in  all  search  episodes  and 
bounds  the  size  of  the  search  trace  that  is  maintained  in 
memory. 
that  optimal  make-span 
plans  are  generally  found  by  PEGG  regardless,  (one  ex(cid:173)
ception  shown  in  bold)  and  speedups  as  high  as  two  or(cid:173)
ders 
are 
achieved. 

enhanced  Graphplan 

Empirically  we 

of  magnitude 

over 

find 

In  Proceedings  of ECP-99,  1999. 

R e f e r e n c es 
Blum  A.  and  Furst  M.1997.  Fast  planning  through  planning 
graph  analysis.  Artificial  Intelligence  90(1-2).  1997. 
Bonet,  B.  and  Geffner,  H.  1999.  Planning  as  heuristic  search: 
New  results. 
Nguyen,  X.  and  Kambhampati,  S.  2000.  Extracting  effective 
and  admissible  state  space  heuristics  from  the  planning  graph. 
In  Proceedings  of.  AAAI-2000. 
Zimmerman,  T.  and  Kambhampati,  S.  1999.  Exploiting  Sym(cid:173)
metry  in  the  Planning-graph  via  Explanation-Guided  Search. 
In  Proceedings  of AAAJ-99,  1999. 
Zimmerman,  T.  and  Kambhampati,  S.  2003.  Using  memory  to 
transform  search  on  the  planning  graph.  ASU  Technical  Re(cid:173)
port  (available  at  http://rakaposhi.eas.asu.edu/pegg-tr.pdO 

POSTER PAPERS 

1527 


Session No.  9  Pattern Recognition II Statistical Approaches 

411 

A NONPARAMETRIC VALLEY-SEEKING 
TECHNIQUE  FOR CLUSTER ANALYSIS 

Warren  Lewis  Gordon  Koontz 

and 

Keinosuke  Fukunaga 

School  of  Electrical  Engineering 

Purdue  University 

West  Lafayette,  Indiana  U.S.A. 

Abstract 

The  problem  of  clustering  multivariate  obser(cid:173)

vations  is  viewed  as  the  replacement  of  a  set  of 
vectors  with  a  set  of  labels  and  representative 
vectors.  A  general  criterion  for  clustering  is 
derived  as  a  measure  of  representation  error. 
Some  special  cases  are  derived  by  simplifying  the 
general  criterion.  A  general  algorithm  for  find(cid:173)
ing  the  optimum  classification  with  respect  to  a 
given  criterion  is  derived.  For  a  particular 
case,  the  algorithm  reduces  to  a  repeated  applica(cid:173)
tion  of  a  straightforward  decision  rule  which  be(cid:173)
haves  as  a  valley-seeking  technique.  Asymptotic 
properties  of  the  procedure  are  developed. 
Numerical  examples  are  presented  for  the  finite 
sample  case. 

I. 

Introduction 

It  is  not  d i f f i c u lt  to  imagine  a  collection 
of  objects  whose  members  can  be  classified  into 
two  or  more  categories  simply  on  the  basis  of 
their  observable  characteristics. 
always  necessary  to  rely  on  a  similar  collection 
of  labeled  objects  as  a  basis  for  classification. 
For  example,  biological  taxonomists  have  classified 
living  things  into  a  large  number  of  meaningful 
categories.  Yet  at  no  time  in  history  did  any 
plant  or  animal  bear  a  label.  Rather,  categories 
have  been  established  without  supervision. 

It  is  not 

Recently,  methods  for  automatic  unsupervised 
classification,  or  clustering,  have  been  proposed. 
A  machine  algorithm  for  clustering  can  be  a  valu(cid:173)
able  tool  in 

i)  pattern  recognition  -  Often,  a  training 

set  of  labeled  objects  is  d i f f i c u lt  or 
impossible  to  obtain.  Further,  a  known 
class  of  objects  may  contain  unknown 
subclasses, 

and 

i i) 

statistical  analysis  -  Cluster  analysis 
may  be  used  to  expose 
the  detailed 
structure  of  a  large  volume  of  data. 

We  w i ll  present  and  discuss  a  family  of  clustering 
algorithms. 

Our  approach  involves  the  use  of  a  clustering 

criterion.  This  criterion  assigns  a  numerical 
value  to  every  possible  classification  of  the 
objects.  Meaningful  classifications  are  assumed 
to  correspond  to  extreme  values  of  the  criterion. 
The  optimum  classification  in  the  sense  of  a  given 
criterion  is  determined  by  means  of  a  clustering 
algorithm.  An  efficient  clustering  algorithm  is 

necessary  because  an  exhaustive  check  of  a ll 
possible  classifications  is  usually  impractical. 
Thus,  for  our  purposes,  the  clustering  problem 
consists  of  two  basic  elements: 

1)  definition  of  a  clustering  criterion 

11)  construction  of  a  clustering  algorithm. 

and 

The  idea  of  using  a  clustering  criterion  is 

not  new.  Many  procedures  reported  in  Ball's 
survey  (1)  are  based  on  c r i t e r i a.  Friedman  and 
Rubin  (2)  present  a  class  of  criteria  and  discuss 
the  property  of  transformation  invarlance. 
Fukunaga  and  Koontz  (3)  show  conditions  where  the 
criteria  of  (2)  become  equivalent  to  a  simpler 
criterion.  Watanabe  (4)  proposes  a  criterion, 
which  he  calls  cohesion,  which  can  detect  more 
subtle  relationships  among  objects  than  palrwlse 
similarities. 

Presently,  no  universal  clustering  criterion 

has  been  defined.  This  is  simply  a  consequence 
of  the  lack  of  a  precise  mathematical  definition 
of  a  cluster.  That  i s,  the  clustering  problem 
is  one  whose  solution  cannot  be  characterized  in 
a  definite  way.  Thus,  in  order  to  derive  a 
mathematical  criterion,  we  must  postulate  a 
rigorous  definition  or  clustering.  This  postulate 
can  then  be  tested  by  experiments  with  objects 
whose  class  structure  is  known  and  well  defined. 

The  remainder  of  this  paper  consists  of 

In  the  next  section, 

four  sections  and  a  summary. 
(section  I I)  we  present  our  characterization  of 
the  clustering  problem  and  compare  it  with  other 
notions.  We  w i ll  then  use  this  characterization 
to  derive  a  criterion. 
In  the  following  section 
we  w i ll  state  and  discuss  a  general  algorithm 
for  finding  the  classification  which  extremizes 
our  criterion.  Section  IV  concerns  the  asympto(cid:173)
tic  behavior  of  the  procedure,  i . e .,  what  happens 
when  the  number  of  objects  is  very  large.  Re(cid:173)
sults  of  computer  experiments  are  given  in  section 
V. 

I I.  A  Clustering  Criterion 

The  criterion  derived  in  this  section  is 
based  on  the  notion  that  information  is  lost 
when  objects  are  represented  only  by  class  labels. 
Suppose  that  each  member  of  a  collection  of  N 
objects  is  represented  by  an  L-dlmenslonal  vector. 
Then  the  set  of  N  vectors, 
a ll  of  the  available  Information  concerning  the 
objects.  The  clustering  operation  replaces  this 
set  of  vectors  with  a  set  of  labels,  {w1,  . .. ,wN}. 
The  i-th  label,  w1,  is  an  integer  between  1  and 
M  (M  < N),  and  denotes  the  class  to  which  X1  is 
assigned.  The  label  set  contains  less  informa(cid:173)
tion  about  the  objects  than  does  the  vector  set. 
Therefore,  clustering  is  viewed  here  as  a  data 
reduction  algorithm  which  destroys  information. 

[X1.  XN}, 

contains 

412 

Session No.  9  Pattern Recognition II Statistical Approaches 

measure  of  this  error  can  serve  as  a  clustering 
criterion  to  be  minimized.  There  are  at  least  two 
ways  to  derive  a  numerical  measure  of  representa(cid:173)
tion  error.  The  method  which  has  been  used  most 
often  in  the  past  is  to  measure  the  error  commit(cid:173)
ted  by  using  a  representative  vector,  C(w1),  aa 
an  estimate  of  Xi.  An  error  vector  can  be  de(cid:173)
fined  as 

a  measure  of  validity  of  the  classification  tree 
(5,6). 

The  general  criterion,  J  is  too  cumbersome 
to  use  in  practice.  The  summation  contains  N2 
terms  in  general.  Therefore,  we  would  like  to 
assign  zero  weight  to  most  of  the  terms.  Suppose 
f  satisfies 

where  Ni.  is  the  population  of  class  j,  then  W  is 
the  total 
criteria  which  are  functions  of  W  are  discussed 
in  (2)  and  (3). 

intragroup  scatter  matrix.  Several 

An  alternate  definition  of  representation 
error  is  used  in  the  present  development.  We 
w i ll  concern  ourselves  with  the  error  committed 
in  estimating  distances  between  pairs  of  vectors. 

Then  a  measure  of  distance  representation  error, 
which  w i ll  be  used  as  a  clustering  criterion,  is 

where  f(Xi,  Xi  )  is  a  set  of  weighting  coefficients. 
This  kind  of  criterion  is  often  used  to  measure 
mapping  error  and  clustering  is  a  kind  of  mapping. 
However,  some  special  considerations  are  import(cid:173)
ant  in  its  use  as  a  clustering  criterion.  First 
of  a l l,  not  all  of  the  distances  are  euclidean. 
A  more  important  point,  however,  is  the  fact  that 
the  wi's  are  variables  and  the  Xi's  are  fixed. 
Due  to  the  discrete  and  unordered  nature  of  the 
wi's,  ordinary  gradient  methods  cannot  be  used 
to  minimize  J. 

Criteria  of  the  same  form  as  J  have  been 

used  in  hierarchical  clustering. 
clustering,  objects  are  classified  according  to 
a  diverging  tree  structure.  A  tree  metric  is 
defined  which  numerically  defines  the  distance 
between  two  objects  according  to  their  position 
on  the  tree.  The  degree  of  f it  between  the 

In  hierarchical 

is  implied. 
JIP  assigns  a  nonzero  penalty  for  each  pair  of 
vectors  closer  together  than  R  and  classified 
into  different  classes. 

If  [6]  is  taken  as  the  definition  of  d  ,  the 

following  special  cases  of  Ji  and  JIR  result: 

J2R  is  the  simplest  criterion  we  w i ll  derive. 
is  equal  to  the  total  number  of  distinct  pairs 
of  vectors  separated  by  a  distance  less  than  R 
and  assigned  to  different  classes.  We  w i ll  some(cid:173)
times  refer  to  J2R  as  the  fixed  neighborhood 
penalty  rule.  The  remainder  of  this  paper  mainly 
concerns  J2R. 

It 

The  following  properties  of  J2R  support  i ts 
use  as  a  clustering  criterion. 
1) Comnut tion For ific tion 

Session No.  9  Pattern Recognition II Statistical Approaches 

413 

J2R  is  evaluated  by  a  counting  process 
rather  than  by  complex  calculations. 
Since  the  vectors  are  fixed,  the  neigh(cid:173)
boring  pairs  need  be  determined  only 
once. 

i i)  Storage  When  R  is  sufficiently  small, 

the  number  of  neighboring  pairs  of 
vectors  is  moderate.  The  storage 
requirement  is  governed  primarily  by 
this  number, 

i i i)  Classification  Contributions  to  JOD 
come  from  pairs  of  vectors  near  the 
boundaries  separating  classes.  Thus, 
It  is  preferable  for  the  boundary  to 
pass  through  a  region  of  low  vector 
concentration.  This  kind  of  classifica 
tion  is  quite  reasonable  when  there  is 
no  supervision  available. 

The  algorithm  for  finding  the  optimum 

assignment  with  respect  to  a  given  criterion  is 
the  second  essential  ingredient  of  clustering. 
Although  clustering  need  not  take  place  in  real 
time,  there  are  s t i ll  practical  constraints 
which  rule  out  inefficient  procedures,  such  as 
exhaustive  searching.  We  have  made  use  of  a 
general  type  of  algorithm.  This  algorithm  can 
be  applied  to  a  wide  variety  of  c r i t e r i a,  but 
in  the  special  cases  of  J2R  it  becomes  particu(cid:173)
larly  easy  to  implement. 

At  this  point,  the  reader  may  wonder  if  a 
K  nearest  neighbor  penalty  rule  can  be  defined. 
The  answer  is  yes,  since  we  can  write 

Notice  that  fk  is  not  symmetric.  Although  the  K 
nearest  neighbor  penalty  rule  has  a  valuable 
it 
counterpart  in  supervised  pattern  recognition, 
is  unsuitable  in  the  present  case.  The  K  nearest 
neighbor  rule  does  not  favor  one  region  over  an(cid:173)
other  because  of  density.  Therefore,  it  may  well 
prescribe  a  boundary  through  a  mode  in  the  vector 
distribution. 

At  this  point,  we  have  a  family  of  nonpara-
metric  criteria  with  three  levels  of  complexity. 
The  parent  criterion,  J,  is  the  most  complex 
and  is  in  the  closest  accord  with  our  original 
concept  of  clustering  (distance  preservation). 
Its  descendants  are  J1  followed  by  J2,  with 
special  cases  JIR  and  J2R.  Criteria  at  the  J1 
level  are  more  general  in  that  they  allow  the 
penalty  to  be  class  dependent,  but  J2R  is  easier 
to  implement  and  admits  an  interpretation  which 
seems  very  suitable.  Unfortunately,  criteria 
of  the  J1  level  and  below  have  an  absolute  mini(cid:173)
mum  of  zero  when  a ll  vectors  are  assigned  to  the 
same  class.  This  is  not  a  serious  problem  in 
practice  because  there  w i ll  be  local  minima 
corresponding  to  more  interesting  classifica(cid:173)
tions.  The  degenerate  case  is  easily  detected. 

We  have  not  specified  how  to  choose  either 
the  number  of  classes,  M,  or  the  region  size,  R. 
We  have  no  rigorous  theory  to  rely  on  here,  and 
we  can  only  offer  suggestions  based  on  experi(cid:173)
mental  results.  Therefore,  we  postpone  discus(cid:173)
sion  of  these  points  until  section  V. 

I I I.  The  Clustering  Algorithm 

Ties  involving  wr(k)  are  resolved  in  its 
favor.  Other  ties  are  resolved  arbitrarily. 
If  any  vector  is  placed  in  a  new 

Step  4: 

class,  return  to  Step  2  and  repeat. 
Otherwise,  stop. 

Note  that  a ll  computation  occurs  in  step  2  and 
the  vectors  are  reclassified  simultaneously  in 
step  3. 

There  is  no  guarantee  that  this  algorithm 
there  is  l i t t le 

w i ll  converge.  Even  if  it  does, 
we  can  say  about  the  strength  of  the  minimum 
obtained.  Fortunately,  empirical  evidence  seems 
to  favor  this  procedure.  Fukunaga  and  Koontz 

414 

Session No.  9  Pattern Recognition II  Statistical Approaches 

iterative  application  of  the  fixed  neighborhood 
decision  rule.  We  can  easily  apply  it  to 
numerical  examples,  and  we  do  this  in  section  V. 
First,  however,  let  us  see  how  the  procedure 
behaves  when  N  is  very  large. 

IV.  Asymptotic  Behavior 

The  performance  of  the  clustering  algorithm 

developed  in  sections  II  and  I II  can  be  studied 
analytically  when  N  is  very  large. 
section,  we  w i ll  derive  the  asymptotic  version 
of  D2R  and  discuss  i ts  properties.  The  asympto(cid:173)
tic  properties  provide  some  insight  into  the 
general  behavior  of  our  procedure.  They  also 
suggest  how  the  procedure  can  be  expected  to 
perform  with  finite  data  sets. 

In  this 

Let  us  f i r st  rewirte  the  expression  for  D2R, 

normalizing  bv  a  factor  of  1/N. 

This  procedure,  which  follows  from  applica(cid:173)

tion  of  the  general  algorithm  to  a  specific 
criterion,  J2R,  is  a  valley  seeking  technique. 
To  see  this,  consider  vectors  along  the  boundary 
separating  class  S1  from  class  S2  at  the  kth 
iteration.  Suppose  there  is  a  heavier  concentra(cid:173)
tion  of  vectors  on  the  s?  side  of  the  boundary. 
Then  vectors  near  the  boundary  are  reclassified 
into  class  S?.  Hence,  the  boundary  moves  into 
the  region  previously  assigned  to  class  s j. 
Therefore,  the  boundary  moves  away  from  the  higher 
concentrations  and  toward  valleys  in  the  d i s t r i(cid:173)
bution. 

the  algorithm  may  get  stuck  with  the 

Two  kinds  of  d i f f i c u l ty  may  arise  when  the 
fixed  neighborhood  decision  rule  is  used.  First 
of  a l l, 
boundary  passing  through  a  region  of  relatively 
sparse  population  when  better  boundaries  exist. 
Secondly,  the  boundary  may  diverge,  leaving  a ll 
of  the  vectors  in  a  single  class.  Both  of  these 
d i f f i c u l t i es  are  combatted  by  altering  the 
i n i t i al  assignment, 
trol  parameter,  R. 

(0),  and  adjusting  the  con(cid:173)

The  clustering  algorithm  and  the  clustering 

criterion  together  make  up  a  clustering  proce(cid:173)
dure .  The  clustering  procedure  has  become  the 

[29] 

is  easily  illustrated  when  the  dimension, 

The  behavior  of  the  decision  rule  corresponding 
to 
L,  is  two.  Figure  1  shows  a  region  around  the 
boundary  separating  classes  S1  and  S2.  For  the 
value  of  R  shown,  Y1  clearly  remains  in  class 
S1.  However,  if  the  probability  mass  within  R 
of  Y2  and  to  the  right  of  the  boundary  is  larger 
than  that  to  the  left  of  the  boundary,  then  Y2 
is  reassigned  to  class  S2. 
[29]  reassigns 
no  vectors,  then  the  boundary  is  said  to  be 
stationary. 

If 

If  R  is  sufficiently  small,  we  can  charact(cid:173)

erize  a  stationary  boundary  rather  nicely. 
Figure  2  shows  a  small  region  about  a  point  on 
the  boundary  between  classes  s1  and  s2.  The 
boundary  has  unit  normal  vector  w  and 
is 
the  gradient  of  the  mixture  density  evaluated 
at  Y, 

i . e ., 

Session No.  9  Pattern Recognition II Statistical  Approaches 

415 

where  V.  is  the  volume  of  Q.  SR 
superscript  T  denotes  transposition.  Agein 
noting  that  R  is  small,  we  assume  that  the 
boundary  splits  SR(Y)  into  two  L  dimensional 
hemispheres  so  that  V1  =  V2.  The  integrals  in 
[32]  are  given  by 

(Y)  and  the 

Suppose  the  left  hand  side  of  [35]  is  positive. 
Then  Y  w i ll  be  assigned  to  s1.  Further,  a ll 
vectors  within  a  small  neighborhood  of  the 
boundary  w i ll  also  go  to  s1.  Thus  the  boundary 
shifts  to  the  right  (see  Fig.  2).  Similarly, 
if  the  right  hand  side  of  [35]  is  negative, 
the 
boundary  moves  to  the  l e f t.  The  condition  for 
stationarity  of  the  boundary  is 

[36 1 

A  final  boundary  between  two  classes  must  be 

stable  as  well  as  stationary.  This  means  that 
if  the  boundary  is  perturbed  it  must  not  tend  to 
move  farther  away  from  the  stationary  point.  We 
can  establish  a  condition  for  unstability  as 
follows.  Figure  3  is  an  exagerated  illustration 
of  a  small  perturbation  of  a  stationary  boundary. 
The  vector  Y'  is  a  point  on  the  new  boundary 
such  that 

is  the  new  unit  normal  vector. 

where  w 
component  of 
the  boundary  w i ll  tend  to  move  farther  away  from 
the  stationary  position.  Hence  the  boundary  is 
unstable 

If  the 
is  negative,  then 

(Y  )  along  w 

if 

Tn  conclusion,  the  final  boundary  must 

satisfy  two  conditions. 

i)  The  component  of  the  gradient  of  the 

density  normal  to  the  boundary  must  be 
zero. 

i i)  The  boundary  may  not  pass  through 

is  negative  seraide-

regions  where 
f i n i t e. 

This  development  shows  that  our  algorithm 

leads  to  reasonable  classifications  in  the 
asymptotic  case.  Hopefully, 
it  also  provides 
insight  into  the  behavior  of  the  algorithm  in 
the  finite  sample  case  as  well. 

V.  Examples 

The  algorithm  has  been  tested  on  a r t i f i c a l ly 
generated  bivariate  data.  There  is  no  additional 
d i f f i c u l ty  in  the  multivariate  case. 

The  value  of  R  has  considerable  effect  on 

the  performance  of  the  algorithm.  We  found 
that  the  procedure  works  best  when  R  is  such  that 
the  number  of  distances  less  than  R  is  10  to  ?0 
times  the  sample  size. 

The  choice  of  M  is  more  d i f f i c u l t. 

In  one 
case,  a  large  value  of  M  resulted  in  most  of  the 
vectors  being  placed  in  one  of  two  classes,  but 
we  cannot  guarantee  that  this  would  always  be 
the  result. 

Figure  4  show  the  results  of  one  example 

with  M=2.  The  i n i t i al  boundary  is  random.  Note 
that  the  data  are  not  linearly  separable. 

The  number  of  iterations  required  in  the 

experiments  ranged  from  4  to  10.  Total  computa(cid:173)
tion  time  was-under  10  seconds  on  a  CDC  6500. 

416 

VI.  Sumnary 

A  clustering  procedure  consists  of  a  criterion 

and  an algorithm.  We have  developed  a  general 
clustering  procedure  of  which  the  fixed  neighbor(cid:173)
hood  decision  rule  is  a  special  case.  The 
asymptotic  behavior  of  the  procedure  was  studied 
and  computer  experiments  testified  to  its 
practical  value. 

The  procedure  has  been  shown  to  be  suitable 
It  has  modest 
even  for  non  ellipsoidal  clusters. 
storage  requirements  and  the  computational  loop, 
which  involves  only  counting,  is  very  rapid. 

References 

(1)  G.H.  Ball,  "Data  analysis  in  the  social 
sciences:  what  about  the  details?",  1965 
Fall  Joint  Computer  Conf..  AFIPS  Proc, 
vol.  27,  pt.  1,  Washington,  D.C.:  Spartan, 
1965,  pp.  533-559. 

(2)  H.P.  Friedman  and  J.  Rubin,  "On  some  invar(cid:173)

iant  criteria  for  grouping  data",  Amer. 
Stat.  Assoc.  J .,  vol.  62,  pp.  1159-1178, 
December,  1967. 

(3)  K.  Fukunaga  and  W.L.G.  Koontz,  "A  criterion 

and  an  algorithm  for  grouping  data",  IEEE 
Trans,  on  Computers,  vol.  C-19,  No.  10, 
pp.  917-923,  October,  1970. 

(4)  M.S.  Watanabe,  Knowing and  Guessing,  New 

York:  Wiley,  1969,  Ch.  8. 

(5)  S.C.  Johnson,  "Hierarchical  clustering 
schemes",  Psychometrika,  vol.  32,  pp. 
241-254,  1967. 

(6)  J.A.  Hartigan,  "Representation  of  similarity 

matrices  by  trees",  Amer.  Stat.  Assoc.  J ., 
vol.  62,  pp.  1140-1158,  December,  1967. 

(7)  K.  Fukunaga  and  W.L.G.  Koontz,  "Application 
of  the  Karlunen-Loeve  expansions  to  feature 
selection  and  ordering",  vol.  C-19,  no.  4, 
pp.  311-318,  April,  1970. 

(8)  E.H.  Ruspini,  "A  new  approach  to  clustering," 

Information  and  Control,  vol.  15,  pp.  22-32, 
1969. 

(9)  F.J.  Rohlf,  "Adaptive  hierarchical 
schemes",  Systematic  Zoology,  vol. 
1,  pp.  58-82,  March  1970. 

clustering 
9,  no. 

(10)  R.  Gnanadesikan  and  M.B.  Wilk,  "Data 

analytic  methods  in  multivariate  statistical 
analysis",  Multivariate  Analysis,  Vol.  I I, 
New York:  Academic  Press,  1969. 

Session No.  9 Pattern Recognition II Statistical Approaches 

Asymptotic  fixed  neighborhood  decision  rule 

Figure  1. 

Classification  of  a  boundary  point. 

Figure  2. 

Session No.  9 Pattern Recognition II Statistical Approaches 

417 

Perturbation  of  a  stationary  boundary 

Figure  3. 

Classification  of  nonlinearly  separable  data 

Figure  A. 


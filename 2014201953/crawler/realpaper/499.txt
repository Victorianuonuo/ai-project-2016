A Theory of Average-Case Compilability in Knowledge Representation 

Hubie Chen 

Department of Computer Science 

Cornell University 

Ithaca, NY 14853, USA 
hubes @ cs.cornell .edu 

Abstract 

Compilability is  a  fundamental property of knowl(cid:173)
edge  representation  formalisms  which  captures 
how succinctly information can be expressed.  Al(cid:173)
though many results concerning compilability have 
been  obtained,  they  are  all  "worst-case"  results. 
We develop a theory of average-case compilability 
which  allows  for the  formal  comparison  and clas(cid:173)
sification  of knowledge  representation  formalisms 
"on average." 

Introduction 

1 
By now, a multitude of knowledge representation formalisms 
have been  proposed and studied in  the  literature,  for exam(cid:173)
ple,  propositional  logic,  default  logic,  and  circumscription 
-  to  name  a  few.  The  comparison  of these  formalisms  has 
been  a  major research  theme  over the  past  decade.  In  par(cid:173)
ticular,  many  results  have  been  obtained  on  the  computa(cid:173)
tional  complexity  of  inference  and  model  checking  -  core 
reasoning tasks associated with each formalism.  The funda(cid:173)
mental  property  of compilability,  which  measures  how  effi(cid:173)
ciently or succinctly a formalism  represents knowledge, has 
also  been  studied.1  There  is  now  a  rich  body  of  results 
concerning  compilability;  however,  all  of  these  results  ad(cid:173)
dress the "worst-case", and are thus susceptible to the com(cid:173)
plaint that  they  do  not  address  "average"  or "typical"  com(cid:173)
pilability.  The  main  contribution  of this  paper  is  a  theoret(cid:173)
ical  framework providing  a  language  and  tools  for compar(cid:173)
ing and classifying the compilability of formalisms on aver(cid:173)
age.  Our framework is built on notions and insights from the 
theory  of compilability  clases  due  to  Cadoli  et  al.  12000a; 
2000b]  and  the  theory  of average-case  time  complexity  due 
to Levin 11986]. 

1.1  Background 
Informally, a formalism A is compilable to 
Compilability. 
a  formalism  B  if for  every  knowledge  base  x  in  formalism 
A,  there  is  a  knowledge  base  in  formalism  D  representing 
the same information as x with size polynomial in the length 

In previous work, compilability has also been called space effi(cid:173)

ciency and succinctness. 

of xr  Intuitively,  this  means  that  formalism  B  is  at  least  as 
space efficient as formalism A:  whatever can be expressed in 
formalism A can be expressed, with about the same level of 
succinctness,  in  formalism B. 

A  number  of  papers  compared  the  compilability  of  dif(cid:173)
ferent  formalisms  [Kautz  et  ai,  1995;  Gogic  el  al,  1995; 
Khardon  and Roth,  1996;  Cadoli  et  al  1996;  1997;  1999; 
2000a; 2000b].  In many cases, these papers rigorously prove 
that one  formalism  B  is  strictly  more  succinct  than  another 
formalism  A  -  that  is,  A  is  compilable  to  B,  but  B  is  not 
compilable  to  A.  This  means  that  there  is  no  way  to  trans(cid:173)
late  knowledge  bases  in  B  to  knowledge  bases  in  A,  unless 
the  translation  is  allowed  to  increase  the  size  of knowledge 
bases  by  a  super-polynomial  amount. 
In  other  words,  any 
translation  of knowledge  bases  in  B  to  knowledge  bases  in 
A  is  inherently exponential in  size.3  Observe that  this  state(cid:173)
ment has the same flavor as the famous "P does not equal NP" 
conjecture from classical complexity theory.  This conjecture 
holds that no algorithm can  solve an NP-complete problem, 
unless the algorithm is allowed to take super-polynomial time 
on some  inputs - or,  put differently, any algorithm solving a 
NP-complete problem is  inherently exponential in  time. 

Notice  that the  above definition  of "formalism  A  is  com(cid:173)
pilable  to  formalism  B"  docs  not  take  into  account  the  dif(cid:173)
ficulty  or  complexity  of  computing  the  translation  between 
knowledge bases  in A and  knowledge bases in  B.  This non-
uniformity is a key feature of the definition:  the existence of 
a succinct translation  is sufficient;  an efficiently computable 
translation  is not necessary.  It turns out that many proofs of 
non-compilability - that  is,  proofs  of statements  of the  form 
"formalism B is not compilable to formalism A" - rely on re(cid:173)
sults from non-uniform complexity theory.  Such proofs often 
are not unconditional, but are contingent upon the widely be(cid:173)
lieved  complexity-theoretic  assumption  that  the  polynomial 
hierarchy does not collapse.4 

2ln the technical portion of this paper, this definition will be cap(cid:173)

tured formally by the 

rcducibility. 

3Here, by "exponential" we mean exceeding every polynomial 

infinitely often. 

4The polynomial hierarchy is a collection of complexity classes 
which includes P, NP, co-NP, and other classes which are in essence 
generalizations of these three classes.  For the intents and purposes 
of this paper, this assumption can be thought of as being similar to 
the (perhaps better known) assumption that P does not equal NP (In 

KNOWLEDGE  REPRESENTATION 

455 

Compilability classes.  The initial papers that demonstrated 
non-compilability  results  [Kautz  et  at.,  1995;  Gogic  et  al, 
1995;  Khardon  and  Roth,  1996;  Cadoli  et al.,  1996;  1997; 
1999]  were  based  on  ad  hoc  proofs,  each  of  which  iso(cid:173)
lated  two particular formalisms and then demonstrated non-
In  [Cadoli  et  al,  2000a; 
compilability  of one  to  the  other. 
2000b],  new  complexity  classes  measuring  compilability, 
called compilability classes,  were introduced; for every clas(cid:173)
sical  complexity  class  C,  it  is  possible  to  define  a  compil(cid:173)
ability class analog of C.  The theory of compilability classes 
made  it  possible  to  systematize  proofs  of non-compilability 
much  in  the  way  classical  complexity  theory  makes  it  pos(cid:173)
sible  to  systematize  proofs  of intractability.  A  proof that  a 
language  is  NP-complete  is  demonstration  that  there  is  no 
polynomial-time algorithm for the language, and that the lan(cid:173)
guage has the same time complexity (up to a polynomial) as 
all other NP-complete languages. Likewise, a proof that a for(cid:173)
malism  is complete for the compilability class  analog of NP 
is demonstration that it is not compilable to any formalism in 
the compilability class  analog of P,  and that the language is 
compilable to and from all other formalisms complete for the 
compilability class  analog of NP.5  (More generally,  when  C 
is a class from the polynomial hierarchy, a proof that a for(cid:173)
malism is complete for the compilability class analog of C is 
demonstration  that  it  is  not compilable  to  any  formalism  in 
the compilability class  analog of C",  if C"  is  below  C  in  the 
polynomial  hierarchy.) 

In  addition  to  providing  a  methodology  for  comparing 
formalisms  with  respect  to  compilability,  the  compilability 
classes capture formally the notion of off-line preprocessing. 
Preprocessing a knowledge base off-line can be of great util(cid:173)
ity  if the  resulting,  processed  knowledge  base  is  in  a  form 
that allows for quick, on-line response to queries (and if many 
queries are expected).  Membership of a formalism  A  in  the 
compilability class analog of P will mean that any knowledge 
base x of A can be preprocessed into a form that does not un(cid:173)
reasonably increase the size of x,  but permits queries to x to 
be processed efficiently (that is, in polynomial time). 

The  compilability  classes  are  part  of a  formal  framework 
for  discussing  compilability,  which  includes  robust  notions 
of reduction  and completeness.  Not  only  is  this  framework 
extremely appealing from a theoretical point of view, but the 
compilability classes are rife with natural complete problems 
- the sine qua non of complexity classes purporting to be use(cid:173)
ful in performing problem classification. Indeed, there do not 
appear to be any knowledge representation formalisms which 
defy classification as complete for a compilability class.  On 
the downside,  many  of the  existing classification  results are 
quite negative, showing that a formalism is complete for (the 
compilability class analog of) NP, coNP, or a higher level of 
the polynomial hierarchy. 

fact, if the polynomial hierarchy does not collapse, then P does not 
equal NP.) 

5Our discussion presumes that the polynomial hierarchy does not 

collapse. 

1.2  Motivations and Approach 
Worst-case  versus  average-case.  Although  crisp  theo(cid:173)
retical  results  can  be  obtained  concerning  compilability 
and  non-compilability,  these  are  worst-case  notions:  non-
compilability  of  formalism  B  to  formalism  A  implies  that 
there is an  infinite family of knowledge bases in B  that can(cid:173)
not be succinctly translated into knowledge bases in  A.  Non-
compilability does not say anything about the density or fre(cid:173)
quency of instances from the family of untranslatable knowl(cid:173)
edge  bases.  This  observation  calls  into  question  the  real-
world  utility  of  studying  compilability  (as  defined  above); 
if  formalism  B  is  compilable  to  formalism  A  for  all  but  a 
pathological  family  of knowledge  bases  arising  infrequently 
in  practice,  then  formalism  B  is,  for  pragmatic  purposes, 
compilable to  formalism A.  Therefore,  a theory  which  per(cid:173)
mits formal results of compilability on average is necessary. 
This  paper lays  the foundations for a  theory of average-case 
compilability. 

Notice that our objection to the worst-case nature of non-
compilability  is  not  truly  novel.  It  has  long  been  observed 
that NP-completeness of a  language L  does  not imply  hard(cid:173)
ness of L on typical or real-world instances.  Our objection is 
really this old observation, masquerading in the new context 
of  compilability. 

One  theory  that  was  developed  in  response  to  this  old 
observation  is  the  theory  of  average-case  time  complexity 
(ACTC),  initiated  by  Levin  [1986].  Our  theory  of average-
case  compilability  will  be  built  on  a  key  notion  of ACTC -
that of "polynomial on  average."  Moreover, there are useful 
analogies between our theory and the theory of ACTC.  Con(cid:173)
sequently, we provide a brief overview of ACTC. 

Average-case time  complexity.  By  definition,  a  language 
L (consisting of strings) is in P if there is an algorithm decid(cid:173)
ing  membership for L  in  polynomial time.  The  idea behind 
ACTC is to relax the requirement in this definition that a suit(cid:173)
able  algorithm  is  one  that  always  runs  in  polynomial  time; 
this is done by placing a probability distribution on all strings 
and allowing an algorithm to take super-polynomial times on 
unlikely strings. 

A  language  paired  with  a probability  distribution  over all 
strings  is  called  a  distributional  language.  While  languages 
are the objects classified by classical complexity theory, dis(cid:173)
tributional  languages are  the objects classified  by  ACTC.  A 
distributional  language 
is  in  the average-case version 
of P,  average-P,  if there  is  an  algorithm  deciding  member(cid:173)
ship  for L  in  time polynomial on 
- a concept to be 
discussed formally later in this paper. 

There is a vast literature on ACTC; for more information, 
we recommend the overviews/surveys [Johnson, 1984; Gure-
vich,  1989;  1991a;  1991b; Impagliazzo,  1995; Wang,  1997; 
Goldreich, 1997] as starting points. 

Average-case  compilability. 
In  laying  down  a  theory  for 
average-case compilability, we want to "soften" the definition 
of "formalism  A  is compilable  to  formalism  J3" by relaxing 
the requirement that there needs to be a translation of knowl(cid:173)
edge bases of strictly polynomial size.  This is done roughly in 

456 

KNOWLEDGE  REPRESENTATION 

analogy to the described development of ACTC. We first de(cid:173)
fine a distributional formalism to be a formalism paired with 
a probability distribution over all knowledge bases.  Then, a 
is  (informally) compilable on 
distributional formalism 
average  to  a formalism  B  if there  is  a translation  of knowl(cid:173)
edge bases of size polynomial on 

Using  the  notion  of  "compilable  on  average,"  we  de(cid:173)
fine  average-case  analogs  of  compilability  classes.  These 
average-case  analogs  contain  distributional  formalisms,  in 
contrast to the compilability classes themselves,  which con(cid:173)
Intuitively,  membership  of  a  distri(cid:173)
tain  pure  formalisms. 
butional  formalism 
in  the  average-case  compilability 
class  analog  of P  will  mean  the  following:  any  knowledge 
base x of A can be preprocessed in a way that tends not to in(cid:173)
crease the size of x by more than a polynomial (with respect 
to U), and that allows for rapid processing of queries to x. 

2  Preliminaries 
In this section, we present notation and assumptions that will 
be used throughout the paper. 

We assume 

to  be  a  fixed  finite  alphabet which  is  used 
to form strings.  We will  at times assume that pairs of strings 
(that is, elements  of 
are represented as strings (that 
is,  elements  of 
;  when this assumption is  made,  we  as(cid:173)
sume  that  the  representation  is  via  a  pairing  function 
such that the length of (x, y)  is linear in 

For a string 
that is, the length of x written 

denote 

in unary notation. 

We  assume  that the  reader has  familiarity  with  basic  no(cid:173)
tions  of computational  complexity  theory -  in  particular,  the 
classes of the polynomial hierarchy (P, NP, CO-NP, 
[Balcdzar 
etc.)  and the polynomial many-one reduction 
et  al.,  1995].  A  language is  a subset  of 
,  that is,  a set of 
strings.  A  complexity  class  is  a  set  of languages.  When  C 
is a reduction, we say that C is 
is a complexity class and 
compatible  with 
B  and 
B 
C.  We  will  assume throughout that 
every complexity class C  is compatible with  the polynomial 
We say that a language B is com(cid:173)
many-one reduction 
plete for a complexity class  C under 
reductions  if  B  C 
and for all 

if for all  languages A  and  B,  A 

C  imply  that  A 

A  function 

is polynomial-size  if there  exists 

a  polynomial P such  that  for  all 
is  polynomial-time  computable  if 
A  function  /  : 
there exist a polynomial p and a Turing machine M such that 
for all 
,  the Turing machine M, on input x, produces 
f(x)  in  time  less  than 
Definition 2.1  A knowledge representation formalism (KRF) 
is a subset  of 
we let 
Fx  denote the set 
Intuitively, each 

When F is a KRF and 

can be thought of as a knowledge 
The  following 

base  (KB), representing the  information 

6In the technical portion of this paper, this definition will be cap(cid:173)

tured formally by the 

reducibility. 

7Notice  that  every  polynomial-time  computable  function  is 
is 

polynomial-size,  but  not  every  polynomial-size 
polynomial-time computable. 

function 

are examples of KRFs which capture model checking and in(cid:173)
ference for 3-SAT formulas. 
Propositional-Logic-MC 
{(x, y)  :  x is a 3-SAT formula and y is a model of x} 
Clause-Inference 

: x is a 3-SAT formula, y is a 3-clause, and 

Notice the generality of the definition of a KRF; the knowl(cid:173)
may be models, formulas, 

edge represented (the various 
or some altogether different combinatorial structures. 

3  Compilability classes and reductions 
This  section  reviews  the  theory  of  compilability  classes. 
There  are  two  different  types  of compilability  classes,  uni(cid:173)
form and non-uniform, but our focus is on the latter, for rea(cid:173)
sons discussed below.  We emphasize that none of the defini(cid:173)
tions,  theorems,  or insights  in  this section are our own,  but 
rather, are due to [Cadoli et al., 2000a; 2000b], on which our 
presentation is based. 

The  formal  definition  of  non-uniform  compilability  may 
by  itself  look  non-intuitive,  so  we  first  attempt  to  describe 
some  of the  ideas  behind  this  definition,  before  giving  the 
actual  definition.  Intuitively,  we  want  to  say  that  a  KRF  F 
is  compilable  if membership queries 
can be decided 
efficiently after the KB x is preprocessed into a new KB / ( x ). 
We want to constrain the size of the new 
; otherwise, 
it may be prohibitively large to store. We arrive at a candidate 
definition of compilability:  say  that a KRF F is  in  COmp-C 
if  for  some  polynomial-size  function  /  and  a  second  KRF 

the property 

for all pairs 

if and only  if 

holds.  Notice that the class C constrains the difficulty of de(cid:173)
ciding  a  post-processing  query 
of course,  such 
queries can be decided efficiently when C  =  P.  However, to 
permit fine classification of KRFs, we leave C as a parameter 
to the definition of COmp-C. 

The  above  definition  misses  one  important  detail  -  non-
If  F  is  a  KRF  such  that  Fx  is  always  a  finite 
uniformity. 
set,  F can never be complete for comp-C when C is a class 
of  the  polynomial  hierarchy  above  P.  At  the  same  time, 
there are  KRFs  with  this  "finiteness" property that are prov-
ably  not  in  comp-P.  As  an  example,  Clause-Inference  is 
in  COmp-CO-NP,  but  is  neither  comp-CO-NP-complete  nor 
in comp-P.8  Hence,  the classes comp-C fail to capture the 
compilability of some very natural KRFs.  (For more informa(cid:173)
tion on these matters, as well as proofs of the claims, we refer 
the reader to [Cadoli et al, 2000a].)  To resolve this issue, we 
allow the  length  of a  query  to  be  known  to  the  compilation 
mapping  /. 
Definition 3.1  (nu-COmp-C)  Let  C  be  a  complexity  class. 
A  KRF  F  belongs  to  nu-COmp-C  if there  exists  a  binary 
polynomial-size  function 
and a  KRF F'  in 
C  such  that  the following property  holds: 

8This discussion  presumes  that the polynomial  hierarchy does 

not collapse. 

KNOWLEDGE  REPRESENTATION 

457 

The fact that the compilation mapping may use the length 
of y may  seem  a  bit  strange;  after all,  we  wish  to  capture, 
after preprocessing on  a  knowledge base  x,  the difficulty  of 
answering  queries  of the  form  y 
Fx.  However,  it  is  rea­
sonable  to  assume  that  we  will  never be  interested  in  such 
a  query  when  the  length  of  y  greatly  exceeds  that  of  x  -
since  such  a query  requires  a  large  amount  of time  to even 
write down.  Under this assumption, Definition 3.1  is equiv­
alent  to  the  "uniform" definition  given  above.  (Formally,  if 
there exists a polynomial p such that (x, y) 
F  implies  that 
\y\ 
then  F  is  in  nu-comp-C  if and  only  if F  is  in 
p(\x\), 
comp-C.) 

The  following  notion  of  reduction,  associated  with  the 
nu-COmp-C classes, allows one to compare the compilability 
of different  KRFs. 
Definition 3.2  (nu-comp  reducibility)  A  KRF  F  is  nu-comp 
reducible to a  KRF F'  (denoted by 
if there 
exist  binary  polynomial-size  functions  fi,f'2  • 
and a  binary polynomial-time  computable  function g : 

such that the following property' holds: 

for  all  pairs 

if and  only  if 

Theorem 3.3  The  nu-comp  reduction  is  transitive  and  is 
compatible  with  the  class  nu-comp-C  (for  every  complexity 
class C). 

We  note  that  Clause-Inference  is  nu-comp-co-NP-

complete, under nu-comp reductions. 

Finally,  we  observe  that  the  nu-comp-C  analog  of  the 

polynomial hierarchy does not collapse. 
Theorem 3.4  lfC\  and C>L  are classes of the polynomial hi(cid:173)
erarchy such  that  C2  is  higher than  C1,  then  nu-COmp-C\ 
is properly contained in nu-COmp-C2 (under the assumption 
that the polynomial hierarchy does not collapse). 

4  Average-case compilability 
In  this  section,  we  present  our  new  theory  of average-case 
compilability. 
Definition 4.1  A  probability  distribution 
valued 

is  a 
such 

real-
that 

function 

from 

The nu-comp-C classes are used to classify and compare 
KRFs;  our new classes will be used to classify and compare 
what we call distributional KRFs. 
Definition 4.2  A  distributional KRF (DKRF) is a pair (F, u) 
consisting  of a  KRF  F  and a probability distribution  u. 

We now define the notion of "polynomial on average.1' This 
definition  is  exactly  that  used  in  the  theory  of average-case 
time  complexity  (up  to  appropriate  changes  of  the  domain 
and range of the functions whose size we wish to measure). 

ready 

We  are  now 

"average-
compilability" classes; formally, each class is a set of DKRFs. 
The definition can be viewed as a relaxation of Definition 3.1. 
The difference is that the translation mapping 

to  define  our  new 

need no longer be of strictly polynomial size,  but is now 

only required to be of size polynomial on average. 
Definition 4.4  (avg-nu-COmp-C)  Let  C  be  a  complexity 
class.  A  DKRF  (F, p)  belongs  to  avg-nu-COmp-C  if there 
exists a binary  function 
of size  polynomial 
on  p-average  and a  KRF  F'  in  C  such  that  the property  of 
Definition  3.1  holds. 

There  is  an  alternative  way  to  define  avg-nu-COmp-C, 
in  terms  of  the  following  type  of  reduction,  which  relates 
DKRFs to KRFs. 
Definition 4.5  (dist-nu-comp  reducibility)  A  DKRF  (F, p) 
is  dist-nu-comp 
(denoted  by 
F
there 
f u n c t
time  computable function 

to  a  KRF  F' 
binary  polynomial-size 

such  that 
is of size polynomial on  -average*9  and the property 

i o n s a nd  a  binary  polynomial-

reducible 

exist 

if 

of Definition  3.2  holds. 
Theorem 4.6  Suppose that 
complexity  class.  The  DKRF 
if 
if  and  only 
nu-comp-C. 

is a DKRF and that C is a 
is  in  avg-nu-COmp-C 
in 

F'  for  some  F' 

This  theorem  has  an  important  corollary:  if nu-comp-C 
has  a  complete  KRF  F' 
then  the  DKRFs  contained  in 
avg-nu-comp-C  are exactly those which reduce to F'.  In or­
der to establish the corollary, the following lemma is needed. 
Lemma 4.7  Suppose that 
is  a  DKRF and that F'  and 
F"  are  KRFs.  If 
F",then 

Roughly, Lemma 4.7 can be viewed as a proof of transitiv­

ity:  i f r e d u c es to F'  and  F'  reduces to F",  then 
reduces to 
(under the right notions of reduction). 
Corollary 4.8  Suppose  that 
is  a  DKRF  C  is a  com(cid:173)
plexity  class  and  the  KRF  Ff  is  nu-COmp-C-complete  un(cid:173)
der  nu-comp  reductions.  Then,  the  DKRF 
is  in 
avg-nu-comp-C  if and  only 

if 

It  is  worth  noting  here  that  the  class  nu-comp-C  has  a 
complete  KRF under  nu-comp reductions  whenever the  un­
derlying complexity class  C has a complete language under 

reductions  fCadoli  et al,  2000a]. 

We  now give  a  notion  of reduction  for the comparison  of 
DKRFs.  When u and v are probability distributions,  we say 
that v dominates p if there exists a polynomial p such that for 
all 
is 
Definition  4.9  (avg-nu-comp reducibility) A  DKRF 
if there exist a nu-
avg-nu-comp reducible to a  DKRF 
comp reduction  ( / 1,  f2,  g)from  F  to Ff and a probability dis(cid:173)
tribution 
where  the  sum  is  over all  (x, /)  such  that [f\(x, I)  =  y  and 
there exists 

dominating  p  such  that 

458 

KNOWLEDGE  REPRESENTATION 

Theorem  4.10  The  avg-nu-comp  reduction  is  transitive  and 
is  compatible  with 
the  class  avg-nu-COmp-C  (for  every  com-
plexity  class  C). 
5  Discussion 
We  now  discuss  some  of the  considerations  behind  the  defi­
nitions  in  the  previous  section.  When 
is  a  probability  distribution  and  S  is   a  subset  of 
let 
is,  the  function  defined  on  

on  5,  that 
,  with  value  equal  to 

denote  the  conditional  distribution  of  

•  Why  is  Levin  s  notion  of  polynomial-on-average  used  in­
stead  of  the  naive  formulation  of  expected  polynomial  size? 

According  to  the  naive  formulation,  a  function  /  : 

has  expected  polynomial  size  (with  respect  to  μ)  if 

there exists  k 

1  such  that for all  n, 

It  is  well  known  that  this  notion  of  expected  polynomial  is 
not  closed  under  polynomials  (see  for  example  [Goldreich, 
1997]).  For instance,  there  exist functions 

and  a  distribution 

and 
/  has  expected  polynomial  size,  but  /'  fails  to  have  expected 
polynomial  size. 

such  that 

In  our  average-case  compilability  theory,  this  "lack  of clo­
sure  under  polynomials"  problem  manifests  itself  in  the  fol­
lowing  way.  Suppose  that,  using  the  alternative  definition  of 
avg-nu-comp-C  given  in  Theorem  4.6,  the  K RF  F'  is  wit­
in  avg-nu-comp-C, 
ness  to the membership of  D K RF  
'.  Surely,  if F"  is  at  least  as 
that  is, 
space  efficient as  F'  (that is, 
, then one ex­
pects  that  F"  would  also  witness  the  membership of (F,  μ)  in 
avg-nu-comp-C;  this  is  the content of Lemma 4.7,  on  which 
Corollary  4.8  relies.  However,  if  expected  polynomial  size 
is  used  instead  of  polynomial  on  average  in  Definition  4.3, 
Lemma  4.7  breaks  down  -  then,  there  would  exist  a  D K RF 
(F,  μ)  and  KRFs  F'  and  F"  such  that 
F\ 
reduce  to  F". 

does  not 

F",  and 

F' 

•  Why  does  a  DKRF  have  one  probability  distribution  -
-  as  opposed  to  an  ensemble 
is  over  some 
finite  subset  of 

over  all  elements  of 
of distributions,  each  of  which 

Defining  a  D K RF  to  be  a  K RF  paired  with  an  ensemble 
of distributions  (each  of which  is,  say,  defined  on  a  different 
string  length)  may  seem  more  natural  than  the  given  defini­
tion,  where there  is only one distribution, on all  strings. 

However,  the  literature  on  ACTC  contains  many  alterna­
tive  formulations  of  "polynomial  on  average"  (equivalent  to 
that  given  in  Definition  4.3)  and  sufficient  conditions  for  a 
function  to be  polynomial  on  average.  (For example,  there  is 
a  different  formulation  of  Definition  4.3  in  terms  of ensem­
bles  of distributions,  each  having  finite  support,  in  [Impagli-
azzo,  1995].)  These  give  rise  to  alternative  formulations  of 
avg-nu-comp-C,  and  so  the  particular  characterizations  we 
give for avg-nu-comp-C (in Definition 4.4 and Theorem 4.6) 
were,  in  some  sense,  chosen  over provably equal  characteri­
zations  on  purely  aesthetic  grounds. 

•  Why  is  a  probability  distribution  over  E*  x  1 *  as  opposed 
to  over  E*  ?  After  all,  the  knowledge  bases  are  represented  by 
elements  of  ,  and  an  element  of  ' E*  x  1*  is  just  a  "slice" 
of a  knowledge  base. 

Let  F  be  a  KRF  conforming  to  the  "bounded-query-
length"  assumption  discussed  in  Section  3,  that  is,  suppose 
that  there  exists  a  polynomial p  such  that  (x, y)  E  F  implies 
|y|  <  P(IxI)-  Then, a distribution over E*  naturally  induces a 
distribution  over  E*  x  1*  for the  padded  version  of F  defined 
as  F'  = 
:  z  -  yDk,  \z\  =  p(\x\),  (x,y)  €  F]  (where 
□ is  an extra padding symbol),  which has the property that all 
strings  in  Fx.  have  the  same  length,  for any  x.  Note that all  of 
the  example  KRFs  in  this  paper already  have  this  property. 

{(x,z) 

6  Example:  model checking for 

circumscription 
this  section,  we 

illustrate 

formulas 

the  use  of  our  new 

In 
the­
ory  by  showing  that  model  checking  for  circumscription 
on  3-SAT 
(formalized  below  and  denoted  by 
Circumscription-MC(d))  is in avg-nu-comp-P,  under a nat­
ural  probability  distribution  where  formulas  are  generated 
by  including  each  clause  independently  with  identical  prob­
ability.  Since  Circumscription-MC(d)  is  nu-comp-co-NP-
hard,10  this  result  gives  a  natural  D K RF  which  is  contained 
in  avg-nu-COmp-P,  but  which  has  a  K RF  which  is  provably 
not  in  nu-COmp-P  (Theorem  3.4). 
is  a  3-SAT 

formula  over 

Suppose 

that 

o

-> 

{ 0 , 1} 
models 
S]  implies  that  

vn],  and  5  is  a  subset  of  Vn.  We  say  that  a  model 

{v1,..., 
a:Vn 
other 
s 
In  addition, 
a model a  :  S 
is  said  to  be  a  minimal  model  if 
it  is  the  restriction of a S-minimal  model  a  :  Vn  -->  { 0 , 1}  (of 

{ 0 , 1}  of  

for  all 

for 

all 

b 

if 

f' 

:i 

We  define  Circumscription-MC(d)  to  be  the  set 

is  a  3-SAT formula on  variable  set  on  Vn  and 

is a minimal  model  of 

Let 

Wn 

set 

: 
is 

'  —> 

denote 

the 
is  a  3-SAT formula on  Vn  and a 
{ 0 , 1}  is an assignment)},  which 
tactically  well-formed  pairs  that  may  or  may  not  be 
Circumscription-MC(d)-
Theorem  6.1  Let  vn,c  denote  the  distribution  on  3-SAT  for(cid:173)
mulas  over  Vn  where  a  formula 
including 
independently  with  probability 
each  of  the  8(n/3)  3-clauses 
0  and  natural  numbers 
(0, (0,1); be any dis(cid:173)

the  set  of  all  syn­
in 

real  numbers  c  > 

(with  c  >  0  and  d 

is  generated  by 

r  all 
Let 
tribution  such 

• 

• 

[dn\. 

that 

for  all  n,  and 

=  0 

i f is  a  3-SAT  formula  on 

10This can be shown using a result in [Gogic et  al,  1995]. 
11  Here,  denotes the usual total ordering on {0,1} where 

KNOWLEDGE  REPRESENTATION 

459 

This theorem is established using techniques from proba(cid:173)
bilistic  combinatorics.  It  is  worth  noting  that  at  c  =  3.73, 
the expected number of satisfying assignments that a random 
3-SAT formula has is exponential. 

7  Conclusions and future work 
In [Papadimitriou, 1996], Papadimitriou writes: 

The  ultimate  and  most  conclusive  criterion  for 
comparing knowledge representation formalisms is 
to compare their expressive power not on arbitrary 
sets of models, but on the "interesting" sets of mod(cid:173)
els, the ones that come up in the "real world."  We 
still hope that a meaningful and convincing formu(cid:173)
lation of this important problem may be possible. 

In this paper, we presented a robust and  flexible  theory of 
average-case  compilability  in  which  the  intuitive  notion  of 
"interesting  sets  of models"  can  be  formalized  as  a  DKRF 
(Definition  4.2),  DKRFs  can  be  classified  (Definition  4.4), 
and  DKRFs  can  be  compared  (Definition  4.9).  We  illus(cid:173)
trated the use of this theory by showing that, under a natural 
probability distribution,  model  checking  for circumscription 
is compilable to the analog of the complexity class  P,  in our 
theory. 

There are many open questions for future investigation; to 

conclude the paper, we list a few. 

•  Can  additional  DKRFs  be  classified  as  being  inside 
avg-nu-comp-P?  We  conjecture  in  particular  that 
Circumscription-MC(d)  - along with the "standard" 3-
SAT probabilistic model with 
for a sufficiently 
low c - is in avg-nu-comp-P. 

, 

•  Are there DKRFs complete for avg-nu-comp-C (where 
C  is  from  the polynomial  hierarchy) under the avg-nu-
comp reduction, or some other notion of reduction com(cid:173)
patible with the avg-nu-comp-C classes? 

•  Can any questions concerning the new avg-nu-comp-C 
to  better  known 

classes  defined  here  be  related 
complexity-theoretic  hypotheses? 

References 
[Balcazar  et  al.,  1995]  J.  L.  Balcazar  and  J.  Diaz  and  J. 
Gabarr6. Structural Complexity I.  Springer-Verlag, Berlin, 
1995. 

[Cadoli et a/., 2000a]  Marco  Cadoli,  Francesco  M.  Donini, 
Paolo  Liberatore,  and  Marco  Schaerf.  Preprocessing 
of  intractable  problems. 
Information  and  Computation, 
176(2):89-120,2000. 

[Cadoli et al,  1999]  Marco  Cadoli,  Francesco  M.  Donini, 
Paolo Liberatore, and Marco Schaerf. The size of a revised 
knowledge base.  Artificial Intelligence,  115(1), 25-64. 

[Cadoli et al, 2000b]  Marco  Cadoli,  Francesco  M.  Donini, 
Paolo  Liberatore,  and  Marco  Schaerf.  Space  Efficiency 
of Propositional  Knowledge  Representation  Formalisms. 
Journal  of  Artificial  Intelligence  Research,  13:1-31,2000. 
[Cadoli et al.,  1997]  Marco  Cadoli,  Francesco  M.  Donini, 
Marco Schaerf, and Riccardo Silvestri.  On Compact Rep(cid:173)
resentations of Propositional Circumscription.  In Theoret(cid:173)
ical Computer Science,  182:183-202. 

[Cadoli et al,  1996]  Marco  Cadoli,  Francesco  M.  Donini, 
Is  Intractability  of Non-Monotonic 
Artificial  Intelligence, 

and  Marco  Schaerf. 
Reasoning  a  Real  Drawback? 
88:215-251. 

[Gogic et al.,  1995]  Goran Gogic, Henry Kautz, Christos Pa(cid:173)
padimitriou, and Bart Selman.  The Comparative Linguis(cid:173)
tics  of Knowledge Representation.  In  Proceedings of the 
14th  International  Joint  Conference  on  Artificial  Intelli(cid:173)
gence, pages 862-869, Montreal, Canada, 1995. 

[Goldreich, 1997]  Oded Goldreich. Notes on Levin's Theory 
of  Average-Case  Complexity.  Electronic  Colloquium  on 
Computational Complexity (ECCC)  4(58):  1997. 

[Gurevich, 1991a]  Yuri  Gurevich.  Average  case  complete(cid:173)
ness.  Journal of Computer and System Sciences, 42:346-
398,1991. 

[Gurevich,  1991b]  Yuri Gurevich.  Average case complexity. 
In  Proceedings  of the  18th  International  Colloquium  on 
Automata,  Languages  and  Programming,  volume  510  of 
Lecture Notes in Computer Science, Springer, pages 615-
628,1991. 

[Gurevich,  1989]  Yuri  Gurevich. 

The  challenger-solver 
game:  variations on the theme of P =?  NP?  EATCS Bul(cid:173)
letin, pages 112-121, 1989. 

[Impagliazzo,  1995]  Russell  Impagliazzo.  A  personal  view 
In  Proceedings  of the  10th 
of average-case  complexity. 
Conference  on  Structure  in  Complexity  Theory,  IEEE 
Computer Society Press, pages  134-147,  1995. 

[Johnson,  1984]  David Johnson.  The NP-completeness col(cid:173)
umn: an ongoing guide. Journal of Algorithms, 5:284-299, 
1984. 

[Kautz et al,  1995]  Henry  Kautz, Michael  Kearns, and Bart 
Selman.  Horn approximations of empirical data.  Artificial 
Intelligence,  75:129-145. 

[Khardon and Roth,  1996]  Roni  Khardon  and  Dan  Roth. 
Reasoning  with  Models.  Artificial  Intelligence  87:187-
213, November 1996. 

[Levin,  1986]  Leonid Levin.  Average case  complete prob(cid:173)

lems.  SIAM Journal on  Computing,  15:285-286,  1986. 

[Papadimitriou,  1996]  Christos  Papadimitriou.  The  Com(cid:173)
plexity of Knowledge Representation.  In  Proceedings of 
the  Eleventh  Annual IEEE  Conference  on  Computational 
Complexity,  pages  244-248,  Philadelphia,  Pennsylvania, 
May  1996. 

[Wang,  1997]  Jie Wang.  Average-case computational com(cid:173)
plexity theory.  (L.  Hemaspaandra and A.  Selman,  eds.), 
Complexity  Theory  Retrospective  II,  Springer-Verlag, 
pages 295-328,1997. 

460 

KNOWLEDGE  REPRESENTATION 


Quantifying information and contradiction in propositional logic 

through test actions 

Sebastien Konieczny 

IRIT - UPS 

Jerome Lang 
IRIT - UPS 

F-31062 Toulouse - France 

F-31062 Toulouse - France 

konieczny@irit.fr lang@irit.fr marquis@cril.univ-artois.fr 

Pierre Marquis 

CRIL - Universite d' Artois 
F-62307 Lens - France 

Abstract 

Degrees of information and of contradiction are in(cid:173)
vestigated within  a uniform propositional  frame-
work, based on test actions.  We consider that the 
degree of information  of a propositional  formula 
is based on the cost of actions needed to identify 
the truth values of each atomic proposition, while 
the degree of contradiction of a formula is based 
on the cost of actions needed to make the formula 
classically consistent. Our definitions are to a large 
extent independent of the underlying propositional 
logic; this flexibility is of prime importance since 
there is no unique, fully accepted logic for reason(cid:173)
ing under inconsistency. 

Introduction 

1 
Information and contradiction are two fundamental aspects 
of knowledge processing.  Quantifying them is an important 
issue when reasoning about beliefs (or preferences) stemming 
from one or different sources. Here are some contexts where 
quantifying information and contradiction is relevant: 
•  diagnosis and testing.  In model-based diagnosis, some 
initial assumptions that each component works correctly are 
made; these assumptions may conflict with actual observa(cid:173)
tions.  Measuring the conflict of the resulting base may be a 
good hint about how hard it will be to identify the faulty com(cid:173)
ponents. 
• preference elicitation. In the interactive process of elicitat-
ing the preference profile of an individual (user) about a set 
of possible alternatives, it is not unfrequent that contradictory 
preferences arise. In this situation, it is useful to quantify and 
localize the contradictions as well as the information about 
the user's preferences, so as to be in position to choose the 
next questions to ask. 
•  belief merging. In this framework, degrees of information 
and contradiction can be the basis on which one can decide 
whether to take or not into account the data conveyed by an 
agent.  If the degree of contradiction of the data given by an 
agent is high, it may be relevant to reject the information, 
since there is a significant evidence that the source is not reli(cid:173)
able; however, this must be balanced by the quantity of infor(cid:173)
mation furnished by the agent, especially when she also gives 
some important and uncontroversial pieces of information. 
•  group decision making.  Contradictions arise frequently 

when trying to reach a compromise among several agents who 
have possibly conflictual preferences about a common deci(cid:173)
sion (like voting, resource sharing, public goods buying). In 
this context, not only it is relevant to compute a global degree 
of conflict (among the set of agents) but also degrees of con(cid:173)
flicts associated with small groups of agents (coalitions) so as 
to localize as precisely as possible where the conflicts are. 

Now, what do "degree of information" and "degree of con(cid:173)
tradiction" mean? There is no consensus about it. The main 
features shared by existing definitions (and there are not nu(cid:173)
merous, cf. Section 7) is that (1) such degrees are numerical 
values, and (2) they vary depending on the representation lan(cid:173)
guage. Thus, one may consider a as fully informative in the 
case where a is the single atomic proposition of the language 
but surely not fully informative when the vocabulary also con(cid:173)
tains b (provided that a and b are independent propositions). 
In  this  paper,  our  point  of view  is  that  it  is  inadequate 
to quantify the information/contradiction conveyed by some 
data without considering at the same time a set of available 
actions and a goal to be reached.  Accordingly, our degrees 
of information and contradiction are defined in an "active" 
way.  Acting so as to reduce inconsistency or to gain infor(cid:173)
mation often relies on performing knowledge-gathering ac(cid:173)
tions (also called tests).  We consider that the degree of in(cid:173)
formation of an information base is based on the number (or 
the cost) of actions needed to identify the truth value of each 
atomic proposition (the lower the cost the more informative 
the base);  and that the degree of contradiction of an infor(cid:173)
mation base is based on the number (or the cost) of actions 
needed to render the base classically consistent.  Thus, both 
degrees are dependent on the language but also on the given 
set of tests and the way plans costs are computed. 

The rest of this paper is organized as follows.  After some 
formal preliminaries in Section 2, we present our framework 
in Section 3.  In order to show the generality of our frame-
work,  we instantiate it to three different propositional  log(cid:173)
ics: classical logic (Section 4), the paraconsistent logic LPm 
(Section  5)  and  a  syntax-based approach to  inconsistency 
handling (Section 6). Related work is given in Section 7, and 
some conclusions in Section 8. 

2  Formal preliminaries and notations 
We consider a propositional language Lps based on a finite 
set of propositional symbols PS and a set of connectives that 
may vary depending on the logic used. Well-formed formulas 

106 

BELIEF REVISION AND UPDATE 

BELIEF REVISION AND UPDATE 

107 

a  plan  is  defined  as  the  maximum  cost  among  its  trajecto(cid:173)
ries);  this  principle,  consisting  in  assuming  the  worst  out(cid:173)
come,  is  known  in decision theory as  Wald criterion.  Other 
criteria could  be  used  instead,  such  as  the  optimistic  crite(cid:173)
rion  obtained  by  replacing  max  by  min.  More  interesting, 
the  criterion  obtained  by  first  using  max  and  then  min  for 
tie-breaking, or the leximax criterion,  allow  for a  better dis(cid:173)
crimination  than  the pure pessimistic  criterion.  The  choice 
of a criterion is fairly independent from the other issues dis(cid:173)
cussed in this paper, which gives our framework a good level 
of  flexibility  and generality.  Due to  space  limitations,  how(cid:173)
ever, we consider only the pessimistic criterion in  the rest of 
the paper. 

This example also shows that mere expansion is not a very 
satisfying revision operator.  Indeed, since it does not enable 
to  purify  any  inconsistent  base  (whatever  the  test  context), 
expansion does not enable as well to disambiguate any incon(cid:173)
sistent base.  Furthermore,  it may  lead to degrees of contra(cid:173)
diction  (or purification costs)  that are not intuitively correct. 

son of this discrepancy between what is expected and what is 
achieved  is  that expanding an inconsistent information base 
always leads to an inconsistent base, while it would be neces(cid:173)
sary to restore consistency2 for achieving purification and dis(cid:173)
ambiguation in classical logic. Note that using AGM revision 
instead of expansion would not help a lot since AGM opera(cid:173)
tors do not behave well when applied to inconsistent bases. 

108 

BELIEF REVISION AND UPDATE 

5  Case study 2:  the paraconsistent logic LPm 
Paraconsistent logics  have been  introduced to  avoid  exfalso 
quodlibet sequitur of classical  logic,  hence  handling  incon(cid:173)
sistent  bases  in  a  much  more  satisfying  way.  While  many 
paraconsistent logics have been defined so far and could be 
used  in  our framework,  we  focus  here  on  the  LPm  logic  as 
defined in [Priest, 1991]. This choice is mainly motivated by 
the fact that this logic is simple enough and has an inference 
relation that coincides with classical entailment whenever the 
information base is classically consistent (this feature  is  not 
shared by many paraconsistent logics). 

6  Case study 3:  "syntax-based" information 

bases 

BELIEF REVISION AND UPDATE 

109 

of information is general enough to recover classical entropy, 
applied to classical logic4. 

Lozinskii  f 1994a]  gives a set of properties that a measure 
of quantity  of information  should  satisfy.  Our  degree  of ig(cid:173)
norance is  fully compatible with Lozinskii's requirements in 
several  cases.  The  degree of information defined  by  Lozin(cid:173)
skii corresponds to the notion in Shannon's theory, assuming 
a uniform distribution over the  set of propositional  interpre(cid:173)
tations5.  It  is  thus  required  that  the  input  information  base 

[Knight,  2003]  reports  some  other postulates  for  a  mea(cid:173)
sure  of quantity  of  information.  Our  measure  d1  does  not 
satisfy  all  of them,  even  in  simple cases  (for space  reasons, 
we cannot detail  it here).  This  contrasts with  the  two mea(cid:173)
sures  introduced  by  Knight,  which  generalize  in  an  elegant 
way Shannon's entropy-based measure to the case the  infor(cid:173)
mation base is an inconsistent set of formulas. However, both 
measures trivialize when the  information  set  is  an  inconsis(cid:173)
tent singleton. 

The  only  two  approaches  we  are  aware  of,  which  con(cid:173)
sider  (non-trivial)  degrees  of inconsistency  defined  for clas-

7  Related work 
To the best of our knowledge, only few proposals for a notion 
of degree of information  can  be  found  in  the  literature,  and 
things are even worse to what concerns the notion of degree 
of contradiction. All existing approaches are stuck to specific 
propositional logics with the corresponding consequence re(cid:173)
lations,  which  address only some  aspects  of the paraconsis-
tency issue, if any (as evoked previously, there is no undebat-
able paraconsistent inference relation). 

Shannon's  information  theory  [Shannon,  1948]  provides 
the  most  famous  approach  on  which  notions  of quantity  of 
information  can  be  defined,  but  it  relies  on  the  assumption 
that  the  available  information  is  given  under  the  form  of a 
probability  distribution;  furthermore,  it  cannot  directly  ad(cid:173)
dress inconsistent data.  Interestingly, our definition of degree 

110 

BELIEF REVISION AND UPDATE 

References 
[Alchourron et  al,  1985]  C.  E.  Alchourron,  P.  Giirdenfors,  and 
D.  Makinson.  On  the  logic  of  theory  change:  Partial  meet 
contraction  and  revision  functions.  Journal  of Symbolic  Logic, 
50:510-530,  1985. 

[Besnard and Hunter,  1995]  P.  Besnard  and  A.  Hunter.  Quasi-
classical  logic:  Non-trivializable classical reasoning from incon(cid:173)
sistent information.  In ECSQARU'95, pages 44-51,  1995. 

[Brewka,  1989]  G. Brewka. Preferred subtheories: an extended log(cid:173)
ical  framework  for default  reasoning.  In  IJCAI'89,  pages  1043-
1048, 1989. 

[Ginsberg and Smith,  1988]  M. Ginsberg and D. E. Smith. Reason(cid:173)
ing  about  action  I:  A  possible  world  approach.  Artificial Intelli(cid:173)
gence, 35:165-195,  1988. 

[Hintikka,  1970]  J.  Hintikka.  On  semantic  information.  Informa(cid:173)

tion and Inference, pages 3-27,  1970. 

I Hunter, 2000]  A.  Hunter.  Reasoning  with conflicting  information 
using  quasi-classical  logic.  Journal  of Logic  and  Computation, 
10:677-703, 2000. 

[Hunter, 2002]  A.  Hunter.  Measuring  inconsistency  in  knowledge 

via quasi-classical models.  In  AAAI'02,  pages 68-73, 2002. 

[Hunter, 2003]  A.  Hunter.  Evaluating  significance  of inconsisten(cid:173)

cies.  In IJCAI'O3 (this issue), 2003. 

[Kemeny,  1953]  J.G.  Kemeny.  A logical measure function. Journal 

of Symbolic Logic,  18:289-308,  1953. 

[Knight, 2002]  K.M.  Knight.  Measuring  inconsistency.  Journal of 

Philosophical  Logic,  31:77-98,  2002. 

I Knight, 2003]  K.M.  Knight.  Two  information  measures  for  in(cid:173)
consistent  sets.  Journal  of Logic,  Language  and  Information, 
12:227-248, 2003. 

[Lang and Marquis,  1998]  J. Lang and P. Marquis.  Complexity re(cid:173)
In 

sults for independence  and definability  in propositional  logic 
KR' 98, pages 356-367,  1998. 

[Lang and Marquis, 2002]  J.  Lang  and  P.  Marquis.  Resolving  in(cid:173)
consistencies  by  variable  forgetting.  In  KR  02,  pages  239-250, 

[Lozinskii,  1994aJ  E. Lozinskii.  Information and evidence in logic 
systems.  Journal  of Experimental  and  Theoretical  Artificial  In(cid:173)
telligence, 6:163-193,  1994. 

[Lozinskii,  1994b]  E. Lozinskii.  Resolving contradictions:  a plau(cid:173)
sible  semantics  for  inconsistent  systems.  Journal  of Automated 
Reasoning, 12:1-31, 1994. 

[Nebel,  1991]  B.  Nebel.  Belief  revision  and  default  resoning: 

syntax-based approaches.  In KR'91, pages 417-428,  1991. 

[Poole,  1988]  D. Poole.  A logical framework for default reasoning. 

Artificial  Intelligence,  36:27-47,  1988. 

[Priest,  1991]  G. Priest.  Minimally inconsistent LP.  Studia Logica, 

50:321-331,  1991. 

[Rescher and Manor,  1970]  N. Rescher and R. Manor. On inference 
from  inconsistent  premises.  Theory  and  Decision,  1:179-219, 
1970. 

[Shannon,  1948]  C.E.Shannon.  A mathematical theory of commu(cid:173)
nication.  Bell Systems  Technical Journal,  27:379-423,623-656, 
1948. 

[Wong and Besnard, 2001]  P. Wong and P. Besnard.  Paraconsitent 
reasoning  as  an  analytic  tool.  Journal  of the  Interest  Group  in 
Propositional  Logic,  9:233-246,  2001. 

8  Conclusion 
The  main  contribution  of the  paper  is  a  uniform  action-based 
framework  for quantifying both  degrees of information  and  of 
contradiction.  The  framework  is  parameterized by  a proposi(cid:173)
tional  logic  (together  with  the  corresponding  notions  of con(cid:173)
sequence,  acceptance, contradiction  and a  revision  operator), 
a  test  context and an  aggregation criterion  for computing plan 
costs.  These  parameters  enable  a  great  flexibility. 

There  are  many  interesting  notions  that  can  be  easily  de(cid:173)
fined  in  our  framework  but  that  we  cannot  mention  here  for 
space  reasons.  Let  us  note  that  through  the  notion  of purifi(cid:173)
cation  plan,  our  approach  for  quantifying  contradiction  also 
allows  to  localize  conflicts.  Note  also  that  notions  of joint  de(cid:173)
grees  and  conditional  degrees  of  information  /  contradiction 
can  be  easily  defined.  Another  simple  extension  would  con(cid:173)
sist  in  taking  advantage  of  additional  knowledge  about  the 
sources  of  information  and  the  origin  of  conflicts  (e.g.,  in  a 
diagnosis  setting,  it can  be  the  case  that  the  failure  of a com(cid:173)
ponent  physically  causes  the  failure  of other components). 

Many  other extensions  of our approach  can  be  envisioned. 
For  instance,  coping  with  preferences  over the  goal  variables 
(determining  whether  a  holds  is  more  important  than  deter(cid:173)
mining  whether  b  holds).  Another  possible  extension  con(cid:173)
cerns the case where ontic actions arc available and the objec(cid:173)
tive  is  to  let  the  actual  world  as  unchanged  as  possible  (i.e., 
we  can  execute invasive  actions  but  we  prefer not  to  do  it). 

BELIEF REVISION AND UPDATE 

111 


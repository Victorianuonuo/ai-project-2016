Qualitatively  Faithful  Quantitative  Prediction 

Dorian Sue, Daniel Vladusic, Ivan Bratko 

Faculty of Computer and Information  Science, University of Ljubljana 

Trzaska 25, 1000 Ljubljana, Slovenia 

{dorian.sue, ivan.bratko, daniel.vladusic}@fri.uni-lj.si 

Abstract 

In this paper we describe a case study in which we 
applied an approach to qualitative machine learning 
to  induce,  from  system's  behaviour  data,  a  qual(cid:173)
itative  model  of a  complex,  industrially  relevant 
mechanical  system  (a  car  wheel  suspension  sys(cid:173)
tem).  The  induced qualitative model  enables nice 
causal  interpretation  of the  relations  in  the  mod(cid:173)
elled  system.  Moreover,  we  also  show  that  the 
qualitative model can be used to guide the quantita(cid:173)
tive modelling process leading to numerical predic(cid:173)
tions that may be considerably more accurate than 
those  obtained by  state-of-the-art numerical  mod(cid:173)
elling methods.  This idea of combining qualitative 
and quantitative machine learning for system iden(cid:173)
tification is in this paper carried out in two stages: 
(1)  induction  of qualitative  constraints  from  sys(cid:173)
tem's behaviour data, and (2) induction of a numer(cid:173)
ical regression function that both respects the qual(cid:173)
itative constraints and fits the training data numer(cid:173)
ically.  We  call  this  approach  Q2  learning,  which 
stands for Qualitatively faithful Quantitative learn(cid:173)
ing. 

Introduction 

1 
It  is  generally accepted that qualitative models are easier to 
understand and reason about than quantitative models. Qual(cid:173)
itative models thus provide a better basis for the explanation 
of phenomena in a modelled system than numerical models. 
In this paper we  describe  a  case  study  in which  we applied 
an approach to qualitative machine learning to induce, from 
system's behaviour data, a qualitative model of a complex, in(cid:173)
dustrially relevant mechanical system (a car wheel suspension 
system).  The induced qualitative model enables nice causal 
interpretation of the relations among the variables in the sys(cid:173)
tem. This is precisely as one would expect from a qualitative 
model. More surprisingly, however, we also show in this case 
study that the qualitative model can be used to guide the quan(cid:173)
titative modelling process that may lead to numerical predic(cid:173)
tions that are considerably more accurate than those provided 
by state-of-the-art numerical modelling methods. 

Thus the main message of this paper is that a combination 
of methods for qualitative and quantitative system identifica(cid:173)

tion has good chances to attain significant improvements over 
numerical  system  identification  techniques,  including  tech(cid:173)
niques of numerical  machine  learning methods,  such as  re(cid:173)
gression trees  [Breiman et ai,  1984] and model trees  [Quin-
lan,  1992].  The potential improvements are in two respects: 
first, the predictions are qualitatively consistent with the prop(cid:173)
erties of the modelled system,  and  in addition they  are also 
numerically more accurate. 

This  idea  of combining  qualitative  and  quantitative  ma(cid:173)
chine learning for system identification is in this paper carried 
out  in  two  stages.  First,  induce qualitative  constraints  from 
system's  behaviour data (training data)  with program QUIN 
(overviewed in  Section  3).  Second,  induce a  numerical  re(cid:173)
gression function that both respects the qualitative constraints 
and fits well the training data numerically (called Qualitative 
to Quantitative transformation or Q2Q for short, described in 
Section 4).  We call this approach Q2  learning, which stands 
for Qualitatively faithful  Quantitative learning.  To underline 
the importance of qualitative  fidelity,  we illustrate in  Section 
2 some problems that numerical learners typically have in re(cid:173)
spect of qualitative consistency.  In  Section  5  we present the 
case study in applying Q2 to the chosen problem of modelling 
car suspension. 

There are several approaches to learning qualitative models 
from numerical data that may support alternative approaches 
to Q2  learning.  These  include the program QMN  [Dzeroski 
and Todorovski,  1995],  QSI  [Say and Kuru,  1996],  SQUID 
[Kay et al,  2000], and QOPH [Coghill et  al,  2002]. 

2  Qualitative difficulties of numerical learning 
Consider a simple container of cylindrical  shape and a drain 
at  the  bottom. 
If we  fill  the  container with  water,  the  wa(cid:173)
ter drains out.  Water level  monotonically decreases,  until  it 
reaches zero.  Suppose we  fill  the container with water,  and 
measure initial  outflow 
and the time behaviour 
of water  level 
.  Since this is a rather simple behaviour, 
one would naturally expect that machine learning techniques 
should be able to fairly  well predict time behaviour of water 
level  if enough  learning examples  are  given.  Quite  surpris(cid:173)
ingly,  even  in  such  simple  cases,  the  usual  numerical  pre(cid:173)
dictors can  give  strange  and qualitatively  unacceptable pre(cid:173)
dictions.  We  illustrate  the  problems  with  a  simple  experi(cid:173)
ment, using well-known techniques for numerical prediction: 
model trees and locally weighted regression. 

1052 

QUALITATIVE  REASONING 

Figure  1:  M5 predictions of water outflow:  empty circles are 
M5  predictions  of level 
=  7.5. 
Other dots are the the learning examples.  Note that M5 pre(cid:173)
dicts that water level increases at time 

on a  test set  with 

In  our  experiment  we  used  container  outflow  simulation 
data to evaluate how different numerical predictors learn the 
time behaviour of water level.  The outflow from a container 
where  c  is  a  parameter depending  on  the 
is 
area ot the drain.  For the simulation we used Euler integra(cid:173)
tion 
seconds and 
We used six example sets,  generated with different 
6. 
corresponding 

initial water levels and initial  outflows 
An example set has 20 examples 
to  19 seconds of simulation. 

where 

We  used  the  Weka  [Witten  and  Frank,  2000J  implemen(cid:173)
tations of locally weighted regression  lAtkeson et  al,  1997] 
(LWR, for short), and M5 regression and model trees  [Quin-
lan, 1992] to learn the time behaviour of level given the initial 
outflow, i.e. 
One example set was used as a 
test set and other five sets (100 examples) for learning. When 
the test set was the set with 
M5 with the default pa(cid:173)
rameters induced a model tree with 9 leaves.  Figure 1 shows 
the learning examples and the M5 prediction of level 
on 
a test set.  Note that M5 predicts that water level increases at 
time 
The same happens if we  change the pruning pa(cid:173)
rameter. This is of course qualitatively unacceptable as water 
level can never increase. Also, there are no learning examples 
where water level increases. 

One might think that this is  an  isolated weird case or M5 
bug.  But  it  is  not.  LWR  makes  a  similar  qualitative  error 
on the test set with 
=  11.25,  when  it predicts that water 
Of course, LWR predictions depend on 
level increases at 
i.e.  the number of neighbors used, but often 
its parameter 
the appropriate 
that doesn't give  qualitative errors on  one 
container, gives qualitative errors when learning from similar 
data  but  with  different  area  of the  drain.  Often,  the  errors 
are even more obvious if we make predictions at the edges of 
the space covered by learning examples, i.e.  using as test set 
=  12.5.  As one would expect,  regression 

=  6.25  or 

trees make similar qualitative errors. 

We  believe  that  other  numerical  predictors  make  similar 
qualitative  errors,  at  least  in  more  complex  domains.  This 
might be quite acceptable in applications where we just want 
to minimize numerical prediction  errors.  But often  it  is also 
important to respect qualitative relations that are either given 

Figure 2:  A  qualitative tree  induced  from  a  set  of examples 
for the  function  Z  =  X2  -  Y2.  The  rightmost leaf,  applying 
when attributes A'  and Y  are positive, says that Z  is strictly 
increasing in its dependence on A'  and strictly decreasing in 
its dependence on}'. 

in  advance  or  hidden  in  the  data. 
Ignoring them  results  in 
clearly unrealistic predictions that a domain expert would not 
approve.  The idea of this paper is to use such qualitative re(cid:173)
lations, either given or induced from data, not only to avoid 
qualitatively incorrect predictions, but also to improve the ac(cid:173)
curacy of numerical prediction. 

3  Qualitative data mining with QUIN 
QU1N  (Qualitative  Induction)  is  a  learning  program  that 
looks  for qualitative  patterns  in  numerical  data  [Sue,  2001; 
Sue and Bratko, 2001; 2002]. Induction of the so-called qual(cid:173)
itative trees is similar to induction of decision trees.  The dif(cid:173)
ference  is  that  in  decision trees the  leaves  are  labelled with 
class  values,  whereas  in  qualitative  trees  the  leaves  are  la(cid:173)
belled with what we call qualitatively constrained functions. 
Qualitatively constrained functions (QCFs  for short) are a 
kind  of monotonicity constraints  that  are  widely  used  in  the 
field  of qualitative reasoning.  A  simple  example of QCF  is: 
Y  =  A/+ (A").  This says that Y is a monotonically increasing 
function of A.  In general, QCFs can have more than one ar(cid:173)
gument.  For example,  Z  =  M+ , -( A,  Y)  says that Z mono(cid:173)
tonically  increases  in  A  and  decreases  in  Y.  We  say  that 
Z  is  positively  related  to  X  and  negatively related  to  Y  If 
both A  and  Y  increase,  then  according to this  constraint,  Z 
may increase, decrease or stay unchanged.  In such a case, a 
QCF cannot make an unambiguous prediction of the qualita(cid:173)
tive change in  Z. 

QUIN takes as input a set of numerical examples and looks 
for  qualitative  patterns  in  the  data.  More  precisely,  QUIN 
looks for regions in the data space  where monotonicity con(cid:173)
straints hold. Such a set of qualitative patterns are represented 
in terms of a qualitative tree.  As in decision trees, the inter(cid:173)
nal nodes in a qualitative tree specify conditions that split the 
attribute space into subspaces.  In a qualitative tree, however, 
each leaf specifies a QCF that holds among the input data that 
fall  into that leaf.  For example, consider a set of data points 
(Xy,Z)  where  Z  =  X2  -  Y2  possibly  with  some  noise 
added.  When QUIN is asked to find in these data qualitative 
constraints on  Z as a function of  X  and Y,  QUIN  generates 
the qualitative tree shown in Figure 2.  This tree partitions the 
data space into four regions that correspond to the four leaves 
of the tree. A different QCF applies in each of the leaves. The 
tree describes how Z qualitatively depends on A  and Y. 

QUIN  constructs  a  tree  in  the  top-down  greedy  fashion, 

QUALITATIVE  REASONING 

1053 

similarly to decision tree induction algorithms.  A more elab(cid:173)
orate  description  of the  QUIN  algorithm  and  its  evaluation 
on a set of artificial domains is given elsewhere  [Sue, 2001; 
Sue and Bratko, 2001 ]. QUIN has been applied to qualitative 
reconstruction of human control  strategies  [Sue,  2001],  and 
to reverse engineer a complex industrial controller for gantry 
cranes [Sue and Bratko, 2002]. 

4  Q2Q transformation 

In  this  section  we  describe  the  qualitative-to-quantitative 
transformation (Q2Q for short). Given a set of numerical data 
and a qualitative tree, Q2Q attempts to find a regression func(cid:173)
tion that fits the data well numerically, and also respects the 
qualitative constraints in the tree.  We say that such a regres(cid:173)
sion function is qualitatively consistent.  In fact, Q2Q finds a 
qualitatively consistent regression function with  good  fit  for 
each leaf in the tree separately.  The overall regression func(cid:173)
tion is then obtained by gluing together the regression func(cid:173)
tions for the leaves. 

The Q2Q procedure is as follows.  First,  we partition the 
learning examples  according to  the  leaves  of the  qualitative 
tree.  These subsets  are  then  used  for learning qualitatively 
consistent functions of the corresponding leaves. Let us focus 
on learning a qualitatively consistent function for some par(cid:173)
ticular leaf.  Suppose we have a leaf with the qualitative con(cid:173)
straint 
We then have to find some monotonically 
increasing  function  that  fits  the  data  well.  One  straightfor(cid:173)
ward solution, used in Q2Q, is to divide the range of variable 

with  a number of equidistant points  (i.e. 

in which we learn from the given data the function values y. 
The result is a set of pairs 
that defines 
a piece-wise linear function which can be easily checked for 
compliance with the given qualitative constraint.  This proce(cid:173)
dure  can be generalized to qualitative constraints of any di(cid:173)
mension. 

In  our  implementation  the  function  values  yi  were  deter(cid:173)
mined with a standard version of locally weighted regression 
(LWR)  [Atkeson et ai,  1997],  which used Gaussian weight(cid:173)
ing function.  Therefore,  the two parameters of the transfor(cid:173)
mation  were the number of equidistant points per dimension 
and the kernel size of the Gaussian weight(cid:173)
ing function 
All possible combina(cid:173)
tions  of these  two  parameters  (4*6=24)  define  the  space  of 
all candidate piece-wise linear functions for each leaf.  These 
sets of candidate functions are exhaustively searched by Q2Q 
to find functions that satisfy given qualitative constraints.  For 
each  leaf,  Q2Q  selects  among these  qualitatively consistent 
piece-wise linear functions one that has best fit with the data 
that fall into this leaf. Quality of the fit is measured with root 
mean squared error, RMSE for short.  It is theoretically pos(cid:173)
sible that none of the candidate functions is qualitatively con(cid:173)
sistent with the QCF in the leaf.  In such a case the Q2Q pro(cid:173)
cedure would fail to find a qualitatively consistent regression 
function.  Although in our experiments this never happened, 
we are working on an improved version of Q2Q that is guar(cid:173)
anteed to find a qualitatively consistent regression function. 

Figure 3:  Intec wheel model:  wheel position is given by 

and  coordinates of the  wheel  center,  and rotation angles 

called  enforced wheel-spin angle 

and 

about axes 
camber 
by horizontal forces 
rotational moment 

and  toe  angle  a,  respectively.  These  are  affected 
elevation of the  road R  and 

and 
that act upon the tyre. 

Intec case study 
Intec wheel model 

5 
5.1 
In this section we present an application of Q2 learning to the 
modelling  of car wheel  suspension  system.  This  is  a com(cid:173)
plex  mechanical  system  of industrial  relevance.  The  model 
and  simulation  software  used  in  this  experiment  were  pro(cid:173)
vided by a German car simulation company Intec.  The main 
role of the application in this paper is to provide a controlled 
experiment to assess the potentials of Q2  learning on a mod(cid:173)
elling problem  of industrial  complexity.  However,  although 
the target model  was already known and developing such a 
model  was  not  an  issue  of practical  relevance,  initially  this 
case  study was  nevertheless motivated by a practical  objec(cid:173)
tive.  Namely, the complexity of Intec's model is so high that 
it cannot be simulated on the present simulation platform in 
real time.  Therefore the practical objective of the application 
of Q2  learning  was  to  speed  up  the  wheel  simulation.  The 
goal thus was to obtain a simplified wheel model that would 
still be sufficiently accurate and at the same time significantly 
simpler than the original model to allow real-time simulation. 
Indeed,  the  simplified model  obtained  with  Q2  is  computa(cid:173)
tionally trivial compared with the original model. 

The Intec wheel model (shown in Figure 3) is a multi-body 
model  of a  front  wheel  suspension  built  in  compliance with 
the physical model assuming no car-body movement and no 
wheel-spin.  In fact, the suspension system is modelled as if 
the car-body is  fixed.  Wheel position is given by 
coordinates of the wheel center. Because the flexible joints in 
multi-body suspension system that links the wheel to the car-
body  allow  several  levels  of displacements,  rotation  angles 
are also measured.  These are called 
about axes 
enforced wheel-spin angle 
re(cid:173)
spectively. 

and toe angle 

camber 

and 

and 

The multi-body simulation software Simpack [Intec, 2002] 
was  used  to  set  up  the  model  and  to  generate  simulation 
traces.  During  simulation,  a  number  of elements  are  act(cid:173)
ing upon the tyre:  two horizontal forces 
vertical 
movement  (measured as  elevation  of the  road  R)  and rota-

and 

1054 

QUALITATIVE  REASONING 

Figure 5:  Induced qualitative tree for  wheel center position. 

test traces have the same road profile as the traces used for 
learning,  but  different profiles of other three  input variables, 
In  the  first  trace  all  of the  three  input 
i.e. 
variables change similar as 
in the trace in Figure 4.  This 
trace was recommended as the most critical test trace by the 
domain expert, who considered it far more difficult (all 4 in(cid:173)
put  variables  change)  than  other 6  test  traces  where  one  or 
two input variables were always zero. 

Inducing a  qualitative  wheel  model  w i th  Q U IN 
5.3 
As  described  above,  QUIN  was  used  to  induce  a  qualita(cid:173)
tive tree for each of the six  output variables, where the input 
variables  were  the  attributes.  All  of the  induced  qualitative 
trees  had  over 99  %  consistency  on  the  learning  set  of ex(cid:173)
amples.  We  say  that  a  QCF  is  consistent  with  an  example 
if the  QCF's  qualitative  prediction  of the  dependent variable 
does  not  contradict the  direction  of change  in  the  example. 
The  level  of consistency of a  qualitative tree  with  the  exam(cid:173)
ples  is  the  percentage  of the  examples  with  which  the  tree 
is consistent.  Consistency of 99% indicates that the  induced 
qualitative model fits the data nearly perfectly. 

The simplest qualitative tree was induced for translation in 

the  axis.  This tree only has one leaf with QCF 
This tree has a simple and obvious explanation. It says that 
changes in the direction of the road change.  If road increases 
then z increases, i.e. the wheel center moves upwards. 

and 

Qualitative  trees  for translations  in  and 

axes are a bit 
more complicated.  Since they have similar explanations we 
will  present just the qualitative tree  for 
translations, given 
in Figure 5. Note that 
is measured in the opposite direction 
as usual, i.e. positive  means wheel center moving in the di(cid:173)
rection of car driving backwards. Both leaves of the tree have 
the same qualitative dependence on 
but  differ  in 
qualitative dependence on road  R.  The qualitative tree says 
that x  is positively related to force 
that acts in the direc(cid:173)
tion  of  Obviously, wheel center position  changes (wheel 
moves  backward  or  forward)  in  the  direction  of force  in 
direction.  Second, 
This 
means that if we push the wheels together (we apply force in 
the  direction), the wheels will move forward 
position de(cid:173)
creasing). This is not so obvious, but can be understood if we 
consider the usual mechanics of wheel suspension. The qual(cid:173)
itative dependence on road R is a bit more complicated.  The 
qualitative tree of Figure 5 says that 
is negatively related to 
R  when 
.  Otherwiseis positively  related to  R, 
When R increases from its minimum to its maximum,   will 
first decrease and than increase, i.e. the wheel center will first 
move forward and than backward. 

is negatively related to  force 

Rotations about axes 

and  are measured by enforced 
respectively.  For 
wheel-spin 
enforced wheel-spin  QUIN induced a simple one-leaf tree 
that says 
changes in the 

camber ,  and toe angle 

Note that 

Figure 4: A typical simulation trace of the Intec wheel model: 
the input variables are on the upper graph, the output variables 
that changes the same as road R) are on the lower 
(except 
graph,  axis is time in steps dt=0.7 seconds.  Note the com(cid:173)
plex behaviour of the output variables resulting from changes 
in Fx  and road R. 

tional  moment 
when braking, 
force) and rotational moment 

.  For example, 
is acting upon the tyre 
when driving through corners (centripetal 

when parking the car. 

During  the  simulation,  input  and  output  variables  are 
logged  to  a  file  called  simulation  trace.  We  used  traces  of 
wheel  simulation  with  different  trajectories  of  input  vari(cid:173)
ables.  Each  trace  lasted  for  70  seconds,  and  was  sampled 
with 
seconds.  In this way a trace gives 100 examples, 
each example contains 10 values, corresponding to the values 
of four input and six output variables at a given time.  Figure 
4 shows a typical simulation trace.  It should be noted that all 
these  traces  correspond to  very  slow  changes  of input  vari(cid:173)
ables, and as a result the traces are illustrative mainly of the 
kinematics of the mechanism, but not also of its dynamics. 

5.2  Details  of experiments 
The  experiments  reported  in  this  paper  were  done  using  a 
black-box  approach.  We  did  not use  any  knowledge of the 
model.  The  simulation  traces  were  provided  by  our  part(cid:173)
ners from Czech Technical University in the European project 
Clockwork. 

In all the experiments we used 7 traces for learning with the 
same road profile as in the trace of Figure 4. In the first learn(cid:173)
ing trace all other three input variables were zero. In the next 
three traces  two  of the  other three  input variables  were zero 
and one other variable 
was changing.  Figure 
4 shows one such trace. The remaining three traces were sim(cid:173)
ilar, but the trajectory of the changing variable was different, 
i.e.  it  first  increased,  stayed unchanged for 20 seconds,  and 
than slowly decreased to zero. Each trace gives 100 examples, 
giving altogether 700 learning examples with  10 variables. 

The task was to learn each of the six output variables as a 
function of input variables.  In this way we have six learning 
problems, where an output variable is the class and the input 
variables are the attributes. For example, angle a was learned 

The prediction accuracy was tested on 7 test traces. All the 

QUALITATIVE  REASONING 

1055 

Figure 6:  Induced qualitative tree for toe angle 

direction of the tyre rotation when driving forward. Consider 
that is positive 
for example the dependence  of  on force 
during braking. Since 
increasing 
to decrease, i.e.  during braking enforced wheel 
spin angle changes  in  the direction of the tyre rotation.  For 
camber angle  QU1N induced a qualitative tree that is similar 
to qualitative trees for  and 

is negatively related to 

translations. 

causes 

The toe angle 

i.e. the rotation about  -axis is effected by 
all input variables and is the most complicated.  The induced 
tree  is  given  in  Figure  6.  We  will  omit  explanation  of this 
qualitative  tree,  since  it requires  understanding of the  flexi(cid:173)
ble nature of the multi-body suspension system that links the 
wheel to the car-body. 

These  qualitative  trees  give  a  good  explanation  of wheel 
suspension system behaviour. Moreover, they provide a qual(cid:173)
itative model of wheel suspension system that enables quali(cid:173)
tative simulation.  In this way, they enable to predict all pos(cid:173)
sible qualitative changes of output variables over an arbitrary 
time  interval  given  qualitative  changes  of all  or  some  input 
variables. This qualitative model also enables to improve nu(cid:173)
merical predictions. 

5.4  Qualitative  correctness  of numerical  predictors 
In this section we illustrate why Q2  learning may have an ad(cid:173)
vantage over the usual numerical predictors.  Figure 7 shows 
predicted with  M5  model tree,  LWR and LWR with opti(cid:173)
mized  parameters,  on the  most critical  test trace,  where  all 
the  input variables are  changing simultaneously.  The figure 
shows that both M5  and LWR sometimes make large errors. 
Moreover these errors are not only numerical, but also qual(cid:173)
itative.  Consider for example the LWR predictions at the be(cid:173)
ginning of the trace.  Here the predicted 
is decreasing, but 
is  increasing.  This error could be avoided by 
the correct 
considering the  induced  qualitative tree  for 
given  in  Fig(cid:173)
ure 6.  Since at the beginning of the test trace road R is near 
zero and increasing, and all other input variables are zero, the 
second  leftmost leaf of the  qualitative tree  applies.  Its  QCF 
requires increasing a since road R is 

increasing. 

As can be observed in Figure 7, M5 and LWR often make 
qualitative  errors.  Q2  predictions  are  qualitatively  correct. 
The  use  of (induced)  qualitative  model  enables  Q2  to  better 
generalize in the areas sparsely covered by the training exam(cid:173)
ples, resulting in better numerical accuracy. 

5.5  Numerical  accuracy  of the  induced  models 
Here we compare the numerical accuracy of the Weka imple(cid:173)
mentations of LWR, M5 model trees and Q2  learning.  All the 

Figure  7:  LWR,  LWR  with  optimized parameters,  M5  and 
Q2  predictions  of  on the most critical test trace.  With each 
method, a at time step 
(on x-axis) was predicted according 
to the values of input variables at time 

in the test trace. 

Figure  8:  Comparing  accuracy  (measured  with  RMSE)  of 
Q2  and LWR with optimized parameters: the left graph com(cid:173)
pares  RMSE  on  the  most  difficult  test  trace  and  the  right 
graph shows  RMSE  on the remaining test traces. 

methods learned from 7 learning traces (also used for learn(cid:173)
ing of qualitative trees) and were tested against 7 test traces 
described in  Section 5.2.  The test results are divided in two 
groups.  The  first  group  consists  of results  on  a  single  test 
trace.  This trace was recommended as the most critical test 
trace by the domain expert, who considered it far more diffi(cid:173)
cult (all 4  input variables are changing simultaneously) than 
the 6 test traces in the second group. 

Learning of qualitatively consistent functions in the leaves 
was performed as described in Section 4.  The best  fitting  re(cid:173)
gression  functions  were  then  taken  and  glued  together  into 
the overall induced numerical model.  We compared the ac(cid:173)
curacy  of our  Q2  method  with  LWR  and  M5.  The parame(cid:173)
ters of LWR  were  optimized for each  output  variable 
...)  according to the  RMSE criterion, through internal cross-
validation on the training set.  When experimenting with M5, 
we noticed that it was grossly inferior both in terms of qual(cid:173)
itative acceptability as well as numerical error.  Attempts at 
optimizing M5's parameters did not help noticeably. 

Figure 8  gives the prediction accuracy for variables 
and 

,  The predictions of the remaining variables  and 

were not much affected by qualitative constraints. The results 
on the most difficult test trace  (left graph  in Figure 8)  show 
that  even  our  simple  Q2Q  method  improves  the  numerical 
prediction on all  the variables  (compared to  LWR).  Results 
on the second test trace group are given on the right graph in 
Figure  8.  As these  traces  were more similar to the  learning 
traces, the improvement of Q2  over LWR is smaller. 

1056 

QUALITATIVE  REASONING 

6  Discussion 
In this paper we introduced a new approach to machine learn(cid:173)
ing in numerical domains, which we call Q2  learning (quali(cid:173)
tatively faithful quantitative learning).  This combines the in(cid:173)
duction of qualitative properties from numerical data and nu(cid:173)
merical regression that respects the induced qualitative prop(cid:173)
erties.  We  showed  by  an  experimental  case  study  that  Q2 
learning may  lead to  the  following advantages compared to 
the usual numerical learning: 

(1) Induced models tend to be qualitatively consistent with 
the  data and  therefore  have  better chances to  correspond to 
the qualitative  mechanisms  in  the domain  of modeling.  For 
example, if the amount of water in a container is decreasing, 
the level of water cannot be increasing. This is important with 
respect to  the  interpretation of induced models and explana(cid:173)
tion of phenomena in the domain based on these models. 

(2) Qualitative  consistency  of induced models  with  learn(cid:173)
ing  data  also  affects  the  accuracy  of the  model's  numerical 
predictions:  numerical  accuracy  may  be  considerably  im(cid:173)
proved. This is illustrated by the experimental results. 

In  respect  of numerical  prediction  accuracy,  in  our  case 
study  Q2  overall  outperformed  all  competing  numerical 
learners.  Among these,  locally  weighted regression  (LWR) 
with optimized parameters (through  internal cross validation 
on the training set) performed best in terms of mean squared 
error.  However  its  performance  may  sharply  degrade  un(cid:173)
der  more  difficult  circumstances.  Consider  LWR-optimized 
performance  on  the  the  most  difficult  test  set  (Figure  7). 
It  achieves  excellent  accuracy  on  the  first  part  of this  trace 
which  is similar to data in the training sets.  LWR-optimized 
accuracy  there  is  actually  better  than  that  of Q2.  However, 
problems begin for LWR in the second part of this trace where 
the input variables start to deviate considerably from the train(cid:173)
ing data, and LWR's predictive error increases sharply.  In this 
part  of the  trace,  Q2  manages to  largely  preserve qualitative 
consistency with the true results, and maintains the numerical 
accuracy at a comparable level as  in the area densely popu(cid:173)
lated by training examples. 

LWR-optimized  was  the  best  among  standard  numerical 
learners,  and  therefore  our presentation  of experimental  re(cid:173)
sults  largely  concentrated  on  comparison  between  Q2  and 
LWR.  The  performance  of M5  was  grossly  inferior  both  in 
terms  of qualitative acceptability as  well  as numerical error. 
Optimizing M5's parameters did not help noticeably. 

It should be  noted that qualitatively  faithful  regression as 
carried out by the Q2Q program is  actually inferior to  LWR 
as a regression method.  Struggling to satisfy qualitative con(cid:173)
sistency, Q2Q is limited to piece-wise linear regression with 
a  small  number of linear segments.  This numerical  inferior(cid:173)
ity of Q2Q usually turns out to be more than compensated by 
preserving qualitative consistency. 

In this paper, qualitative constraints for Q2Q were induced 
from training data with QUIN. Alternatively, such constraints 
can be defined directly by a domain expert. In such a case, the 
Q'2 learning can be viewed as an approach that enables the use 
of expert's qualitative knowledge in system identification. 

Among  the  limitations  of our  realization  of Q2,  the  prim(cid:173)
itive numerical  regression method  in Q2Q should be noted. 

This method allows sharp changes in variable values (discon(cid:173)
tinuities in the variables'  derivatives) at the borders between 
leaves  of a  qualitative  tree.  Future  work  should  include  a 
method for smoothing such discontinuities. 

Acknowledgments 
The  work  reported  in  this  paper  was  partially  supported  by 
the  European  Fifth  Framework  project  Clockwork  and  the 
Slovenian  Ministry  of Education,  Science  and  Sport.  We 
thank  A.  Eichberger of Intec,  for  providing  the  wheel  sus(cid:173)
pension model and the  Simpack simulation software  for this 
study,  and  for helpful  notes  on  the  model.  M.Valasck  and 
RSteinbauer from Czech Technical  University also helped in 
the Intec case study. 

References 
[Atkesonc/tf/.,  1997]  C.G  Atkeson,  A.W  Moore,  and  SA. 
Schaal.  Locally  weighted  learning.  Artificial Intelligence 
Review, 11:11-73,1997. 

[Breiman et al,  1984]  L.  Breiman, J.H.  Friedman, R.A.  01-
shen, and C.J. Stone. Classification and Regression Trees. 
Wadsworth, Belmont, California, 1984. 

[Coghill?/tf/.,2002]  G.M.  Coghill,  Garrett  S.M.,  and  R.D. 
King.  Learning  qualitative  models  in  the  presence  of 
In  Proceedings of the  QR'02  Workshop  on  Qual(cid:173)
noise. 
itative Reasoning, 2002.  Sitges, Spain. 

[Dzeroski and Todorovski,  1995]  S.  Dzeroski and L. Todor-
ovski.  Discovering dynamics:  from  inductive  logic  pro(cid:173)
./.  Intell.  Information 
gramming  to  machine  discovery. 
Syst., 4:89-108, 1995. 

[Intec, 2002]  INTEC  GmbH.  Simpack  software.  2002. 

www.simpack.com. 

[Kay etai, 2000]  H.  Kay,  B.  Rinner, and B.  Kuipcrs.  Semi(cid:173)
quantitative  system  identification.  Artificial  Intelligence, 
119:103-140,2000. 

[Quinlan,  1992]  J.R.  Quinlan.  Leraning  with  continuous 
In  Proc.  5th  Australian  Joint  Conf.  on  Artificial 

classes. 
Intelligence, pages 343-348,  1992. World Scientific. 

[Say andKuru,  1996]  A.C.C.  Say and  S.  Kuru.  Qualitative 
system  identification:  deriving  structure  from  behavior. 
Artificial  Intelligence,  83:75-141,  1996. 

[Sue and Bratko, 2001]  D.  Sue  and  I.  Bratko.  Induction  of 
qualitative trees. In L. De Raedt and R Flach, editors, Proc. 
12th  European  Conference  on  Machine  Learning,  pages 
442-453. Springer, 2001. Freiburg, Germany. 

[Sue and Bratko, 2002]  D. Sue and I. Bratko.  Qualitative re(cid:173)
In  Proc.  19th  Int.  Conf  on  Machine 

verse  engineering. 
Learning, pages 610-617. Morgan Kaufmann, 2002. 

LSuc,  2001]  D.  Sue.  Machine  reconstruction  of  human  con(cid:173)
trol strategies. PhD thesis, Faculty of Computer and Infor(cid:173)
mation Sc, University of Ljubljana, Slovenia, 2001. 

[Witten and Frank, 2000] 

Witten  and  E.  Frank.  Data 
Mining:  Practical  Machine  Learning  Tools  and  Tech(cid:173)
niques  with Java  Implementations,  chapter 8,  pages 265-
320. Morgan Kaufmann, San Francisco, 2000. 

QUALITATIVE  REASONING 

1057 


Dynamic Interactions Between Goals and Beliefs

Steven Shapiro and Gerhard Brewka

Computer Science Inst.
University of Leipzig
Augustusplatz 10–11

04109 Leipzig, Germany

{shapiro,brewka}@informatik.uni-leipzig.de

Abstract

Shapiro et al. [2005; 2006], presented a framework
for representing goal change in the situation calcu-
lus. In that framework, agents adopt a goal when
requested to do so (by some agent reqr ), and they
remain committed to the goal unless the request is
cancelled by reqr . A common assumption in the
agent theory literature, e.g., [Cohen and Levesque,
1990; Rao and Georgeff, 1991], is that achieve-
ment goals that are believed to be impossible to
achieve should be dropped. In this paper, we incor-
porate this assumption into Shapiro et al.’s frame-
work, however we go a step further. If an agent be-
lieves a goal is impossible to achieve, it is dropped.
However, if the agent later believes that it was mis-
taken about the impossibility of achieving the goal,
the agent might readopt the goal. In addition, we
consider an agent’s goals as a whole when making
them compatible with their beliefs, rather than con-
sidering them individually.

1 Introduction

Shapiro et al. [2005; 2006], presented a framework for repre-
senting goal change in the situation calculus. In that frame-
work, agents adopt a goal when requested to do so (by some
agent reqr ), and they remain committed to the goal unless
the request is cancelled by reqr . A common assumption in
the agent theory literature, e.g., [Cohen and Levesque, 1990;
Rao and Georgeff, 1991], is that achievement goals that are
believed to be impossible to achieve should be dropped. In
this paper, we incorporate this assumption into Shapiro et
al.’s framework, however we differ from previous approaches
in two respects. If an agent believes a goal is impossible to
achieve, it is dropped. However, if the agent revises its be-
liefs, it may later come to believe that it was mistaken about
the impossibility of achieving the goal. In that case, the agent
should readopt the goal. To our knowledge, this has not been
considered in previous approaches. In addition, most frame-
works1 consider goals in isolation when checking compatibil-

1Bell and Huang [1997] consider the compatibility of all of an
agent’s goals with its beliefs, but they do not consider the possibility
of readopting a goal previously believed impossible.

ity with beliefs. However, it could be the case that each goal
individually is compatible with an agent’s beliefs, but the set
of all goals of the agent is incompatible with its beliefs.

In Sec. 2, we present the situation calculus and Reiter’s
action theories, which form the basis of our framework.
In Sec. 3, we present Shapiro et al.’s framework, and in
Sec. 4, we show how to extend the framework to take into
consideration the dynamic interactions between beliefs and
goals. Some properties of the new framework are presented
in Sec. 5. In Sec. 6, we sketch how to extend the framework
further so that achievement goals that are believed to have
been already achieved are dropped by the agents. We con-
clude in Sec. 7.

2 Representation of Action and Beliefs

The basis of our framework for goal change is an action the-
ory [Reiter, 2001] based on the situation calculus [McCarthy
and Hayes, 1969; Levesque et al., 1998]. The situation calcu-
lus is a predicate calculus language for representing dynam-
ically changing domains. A situation represents a possible
state of the domain. There is a set of initial situations cor-
responding to the ways the agents believe the domain might
be initially. The actual initial state of the domain is repre-
sented by the distinguished initial situation constant, S0. The
term do(a, s) denotes the unique situation that results from
the agent doing action a in situation s. Thus, the situations
can be structured into a set of trees, where the root of each
tree is an initial situation and the arcs are actions. The se-
quence of situations that precedes a situation s in its tree is
called history of s.
Predicates and functions whose value
may change from situation to situation (and whose last argu-
ment is a situation) are called ﬂuents. For instance, we might
use the ﬂuent INROOM(Agt, R1, S) to represent the fact that
agent Agt is in room R1 in situation S. The effects of actions
on ﬂuents are deﬁned using successor state axioms [Reiter,
2001], which provide a succinct representation for both ef-
fect axioms and frame axioms [McCarthy and Hayes, 1969].

We will be quantifying over formulae, so we assume that
we have an encoding of formulae as ﬁrst-order terms. As
shown by De Giacomo et al. [2000], this is laborious but
straightforward.
It includes introducing constants for vari-
ables, deﬁning substitution, introducing a Holds predicate to

IJCAI-07

2625

deﬁne the truth of formulae, etc. We assume we have such an
axiomatization, and so we will freely quantify over formulae
here (using ﬁrst-order quantiﬁers). To simplify notation, we
ignore the details of the encoding and use formulae directly
instead of the terms that represent them.

We will also be using lists of formulae, so we need an ax-
iomatization of lists. We do not present the details here but
such a formalization is well known. We use the functions
car(l), cdr(l), cons(ψ, l), reverse(l), and remove(ψ, l); and
the relation member(ψ, l) with their usual meanings. nil de-
notes the empty list. We will also use lists of formulae (with-
out repetitions) to represent ﬁnite sets of formulae, and there-
fore use ﬁnite sets when it is convenient, along with the usual
set functions and relations.

To axiomatize a dynamic domain in the situation calculus,
we use Reiter’s [2001] action theory, which consists of (1)
successor state axioms for each ﬂuent; (2) initial state axioms,
which describe the initial state of the domain and the initial
mental states of the agents; (3) unique names axioms for the
actions, and domain-independent foundational axioms (given
below); and (4) the axioms to encode formulae as terms, and
to deﬁne lists of (terms for) formulae.2

Unique names axioms are used to ensure that distinct ac-
tion function symbols denote different actions. For distinct
action function symbols, a1 and a2, we need an axiom of the
following form:3

Axiom 2.1

a1((cid:3)x) (cid:2)= a2((cid:3)y).

Also, for each action function symbol, a, we need an axiom
of the following form:

Axiom 2.2

a((cid:3)x) = a((cid:3)y) ⊃ (cid:3)x = (cid:3)y.

We want the situations to be the smallest set generated by
sequences of actions starting in an initial situation. We ax-
iomatize the structure of the situations with foundational ax-
ioms based on the ones listed in Levesque et al. [1998] for the
language of the “epistemic situation calculus”. We ﬁrst deﬁne
the initial situations to be those that have no predecessors:

We also need an axiom stating that do is injective.

Axiom 2.4

do(a1, s1) = do(a2, s2) ⊃ (a1 = a2 ∧ s1 = s2)

The induction axiom for situations says that if a property
P holds of all initial situations, and P holds for all successors
to situation s if it holds for s, then P holds for all situations.

Axiom 2.5
∀P.[(∀s.Init(s) ⊃ P (s)) ∧ (∀a, s.P (s) ⊃ P (do(a, s)))] ⊃

∀sP (s).

We now deﬁne precedence for situations. We say that s
strictly precedes s(cid:2) if there is a (non-empty) sequence of ac-
tions that take s to s(cid:2).

Axiom 2.6

∀s1, s2.s1 ≺ s2 ≡ (∃a, s.s2 = do(a, s) ∧ (s1 (cid:9) s)),

where s1 (cid:9) s2
s2.

def= s1 = s2 ∨ s1 ≺ s2 denotes that s1 precedes

Although belief change plays a large role in this paper,
the focus is on the goal change framework. Belief change
frameworks in Reiter’s action theory framework have been
developed [Shapiro et al., 2000; Shapiro and Pagnucco, 2004;
Shapiro, 2005], however we will not assume a particular
framework here.
Instead, we will make a few general as-
sumptions about the belief change framework as needed. In
particular, we assume a possible worlds approach (with B as
the accessibility relation) using situations as possible worlds.
The accessible situations are the ones that are used to deter-
mine the beliefs of the agent, as usual. These would cor-
respond to, e.g., the most plausible accessible situations of
Shapiro et al. [2000] or simply the situations that the agent
considers possible in a framework without plausibilities over
situations. Therefore, we assume that the language contains
a distinguished predicate B(agt , s(cid:2), s). We also assume that
the agents’ beliefs are always consistent:

Init(s(cid:2)) def= ¬∃a, s.s(cid:2) = do(a, s)

Axiom 2.7

We declare S0 to be an initial situation.

Axiom 2.3

Init(S0)

2Action theories normally also include axioms to specify the dis-
tinguished predicate Poss(a, s) which is used to described the con-
ditions under which it is physically possible to execute an action,
however to simplify notation, we omit the use of Poss here and as-
sume that it is always possible to execute all actions.

3We adopt the convention that unbound variables are universally

quantiﬁed in the widest scope.

∀s∃s(cid:2)B(agt , s(cid:2), s).

The beliefs of the agent are deﬁned as those formulae true in
all the accessible situations:

Bel(agt , φ, s) def= ∀s(cid:2).B(agt , s(cid:2), s) ⊃ φ[s(cid:2)].

Here, φ is a formula that may contain the distinguished con-
stant Now instead of its (ﬁnal) situation argument. φ[s] de-
notes the formula that results from substituting s for Now in
φ. When the intended meaning is clear, we may suppress this
situation argument of φ.

IJCAI-07

2626

3 Goal Change

Shapiro et al. [2005; 2006], presented a framework for repre-
senting goal change in the situation calculus. In that frame-
work, agents adopt goals when requested to do so (by some
agent reqr ) and they remain committed to their goals unless
the request is cancelled by reqr . One problem with this ap-
proach is that an agent will retain a goal even if it believes
the goal is impossible to achieve. We address this problem
here. We ﬁrst introduce Shapiro et al.’s [2006] framework,
and then show how it can be changed to better reﬂect the in-
tuitive interactions between beliefs and goals.

An agent’s goals are future oriented. For example, an agent
might want some property to hold eventually, i.e., the agent’s
goal is of the form Ev(ψ), for some formula ψ. We eval-
uate formulae such as these with respect to a path of situa-
tions rather than a single situation, and we call such formu-
lae goal formulae. Cohen and Levesque [1990] used inﬁnite
time-lines to evaluate such formulae, but for simplicity, we
evaluate goal formulae with respect to ﬁnite paths of situa-
tions which we represent by pairs of situations, (Now, Then),
such that Now (cid:9) Then. Now corresponds to the “current
time” on the path of situations deﬁned by the sequence of
situations in the history of Then. Goal formulae may con-
tain two situation constants, Now and Then. For exam-
ple, ∃r.INROOM(JOHN, r, Now)∧¬INROOM(JOHN, r, Then)
could be used to denote the goal that John eventually leaves
the room he is in currently. If ψ is a goal formula then ψ[s, s(cid:2)]
denotes the formula that results from substituting s for Now
and s(cid:2) for Then. When the intended meaning is clear, we may
suppress these situation arguments of goal formulae.

Following Cohen and Levesque [1990], Shapiro et al.
model goals using an accessibility relation over possible
worlds (situations, in our case). The accessible worlds are
the ones that are compatible with what the agent wants to be
the case. Shapiro et al.’s W accessibility relation, like the B
relation, is a relation on situations. Intuitively, W (agt , s(cid:2), s)
holds if in situation s, agt considers that in s(cid:2) everything that
it wants to be true is actually true. For example, if the agent
wants to become a millionaire in a situation S, then in all sit-
uations W -related to S, the agent is a millionaire, but these
situations can be arbitrarily far in the future.

Following Cohen and Levesque [1990], the goals of the
agent should be compatible with what it believes. The situ-
ations that the agent wants to actualize should be on a path
from a situation that the agent considers possible. Therefore,
the situations that will be used to determine the goals of an
agent will be the W -accessible situations that are also com-
patible with what the agent believes, in the sense that there
is B-accessible situation in their history. We will say that s(cid:2)
Bagt,s-intersects s(cid:2)(cid:2) if B(agt , s(cid:2)(cid:2), s) and s(cid:2)(cid:2) (cid:9) s(cid:2). We will
suppress agt or s if they are understood from the context.
Shapiro et al. deﬁne the goals of agt in s to be those formu-
lae that are true in all the situations s(cid:2) that are W -accessible
from s and that B-intersect some situation, s(cid:2)(cid:2):

GoalSLL(agt , ψ, s) def=
∀s(cid:2), s(cid:2)(cid:2).W (agt , s(cid:2), s) ∧ B(agt , s(cid:2)(cid:2), s) ∧ s(cid:2)(cid:2) (cid:9) s(cid:2) ⊃

ψ[s(cid:2)(cid:2), s(cid:2)].

Note that s(cid:2)(cid:2) corresponds to the “current situation” (or the
current time) in the path deﬁned by s(cid:2). We deﬁne a similar
accessibility relation C below and deﬁne Goal in the same
way but using C instead of W .

Shapiro et al. specify how actions change the goals of
agents. They do not give a successor state axiom for W di-
rectly, instead they use an auxiliary predicate, REQUESTED.
REQUESTED records which goals have been requested of and
adopted by an agent, as well as which situations should be
dropped from W to accommodate these requests. When a
request is cancelled, the corresponding goal (and dropped sit-
uations) are removed from the REQUESTED relation. A re-
quested goal is adopted by an agent if the agent does not cur-
rently have a conﬂicting goal. This maintains consistency of
goals. REQUESTED(agt , ψ, s(cid:2), s) holds if some agent has re-
quested that agt adopt ψ as a goal in situation s and this re-
quest causes agt to cease to want situation s(cid:2). Here is the
successor state axiom for REQUESTED:

Axiom 3.1

REQUESTED(agt , ψ, s(cid:2), do(a, s)) ≡
((∃reqr .a = REQUEST(reqr , agt , ψ) ∧

W −(agt, ψ, a, s(cid:2), s)) ∨
(REQUESTED(agt , ψ, s(cid:2), s) ∧
¬∃reqr .a = CANCELREQUEST(reqr , agt , ψ))),

where W − is deﬁned below. An agent’s goals are expanded
when it is requested to do something by another agent. After
the REQUEST(requester , agt , ψ) action occurs, agt should
adopt the goal that ψ, unless it currently has a conﬂicting goal
(i.e., we assume agents are maximally cooperative). There-
fore, the REQUEST(requester , agt , ψ) action should cause
agt to drop any paths in W where ψ does not hold. This
is taken into account in the deﬁnition of W −:

W −(agt , ψ, a, s(cid:2), s) def=

∃s(cid:2)(cid:2).B(agt , s(cid:2)(cid:2), s) ∧ s(cid:2)(cid:2) (cid:9) s(cid:2) ∧ ¬ψ[do(a, s(cid:2)(cid:2)), s(cid:2)].

According to this deﬁnition, s(cid:2) will be dropped from W ,
due to a request for ψ, if s(cid:2) B-intersects some s(cid:2)(cid:2) such that
ψ does not hold on the path (do(a, s(cid:2)(cid:2)), s(cid:2)). The reason that
we check whether ¬ψ holds at (do(a, s(cid:2)(cid:2)), s(cid:2)) rather than at
(s(cid:2)(cid:2), s(cid:2)) is to handle goals that are relative to the current time.
If, for example, ψ states that the very next action should be
to get some coffee, then we need to check whether the next
action after the request is getting the coffee. If we checked
¬ψ at (s(cid:2)(cid:2), s(cid:2)), then the next action would be the REQUEST
action.

We also have to assert that initially no requests have been

made. We do so with the following initial state axiom:

Axiom 3.2

Init(s) ⊃ ¬REQUESTED(agt , ψ, s(cid:2), s).

Shapiro et al. deﬁned W in terms of REQUESTED. s(cid:2) is
W -accessible from s iff there is no outstanding request that
caused s(cid:2) to become inaccessible.

W (agt , s(cid:2), s) def= ∀ψ.¬REQUESTED(agt , ψ, s(cid:2), s))

IJCAI-07

2627

4 Dynamic interactions between goals and

beliefs

where chooseMin is a function which takes a ﬁnite set of for-
mulae and returns an element of the set that is minimal in ≤:

A common assumption in the agent theory literature [Cohen
and Levesque, 1990; Rao and Georgeff, 1991] is that achieve-
ment goals that are believed to be impossible to achieve
should be dropped. However, we go a step further. If some
time later, an agent revises its beliefs and decides that the goal
is achievable after all, the agent should reconsider and possi-
bly readopt the goal. Also, the previous focus has been on
individual goals that are incompatible with an agent’s beliefs.
However, it could be the case that each goal is individually
compatible with an agent’s beliefs but the set of goals of the
agent is incompatible, so some of them should be dropped.

First, we make precise the notion of a ﬁnite set of goal
formulae being compatible with an agent’s beliefs. We say
that a ﬁnite set of goal formulae α is B-consistent in situation
s, if there exists a path (s(cid:2)(cid:2), s(cid:2)) such that s(cid:2)(cid:2) is B-accessible
from s, and none of the goals in α caused s(cid:2) to be dropped
from W .

BCons(agt , α, s) def=

∃s(cid:2), s(cid:2)(cid:2).B(agt , s(cid:2)(cid:2), s) ∧ s(cid:2)(cid:2) (cid:9) s(cid:2) ∧

∀ψ.ψ ∈ α ⊃ ¬REQUESTED(agt , ψ, s(cid:2), s).

If α is a singleton, we may replace it with its element.

To make its goals compatible with its beliefs, an agent
takes the set of requested formulae which may be B-
inconsistent and chooses a maximally B-consistent set to be
its goals. We assume that each agent has a preorder (≤)
over goal formulae corresponding to a prioritization of goals.
ψ ≤ ψ(cid:2) indicates that ψ has equal or greater priority than ψ(cid:2).
This ordering could be used to, e.g., represent that an agent
gives different priorities to requests from different sources,
or to give higher priority to emergency requests. The agent
chooses a maximally B-consistent subset of the requested for-
mulae respecting this ordering. To simplify notation, we ﬁx
here a single such ordering for all agents, but in practice dif-
ferent agents will have different orderings, and it is not difﬁ-
cult to generalize the deﬁnitions to accommodate this.

Let:

reqs(agt , s) def= {ψ | ∃s(cid:2)REQUESTED(agt , ψ, s(cid:2), s)},

denote the set of formulae that have been requested for agt in
situation s. Since there are no requests initially, and an action
adds at most one goal formula to the set of requests, it is easy
to see that this set is ﬁnite in any situation. Therefore, we
can consider the set of requests in a situation to be a list. The
list is sorted according to the priority ordering (≤), using the
recursively deﬁned function sort(α), which takes a ﬁnite set
α and returns a list of elements of α sorted according to ≤:4

Axiom 4.1

sort(α) = l ≡

if α = nil then l = nil else

l = cons(chooseMin(α),

sort(remove(chooseMin(α), α)),

4if P then A else B is an abbreviation for (P ⊃ A)∧(¬P ⊃ B)

Axiom 4.2

chooseMin(α) = x ⊃ ∀y ∈ α.y ≤ x ⊃ x ≤ y.

After the set of requests is sorted, a maximally B-consistent
sublist is selected that respects the ordering, using the func-
tion makecons, which is deﬁned using a recursively deﬁned
auxiliary function makecons(cid:2).5

Axiom 4.3
makecons(cid:2)(agt , l, s) = l(cid:2) ≡

if l = nil then l(cid:2) = nil else

if BCons(agt , cons(car(l),

makecons(cid:2)(agt , cdr(l), s)), s) then
l(cid:2) = cons(car(l), makecons(cid:2)(agt , cdr(l), s)) else
l(cid:2) = makecons(cid:2)(agt , cdr(l), s).

makecons(agt , l, s) = reverse(makecons(cid:2)(agt ,

reverse(l), s)).

In other words, the list α is checked starting from the end
to see if the last element is B-consistent with the result of
recursively making the rest of the list B-consistent. If it is B-
consistent, then it is added to the result, otherwise it is left out.
Finally, the resulting list is reversed to restore the ordering.

This list is used to deﬁne our accessibility relation for
goals. First, we deﬁne CHOSEN(agt , ψ, s(cid:2), s) (in analogy to
Shapiro et al.’s REQUESTED), which holds if ψ was chosen
by agt and that choice should cause s(cid:2) to be dropped from the
accessibility relation (i.e., REQUESTED(agt , ψ, s(cid:2), s) holds).

CHOSEN(agt , ψ, s(cid:2), s) def=

member(ψ, makecons(sort(reqs(agt , s)), s)) ∧
REQUESTED(agt , ψ, s(cid:2), s).

We deﬁne a new accessibility relation for goals,
C(agt , s(cid:2), s), based on the chosen set of goal formulae rather
than the requested set. Intuitively, s(cid:2) is a situation that the
agent wants to realize in situation s. We say that C(agt , s(cid:2), s)
holds if s(cid:2) is a situation that was not caused to be dropped by
any chosen goal formula ψ:

C(agt , s(cid:2)s) def= ∀ψ¬CHOSEN(agt , ψ, s(cid:2), s).

Finally, the goals of the agent are deﬁned analogously to
the way it was done by Shapiro et al., but using C instead of
W :

Goal(agt , ψ, s) def=

∀s(cid:2), s(cid:2)(cid:2).C(agt , s(cid:2), s) ∧ B(agt , s(cid:2), s) ∧ s(cid:2)(cid:2) (cid:9) s(cid:2) ⊃

ψ[s(cid:2)(cid:2), s(cid:2)].

5A similar function was deﬁned in Booth and Nittka [2005]. This
way of handling preferences can also be viewed as a special case of
[Brewka, 1989].

IJCAI-07

2628

5 Properties

We now consider some properties of goal change. Let Σ con-
sist of the encoding axioms, the axioms deﬁning lists, and
Axioms 2.1–4.3. Our ﬁrst result is that the agents’ goals are
always (simply) consistent.

formulae α, if the priority of ψ is at least as high as the prior-
ity of any goal formula in the set, and any goal formula in the
set whose priority is at least as high as ψ is equal to ψ.

Hp(ψ, α) def= (∀ψ(cid:2) ∈ α.ψ ≤ ψ(cid:2)) ∧

(∀ψ(cid:2)(cid:2) ∈ α.ψ(cid:2)(cid:2) ≤ ψ ⊃ ψ(cid:2)(cid:2) = ψ(cid:2)).

Theorem 5.1

Σ |= ∀agt , s.¬Goal(agt , FALSE , s).

As we have discussed, it should be the case that if an agent
believes a goal ψ is impossible to achieve then the agent
should drop the goal. For this theorem, we assume that ψ
is an achievement goal, i.e., of the form eventually ψ(cid:2) for
some goal formula ψ(cid:2). The theorem states that if an agent
believes that ψ is impossible to achieve, then the agent does
not have the goal Ev(ψ). We need to give a deﬁnition for Ev
to be used both inside the Bel operator and the Goal opera-
tor. Since belief formulae take a situation as an argument and
goal formulae take a path as an argument, we need two deﬁ-
nitions in order to use them in the two contexts, therefore, we
overload the deﬁnition.

In the belief context, Ev(ψ, s) takes a single situation ar-
gument. It holds if there exists a path (s(cid:2)(cid:2), s(cid:2)) in the future of
s such that ψ[s(cid:2)(cid:2), s(cid:2)] holds.

For this theorem, we need an assumption about the belief
change framework. Namely, it must be the case that request
actions are not “belief producing”. More precisely, if a situ-
ation s(cid:2)(cid:2) is accessible after a request action was executed in
situation s, then s(cid:2)(cid:2) came about by executing the same request
action in a situation s(cid:2) accessible from s. In other words, suc-
cessor situations are not dropped from the B relation after a
request action is executed.

Axiom 5.4
B(agt , s(cid:2)(cid:2), do(REQUEST(reqr , agt , ψ), s)) ⊃
∃s(cid:2).s(cid:2)(cid:2) = do(REQUEST(reqr , agt , ψ), s(cid:2)) ∧ B(agt , s(cid:2), s).

Theorem 5.5

Σ ∪ {Axiom 5.4} |= ∀agt , ψ, reqr , s.

BCons(agt , ψ, do(REQUEST(reqr , agt , ψ), s)) ∧
Hp(ψ, ({ψ} ∪ reqs(agt , s))) ⊃

Goal(agt , ψ, do(REQUEST(reqr , agt , ψ), s)).

Ev(ψ, s) def= ∃s(cid:2)(cid:2), s(cid:2).s (cid:9) s(cid:2)(cid:2) ∧ s(cid:2)(cid:2) (cid:9) s(cid:2) ∧ ψ[s(cid:2)(cid:2), s(cid:2)].

6 Future Work

In the goal context, Ev(ψ, s, s(cid:2)) takes a path (a pair of sit-
uations) as an argument. It holds if there is a situation s(cid:2)(cid:2) in
the future of s such that ψ[s(cid:2)(cid:2), s(cid:2)] holds.

Ev(ψ, s, s(cid:2)) def= ∃s(cid:2)(cid:2).s (cid:9) s(cid:2)(cid:2) ∧ s(cid:2)(cid:2) (cid:9) s(cid:2) ∧ ψ[s(cid:2)(cid:2), s(cid:2)].

Note that we suppress the situation arguments of Ev when it
is passed as an argument to Bel or Goal.

Theorem 5.2

Σ |= ∀agt , ψ, s.Bel(agt , ¬Ev(ψ), s) ⊃

¬Goal(agt , Ev(ψ), s).

As a corollary, we have a result about belief contraction. If
an agent has Ev(ψ) as a goal in s but after an action a occurs,
the agent believes ψ is impossible to achieve, then the agent
drops the goal that Ev(ψ).

Corollary 5.3

Σ |= ∀agt , a, ψ, s.Goal(agt , Ev(ψ), s) ∧

Bel(agt , ¬Ev(ψ), do(a, s)) ⊃

¬Goal(agt , Ev(ψ), do(a, s)).

Another interaction between achievement goals and beliefs is
that once an agent believes that an achievement goal has been
realized, it should drop that goal. We have not addressed this
yet, but it will not be difﬁcult to add it to our framework, as
described in the following. In the context of belief change,
the agent may believe that a goal has been achieved but later
change its mind. In this case, the agent should ﬁrst drop its
achievement goal, but later readopt it after the mind change.
Therefore, we need to check whether it is the case that agent
believes that an achievement goal ψ has been false continu-
ously since the last request for ψ. If not ψ, should be dropped.
This can be formalized in the situation calculus as follows.
We ﬁrst deﬁne a predicate Prev(a, s(cid:2), s), which holds iff the
last occurrence of a in the history of s occurs just before sit-
uation s(cid:2).

Prev(a, s(cid:2), s) def=

∃s(cid:2)(cid:2).s(cid:2) = do(a, s(cid:2)(cid:2)) ∧ s(cid:2) (cid:9) s ∧

∀s∗, a∗.s(cid:2) ≺ do(a∗, s∗) (cid:9) s ⊃ a (cid:2)= a∗.

Then, we say that ψ is live in situation s, if the agent believes
that ψ has been continuously false since that last request for
ψ:

We also have a result concerning the expansion of goals. If
an agent gets a request for ψ, it will not necessarily adopt ψ
as a goal, for example, if it has a conﬂicting higher priority
goal. But if ψ is the highest priority goal formula, and it is
B-consistent, it should be adopted as a goal. We say that a
goal formula ψ is highest priority among a ﬁnite set of goal

IJCAI-07

2629

(∃s(cid:2)(cid:2), reqr .Prev(request(reqr , agt , ψ), s(cid:2)(cid:2), Now) ∧
(cid:9) s(cid:2)(cid:2) ⊃

1.s(cid:2)(cid:2) (cid:9) s∗ (cid:9) Now ∧ s(cid:2)(cid:2) (cid:9) s∗

∀s∗, s∗

1

¬ψ[s∗

1, s∗]),

Live(ψ, s) def=

Bel(agt ,

s).

[Levesque et al., 1998] Hector J. Levesque, Fiora Pirri, and
Raymond Reiter. Foundations for a calculus of situa-
tions. Electronic Transactions of AI (ETAI), 2(3–4):159–
178, 1998.

[McCarthy and Hayes, 1969] John McCarthy and Patrick J.
Hayes. Some philosophical problems from the standpoint
of artiﬁcial intelligence. In Bernard Meltzer and Donald
Michie, editors, Machine Intelligence 4. Edinburgh Uni-
versity Press, 1969.

[Rao and Georgeff, 1991] A. S. Rao and M. P. Georgeff.
Asymmetry thesis and side-effect problems in linear time
and branching time intention logics. In John Mylopoulos
and Raymond Reiter, editors, Proceedings of the Twelfth
International Joint Conference on Artiﬁcial Intelligence
(IJCAI-91), pages 498–504. Morgan Kaufmann, 1991.

[Reiter, 2001] Raymond Reiter. Knowledge in Action: Logi-
cal Foundations for Specifying and Implementing Dynam-
ical Systems. MIT Press, Cambridge, MA, 2001.

[Shapiro and Pagnucco, 2004] Steven Shapiro and Maurice
Pagnucco. Iterated belief change and exogenous actions
in the situation calculus.
In R. L´opez de M´antaras and
L. Saitta, editors, Proceedings of the 16th European Con-
ference on Artiﬁcial Intelligence (ECAI-04), pages 878–
882, Amsterdam, 2004. IOS Press.

[Shapiro et al., 2000] Steven Shapiro, Maurice Pagnucco,
Iterated be-
Yves Lesp´erance, and Hector J. Levesque.
lief change in the situation calculus.
In A. G. Cohn,
Fausto Giunchiglia, and Bart Selman, editors, Principles
of Knowledge Representation and Reasoning: Proceed-
ings of the Seventh International Conference (KR2000),
pages 527–538, San Francisco, CA, 2000. Morgan Kauf-
mann.

[Shapiro et al., 2005] Steven Shapiro, Yves Lesp´erance, and
Hector J. Levesque. Goal change.
In L. P. Kaelbling
and A. Safﬁotti, editors, Proceedings of the Nineteenth
International Joint Conference on Artiﬁcial Intelligence
(IJCAI-05), pages 582–588, Denver, CO, 2005. Profes-
sional Book Center.

[Shapiro et al., 2006] Steven Shapiro, Yves Lesp´erance, and
Hector J. Levesque. Goal change in the situation calculus.
Journal of Logic and Computation, to appear, 2006.

[Shapiro, 2005] Steven Shapiro. Belief change with noisy
sensing and introspection. In Leora Morgenstern and Mau-
rice Pagnucco, editors, Working Notes of the IJCAI-05
on Nonmonotonic Reasoning, Action, and Change (NRAC
2005), pages 84–89, 2005.

If ψ is believed to have been already achieved, it should
be dropped regardless of the agent’s other goals and should
therefore not be taken into account when determining the
maximally B-consistent set. Therefore, for all goals ψ s.t.
¬Live(ψ, s), we remove ψ from reqs(agt , s) before it is
passed to makecons. This ensures that ψ will not be chosen
and will therefore not be a goal of the agent.

7 Conclusion

In this paper, we extended a previous framework for goal
change so that agents drop goals that are believed impossible
to achieve. To our knowledge this is the ﬁrst framework to
take into account the possibility that the agents change their
minds about the impossibility of their goals. When this hap-
pens the agents may readopt goals that were previously be-
lieved to be impossible to achieve. Some properties about
goal consistency, contraction, and expansion were shown. We
also sketched how to further extend the framework so that
agents will drop achievement goals that are believed to be al-
ready achieved. Again, agents might later change their mind
about whether the goal has been already achieved and possi-
bly readopt the goal.

8 Acknowledgments

We would like to thank M. David Sadek for suggesting this
avenue of research. We also thank Philip R. Cohen, Andreas
Herzig, Yves Lesp´erance, Hector J. Levesque, and Sebastian
Sardina for useful discussions pertaining to this paper.

References

[Bell and Huang, 1997] John Bell and Zhisheng Huang. Dy-
namic goal hierarchies. In Lawrence Cavedon, Anand S.
Rao, and Wayne Wobcke, editors, Intelligent Agent Sys-
tems, Theoretical and Practical Issues, Based on a Work-
shop Held at PRICAI’96, Cairns, Australia, August 26-30,
1996, number 1209 in Lecture Notes in Computer Science,
pages 88–103. Springer-Verlag, 1997.

[Booth and Nittka, 2005] Richard Booth and Alexander Nit-
tka. Reconstructing an agent’s epistemic state from obser-
vations. In L. P. Kaelbling and A. Safﬁotti, editors, Pro-
ceedings of the Nineteenth International Joint Conference
on Artiﬁcial Intelligence (IJCAI-05), pages 394–399, Den-
ver, CO, 2005. Professional Book Center.

[Brewka, 1989] Gerhard Brewka. Preferred subtheories: An
extended logical framework for default reasoning. In N. S.
Sridharan, editor, Proceedings 11th International Joint
Conference on Artiﬁcial Intelligence (IJCAI-89), pages
1043–1048. Morgan Kaufmann, 1989.

[Cohen and Levesque, 1990] Philip R. Cohen and Hector J.
Levesque. Intention is choice with commitment. Artiﬁcial
Intelligence, 42:213–261, 1990.

[De Giacomo et al., 2000] Giuseppe De Giacomo, Yves
Lesp´erance, and Hector J. Levesque. ConGolog, a con-
current programming language based on the situation cal-
culus. Artiﬁcial Intelligence, 121(1–2):109–169, 2000.

IJCAI-07

2630


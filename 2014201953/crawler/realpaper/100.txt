Scale-Based Monotonicity Analysis in Qualitative Modelling with Flat Segments

Martin Brooks1 and Yuhong Yan1 and Daniel Lemire2

1National Research Council
200 Montreal Road, M-50

Ottawa, ON K1A 0R6

martin.brooks@nrc.gc.ca
yuhong.yan@nrc.gc.ca

Abstract

Qualitative models are often more suitable than
classical quantitative models in tasks such as
Model-based Diagnosis (MBD), explaining system
behavior, and designing novel devices from ﬁrst
principles. Monotonicity is an important feature
to leverage when constructing qualitative models.
Detecting monotonic pieces robustly and efﬁciently
from sensor or simulation data remains an open
problem. This paper presents scale-based mono-
tonicity:
the notion that monotonicity can be de-
ﬁned relative to a scale. Real-valued functions de-
ﬁned on a ﬁnite set of reals e.g.
sensor data or
simulation results, can be partitioned into quasi-
monotonic segments, i.e. segments monotonic with
respect to a scale, in linear time. A novel segmen-
tation algorithm is introduced along with a scale-
based deﬁnition of “ﬂatness”.

Introduction

1
Qualitative models are used in applications such as model-
based diagnosis [Yan, 2003; Struss, 2002], explaining system
behavior [Šuc, 2003; Forbus, 1984; Kuipers, 1986], and de-
signing novel devices from ﬁrst principles [Williams, 1992].
It is a challenge to build qualitative models for complex real
world engineering systems. Current research efforts are on
automatic generation of qualitative models from numerical
data obtained by numerical simulation or sensors. Many pro-
posed methods, such as in [Struss, 2002] and [Console et al.,
2003], work only when the functions are piecewise mono-
tonic. Hence, partitioning data arrays into quasi-monotonic
segments is an important problem [Yan et al., 2004b; 2004a].
One could segment a data array in monotonic segments
using a naïve algorithm: simply segment wherever there is
an extremum. For example, it is easy to segment the array
{0, 1, 2, 3, 2, 1, 0} in two monotonic segments. How-
ever, monotonic segments must be signiﬁcant for the appli-
cation at hand and algorithms must be robust because the
data will unavoidably be subject to noise or insigniﬁcant
features. Hence, we might want to consider that the array
{0, 1.2, 1.1, 3, 2, 1, 0} has only two “signiﬁcant” mono-
tonic segments since the drop from 1.2 to 1.1 is not large
enough to indicate a downward trend. Also algorithms must

2University of Quebec

4750 avenue Henri-Julien
Montréal, QC H2T 3E4
lemire@ondelette.com

be fast (O(n)) and not require excessive amounts of memory
(O(n)).

In this paper, we address the “scale-based monotonicity”
problem: monotonicity can be deﬁned relative to a scale. We
deﬁne a metric for monotonic approximation. We solve the
following problem: given a number of segments K, ﬁnd K
segments having the smallest monotonic approximation error.

In short, the main novel results of this paper are:
1. A novel optimal segmentation algorithm;
2. A novel deﬁnition of scale-based ﬂatness.

2 Monotonicity in Qualitative Modelling
We just list a few examples to show that the monotonic
features are important to leverage in qualitative modeling.
In QSIM [Kuipers, 1986], the qualitative value QV ( f ,t), is
the pair <qmag, qdir>, where qmag is a landmark value
li or an interval made up of landmark values (li, li+1), and
qdir takes a value from the sign domain (+,0,-) according
the f 0(t) is increasing, steady, or decreasing respectively.
In Model-based Diagnosis (MBD), qualitative models are
used to describe system behavior. Two kinds of qualitative
models, based respectively on absolute or relative quanti-
ties, rely on the monotonicity of the function. The ﬁrst one
is Finite Relation Qualitative Model (FRQ) [Struss, 2002;
Yan, 2003], where the qualitative relation is represented by
tuples of real valued intervals. For a monotonic segment
[xa, xb] of f , the tuple is determined by the bounding rect-
angle (cid:3) = [xa, xb] × [min( f (xa), f (xb)),max( f (xa), f (xb))],
xb ⇒ (x, y) ∈ (cid:3). The second one is Qualitative Deviation
Model [Console et al., 2003] where the qualitative relation
is represented by the sign of deviation [D y] from a refer-
ence point xre f deﬁned as +,-, or 0, according to whether
is
f
monotonic increasing, we have [D y] = sign( f (x)− f (xre f )) =
sign(x− xre f ) = [D x].

that is the smallest rectangle such that y = f (x)Vxa ≤ x ≤

is increasing, decreasing or ﬂat. For example, if f

In this paper, we focus on abstracting qualitative models
from scattered data obtained from numerical simulation or
sensors. This work is motivated by at least two kinds of appli-
cations. First, when applying model-based diagnosis to real
world engineering systems, we need to build symbolic qual-
itative models by a numerical simulation of the engineering
models. Second, we need to explain system behaviors from

sensor data. Qualitative model abstraction in this paper is
deﬁned as transforming numerical values into qualitative val-
ues and functions into qualitative constraints [Šuc and Bratko,
2001]. Monotonicity Analysis is deﬁned as partitioning a ﬁ-
nite series of real values x1, x2, . . . , xn over an interval D into
“monotonic” segments.

Monotonicity analysis is a crucial step to abstract qualita-
tive models from scattered data but it rises a problem. The
difﬁculty is that when the scattered data contains noise, the
monotonicity is not absolute in a neighborhood of any point.
The monotonic segments need to be signiﬁcant in the context
of the problem. The small ﬂuctuations caused by noise must
be ignored. This requires that noise be removed by compu-
tationally efﬁcient methods such that the number of remain-
ing segments is dependent only on the characterization of the
noise.

Linear splines is an obvious approach to solve this prob-
lem. We can use top-down, bottom-up and sliding window
algorithms to approximate the data with a set of straight
lines [Keogh et al., 2001]. Then the same-sign slopes can be
aggregated as monotonic segments. Various algorithms are
derived from classic algorithms [Key et al., 2000] [Hunter
and McIntosh, 1999]. The downside of these methods is that
there is no link between linear spline approximation error and
the actual monotonicity of the data (consider y = ex). Hence,
using linear splines, it is difﬁcult to specify either the desired
number of monotonic segments or some “monotonicity error”
threshold. In addition, linear ﬁtting algorithms are relatively
expensive.

Inductive learning is used in [Šuc and Bratko, 2001] to au-
tomatically construct qualitative models from quantitative ex-
amples. The induced qualitative model is a binary tree, called
a qualitative tree, which contains internal nodes (called splits)
and qualitatively constrained functions at the leaves. A quali-
tative constrained function takes the form Ms1,...,sm : Rm 7→ R,
si ∈ {+,−} and represents a function with m real-valued at-
tributes strictly monotone increasing with respect to the i-th
attribute if si = +, or strictly monotone decreasing if si = −.
For example, f = M+,−(x, y) means f is increasing when x
is increasing, and decreasing when y is increasing. A split
is a partition of a variable. The unsupervised learning algo-
rithm eq-QUIN determines the landmarks for the splits. The
training data set is all possible pairs of data points. eq-QUIN
checks the best split against all possible hypotheses. Its com-
plexity is O(n22m). QUIN is a more efﬁcient algorithm that
uses greedy search. Its complexity is O(n2m2). We do not
address multidimensional data in this paper.

3 Monotonicity Error
In this section, we deﬁne a measure of monotonicity. Sup-
pose we are given a set of n ordered samples noted F :
D = {x1, . . . , xn} ⊂ R → R with real values F(x1), . . . , F(xn)
and x1 < x2 < . . . < xn. We deﬁne, F|[a,b] as the restric-
tion of F over D ∩ [a, b]. We seek the best monotonic (in-
creasing or decreasing) function f : R → R approximating
F. Let W
↓) be the set of all monotonic increas-
ing (resp. decreasing) functions. The Optimal Monotonic
Approximation Function Error (OMAFE) of F is given by

↑ (resp.

min f∈W maxx∈D| f − F| where W
or - is associated to W
↓.

↑ or W

is either W

↑ or W

↓. Sign +

[min D,max D] =S

The segmentation of a set D is a sequence S =
X1, . . . , Xm of closed intervals (called “segments”) in D with
i Xi such that max Xi = min Xi+1 and Xi ∩
X j = /0 for j 6= i, i + 1, i − 1. Alternatively, we can deﬁne a
segmentation from the set of points Xi ∩ Xi+1 = {yi}. Given
F : {x1, . . . , xn} → R and a segmentation {Xi}, the Optimal
Piecewise Monotonic Approximation Function Error (OP-
MAFE) of the segmentation is given by maxi OMAFE(F|Xi)
where the directions of the segments Xi are alternating and
such that the direction of the ﬁrst segment is chosen so as to
minimize the OPMAFE.

Solving for a best monotonic function can be done as fol-
lows. If we seek the best monotonic increasing function, we
ﬁrst deﬁne f ↑(x) = max{F(y) : y ≤ x} (the maximum of all
previous values) and f ↑(x) = min{F(y) : y ≥ x} (the mini-
mum of all values to come). If we seek the best monotonic
decreasing function, we deﬁne f ↓(x) = max{F(y) : y ≥ x}
(the maximum of all values to come) and f ↓(x) = min{F(y) :
y ≤ x} (the minimum of all previous values). These func-
tions which can be computed in linear time are all we need
to solve for the best approximation function as shown by the
next theorem which is a well-known result [Brooks, 1994;
Ubhaya, 1974].
Theorem 1. Given F : D = {x1, . . . , xn} → R, a best mono-
tonic increasing approximation function to F is given by
f↑ = f ↑+ f ↑
tion function is given by f↓ = f ↓+ f ↓
2
ror (OMAFE) is given by maxx∈D
| f ↓(x)− f ↓(x)|
creasing) or maxx∈D

and a best monotonic decreasing approxima-

. The corresponding er-

(monotonic decreasing).

2

| f ↑(x)− f ↑(x)|

2

(monotonic in-

2

The implementation of the algorithm suggested by the the-
orem is straight-forward. Given a segmentation, we can com-
pute the OPMAFE in O(n) time using at most two passes.
The functions f↑ and f↓ are sometimes called the standard
optimal monotone functions as the solution is not unique in
general.

4 Scale-Based Monotonicity
We present the notion of scale-based monotonicity. The in-
tuition is that the ﬂuctuations within a certain scale can be
ignored.
Given an ordered set of real values D = {x1, x2, . . . , xn},
consider F : D = {x1, x2, . . . , xn} 7→ R. Given some toler-
ance value d > 0, we could say that the data points are not
going down or are upward monotone, if consecutive mea-
sures do not go down by more than d , that is, are such that
F(xi)− F(xi+1) < d . However, this deﬁnition is not very use-
ful because measures can repeatedly go down and thus the
end value can be substantially lower than the start value. A
more useful deﬁnition of upward monotonicity would be to
require that we cannot ﬁnd two values xi and x j (xi < x j) such
(i.e. F(xi)− F(x j) < d ).
that F(x j) is lower than F(xi) by d
This deﬁnition is more useful because in the worst case, the

W
5 A Scale-Based Algorithm for

Quasi-Monotonic Segmentation

Suppose that D is a ﬁnite set of reals having at least two ele-
ments. F is a real-valued function on D, i.e., F : D → R.
We begin by deﬁning a segmentation at scale d or a d -

segmentation.
Deﬁnition 3. Let S = X1, . . . , Xn be a segmentation of D, and
let d > 0, then S is a d -segmentation of F when all the fol-
lowing conditions hold.

rections.

• Each Xi is d -monotone.
• Each Xi for i 6= 1, n is strictly d -monotone.
• At least one Xi is strictly d -monotone.
• Adjacent strictly d -monotone segments have opposite di-
• For each strictly d -monotone Xi, and for all x ∈ Xi, F(x)
lies in the closed interval bounded by F(min Xi) and
F(max Xi).
• When X1 is not strictly d -monotone, then for all x ∈
X1 − max X1, F(x) lies in the open interval bounded
by F(min X2) and F(max X2); and when Xn is not
strictly d -monotone, then for all x ∈ Xn − min Xn, F(x)
lies in the open interval bounded by F(min Xn−1) and
F(max Xn−1).

Each Xi is called a d -segment of S. When strictly d -monotone,
Xi is a proper d -segment; when not strictly d -monotone, X1
or Xn is an improper d -segment. For strictly d -monotone Xi,
min Xi and max Xi are d -extrema.

As the following theorems show, all d -segmentations are
“equivalent” and the monotonic approximation error (OP-
MAFE) is known precisely.
Theorem 2. Let d > 0. Let S1 and S2 be d -segmentations of
F; then |S1| = |S2|. Furthermore, the ﬁrst d -segments of S1
and S2 are both either proper or improper, and similarly for
the last d -segments.
Theorem 3. Let d > 0. Let S be any d -segmentation of F;
then the monotonic approximation error (OPMAFE) ≤ d /2.
Now we want to compute a d -segmentation given F. There
are two approaches depending on the application one has in
mind. The ﬁrst one is to choose d
and then solve for the
segments [Brooks, 1994], when the magnitude of noise is
known. When one doesn’t know how to choose d , the second
approach is to set the maximal number of segments K, espe-
cially when one knows the shape of the function. We focus
on this second approach. We begin by labelling the extrema
with a corresponding scale D (x).
Deﬁnition 4. Let x ∈ D be a local extremum of F. x’s delta-
scale D (x) is the largest d > 0 such that there exists a d -
segmentation having x as a d -extremum.

It is immediate from the deﬁnition that if d > D (x), then
x can’t be a d -extremum, but also that if x is a d -extremum,
then D (x) ≥ d . This is stated in the following proposition.
Proposition 1. Let d > 0, and let S = X1, . . . , Xn be a d -
segmentation of F. Then D (x) ≥ d
for every d -extremum, x, of
S.

Figure 1: A d -pair.

(F(xl)− F(xk) ≥ d ).

last measure will be only d
smaller than the ﬁrst measure.
However, we are still not guaranteed that the data does in fact
increase at any point. Hence, we add the constraint that we
can ﬁnd at least two data points xk < xl such that F(xl) is
greater than F(xk) by at least d
To summarize, given some value d > 0, we say that a se-
quence of measures is upward d -monotone if no two succes-
sive measures decrease by as much as d , and at least one pair
of successive measures increases by at least d . Similarly, we
say that a set of measures is downward d -monotone if no two
successive measures increase by as much as d , and at least
two measures decrease by at least d .
in [Brooks, 1994] using d -pairs (see Fig. 1):
Deﬁnition 1.

This generalized deﬁnition of monotonicity was introduced

• x < y∈ D is a d -pair (or a pair of scale d ) for F if |F(y)−
F(x)| ≥ d and for all z ∈ D, x < z < y implies |F(z)−
F(x)| < d and |F(y)− F(z)| < d .

• A d -pair’s direction is increasing or decreasing accord-

ing to whether F(y) > F(x) or F(y) < F(x).

Notice that pairs of scale d having opposite directions can-
not overlap but they may share an endpoint. Pairs of scale d
of the same direction may overlap, but may not be nested for
a certain d .

We can deﬁne d -monotonicity as follows:

Deﬁnition 2. Let X be an interval, F is d -monotone on X
if all d -pairs in X have the same direction; F is strictly d -
monotonic when there exists at least one such d -pair. In this
case:

• F is d -increasing on X if X contains an increasing d -

pair.

• F is d -decreasing on X if X contains a decreasing d -pair.
0
if it is of scale d

We say that a pair is signiﬁcant at scale d
for d

0 ≥ d .

In the next section, we discuss how to partition the data set

into monotonic segments.

ddd−pair• it has no repeated maxima or minima;
• there is no superset X00

d of X0

trema;

in Xd without repeated ex-

We observe that there must be a smallest d

such that the
cardinality of Xd = {x|D (x) ≥ d } is at most K. However the
set Xd might contain repeated maxima or repeated minima
and those might be removed before the set Xd can deﬁne a
d -segmentation. This is stated in the next theorem.
Theorem 4. Let d > 0 and consider the set of extrema Xd =
{x|D (x) ≥ d } of F as an ordered set. Each extremum in Xd
is either a maximum or a minimum, and a sequence of two
d ⊂ Xd
minima or two maxima is possible. Consider a subset X0
such that

then there exists a d -segmentation S of F such that X0
actly the set of d -extrema of S.

is ex-

In order to compute D (x) for all extrema x, it is useful to

introduce the following deﬁnitions.
Deﬁnition 5. Opposite-sense extrema x < z ∈ X ⊂ D are an
extremal pair for X if for all y ∈ X, x < y < z implies F(y)
lies in the closed interval deﬁned by F(x) and F(z). The ex-
tremal pair has an extent [F(x), F(z)] or [F(z), F(x)] and
increasing or decreasing direction according to F(x) < F(z)
or F(x) > F(z). An extremal pair is maximal for X when
no other extremal pair in X has larger extent. Similarly, an
extremal pair is a maximal increasing (decreasing) when no
increasing (decreasing) extremal pair has larger extent.

Intuitively, a maximal extremal pair forms the largest de-
creasing (increasing) segment inside a larger increasing (de-
creasing) segment X.
Deﬁnition 6. We recursively deﬁne ordinary and special in-
tervals: D is an ordinary interval. When I is an ordinary
interval, then an interval J ⊂ I is special when J’s endpoints
constitute a maximal extremal pair in I; in this case J has
direction inherited directly from its endpoints. Recursively, if
J is special and interval J0 ( J has endpoints constituting a
maximal extremal pair in J of direction opposite to that of J,
then J0 is special. Let Ji, i = 1 . . . n be all special intervals in
an ordinary or special interval X. Choose any nonempty sub-
set of the collection {Ji}, and let S be the segmentation of X
deﬁned by the endpoints of the special intervals in the subset.
Then any segment I ∈ S that does not contain any of the Ji is
an ordinary interval.

One can see that the special intervals are nested. The
endpoints x, z of the special intervals have D (x) = D (z) =
|F(x)− F(z)|. We can call them “twins” due to the same D (x)
value. The ordinary intervals are not nested. The endpoints
of the ordinary intervals have different D (x). We call each of
them “singleton”.

If there are several extrema having an equal value, the way
to choose the endpoints to constitute a maximal extremal pair
is not unique. Therefore, there are different sets of special
intervals. They are equivalent. For simplicity, we do not con-
sider the case of equal valued extrema in Algorithm 1 and
Algorithm 2. Instead, we explain how to deal with it verbally
after introducing the algorithms.

Algorithm 1 computes D (x) for every extremum x ∈ D. The
input to Algorithm 1 is a list of extrema - i.e. the extrema data
points in the data array.

Algorithm 1 This algorithm labels the extrema in linear time
(as in Deﬁnition 4).

INPUT: data - a list of the successive extremal values of F, al-
ternating between maximum and minimum. Each element is an
"extremum record", having three ﬁelds: value, sense, and index.
Index identiﬁes the data point, value gives F’s value at that data
point, and sense indicates whether the extremum is a maximum
or minimum.
OUTPUT: a list of "scale records". Each scale record has two
ﬁelds: scale and extrema_list. Extrema_list comprises either one
or two extremum records. The semantics of a scale record is that
the indicated extrema have the indicated scale as delta-scale.
Notes about the algorithm: 1) Lists are accessed by element
numbers; e.g. data(2) is the second element of data. 2) Lists
are manipulated with functions Push and Pop. Push( thing, list )
adds thing to list, resulting in thing being the ﬁrst element of list.
Pop(list) removes the ﬁrst element of list, and returns this ﬁrst
element as the value of the function call. 3) MakeList(item1..∗)
pushes the items into a list generated, starting from the ﬁrst item.
4) MakeScaleRecord creates a scale record.

LET extrema = empty_list
LET scales = empty_list
Push( Pop(data),extrema)
Push( Pop(data),extrema)
for next_extremum IN data do

while Length(extrema) > 1 AND {{sense.next_extremum =
"maximum" AND value.next_extremum > value.extrema(2)}
OR {sense.next_extremum = "minimum" AND
value.next_extremum < value.extrema(2)}} do
Push(MakeScaleRecord( | value.extrema(1) -
value.extrema(2) |, MakeList( extrema(2), extrema(1) )),
scales)
Pop(extrema)
Pop(extrema)

if Length( extrema ) = 0 then

Push( Pop( extrema_list.scales(1) ), extrema )

while Length(extrema) > 1 do

Push( next_extremum, extrema )
Push( MakeScaleRecord (| value.extrema(1) -
value.extrema(2) |, MakeList( extrema(1) )), scales)
Pop(extrema)

Push ( extrema(1), extrema_list.scales(1) )
RETURN scales

The principle of Algorithm 1 is as follows: the while loop
inside the for loop detects the special interval and labels the
both endpoints with the difference of their values. But if the
extrema list is empty after this labelling, which means that
the top item in the list needs to be checked against the com-
ing data, this top item is popped back to extrema in the if fol-
lowing the above while. Then the next extremum is pushed
into the extrema list for the next for loops. The while outside
the for loop determines the d
for the ordinary intervals. The
last push states that the last extremum in the extrema list has
the same scale as the one just popped into scale. Actually the
endpoints of the biggest ordinary interval are always the last
two pushed into scale record list and their scales are identical.

d
d
If one desires no more than K segments, it is a simple mat-
ter of picking d as small as possible so that you have no more
than K + 1 signiﬁcant extrema (D (x) ≥ d ) together with the
ﬁrst and last extrema (indexes 0 and n − 1). Algorithm 2
shows how once the labelling is complete, one can compute
the segmentation in time O(nK log K) which we consider to
be linear time when K is small compared to n. It also uses
a ﬁxed amount of memory (O(K)). The principle of Algo-
rithm 2 is to select the K + 1 data points to be the endpoints
of K segments. Since the endpoints of D are by default, the
remaining endpoints come from the largest d -extrema. The
for is to select the largest d -extrema. As in Algorithm 1, we
know that the labels are either "twins" or "singleton". Thus
in the for loop, a maximum of K + 2 data points are chosen.
Then, after the ﬁrst if outside for, the smallest d -extrema are
removed to reduce the total endpoints to at most K + 1. The
rest of the code checks whether the ﬁrst and the last endpoints
of D are included. If not, the smallest d -extrema will be re-
moved in favor of the ﬁrst and the last endpoints. The number
of segments can be less than K since several extrema can take
the same d value and can be removed together.

Algorithm 2 Given a labelling algorithm (see Algorithm 1),
this algorithm returns an optimal segmentation using at most
K segments. It is assumed that there are at least K +1 extrema
to begin with.
INPUT: an array d containing the values indexed from 0 to n− 1
INPUT: K a bound on the number of segments desired
OUTPUT: segmentation points as per Theorem 4 and Proposi-
tion 1
L ← empty array (capacity K + 3)
for e is index of an extremum in d having scale d , e are visited in
increasing order do
insert (e,d ) in L so that L is sorted by scale in decreasing order
(sort on d ) using binary search
if length(L) = K + 3 then

pop last(L)

if length(L) > K+1 then
if indexes {0, n− 1} 6⊂ L then

remove all elements of L having the scale of last(L).
if (index 0 ∈ L OR index n − 1 ∈ L) AND length(L) = K+1
then
if (index 0 6∈ L AND index n−1 6∈ L) AND length(L) ≥ K then
RETURN: the indexes in L adding 0 and/or n− 1 when not al-
ready present

remove all elements of L having the scale of last(L).

remove all elements of L having the scale of last(L).

Algorithms 1 and 2 assume that no two extrema can have
the same value. In case of equal valued extrema, we can mod-
ify these algorithms without increasing their complexity. In
Algorithm 1, we can generalize ScaleRecord so that entries in
the extrema_list ﬁeld can also be lists of extrema of the same
value. Then in the while loop, we can put the extrema of the
same value into the corresponding list. For Algorithm 2, we
just need to remember that whenever removing a middle point
between two equal valued extrema, the two equal valued ex-
trema have to be treated as one extrema to avoid having two
adjacent minima or maxima.

Figure 2: Input ﬂow of tank A (left) and level of tank B (right)
with added white noise.

6 Scale-Based Flatness
For applications, it is important to be able to ﬁnd “ﬂat” seg-
ments robustly in a data set. We propose the following deﬁ-
nition:
Deﬁnition 7. Given d > 0, consider a d -monotonic segment
in a d -segmentation, an interval I in this segment is d -ﬂat if
the standard optimal monotonic approximating function (as
deﬁned in Theorem 1) is constant over I.

Because we can compute the standard optimal monotonic
approximating function in O(n) time, we can ﬁnd ﬂat seg-
ments in O(n) time.
The next proposition addresses the ﬂat segments in differ-
ent d -segmentations. The ﬁrst point is derived from the fact
that when the d
increases, the new segments are always the
result of mergers of the previous segments. Hence, the seg-
ments at a small d are always subsets of the segments at a
larger d . The second point says that the old ﬂat segments will
still be ﬂat in the merged segment.
Proposition 2. Let S be a d -segmentation and let S0 be a d
segmentation for d
subset of some d
in X, I is also d

0-
0 > d , then 1) any d -segment X ∈ S is a
0-segment X0 ∈ S0, 2) for any d -ﬂat interval I
0-ﬂat in X0.

7 Experimental Results
7.1 Cascade Tank Sample Data
As a source of synthetic data, we consider a system which
consists of two cascade tanks A and B. Each tank has an in-
put pipe (incoming water) and an output pipe (outgoing wa-
ter). Tank A’s output pipe is the input pipe of tank B. In the
equations below, let A and B be the level of water for the two
tanks respectively. The change of water level is proportional
to the difference of the input ﬂow and the output ﬂow. As-
sume in is the input ﬂow of tank A. f (A) is the out ﬂow of
tank A. g(B) is the output ﬂow of tank B [Kuipers, 1994].
Thus, we have

A0 = in− f (A)
B0 = f (A)− g(B)

In principle, f (A) and g(B) are increasing functions. We
assume f (A) = k1A and g(B) = k2B. If we variate the input
ﬂow of tank A, in, we can control the level of tank B. In this
way, we generated some sample data and added noise to it.
See Fig. 2 for the the input ﬂow, in, of tank A at the left and
the level of tank B at the right.

051015−0.500.511.52051015−0.0100.010.020.030.040.050.06References
[Brooks, 1994] Martin Brooks. Approximation complexity
for piecewise monotone functions and real data. Comput-
ers and Mathematics with Applications, 27(8), 1994.

[Console et al., 2003] L. Console, G. Correndo, and C. Pi-
cardi. Deriving qualitative deviations from matlab models.
In Proc. of 14th Int. Workshop on Principles of Diagnosis,
pages 87–92, 2003.

[Forbus, 1984] K. Forbus. Qualitative process theory. Artiﬁ-

cial Intelligence, 24:85–168, 1984.

[Hunter and McIntosh, 1999] Jim Hunter and Neil McIn-
tosh. Knowledge-based event detection in complex time
series data. In Proc. of Artiﬁcial Intelligence in Medicine
and Medical Decision Making (AIMDM99), LNAI 1620,
pages 271–280, 1999.

[Keogh et al., 2001] Eamonn J. Keogh, Selina Chu, David
Hart, and Michael J. Pazzani. An online algorithm for seg-
menting time series. In ICDM, pages 289–296, 2001.

[Key et al., 2000] Herbert Key, Bernhard Rinner, and Ben-
jamin Kuipers. Semi-quantitative system identiﬁcation.
Artiﬁcial Intelligence, 119, 2000.

[Kuipers, 1986] B.J. Kuipers. Qualitative simulation. Artiﬁ-

cial Intelligence, 29:289–338, 1986.

[Kuipers, 1994] Benjamin Kuipers. Qualitative Reasoning.

the MIT press, 1994.

[Struss, 2002] P. Struss. Automated abstraction of numeri-
cal simulation models - theory and practical experience.
In Proc. of 16th Int. Workshop on Qualitative Reasoning,
pages 161–168, 2002.

[Ubhaya, 1974] V. A. Ubhaya. Isotone optimization I. Jour-

nal of Approximation Theory, 12:146–159, 1974.

[Šuc and Bratko, 2001] D. Šuc and I. Bratko.

Induction of
qualitative tree. In Proc. of European Conference of Ma-
chine Learning in 2001 (LNCS 2167), pages 442–453.
Springer, 2001.

[Šuc, 2003] D. Šuc. Machine Reconstruction of Human Con-
trol Strategies, volume 99 of Frontiers in Artiﬁcial Intel-
ligence and Applications.
IOS Press, Amsterdam, The
Netherlands, 2003.

[Williams, 1992] B. Williams.

Interaction-based invention:
designing devices from ﬁrst principles.
In Recent Ad-
vances in Qualitative Physics, pages 413–433. MIT Press,
Cambridge, MA, 1992.

[Yan et al., 2004a] Yuhong Yan, Daniel Lemire, and Martin
Brooks. Monotone pieces analysis for qualitative model-
ing. In Proceedings of ECAI MONET’04, 2004.

[Yan et al., 2004b] Yuhong Yan, Daniel Lemire, and Martin
Brooks. Monotonicity analysis for constructing qualitative
models. In Proceedings of MBR’04, 2004.

[Yan, 2003] Y. Yan. Qualitative model abstraction for diag-
nosis. In Proc. of 17th Int. Workshop on Qualitative Rea-
soning, pages 171–179, 2003.

Figure 3: OPMAFE versus number of segments K for tank B
with (above) and without white noise (below).

7.2 Optimal Algorithm vs. Top-down Algorithm
We choose the top-down linear spline approximation algo-
rithm to compare the performance with the optimal algorithm
developed in this paper. The top-down algorithm is a reﬁned
process choosing best split points that minimizes the linear
regression error. The top-down algorithm we implemented
has complexity of O(nK2) which is the best complexity on
record. The optimal algorithm in this paper is O(nK log K).
The optimal algorithm is faster for K sufﬁciently large. We
also compare the monotonicity approximation error using
OPMAFE. Figure 3 compares the OPMAFE for segmenta-
tions computed using either algorithm. The optimal algo-
rithm we presented in this paper performs signiﬁcantly better
than the top-down algorithm. Note that OPMAFE is an abso-
lute error measurement (which grows with the amplitude of
the data).

8 Conclusion
Monotonicity is the most important feature to leverage when
building the qualitative model from the scattered numerical
data. This paper gives a solid mathematical foundation to
the problem of deﬁning monotonicity based on scale theory,
where the values of the data points, not the distance between
the data points, determine the monotonicity. It proves to be
suitable in qualitative modeling. We present efﬁcient algo-
rithms to segment the data set into monotonic segments in
linear time. We will continue to work on the multidimen-
sional case.

510152025303500.0020.0040.0060.0080.010.0120.014KOPMAFEoptimaltopdown51015202530350123456789x 10−3KOPMAFEoptimaltopdown
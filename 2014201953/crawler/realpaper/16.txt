Stacked Sequential Learning

William W. Cohen

Vitor R. Carvalho

Center for Automated Learning & Discovery

Language Technologies Institute

School of Computer Science
Carnegie Mellon University

Pittsburgh, PA 15213
wcohen@cs.cmu.edu

School of Computer Science
Carnegie Mellon University

Pittsburgh, PA 15213
vitor@cs.cmu.edu

Abstract

We describe a new sequential learning scheme
called “stacked sequential learning”. Stacked se-
quential learning is a meta-learning algorithm, in
which an arbitrary base learner is augmented so
as to make it aware of the labels of nearby exam-
ples. We evaluate the method on several “sequen-
tial partitioning problems”, which are characterized
by long runs of identical labels. We demonstrate
that on these problems, sequential stacking consis-
tently improves the performance of non-sequential
base learners;
that sequential stacking often im-
proves performance of learners (such as CRFs)
that are designed speciﬁcally for sequential tasks;
and that a sequentially stacked maximum-entropy
learner generally outperforms CRFs.

Introduction

1
In this paper, we will consider the application of sequential
probabilistic learners to sequential partitioning tasks. Se-
quential partitioning tasks are sequential classiﬁcation tasks
characterized by long runs of identical labels: examples of
these tasks include document analysis, video segmentation,
and gene ﬁnding.

Motivated by some anomalous behavior observed for one
sequential learning method on a particular partitioning task,
we will derive a new learning scheme called stacked sequen-
tial learning. Like boosting, stacked sequential learning is
a meta-learning method, in which an arbitrary base learner
is augmented—in this case, by making the learner aware of
the labels of nearby examples. Sequential stacking is simple
to implement, can be applied to virtually any base learner,
and imposes only a constant overhead in training time: in our
implementation, the sequentially stacked version of the base
learner A trains about seven times more slowly than A.

In experiments on several partitioning tasks, sequential
stacking consistently improves the performance of non-
sequential base learners. More surprisingly, sequential stack-
ing also often improves performance of learners speciﬁcally
designed for sequential tasks, such as conditional random
ﬁelds and discriminatively trained HMMs. Finally, on our
set of benchmark problems, a sequentially stacked maximum-

entropy learner generally outperforms conditional random
ﬁelds.

2 Motivation: A Hard Task for MEMMs
To motivate the novel learning method that we will describe
below, we will ﬁrst analyze the behavior of one well-known
sequential learner on a particular real-world problem. In a re-
cent paper [Carvalho and Cohen, 2004], we evaluated a num-
ber of sequential learning methods on the problem of recog-
nizing the “signature” section of an email message. Each
line of an email message was represented with a set of hand-
crafted features, such as “line contains a possible phone num-
ber”, “line is blank”, etc. Each email message was repre-
sented as a vector x of feature-vectors x1, . . . , xn, where xi
is the feature-vector representation of the i-th line of the mes-
sage. A line was labeled as positive if it was part of a signa-
ture section, and negative otherwise. The labels for a message
were represented as another vector y, where yi is the label for
line i.

The dataset contains 33,013 labeled lines from 617 email
messages. About 10% of the lines are labeled “positive”. Sig-
nature sections always fall at the end of a message, usually in
the last 10 lines. In the experiments below, the data was split
into a training set (of 438 sequences/emails), and a test set
with the remaining sequences, and we used the “basic” fea-
ture set from Carvalho & Cohen.
The complete dataset is represented as a set S of exam-
ples S = {(x1, y1), . . . , (xt, yt), . . . , (xm, ym)}. Sequen-
tial learning is the problem of learning, from such a dataset,
a sequential classiﬁer—i.e., a function f such that f(x)
produces a vector of class labels y. Clearly, any ordinary
non-sequential learning algorithm can be used for sequential
learning, by ignoring the sequential nature of the data1.

In the previous paper [Carvalho and Cohen, 2004], we re-
ported results for several non-sequential and sequential learn-
ers on the signature-detection problem,
including a non-
sequential maximum entropy learner [Berger et al., 1996]

1Speciﬁcally, one could build a dataset of non-sequential exam-
ples (xt,i, yt,i) from S, and use it to train a classiﬁer g that maps a
single feature-vector x to a label y. One can then use g to classify
each instance xi in the vector x = (cid:104)x1, . . . , xn(cid:105) separately, ignoring
its sequential position, and append the resulting predictions yi into
an output vector y.

Method Noise
ME
MEMM
CRF
MEMM 10%
CRF
10%

Error Min Error
3.47
31.83
1.17
2.18
1.85

3.20
4.26
1.17
2.18
1.84

Table 1: Performance of several sequential learners on the
signature-detection problem.

(cid:81)

(henceforth ME) and conditional random ﬁelds [Lafferty et
al., 2001] (henceforth CRFs). Another plausible sequen-
tial learning method to apply to this task are maximum-
entropy Markov models (MEMMs) [McCallum et al., 2000],
also called maximum-entropy taggers [Ratnaparkhi, 1999],
conditional Markov models [Klein and Manning, 2002],
and recurrent sliding windows [Dietterich, 2002].
In this
model,
the conditional probability of a label sequence y
given an instance sequence x is deﬁned to be Pr(y|x) =
The local model Pr(yi|yi−1, xi) is
i Pr(yi|yi−1, xi).
learned as follows. First one constructs an extended dataset,
which is a collection of non-sequential examples of the
form ((xi, yi−1), yi), where (xi, yi−1) denotes an instance
in which the original feature vector for xi is augmented by
adding a feature for yi−1. We will call (xi, yi−1) an extended
instance, and call yi−1 a history feature. Note that yi is the
class label for the extended example ((xi, yi−1), yi).

After constructing extended instances, one trains a
maximum-entropy conditional model from the extended
dataset. Inference is done by using a Viterbi search to ﬁnd
the best label sequence y.

MEMMs have a number of nice properties. Relative to
the more recently-proposed CRF model, MEMMs are easy to
implement, and (since no inference is done at learning time)
relatively quick to train. MEMMs can also be easily general-
ized by replacing the local model with one that uses a longer
“history” of k previous labels—i.e., a model of the form
Pr(yi|yi−1, . . . , yi−k, xi)—and replacing the Viterbi search
with a beam search. Such a learner scales well with the his-
tory size and number of possible classes y.

Unfortunately, as Table 1 shows, MEMMs perform ex-
tremely badly on the signature-detection problem, with an
error rate many times the error rate of CRFs.
In fact, on
this problem, MEMMs perform much worse than the non-
sequential maximum-entropy learner ME.2

The MEMM’s performance is better if one changes the
threshold used to classify examples. Letting ˆpi be the prob-
ability Pr(yi = +|xi, yi−1) as computed by MEMM, we
found, for each learner, the threshold θ such the rule [(yi =
+) ⇔ (ˆpi > θ)] gives the lowest test error rate. The column
labeled “Min Error” in Table 1 gives this “best possible” re-
sult. The “Min Error” for MEMMs is much improved, but
still higher than non-sequential ME.

The high error occurs because on many test email mes-

2We used the implementations of ME, MEMMs, and CRFs pro-
vided by Minorthird [Minorthird, 2004], with a limit of 50 optimiza-
tion iterations. This limit does not substantially change the results.

sages, the learned MEMM makes a false positive classiﬁca-
tion somewhere before the signature starts, and then “gets
stuck” and marks every subsequent line as part of a signature.
This behavior is not consistent with previously-described lim-
itations of MEMMs. It is known that MEMMs can represent
only a proper subset of the distributions that can be repre-
sented by CRFs [Lafferty et al., 2001]; however, this “label
bias problem” does not explain why MEMMs perform worse
than non-sequential ME, since MEMMs clearly can represent
strictly more distributions that ME.

Klein and Manning [2002] also describe an “observation
bias problem”, in which MEMMs give too little weight to
the history features. However, in this case, relative to the
weights assigned by a CRF, MEMM is seems to give too
much weight to the history features. To verify this, we en-
couraged the MEMM to downweight the history features by
adding noise to the training (not test) data: for each training
email/sequence x, we consider each feature-vector xi ∈ x
in turn, and with probability 0.10, we swap line xi (together
with its label yi) with some other xj, yj chosen uniformly
from the sequence. Adding this “sequence noise” almost dou-
bles the error rate for CRFs, but reduces the error rate for
MEMMs. (Of course, this type of noise does not affect non-
sequential ME.) This experiment supports the hypothesis that
MEMM is overweighting history features.

3 Stacked Sequential Learning
3.1 Description
The poor results for MEMM described above can be intu-
itively explained as a mismatch between the data used to train
the local models Pr(yi|yi−1, x) of the MEMM, and the data
used to test these models. With noise-free training data, it
is always the case that a signature line is followed by more
signature lines, so the MEMM’s local model tends to weight
the history feature yi−1 heavily. However, this strong regu-
larity does not hold when the model is applied to test data in
the course of executing the Viterbi algorithm, because then
the local model is applied to extended examples (xi, ˆyi−1),
where ˆyi−1 is a prediction made by the local model.

In theory, of course, this training/test mismatch is compen-
sated for by the Viterbi search, which is in turn driven by the
conﬁdence estimates produced by the local model. However,
if the assumptions of the theory are violated (for instance,
if there are high-order interactions not accounted for by the
maximum-entropy model), the local model’s conﬁdence esti-
mates may be incorrect.

One way to correct the training/test mismatch is to mod-
ify the extended dataset that is used to train the local model
so that the true previous class yi−1 in an extended instance
(xi, yi−1) is replaced by a predicted previous class ˆyi−1. Be-
low we will outline a way to do this.
Assume that one is given a sample S = {(xt, yt)} of size
m, and a sequential learning algorithm A. Previous work
on a meta-learning method called stacking [Wolpert, 1992]
suggests the following scheme for constructing a sample of
(x, ˆy) pairs in which ˆy is a vector of “predicted” class-labels
for x. First, partition S into K equal-sized disjoint sub-
sets S1, . . . , SK, and learn K functions f1, . . . , fK, where

Stacked Sequential Learning.
Parameters: a history size Wh, a future size Wf , and a cross-
validation parameter K.
Learning algorithm: Given a sample S = {(xt, yt)}, and a sequen-
tial learning algorithm A:
1. Construct a sample of predictions ˆyt for each xt ∈ S as fol-

lows:
(a) Split S into K equal-sized disjoint subsets S1, . . . , SK
(b) For j = 1, . . . , K, let fj = A(S − Sj)
(c) Let ˆS = {(xt, ˆyt) : ˆyt = fj(xt) and xt ∈ Sj}

2. Construct an extended dataset S(cid:48) of instances (x(cid:48)

t, yt) by con-
(cid:96)t(cid:105) where
verting each xt to x(cid:48)
x(cid:48)
i = (xi, ˆyi−Wh , . . . , ˆyi+Wf ) and ˆyi is the i-th component of
ˆyt, the label vector paired with xt in ˆS.

t as follows: xt

1, . . . , x(cid:48)

(cid:48) = (cid:104)x(cid:48)

3. Return two functions: f = A(S) and f(cid:48) = A(S(cid:48)).

Inference algorithm: given an instance vector x:

1. Let ˆy = f (x)
2. Carry out Step 2 above to produce an extended instance x(cid:48)

(using ˆy in place of ˆyt).

3. Return f(cid:48)(x(cid:48)).

Table 2: The sequential stacking meta-learning algorithm.

fj = A(S − Sj). Then, construct the set

ˆS = {(xt, ˆyt) : ˆy = fj(xt) and xt ∈ Sj}

In other words, ˆS pairs each xt with the ˆyt associated with
performing a K-fold cross-validation on S. The intent of this
method is that ˆy is similar to the prediction produced by an f
learned by A on a size-m sample that does not include x.

This procedure is the basis of the meta-learning algorithm
of Table 2. This method begins with a sample S and a se-
quential learning method A. In the discussion below we will
assume that A is ME, used for sequential data.
Using S, A, and cross-validation techniques, one ﬁrst pairs
with each xt ∈ S the vector ˆyt associated with performing
cross-validation with ME. These predictions are then used to
create a dataset S(cid:48) of extended instances x(cid:48), which in the sim-
plest case, are simply vectors composed of instances of the
form (xi, ˆyi−1), where ˆyi−1 is the (i − 1)-th label in ˆy.
The extended examples S(cid:48) are then used to train a model
f(cid:48) = A(S(cid:48)). If A is the non-sequential maximum-entropy
learner, this step is similar to the process of building a “local
model” for an MEMM: the difference is that the history fea-
tures added to xi are derived not from the true history of xi,
but are (approximations of) the off-sample predictions of an
ME classiﬁer.
At inference time, f(cid:48) must be run on examples that have
been extended by adding prediction features ˆy. To keep the
“test” distribution similar to the “training” distribution, f will
not be used as the inner loop of a Viterbi or beam-search
process; instead, the predictions ˆy are produced using a non-
sequential maximum-entropy model f that is learned from S.
The algorithm of Table 2 simply generalizes this idea from

ME to an arbitrary sequential learner, and from a speciﬁc his-
tory feature to a parameterized set of features.

In our experiments, we introduced one small but important
reﬁnement: each “history feature” ˆy added to an extended
example is not simply a predicted class, but a numeric value
indicating the log-odds of that class. This makes accessible
to f(cid:48) the conﬁdences previously used by the Viterbi search.

3.2 Initial results
We applied stacked sequential learning with ME as the base
learner (henceforth s-ME) to the signature-detection dataset.
We used K = 5, Wh = 1, and Wf = 0. The s-ME method
obtains an error rate of 2.63% on the signature-detection
task—less than the baseline ME method (3.20%) but still
higher than CRFs (1.17%). However, certain extensions dra-
matically improve performance.

For s-ME, the only impact of more “history” features is
to add new features to the extended instances; hence like
MEMMs, s-ME can efﬁciently handle large histories. On the
signature-detection task, increasing the history size to 11 re-
duces error (slightly) to 2.38%.

For s-ME, the extended instance for xi can include pre-
dicted classes not only of previous instances, but also of “fu-
ture” instances—instances that follow xi in the sequence x.
We explored different “window sizes” for s-ME, where a
“window size” of W means that Wh = Wf = W , i.e., the W
previous and W following predicted labels are added to each
extended instance. A value of W = 20 reduces error rates to
only 0.71%, a 46% reduction from CRF’s error rate of 1.17%.
This improvement is statistically signiﬁcant.3

Finally, stacked sequential learning can be applied to any
learner—in particular, since the extended examples are se-
quential, it can be applied any sequential learner. We evalu-
ated stacked sequential CRFs (henceforth s-CRFs) with vary-
ing window sizes on this problem. A value of W = 20 re-
duces error rates to 0.68%, a statistically signiﬁcant improve-
ment over CRFs. However, for moderately large window val-
ues, there was little performance difference between s-CRF
and s-ME.

3.3 Discussion
A graphical view of a MEMMs is shown in Figure 1(a). We
use the usual convention in which nodes for known values are
shaded. Each node is associated with a maximum-entropy
conditional model which deﬁnes a probability distribution
given its input values.

Figure 1(b) presents a similar graphical view of the classi-
ﬁer learned by sequential stacking with Wh = 1 and Wf = 0.
Inference in this model is done in two stages: ﬁrst the middle
layer is inferred from the bottom later, then the top layer is
inferred from the middle layer. The nodes in the middle layer
are partly shaded to indicate that their hybrid status—they are
considered outputs by the model f, and inputs by the model
f(cid:48).

3A two-tailed paired t-test rejects with > 95% conﬁdence the
null hypothesis that the difference in error rate between s-ME and
CRF on a randomly selected sequence x has a mean of zero.

(a) MEMM

(b) Sequential stacking, Wh = 1, Wf = 0

(c) Sequential stacking, W = 2

Figure 1: Graphical views of alternative sequential-stacking schemes.

Task
A/aigen
A/ainn
A/aix
T/aigen
T/ainn
T/aix
1/video
2/video
mailsig

MEMM
53.61
70.09
13.86
0.30
1.36
3.51
11.39
8.86
31.83

ME
8.02
6.61
5.02
2.60
1.39
1.25
12.66
8.86
3.47

CRF
20.35
2.14
6.83
2.39
0.28
5.26
12.66
7.59
1.17

s-ME s-CRF
5.78
6.91
1.67
3.65
4.59
11.79
0.00
1.92
0.00
0.28
0.05
4.44
12.66
13.92
3.80
7.59
0.77
1.08

Table 3: Error rate comparison of different sequential algo-
rithms on a set of nine benchmark tasks.

One way to interpret the hybrid layer is as a means of mak-
ing the inference more robust. If the middle-layer nodes were
treated as ordinary unobserved variables, the top-layer con-
ditional model (f(cid:48)) would rely heavily on the conﬁdence as-
sessments of the lower-layer model (f). Forcing f(cid:48) treat these
variables as observed quantities allows f(cid:48) to develop its own
model of how the ˆy predictions made by f correlate with the
actual outputs y. This allows f(cid:48) to accept or downweight
f’s predictions, as appropriate. As suggested by the dotted
line in the ﬁgure, stacking conceptually creates a “ﬁrewall”
between f and f(cid:48), insulating f(cid:48) from possible errors in conﬁ-
dence made by f.

Figure 1(c) shows a sequential stacking model with a win-
dow of Wh = Wf = 2. To simplify the ﬁgure, only the edges
that eventually lead to the node Yi are shown.

To conclude our discussion, we note that as described,
sequential stacking increases run-time of the base learning
method by approximately a constant factor of K + 2. (To see
this, note sequential stacking requires training K + 2 classi-
ﬁers: the classiﬁers f1, . . . , fK used in cross-validation, and
the ﬁnal classiﬁers f and f(cid:48).) When data is plentiful but train-
ing time is limited, it is also possible to simply split the orig-
inal dataset S into two disjoint halves S1 and S2, and train
two classiﬁers f and f(cid:48) from S1 and S(cid:48)
2 respectively (where
S(cid:48)
2 is S2, extended with the predictions produced by f). This
scheme leaves training time approximately unchanged for a
linear-time base learner, and decreases training time for any
base learner that requires superlinear time.

4 Further Experimental Results
4.1 Additional segmentation tasks
We also evaluated non-sequential ME, MEMMs, CRFs, s-
ME, and s-CRFs on several other sequential partitioning
tasks. For stacking, we used K = 5 and a window size of
Wh = Wf = 5 on all problems. These were the only parame-
ter values explored in this section, and no changes were made
to the sequential stacking algorithm, which was developed
based on observations made from the signature-detection task
only.

One set of tasks involved classifying lines from FAQ doc-
uments with labels like “header”, “question”, “answer”, and
“trailer”. We used the features adopted by McCallum et al

Figure 2: Comparison of the error rates for s-ME with the
error rates of ME, MEMM, and CRFs.

[2000] and the three tasks (ai-general, ai-neural-nets, and aix)
adopted by Dietterich et al [2004]. The data consists of 5-
7 long sequences, each sequence corresponding to a single
FAQ document; in total, each task contains between 8,965
and 12,757 labeled lines. Our current implementation of se-
quential stacking only supports binary labels, so we consid-
ered the two labels “trailer” (T) and “answer” (A) as separate
tasks for each FAQ, leading to a total of six new benchmarks.

Xi-1XiXi+1Yi-1YiYi+1Xi-1XiXi+1Yi-1YiYi+1^^^Yi-1YiYi+1Xi-1XiXi+1Yi-1YiYi+1^^^Yi-1YiYi+1Xi-2Yi-2^Yi-2Xi+1Yi+1^Yi+1 0 5 10 15 20 25 0 5 10 15 20 25Error: Stacked MaxentError: other learnervs Maxentvs MEMMvs CRFx=yAnother set of tasks were video segmentation tasks, in
which the goal is to take a sequence of video “shots” (a
sequence of adjacent frames taken from one camera) and
classify them into categories such as “anchor”, “news” and
“weather”. This dataset contains 12 sequences, each corre-
sponding to a single video clip. There are a total of 418 shots,
and about 700 features, which are produced by applying LDA
to a 5x5, 125-bin RGB color histogram of the central frame
of the shot. (This data was provided by Yik-Cheung Tam and
Ming-yu Chen.) We constructed two separate video partition-
ing tasks, corresponding to the two most common labels.

All eight of these additional

tasks are similar to the
signature-detection task in that they contain long runs of iden-
tical labels, leading to strong regularities in constructed his-
tory features. Error rates for the learning methods on these
eight tasks, in addition to the previous signature-detection
task, are shown in Table 3. In each case a single train/test
split was used to evaluate error rates. The bold-faced entries
are the lowest error rate on a row.

We observe that MEMMs suffer extremely high error rates
on two of the new tasks (ﬁnding “answer” lines for ai-general
and ai-neural-nets), suggesting that the “anomalous” behavior
shown in signature-detection may not be uncommon, at least
in sequential partitioning tasks.

Also, comparing s-ME to ME, we see that s-ME improves
the error rate in 8 of 9 tasks, and leaves it unchanged once.
Furthermore, s-ME has a lower error rate than CRFs 7 of 9
times, and has the same error rate once. There is only one
case in which MEMMs have a lower error rate than s-ME.

Overall, s-ME seems to be preferable to either of three
older approaches (ME, MEMMs, and CRFS). This is made
somewhat more apparent by the scatter plot of Figure 2. On
this plot, each point is placed so the y-axis position is the er-
ror of s-ME, and the x-axis position is the error of an earlier
learner; thus points below the line y = x are cases where s-
ME outperforms another learner. (For readability, the range
of the x axis is truncated—it does not include the highest er-
ror rates of MEMM.)

Stacking also improves CRF on some problems, but the
effect is not as consistent: s-CRF improves the error rate on 5
of 9 tasks, leaves it unchanged twice, and increases the error
rate twice. In the table, one of the two stacked learners has
the lowest error rate on 8 of the 9 tasks.

Applying a one-tailed sign test, the improvement of s-ME
relative to ME is statistically signiﬁcant (p > 0.98), but the
improvement of s-CRF relative to CRF is statistically not
(p > 0.92). The sign test does not consider the amounts
by which error rates are changed, however. From the ﬁg-
ures and tables, it is clear that error rates are often lowered
substantially, and only once raised by more than a very small
proportion (for the “A/aix” benchmark with CRFs).
4.2 A sequence classiﬁcation task
As a ﬁnal test, we explored one additional non-trivial task:
classifying popular songs by emotion. This task was consid-
ered after all code development was complete, and hence is
a completely prospective test of sequential stacking. A col-
lection of 201 popular songs was annotated by two students
on a ﬁve-point scale: “very happy”(5), “happy”(4), “neu-

Task
music2
music5

MEMM
28.14
64.97

ME
21.40
67.00

CRF
21.40
67.00

s-ME s-CRF
13.50
18.51
63.45
64.46

Table 4: Comparison of different sequential algorithms on a
music classiﬁcation task.

tral”(3), “sad”(2) and “very sad”(1). The Pearson’s corre-
lation [Walpole et al., 1998] was used as a measure of inter-
annotator agreement. The Pearson’s correlation coefﬁcient
ranges from −1 (perfect disagreement) to +1 (perfect agree-
ment),and the inter-annotator agreement between the two stu-
dents was 0.643.

To learn a song classiﬁer, we represented each song as a
sequence of 1-second long “frames”, each frame being la-
beled with the emotion for the song that contains it. We then
learned sequence classiﬁers from these labeled sequences. To
classify an unknown song, we used the sequence classiﬁer to
label the frames for the song, and ﬁnally labeled the song with
the most frequent predicted frame label.

The “frames” are produced by extracting certain numerical
properties from a waveform representation of the song every
20ms, and then averaging over 1-second intervals. Each 1-
second frame has as features the mean and standard deviation
of each property. The numerical properties were computed
using the Marsyas toolkit [Tzanetakis and Cook, 2000], and
are based on the short-time Fourier transform, tonality, and
Cepstral coefﬁcients.

The music dataset contains 52,188 frames from the 201
songs, with 130 features for each frame. We looked at two
versions of the problem: predicting all of the ﬁve labels (mu-
sic5), and predicting only “happy” versus “sad” labels (mu-
sic2). Preliminary experiments suggested that large windows
were effective, thus we used the following parameters in the
experiments: K = 2 and a window size of Wh = Wf = 25
on music5 problem, and K = 5 and Wh = Wf = 25 on
music2.

The results are summarized in Table 4. Both s-CRF and
s-ME outperform their non-stacking counterparts, and s-ME
outperforms CRFs. Furthermore, s-CRF improves substan-
tially over unstacked CRFs on the two-class problem, reduc-
ing the error rate by more than 27%.

5 Conclusions
Sequential partitioning tasks are sequential classiﬁcation
tasks characterized by long runs of identical labels: exam-
ples of these tasks include document analysis, video segmen-
tation, and gene ﬁnding. In this paper, we have evaluated the
performance of certain well-studied sequential probabilistic
learners to sequential partitioning tasks. It was observed that
MEMMs sometimes obtain extremely high error rates. Er-
ror analysis suggests that this problem is neither due to “label
bias” [Lafferty et al., 2001] nor “observation bias” [Klein and
Manning, 2002], but to a mismatch between the data used to
train the MEMM’s local model, and the data on which the
MEMM’s local model is tested. In particular, since MEMMs
are trained on “true” labels and tested on “predicted” labels,
the strong correlations between adjacent labels associated se-

quential partitioning tasks can be misleading to the MEMM’s
learning method.

Motivated by these issues, we derived a novel method in
which cross-validation is used correct this mismatch. The
end result is a meta-learning scheme called stacked sequen-
tial learning. Sequential stacking is simple to implement,
can be applied to virtually any base learner, and imposes an
constant overhead in learning time (the constant being the
number of cross-validation folds plus two). In experiments
on several partitioning tasks, sequential stacking consistently
improves the performance of two non-sequential base learn-
ers, often dramatically. On our set of benchmark problems,
sequential stacking with a maximum-entropy learner as the
base learner outperforms CRFs 7 of 9 times, and ties once.
Also, on a prospective test conducted on a completely new
task, sequential stacking improves the performance of both
CRFs and maximum-entropy learners, leading in one case to
a 27% reduction in error over conventional CRFs.

One of the more surprising results (for us) is that sequen-
tial stacking also often improves performance of conditional
random ﬁelds, a learner speciﬁcally designed for sequential
tasks. In a longer version of this paper, we conducted simi-
lar with two margin-based base learners: the non-sequential
voted perceptron algorithm (VP)[Freund and Schapire, 1998]
and a voted-perceptron based training scheme for HMMs pro-
posed by Collins (VPHMMs) [Collins, 2002], with qualita-
tively similar results.

Some initial experiments on a named entity recognition
problem suggest that sequential stacking does not improve
performance on non-partitioning problems; however, in fu-
ture work, we plan to explore this issue with more detailed
experimentation.
Acknowledgements
The authors wish to thank many friends and colleagues for
input, in particular David McAllister. We are also grateful to
Yik-Cheung Tam and Ming-yu Chen for providing the video-
segmentation data, and to Chih-yu Chao for the popular mu-
sic dataset labels. This material is based upon work sup-
ported by the Defense Advanced Research Projects Agency
(DARPA). Any opinions, ﬁndings and conclusions or recom-
mendations expressed in this material are those of the au-
thors and do not necessarily reﬂect the views of the Defense
Advanced Research Projects Agency, or the Department of
Interior-National Business Center.

References
[Berger et al., 1996] Adam L. Berger, Vincent J. Della
Pietra, and Stephen A. Della Pietra. A maximum entropy
approach to natural language processing. Comput. Lin-
guist., 22(1):39–71, 1996.

[Carvalho and Cohen, 2004] Vitor

and
Learning to extract signature
William W. Cohen.
and reply lines from email.
In Proceedings of the Con-
ference on Email and Anti-Spam 2004, Mountain View,
California, 2004.

Carvalho

R.

[Collins, 2002] Michael Collins. Discriminative training
methods for hidden markov models: Theory and experi-

ments with perceptron algorithms. In Empirical Methods
in Natural Language Processing (EMNLP), 2002.

[Dietterich et al., 2004] Thomas G. Dietterich, Adam
Ashenfelter, and Yaroslav Bulatov. Training conditional
random ﬁelds via gradient tree boosting. In Proceedings of
the 21th International Conference on Machine Learning
(ICML), 2004.

[Dietterich, 2002] Thomas G. Dietterich. Machine learning
for sequential data: A review.
In Structural, Syntactic,
and Statistical Pattern Recognition, pages 15–30. Springer
Verlag, New York, 2002.

[Freund and Schapire, 1998] Yoav Freund and Robert E.
Schapire. Large margin classiﬁcation using the percep-
tron algorithm. In Computational Learning Theory, pages
209–217, 1998.

[Klein and Manning, 2002] Dan Klein and Christopher D.
Manning. Conditional structure versus conditional esti-
mation in nlp models. In Workshop on Empirical Methods
in Natural Language Processing (EMNLP), 2002.

[Lafferty et al., 2001] John Lafferty, Andrew McCallum,
and Fernando Pereira. Conditional random ﬁelds: Proba-
bilistic models for segmenting and labeling sequence data.
In Proceedings of the International Conference on Ma-
chine Learning (ICML-2001), Williams, MA, 2001.

[McCallum et al., 2000] Andrew McCallum, Dayne Freitag,
and Fernando Pereira. Maximum entropy markov models
for information extraction and segmentation. In Proceed-
ings of the International Conference on Machine Learning
(ICML-2000), pages 591–598, Palo Alto, CA, 2000.

[Minorthird, 2004] Minorthird:

fying names and ontological
ing heuristics
http://minorthird.sourceforge.net, 2004.

Methods
relations
inducing regularities

for

for
identi-
in text us-
from data.

[Ratnaparkhi, 1999] Adwait Ratnaparkhi. Learning to parse
natural language with maximum entropy models. Machine
Learning, 34, 1999.

[Toutanova et al., 2003] Kristina Toutanova, Dan Klein,
Christopher D. Manning, and Yoram Singer. Feature-rich
part-of-speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL 2003, 2003.

[Tzanetakis and Cook, 2000] George Tzanetakis and Perry
Cook. MARSYAS: A Framework for Audio Analysis. Or-
ganized Sound, Cambridge University Press 4(3), 2000.

[Walpole et al., 1998] Ronald E. Walpole, Raymond H. My-
ers, Sharon Myers and Sharon L. Myers. Probability and
Statistics for Engineers and Scientists. In Prentice Hall,
1998.

[Wolpert, 1992] David H. Wolpert. Stacked generalization.

Neural Networks, 5:241–259, 1992.


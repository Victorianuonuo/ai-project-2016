Supervised Latent Semantic Indexing Using Adaptive Sprinkling

Sutanu Chakraborti, Rahman Mukras, Robert Lothian, Nirmalie Wiratunga,

Stuart Watt, and David Harper

{sc, ram, rml, nw, sw, djh}@comp.rgu.ac.uk

School of Computing, The Robert Gordon University,

St Andrew Street, Aberdeen, UK. AB25 1HG

Abstract

Latent Semantic Indexing (LSI) has been shown to
be effective in recovering from synonymy and pol-
ysemy in text retrieval applications. However, since
LSI ignores class labels of training documents, LSI
generated representations are not as effective in
classiﬁcation tasks. To address this limitation, a
process called ‘sprinkling’ is presented. Sprinkling
is a simple extension of LSI based on augmenting
the set of features using additional terms that en-
code class knowledge. However, a limitation of
sprinkling is that it treats all classes (and classi-
ﬁers) in the same way. To overcome this, we pro-
pose a more principled extension called Adaptive
Sprinkling (AS). AS leverages confusion matrices
to emphasise the differences between those classes
which are hard to separate. The method is tested on
diverse classiﬁcation tasks, including those where
classes share ordinal or hierarchical relationships.
These experiments reveal that AS can signiﬁcantly
enhance the performance of instance-based tech-
niques (kNN) to make them competitive with the
state-of-the-art SVM classiﬁer. The revised repre-
sentations generated by AS also have a favourable
impact on SVM performance.

1 Introduction
Support Vector Machines (SVM) and k-nearest neighbours
(kNN) are two well-studied machine learning approaches to
text classiﬁcation. They are both applied on the vector space
model deﬁned over a bag of words (BOW). In this model,
documents are represented as vectors over a set of dimen-
sions, each corresponding to a word in the BOW. Despite its
wide usage, it has been argued that the BOW approach fails to
scale up for classiﬁcation tasks, principally because of its in-
ability to handle polysemy and synonymy [Sebastiani, 2002].
Existing research reveals two broad ways of overcoming
this limitation. The ﬁrst involves using background knowl-
edge such as external thesauri or repositories [Zelikovitz and
Hirsh, 2001; Gabrilovich and Markovitch, 2005]. The sec-
ond approach is introspective, in that it does not rely on
any external knowledge. Typically complex features are ex-
tracted from the BOW, using factor analysis [Deerwester et

al., 1990], word clustering [Bekkerman et al., 2003] or rule
induction [Cohen and Singer, 1996].

Introspective techniques rely on exploiting the duality of
term and document spaces. Documents are regarded as sim-
ilar when they have similar terms; but terms are, in turn, re-
garded as similar when they occur in similar documents. La-
tent Semantic Indexing (LSI) is an introspective technique
that uses factor analysis to resolve this circularity. Words
and documents are mapped to a lower-dimensional “concept”
space. LSI has recently been applied to real world text clas-
In [Gee, 2003] LSI has been applied
siﬁcation problems.
to spam classiﬁcation, and performances competitive with
Naive Bayes classiﬁer reported. In a study by Zelikovitz et al
[2001], LSI-based classiﬁers have been extended to accom-
modate background knowledge.

However, an inherent limitation of LSI when applied to
classiﬁcation is that it fails to exploit class knowledge of
training documents. If taken into account, class knowledge
can lead LSI to promote inferred associations between words
representative of the same class, and attenuate word associ-
ations otherwise. In this paper, we combine LSI with class
knowledge to produce revised document representations from
the vector space model. We show that both kNN and SVM
beneﬁt from these revised representations.

Our work expands on and extends recent work by Chakrab-
orti et al [2006], where a simple approach called “sprinkling”
was proposed to integrate class knowledge into LSI. The
basic idea involves encoding class labels as artiﬁcial terms
which are appended to training documents. In the binary clas-
siﬁcation case, for example, two additional terms correspond-
ing to classes c1 and c2 are appended to documents belonging
to c1 and c2 respectively. LSI is performed on the augmented
term-document matrix, resulting in class-speciﬁc word asso-
ciations being promoted. Thus, documents and words belong-
ing to the same class are pulled closer to each other. To fur-
ther emphasise class knowledge, more than one term could be
sprinkled per class.

The basic sprinkling approach treats all classes equally.
This is a limitation for the many multi-class problems with
explicit relationships between classes. Two examples are hi-
erarchical classes (e.g. Yahoo directory) and ordinal classes
(e.g. ratings 1 to 5 in movie review, each rating treated as a
class). Also, sprinkling is blind to classiﬁer needs. In reality,
pairs of classes found to be easily separable by one classiﬁer

IJCAI-07

1582

could be difﬁcult to discriminate for another.

In this paper, we present theoretical insights to explain
why sprinkling works, and validate the same empirically. We
then propose a principled extension, called Adaptive Sprin-
kling (AS) that addresses the limitations mentioned above.
The key idea behind AS is to leverage confusion matrices re-
ported by classiﬁers to emphasise differences between those
classes which are hard to separate. We show that this ap-
proach implicitly takes into account explicit relationships be-
tween classes in multi-class problems. Experimental results
are reported on three different types of classiﬁcation problems
involving disjoint, ordinal and hierarchical classes.

The contributions of this paper are threefold. First, we
show that sprinkled LSI can be used to signiﬁcantly improve
the performance of instance based learners like kNN, to make
them competitive with state-of-the-art classiﬁers like SVM.
This has practical implications in situations where fast in-
cremental updates and lazy learning, which are strengths of
kNN, are desirable. Secondly, our research highlights the
wealth of information hidden in confusion matrices, which
can be exploited to improve classiﬁcation. The classiﬁer-
speciﬁc nature of this information is especially useful in this
regard. Thirdly, we show that AS-generated LSI representa-
tions have a favourable inﬂuence on SVM performance.

2 Sprinkling

2.1 Latent Semantic Indexing

The purpose of LSI is to extract a smaller number of dimen-
sions that are more robust indicators of meaning than indi-
vidual terms. LSI uses the Singular Value Decomposition
(SVD) to arrive at these latent dimensions. In principle, SVD
achieves a two-mode factor analysis and positions both terms
and documents in a single space deﬁned over the extracted
dimensions. SVD breaks down the original term document
matrix into three matrices, two of which show the revised rep-
resentations of words and documents in terms of the new di-
mensions, and the third associates “weights” to these dimen-
sions. The thesis behind LSI is that less important dimen-
sions correspond to “noise” due to word-choice variability.
A reduced rank approximation to the original matrix is con-
structed by dropping these noisy dimensions. This approx-
imation is a smoothed (blurred) version of the original, and
is expected to model associations between terms and docu-
ments in terms of the underlying concepts more accurately.
An interesting property of SVD is that the generated approx-
imation is the closest matrix of its rank to the original in the
least-squares sense.

While LSI has been shown to be successful in retrieval ap-
plications, it has certain limitations when applied to classiﬁ-
cation tasks. As noted earlier, since LSI is blind to class labels
of training documents, the extracted dimensions are not nec-
essarily the best in terms of discriminating between classes.
Also, infrequent words with high discriminatory power may
be ﬁltered out by LSI. The idea of sprinkling was conceived
as a simple and intuitive way of addressing these limitations.

s
t
n
e
m
u
c
o
d

1
c

2
c

terms

1  1  1  0  0  0

1  0  1  0  0  0

1  1  1  0  0  0

0  0  0  1  1  1

0  0  0  1  0  1

1  0  0  1  1  1

Sprinkle

1  1  1  0  0  0

1  0  1  0  0  0

1  1  1  0  0  0

0  0  0  1  1  1

0  0  0  1  0  1

1  0  0  1  1  1

1  0

1  0

1  0

0  1

0  1

0  1

1.10   0.74   1.04  -0.02   0.02  -0.02

 1.04  -0.02

LSI

0.90   0.60   0.84   0.01   0.03   0.01

 0.84   0.01

1.10   0.74   1.04  -0.02   0.02  -0.02

 1.04  -0.02

0.27  -0.08  -0.11   1.03   0.74   1.03

-0.11   1.03

0.21  -0.07  -0.09   0.83   0.60   0.83

-0.09   0.83

0.60   0.12   0.18   1.10   0.80   1.10

 0.18   1.10

Test document

1  0  0  1  1  1

    Text

Classifier

Result (c  or c  )
2

1

1.10   0.74   1.04  -0.02   0.02  -0.02

0.90   0.60   0.84   0.01   0.03   0.01

1.10   0.74   1.04  -0.02   0.02  -0.02

0.27  -0.08  -0.11   1.03   0.74   1.03

0.21  -0.07  -0.09   0.83   0.60   0.83

0.60   0.12   0.18   1.10   0.80   1.10

unsprinkle

Figure 1: An Example of Sprinkling.

2.2 Sprinkling

We illustrate the basic idea behind sprinkling with an ex-
ample.
Figure 1 shows a trivial term-document matrix
constructed from six documents belonging to two different
classes.
Instead of performing LSI directly on this matrix,
sprinkling augments the feature set with artiﬁcial terms cor-
responding to the class labels. These terms act as carriers of
class knowledge. In Figure 1, terms t1, and t2 are “sprinkled”
to documents belonging to classes c1, and c2 respectively.
SVD is then performed on the augmented term document ma-
trix. Noisy dimensions corresponding to low singular values
are dropped and a lower rank approximation constructed in
the usual manner. To make training document representa-
tions compatible with test documents, the sprinkled dimen-
sions are dropped. This is referred to as “unsprinkling” in
Figure 1. The test and training documents can now be com-
pared in the usual manner using kNN or SVM. While in the
example above we used one sprinkled term per class, in prin-
ciple we can sprinkle more terms per class, to boost the con-
tribution of class knowledge to the classiﬁcation process.

2.3 Adaptive Sprinkling

Adaptive Sprinkling (AS) is motivated by the need to address
two main limitations of the basic sprinkling approach. First,
in a multi-class situation, sprinkling treats all classes equally
and disregards relationships between classes, as deﬁned over
ordinal and hierarchical classes. Furthermore, even in cases
where classes have no explicit relation between them, some
classes are more easily separable than others, so the number
of sprinkled terms should depend on the complexity of the
class decision boundary. Secondly, the classes found confus-
ing by a kNN classiﬁer could be different from those found
confusing by SVM, and ideally the sprinkling process should
adapt to classiﬁer needs.

The key to AS is its exploitation of the confusion matrices
generated by classiﬁers like kNN and SVM. A confusion ma-
trix compares a classiﬁer’s predictions against expert judge-
ments on a class-by-class basis. The non-diagonal values in
this matrix are indicative of classes that the classiﬁer ﬁnds
hard to separate; the lower the values, the more easily sep-
arable the classes. Referring to classiﬁcation errors in the
example confusion matrix of Figure 2, we readily infer that
classes 1 and 9 are easy to tell apart, while classes 1 and 2 are
harder to discriminate. AS is based on the intuition that rela-
tively more sprinkled terms are to be allocated between hard-

IJCAI-07

1583

1. comp.graphics

130    10      14     18     20       0       5        2      1

2. comp.os.ms-windows.misc

 31    110     13     19     19       2        3       2      1

3. comp.sys.ibm.pc.hardware

 25     19     111    36       6       1        2       0      0

4. comp.sys.mac.harware

15       1       41    130      7       1       3       2       0

5. comp.windows.xp

34      16       5       6     135      0       2       2       0

6. rec.autos

19       1        1       7        3     148     16     4       1

7. rec.motorcycles

13       1        2       5        5      30    143     1       0

t
n
e
m
e
g
d
u

j
 
t
r
e
p
x
e

8. rec.sport.baseball

 4        3        3      13       6       9       7     143   12

9. rec.sport.hockey

 4        3       2       4        2       2        1      12    170

classifiers predictions

Figure 2: A confusion matrix over the 20NG hierarchy.
Shaded regions correspond to the comp and rec subtrees.

for  i = 1  to  m-1  {      /* m is the number of classes. */

      for  j = i+1  to  m  {

1.

Compute normalized mutual class complexity between classes

c  ,ci

j

as follows:

mcc       ( i , j ) =

norm

mcc( i , j )

mcc( i , j )

max

s =   MSL     mcc        ( i , j )

norm

Sprinkle s terms in all documents belonging to class      and s others

c i

in all documents belonging to class

c j

2.

3.

}

}

Figure 3: Determining the number of Sprinkled terms.

to-discriminate classes. Interestingly, we found that confu-
sion matrices also implicitly carry information about explicit
class relationships as in ordinal and hierarchical classes. For
example, in Figure 2, we see that the two shaded regions cor-
respond to confusion between classes within the comp and
rec subtrees of 20 newsgroups (20NG) [Lang, 1995]. The
confusion between classes from the two disjoint subtrees is
smaller.

(cid:2)

(cid:2)

i qij, and P (j|i) = qij /

AS determines the number of sprinkled terms for each class
from the confusion matrix. Let qij be a non-diagonal element
of the confusion matrix Q, showing the number of documents
of class ci being misclassiﬁed as class cj. We deﬁne proba-
bilities P (i|j) and P (j|i) as the probability of class ci being
misclassiﬁed as class cj, and vice versa, respectively. These
probabilities can be estimated from the confusion matrix as
follows: P (i|j) = qij/
j qij.
We then deﬁne the “mutual complexity” between classes ci
and cj as mcc(i, j) = [P (i|j) + P (j|i)]/2. The asymmet-
ric confusion matrix Q is now transformed into a mutual
complexity matrix M , which is symmetric. The pseudo-
code in Figure 3 shows how sprinkled terms can be gener-
ated based on the matrix M . The maximum sprinkling length
M SL is empirically determined. In our experiments we used
M SL = 8. We note that the mutual class complexity val-
ues are normalised and used as weights to vary the number
of sprinkled terms as a fraction of M SL. Thus the inﬂuence
of class knowledge is greater for those classes that are more
difﬁcult to discriminate.

2.4 Why does Sprinkling Work?
The improved performance of sprinkled LSI in classiﬁcation
tasks can be explained using empirical observations made in

Document Clustering W/B and mean square error with sprinkling 
0.16

0.16

Term Clustering: W/B and mean square error with sprinkling

8

The number alongside each marker
is the number of terms sprinkled

 

r
o
r
r
E
e
r
a
u
q
S
n
a
e
M

 

0.15

0.14

0.13

0.12

0.11

0.1

0.1

 

r
o
r
r
E
e
r
a
u
q
S
n
a
e
M

 

0.15

0.14

0.13

0.12

0.11

0.1

6

4

2

0

0.15

0.2

W/B

0.25

0.3

0.35

8

The number alongside each marker
is the number of terms sprinkled

6

4

2

0

0.17

0.18

W/B

0.19

0.2

Figure 4: Document and Term Clustering.

[Kontostathis and Pottenger, 2006]. Their work reveals close
correspondence between LSI and higher order associations
between terms. A word w1 is said to have a ﬁrst order as-
sociation with another word w2, if they co-occur in at least
one document. w1 and w2 share a second order association
if there is at least one term w3 that co-occurs with w1 and
w2 in distinct documents. Similarly, we can extend this to
orders higher than 2. Kontostathis and Pottenger provide ex-
perimental evidence to show that LSI boosts similarity be-
tween terms sharing higher order associations. In the light of
this observation, it is interesting to note that sprinkled terms
boost second-order associations between terms related to the
same class, hence bringing them closer.

We carried out an analysis of the effect of sprinkling on
LSI, to verify the hypothesis that sprinkling leads to bet-
ter term and document representations. Starting with docu-
ment representations generated by sprinkled LSI and treat-
ing each class as a cluster, we compute the “within-cluster”
and “between-cluster” point scatters [Hastie et al., 2001]
that measure cluster separability. The ratio of within- and
between-cluster point scatters, referred to as the W/B mea-
sure, is used as a measure of cluster separability. The lower
the value, the more separable the clusters. For analysis of
term clustering, terms prototypical of classes were manually
identiﬁed and the impact of sprinkling on their revised repre-
sentations investigated.

Figure 4 shows that with increased number of sprinkled
terms, the W/B measure falls conspicuously. However there
is a second factor to be taken into account. Sprinkled LSI dis-
torts the original term document matrix D to a class-enriched
LSI approximation DS. However, DS is no longer the best
k-rank approximation to the D in the least-square sense. The
vertical axes of the graphs in Figure 4 show the mean square
of errors between D and DS. We see that the reduction in
W/B achieved by sprinkling is at the cost of losing informa-
tion on D. Thus very large number of sprinkled terms may be
detrimental to classiﬁcation performance, as it may overem-
phasise class-knowledge. Ideally we would like a trade off
between “under-” and “over-sprinkling”, that gives us the best
of both worlds: improve class-discrimination while not over-
looking speciﬁc patterns in D.

3 Empirical Evaluation

We evaluated Adaptive Sprinkling on three types of clas-
siﬁcation problems. The ﬁrst involves hierarchical classes,
which have an is-a taxonomy deﬁned over them. The second
type has an ordinal relationship deﬁned between classes. For

IJCAI-07

1584

example, a textual review accompanied by a rating of 1 (on a
10 point scale) is expected to be more similar to one rated at
2 than another at 10. If numeric ratings are treated as class la-
bels, similarity between classes is a function of this ordering.
Finally, we consider orthogonal problems where classes bear
no explicit relationship to each other.

3.1 Experimental Methodology
We used the following datasets in our experiments:
1. The hierarchical dataset: This dataset was formed from
the 20 Newsgroups collection [Lang, 1995] which has seven
sub-trees: comp, rec, talk, alt, misc, soc, and sci. We selected
the comp and rec sub-trees which contain 5 and 4 classes (cor-
responding to leaf-nodes) respectively. We used 500 docu-
ments from each of these nine classes.
2. The ordinal dataset: Classiﬁcation between ordinal
classes is an interesting problem in sentiment analysis litera-
ture [Pang and Lee, 2005]. However, due to the relative youth
of the ﬁeld, no suitable benchmark datasets was readily avail-
able. We therefore compiled a new dataset from reviews on
the “actors and actresses” sub-topic of the Rateitall.com opin-
ion website. Each review contained an integer rating (1 to 5
inclusive) assigned by the author. These ratings were used
as the class labels. We removed all reviews having less than
10 words, and created 5 equally distributed classes, each with
500 reviews.
3. The orthogonal dataset: We used the acq, crude, and
earn classes of the Reuters-21578 collection [Reuters, 1997]
to form this dataset. 500 documents were selected from each
class, such that each document belongs to at most one class.
All three datasets underwent similar pre-processing. After
stop word removal and stemming, binary valued term-docum-
ent matrices were constructed. For each of the datasets, Infor-
mation Gain (IG) [Sebastiani, 2002] was used to select the top
1000 discriminating words. For experiments using SVM, we
implementation [Joachims, 1998]. A
used the SVM
linear kernel was used as this was found to be best for text
classiﬁcation problems [Joachims, 1998]. We use accuracy
as a measure of classiﬁer effectiveness since this is known to
be appropriate for single labelled documents in datasets with
equal class distributions [Gabrilovich and Markovitch, 2004].
For all datasets we performed classiﬁcation using 10 equally
sized train-test pairs, and used the paired t-test to assess sig-
niﬁcance.

multiclass

3.2 The Effect of Sprinkling on Confusion

Matrices

As described in Section 2.3, high off-diagonal values in a con-
fusion matrix indicate classes that the classiﬁer ﬁnds hard to
separate. This forms the intuitive basis for using the con-
fusion matrix to generate the sprinkling codes.
In our ex-
periments, a 5-fold cross-validation on the raw training data
yields ﬁve confusion matrices, which are used to construct an
average confusion matrix Q. Sprinkled terms are generated
based on Q, and LSI is performed on the sprinkled repre-
sentation. The same classiﬁer is then applied to the revised
representations, yielding a new confusion matrix Q(cid:2). Com-
paring Q and Q(cid:2) provides direct evidence of the quality of
the revised representation.

A: Before sprinkling (hierarchical dataset)

B: After sprinkling (hierarchical dataset)

classifier

classifier

C: Before sprinkling (ordinal dataset)

D: After sprinkling (ordinal dataset)

classifier

classifier

E: Before sprinkling (orthogonal dataset)

F: After sprinkling (orthogonal dataset)

t
r
e
p
x
e

t
r
e
p
x
e

t
r
e
p
x
e

comp.graphics

comp.os.ms-windows.misc

comp.sys.ibm.pc.hardware

comp.sys.mac.harware

comp.windows.xp

rec.autos

rec.motorcycles

rec.sport.baseball

rec.sport.hockey

rating 1

rating 2

rating 3

rating 4

rating 5

acq

crude

earn

classifier

classifier

Figure 5: Confusion matrices before (left column) and after
(right column) sprinkling

Figure 5 is a qualitative illustration of the effects of AS on
the initial confusion matrices, which result from applying a
kNN classiﬁer to three datasets. Each element of the matrix is
mapped onto a cell colour. Light colours signify many entries
in that cell, dark ones signify few.
Ideally all cells except
those on the diagonal should be dark, as this indicates total
agreement between the expert and the classiﬁer.

In all three datasets, we observe that AS results in a reduc-
tion in inter-class confusion. The ﬁrst column in the matrix
of Figure 5A and the second one of Figure 5C, reveal pairs
of classes that kNN ﬁnds hard to classify. Interestingly, AS
succeeded in reducing inter-class confusion, as is revealed by
the near-diagonal patterns in matrices of Figures 5B and 5D.
A closer look at the confusion matrices obtained after
sprinkling reveals patterns that are consistent with the rela-
tionship between classes. In the hierarchical dataset, the con-
fusion is mainly between classes within the same sub-tree.
There are two broad confusion zones, one between the ﬁve
classes of the comp subtree, the other between four classes
of rec. Furthermore very closely related classes like those
corresponding to PC and MAC hardware, and those relat-
ing to autos and motorcycles are hard to discriminate, and
this is reﬂected in the lighter shades in the corresponding
cells of Figure 5B. For ordinal classes, the confusion matrix
of Figure 5D shows that AS has implicitly mined the sim-
ilarity between rating classes and attenuated confusion be-
tween distant classes. This is evident from the broad pattern
of light shades along the diagonal, and darker shades else-
where. This is expected as adjacent classes of an ordinal
dataset are the most similar. The orthogonal dataset has the
least confusion between classes since there is no explicit re-

IJCAI-07

1585

Hierarchical Ordinal Orthogonal

Hierarchical Ordinal Orthogonal

kNNC

Baseline
LSI
LSI+AS
Baseline
LSI
LSI+AS
SVM Baseline

kNNE

48.02
49.53
60.40
20.80
35.73
59.38
65.47

25.84
29.08
31.00
25.40
29.00
30.16
30.12

93.47
94.80
95.20
78.60
91.87
93.80
94.27

Table 1: kNN performance before and after sprinkling.

lationship between them. Figures 5E and 5F show that sprin-
kling has a positive effect in reducing inter-class confusion.
In particular, the confusion between classes acq and crude has
been markedly reduced. We sought an empirical explanation
for this by studying similarity between terms [Kontostathis
and Pottenger, 2006] before and after AS. It was observed
that similarity between words were boosted if they related
strongly to the same class, and attenuated otherwise. For ex-
ample, “opec” and “reﬁnery”, both relevant to the class crude,
were drawn closer, while “dividend” (from earn) and “crude”
(from crude) were moved apart.

3.3 The Effect of Sprinkling on kNN and SVM
To assess the impact of sprinkling we constructed three rep-
resentations of each dataset: the raw term-document matrix
(baseline), the LSI-generated reduced dimensional represen-
tation (LSI), and the approximation of the original matrix
generated by sprinkled LSI (LSI+AS).

Effects of sprinkling on kNN: We used two variants
of kNN, the ﬁrst based on the Euclidean distance measure
(kNNE) and the second on cosine similarity (kNNC). Both
use a weighted majority vote from the 3 nearest neighbours.
Table 1 reports kNN performances, before and after sprin-
kling, at the LSI dimension empirically found best. These
are compared against baseline SVM performance. For each
dataset, the performances signiﬁcantly better (p < 0.05) than
the rest, are shown in bold. Firstly, we observe that AS leads
to sizable improvements in performance of both kNNE and
kNNC over the respective baselines. kNNE and kNNC per-
formances with LSI+AS are signiﬁcantly better than LSI on
all datasets. Secondly, LSI+AS enhances kNN performance
to be competitive with, and occasionally outperform, baseline
SVM. Figure 6 shows kNNC and kNNE performances over
various LSI dimensions. We note that LSI+AS consistently
outperforms LSI at all dimensions, on both measures.

The poor performance of all classiﬁers on the ordinal
dataset can be attributed to classes that are not neatly sepa-
rable. This is partly caused by subjective differences between
reviewers, who use different ratings to express similar judge-
ments. The positive impact of AS on confusion matrices in
Figure 5D suggests that a regression-based technique can fare
better than a classiﬁer that attempts to predict a precise rat-
ing. Furthermore, the IG measure used for feature selection
assumes classes to be disjoint and needs to be reformulated
to accommodate inter-class similarity.

Effects of sprinkling on SVM: Table 2 shows the im-
pact of sprinkling on SVM performance.
It may be noted
that the confusion matrix used to generate sprinkled terms re-

Baseline

SVM LSI

LSI+AS

65.47
65.71
66.33

30.12
31.12
32.08

94.27
95.27
95.27

Table 2: SVM performance before and after sprinkling.

ﬂected weaknesses speciﬁc to SVM, hence AS should ideally
emphasise differences between classes that SVM on its own
found hard to classify. The results are in line with our ex-
pectation, as LSI+AS signiﬁcantly (p < 0.05) outperforms
the baseline on all three datasets. There is some evidence to
suggest that LSI alone improves SVM performance, but the
difference is not statistically signiﬁcant except for the orthog-
onal dataset.

4 Related Work

We are aware of three other efforts that extend LSI to ac-
commodate class knowledge. Sun et al [2004] presents a
technique called SLSI that is based on iteratively identifying
discriminative eigenvectors from class-speciﬁc LSI represen-
tations. In their study, no signiﬁcant improvement of SLSI
over baseline SVM was reported. Additionally, SLSI involves
km SVD computations, corresponding to k iterations over
m classes, making it computationally expensive. In another
study, Wiener et al [1995] use a combination of what they re-
fer to as local LSI and global LSI. Local LSI representations
are constructed for each class, in a similar spirit to SLSI. Test
documents are compared against each local LSI representa-
tion independently.
In addition to computation overheads,
one shortcoming of the approach is that similarities between
test documents across the local LSI representations cannot be
compared easily. Finally, Wang et al [2005] presents a the-
oretical model to accommodate class knowledge in LSI. No
empirical studies were reported, but the authors note that their
approach slows down when a document belongs to more than
one class. In contrast, sprinkled terms can comfortably reﬂect
afﬁliation of documents to more than one class, and this has
no adverse implication on time performance.

When compared to AS, a general shortcoming of all of
the above mentioned approaches is that they fail to take into
account relationships between classes. A second relative
strength of our approach is that it is simple and can eas-
ily be integrated into existing LSI implementations. Unlike
most of the approaches above, the time complexity of our
algorithm is independent of the number of classes.
In all
our benchmark experiments, computing SVD over an aug-
mented term-document matrix takes less than 5% additional
time compared to SVD on the original matrix.

5 Conclusion and Future Work

The ﬁrst contribution of our paper is extending LSI to su-
pervised classiﬁcation tasks and generating revised document
representations that can be used by any technique founded
on the vector space model. The experimental results ver-
ify that we have succeeded in enhancing the performance of
instance-based learners like kNN to make them comparable

IJCAI-07

1586

Sprinkling at various LSI dimensions (Hierarchical dataset)

Sprinkling at various LSI dimensions (Ordinal dataset)

Sprinkling at various LSI dimensions (Orthogonal dataset)

y
c
a
r
u
c
c
a

0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3

0.25

0.2

kNNC LSI
kNNC LSI+AS
kNNE LSI
kNNE LSI+AS

50

100

150

200

250
LSI dimensions

y
c
a
r
u
c
c
a

0.31

0.3

0.29

0.28

0.27

0.26

0.25

0.24

kNNC LSI
kNNC LSI+AS
kNNE LSI
kNNE LSI+AS

20

40

60

300

350

400

80

100

120

LSI dimensions

0.95

0.9

0.85

0.8

0.75

y
c
a
r
u
c
c
a

140

160

180

200

0.7

10

20

30

40

kNNC LSI
kNNC LSI+AS
kNNE LSI
kNNE LSI+AS

70

80

90

100

50

60

LSI dimensions

Figure 6: kNN performance at various LSI dimensions.

to state-of-the-art techniques like SVM. This has strong prac-
tical implications for applications where lazy incremental up-
dates are desirable. Also while SVM-like kernel methods
suffer from the “black-box” syndrome, kNN is well recog-
nised to be suitable for explanation and visualisation, making
expert-initiated reﬁnement possible.

Experiments on hierarchical and ordinal datasets conclu-
sively demonstrate that confusion matrices implicitly cap-
ture class structure, which can be exploited to reduce con-
fusion between classes. To our knowledge, ours is the ﬁrst
work combining the strengths of LSI, like higher order co-
occurrence modeling and ability to recover from word choice
variability, with the knowledge of class relationships as in-
ferred from confusion matrices. The result is a revised vector
space representation that adapts itself to classiﬁer needs.

We have also shown that AS-generated SVM representa-
tions result in signiﬁcant improvements in SVM performance.
The results obtained are best-in-line for all three datasets.

As part of future work, we plan to devise a more principled
approach for estimating the M SL parameter in the AS algo-
rithm. Our current idea is to use cross-validation over training
data to arrive at a good value. Finally, we are also investi-
gating an extension of AS that sprinkles documents carrying
background knowledge of term associations. This has the po-
tential to generalise AS to a comprehensive framework that
uniﬁes background and introspectively acquired knowledge,
while addressing class-speciﬁc confusion.

References

[Bekkerman et al., 2003] Ron Bekkerman, Ran El-Yaniv, Naftali
Tishby, and Yoad Winter. Distributional Word Clusters vs. Words
for Text Categorization. Journal of Machine Learning Research,
3:1183–1208, 2003.

[Chakraborti et al., 2006] Sutanu Chakraborti, Robert Lothian, Nir-
malie Wiratunga, and Stuart Watt. Sprinkling: Supervised Latent
Semantic Indexing. In ECIR, pages 510–514. Springer, 2006.

[Cohen and Singer, 1996] William W. Cohen and Yoram Singer.
Context-sensitive Learning Methods for Text Categorization. In
SIGIR, pages 307–315. ACM Press, 1996.

[Deerwester et al., 1990] Scott C. Deerwester, Susan T. Dumais,
Thomas K. Landauer, George W. Furnas, and Richard A. Harsh-
man. Indexing by Latent Semantic Analysis. JASIS, 41(6):391–
407, 1990.

[Gabrilovich and Markovitch, 2004] Evgeniy Gabrilovich

and
Shaul Markovitch. Text Categorization with Many Redundant
Features: Using Aggressive Feature Selection to Make SVMs
Competitive with C4.5. In ICML, pages 321–328. ACM Press,
2004.

[Gabrilovich and Markovitch, 2005] Evgeniy Gabrilovich

and
Shaul Markovitch. Feature Generation for Text Categorization
Using World Knowledge. In IJCAI, pages 1048–1053, 2005.

[Gee, 2003] Kevin R. Gee. Using Latent Semantic Indexing to Fil-
ter Spam. In SAC’03: Proc. of ACM symposium on App. Comp.,
pages 460–464. ACM Press, 2003.

[Hastie et al., 2001] T. Hastie, R. Tibshirani, and J. H. Friedman.

The Elements of Statistical Learning. Springer, 2001.

[Joachims, 1998] Thorsten Joachims. Text Categorization with Su-
port Vector Machines: Learning with Many Relevant Features. In
ECML, pages 137–142. ACM Press, 1998.

[Kontostathis and Pottenger, 2006] April

and
William M. Pottenger. A framework for understanding La-
tent Semantic Indexing (LSI) performance.
Information
Processing Management, 42(1):56–73, 2006.

Kontostathis

[Lang, 1995] Ken Lang. NewsWeeder: learning to ﬁlter netnews.

In ICML’95, pages 331–339. Morgan Kaufmann, 1995.

[Pang and Lee, 2005] Bo Pang and Lillian Lee. Seeing stars: Ex-
ploiting class relationships for sentiment categorization with re-
spect to rating scales. In ACL, pages 115–124, 2005.

[Reuters, 1997] Reuters. Reuters-21578 Text classiﬁcation corpus.

daviddlewis.com/resources/testcollections/reuters21578/, 1997.

[Sebastiani, 2002] Fabrizio Sebastiani. Machine Learning in Auto-

mated Text Categorization. ACM, 34(1):1–47, 2002.

[Sun et al., 2004] Jian-Tao Sun, Zheng Chen, Hua-Jun Zeng,
Yuchang Lu, Chun-Yi Shi, and Wei-Ying Ma. Supervised La-
tent Semantic Indexing for Document Categorization. In ICDM,
pages 535–538. IEEE Press, 2004.

[Wang et al., 2005] Ming-Wen Wang, Jian-Yun Nie, and Xue-
In Proc.

Qiang Zeng. A latent semantic classiﬁcation model.
of 14th ACM CIKM’05, pages 261–262. ACM Press, 2005.

[Wiener et al., 1995] Erik Wiener, Jan O. Pedersen, and Andreas S.
In

Weigend. A Neural Network Approach to Topic Spotting.
Proc. of SDAIR’95, pages 317–332, 1995.

[Zelikovitz and Hirsh, 2001] Sarah Zelikovitz and Haym Hirsh.
Using LSI for Text Classiﬁcation in the Presence of Background
Text. In Henrique Paques, Ling Liu, and David Grossman, edi-
tors, Proc. of 10th ACM CIKM’01, pages 113–118. ACM Press,
2001.

IJCAI-07

1587


Exploiting Known Taxonomies in Learning Overlapping Concepts

Lijuan Cai and Thomas Hofmann

Department of Computer Science

Brown University, Providence, RI, USA

{ljcai, th}@cs.brown.edu

Abstract

Many real-world classiﬁcation problems involve
large numbers of overlapping categories that are ar-
ranged in a hierarchy or taxonomy. We propose to
incorporate prior knowledge on category taxonomy
directly into the learning architecture. We present
two concrete multi-label classiﬁcation methods, a
generalized version of Perceptron and a hierarchi-
cal multi-label SVM learning. Our method works
with arbitrary, not necessarily singly connected tax-
onomies, and can be applied more generally in
settings where categories are characterized by at-
tributes and relations that are not necessarily in-
duced by a taxonomy. Experimental results on
WIPO-alpha collection show that our hierarchical
methods bring signiﬁcant performance improve-
ment.

1 Introduction
Many real-world classiﬁcation tasks involve large numbers
of overlapping categories. Prominent examples include the
International Patent Classiﬁcation scheme (approx. 69,000
patent groups), the Open Directory project (approx. 590,000
categories for Web pages), and the Gene Ontology (ap-
prox. 17,000 terms to describe gene products). In most cases,
instances are assigned to more than one category, since cat-
egories are rarely mutually exclusive. This leads to large
scale multi-label classiﬁcation problems. The categories are
typically organized in hierarchies or taxonomies, most com-
monly by introducing superordinate concepts and by relating
categories via ‘is-a’ relationships. Multiply connected tax-
onomies are not uncommon in this context.

We believe that

taxonomies encode valuable domain
knowledge which learning methods should be able to capital-
ize on, in particular since the number of training examples for
individual classes may be very small when dealing with tens
of thousands or more classes. The potential loss of valuable
information by ignoring class hierarchies has been pointed
out before and has led to a number of approaches that employ
different ways to exploit hierarchies [McCallum et al., 1998;
Wang et al., 1999; Dumais and Chen, 2000].

In this paper, we present an approach for systematically
incorporating domain knowledge about the relationships be-

tween categories into the Perceptron learning and SVM clas-
siﬁcation architecture. The rest of the paper is organized as
follows. Section 2 introduces two ways of representing tax-
onomy knowledge. First, derive pairwise similarities between
categories based on their relative locations in the taxonomy
in order to tie learning across categories. Second, adapt the
standard 0/1 loss function to weigh misclassiﬁcation errors in
accordance with the taxonomy structure. In Section 3 we for-
mulate the hierarchical learning problem in terms of a joint
large margin problem, for which we derive an efﬁcient train-
ing algorithm. In Section 4 we propose a hierarchical Percep-
tron algorithm that exploits taxonomies in a similar fashion.
Section 5 examines related work. Section 6 presents exper-
imental results which show the two new hierarchical algo-
rithms bring signiﬁcant improvements on all metrics. Con-
clusions and future work are discussed in Section 7.

2 Utilizing Known Taxonomies

2.1 Problem Setting

We assume the patterns such as documents are represented

as vectors, x ∈ X ⊆ Rd, which can be mapped to a higher
dimensional feature space as φ(x). We denote the set of cat-
egories by Y = {1, . . . , q}, a category by y ∈ Y, and a
label set by Y ∈ P(Y) where P(Y) is the power set of Y.
A taxonomy is a directed acyclic graph (V, E) with nodes
V ⊇ Y such that the set of terminal nodes equals Y, formally
Y = {y ∈ V : (cid:2)v ∈ V, (y, v) ∈ E}. Note that we do

not assume that a taxonomy is singly connected (tree or for-
est), but allow for converging nodes. In some cases one wants
to express that items belong to a super-category, but to none

of the terminal categories in Y, we suggest to model this by

formally adding one terminal node to each inner node, repre-
senting a “miscellaneous” category; this avoids the problem
of partial paths.

In multi-label learning, we aim at ﬁnding a mapping f :
X → P(Y), based on a sample of training pairs {(xi, Yi), i =
1, . . . , n} ⊆ X × P(Y). A popular approach as suggested,
for instance by [Schapire and Singer, 2000], is to actually
learn a ranking function over the categories for each pattern,

g : X → Sq, where Sq is the set of permutations of ranks 1
to q. In order to get a unique subset of labels, one then needs
address the additional question on how to select the number
of categories a pattern should be assigned to.

IJCAI-07

714

It is common to deﬁne the ranking function g implic-
itly via a scoring function F : X × Y → R, such that
g(x)(y) < g(x)(y(cid:2)) whenever F (x, y) > F (x, y(cid:2)), i.e. cate-
gories with higher F -values appear earlier in the ranking (for
ease of presentation we ignore ties). Notation g(y) is used
when clear from context.

2.2 Class Attributes
Following [Cai and Hofmann, 2004] we suggest to use scor-
ing functions F that are linear in some joint feature repre-
sentation Φ of inputs and categories, namely F (x, y; w) ≡
(cid:7)w, Φ(x, y)(cid:8), where w is a weight vector.
Following
[Tsochantardis et al., 2004; Cai and Hofmann, 2004] Φ will
be chosen to be of the form Λ(y) ⊗ φ(x), where Λ(y) =
(λ1(y), . . . , λs(y))T ∈ Rs refers to an attribute vector repre-
senting categories and ⊗ is the Kronecker product. One can

(cid:2)
interpret w in terms of a stacked vector of individual weight
s )T , leading to the additive de-
vectors, i.e. w = (wT
1 , . . . , wT
r=1 λr(y)(cid:7)wr, x(cid:8). The general
s
composition F (x, y; w) =
idea is that the notion of class attributes will allow general-
ization to take place across (similar) categories and not just
across training examples belonging to the same category. In
the absence of such information, one can set s = q and deﬁne
λr(y) = δry which leads to F (x, y) = (cid:7)wy, x(cid:8).

How are we going to translate the taxonomy information
into attributes for categories? The idea is to treat the nodes in
the taxonomy as properties. Formally, we deﬁne

(cid:3)

tv,
0,

if v ∈ anc(y)

otherwise ,

λv(y) =

where tv ≥ 0 is the attribute value for node v. In the simplest
case, tv can be set to a constant, like 1. We denote by anc(y)
the set of ancestor nodes of y in the taxonomy including y
itself (for notational convenience). This leads to an intuitive
decomposition of the scoring function F into contributions
from all nodes along the paths from a root node to a speciﬁc
terminal node.

(1)

and c+
for assigning an irrelevant item. For any label set Y ,
deﬁne anc(Y ) ≡ {v ∈ V : ∃y ∈ Y, v ∈ anc(y)}. Now we

can quantify the loss in the following manner:

(cid:11)(Y, ˆY ) = c−

sv + c+

sv − (c− + c+)

(cid:4)

(cid:4)

(2)

v∈anc(Y )

v∈anc( ˆY )

(cid:4)

sv
v∈anc(Y )
∩anc( ˆY )

Note that only nodes in the symmetric difference anc(Y ) (cid:12)
anc( ˆY ) contribute to the loss. In the following we will sim-
plify the presentation by assuming that c− = c+ = 1. Then,
by further setting sv = 1 (∀v ∈ V) one gets (cid:11)(Y, ˆY ) =
|anc(Y ) (cid:12) anc( ˆY )|. Intuitively, this means that one colors all
nodes that are on a path to a node in Y with one color, say
blue, and all nodes on paths to nodes in ˆY with another color,
say yellow. Nodes that have both colors (blue+yellow=green)
are correct, blue nodes are the ones that have been missed and
yellow nodes are the ones that have been incorrectly selected;
both types of mistakes contribute to the loss proportional to
their volume.

During training, this loss function is difﬁcult to deal with
directly, since it involves sets of labels. Rather, we would like
to work with pairwise contributions, e.g. involving terms

(cid:11)(y, y(cid:2)) = |anc(y) (cid:12) anc(y(cid:2))| .

(3)

In singly connected taxonomies Eq. (3) is equivalent to the
length of the (undirected) shortest path connecting the nodes
y and y(cid:2)
, suggested by [Wang et al., 1999]. In order to relate
the two, we state the following proposition.
Proposition 1. For any Y, ˆY ⊆ Y satisfying Y (cid:15)⊆ ˆY and
ˆY (cid:15)⊆ Y ,

|anc(Y ) (cid:12) anc( ˆY )| ≤

|anc(y) (cid:12) anc(ˆy)|

(cid:4)

y∈Y − ˆY
ˆy∈ ˆY −Y

For learning with ranking functions g, we translate this into

(cid:11)(Y, g) =

|anc(y) (cid:12) anc(ˆy)| .

(4)

(cid:4)

2.3 Loss Functions

A standard loss function for the multi-label case is to use
the symmetric difference between the predicted and the ac-
tual label set, i.e. to count the number of correct categories
missed plus the number of incorrect categories that have been

assigned, (cid:11)0(Y, ˆY ) ≡ |Y (cid:12) ˆY |.

Yet, in many applications, the actual loss of a predicted la-
bel set relative to the true set of category labels will depend
on the relationship between the categories. As a motivation
we consider the generic setting of routing items based on their
membership at nodes in the taxonomy. For instance, in a news
routing setting, readers may sign-up for speciﬁc topics by se-
lecting an appropriate node, which can either be a terminal
node in the taxonomy (e.g. the category “Soccer”) or an in-
ner node (e.g. the super-category “Sports”). Note that while
we assume that all items can be assigned to terminal nodes of
the taxonomy only, customers may prefer to sign-up for many
categories en bloc by selecting an appropriate super-category.

We assume that there is some relative sign-up volume sv ≥
0 for each node as well as costs c−

of missing a relevant item

y∈Y, ˆy∈Y−Y

g(y)>g( ˆy )

We look at every pair of categories where an incorrect cate-
gory comes before a correct category in the order deﬁned by g
and count the symmetric difference of the respective ancestor
sets as the corresponding loss.

3 Hierarchical Support Vector Machines

3.1 Multi-label Classiﬁcation
We generalize the multiclass SVM formulation of [Crammer
and Singer, 2001] to a multi-label formulation similar to that
in [Elisseff and Weston, 2001]. For a given set of correct

categories Yi we denote the complement by ¯Yi = Y − Yi.

Then following [Elisseff and Weston, 2001] we approximate
the separation margin of w with respect to the i-th example
as

γi(w) ≡ min
y∈Yi,¯y∈ ¯Yi

(cid:7)Φ(xi, y) − Φ(xi, ¯y), w(cid:8) .

(5)

Our formulation aims at maximizing the margin over the
whole training set, i.e. maxw:(cid:4)w(cid:4)=1 mini γi(w). This is

IJCAI-07

715

equivalent to minimizing the norm of the weight vector w
while constraining all (functional) margins to be greater than
or equal to 1. A generalized soft-margin SVM formulation
can be obtained by introducing slack variables ξi’s. The
penalty is scaled proportional to the loss associated with
the violation of the respective category ordering, a mech-
anism suggested before (cf. [Tsochantardis et al., 2004;
Cai and Hofmann, 2004]). Putting these ideas together yields
the convex quadratic program (QP)

(cid:17)w(cid:17)2 + C

min
w,ξ

1
2

n(cid:4)

ξi

i=1

s.t. (cid:7)w, δΦi(y, ¯y)(cid:8) ≥ 1 − ξi(cid:11)(y, ¯y)
where δΦi(y, ¯y) ≡ Φ(xi, y) − Φ(xi, ¯y).

ξi ≥ 0,

(∀i) .

,

(6)

(∀i, y ∈ Yi, ¯y ∈ ¯Yi)

This formulation is similar to the one used in [Schapire and
Singer, 2000] and generalizes the ranking-based multi-label
SVM formulation of [Elisseff and Weston, 2001]. Follow-
ing [Crammer and Singer, 2001] we have not included bias
terms for categories. The efforts are put into correctly or-
dering each pair of positive/negative labels. We can use a
size prediction mechanism such as the one in [Elisseff and
Weston, 2001] to convert the category ranking into an actual
multi-label classiﬁcation.

The dual QP of (6) becomes

n(cid:4)

(cid:4)

max

α

Θ(α) =

− 1
2

i=1

(cid:4)

y∈Yi
¯y∈ ¯Yi

(cid:4)

αiy ¯y

(cid:4)

(7)

αiy ¯yαjr¯r(cid:7)δΦi(y, ¯y), δΦj (r, ¯r)(cid:8),

i,j

y∈Yi
¯y∈ ¯Yi

r∈Yj
¯r∈ ¯Yj

s.t. αiy ¯y ≥ 0 (∀i, y ∈ Yi, ¯y ∈ ¯Yi) ,

(cid:4)

y∈Yi
¯y∈ ¯Yi

αiy ¯y(cid:11)(y, ¯y)

≤ C (∀i) .

Note that

(cid:7)δΦi(y, ¯y), δΦj(r, ¯r)(cid:8)
=(cid:7)Λ(y) − Λ(¯y), Λ(r) − Λ(¯r)(cid:8)(cid:7)φ(xi), φ(xj )(cid:8) .
(cid:2)

Herein one can simply replace the inner products by corre-
sponding kernel functions. It is straightforward to observe
i ξi yields an upper bound on the training loss of
that
the resulting classiﬁer measured by (cid:11) in the following sense.

(8)

1
n

Deﬁne the maximum loss as

(cid:11)x(Y, g) ≡

max

y∈Y,¯y∈ ¯Y :g(y)≥g(¯y)

(cid:11)(y, ¯y) .

(9)

The maximal loss over a set of examples is deﬁned as

(cid:11)x(Yi, g(xi; F ))

(10)

n(cid:4)

i=1

(cid:11)x(F,{xi, Yi}n
i=1) ≡ 1
(cid:2)

n

Proposition 2. Denote by ( ˆw, ˆξ) a feasible solution of the
ˆξi is an upper bound on the empiri-
QP in (6). Then
cal maximal loss (cid:11)x(F (; ˆw),{xi, Yi}n

n
i=1

1
n

i=1).

Algorithm 1 Hierarchical Multilabel SVM

1: inputs: training data {xi, Yi}n
i=1, tolerance  ≥ 0
2: initialize Si = ∅, αiy ¯y = 0, ∀ i, y ∈ Yi, ¯y ∈ ¯Yi.

3: repeat
4:
5:

6:
7:

i=1 ψi

select ˆi = argmaxn
select (ˆy, ˆ¯y) = argmaxy∈Yˆi,¯y∈ ¯Yˆi
expand working set: Sˆi = Sˆi
solve QP over subspace {αˆiy ¯y : (y, ¯y) ∈ Sˆi
}
reduce working set: Sˆi = Sˆi

Gˆiy ¯y
∪ {(ˆy, ˆ¯y)}
− {(y, ¯y) : αˆiy ¯y = 0}

8:
9: until ψˆi

≤ 

Note that to minimize (an upper bound on) the loss in
Eq. (4), we could simply assign one slack variable ξiy ¯y for
every triplet of instance, positive label, and negative label.
This leads to a dual program similar to Eq. (7) except the sec-
ond set of constraints become
We have yet to explore this direction.

(cid:6)(y,¯y) ≤ C ∀i, y ∈ Y, ¯y ∈ ¯Y .

αiy ¯y

(11)

(12)

(13)

3.2 Optimization Algorithm
The derived QP can be very large. We therefore employ an
efﬁcient optimization algorithm that is inspired by the SMO
algorithm [Platt, 1999] and that performs a sequence of sub-
space ascents on the dual, using the smallest possible subsets
of variables that are not coupled with the remaining variables
through constraints. Our algorithm successively optimizes

over subspaces spanned by {αiy ¯y : y ∈ Yi, ¯y ∈ ¯Yi} for some
selected instance i. Moreover an additional variable selec-
tion is performed within each subspace. This strategy is also
known as column generation [Demiriz et al., 2002]. Deﬁne

(cid:4)

w(α) ≡ n(cid:4)
Giy ¯y ≡ (cid:11)(y, ¯y) (1 − (cid:7)δΦi(y, ¯y), w(α)(cid:8))
(cid:6)

αiy ¯yδΦi(y, ¯y)

y∈Yi,¯y∈ ¯Yi

(cid:5)

i=1

{Giy ¯y}

,

0

max

i,y∈Yi,¯y∈ ¯Yi
min i,y∈Yi , ¯y∈ ¯Yi

:

li ≡ max
⎧⎪⎨
⎪⎩

min

(cid:6)

αiy ¯y >0

Giy ¯y

if ζi = 0

ui ≡

min i,y∈Yi , ¯y∈ ¯Yi

(cid:5)
where ζi = C − (cid:2)
(cid:6)(y,¯y) . Deﬁne ψi ≡ li − ui.
can be shown that ψi = 0 (∀i) is the necessary and sufﬁcient

By derivation similar to that in [Cai and Hofmann, 2004], it

if ζi > 0 ,

y∈Yi,¯y∈ ¯Yi

Giy ¯y , 0

αiy ¯y >0

(14)

αiy ¯y

:

condition for a feasible solution to be optimal. Hence the
score ψi is used for selecting subspaces. Giy ¯y is also used to
select new variables to expand the active set of each subspace.
The resulting algorithm is depicted in Algorithm 1.

More details on convergence and sparseness of a more gen-
eral class of algorithms can be found in [Tsochantardis et al.,
2004].

4 Hierarchical Perceptron
Although SVM is competitive in generating high-quality
classiﬁers, it can be computationally expensive. The Percep-
tron algorithm [Rosenblatt, 1958] is known for its simplicity

IJCAI-07

716

Algorithm 2 Hierarchical Minover Perceptron algorithm

1: inputs: training data {xi, Yi}n
2: Initialize αiy ¯y = 0, ∀ i, y ∈ Yi, ¯y ∈ ¯Yi

i=1, desired margin c.

3: repeat
4:

(ˆi, ˆy, ˆ¯y) = argmini,y∈Yi,¯y∈ ¯Yi
if (cid:7)w(α), δΦˆi(ˆy, ˆ¯y)(cid:8) > c then

(cid:7)w(α), δΦi(y, ¯y)(cid:8)

5:
6:
7:
8:

terminate with a satisfactory solution

else

αˆiˆy ˆ¯y
end if

← αˆiˆy ˆ¯y + (cid:11)(ˆy, ˆ¯y)

9:
10: until maximal number of iterations are performed

and speed.
In this section, we propose a hierarchical Per-
ceptron algorithm 2 using the minimum-overlap (Minover)
learning rule [Krauth and M´ezard, 1987].

The Minover Perceptron uses the instance that violates the
desired margin the worst to update the separating hyperplane.
We can also deal with the instances sequentially, as in a truly
online fashion. Using the minimum overlap selection rule
effectively speeds up the convergence and yields sparser so-
lutions.

If using taxonomy-based class attribute scheme in Eq. (1)
with tv = 1, the simple update rule in step 8 of Algorithm 2
can be decomposed into

wv ← wv + (cid:11)(y, ¯y)φ(xi) ∀v : v ∈ anc(y) − anc(¯y)
wv ← wv − (cid:11)(y, ¯y)φ(xi) ∀v : v ∈ anc(¯y) − anc(y)

Only the weight vectors of those nodes that are predeces-
sors of y or ¯y but not both will be updated. Other nodes are
left intact. This strategy is also used in [Dekel et al., 2004] for
online multiclass classiﬁcation. The more severe the loss is
incurred, the more dramatic the update will be. Moreover step
8 not only updates the scoring functions of the two classes in
question, but also spread the impact to other classes sharing
affected ancestors with them.

5 Related Work

Many approaches for hierarchical classiﬁcation use a decision
tree like architecture, associating with each inner node of the
taxonomy a classiﬁer that learns to discriminate between the
children [Dumais and Chen, 2000; Cesa-Bianchi et al., 2005].
While this offers advantages in terms of modularity, the local
optimization of (partial) classiﬁers at every inner node is un-
able to reﬂect a more global objective.

[Cesa-Bianchi et al., 2005] introduces a loss function,
called the H-loss, speciﬁcally designed to deal with the
case of partial and overlapping paths in tree-structured tax-
onomies. [Cesa-Bianchi et al., 2006] has proposed B-SVM,
which also uses the H-loss and uses a decoding scheme
that explicitly computes the Bayes-optimal label assignment
based on the H-loss and certain conditional independence as-
sumptions about label paths. The loss function we proposed
in Eq. (2) exploits the taxonomy in a different way from H-
loss, partly because we always convert partial path categories
to complete path ones. Our loss function is inspired by real
applications like routing and subscription to a taxonomy. So

misclassiﬁcations are penalized along all ancestors that miss
relevant patterns or include irrelevant ones. In H-loss, how-
ever, if punishment already occurs to a node, its descendents
are not penalized again. In addition, our loss function works
with arbitrary taxonomy, not just trees.

[Rousu et al., 2005] applies the Maximum-Margin Markov
Networks [Taskar et al., 2003] to hierarchical classiﬁcation
where the taxonomy is regarded as Markov Networks. They
propose a simpliﬁed version of H-loss that decomposes into
contributions of edges so as to marginalize the exponential-
sized problem into a polynomial one. In our methods, learn-
ing occurs on taxonomy nodes instead of edges. We view the
taxonomy as a dependency graph of “is-a” relation.

[Cai and Hofmann, 2004] proposes a hierarchical SVM
that decomposes discriminant functions into contributions
from different levels of the hierarchy, the same way as this
work. Compared to [Cai and Hofmann, 2004], which was re-
stricted to multiclass classiﬁcation, however, we deal with the
additional challenge posed by overlapping categories, i.e. the
multi-label problem, for which we employ the category rank-
ing approach proposed in [Schapire and Singer, 2000].

In summary, our major contributions are: 1) Formulate
multilabel classiﬁcation as a global joint learning problem
that can take taxonomy information into account. 2) Exploit
taxonomy by directly encoding structure in the scoring func-
tion used to rank categories 3) Propose a novel taxonomy-
based loss function between overlapping categories that is
motivated by real applications. 4) Derive a sparse optimiza-
tion algorithm to efﬁciently solve the joint SVM formula-
tion. Compared to multiclass classiﬁcation, sparseness is
even more important now that there are more constraints and
hence more dual variables. 5) Present a hierarchical Percep-
tron algorithm that takes advantage of the proposed methods
of encoding known taxonomies.

6 Experiments

6.1 Experimental Setup

In this section we compare our hierarchical approaches
against their ﬂat counterparts on WIPO-alpha, a data set com-
prising patent documents.

Taxonomy-derived attributes are employed in the hierar-
√
chical approaches. For comparison purpose, tv in Eq. (1) is
depth where depth ≡ maxy |{anc(y)}|, so that
set to 1/
maxy (cid:17)Λ(y)(cid:17) = 1 for either the ﬂat or the hierarchical mod-

els. In the experiments, hierarchical loss equals half the value
in Eq. (3) for historical reason. The hierarchical learning em-

ploys this hierarchical loss while the ﬂat one employs the 0−1
loss. We used a linear kernel and set C = 1. Each instance is
normalized to have 2-norm of 1. In most experiments, the test
performance is evaluated by cross-validation and then macro-
averaging across folds.

The measures we used include one-accuracy, average pre-
cision, ranking loss, maximal loss, and parent one-accuracy.
The ﬁrst three are standard metrics for multilabel classiﬁca-
tion problem [Schapire and Singer, 2000; Elisseff and We-
ston, 2001]. One-accuracy (acc) measures the empirical
probability of the top-ranked label being relevant to the doc-
ument. Average precision (prec) measures the quality of la-

IJCAI-07

717

section

#cat

#doc

846
A
B 1514
C 1152
237
D
267
E
F
862
565
G
H
462

15662
17626
14841
2194
3586
8011
12944
13178

#cat
/doc
1.44
1.48
2.11
1.53
1.34
1.45
1.29
1.35

acc (%)

prec (%)

(cid:2)x-loss

rloss (%)

pacc (%)

ﬂat
53.8
37.3
45.3
47.3
38.7
36.6
47.2
48.7

hier
54.2
37.8
45.4
48.6
38.7
37.6
47.2
49.2

ﬂat
56.0
40.8
45.5
52.7
44.9
41.3
52.3
55.1

hier
57.3
42.5
45.9
55.0
46.5
43.4
52.8
56.2

ﬂat
1.66
2.08
2.10
1.82
1.99
2.07
1.73
1.63

hier
1.34
1.76
1.68
1.45
1.63
1.69
1.50
1.34

ﬂat
9.09
12.6
10.1
11.7
12.7
11.6
10.5
8.25

hier
3.85
5.38
4.95
7.35
7.44
5.13
5.46
4.15

ﬂat
70.3
59.9
67.8
67.7
63.7
59.7
64.9
66.5

hier
73.5
65.0
73.3
71.6
66.2
65.0
67.0
69.6

Table 1: SVM experiments on WIPO-alpha corpus. Each row is on categories under the speciﬁed top level node (i.e. section). The results
are from random 3-fold cross-validation. Better performance is marked in bold face. “#cat/doc” refers to the average number of categories
per document, “ﬂat” the ﬂat SVM and “hier” the hierarchical SVM.

Figure 1: The four columns, from left to right, depict one accuracy
for ﬂat and hierarchical Perceptron, and average precision for ﬂat
and hierarchical Perceptron.

section

A
B
C
D
E
F
G
H

acc (%)
ﬂat hier
14.8 16.3
12.4 14.0
14.8 16.3
19.4 21.3
14.4 15.2
13.4 14.9
11.4 12.4
15.1 16.2

prec (%) (cid:2)x-loss
ﬂat hier
ﬂat hier
2.71 2.28
16.8 20.5
2.78 2.39
14.0 17.9
15.6 18.2
2.79 2.23
2.58 2.06
24.0 27.3
2.66 2.19
19.9 22.4
2.74 2.25
16.6 20.2
2.75 2.40
14.9 18.4
19.2 22.6
2.67 2.12

pacc (%)
ﬂat hier
39.3 47.3
40.1 48.6
46.6 58.1
49.3 56.3
45.2 50.4
41.5 51.1
35.1 41.9
40.2 48.4

Table 2: SVM experiments on WIPO-alpha corpus with subsam-
pling. Three documents or less are sampled for each category.

bel rankings. Precision is calculated at each position where a
positive label occurred, as if all the labels ranked higher than
it including itself are predicted as relevant. These precision
values are then averaged to obtain average precision. Rank-
ing loss (rloss) measures the average fraction of positive label
and negative label pairs that are misordered. These metrics
are described in details in [Schapire and Singer, 2000].

Maximal loss, denoted by (cid:11)x, was introduced in Eq. (10).

It is measured by the hierarchical loss function. We also eval-
uate parent one-accuracy (pacc), which measures the one-
accuracy at the category’s parent nodes level.

6.2 Experiments on WIPO-alpha Collection

WIPO-alpha collection comprises patent documents released
by the World Intellectual Property Organization (WIPO) 1.
which are classiﬁed into IPC categories. IPC is a 4-level hier-
archy consisting of sections, classes, subclasses, and groups.
The categories in our experiments refer to main groups which
are all leaves at the same depth in the hierarchy. Each doc-

)

%

(
 

e
c
n
a
m
r
o

f
r
e
p

70

60

50

40

30

20

10

flat acc
hier acc
flat pacc
hier pacc
8
10

9

1

2

3

4

5

6

7

sample number per category

)

%

(
 

e
c
n
a
m
r
o

f
r
e
p

50

45

40

35

30

25

20

15

10

5

flat prec
hier prec
flat rloss
hier rloss

1

2

3

4

5

6

7

8

9

10

sample number per category

Figure 2: Flat and hierarchical SVM on section D data, with vary-
ing training set size. A small number of documents are sampled
from each category for training purpose. The learned classiﬁers are
tested on all remaining documents. This is repeated 10 times for
each sampling number. The bars depict sample standard deviation.

ument is labeled with one primary category as well as any
number of secondary categories. Both types of categories are
used to form a multi-label corpus. We have performed in-
dependent experiments on taxonomies under the 8 top-level
sections.

Document parsing was performed with the Lemur toolkit 2.
Stop words are removed. Stemming is not performed. Word
counts from title and claim ﬁelds are used as document fea-
tures. Table 1 summarizes the SVM performance across the
sections. The hierarchical SVM signiﬁcantly outperforms the

ﬂat SVM in terms of (cid:11)x-loss, ranking loss and parent ac-

curacy in each individual setting. This can be attributed not
only to the fact that the hierarchical approach explicitly op-

timizes an upper bound on the (cid:11)x-loss, but also to the spe-

ciﬁc hierarchical form of the discriminant function. More-
over, hierarchical SVM often produces higher classiﬁcation
accuracy and average precision with gains being more mod-
erate. To see if the improvement is statistically signiﬁcant,
we conducted 10-fold cross-validation on section E and then
paired permutation test. The achieved level of signiﬁcance is
less than 0.08 for one accuracy, and less than 0.005 for the
other four measures.

Figure 1 depicts the performance of Perceptron algorithm
with the same setting. We allow Perceptron to run until
convergence. It takes signiﬁcantly less time than SVM but
reaches lower performance. We observe the hierarchical Per-
ceptron performs better in all cases.

In addition we randomly sampled 3 documents from each

1www.wipo.int/ibis/datasets

2www.lemurproject.org

IJCAI-07

718

18

)

%

(
 
y
c
a
r
u
c
c
a
 
e
n
o
 
n
o
r
t
p
e
c
r
e
P

i

 
l
a
c
h
c
r
a
r
e
h

i

16

14

12

10

8

6

6

35

)

%

8

flat Perceptron one accuracy (%)

10

12

14

16

i

(
 
s
s
o
l
 
g
n
k
n
a
r
 
n
o
r
t
p
e
c
r
e
P

i

 
l
a
c
h
c
r
a
r
e
h

i

30

25

20

15

15
flat Perceptron ranking loss (%)

30

20

25

)

%

i

i

(
 
n
o
s
c
e
r
p
 
e
g
a
r
e
v
a
 
n
o
r
t
p
e
c
r
e
P

i

 
l
a
c
h
c
r
a
r
e
h

i

)

%

(
 
y
c
a
r
u
c
c
a
 
e
n
o
 
t
n
e
r
a
p
 
n
o
r
t
p
e
c
r
e
P

i

 
l
a
c
h
c
r
a
r
e
h

i

18

35

20

18

16

14

12

10

8

8

55

50

45

40

35

30

25

10

12

14

16

18

20

flat Perceptron average precision (%)

20

20

25

30

35

40

45

50

55

flat Perceptron parent one accuracy (%)

Figure 3: Flat and hierarchical Perceptron on subsampled data.

category to simulate the situation where data are only avail-
able in small quantities. The results in Table 2 show that
the hierarchical SVM outperforms the ﬂat SVM in all cases.
The relative gains are somewhat higher than for the complete
training set. Fig 2 demonstrates how the performance gains
vary with the size of training data. We observe that hierarchi-
cal SVM excels in all runs. The gains appear to be slightly
larger when the training set is sparser, except for one accu-
racy.

Figure 3 compares ﬂat and hierarchical Perceptron with the
same subsampling setting as above. Each 3-fold cross vali-
dation on a random subset of documents under one section
constitutes one sample in the ﬁgure, with each section con-
tributing 3 samples. We observe the hierarchical approach
helps with one accuracy most times. It always signiﬁcantly
improves average precision, ranking loss and parent accuracy.

7 Conclusions
In this paper a hierarchical loss function has been derived
from real applications. We have proposed a large margin ar-
chitecture for hierarchical multilabel categorization.
It ex-
tends the strengths of Support Vector Machine classiﬁcation
to take advantage of information about class relationships en-
coded in a taxonomy. The parameters of the model are ﬁtted
by optimizing a joint objective. A variable selection algo-
rithm has been presented to efﬁciently deal with the result-
ing quadratic program. We have also proposed a hierarchical
Perceptron algorithm that couples the discriminant functions
according to the given hierarchy and employs the hierarchical
loss in its updating rule. Our experiments show the proposed
hierarchical methods signiﬁcantly outperform the ﬂat meth-
ods. Although they aim at reducing the hierarchical loss, the
taxonomy-based approaches improve other measures such as
one accuracy and average precision. Future work includes
more directly working with the hierarchical loss we proposed
and comparing our methods with other hierarchical methods.

References
[Cai and Hofmann, 2004] Lijuan Cai and Thomas Hofmann. Hier-
archical document categorization with support vector machines.
In Proceedings of the Conference on Information and Knowledge
Management (CIKM), 2004.

[Cesa-Bianchi et al., 2005] N. Cesa-Bianchi, C. Gentile, A. Tironi,
and L. Zaniboni. Incremental algorithms for hierarchical classi-
ﬁcation. In Advances in Neural Information Processing Systems,
2005.

[Cesa-Bianchi et al., 2006] N. Cesa-Bianchi, C. Gentile,

and
L. Zaniboni. Hierarchical classiﬁcation: Combining bayes with
svm.
In Proceedings of the 23rd International Conference on
Machine Learning, 2006.

[Crammer and Singer, 2001] K. Crammer and Y. Singer. On the
algorithmic implementation of multi-class kernel-based vector
machines. Journal of Machine Learning Research, 2:265–292,
2001.

[Dekel et al., 2004] Ofer Dekel, Joseph Keshet, and Yoram Singer.
Large margin hierarchical classiﬁcation.
In Proceedings of the
21st International Conference on Machine Learning (ICML),
2004.

[Demiriz et al., 2002] Ayhan Demiriz, Kristin P. Bennett, and John
Shawe-Taylor. Linear programming boosting via column gener-
ation. Machine Learning, 46(1–3):225–254, 2002.

[Dumais and Chen, 2000] S. T. Dumais and H. Chen. Hierarchical

classiﬁcation of Web content. In SIGIR, pages 256–263, 2000.

[Elisseff and Weston, 2001] Andr´e Elisseff and Jason Weston. A
kernel method for multi-labelled classiﬁcation. In Proceedings
of the Neural Information Processing Systems conference (NIPS),
pages 681–687, 2001.

[Krauth and M´ezard, 1987] Werner Krauth and Marc M´ezard.
Learning algorithms with optimal stability in neural networks.
Journal of Physics A., 20(745):L745–L752, 1987.

[McCallum et al., 1998] A. McCallum, R. Rosenfeld, T. Mitchell,
and A. Ng. Improving text clasiﬁcation by shrinkage in a hier-
archy of classes. In Proceedings of the International Conference
on Machine Learning (ICML), pages 359–367, 1998.

[Platt, 1999] J. C. Platt. Fast training of support vector machines
using sequential minimal optimization.
In Advances in Kernel
Methods - Support Vector Learning, pages 185–208. MIT Press,
1999.

[Rosenblatt, 1958] Frank Rosenblatt. The perceptron: A proba-
bilistic model for information storage and organization in the
brain. Psychological Review, 65(6):386–408, 1958.

[Rousu et al., 2005] Juho Rousu, Craig Saunders, Sandor Szedmak,
and John Shawe-Taylor. Learning hierarchical multi-category
text classiﬁcation models. In 22nd International Conference on
Machine Learning (ICML), 2005.

[Schapire and Singer, 2000] R. E. Schapire and Y. Singer. Boost-
exter: A boosting-based system for text categorization. Machine
Learning, 39(2/3):135–168, 2000.

[Taskar et al., 2003] Benjamin Taskar, Carlos Guestrin,

and
Daphne Koller. Max-margin markov networks. In Proceedings
of Neural Information Processing Systems conference (NIPS),
2003.

[Tsochantardis et al., 2004] I.

Tsochantardis,

T. Hofmann,
Support vector machine learn-
T. Joachims, and Y. Altun.
In
ing for
Proceedings of the 21st International Conference on Machine
Learning (ICML’04), 2004.

interdependent and structured output spaces.

[Wang et al., 1999] K. Wang, S. Zhou, and S. C. Liew. Building
hierarchical classiﬁers using class proximity. In Proceedings of
VLDB-99, 25th International Conference on Very Large Data
Bases, pages 363–374, 1999.

IJCAI-07

719


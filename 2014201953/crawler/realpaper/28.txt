A Probabilistic Learning Method for XML Annotation of Documents

Boris Chidlovskii1

1Xerox Research Centre Europe

6, chemin de Maupertuis

38240 Meylan, France

J´erˆome Fuselier1,2

2Universit´e de Savoie - Laboratoire SysCom

Domaine Universitaire

73376 Le Bourget-du-Lac, France

Abstract

We consider the problem of semantic annota-
tion of semi-structured documents according to
a target XML schema. The task is to annotate
a document in a tree-like manner where the an-
notation tree is an instance of a tree class de-
ﬁned by DTD or W3C XML Schema descrip-
tions. In the probabilistic setting, we cope with
the tree annotation problem as a generalized
probabilistic context-free parsing of an obser-
vation sequence where each observation comes
with a probability distribution over terminals
supplied by a probabilistic classiﬁer associated
with the content of documents. We determine
the most probable tree annotation by maximiz-
ing the joint probability of selecting a terminal
sequence for the observation sequence and the
most probable parse for the selected terminal
sequence.

1 Introduction
The future of the World Wide Web is often associated
with the Semantic Web initiative which has as a tar-
get a wide-spread document reuse, re-purposing and ex-
change, achieved by means of making document markup
and annotation more machine-readable. The success of
the Semantic Web initiative depends to a large extent on
our capacity to move from rendering-oriented markup of
documents, like PDF or HTML, to semantic-oriented
document markup, like XML and RDF.

In this paper, we address the problem of semantic an-
notation of HTML documents according to a target XML
schema. A tree-like annotation of a document requires
that the annotation tree be an instance of the target
schema, described in a DTD, W3C XML Schema or an-
other schema language. Annotation trees naturally gen-
eralize ﬂat annotations conventionally used in informa-
tion extraction and wrapper induction for Web sites.

The migration of documents from rendering-oriented
formats, like PDF and HTML, toward XML has recently
become an important issue in various research commu-
nities [2; 3; 11; 12]. The majority of approaches either
make certain assumptions about the source and target

XML documents, like a conversion through a set of lo-
cal transformations [3], or entail the transformation to
particular tasks, such as the semantic annotation of dy-
namically generated Web pages in news portals [11] or
the extraction of logical structure from page images [12].
In this paper, we consider the general case of tree an-
notation of semi-structured documents. We make no
assumptions about the structure of the source and tar-
get documents or their possible similarity. We repre-
sent the document content as a sequence of observations
x = {x1, . . . , xn}, where each observation xi is a con-
tent fragment. In the case of HTML documents, such
a fragment may be one or multiple leaves, often sur-
rounded with rich contextual information in the form of
HTML tags, attributes, etc. The tree annotation of se-
quence x is given by a pair (y, d), where y and d refer to
leaves and internal nodes of the tree, respectively. The
sequence y = {y1, . . . , yn} can be seen, on one side, as
labels for observations in x, and on the other side, as a
terminal sequence for tree d that deﬁnes the internal tree
structure over y according to the target XML schema.
In supervised learning, the document annotation sys-
tem includes selecting the tree annotation model and
training the model parameters from a training set S
given by triples (x, y, d). We adopt a probabilistic set-
ting, by which we estimate the probability of an anno-
tation tree (y, d) for a given observation sequence x and
address the problem of ﬁnding the pair (y, d) of maximal
likelihood.

We develop a modular architecture for the tree an-
notation of documents that includes two major compo-
nents. The ﬁrst component is a probabilistic context-free
grammar (PCFG) which is a probabilistic extension to
the corresponding (deterministic) XML schema deﬁni-
tion. The PCFG rules may be obtained by rewriting the
schema’s element declarations (in the case of a DTD)
or element and type deﬁnitions (in the case of a W3C
XML Schema) and the rule probabilities are chosen by
observing rule occurrences in the training set, similar
to learning rule probabilities from tree-bank corpora for
NLP tasks. PCFGs oﬀer the eﬃcient inside-outside al-
gorithm for ﬁnding the most probable parse for a given
sequence y of terminals. The complexity of the algo-
rithm is O(n3 ·|N|), where n is the length of sequence y

and |N| is the number of non-terminals on the PCFG.
The second component is a probabilistic classiﬁer for
predicting the terminals y for the observations xi in x.
In the case of HTML documents, we use the maximum
entropy framework [1], which proved its eﬃciency when
combining content, layout and structural features ex-
tracted from HTML documents for making probabilistic
predictions p(y) for xi.

With the terminal predictions supplied by the con-
tent classiﬁer, the tree annotation problem represents
the generalized case of probabilistic parsing, where each
position i in sequence y is deﬁned not with a speciﬁc
terminal, but with a terminal probability p(y). Conse-
quently, we consider the sequential and joint evaluations
of the maximum likelihood tree annotation for observa-
tion sequences. In the joint case, we develop a general-
ized version of the inside-outside algorithm that deter-
mines the most probable annotation tree (y, d) according
to the PCFG and the distributions p(y) for all positions
i in x. We show that the complexity of the generalized
inside-outside algorithm is O(n3·|N|+n·|T|·|N|), where
n is the length of x and y, and where |N| and |T| are the
number of non-terminals and terminals in the PCFG.

We also show that the proposed extension of the
inside-outside algorithm imposes the conditional inde-
pendence requirement, similar to the Naive Bayes as-
sumption, on estimating terminal probabilities. We test
our method on two collections and report an important
advantage of the joint evaluation over the sequential one.

2 XML annotation and schema

XML annotations of documents are trees where inner
nodes determine the tree structure, and the leaf nodes
and tag attributes refer to the document content. XML
annotations can be abstracted as the class T of unranked
labeled rooted trees deﬁned over an alphabet Σ of tag
names [9]. The set of trees over Σ can be constrained
by a schema D that is deﬁned using DTD, W3C XML
Schema or other schema languages.

DTDs and an important part of W3C XML Schema
descriptions can be modeled as extended context-free
grammars [10], where regular expressions over alpha-
bet Σ are constructed by using the two basic opera-
tions of concatenation · and disjunction | and with occur-
rence operators ∗ (Kleene closure), ? (a? = a|) and +
(a+ = a·a∗). An extended context free grammar (ECFG)
is deﬁned by the 4-tuple G = (T, N,S, R), where T and
N are disjoint sets of terminals and nonterminals in Σ,
Σ = T ∪ N; S is an initial nonterminal and R is a ﬁnite
set of production rules of the form A → α for A ∈ N,
where α is a regular expression over Σ = T ∪ N. The
language L(G) deﬁned by an ECFG G is the set of ter-
minal strings derivable from the starting symbol S of
G. Formally, L(G) = {w ∈ Σ∗|S ⇒ w}, where ⇒ de-
notes the transitive closure of the derivability relation.
We represent as a parse tree d any sequential form that
reﬂects the derivational steps. The set of parse trees for

G forms the set T (G) of unranked labeled rooted trees
constrained with schema G.

2.1 Tree annotation problem
When annotating HTML documents accordingly to a
target XML schema, the main diﬃculty arises from the
fact that the source documents are essentially layout-
oriented, and the use of tags and attributes is not nec-
essarily consistent with elements of the target schema.
The irregular use of tags in documents, combined with
complex relationships between elements in the target
schema, makes the manual writing of HTML-to-XML
transformation rules diﬃcult and cumbersome.

In supervised learning, the content of source docu-
ments is presented as a sequence of observations x =
{x1, . . . , xn}, where any observation xi refers to a con-
tent fragment, surrounded by rich contextual informa-
tion in the form of HTML tags, attributes, etc. The tree
annotation model is deﬁned as a mapping X → (Y, D)
that maps the observation sequence x into a pair (y, d)
where y={y1, . . . , yn} is a terminal sequence and d is a
parse tree of y according to the target schema or equiv-
alent PCFG G, S ⇒ y. The training set S for training
the model parameters is given by a set of triples (x, y, d).
To determine the most probable tree annotation (y, d)
for a sequence x, we maximize the joint probability
p(y, d|x, G), given the sequence x and PCFG G. Us-
ing the Bayes theorem and the independence of x and
G, we have

p(y, d|x, G) = p(d|y, G) · p(y|x),

(1)
where p(y|x) is the probability of terminal sequence y
for the observed sequence x, and p(d|y, G) is the proba-
bility of the parse d for y according the PCFG G. The
most probable tree annotation for x is a pair (y, d) that
maximizes the probability in (1),

(y, d)max = argmax

(y,d)

p(d|y, G) · p(y|x).

(2)

In the following, we build a probabilistic model for
tree annotation of source documents consisting of two
components to get the two probability estimates in (2).
The ﬁrst component is a probabilistic extension of the
target XML schema; for a given terminal sequence y,
it ﬁnds the most probable parse p(d|y, G) for sequences
according to the PCFG G, where rule probabilities are
trained from the available training set. The second com-
ponent is a probabilistic content classiﬁer C, it estimates
the conditional probabilities p(y|xi) for annotating ob-
servations xi with terminals y ∈ T . Finally, for a given
sequence of observations x, we develop two methods for
ﬁnding a tree annotation (y, d) that maximizes the joint
probability p(y, d|x, G) in (1).

3 Probabilistic context-free grammars
PCFGs are probabilistic extensions of CFGs, where each
rule A → α in R is associated with a real number p in
the half-open interval (0; 1]. The values of p obey the

restriction that for a given non-terminal A ∈ N, all rules
for A must have p values that sum to 1,

(cid:1)

∀A ∈ N :

r=A→α,r∈R

p(r) = 1.

(3)

PCFGs have a normal form, called the Chomsky Nor-
mal Form (CNF), according to which any rule in R is
either A → B C or A ∈ b, where A, B and C are non-
terminals and b is a terminal. The rewriting of XML
annotations requires the binarization of source ranked
trees, often followed by an extension of the nonterminal
set and the underlying set of rules. This is a conse-
quence of rewriting nodes with multiple children as a
sequence of binary nodes. The binarization rewrites any
rule A → B C D as two rules A → BP and P → C D,
where P is a new non-terminal.

A PCFG deﬁnes a joint probability distribution over
Y and D, random variables over all possible sequences
of terminals and all possible parses, respectively. Y and
D are clearly not independent, because a complete parse
speciﬁes exactly one or few terminal sequences. We de-
ﬁne the function p(y, d) of a given terminal sequence
y ∈ Y and a parse d ∈ D as the product of the p values
for all of the rewriting rules R(y, d) used in S ⇒ y. We
also consider the case where d does not actually corre-
spond to y,

(cid:2) (cid:3)

p(y, d) =

0,

r∈R(y,d) p(r),

if d is a parse of y
otherwise.

(cid:4)

The values of p are in the closed interval [0; 1].

In
the cases where d is a parse of y, all p(r) values in the
product will lie in the half open interval (0; 1], and so
In the other case, 0 is in [0; 1] too.
will the product.
However, it is not always the case that
d,y p(y, d) = 1.
The PCFG training takes as evidence the corpus of
terminal sequences y with corresponding parses d from
the training set S. It associates with each rule an ex-
pected probability of using the rule in producing the cor-
pus. In the presence of parses for all terminal sequences,
each rule probability is set to the expected count nor-
malized so that the PCFG constraints (3) are satisﬁed:

p(A → α) =

count(A → α)

(cid:4)
A→β∈R count(A → β) .
3.1 Generalized probabilistic parsing
PCFGs are used as probabilistic models for natural lan-
guages, as they naturally reﬂect the “deep structure”
of language sentences rather than the linear sequences
of words.
In a PCFG language model, a ﬁnite set of
words serve as a terminal set and production rules for
non-terminals express the full set of grammatical con-
structions in the language. Basic algorithms for PCFGs
that ﬁnd the most likely parse d for a given sequence y or
choose rule probabilities that maximize the probability
of sentence in a training set, represent [6] eﬃcient ex-
tensions of the Viterbi and Baum-Welsh algorithms for
hidden Markov models.

The tree annotation model processes sequences of ob-
servations x = {x1, . . . , xn} from the inﬁnite set X,
where the observations xi are not words in a language
(and therefore terminals in T ) but complex instances,
like HTML leaves or groups of leaves.

(cid:4)

Content fragments are frequently targeted by vari-
ous probabilistic classiﬁers that produce probability es-
timates for labeling an observation with a terminal in
T , p(y|xi), y ∈ T , where
y p(y|xi) = 1. The tree an-
notation problem can therefore be seen as a generalized
version of probabilistic context-free parsing, where the
input sequence is given by the probability distribution
over a terminal set and the most probable annotation
tree requires maximizing the joint probability in (2).

A similar generalization of probabilistic parsing takes
place in speech recognition. In the presence of a noisy
channel for speech streams, parsing from a sequence of
words is replaced by parsing from a word lattice, which
is a compact representation of a set of sequence hypothe-
ses, given by conditional probabilities obtained by spe-
cial acoustic models from acoustic observations [4].

4 Content classiﬁer
To produce terminal estimates for the observations xi,
we adopt the maximum entropy framework, according to
which the best model for estimating probability distribu-
tions from data is the one that is consistent with certain
constraints derived from the training set, but otherwise
makes the fewest possible assumptions [1]. The distri-
bution with the fewest possible assumptions is one with
the highest entropy, and closest to the uniform distri-
bution. Each constraint expresses some characteristic
of the training set that should also be present in the
learned distribution. The constraint is based on a binary
feature, it constrains the expected value of the feature
in the model to be equal to its expected value in the
training set.

One important advantage of maximum entropy models
is their ﬂexibility, as they allow the extension of the rule
system with additional syntactic, semantic and prag-
matic features. Each feature f is binary and can depend
on y ∈ T and on any properties of the input sequence
x. In the case of tree annotation, we include the content
features that express properties on content fragments,
like f1(x, y) =“1 if y is title and x’s length is less then
20 characters, 0 otherwise”, as well as the structural and
layout features that capture the HTML context of the
observation x, like f2(x, y)=“1 if y is author and x’s
father is span, 0 otherwise”.

With the constraints based on the selected features
f(x, y), the maximum entropy method attempts to max-
imize the conditional likelihood of p(y|x) which is repre-
sented as an exponential model:

(cid:5)(cid:1)

(cid:6)
λα · fα(x, y)

,

(4)

p(y|x) =

1

Zα(x) exp

α

where Zα(x) is a normalizing factor to ensure that all
the probabilities sum to 1,

(cid:1)

(cid:5)(cid:1)

exp

y

α

Zα(x) =

(cid:6)

λαfα(x, y)

.

(5)

For the iterative parameter estimation of the Max-
imum Entropy exponential models, we use one of the
quasi Newton methods, namely the Limited Memory
BFGS method, which is observed to be more eﬀective
than the Generalized Iterative Scaling (GIS) and Im-
proved Iterative Scaling (IIS) for NLP and IE tasks [7].

5 Sequential tree annotation

We use pairs (x, y) from triples (x, y, d) of the training
set S to train the content classiﬁer C and pairs (y, d)
to choose rule probabilities that maximize the likelihood
for the instances in the training set. C predicts the ter-
minal probabilities p(y|x) for any observation x, while
the inside-outside algorithm can ﬁnd the parse d of the
highest probability for a given terminal sequence y.

By analogy with speech recognition, there exists a
naive, sequential method to combine the two compo-
nents C and G for computing a tree annotation for se-
quence x. First, from C’s estimates p(y|x), we deter-
mine the (top k) most probable sequences ymax,j for x,
j = 1, . . . , k. Second, we ﬁnd the most probable parses
p(d|ymax,j, G); and ﬁ-
for all ymax,j, dmax,j = argmax
nally, we choose the pair (ymax,j, dmax,j) that maximizes
the product p(ymax,j) × p(dmax,j).

d

The sequential method works well if the noise level
is low (in speech recognition) or if the content classiﬁer
(in the tree annotation) is accurate enough in predicting
terminals y for xi. Unfortunately, it gives poor results
once the classiﬁer C is far from 100% accuracy in y pre-
dictions, as it faces the impossibility of ﬁnding any parse
for all the top k most probable sequences ymax,j.

Example. Consider an example target schema given
by the following DTD:

<!ELEMENT Book
<!ELEMENT Section
<!ELEMENT author
<!ELEMENT title
<!ELEMENT para
<!ELEMENT footnote

(author, Section+)>
(title, (para | footnote)+)>
(#PCDATA)>
(#PCDATA)>
(#PCDATA)>
(#PCDATA)>

The reduction of the above schema deﬁnition to
the Chomsky Normal Form will introduce extra non-
terminals, so we get the PCFG G = (T, N,S, R),
where the terminal set is T ={author, title, para,
footnote}, the nonterminal set is N= {Book, Author,
SE, Section, TI, ELS, EL}, S= Book, and R includes
twelve production rules.

Assume that we have trained the content classiﬁer C
and the PCFG G and have obtained the following prob-
abilities for the production rules in R:

(0.7) Book → AU SE
(0.6) SE → Section SE
(0.2) Section → TI EL
(0.6) ELS → EL ELS
(1.0) TI → title
(0.2) EL → footnote.

(0.3) Book → AU Section
(0.4) SE → Section Section
(0.8) Section → TI ELS
(0.4) ELS → EL EL
(1.0) AU → author
(0.8) EL → para
Assume now that we test the content classiﬁer C and
PCFG G on a sequence of ﬁve unlabeled observations
x = {x1, . . . , x5}. Let the classiﬁer C estimate the prob-
ability for terminals in T as given in the following table:

author
title
para
footnote

x1
0.3
0.4
0.1
0.2

x2
0.2
0.4
0.2
0.2

x3
0.1
0.3
0.5
0.1

x4
0.1
0.3
0.2
0.4

x5
0.2
0.3
0.2
0.2

i

According to the above probability distribution, the
most probable terminal sequence ymax is composed of
the most probable terminals for all xi, i = 1, . . . , 5.
It is ’title title para footnote title’ with prob-
ability p(ymax) = p(ymax|x) = Πi · p(ymax
|xi) =
0.4 · 0.4 · 0.5 · 0.4 · 0.3 = 0.0096. However, ymax has
no corresponding parse tree in G. Instead, there exist
two valid annotation trees for x, (y1, d1) and (y2, d2),
as shown in Figure 1.
In Figure 1.b, the terminal
sequence y2=‘author title para title para’ with
the parse d2=Book(AU SE(Section (TI EL) Section (TI
EL))) maximizes the joint probability p(y, d|x, G), with
p(y2) = 0.3 · 0.4 · 0.5 · 0.3 · 0.2 = 0.0036, and
p(d2)=p(Book → AU SE) · p(AU → author) ×

p(SE → Section Section) · p(Section → TI EL) ×
p(TI → title)2 · p(EL → para)2 ×
p(Section → TI EL)
=0.7 · 1.0 · 0.4 · 0.2 · 1.02 · 0.82 · 0.2 = 0.007172.

Jointly, we have p(y2)×p(d2) ≈ 2.58·10−5. Similarly, for
the annotation tree in Figure 1.a, we have p(y1)×p(d1) =
0.0048 · 0.0018432 ≈ 8.85 · 10−6.

Book

Section

a)

b)

Book

ELS

ELS

AU TI

EL

EL

EL

author

title para

footnote

para

x1

x2

x3

x4

x5

d

1

1

y

x

SE

Section

Section

AU

TI

EL

TI

EL

author title para title para

x1

x2

x3

x4

x5

d

2

2

y

x

Figure 1: Tree annotations for the example sequence.

6 The most probable annotation tree
As the sequential method fails to ﬁnd the most probable
annotation tree, we try to couple the selection of termi-
nal sequence y for x with ﬁnding the most probable parse
d for y, such that (y, d) maximizes the probability prod-
uct p(d|y, G)· p(y|x) in (2). For this goal, we extend the

basic inside-outside algorithm for terminal PCFGs [6].
We redeﬁne the inside probability as the most probable
joint probability of the subsequence of y beginning with
index i and ending with index j, and the most proba-
ble partial parse tree spanning the subsequence yj
i and
rooted at nonterminal A:

βA(i, j) = maxA,yj

i

p(Ai,j ⇒ yj

i ) · p(yj

i|x).

(6)

The inside probability is calculated recursively, by tak-
ing the maximum over all possible ways that the nonter-
minal A could be expanded in a parse,

βA(i, j) = maxi≤q≤j p(A → BC) · p(B ⇒ yq

i ) ×

p(C ⇒ yj

q+1) · p(yj

i|x).

To proceed further, we make the independence as-
sumption about p(y|x), meaning that for any q, i ≤ q ≤
q+1|x). Then, we can
i|x) = p(yq
j, we have p(yj
rewrite the above as follows

i |x) · p(yj

βA(i, j) = maxi≤q≤j p(A → BC) · p(B ⇒ yq
i ) ×
i |x) · p(yj

(7)
(8)
= maxi≤q≤j p(A → BC) · βB(i, q) · βC(q + 1, j)(9)

q+1) · p(yq

p(C ⇒ yj

q+1|x)

The recursion is terminated at the βS(1, n) which gives
the probability of the most likely tree annotation (y, d),

βS(1, n) = max p(S ⇒ yn

1 ) · p(yn

1|x),

where n is the length of both sequences x and y.

The initialization step requires some extra work, as we
select among all terminals in T being candidates for yk:

βA(k, k) = maxyk p(A → yk) · p(yk|x).

(10)
It can be shown that the redeﬁned inside function con-
verges to a local maximum in the (Y, D) space. The extra
work during the initialization step takes O(n · |T| · |N|)
time which brings the total complexity of the extended
IO algorithm to O(n3 · |N| + n · |T| · |N|).
(cid:3)n
The independence assumption established above rep-
resents the terminal conditional independence, p(y|x) =
i=1 p(yi|x) and matches the Naive Bayes assumption.
The assumption is frequent in text processing; it sim-
pliﬁes the computation by ignoring the correlations be-
tween terminals. Here however it becomes a require-
ment for the content classiﬁer. While PCFGs are as-
sumed to capture all (short- and long- distance) re-
lations between terminals, the extended inside algo-
rithm (9)-(10) imposes the terminal conditional inde-
pendence when building the probabilistic model. This
impacts the feature selection for the maximum entropy
model, by disallowing features that include terminals of
neighbor observations yi−1, yi+1, etc, as in the maxi-
mum entropy extension with HMM and CRF models [8;
5].

7 Experimental results

The second collection, called TechDoc,

We have tested our method for XML annotation on two
collections. One is the collection of 39 Shakespearean
plays available in both HTML and XML format.1 60
scenes with 17 to 189 leaves were randomly selected for
the evaluation. The DTD fragment for scenes consists of
4 terminals and 6 non-terminals. After the binarization,
the PCFG in CNF contains 8 non-terminals and 18 rules.
includes 60
technical documents from repair manuals. 2 The tar-
get documents have a ﬁne-grained semantic granularity
and are much deeper than in the Shakespeare collection;
the longest document has 218 leaves. The target schema
is given by a complex DTD with 27 terminals and 35
nonterminals. The binarization increased the number
of non-terminals to 53. For both collections, a content
observation refers to a PCDATA leaf in HTML.

To evaluate the annotation accuracy, we use two met-
rics. The terminal error ratio (TER) is similar to the
word error ratio used in natural language tasks; it mea-
sures the percentage of correctly determined terminals in
test documents. The second metric is the non-terminal
error ratio (NER) which is the percentage of correctly
annotated sub-trees.

As content classiﬁers, we test with the maximum en-
tropy (ME) classiﬁer. For the ME model, we extract 38
content features for each observation, such as the number
of words in the fragment, its length, POS tags, textual
separators, etc. Second, we extract 14 layout and struc-
tural features include surrounding tags and all associated
attributes. Beyond the ME models, we use the maximum
entropy Markov models (MEMM) which extends the ME
with hidden Markov structure and terminal conditional
features [8]. The automaton structure used in MEMM
has one state per terminal.

In all tests, a cross-validation with four folds is used.
ME and MEMM were ﬁrst tested alone on both col-
lections. The corresponding TER values for the most
probable terminal sequences ymax serve a reference for
methods coupling the classiﬁers with the PCFG. When
coupling the ME classiﬁer with the PCFG, we test both
the sequential and joint methods. Additionally, we in-
cluded a special case MEMM-PCFG where the content
classiﬁer is MEMM and therefore the terminal condi-
tional independence is not respected.

The results of all the tests are collected in Table 1.
The joint method shows an important advantage over
the sequential method, in particular in the TechDoc case,
where the ME content classiﬁer alone achieves 86.23%
accuracy and the joint method reduces the errors in ter-
minals by 1.36%.
Instead, coupling MEMM with the
PCFG reports a decrease of TER values and a much less
important NER increase.

1http://metalab.unc.edu/bosak/xml/eg/shaks200.zip.
2Available from authors on request.

Method

ME
MEMM
Seq-ME-PCFG
Jnt-ME-PCFG
Jnt-MEMM-PCFG

TechDoc

TER
86.23
78.16
86.23
87.59
75.27

NER
–
–
9.38
72.95
56.25

Shakespeare
NER
TER
–
100.0
–
99.91
82.87
100.0
99.97
99.79
98.09
94.01

Table 1: Evaluation results.

8 Relevant Work
Since the importance of semantic annotation of doc-
uments has been widely recognized, the migration of
documents from rendering-oriented formats, like PDF
and HTML, toward XML has become an important re-
search issue in diﬀerent research communities [2; 3; 11;
12]. The majority of approaches either constrain the
XML conversion to a domain speciﬁc problem, or make
diﬀerent kinds of assumptions about the structure of
source and target documents. In [11], the source HTML
documents are assumed to be dynamically generated
through a form ﬁlling procedure, as in Web news portals,
while a portal subject ontology permits the semantic an-
notation of the generated documents.

Transformation-based learning is used for automatic
translation from HTML to XML in [3]. It assumes that
source documents can be transformed into target XML
documents through a series of proximity tag operations,
including insert, replace, remove and swap. The transla-
tion model trains a set of transformation templates that
minimizes an error driven evaluation function.

In document analysis research, Ishitani in [12] applies
OCR-based techniques and the XY-cut algorithm in or-
der to extract the logical structure from page images
and to map it into a pivot XML structure. While logical
structure extraction can be automated to a large extent,
the mapping from the pivot XML to the target XML
schema remains manual.

In natural language tasks, various information extrac-
tion methods often exploit the sequential nature of data
to extract diﬀerent entities and extend learning models
with grammatical structures, like HMM [8] or undirected
graphical models, like Conditional Random Fields [5].
Moreover, a hierarchy of HMMs is used in [13] to improve
the accuracy of extracting speciﬁc classes of entities and
relationships among entities. A hierarchical HMM uses
multiple levels of states to describe the input on diﬀerent
level of granularity and achieve a richer representation
of information in documents.

9 Conclusion
We propose a probabilistic method for the XML anno-
tation of semi-structured documents. The tree annota-
tion problem is reduced to the generalized probabilistic
context-free parsing of an observation sequence. We de-
termine the most probable tree annotation by maximiz-
ing the joint probability of selecting a terminal sequence

for the observation sequence and the most probable parse
for the selected terminal sequence.

We extend the inside-outside algorithm for probabilis-
tic context-free grammars. We beneﬁt from the available
tree annotation that allows us to extend the inside func-
tion in a rigorous manner, and avoid the extension of the
outside function which might require approximation.

The experimental results are promising.

In future
work, we plan to address diﬀerent challenges in automat-
ing the HTML-to-XML conversion. We are particularly
interested in extending the annotation model with the
source tree structures that have been ignored so far.

10 Acknowledgement
This work is supported by VIKEF Integrated Project
co-funded under the EU 6th Framework Programme.

References
[1] A. L. Berger, S. Della Pietra, and V. J. Della Pietra.
A maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39–71, 1996.
[2] N. Sundaresan, Ch. Chung, M. Gertz. Reverse engineer-
ing for web data: From visual to semantic structures. In
Proc. 18th ICDE’02, San Jose, California, 2002.
J.R. Curran and R.K. Wong. Transformation-based
learning for automatic translation from HTML to XML.
In Proc. 4th Australas. Doc. Computing Symp., 1999.

[3]

[4] K. Hall and M. Johnson. Language modeling using ef-
In IEEE Autom.

ﬁcient best-ﬁrst bottomup parsing.
Speech Recogn. and Understanding Workshop, 2003.
J. Laﬀerty, A. McCallum, and F. Pereira. Conditional
random ﬁelds: Probabilistic models for segmenting and
labeling sequence data. In Proc. of 18th ICML’01, San
Francisco, CA, pages 282–289, 2001.

[5]

[6] K. Lari and S. J. Young. The estimation of stochas-
tic context-free grammars using the inside-outside algo-
rithm. Computer Speech and Language, 4:35–56, 1990.
[7] R. Malouf. A comparison of algorithms for maximum
entropy parameter estimation. In Proc. of 6th Conf. on
Natural Language Learning, pages 49–55, 2002.

[8] A. McCallum, D. Freitag, and F. Pereira. Maximum
entropy Markov models for information extraction and
segmentation. In Proc. 17th ICML’00, San Francisco,
CA, pages 591–598, 2000.

[9] Frank Neven. Automata Theory for XML Researchers.

SIGMOD Record, 31(3):39–46, 2002.

[10] Yannis Papakonstantinou and Victor Vianu. DTD In-
ference for Views of XML Data. In Proc. of 19th ACM
PODS, Dallas, Texas, USA, pages 35–46, 2000.

[11] I.V. Ramakrishnan, S. Mukherjee, G. Yang. Automatic
annotation of content-rich web documents: Structural
and semantic analysis. In Intern. Sem. Web Conf., 2003.
[12] Y. Ishitani. Document transformation system from pa-
pers to xml data based on pivot document method. In
Proc. of 7th ICDAR’03, pages 250–255, 2003.

[13] M. Skounakis, M. Craven, and S. Ray. Hierarchical hid-
den markov models for information extraction. In Proc.
of 18th IJCAI, Acapulco, Mexico, 2003.


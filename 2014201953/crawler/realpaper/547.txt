Evaluating Coverage for Large Symbolic NLG Grammars 

Charles B. Callaway 

ITC-irst Istituto per la Ricerca Scientifica e Tecnologica 

via Sommarive, 18 - Povo 

38050 Trento, Italy 
callaway@itc.it 

Abstract 

After  many  successes,  statistical  approaches  that 
have  been  popular  in  the  parsing  community  are 
now making headway into Natural  Language Gen(cid:173)
eration (NLG). These systems are aimed mainly at 
surface  realization,  and  promise  the  same  advan(cid:173)
tages that make statistics  valuable for parsing:  ro(cid:173)
bustness, wide coverage and domain independence. 
A  recent  experiment  aimed  to  empirically  verify 
the linguistic coverage for such a statistical surface 
realization  component  by  generating  transformed 
sentences from the Penn TreeBank corpus. This ar(cid:173)
ticle  presents  the empirical results  of a similar ex(cid:173)
periment to evaluate the coverage of a purely sym(cid:173)
bolic surface realizer. We present the problems fac(cid:173)
ing a symbolic approach on the same task, describe 
the results of its evaluation, and contrast them with 
the  results  of the  statistical  method  to  help  quan(cid:173)
titatively  determine the  level  of coverage currently 
obtained by NLG surface realizers. 

Introduction 

1 
Like parsing, text generation offers enormous potential bene(cid:173)
fits for more natural interaction with computers. Examples of 
applications  which  could  be greatly  improved include auto(cid:173)
matic  technical  documentation,  intelligent tutoring  systems, 
and  machine  translation,  among  many  others.  Historically, 
natural language generation (NLG) has focused on the study 
of symbolic pipelined architectures which receive knowledge 
structures and goals from knowledge-based applications and 
which proceed to progressively add linguistic information. 

In  the  last  few  years,  the  same  paradigm  shift  which 
occurred  in  the  parsing  community,  the  use  of  statisti(cid:173)
cal/empirical methods, has begun to influence the NLG com(cid:173)
munity  as  well.  As  with  parsing,  statistical  generation 
promises benefits such as robustness in the face of bad data, 
wider coverage, domain and language independence, and less 
need for costly resources such as grammars. But unlike pars(cid:173)
ing,  which starts with a very flat representation (text) which 
is easily accessible in  large quantities to both  statistical  and 
symbolic  methods,  the  semantic  input  for  NLG  is  typically 
associated with large knowledge-based systems. The types of 
corresponding  corpora which  would  be  necessary  for  using 

statistical  processes,  pairs  of subgraphs of knowledge bases 
and their texts, do not currently exist in large quantities. 

Because  of  this  representation  problem,  most  statistical 
systems  have  concentrated  on  replacing  existing  individual 
components in the standard NLG pipelined architecture [Re-
iter,  1994]  without  changing  the  remaining  original  sym(cid:173)
bolic  modules.  The  most  popular  candidate  has  been  the 
surface realization module [Elhadad,  1991; Bateman,  1995; 
Lavoie  and  Rambow,  1997;  White  and  Caldwell,  1998], 
which is responsible for converting the syntactic representa(cid:173)
tion of a sentence into the actual text seen by the user.  Thus 
current statistical generators are still dependent on remaining 
architectural modules in  a system to function and do not by 
themselves account for a large amount of linguistic phenom(cid:173)
ena: pronominalization, revision, definiteness, etc. 

However,  statistical  surface  realizers 

[Langkilde  and 
Knight,  1998;  Bangalore  and  Rambow,  2000;  Ratnaparkhi, 
2000;  Langkilde-Geary,  2002]  have  focused  attention  on  a 
number  of  problems  facing  standard,  pipelined  NLG  that 
have until now been generally considered future work:  large-
scale,  data-robust  and  language-  and  domain-independent 
generation.  In  addition,  as  Langkilde points  out,  empirical 
evaluation  has  not been  standard practice  in  the NLG  com(cid:173)
munity, which has instead relied either on the software engi(cid:173)
neering practice of regression testing with a suite of examples 
or theoretical evaluations [Robin and McKeown,  1995]. 

This  paper presents  the  analogue  of this  recent  statistical 
experiment  using  a  well-known  off-the-shelf symbolic  sur(cid:173)
face  realizer,  using  an  augmented  generation  grammar  that 
includes  support  for  dialogue  and  additional  syntactic  cov(cid:173)
erage.  We  first  describe in  the  following  section  the repre(cid:173)
sentations and processes needed to understand its evaluation. 
We  then  detail  our implemented system for converting sen(cid:173)
tences  from  a  large  corpus  into  a  systemic  functional  nota(cid:173)
tion,  present an  evaluation  of that  system  and  the  grammar 
itself using  Section  23  of the  Penn TreeBank  [Marcus  el al, 
1993], and  finally  discuss the implications of that evaluation. 

2  Sentence Representations 
To undertake a  large-scale evaluation  of a  symbolic  surface 
realizer, we must  first  find  a large quantity of sentence plans 
with which to produce text. However, most text planners can(cid:173)
not generate either the  requisite  syntactic  variation  or quan(cid:173)
tity  of text,  and  we  thus  cannot  turn  to  implemented  gener-

NATURAL  LANGUAGE 

811 

Figure 1: A Perm TreeBank Annotated Sentence and Corresponding FUF/SURGE Functional Description 

ation systems as a source.  To solve this problem, Langkilde 
trained a statistical  algorithm  [Langkilde-Geary, 2002] on a 
substitute set of sentence plans:  the Penn TreeBank [Marcus 
et ai,  1993], a collection of sentences from newspapers such 
as the Wall  Street Journal, which have been hand-annotated 
for syntax by linguists. An example sentence is shown on the 
left side of Figure  1.  Hierarchical syntactic/semantic bracket(cid:173)
ing is provided along with the syntactic categories of lexemes 
and symbols in the newspaper texts. 

Unfortunately, text planners currently in use do not gener(cid:173)
ate  representations of the form found in the Penn TreeBank, 
opting instead to use more fully-developed syntactic theories, 
such  as  HPSG  [Pollard  and  Sag,  1994],  from  the  linguis(cid:173)
tics community.  Because annotated texts do not exist in this 
form, Langkilde created a pre-processing system to translate 
from the TreeBank annotation into the language accepted by 
the  HALOGEN  statistical  surface  realizer  [Langkilde-Geary, 
2002].  HALOGEN  uses these  translations to create a forest 
lattice whose paths from start to finish represent many possi(cid:173)
ble versions of a single sentence.  Separately, a larger corpus 
is processed to obtain bigram or trigram frequencies, which 
are  then  used  to  rank  the  possible  sentence  versions  based 
on word adjacency.  The highest ranked sentence is then pre(cid:173)
sented as the final output of the system. 

In contrast, most deep surface realizers are symbolic rather 
than  statistical,  and consist of components that check gram(cid:173)
matical  constraints,  appropriately linearize constituents,  and 
adjust  for  morphology  and  formatting.  One  such  system 
in  wide  use,  FUF/SURGE  [Elhadad,  1991],  combines  ideas 
from  systemic  functional  grammars  and  head-driven  phrase 
structure grammars.  An example of the F UF representation, 
known  as  afunctional description  is  shown  on  the right side 
of Figure 1.  SURGE is the largest generation grammar for En(cid:173)
glish, and has the largest regression test suite available.  But 
as  Langkilde pointed out,  500 test examples are insufficient 
to empirically demonstrate the coverage of a grammar. 

To  arrive  at  a  set  of sentence  plans  which  is  representa(cid:173)
tive  of English,  as  well  as  to  evaluate  the  coverage  of the 
FUF/SURGE surface realizer in a way which can be directly 
compared to the HALOGEN evaluation, we likewise used the 
Penn TreeBank as a sentence source.  Because our represen(cid:173)
tations  are  also  different,  we  (as  Langkilde)  needed  a  pre(cid:173)
processing system to convert from the TreeBank notation into 
the functional descriptions expected by the surface realizer. 
Our pre-processor thus performs top-down structure traver(cid:173)
sal  of  a  sentence  annotated  in  Penn  TreeBank  format  and 

Figure 2:  Penn TreeBank Notation and Normalized Form 

builds  the  corresponding  functional  description.  The  pre(cid:173)
processor is organized as a context-sensitive, proceduralized 
rewriting  grammar  which  matches  input  symbols  to  output 
symbols.  The  resulting  functional  descriptions can  then  be 
given  to  the  FUF/SURGE  surface  realizer,  and  the  sentence 
string  it  produces can  be  lexically  compared  to  the  original 
sentence  in  various ways  to determine how  well  the  surface 
realizer performs at sentence generation. 

Implementation 

3 
The  implementation  necessary  for  evaluating  the  coverage 
of FUF/SURGE  comprised  three  processes:  (1)  normalizing 
the  syntactic/semantic  representations,  (2)  transforming  the 
normalizations into  functional descriptions,  and  (3) generat(cid:173)
ing the sentence itself with a surface realizer.  The normaliz(cid:173)
ing phase is necessary to convert the original Penn TreeBank 
structures into a LlSP-readable format (Figure 2), which was 
accomplished with a series of regular expression transforma(cid:173)
tions on the original text file. 

The most time-consuming aspect of the procedure was cre(cid:173)
ating the transformation component, which was highly anal(cid:173)
ogous to writing parsing rules by hand.  The resulting com(cid:173)
ponent contained 4000 lines of code  and  approximately 900 
rules,  although  most  of the  actual  computational  effort  was 
spent  instead  in  surface  realization.  Most  of  the  problems 
encountered were  the  result of differences  in  the  underlying 
grammars themselves.  For example, the Penn TreeBank has 
a more hierarchical noun phrase structure than the flatter rep(cid:173)
resentation of SURGE'S systemic functional grammar. 

The  final  task  involved  changing  the  surface  realization 
component (1) to add additional branches and surface forms 
to  the  grammar  that  were  not  originally  present  in  order to 
produce surface forms not previously possible, (2) to add new 
punctuation and capitalization rules1, and (3) to update irreg-

]The Penn TreeBank, because it is a newspaper corpus, contains 
many  newspaper  headlines  and  stock  quotes  with  domain-specific 

812 

NATURAL  LANGUAGE 

Table  1:  Comparison  with  HALOGEN  fLangkilde, 2002J 

ular  morphology due  to  the  vast  number of words  the  system 
had  not  previously  seen.  The  principal  linguistic  problems 
uncovered by  this  phase  include: 

•  Quotations:  Newspaper  text  generally  contains 

large 
amounts  of  complex  quotations,  such  as  splitting  a 
quoted  phrase  to  insert  the  speaker  in  the  middle,  or 
merging a quote  into an  unquoted part of the  sentence: 
"1  have this feeling that it's built on  sand," she says, that 
the  market rises  "but  there's no  foundation to  it." 

•  Punctuation  scoping:  Problems  related  to  the  use  of 
punctuation  with  tree  structureslDoran,  19981.  For  ex(cid:173)
ample, S U R GE has a flat representation for noun phrases, 
causing  difficulties  with  phrases  such  as  The  major  "cir(cid:173)
cuit  breakers"  where  SURGE  cannot  insert  punctuation 
between  the  adjective and nominal classifier. 

•  Adverb  and  clause  ordering:  Because  satellite  clauses 
in  SURGE  are  placed  using  semantic  information,  they 
sometime appear in  different  (though still  grammatically 
acceptable)  positions  than  were  specified  in  the  original 
sentence  [Elhadad  et  ai,  2001 J.  This  can  oftentimes 
cause  a  perfectly  acceptable  sentence  to  be  produced, 
but  highly  skew  automatic  measurements  of correctness 
such  as  tree  edit  distance  (simple  string  accuracy).  For 
example,  contrast:  "Exports  fell  29%  in  the  first  few 
months" vs.  "In  the  first  few  months, exports fell  29%." 

•  Semantic  roles:  SURGE  has  a  hybrid  syntactic/semantic 
representation,  whereas  the  Penn  TreeBank  is  purely 
syntactic.  Thus  some  guessing  must  be  done  to  fill  in 
semantic  roles  in  the  corresponding  functional  descrip(cid:173)
tion.  Wrong guesses  can  lead  to  incorrect surface  forms 
even  when  the  remainder  of the  transformation  was  ac(cid:173)
complished  successfully. 

While  the normalization process  is  slow  (10-12  hours each 
for WSJ23  and  WSJ24),  it  occurs offline  and  only  once.  The 
transformation process is quite fast,  requiring on average 0.12 
seconds per sentence.  Meanwhile, the  FUF/SURGE  system is 
relatively slow,  as  it requires the use of functional unification, 
a  task  of  inherent  complexity  due  to  backtracking.  Section 
23  needed  8,397.2  seconds  to  generate  2372  sentences  (with 
a  longest  exact  match  of 48  words),  while  section  24  needed 
5,590.4  seconds  to  generate  1,326  sentences  (with  a  longest 
exact  match  of 44),  for  a  combined  average  of  3.72  seconds 
per  sentence.2  This  contrasts  with  statistical  approaches  like 
Langkilde's,  which  require  27.1-55.5  seconds  depending  on 

formatting not typically used in NLG systems, such as:  "8  13/16% 
high, 8 1/2% low, 8 5/8% near closing bid, 8 3/4% offered." 

Additionally,  a  compiled  C  version  of  FUF can  produce  sen(cid:173)

tences using the same grammar on the order of 0.1  seconds. 

algorithm  parameters,  and  gets  exponentially  worse  if it  uses 
trigrams or larger models  in  an  attempt to  improve quality. 

4  Experiments and Results 
In  order to  evaluate  the  coverage of the  SURGE  grammar,  we 
used  the  standard  train  and  test  methodology.  Unlike  typi(cid:173)
cal  machine  learning  experiments,  adjustments  to  the  trans(cid:173)
formation  rule  set  were  done  by  hand,  although  the  evalua(cid:173)
tion  of the  resulting  sentences  was  performed  automatically. 
Training  took  place  over a  period  of several  months,  consist(cid:173)
ing  of multiple  iterations  over  Penn  TreeBank  Sections  0-22 
and  24  to both  improve the  number of sentences  which  could 
be  generated  and  to  match  as  closely  as  possible  the  origi(cid:173)
nal  sentences.  These  two  goals  were  accomplished solely  by 
adding rules to the transformation set and by updating  S U R GE 
grammar rules,  notably aspects pertaining to the  stock  market 
domain,  in  addition to  support for extended quotations which 
was added  in  previous work  iCallaway and  Lester,  20021. 

We  considered 

two 

that  were 

types  of  string  matches: 

exact 
matches 
identical  character-by-character,  and 
"close"  matches  which  were  two  words  or  less  longer  or 
shorter than the original sentence.  There were several motiva(cid:173)
tions  for this  second  choice:  (1)  many  sentences  were  equiv(cid:173)
alent  except  for  a  minor  missing/extra  punctuation  mark  or 
wrongly  capitalized  word  (especially  with  newspaper  head(cid:173)
lines);  (2)  as  mentioned  previously,  movable  clauses  (so-
called  circumstantials  in  SURGE  nomenclature)  could  be  put 
in  multiple  acceptable  locations;  and  (3)  sentences  with  al(cid:173)
most  the  exact  number  of  words,  especially  sentences  with 
more  than  15  words,  were  much  more  likely  to  at  least  have 
all  of  the  various  phrases  present  when  they  were  within 
two  words  or  the  original  sentence's  length.  We  utilized  the 
N1ST  Simple  String  Accuracy  (SSA)  as  an  automatic  eval(cid:173)
uation  score  (the  same  as  used  in  Langkilde's  work),  where 
the  smallest  number of Adds,  Deletions,  and  Insertions  were 
used  to calculate  accuracy:  1  - (A + D  + T)/#Characters. 

The  only  previous measure  of generation  coverage for Sec(cid:173)
tion  23  of  the  Penn  TreeBank  is  that  of  [Langkilde-Geary, 
2002],  who  defined  coverage  as  the  number of sentences  for 
which  the  surface  realizer  produced  strings.  As  seen  in  Ta(cid:173)
ble  1,  our  system  achieved  87.8%  on  one  of the  training  sets 
and  88.7%  on  the  test  set  compared  to  between  76.2%  and 
83.3% for HALOGEN depending on its algorithm parameters. 
A  more  detailed  examination  of the  coverage and  accuracy 
of  the  system  is  found  in  Table  2  for  WSJ  Section  24  and 
Table  3  for Section  23.  Both  tables  are  broken down  by  sen(cid:173)
tence  length,  which  shows  that  the  results  are  highly  skewed 
towards  sentence  of smaller  length,  as  would  be  expected  in 
a  test  for  exact  matches. 
It  should  be  noted,  however,  that 
surface  realizers  are  rarely  called  upon  to  generate  sentences 

NATURAL LANGUAGE 

813 

Table 3:  Sentence coverage/accuracy for the unseen WSJ23 sentences grouped by word length 

with  the extended lengths and complexities found in  highly 
educated newspaper text.  Finally, the "valid FD" column in(cid:173)
dicates that the transformation program is very good at pro(cid:173)
ducing valid  functional descriptions,  even  if they eventually 
are discarded by the grammar as being erroneous. 

The test set had a slightly higher coverage than the train(cid:173)
ing set shown  above, as  well as a higher number of perfect 
matches and a better score using the NIST SSA measure. Al(cid:173)
though we  trained on  other sections  of the  Penn  TreeBank, 
time constraints due to the  large amount of time required to 
generate all test sentences prevented us from having a fuller 
comparison set.  Additionally,  Section  24 was  the  first  sec(cid:173)
tion  we  trained  on,  and  it  is  likely  that  later  sections  were 
more  similar  to  Section  23,  or  that  the  amount  of domain-
specific stock market constructions were imbalanced. Finally, 
the "close matches" column shows how many candidate sen(cid:173)
tences might be nearly exactly matching, and the sum of these 
two is reflected in the final category "combined." 

One interesting observation is that this evaluation (and cor(cid:173)
respondingly,  the  evaluation  of  HALOGEN)  is  not  only  an 
evaluation of the underlying surface realizer,  but also of the 
accompanying transformation program that converts the Penn 
TreeBank notation into the specifications it expects.  We thus 
set out to perform a minor, secondary evaluation to determine 
if it were possible to find a baseline metric for how many sen(cid:173)
tences could still be generated by the surface realizer even if 
the corresponding FDs could not be produced for them by the 
transformation program. 

We thus randomly selected 25 sentences from each of the 
two sections which were not either perfect matches or "close" 

matches,  i.e.,  they  were  not in  the  "combined" match  cate(cid:173)
gory.  While these sets  included some of the problems listed 
in Section 3, 41  of the 50 were capable of being rendered by 
hand as FDs which produced exact matches without changes 
to  the  grammar,  6  required  minor changes  to  the  grammar 
which  were  quickly  performed,  and  the  remaining  3  sen(cid:173)
tences required major grammar changes which have still  not 
yet been made. The latter were sentences that still do not have 
satisfactory linguistic analyses in the linguistic literature. We 
thus conclude that with better transformation rules, we could 
then obtain close to 95% coverage. 

5  Discussion 
Surface realization is probably the most understood and com(cid:173)
petent task in NLG today. There is a high possibility that sur(cid:173)
face realization can already be considered a solved problem, 
except with regard to problems introduced by new languages 
or highly  specialized  domains.  However,  there  are  two  re(cid:173)
lated unsolved problems inherent in the process described in 
this paper. 

5.1  Automatic Evaluation  of Output 
Evaluation of NLG systems face the same problems as those 
that  confront  Machine  Translation  systems:  Given  a  set 
of generated  sentences,  how  do  you  tell  how  "good"  they 
are  in  general,  and  how  often  you  can  produce  good  sen(cid:173)
tences  in  a  given  context or  application.  Work  in  machine 
translation  has  shifted  to  large-scale  evaluations  which  re(cid:173)
quire automatic evaluation techniques [Papineni et al., 2001; 

814 

NATURAL LANGUAGE 

Doddington,  2002]  because  human  graders  cannot  hope  to 
examine all of the responses in a short enough period of time. 
Yet  current  evaluation  techniques  are  completely  nu(cid:173)
meric/statistical in nature and do not attempt to measure se(cid:173)
mantic content (such as the well-known example of a missing 
"not"  in  a  system's  output).  Furthermore,  these  techniques 
are ill-equipped to evaluate the types of sentences produced 
by symbolic NLG systems.  For example, by changing a fea(cid:173)
ture specifying that, say, a particularly lengthy purpose clause 
should go at the beginning rather than the end of a sentence, a 
string edit distance metric will report a very large error when 
in terms of the system's input, only one "error" has occurred. 
As  an  example  of  this  type  of  problem,  consider  that 
the  string edit distance  between  the  following  original  sen(cid:173)
tence and that produced by the  transformation program and 
FUF/SURGE  with  input  from  the  Penn  TreeBank  would  be 
83, resulting in a N1ST SSA 50.4% match: 

Freddie  Mac  said  the  principal-only  securities 
were priced at 58  1/4 to yield 8.45%, assuming an 
average life  of eight years  and  a  prepayment of 
160%  of the PSA model. 

Freddie  Mac  said  assuming  an  average  life 
of  eight  years  and  a  prepayment  of  160%  of 
the PSA model, the principal-only securities were 
priced at 58  1/4 to yield 8.45%. 

Obviously 50.4%  is  a  poor score  for such  sentences,  but 
many such examples were found in our corpus and their low 
scores  were  factored  into  the NIST Simple String Accuracy 
ratios in Tables 2 and 3. 

5.2  Symbolic vs.  Statistical  Approaches 
Almost all  fully-developed NLG  systems to-date operate on 
data  specified  in  a  knowledge  base  from  some  other  sys(cid:173)
tem.  The  fact  that  this  data  has  typically  been  represented 
as  highly-structured data  has  been  an  impediment to  tradi(cid:173)
tional machine learning techniques which have previously op(cid:173)
erated mostly on fiat, unstructured text data.  It is also gener(cid:173)
ally stated that statistical methods are more robust than their 
symbolic  counterparts and  more  easily  adapted to new data 
sets.  The data in the previous section seems to indicate that 
HALOGEN, a statistical system, performs substantially better 
on longer sentences, even if it has lower overall coverage. But 
there are several advantages in favor of symbolic techniques. 
First, the transformation program presented above can be 
tweaked  to  an  arbitrary  level  of perfection by  progressively 
adding  more  rules.  Most  statistical  and  machine  learning 
systems however have eventually reached a boundary where 
progress  becomes  seemingly  exponentially  more  difficult. 
Second, errors which are encountered during processing can 
be examined and  fixed  because the grammars and other re(cid:173)
sources are logically and semantically connected to the lan(cid:173)
guage being generated, rather than being a set of numbers.  If 
however a statistical generator must create new output forms 
not  contained  in  the  initial  corpus  or  model,  it  must  be  re(cid:173)
trained from scratch. 

Finally,  symbolic  surface realizers  allow  a  wide  range of 
optional operations which statistical programs currently can't 

offer, for example, the capability of adding formatting state(cid:173)
ments  in  HTML,  modifying punctuation,  generating dialect 
differences, adding prosody for TTS, etc.  While this may be 
due to  the  multiple decades of history over which  symbolic-
systems have been developed, it may also be due to the lack of 
annotated corpora that support statistical algorithms, or even 
potentially the impossibility of having a corpus at all, such as 
in large narratives [Callaway and Lester, 2002]. 

Additionally, there are  some disadvantages to the current 
approaches undertaken  in  statistical  NLG research.  For in(cid:173)
stance,  symbolic NL generation systems are already consid(cid:173)
ered slow, and FUF/SURGE is generally considered to be the 
slowest  in  the NLG community.  And yet the data from Ta(cid:173)
ble 3 shows that HALOGEN is anywhere from 6.5 to 16 times 
slower, and thus a 10-sentencc paragraph might need 4 min(cid:173)
utes or longer to be generated.  Moreover, these approaches 
use techniques such as n-gram models, where n must be in(cid:173)
creased to improve quality, but results in even slower genera(cid:173)
tion times and exponentially larger storage space. 

5.3  Potential  Applications 
The  transformation  program  presented  here  has  additional 
side  benefits  besides  helping  calculate  the  coverage  of  a 
grammar.  For example,  in  generation  systems  where  non-
linguists  must  maintain  old  data  and  add  new  data,  such 
a  program  allows  them  to  write  sample  sentences  in  the 
syntax-only  TreeBank  notation,  which  is  much  easier  for 
non-linguists, and then convert those sentences directly into 
a  more  linguistically-manageable  form  for generation  (e.g., 
functional descriptions).  Graphical editing tools for linguistic 
data such as G A TE [Bontcheva et a/., 2002] or similar author(cid:173)
ing tools could quicken the process even more. 

Additionally,  the transformation program can be used as 
an  error-checker  for  the  well-formedness  of sentences  con(cid:173)
tained in the TreeBank. Rules could be (and have been) added 
alongside the normal transformation rules that detect when er(cid:173)
rors are encountered, categorize them, and make them avail(cid:173)
able  to  the  corpus  creator  for correction.  This  extends  not 
only to annotation errors by the corpus creator detectable at 
the syntax tree level, but even morphology errors such as in-
correct verbs, typos, or British/American English differences 
by the original author of the text. Both of these tasks are much 
more difficult for a statistical system to accomplish, requiring 
separate retraining in the first case and locating or creating a 
corpus of possible mistakes in the second, much like what is 
done with tutoring systems where databases of potential stu(cid:173)
dent errors are painstakingly constructed. 

6  Conclusions and Future Work 
Recent  years  have  seen  the  arrival  of statistical  approaches 
to  the  field  of Natural  Language  Generation,  much  as  was 
seen in parsing a decade ago.  Of the many possible compo(cid:173)
nents in the standard NLG pipelined architecture, almost all 
of these statistical systems have focused on the surface real(cid:173)
ization component, offering the same robustness, wide cover(cid:173)
age, and domain- and language-independence as for parsing. 
Recent  experiments  with  one  statistically-based  system, 
HALOGEN,  showed that it could achieve respectable cover-

NATURAL  LANGUAGE 

815 

age  without  the  typical  development  costs  inherent  in  pro(cid:173)
ducing grammars for symbolic NLG. By taking as input sen(cid:173)
tences from the Penn TreeBank, H A L O G BN was able to gen(cid:173)
erate a substantial enough quantity of sentences to allow for 
an  empirical  analysis  of  grammatical  coverage  of  English. 
This  paper  represents  the  analogous  effort  for  a  symbolic 
generation system using the FUF/SURGH systemic realization 
system, which includes the largest generation grammar. 

We  presented the results of a grammatical coverage eval(cid:173)
uation  experiment  that  showed  the  symbolic  system  had  a 
higher level of coverage of English as represented by the Penn 
TreeBank.  We  also  contrasted  the  statistical  and  symbolic 
methodologies and concluded  with  ideas  for future applica(cid:173)
tions for easier creation of NLG systems and automatic error-
checking for large-scale corpora.  We also plan to further de(cid:173)
velop our Italian  generation  grammar developed  at  ITC-irst 
[Novello and Callaway, 2003J with a similarly annotated cor(cid:173)
pus of Italian newspaper text. 

7  Acknowledgements 
This work was funded by the PEACH project, granted by the 
Autonomous Province of Trento in Italy. 

References 
[Bangalore and Rambow, 2000]  Srinivas  Bangalore  and 
Owen  Rambow.  Exploiting  a  probabilistic  hierarchical 
In  COLING-2000:  Proceedings 
model  for  generation. 
of the  18th  International  Conference  on  Computational 
Linguistics, Saarbruecken, Germany, 2000. 

(multilingual) 

[Bateman,  1995J  John  A.  Bateman.  KPML:  The  KOMET-
environment. 
penman 
Technical  Report  Release  0.8, 
Institut  fiir  Integrierte 
Publikations-  und  Informationssysteme  (IPS1),  GMD, 
Darmstadt, 1995. 

development 

[Bontcheva^/a/., 20021  K.  Bontchcva,  H.  Cunningham, 
V. Tablan, D. Maynard, and H. Saggion.  Development of 
reusable  and  robust  language processing components  for 
information systems using gate. In Proceedings of the 3rd 
International  Workshop  on  Natural  language  and  Infor(cid:173)
mation Systems, 2002. 

[Callaway and Lester, 2002J  Charles  B.  Callaway  and 
James  C.  Lester.  Narrative  prose  generation.  Artificial 
Intelligence,  139(2):213-252,2002. 

[Doddington, 20021  George  Doddington.  Automatic  eval(cid:173)
uation  of  machine  translation  quality  using  n-gram  co(cid:173)
occurrence statistics.  In  Proceedings of the 2002  Confer(cid:173)
ence  on  Human  Language  Technology,  San  Diego,  CA, 
March 2002. 

[Doran,  1998]  Christine  Doran. 

Incorporating  Punctuation 
into the Sentence Grammar:  A  Lexicalized Tree Adjoining 
Grammar Perspective.  PhD thesis,  University of Pennsyl(cid:173)
vania, Philadelphia, PA, 1998. 

[Elhadad, 1991]  Michael Elhadad.  FUF: The universal uni(cid:173)
fier user manual version 5.0. Technical Report CUCS-038-
91,  Department of Computer  Science,  Columbia  Univer(cid:173)
sity, 1991. 

[Elhadad etal,  2001]  Michael Elhadad, Yacl Netzer, Regina 
Barzilay, and Kathleen McKeown.  Ordering circumstan(cid:173)
tials  for multi-document  summarization.  In  Proceedings 
of the Bar-1 Ian Symposium on the Foundations of Artifi(cid:173)
cial Intelligence, Jerusalem, Israel, June 2001. 

[Langkilde and Knight,  1998]  Irene  Langkilde  and  Kevin 
Knight.  Generation  that  exploits  corpus-based  statistical 
knowledge. In COLING-ACL-98: Proceedings of the Joint 
36th  Meeting  of the  Association  for  Computational  Lin(cid:173)
guistics  and  the  17th  International  Conference  on  Com(cid:173)
putational Linguistics,  pages  704-710,  Montreal,  Canada, 
August  1998. 

[Langkilde-Geary, 2002]  Irene Langkilde-Geary.  An empir(cid:173)
ical verification of coverage and correctness for a general-
purpose sentence  generator.  In  Second International Nat(cid:173)
ural Language Generation Conference, pages  17-24, Har-
riman, NY, July 2002. 

[Lavoie and Rambow,  1997]  Benoit Lavoie and Owen Ram(cid:173)
bow.  A  fast  and portable realizer for text generation sys(cid:173)
In  Proceedings  of the  5th  Conference  on  Applied 
tems. 
Natural Language Processing,  1997. 

[Marcus et ai,  19931  M.  Marcus,  B.  Santorini, 

and 
M.  Marcinkiewicz. 
Building  a  large  annotated  cor(cid:173)
pus  of  English:  The  PennTreeBank.  Computational 
Linguistics,  26(2),  1993. 

[Novello and Callaway, 2003]  Alessandra  Novello 

and 
Charles Callaway.  Porting to an italian surface realizer: A 
case study. In Proceedings of the 9th European Workshop 
on NLG, Budapest, Hungary, April 2003. 

[Papineni et ai, 2001]  K. Papineni, S. Roukos, T. Ward, and 
W. J. Zhu.  BLEU:  A  method for automatic evaluation of 
mt.  Technical  Report RC22176, IBM Research Division, 
T. J. Watson Research Center, New York, September 2001. 
[Pollard and Sag,  1994]  C. Pollard and 1. Sag.  Head-Driven 
Phrase  Structure  Grammar.  The  University  of Chicago 
Press, Chicago, 1994. 

[Ratnaparkhi, 2000]  Adwait  Ratnaparkhi.  Trainable  meth(cid:173)
ods for surface natural  language generation.  In Proceed(cid:173)
ings  of the  First  North  American  Conference  of the ACL, 
Seattle, WA, May 2000. 

[Reiter,  1994]  Ehud Reiter.  Has a consensus NL generation 
architecture appeared,  and is it psycholinguistically plau(cid:173)
sible?  In  Proceedings of the Seventh International  Work(cid:173)
shop  on  Natural  Language  Generation,  pages  163-170, 
Kennebunkport, ME,  1994. 

[Robin and McKeown,  1995]  Jacques  Robin  and  Kathy 
McKeown.  Empirically designing  and  evaluating  a  new 
revision-based  model  for  summary  generation.  Artificial 
Intelligence,  85(1-2),  1995. 

[White and Caldwell,  1998]  Michael  White  and  Ted  Cald(cid:173)
well.  EXEMPLARS:  A  practical,  extensible  framework 
for dynamic  text  generation.  In  Proceedings of the Ninth 
International Workshop on NLG, pages 266-275, Niagara-
on-the-Lake, Ontario, August 1998. 

816 

NATURAL  LANGUAGE 


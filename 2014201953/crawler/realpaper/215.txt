Shallow Semantics for Relation Extraction

Sanda Harabagiu, Cosmin Adrian Bejan and Paul Mor˘arescu

Human Language Technology Research Institute

University of Texas at Dallas

Richardson, TX 75083-0688, USA

sanda,ady,paul@hlt.utdallas.edu

Abstract

This paper presents a new method for extracting
meaningful relations from unstructured natural lan-
guage sources. The method is based on informa-
tion made available by shallow semantic parsers.
Semantic information was used (1) to enhance a
dependency tree kernel; and (2) to build semantic
dependency structures used for enhanced relation
extraction for several semantic classiﬁers.
In our
experiments the quality of the extracted relations
surpassed the results of kernel-based models em-
ploying only semantic class information.

1 Introduction
With the advent of the Internet, more and more information
is available electronically. Most of the time, information on
the Internet is unstructured, generated in textual form. One
way of automatically identifying information of interest from
the vast Internet resources is by recognizing relevant entities
and meaningful relations they share. Examples of entities are
PERSONS, ORGANIZATIONS and LOCATIONS. Examples of
relations are ROLE, NEAR and SOCIAL.

In the 90s,

the Message Understanding Conferences
(MUCs) and the TIPSTER programs gave great impetus to
research in information extraction (IE). The systems that par-
ticipated in the MUCs have been quite successful at recog-
nizing relevant entities, reaching near-human precision with
over 90% accuracy. More recently, the Automatic Content
Extraction (ACE) program focused on identifying not only
relevant entities, but also meaningful relations between them.
If success in recognizing entities with high precision was at-
tributed to the usage of ﬁnite-state transducers, in the last
three years the dominant successful technique for extracting
relations was based on kernel methods.

Kernel methods are non-parametric density estimation
techniques that compute a kernel function to measure the sim-
ilarity between data instances. [8] introduced the formaliza-
tion of relation extraction in terms of tree kernels, which are
kernels that take advantage of syntactic trees. [1] extended the
work by using dependency trees that describe the grammati-
cal relations between the words of a sentence. Furthermore,
each word of a dependency tree is augmented with lexico-
semantic information, including semantic information avail-

able from the WordNet database [2]. The average precision
of relation extraction using dependency trees was reported in
[1] to be 67.5%. Since meaningful relations between relevant
entities are of semantic nature, we argue that additional se-
mantic resources should be used for extracting relations from
texts. In this work, we were interested in investigating the
contribution of two shallow semantic parsing techniques to
the quality of relation extraction.

We explored two main resources:

PropBank and
FrameNet. Proposition Bank or PropBank is a one mil-
lion word corpus annotated with predicate-argument struc-
tures. The corpus consists of the Penn Treebank 2 Wall
Street Journal texts (www.cis.upenn.edu/(cid:24)treebank). The
PropBank annotations were performed at University of
Pennsylvania (www.cis.upenn.edu/(cid:24)ace).
To date Prop-
Bank has addressed only predicates lexicalized by verbs,
proceeding from the most
to the least common verbs
while annotating verb predicates in the corpus.
The
FrameNet project (www.icsi.berkeley.edu/(cid:24)framenet) pro-
duced a lexico-semantic resource encoding a set of frames,
which represent schematic representations of situations char-
acterized by a set of target words, or lexicalized predicates,
which can be verbs, nouns or adjectives. In each frame, vari-
ous participants and conceptual roles are related by case-roles
or theta-roles which are called frame elements or FEs. FEs
are local to each frame, some are quite general while oth-
ers are speciﬁc to a small family of lexical items. FrameNet
annotations were performed on a corpus of over three mil-
lion words. Recently, semantic parsers using PropBank and
FrameNet have started to become available. In each sentence,
verbal or nominal predicates are discovered in relation to their
arguments or FEs. Our investigation shows that predicate-
arguments structures and semantic frames discovered by shal-
low semantic parsers play an important role in discovering
extraction relations. This is due to the fact that arguments
of extracted relations belong to arguments of predicates or to
FEs.

To investigate the role of semantic information for relation
extraction we have used two shallow semantic parsers, one
trained on PropBank and one on FrameNet. We used the
semantic information identiﬁed by the parsers in two ways.
First it was used to enhance the features of dependency ker-
nels. Second, it was used to generate a new representation,
called semantic dependency structure. The results of the ex-

periments indicate that not only the precision of kernel meth-
ods is improved, but also relation extraction based on shallow
semantics outperforms kernel-based methods.

The remainder of this paper is organized as follows. Sec-
tion 2 describes the semantic parsers. Section 3 details the
kernel methods and their enhancement with semantic infor-
mation. In Section 4 we show how relation extraction based
on semantic information is produced. Section 5 details the
experimental results and Section 6 summarizes our conclu-
sions.

2 Shallow Semantic Parsing
Shallow semantic information represented by predicates and
their arguments, or frames and their FEs, can be identiﬁed
in text sentences by semantic parsers. The idea of automat-
ically identifying and labeling shallow semantic information
was pioneered by [3]. Semantic parsers operate on the output
of a syntactic parser. When using the PropBank information,
the semantic parser (1) identiﬁes each verbal predicate and (2)
labels its arguments. The expected arguments of a predicate
are numbered sequentially from Arg0 to Arg5. Additionally,
the arguments may include functional tags from Treebank,
e.g. ArgM-DIR indicates a directional, ArgM-LOC indicates
a locative and ArgM-TMP stands for a temporal. When us-
ing the FrameNet information, the semantic parser (1) iden-
tiﬁes the target words; (2) disambiguates the semantic frame
for each target word; and (3) labels the FEs that relate to the
target word based on the frame deﬁnition. For example, in
FrameNet the EMPLOYMENT frame has the FEs: Employee,
Employer, Purpose, Compensation, Manner, Place, Position,
Time, Field, Duration and Task.

 

e
e
r
T
e
s
r
a
P
 
c
i
t
c
a

t

n
y
S

S

NP

VP

VP

NP

PP

NP

The Chippewas

hired

Eckstein

as

a lobbyist 

Semantic
Parser
(PropBank)

Semantic
Parser
(FrameNet)

Task 1:

[The Chippewas]

hired

[Eckstein]

[as a lobbyist]

Task 2:

Arg0

PREDICATE

Arg1

ArgM

Task 1:

[The Chippewas]

hired

[Eckstein]

[as a lobbyist]

Task 2:

Employer

TARGET

Employee

Position

Figure 1: Sentence with labeled semantic roles.

Figure 1 illustrates the output of both semantic parsers
when processing a sentence. The parsing consists of two
tasks: (1) identifying the parse tree constituents correspond-
ing to arguments of each predicate or FEs of each frame; and
(2) recognizing the role corresponding to each argument or
FE. Each task is cast as a separate classiﬁer. For example,
task 1 identiﬁes the two NPs and the PP as arguments and
FEs respectively. The second classiﬁer assigns the speciﬁc
roles: Arg0, Arg1 and ArgM given the predicate “hired” and
FEs Employer, Employee and Position given the target word
“hired”. In the case of semantic parsing parsed on FrameNet,
a third task of ﬁnding the frame corresponding to the sentence
is cast as a third classiﬁcation problem.

− PHRASE TYPE (pt): This feature indicates the syntactic
type of the phrase labeled as a frame element, e.g.
NP for Employer in Figure 1.
− PARSE TREE PATH (path): This feature contains the path
in the parse tree between the predicate phrase and the 
target word, expressed as a sequence of nonterminal
labels linked by direction symbols (up or down), e.g.
NP    S    VP    VP for Employer in Figure 1.
− POSITION (pos) − Indicates if the constituent appears  
before or after the predicate in the sentence.
− VOICE (voice) − This feature distinguishes between
active or passive voice for the predicate phrase.

− HEAD WORD (hw) − This feature contains the head word
of the evaluated phrase. Case and morphological information
are preserved.
− GOVERNING CATEGORY (gov) − This feature applies to
noun phrases only, and it indicates if the NP is dominated
by a sentence phrase (typical for subject arguments with
active−voice predicates), or by a verb phrase (typical 
for object arguments).
− TARGET WORD − In our implementation this feature
consists of two components: (1) WORD: the word itself with
the case and morphological information preserved; and 
(2) LEMMA which represents the target normalized to lower 
case and infinitive form for the verbs or singular for nouns. 

Figure 2: Feature Set 1 (FS1).

− CONTENT WORD (cw) − Lexicalized feature that selects an informative 
word from the constituent, different from the head word.

PART OF SPEECH OF HEAD WORD (hPos) − The part of speech tag of
the head word.

PART OF SPEECH OF CONTENT WORD (cPos) −The part of speech 
tag of the content word.

NAMED ENTITY CLASS OF CONTENT WORD (cNE) − The class of 
the named entity that includes the content word

BOOLEAN NAMED ENTITY FLAGS − A feature set comprising: 
− neOrganization: set to 1 if an organization is recognized in the phrase
− neLocation: set to 1 a location is recognized in the phrase
− nePerson: set to 1 if a person name is recognized in the phrase
− neMoney: set to 1 if a currency expression is recognized in the phrase
− nePercent: set to 1 if a percentage expression is recognized in the phrase
− neTime: set to 1 if a time of day expression is recognized in the phrase
− neDate: set to 1 if a date temporal expression is recognized in the phrase 

Figure 3: Feature Set 2 (FS2).

Several classiﬁers were previously tried for this problem:
decision trees [7], and Support Vector Machines (SVMs) [6].
Based on the results of the CoNLL and Senseval-3 evalua-
tions1, we selected an implementation based on SVMs, us-
ing the SVMlight package 2. For the semantic parser based
on PropBank, we combined feature sets: FS1 (illustrated in
Figure 2) introduced in [3], FS2 (illustrated in Figure 3) in-
troduced in [7], FS3 (illustrated in Figure 4) introduced in
[6]. For the semantic parser based on FrameNet we have also
added the feature set FS4 (illustrated in Figure 5). The se-

1The Conference on Natural Language Learning (CoNLL)
(http://cnts.uia.ac.be/signll/shared.html) has focused in 2004 on the
problem of semantic role labeling, using PropBank data. Senseval-
3 (www.senseval.org/senseval3) had a semantic role labeling task,
using FrameNet data.

2http://svmlight.joachims.org/

mantic parser using FrameNet also required the disambigua-
tion of the frame. To disambiguate the frame, we used the
BayesNet algorithm implemented in the Weka learning pack-
age 3. The features that were used are (1) the target word; (2)
the part-of-speech of the target word; (3) the phrase type of all
the FEs (e.g. NP, VP, PP); and (4) the grammatical function.
The grammatical function is deﬁned in Figure 4. To learn
the grammatical function, we again used the SVM with the
features FS1, FS2, the features Human and Target-Type
from FS3 and only the feature PP-PREP from FS4.

HUMAN: This feature indicates whether the syntactic phrase is either
(1) a personal pronoun or
(2) a hyponym of sense 1 of PERSON in WordNet

SUPPORT_VERBS that are recognized for adjective or noun target words
have the role of predicate for the FEs. For example, if the target = clever,
in the sentence "Smith is very clever, but he’s no Einstein", the 
FE = Smith is an argument of the support verb ’is’ rather than of the
target word. The values of this feature are either (1) The POS of the head
of the VP containing the target word or (2) NULL if the target word does
not belong to a VP

TARGET−TYPE: the lexical class of the target word, e.g. VERB, NOUN
or ADJECTIVE

LIST_CONSTITUENT (FEs): This feature represents a list of the syntactic
consituents covering each FE of the frame recognized in a sentence. 
For the example illustrated in Figure 1, the list is: [NP, NP, PP]

Grammatical Function: This feature indicates whether the FE is:
− an External Argument (Ext)
− an Object (Obj)
− a Complement (Comp)
− a Modifier (Mod)
− Head noun modified by attributive adjective (Head)
− Genitive determiner (Gen)
− Appositive (Appos)

LIST_Grammatical_Function: This feature represents a list of the 
grammatical functions of the FEs recognized in the sentence.

NUMBER_FEs: This feature indicates how many FEs were recognized 
in each sentence.

FRAME_NAME: This feature indicates the name of the semantic frame 
for which FEs are labeled

COVERAGE: This feature indicates whether there is a syntactic structure
in the parse tree that perfectly covers the FE

CORE: This feature indicates whether the FE is one that instantiates
a conceptually necessary participant of a frame. For example, in the 
REVENGE frame, Punishment is a core element. The values of this feature
are: (1) core; (2) peripheral and (3) extrathemathic. FEs that mark notions
such as Time, Place, Manner and Degree are peripheral. Extratematic
FEs situate an event against a backdrop of another event, by evoking 
a larger frame for which the target event fills a role.

SUB_CORPUS: In FrameNet, sentences are annotated with the name
of the subcorpus they belong to. For example, for a verb target word,
V−swh represents a subcorpus in which the trget word is a predicate
to a FE included in a relative clause headed by a wh−word.

Figure 4: Feature Set 3 (FS3).

3 Dependency Tree Kernels
In [1] the relation extraction problem was cast as a classiﬁ-
cation problem based on kernels that operate on dependency
trees. Kernels measure the similarity between two instances
If X is the instance space, a kernel function
of a relation.
is a mapping K:X(cid:2)X![0; 1) such that given two instances

3http://www.cs.waikato.ac.nz/(cid:24)ml

PARSE TREE PATH WITH UNIQUE DELIMITER −  This feature removes
the direction in the path, e.g. VBN−VP−ADVP

PARTIAL PATH − This feature uses only the path from the constituent to
the lowest common ancestor of the predicate and the constituent

FIRST WORD − First word covered by constituent
FIRST POS − POS of first word covered by constituent

LAST WORD − Last word covered by the constituent
LAST POS − POS of last word covered by the constituent

LEFT CONSTITUENT − left sibling constituent label

LEFT HEAD − Left sibling head word

LEFT POS HEAD − Left sibling POS of head word

RIGHT CONSTITUENT − right sibling constituent label

RIGHT HEAD − Right sibling head word

RIGHT POS HEAD − Right sibling POS of head word

PP PREP − If constituent is labeled PP get first word in PP

DISTANCE − Distance in the tree from constituent to the target word

Figure 5: Feature Set 4 (FS4).

x and y, K(x; y) = Pi (cid:30)i(x)(cid:30)i(y) = (cid:30)(x) (cid:1) (cid:30)(y), where

(cid:30)i(x) is some feature function over the instance x. The in-
stances can be represented in several ways. First, each sen-
tence where a relation of interest occurs can be viewed as a
list of words. Thus, the similarity between two instances rep-
resented in this way is computed as the number of common
words between the two instance sentences. All words from
instances x and y are indexed and (cid:30)i(x) is the number of
times instance x contains the word referenced by i. Such a
kernel is known as bag-of-words kernel. When sentences are
represented as strings of words, string kernels, count the num-
ber of common subsequences in the two strings and weight
their matches by their length. Thus (cid:30)i(x) is the number of
times string x contains the subsequence referenced by i. If
the instances are represented by syntactic trees, more com-
plex kernels are needed. A class of kernels, called convolu-
tion kernels, was proposed to handle such instance represen-
tations. Convolution kernels measure the similarity between
two structured instances by summing the similarity of their
substructures. Thus, given all possible substructures in in-
stances x and y, (cid:30)i(x) counts not only the number of times
the substructure referenced by i is matched into x, but also
how many times it is matched into any of its substructures.

Given a training set T =fx1; : : : xN g, kernel methods com-
pute the Gram matrix G such that Gij=K(xi; xj). G enables
a classiﬁer to ﬁnd a hyperplane which separates instances of
different classes. G enables classiﬁers to ﬁnd a separating hy-
perplane that separates positive and negative examples. When
a new instance y needs to be classiﬁed, y is projected into the
feature space deﬁned by the kernel function. Classiﬁcation
consists of determining on which side of the separating hy-
perplane y lies. Support Vector Machines(SVMs) formulate
the task of ﬁnding the separating hyperplane as a solution to a
quadratic programming problem.Therefore, following the so-
lution proposed by [1], they are used for classifying relation
instances in texts.

To measure the similarity between two instances of the

S

NP

NNP

Masachusetts

NN

NN
state trooper

VBZ
is

VP

VP

VBG

NP

PP

demanding

DT
an

NN

IN
apology from

(d)

demanding

trooper

apology

attorneys
REL−A
Arg0
Employee

NP

CD
one

IN
of

PP

NP

NPB

SBAR

represented

Predicate/Target

yesterday

Woodward
REL−B
Arg1
Employer

(a)

DT
A

(b)

S

(demanding)

NP

(trooper)

NNP

DT
A Masachusetts

NN

NN
state trooper

VBZ
is

VP

(demanding)

VP

(demanding)

VBG

NP

(apology)

PP

(attorneys)

DT

NN

WHNP

S

the attorneys

demanding

DT
an

NN

apology

IN
from

(c)

demanding

trooper

apology

attorneys

NP

(attorneys)

CD
one

IN
of

PP

(attorneys)

NP

(attorneys)

NPB

(attorneys)

SBAR

(represented)

WP

who

IN

until

PP

VP

NP

VBD

represented

NPB

yesterday

NP

FW

au

NPB

British

NN

NNP

NNP

pair

Louise Woodward

represented

DT

NN

WHNP

S

(represented)

yesterday

Woodward

the attorneys

WP

who

IN

until

PP

(yesterday)

VP

(represented)

NP

VBD

NP

(Woodward)

represented

NPB

yesterday

NPB

British

FW

au

NN

NNP

NNP

pair

Louise Woodward

Figure 6: (a) Syntactic parse; (b) Head word propagation; (c) Dependency tree; (d) Semantic dependency tree.

same extraction relation both [1] and [8] relied on kernels
that operate on trees expressing syntactic information. To
build such a tree we have used the Collins syntactic parser.
When using this parser, for each constituent in the parse tree,
we also have access to a dependency model that enables the
mapping of parse trees into sets of binary relations between
the head-word of each component and its sibling words. For
example, Figure 6(a) describes the parse tree of a sentence.
For each possible constituent in the parse tree, rules ﬁrst de-
scribed in [4] identify the head-child and propagate the head
word to the parent. Figure 6(b) illustrates the propagation
for the parse tree illustrated in Figure 6(a). When the prop-
agation is over, head-modiﬁer relations are extracted, gener-
ating a dependency structure. Figure 6(c) illustrates the de-
pendency structure of the sentence that was analyzed syntac-
tically in Figure 6(a). The nodes of this dependency structure
were augmented with features, to enable the calculation of
the kernel. Figure 7 lists the features that were assigned to
each node in the dependency tree. Two sets of features were
used: F1, the features proposed in [1] and F2, a new set of
features that we have added. Feature set F1 contains: (1) the
word; (2) the part-of-speech (POS) (24 values); (3) a gener-
alized POS (5 values); (4) the tag of the nonterminal from the
parse tree having the feature word as a head; (5) the entity
type, as deﬁned by the ACE guidelines; (6) the entity level
(e.g. name); (7) the relation argument (e.g. REL-A); and (8)
a WordNet hypernym. The new set of features F2 contains:
(1) the predicate argument number provided by the semantic
parser when using PropBank; (2) the predicate target for that
argument; (3) the FE provided by the semantic parser when

using FrameNet; (4) the target word for the frame; (5) the
grammatical function; (6) the Frame; (7) the WordNet do-
main; (8) the WordNet concept that expressed the type of re-
lation in which the word may belong within the domain; (9) a
set of other related WordNet concepts (e.g. direct hypernym);
(10) the ProbBank concepts that most frequently occur as ar-
guments for the predicate; (11) the FrameNet concepts that
most frequently occur in FEs for the frame. The features and
their values for the node “attorneys” are listed in Figure 7.

Feature Set F1
1 Word
2 Part−of−speech
3 General−POS
4 Syntactic chunk tag
5 Entity−type
6 Entity−level
7 Relation−argument
8 WordNet hypernym

Example
attorneys
NN
noun
NP
PERSON
nominal
REL−A
Individual

Feature Set F2
 1 Predicate−argument number
 2 Predicate Target
 3 FE
 4 Target word
 5 Gramatical function
 6 Frame
 7 WordNet Domain
 8 WordNet Relation Concept
 9 WordNet Semantic Concepts
10 PropBank(WN) Concepts
11 FrameNet(WN) Concepts

Example

Arg0
represent
Employee
represent
Ext
Employment
jurisprudence
lawyer−client
professional,paralegal
banker, lawyer, share
relation, men

Figure 7: Sets of features assigned to each node in the dependency
tree.

The features are used by a tree kernel function K(T1; T2)
that returns a similarity score in the range (0; 1). We pre-
ferred the more general version of the kernel introduced in
[1] to the kernel described by [8]. This kernel is based
on two functions deﬁned on the features of tree nodes: a
matching function m(ti; tj) 2 f0; 1g and a similarity func-
tion s(ti; tj) 2 (0; 1). The feature vector of a tree node
(cid:30)(ti) = fv1; : : : vdg consists of two possibly overlapping
subsets (cid:30)m(ti) (cid:18) (cid:30)(ti) and (cid:30)s(ti) (cid:18) (cid:30)(ti). As in [1], (cid:30)m(ti)

are used by the matching function and (cid:30)s(ti) are used by the
similarity function. The two functions are deﬁned by:

and

if (cid:30)m(ti) = (cid:30)m(tj)
otherwise

0

m(ti; tj) = (cid:26) 1
s(ti; tj) = Xvq 2(cid:30)s(ti) Xvr 2(cid:30)s(tj )

C(vq; vr)

where C(vq; vr) is some compatibility function between the
two feature values. For example, in the simplest case where

C(vq; vr) = (cid:26) 1

0

if vq = vr
otherwise

s(ti; tj) returns the number of feature values in common be-
tween the feature vectors (cid:30)s(ti) and (cid:30)s(tj). For two depen-
dency trees T1 and T2 with root nodes r1 and r2 the tree ker-
nel is deﬁned as:

K(T1; T2) = (cid:26) 0

s(r1; r2) + Kc(r1[c]; r2[c])

if m(r1; r2)=0
otherwise

where Kc is a kernel function over the children of the nodes
r1 and r2. Let a and b be sequences of indices such that a
is a sequence a1 (cid:20) a2 (cid:20) : : : (cid:20) an and likewise for b. Let
d(a) = an (cid:0) a1 + 1 and l(a) be the length of a. Then for
every ti 2 T1 and tj 2 T2, we have
Kc(ti[c]; tj[c]) = Xa;b;l(a)=l(b)

(cid:21)d(a)(cid:21)d(b)K(ti[a]; tj[b])

where (cid:21)2(0; 1) represents a decay factor that penalizes
matching subsequences that are spread out within the child
sequences. The deﬁnition of Kc, the kernel function over
children, assumes that the matching function used in the def-
inition of the tree kernel K(ti[a]; tj[b]) operates not only on
single nodes, but also on node sequences ti[a] or tj[b]. If all
the nodes in the sequence are matched, m(ti[a]; tj[b]) = 1.
For each matching pair of nodes (ai; bj) is a matching sub-
sequence, we accumulate the result of the similarity func-
tion s(ai; bj) and then recursively search for matching sub-
sequences of their children.

As in [1], we implemented two types of tree kernels: a con-
tiguous kernel and a sparse kernel. A contiguous kernel only
matches children subsequences that are uninterrupted by non-
matching nodes. Therefore d(a) = l(a). A sparse tree ker-
nel, by contrast, allows non-matching nodes within matching
subsequences.

4 Relation Extraction
When analyzing the dependency kernels, we noticed that
only few nodes bear semantic information derived by the se-
mantic parsers. We also noticed that these nodes are clus-
tered together in the dependency tree. For example, Fig-
ure 6(d) illustrates the cluster of nodes from the dependency
tree that contains semantic information.
Instead of using
the entire dependency tree to compute similarities, we se-
lected sub-trees that contain nodes having values for the fea-
tures from set F2 (illustrated in Figure 7). Typically these
nodes correspond to target predicates and their arguments
or FEs. This allowed us to compare trees of the form

m[(cid:30)2

m=fF Eg and (cid:30)3

S(r(T1); r(T2)) + Sp(T1; T2)

K(T1; T2) = (cid:26) 0

if m(c(T1); c(T2))=0
otherwise
where the matching function is performed only on
the children.
The matching features that were used
m=fGeneral-P OS, Entity-T ype, Relation-
comprised (cid:30)1
argumentg, (cid:30)2
m=fP redicate-argument
numberg. m(c(T1); c(T2))=1 if there is a combination of
the pair of arguments that has the same matching features.
For example, in the case of SDT (R1) and SDT (R2), the
combination is f(\attorneys”, \lawyer”) and (\intern”,
\W oodward”)g when using (cid:30)1
m, since both attorney
and lawyer are nouns, Persons, REL-A and they are both cov-
ered by the FE Employee in their respective frames. To mea-
sure the similarity between the verbal predicates, the func-
tion S(r(T1); r(T2)) measures the semantic compatibility of
the predicates. The features used for measuring similarity of
predicates are (cid:30)P
S =fF rame, P redicate T arget, W ordN et
Domaing. The compatibility measures assigned to each fea-
ture are: 1 if the same predicate target, 0.9 if both predi-
cates are targets of the same frame, 0.7 if both predicates
are targets of frames from the same event structure, 0.5 if
both predicates belong to the same WordNet domain and 0.3
if the predicates are not covered in FrameNet, but have the
same arguments in PropBank as other predicates from the
same frame. The predicate similarity brings forward semantic
frames that characterize a type of extraction relation. For ex-
ample, in the case of Role Client relation, such frames were
(a) Employment end and its subframes from the event struc-
ture; Employment Start; and (b) Commerce Sell or (c) Com-
merce Buy.

SDT (R1) [\attorneys”!\represented” \W oodward”]
and SDT (R2)[\intern”!\dismissed” \lawyer”]. We
called such trees semantic dependency trees since they are
characterized by semantic features present in the nodes of de-
pendency trees. Semantic dependency trees (SDTs) are bi-
nary trees containing three nodes: a verbal predicate that is
the root of the tree; and two children nodes, each an argu-
ment of the predicate. To measure the similarity of two SDTs
we built a very simple kernel:

The Sp(T1; T2) similarity focuses on the combination of
FEs or predicate-arguments that are identiﬁed for the children
of the SDTs. The similarity features that are used are fFE,
Predicate-argument number, Grammatical Function, Word-
Net Domain, WordNet Relation Conceptg. For example,
when the similarity Sp(SDT (R1); SDT (R2)) is computed,
since we have an fEmployer Employeeg relation between
the FEs in both SDTs, the conﬁdence assigned based on iden-
tical FE-FE relation is 1.
If we have identical Predicate-
argument numbers, the conﬁdence is 0.6. For identical Word-
Net domains, 0.4 and for the same WordNet relation concept,
we assign the conﬁdence 0.7.

One limitation of the SDTs stems from the fact that this for-
malism cannot capture extraction relations that are expressed
in the same noun phrase, e.g. “our customers”, “his urolo-
gist” or “George’s high school”. To recognize such relations
we consider that they relate to some arguments of “unspec-
iﬁed” predicates.
In the case when NPs contain pronouns,
they are resolved by a successful coreference resolution algo-
rithm [5], and the pronoun is substituted by a pair (referent,

CATEGORY). For example, the pronoun “our” from “our cus-
tomers” is replaced by (“IBM”, ORGANIZATION)

For the noun phrase “our customers”,

By measuring the semantic similarity between the pair
(NP head, Category) and any pair of arguments of a pred-
icate from the training corpus, a plausible predicate can
be found.
the
SDT [\shop” \billed”!\customers”] was deemed the
most relevant, assigning the predicate “billed” to the two
arguments: ORGANIZATION and “customers”.
The se-
mantic similarity was measured by enhancing the Word-
similarity measure is publicly available resource
Net,
(www.d.umn.edu/(cid:24)tpederse/Pubs/AAAI04PedersenT.pdf),
to
handle semantic classes identiﬁed for entities, e.g. ORGA-
NIZATION, DISEASE.

5 Experimental results
To evaluate the role of shallow semantics provided by se-
mantic parsers on relation extraction we have used the Auto-
matic Content Extraction (ACE) corpus available from LDC
(LDC2003T11). The data consists of 422 annotated text doc-
uments gathered from various newspapers and broadcasts.
Five entity types have been annotated (PERSON, ORGANI-
ZATION, GEO-POLITICAL ENTITY, LOCATION, FACILITY)
along with the 24 types of relations. The relations are listed
in Figure 8.

At_Located
Role_Staff
Role_Member
Role_Mgmt
Part_Part−of
At_Based−in

At_Residence
Near_Relative−loc
Role_Citizen
Soc_Professional
Part_Subsidiary
Soc_Parent

Role_Affiliate
Role_Client
Role_Owner
Role_Other
Soc_Relative
Soc_Personal

Role_Founder
Soc_Associate
Soc_Spouse
Soc_Sibling
Part_Other
Soc_Grandparent

Figure 8: Extraction relations evaluated in ACE.

We implemented the same ﬁve kernels as [1]: K0=sparse
kernel, K1=contiguous kernel, K2=bag-of-words kernel and
K3=K0+K2 and K4=K1+K2 and used ﬁrst only the feature
set F1 from Figure 7 and then both feature sets F1 and F2. The
comparison of the kernel performance of the two experiments
is listed in Figure 9.

Kernel

Avg. F−score

Avg. Precision

Features
F1
52.3
55.1
38.2
54.8
60.5

F1+F2
55.8
61.4
48.7
61.3
72.2

F1+F2
42.71
42.33
35.96
48.92
55.06
Figure 9: Kernel performance comparison.

F1+F2
34.6
32.3
28.5
40.7
44.5

Features
F1
28.62
29.78
12.12
27.66
30.4

Avg. Recall
Features
F1
19.7
20.4
7.2
18.5
20.3

K0
K
1
K
2
K
3
K
4

We used each kernel within an SVM (we augmented the
SVMlight implementation to include our kernels). We choose
to train on all 24 relations, not only on the ﬁrst 5-high level
relation types as was done in [1]. The results indicate that
on average, for K4, the best performing kernel, we obtained
an increase of 24.66% in the F-score when features provided
by the semantic parsers were added. When relying on SDTs,
the average Precision that was obtained was 89.3%, the re-
call was 76.4%, thus an F-score of 82.35%, when using the
same data as [1]. Figure 10 illustrates F-score obtained when
using several other machine learning techniques available in

the Weka package. In Figure 10 P indicates results for rela-
tions involving a predicate, NP indicates results for relations
within an NP and C indicates the combined results. The re-
sults show that frame semantics produce an enhancement of
53.24% over previous state-of-art results in relation extrac-
tion. Furthermore, they show that semantic representations
such as frames or predicate-argument structures have a wider
impact on classiﬁcation performance than the classiﬁcation
technique.

Bayes
Nets

Naive
Bayes

AdaBoost Bagging

Stacking

ID3

J48 Random
Forest

Random

Tree

SVM

Kernels
on SDTs

P

77.97 79.10

62.14

82.48

62.14

73.45

83.05

82.48

79.09

81.35

82.17

63.71

72.35

84.81 83.10

80.63

83.92

83.85

NP

C

79.32

82.52
78.43 80.24

63.10
62.46

85.22
83.39

79.60
Figure 10: Results of relation extraction.

72.71 83.64 82.69

62.66

82.21 82.73

In the ACE data 61.71% of the training/testing data could
be cast into SDTs. The semantic similarity between argu-
ments of a relation within the same NP and arguments present
in SDTs allowed the extraction with an average F1-score of
78.41%. The quality of the extraction results depend on the
quality of the semantic parsers, that obtained F-scores of over
90% in recent SENSEVAL evaluations.

6 Conclusions
In this paper we have introduced a new dependency structure
that relies on semantic information provided by shallow se-
mantic parsers. This structure enabled the extraction of rele-
vant relations with better performance than previous state-of-
the-art kernel methods. Furthermore, the semantic features
enabled similarly good results to be obtained with a few other
learning algorithms. We also used compatibility functions
that made use of semantic knowledge. This framework could
be extended to allow processing of idiomatic predicates, e.g.
[PERSON “lobbying on behalf of”” ORGANIZATION], and
combined predications.

References
[1] A. Culotta and J. Sorensen. Dependency tree kernels for relation

extraction. In Proceedings of the ACL-2004, 2004.

[2] C. Fellbaum. WordNet: An Electronic Lexical Database. MIT

Press., 1998.

[3] Daniel Gildea and Daniel Jurasky. Automatic labeling of se-
mantic roles. Computational Linguistic, 28(3):496–530, 2002.
[4] F. Jelineck, J. Lafferty, D. Magerman, R. Mercer, A. Ratna-
parkhi, and S. Roukos. Decision tree parsing using a hidden
derivational model. In Proceedings of the HLT Workshop-1994.
[5] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.
A Mention-Synchronous Coreference Resolution Algorithm
Based On the Bell Tree. In Proceedings of the ACL-2004, 2004.
[6] S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Martin, and
D. Jurafsky. Support Vector Learning for Semantic Argument
Classiﬁcation. Journal of Machine Learning Research, 2004.

[7] M. Surdeanu, S. M. Harabagiu, J. Williams, and J. Aarseth. Us-
ing Predicate-Argument Structures for Information Extraction.
In Proceedings of the ACL-2003, 2003.

[8] D. Zelenko, C. Aone, and A. Richardella. Kernel Methods for
Relation Extraction. In Proceedings of the EMNLP-2002, pages
71–78, 2002.


An Information-Theoretic Analysis of Memory Bounds in a Distributed Resource

Allocation Mechanism

Ricardo M. Araujo and Luis C. Lamb

Instituto de Inform´atica, Universidade Federal do Rio Grande do Sul

91501-970, Porto Alegre, Brazil

rmaraujo@inf.ufrgs.br, LuisLamb@acm.org

Abstract

Multiagent distributed resource allocation requires
that agents act on limited, localized information
with minimum communication overhead in order
to optimize the distribution of available resources.
When requirements and constraints are dynamic,
learning agents may be needed to allow for adap-
tation. One way of accomplishing learning is to
observe past outcomes, using such information to
improve future decisions. When limits in agents’
memory or observation capabilities are assumed,
one must decide on how large should the obser-
vation window be. We investigate how this de-
cision inﬂuences both agents’ and system’s per-
formance in the context of a special class of dis-
tributed resource allocation problems, namely dis-
persion games. We show by numerical experi-
ments over a speciﬁc dispersion game (the Minority
Game) that in such scenario an agent’s performance
is non-monotonically correlated with her memory
size when all other agents are kept unchanged. We
then provide an information-theoretic explanation
for the observed behaviors, showing that a down-
ward causation effect takes place.

Introduction

1
An important class of natural problems involving distributed
resource allocation mechanisms may be modeled by disper-
sion games [9]. Problems in this class are of anti-coordination
nature: agents prefer actions that are not taken by other
agents. Stock exchange and load balancing in networks are
instances of such problems [8]. In iterated versions of such
games learning is an important component of the decision-
making process. While each agent may try and learn individ-
ual behaviors of all agents, this is a complex task especially
when the total number of agents in the system is unknown,
not constant or very large. A possible solution is to consider
other agents’ actions as indistinguishable from the environ-
ment - an idea behind the so-called 0-level agents [17] which,
for some scenarios, yields the best results [10]. In such cases,
only the joint actions of all agents are considered and they
may be represented as a single global outcome. The computa-
tional cost of analyzing one single global outcome is expected

to be much smaller than that of analyzing multiple outcomes
produced by individual agents. More importantly, such a cost
does not depend on the number of agents, and its computation
may be feasible even without knowledge of such number.

In repeated dispersion games, agents may learn from a se-
ries of past outcomes in order to improve their decisions. A
natural issue is how large should the observation window be.
While bounds in agents’ memory or observation capabilities
are often assumed [14], there is a paucity of explanation about
how such limits are established. In this paper we tackle this
problem in a specialized version of a dispersion game known
as the Minority Game (MG) [4]. In the MG, a ﬁnite number of
agents decide over a (also ﬁnite) set of actions and those who
choose the action chosen by the smallest number of agents are
rewarded. When the game is repeated, agents have the chance
to learn from previous rounds and improve their performance
by considering a ﬁnite window of past outcomes (they have
ﬁnite memory) in order to try and learn temporal patterns.

Our contribution is twofold. First, we systematically simu-
late cases in the MG composed of agents with different mem-
ory sizes and compare their performances to a ﬁxed class of
(other) agents with ﬁxed memory sizes, using two different
learning algorithms. The results show a clear non-monotonic
relation between access to information and agent’s perfor-
mance, unveiling cases where more information translates
into higher gains and cases where it is harmful. Further, we
relate those ﬁndings to the system’s global efﬁciency state.
Second, we provide an information-theoretic explanation to
the observed behaviors. We show that the size of the gen-
erated outcome patterns does not always match the system’s
memory size, thus allowing agents with larger memory sizes
to exploit this information.

The remainder of the paper is organized as follow. We start
by discussing relevant related work. We then present the MG
model and terminology. Next, we detail our methodology
and the results of the experimental studies using two different
learning algorithms. Finally, we analyze the results and point
out directions for future research.

2 Related Work
While previous works have focused on problems relating ho-
mogeneous memory size to global system performance, little
effort has been put into understanding the relation between
individual memory size and individual agent’s performance.

IJCAI-07

212

Homogeneous agents involved in self-play are typically as-
sumed, with emphasis in system dynamics rather than indi-
vidual agents. However, as argued in [15], a more appropriate
agenda for multi-agent learning “asks what the best learning
strategy is for a ﬁxed class of the other agents in the game”,
as this allows the construction of effective agents. We fol-
low this methodology here. In the context of the MG, little
work so far has been directed to and concerned with indi-
vidual agent’s performance. An exception to that is [16], in
which a genetic algorithmic-based agent is shown to outper-
form traditional ones, but little investigation about the condi-
tions on why this happens is done. Other learning algorithms
have been applied to the MG [1, 2, 5]; however, homogeneous
agents are assumed with focus on global properties.

Heterogeneous memory sizes are studied in [11], where an
evolutionary setup is used to search for the best memory size
for the game; however, they do not ﬁx a class of agents, al-
lowing every agent to evolve independently. A similar setup
is used in [5], but their concern is average memory size. We
shall provide alternative explanation for some of their results.
In [6] it is stated that agents with larger memories perform
better than agents with smaller ones. Nonetheless, they con-
sider a varying number of agents endowed with only one ex-
tra bit of memory; we will show that having larger memories
may not lead to better performance.

N

3 A multiagent market scenario
The Minority Game (MG) was initially proposed in the con-
text of Econophysics, as a formalization of the El Farol Bar
Problem [3]. In these models, the main interest is the type of
information structure created by multiple interacting bounded
rational agents.
In the MG, there are two groups and an
odd number (N) of agents. At each round, agents must
choose concurrently and without communication to be in
only one of the two groups. At each round t, after all de-
cisions are made, a global outcome is calculated: A(t) =
(cid:2)
i=0 ai(t), where ai(t) is the decision of agent i in round
t and ai(t) ∈ {+1,−1}. Agents’ wealth is distributed ac-
cording to wi(t) = wi(t − 1) + ai(t)sign(A(t − 1)), where
sign(·) is +1 or −1 according to A(t − 1) being positive or
negative (other functions of A(t) may be used). Initial values
are set to zero. A(t) encodes which group had the minority of
agents and only those in this group are rewarded, while those
in the majority are penalized. There is no equilibrium point
in which all agents proﬁt, and the expectation of the majority
is always frustrated due to the anti-coordination nature of the
game: if many believe a particular group will be the minority
one, then they will actually end up in the majority. In spite
of its simplicity, the MG offers a rich and complex behavior
that was shown to represent several scenarios of decentral-
ized resource allocation, such as ﬁnancial markets and trafﬁc
commuters scenarios [4]. In [9], this model was generalized
and shown to belong to a broader class of dispersion games.
One of the main concerns in the MG is with the game’s
global efﬁciency, i.e. the number of agents winning the game.
From the viewpoint of the system as a whole, one is interested
in maximizing the number of winners, so that resources are
better distributed. When the MG was originally introduced,

agents were modeled using a simple inductive learning algo-
rithm to make decisions, corresponding to a simpliﬁed model
of how human decisions in complex environments are actu-
ally made [3]. Since this algorithm has been widely used and
studied in other applications, it is of our interest to make use
of it in a straightforward way (we refer to it as the traditional
learning algorithm).
In this algorithm, each agent has ac-
cess to the m last outcomes of the game and she is given
a set S of hypotheses, randomly selected from a ﬁxed hy-
potheses space H. A hypothesis maps all possible patterns
of past outcomes over m to a decision (i.e. what group to
choose in the next round). m is known as the memory size
of the game. A hypothesis is effectively represented by a bi-
nary string of length 2m and |H| = 22m. The learning al-
gorithm keeps a measure fi,j(t) of the effectiveness of each
hypothesis j held by agent i in time t, which is updated by
fi,j(t) = fi,j(t − 1) + aisign(A(t − 1)). This measure is
often called virtual points. An agent then uses her hypothesis
with the maximum number of virtual points argmaxjfi,j(t)
to commit to a decision in the next round of the game. Ties
are broken by coin toss. This algorithm is very simple and
provides more adaptation than learning, since agents are un-
able to better explore strategies outside their initial set. In or-
der to overcome the limitations of this algorithm and to allow
for comparisons with a different learning strategy, we shall
introduce a new evolutionary-based learning algorithm.

4 Methodology and Experiments
Due to the nature of our interest in the model, we consider
agents with different hypotheses space sizes (i.e. different
memory sizes), in order to observe the effects of such dif-
ferences in agents’ performance1. We split the MG into two
parts. The environment consists of a traditional MG with a set
N of agents, as described above (with homogeneous agents).
The control group G is a group of agents which sample hy-
potheses from a different space from that of the agents in the
environment, differing only in its size |H|. Since hypotheses
space size in our setup is deﬁned only by each agent’s mem-
ory size, we shall denote the environment’s by me and the
control group’s by mg.
In this work we assume |G| = 1. [6] investigates larger
groups in similar fashion; however in their experiments the
memory size is not systematically modiﬁed and their interest
is on the effects of changes in group sizes. We will observe
the performance for the resulting control agent when her hy-
potheses space size mg varies in relation to me. We also
want to verify how her performance (deﬁned by the average
wealth wg) changes with mg and compare it to the environ-
ment’s performance (deﬁned by the average wealth we of a
random agent in the environment). In order to do so, we mea-
sure the control agent’s gain: wg/we. All results are averages
over 200 randomly initialized games and they have been sta-
tistically tested for signiﬁcance using a standard t-test with
95% conﬁdence interval. Each game was run for 100,000
rounds and we have used the standard values found in the
literature for all other parameters, namely: |N| + |G| = 101
1When dealing with homogeneous agents, hypotheses are drawn

from the same hypotheses space.

IJCAI-07

213

Figure 1: Control agent gains for combinations of me and
mg. Lighter shades denote higher gains.
and |S| = 2, so comparisons could be easily made 2. We shall
detail experiments using the traditional and the evolutionary
learning algorithms.
4.1 Traditional Learning Algorithm
We start with the traditional learning algorithm in both the
control agent and agents in the environment.
In Fig. 1
we show the control agent’s gain for every combination of
me ∈ [1, 15] and mg ∈ [1, 15]. These ranges may seem
somewhat limited, but are adequate for our purposes. The ex-
ponential increase of hypotheses space with m and the large
number of agents often makes experimentation with larger
memories intractable.
In the resulting topography, we ob-
serve some interesting behaviors. For me ≤ 3, higher than
unit gains are obtained whenever mg > me. On the other
hand, for values immediately above me = 3 we can observe
that whenever mg > me, wg falls below unit, showing that
the target agent does worse by having a larger hypotheses
space. For larger me, wg is always below unit except where
mg = me, when the gain is, as expected, exactly unitary.

Let us now observe in detail what happens in speciﬁc
points of the observed regions by sectioning the topography
(see Fig. 2). For the ﬁrst case, we take me = 3. We can
see that our control agent beneﬁts from a larger hypotheses
space. Interestingly, having smaller spaces seem to cause no
harm and the agent performs as if mg = me. We can also
observe that the transition from one of the above cases to an-
other is quite sharp and further increases in mg provide no ad-
ditional gains. Instead, a logarithmic decrease of wealth may
be observed with further increases in mg. The highest gain
occurs precisely where mg = me + 1, where a spike may be
observed. As for the second case, we detail the behavior for
me = 6. We observe a logarithmic drop in wg for mg > me.
For mg < me, a small decrease of gains can be observed.
Thus, in this region no mg does better than unit and, interest-
ingly, having access to a larger input window is harmful to the
agent. For larger values of me, losses become larger when-
ever mg < me. Having access to a larger information win-

2It is known that larger S does not change the qualitative behav-

ior of the system [4]

Figure 2: Control agent gain for me = 3 (solid line) and
me = 6 (dashed line) with agents using the traditional learn-
ing algorithm

Figure 3: Control agent gain for varying me and mg = me+1

dow is harmful to the agent, except when the environment has
small memory sizes. To observe such phenomena, we plot the
control agent’s gain when me varies and we set mg = me+1.
This is shown in Fig. 3, where it is made clear that there is
a non-monotonic relation between memory size and agent’s
performance. We observe that gains are high for small values
of me, then they become smaller than unit, reaching a mini-
mum and later increasing again to ﬁnally converge to a value
slightly below unit. Having worse performance when access-
ing more information may seem counter-intuitive. One could
argue that this is due to the larger hypotheses space, which
makes ﬁnding a good hypothesis harder. Even though this
may be part of the explanation, it does not account for all of it
since we observe a non-linear relationship between gains and
hypotheses space size, as we shall further investigate.
4.2 Evolutionary learning
It could be argued that the observed behavior is only but a pe-
culiar effect of the learning algorithm used, whose main lim-
itation is the inability to explore the hypotheses space during
the game, i.e. an agent may only use the hypotheses given at
the start of a run. In order to address this concern, we have re-
peated the experiments using an evolutionary-based learning
algorithm so as to allow agents to further explore the hypothe-
ses space. Some different evolutionary-based algorithms for
the MG have been proposed [5, 11, 16]. We chose an adapta-
tion of the one proposed in [2], due to its simplicity and good

IJCAI-07

214

Algorithm Evol-Learning

w ← 0;
S ← random hypotheses ∈ H;
foreach s ∈ S do
ﬁtness(s) ← 0;
end
while not end of the game do
obs ← window of size m of past outcomes;
h ← arg maxs ﬁtness(s);
decision ← decision of h using observation obs;
commit(decision);
if decision = outcome then w ← w + 1;
foreach s ∈ S do

if decision of s using obs = outcome then

ﬁtness(s) ← ﬁtness(s) + 1;
ﬁtness(s) ← ﬁtness(s) - 1;

else

end

end
with probability pr do

worst hypothesis s ← best hypothesis k;
with probability pm, ﬂip bits in s;

Figure 5: A(t) for agents using evolutionary-based algorithm

end

end

end

Figure 4: Evolutionary Learning Algorithm

performance. While we could have applied a more elaborated
algorithm used in similar games, such as [7], we have chosen
to create an algorithm that is closely related to the traditional
one, but presenting better learning characteristics. By doing
so we are able to better understand and compare results ob-
tained using both algorithms.

In this algorithm (depicted in Fig. 4), each agent starts
with S hypotheses and, at every round, with probability pr,
she discards her worst performing hypothesis and replaces
it with a copy of her best performing one. Each bit of this
copy is then ﬂipped with probability pm. This allows agents
to search for better hypotheses, continually introducing new
ones to the game.
It is interesting to note that, differently
from other evolutionary learning algorithms such as [16], this
one does not require a global ordering of agents based on their
performances. Agents retain their autonomous characteristics
by not relying on a central authority to decide which hypothe-
ses among all agents are to be replaced, thus preserving the
distributed nature of the game. Figure 5 shows A(t) for a
typical run when agents are using the new proposed learning
algorithm and pr = 0.01 and pm = 0.001. Clearly, learn-
ing is taking place (when using the traditional algorithm, no
decrease in oscillations is observed, even for very long runs).
For all experiments using this algorithm we consider only re-
sults after 2,000 rounds, in order to observe the “steady” state
of the system. Figure 6 shows the control agent’s gain for
every combination of me ∈ [1, 15] and mg ∈ [1, 15] when
agents use the evolutionary-based learning algorithm. We
now observe a different behavior when compared to our pre-
vious case. There are no regions where larger memories are
beneﬁcial, each extra bit beyond the environment’s memory
size is harmful to the target agent. A logarithmic decrease
with mg is observed for mg > me, for all tested values of
me. In Fig. 7 we detail the behavior for some values of me.

Figure 6: Control agent gains for combinations of me and
mg. Lighter shades denote higher gains.

As with the traditional learning algorithm, for mg < me no
considerable losses or gains are observed.

5 Analyzing the Results
5.1 Dynamics of the game and its efﬁciency regions
As stated above, one of the main concerns with the MG is its
global efﬁciency, i.e. how many agents are winning the game
during a run. A typical way of measuring the temporal efﬁ-
ciency of resource distribution is by means of the statistical
variance σ2 of A(t) [4]. The larger the variance is, the larger
the waste of resources, making the system less efﬁcient. The
variance σ2 is a function only of the number of agents in the
game and their (homogeneous) memory size [13]. Since we
keep the number of agents ﬁxed and the control group is uni-
tary, we consider σ2 only as a function of me, the memory
size of agents within the environment. Figure 8 shows σ2 as
a function of me when all agents use the traditional learning
algorithm. The same plot is observed for any mg. Three re-
gions of efﬁciency can be observed. For small me, high vari-
ance is observed, thus low efﬁciency characterize the system.
For large me, the system has precisely the variance expected
if all agents were deciding randomly (i.e. the Random Case
Game - RCG). Intermediate values of me are correlated with
smaller, better than random, variances. This last case is often
the main subject of interest in the MG, since it indicates that
agents are able to self-organize to improve efﬁciency.

IJCAI-07

215

Figure 7: Target agent’s gain versus mg for me = 3 (solid
line) and me = 6 (dashed line), using the evolutionary learn-
ing algorithm

Comparing Fig. 8 with the plotted gain using the traditional
algorithm (Fig. 3), we observe that higher than unit gains are
associated with regions with high variances, gains below unit
are associated with regions with small variance and increases
in gains follow increases in variance. This indicates a cor-
relation between system’s global efﬁciency and exploitation
possibilities of individual agents, where larger memories are
beneﬁcial when the system is behaving inefﬁciently (worse
than random) whereas when it is efﬁcient having larger mem-
ories becomes harmful. The same behavior is observed in the
evolutionary setup. We can also observe in Fig. 8 that the
variance curve is quite different from the one using the tradi-
tional algorithm - there is a smooth transition between a very
low variance and the expected for the RCG. Since there is no
inefﬁcient region we would expect, following analogous rea-
soning, that no agent with mg > me would perform better
than the agents in the environment and this was actually ob-
served (see Fig. 6). Such results are interesting, since they re-
late the efﬁciency of the system as a whole with individual ex-
ploitation possibility (by means of larger memories) and indi-
cate the existence of stability points in the system’s efﬁciency
regions. For instance, if we take an evolutionary version of
the game where all agents start with a small memory and are
allowed to increase or decrease their memory size during a
run, we would expect that there would be an initial incentive
towards larger memories, leading the system towards higher
efﬁciency points. However, such incentive would stop when
the system reaches an efﬁcient point as larger memories then
become harmful. Thus, this efﬁcient point would be a stabil-
ity point. Such experiment was conducted in [5] where initial
memory growth was observed, halting at the predicted mem-
ory size. Such behavior was attributed to the simple nature of
the game. Here we propose a more detailed, distinctive expla-
nation relating individual agent gain to the efﬁciency region
of the system. This is a case of downward causation, where
game dynamics are initially ﬁxed by agents but such dynam-
ics end up ﬁxing possible (or proﬁtable) agents’ behaviors.
5.2 Mutual Information
In [13] homogeneous memory sizes and the traditional learn-
ing algorithm are used to show that there is different informa-
tion available to the agents for different memory sizes. They

Figure 8: σ2 as a function of me for the traditional (solid line)
and the evolutionary algorithm (dashed line)

have measured this information by the conditional probability
P (1|μk) - the probability of having a “1” following a binary
sequence μk of length k. They have shown that the inefﬁ-
cient region is actually informationally efﬁcient, in the sense
that P (1|μk) is exactly 0.5 for all μk whenever all agents
have memory of size k. On the other hand, in the efﬁcient
region there is an asymmetry in the probability distribution
and there are μk that offer predictive power. The informa-
tion analysis in [13] focused in cases where k = m, i.e.
the measurement of information available to agents with the
same memory size as their peers. We wish to access the in-
formation available within k (cid:4)= m, since it represents our
target agent with different memory sizes. In order to do so,
we have to deﬁne a more precise information measure. To
measure the asymmetry in the distribution of the predictive
power of each hypothesis, we use the concept of mutual in-
formation [12] between a string μk and its outcome, comput-
(cid:4)
ing the average information contained in each string: I(k) =
1|H|
,
where p1 and p0 are the probabilities of a “1” or a “0” occur-
ring, respectively; p1(μk) and p0(μk) are the probabilities of
“1” or a “0” immediately occurring after a string μk ∈ H, re-
spectively; pu(μk) = p1(μk)+p0(μk). In the above equation,
I(k) is zero whenever the probabilities of observing “1” or
“0” are the same for all μk. The highest the value of I(k), the
highest the average asymmetry of probabilities, indicating the
existence of hypotheses with predictive power. We have mea-
sured the information by executing runs of 200,000 rounds
and recording each outcome, resulting in a binary string of
200,000 symbols. Mutual information is then measured over
this string and averaged over 50 independent runs.

μk p1(μk)log2

p1(μk)
pu(μk)p1

+ p0(μk)log2

p0(μk)
pu(μk)p0

(cid:2)

(cid:3)

(cid:4)

(cid:3)

We are now in position to measure the information avail-
able for some hypotheses of different lengths when agents
are using the traditional learning algorithm. For each value of
me, we let k assume values below, equal to, and above me.
Figure 9 shows results for me = 3 and me = 6 (the same val-
ues detailed in the previous sections). We observe that I(k)
for me = 3 remains close to zero for k ≤ me. Information
available to memory sizes below me is almost the same as for
me, which explains why agents with smaller memories do not
present lower gains in Fig. 2. For k = me + 1 we observe
a substantial increase in the available information and further

IJCAI-07

216

ing in decreasing agent performance when we increase her
memory above an optimal size. The results presented here
lead to a better understanding of the emergent patterns created
from multiagent learning interactions using an information-
theoretic analysis. In particular, we have provided arguments
to help choosing the best memory size when designing an
agent or strategy to play the MG. While it is well known that
more memory is not necessarily better for collective perfor-
mance, we showed that this is also true for individual agent
performance. This result contributes for the construction of
better algorithms, that may take into account the system’s ef-
ﬁciency region when deciding between different hypotheses
space to consider.

Acknowledgments: This work has been partly supported by

CNPq and CAPES.

References
[1] M. Andrecut and M.K. Ali. Q learning in the minority game.

Physical Review E, 64(067103), 2001.

[2] R. M. Ara´ujo and L. C. Lamb. Towards understanding the role
of learning models in the dynamics of the minority game. In
Proc. of 16th IEEE ICTAI 2004, pages 727–731, 2004.

[3] W. B. Arthur.

Inductive reasoning and bounded rationality.

American Economic Review, 84:406–411, 1994.

[4] D. Challet, M. Marsili, and Y.-C. Zhang. Minority Games.

Oxford University Press, 2004.

[5] D. Challet and Y.-C. Zhang. Emergence of cooperation and
organization in an evolutionary game. Physica A, 246, 1997.
[6] D. Challet and Y.-C. Zhang. On the minority game: Analytical

and numerical studies. Physica A, 256:514–532, 1998.
Jacob W. Crandall and Michael A. Goodrich. Learning to
compete, compromise, and cooperate in repeated general-sum
games. In Proc. ICML 05, pages 161 – 168. ACM Press, 2005.
[8] E. Gelenbe, E. Seref, and Z. Xu. Towards networks with intel-

[7]

ligent packets. Proc. IEEE-ICTAI, 1999.

[9] Trond Grenager, Rob Powers, and Yoav Shoham. Dispersion
games: general deﬁnitions and some speciﬁc learning results.
In Proc. AAAI 02, pages 398 – 403. AAAI Press, 2002.

[10] J. Hu and M. Wellman. Online learning about other agents in a
dynamic multiagent system. In Proc. Agents-98. ACM, 1998.
[11] Y. Li, R. Riolo, and R. Savit. Evolution in minority games ii:

Games with variable strategy spaces. Physica A, 276, 2000.

[12] David J. MacKay. Information Theory, Inference & Learning

Algorithms. Cambridge University Press, 2003.

[13] R. Manuca, Y. Li, R. Riolo, and R. Savit. The structure of
adaptive competition in minority games. Physica A, 282, 2000.
[14] Rob Powers and Yoav Shoham. Learning against opponents

with bounded memory. In IJCAI 05, pages 817–822, 2005.

[15] Y. Shoham, R. Powers, and T. Grenager. On the agenda(s) of
research on multi-agent learning. In Proc. of AAAI Fall Sym-
posium on Artiﬁcial Multi-Agent Learning, 2004.

[16] M. Sysi-Aho, A. Chakraborti, and K. Kasti.

Intelligent mi-
nority game with genetic-crossover strategies. Eur. Phys. J. B,
34(00234):373–377, 2003.

[17] J. M. Vidal and E. H. Durfee. Agents learning about agents:
A framework and analysis. In Multiagent Learning Workshop,
AAAI97, 1997.

Figure 9: I(k) versus k for me = 3 (solid line) and me = 6
(dashed line), using the traditional learning algorithm

increases in k lead to a logarithmic decrease in I(k). This,
again, is in accord with the observed behavior for the target
agent’s gains and is evidence that agent’s interactions are cre-
ating patterns that are of greater length than their memory
sizes. For me = 6, the mutual information plot also closely
follows the target agent’s gain plot (Fig. 2), where the highest
information is present precisely at k = me. In both cases the
highest value of I(k) is followed by a logarithmic fall with
further increases in k. This fall is expected: take kp as the
value of k associated with the highest value of I(k); if we
increase the hypotheses such that k > kp, the informative hy-
potheses previously at kp becomes more spread out through
the hypotheses space. For example, if kp = 3, we could have
a hypothesis that detects the pattern “001” but, for k = 4, this
pattern is found in two hypotheses - “1001” and “0001” - but
only one of them may be actually happening. We conclude
that mutual information provides a good explanation for the
target agent’s gains with different memory sizes. It is worth
observing that for some cases using the traditional learning
algorithm, patterns of lengths different from the agent’s mem-
ory size are created - as can be inferred from the target agent’s
exploitation possibility when having mg > me.
6 Conclusions
The number of game rounds considered by each agent is a
central issue in several dispersion games, such as the MG. By
means of a methodology including extensive simulations, we
have analyzed emerging patterns resulting from the agents’
interaction, relating them to the possibility of exploitation of
certain setups. We have shown that having access to more
information is not always an advantage and that it can ac-
tually be harmful. Experiments with the traditional learning
algorithm have shown that there is a region, related to smaller
memory sizes, where an agent with larger memory could ex-
ploit the system and obtain higher gains, whereas the same
agent could have its gain reduced at another region. We have
also measured the mutual information associated to different
strings at the outcome history. We have related this infor-
mation to the observed behavior, showing that agents in the
system often generate patterns that are not of the same size of
their memory sizes, but which can be exploited by an agent
with larger memory. On the other hand, larger strings make
information sparser in the space of possible patterns, result-

IJCAI-07

217


A Learning Algorithm for Web Page Scoring Systems 

Michelangelo Diligenti, Marco Gori, Marco Maggini 

Dipartimento di Ingegneria dell'Informazione, Universita di Siena, Italy 

{michi, marco, maggini}@dii.unisi.it 

Abstract 

Hyperlink analysis is a successful approach to de(cid:173)
fine algorithms which compute the relevance of a 
document on the basis of the citation graph. In this 
paper we propose a technique to learn the parame(cid:173)
ters of the page ranking model using a set of pages 
labeled as relevant or not relevant by a supervisor. 
In particular we describe a learning algorithm ap(cid:173)
plied to a scheme similar to PageRank.  The rank(cid:173)
ing algorithm is based on a probabilistic Web surfer 
model and its parameters are optimized in order to 
increase the probability of the surfer to visit a page 
labeled as relevant and to reduce it for the pages 
labeled as not relevant.  The experimental results 
show the effectiveness of the proposed technique in 
reorganizing the page ordering in the ranking list 
accordingly to the examples provided in the learn(cid:173)
ing set. 

Introduction 

1 
PageRank  [Page et al,  1998] is the most popular algorithm 
to  compute  the  page  relevance  in  a  collection  of hyper-
linked documents, like the Web.  This algorithm exploits the 
topology of the citation graph in order to determine the au(cid:173)
thority of each page using the intuitive idea that authorita(cid:173)
tive documents are linked by many authoritative documents. 
Thus, the original algorithm computes a ranking which de(cid:173)
pends  only  on  the  graph  connectivity  without  considering 
the  page  contents.  However,  focused  versions  of PageR(cid:173)
ank have been proposed in the literature [Haveliwala, 2002; 
Richardson and Domingos, 2002; Diligenti et al, 2002] in or-
der to define more selective rankings for topic-specific search 
engines. These approaches allow a limited degree of adapta(cid:173)
tion related to the choice of the specific topic. A text classifier 
is trained using examples of pages on the topic of interest but 
no further level of learning is considered in the ranking mod(cid:173)
els. In particular, the classifier is trained independently on the 
ranking algorithm. 

We propose a more general approach in which the learning 
algorithm is defined directly on the ranking model.  In this 
case there is not a predefined topic, but a supervisor provides 
examples of pages to be promoted and pages which are con(cid:173)
sidered as not relevant.  Thus, the learning algorithm is able 

to manage both positive examples (relevant pages) and neg(cid:173)
ative examples (not relevant pages).  The ranking function is 
based on the Web surfer model [Diligenti et al.  2002].  The 
relevance of a page is defined by the probability of the surfer 
to visit the page when considering the stationary probability 
distribution of the Markov chain which describes the surfer 
behavior.  Basically the learning algorithm computes the pa(cid:173)
rameters which define the surfer model in order to reduce the 
probability of visiting a negative example and to increase the 
probability of being in a positive example. In order to reduce 
the number of free parameters, the surfer model is simplified 
by dividing the pages among a set of categories and assuming 
that the surfer behavior depends only on the category of the 
page where it is located.  In particular, the set of categories 
does not need to be predefined but it is determined by a clus(cid:173)
tering algorithm based on the document contents. Moreover, 
whereas the focused versions of the PageRank simply direct 
the  Web  surfer to the most promising page (i.e.  the page 
whose content is the most similar to the content of a target 
page), the proposed algorithm can exploit hierarchies of top(cid:173)
ics, allowing the surfer to follow complex paths, composed 
by many steps, leading to relevant pages. 

The paper is organized as follows.  In the next section the 
web surfer model is reviewed. Then, in section 3 the learning 
algorithm is described.  Section 4 reports some experiments 
which show the effectiveness of the proposed learning algo(cid:173)
rithm. Finally, in section 5 the conclusions are drawn. 

2  Random Walks on the Web graph 
Random walk theory has been widely used to compute the 
absolute relevance of a page in the Web [Page et  al,  1998; 
Lempel and Moran, 2000]. The Web is represented as a graph 
G, where each Web page is a node and a link between two 
nodes represents a hyperlink between the associated pages. 
A common assumption is that the relevance  Ip  of page p is 
represented by the probability of ending up in that page dur(cid:173)
ing a walk on this graph. 

We consider the model of a Web surfer who can perform 
one out of two atomic actions while visiting the Web graph: 
jumping  to  a  node  of the  graph  (action  J)  or  following a 
hyperlink from the current page (action I).  In general,  the 
action taken by the surfer will depend on the page contents 
and the links it contains.  We can model the user's behavior 
by a set of probabilities which depend on the current page: 

LEARNING 

575 

is  the  probability  of  following  one  hyperlink  from 
page q, and 
is  the  probability  of jumping  from  page 
q.  These  values  must  satisfy  the  normalization  constraint, 

The  two  actions  need  to  specify  their  targets.  Assum(cid:173)
ing that surfer behavior is time-invariant, then the targets are 
of jumping  from  page 
specified by the  probability 
q to page p,  and the  probability 
of selecting  a  hy(cid:173)
perlink  from  page  q  to  page  p. 
is  not  null  only 
for  the  pages p  linked  directly  by  page 
being  ch(q)  the  set  of the  children  of node  q  in  the  graph 
G.  These  sets  of values  must  satisfy  the  probability  nor(cid:173)
malization  constraints 
and 

The  model  considers  a  temporal  sequence  of actions per(cid:173)
formed by the surfer and it can be used to compute the proba(cid:173)
bility that the surfer is located in page p at time 
The 
probability distribution on the pages of the Web is updated by 
taking into account the actions at time 
using the follow(cid:173)
ing equation 

is the set of the parents of node p. The relevance 
where 
of a page p, 
is computed using the stationary distribution 
of the  Markov  Chain  of equation  (1).  This  is  a  simplified 
version of the model proposed in [Diligenti et  al,  2002]. 

3  Learning the page rank 
The random surfer model which is used to compute the page 
relevance scores depends on the values assigned to the proba(cid:173)
bilities 
In most of the 
approaches  proposed  in  the  literature,  these  parameters  are 
predefined  or  computed  from  some  features  describing  the 
page p and/or the page q.  For example, in the original PageR-
ank scheme 
and 

and 

However, in the general case, it would be infeasible to es(cid:173)
timate the parameters without any assumption to reduce their 
number.  In particular, we assume that the pages are grouped 
into n clusters according to their content and that the surfer's 
behavior  for  any  page  p  is  dependent  only  on  the  cluster 
which p  belongs  to. 
In  our  experiments  we  used  classical 
clustering techniques (e.g. k-means) on the bag-of-words rep(cid:173)
resentation of the pages  [Kobayashi and Takeda,  2000].  By 
this hypothesis, the parameters of the model are computed as: 

(2) 
(2) 

Figure  1:  The context graph built by following the links back(cid:173)
wards from an example page. 

the  parameter 

is the 

The values 

to a page in cluster 

is  the  cluster which  page  q belongs to, 

where 
number  of pages  in  cluster 
is  the  tendency  of the  surfer  to  follow  a  hyperlink  from  a 
and the parameter 
page  in  cluster 
is the tendency of the surfer to jump from a page 
to a page of cluster 

of  cluster 
are initialized to 1, such that all links are equally likely to be 
followed.  Because of the model assumptions, the stronger is 
the tendency of the surfer to move to a cluster, the higher are 
the scores associated to the pages in that cluster. 

3.1  The learning  set 
We assume that a supervisor provides a set pages labeled as 
relevant and not relevant.  These examples are used to define 
a specific view of the collection of documents contained  in a 
search engine.  In particular the supervisor provides: 
which should get a high rank; 
which should get a low rank. 

a set of pages 
a set of pages 

In  the  first  case  we  refer  to  "positive"  or  "good"  pages, 
whereas in the latter case to "negative" or "bad" pages. 

links only nodes in layer  

Each example page is associated to its Context Graph [Dili(cid:173)
genti et  al,  2000], representing how the page can be reached 
from the Web.  The graph is organized into layers as shown 
in  figure  1.  The  example page  is  the  only  node  in  layer 0. 
Then, each node in layer 
Thus,  a page  belongs  to  layer  i  if it  is  at  least  i  links  away 
from the example page.  The depth K of the context graph  is 
defined as the number of layers in the graph including level 0. 
For a given example, the context graph can be built by back-
crawling the Web adding to layer 
the pages which link 
the pages in layer i, up to a maximum level K.  Moreover, all 
the pages that are  linked by at least a page  of the graph are 
also considered. 

576 

LEARNING 

and 

The  learning  set  consists of the  context graphs  built  from 
is the set of all  the pages 
the examples  in 
contained  in  the  learning set; 
is  the  union  of the 
pages in the layer k of the context graphs of the positive (neg(cid:173)
ative) examples. 
3.2  Learning the transition parameters 
We  define  the  set  of transition  probabilities   

to a page in the cluster 

which  represent  the  proba(cid:173)
bilities of the surfer to select a hyperlink  from a page  in the 
cluster 
The rank of the pages in 
the set 
is decreased if we increase the probability of the paths leading 
and we reduce the probability of paths leading 
to pages in 
to pages in 
.  This effect can be obtained by maximizing 
the following cost function 

is increased, whereas the rank of the pages in 

where 
is a discount factor which is used to reduce 
the  impact on the cost function  of the  links too far from  the 
target page. 

The probability 

of moving  one 
level in the context graph towards a good page, can be rewrit(cid:173)
ten as 

where 
reflects the surfer model  assump(cid:173)
tion that the links are selected depending only on the clusters 

and 

.  The probability  

of following a link to a page in the cluster c3  of layer A: 
of a context graph 

of  layer 

from a page in the cluster 
can be estimated as 

where 
the set A to the pages in the set B. 

is  the  number of links  from  the pages  in 

On the other hand the term 

repre(cid:173)
sents the probability that the surfer is located in a page of the 
cluster 
of 
a positive context graph.  This probability  can  be  estimated 
using the scores of the pages in cluster 
com(cid:173)
puted using the current values of the parameters, as 

given that the page belongs to the  layer 

and in 

Similar relationships can be derived for the negative set. 
The cost function (3) can be optimized by updating the pa(cid:173)
using  gradient  ascent.  The  gradient  with  re(cid:173)
can be 

rameters  in 
spect to the transition parameters 
computed using equation (4), yielding 

where  we neglected that 

and 

actually depend on the transition parameters. 
are essentially a not 

Since the model parameters 
normalized version of the transition parameters 

we can update them directly as  

where 77 is the learning rate. 

Once the model parameters are updated, a new score dis(cid:173)
tribution can be computed, thus updating the estimate of the 
variables 
using equation (6).  There(cid:173)
fore, the function optimization and the score estimation can 
be  iterated,  using  an  EM  style  algorithm  [Dempster et  0/., 
1977], till a termination criterion is satisfied. 
3.3  Learning  the j u mp  parameters 
The transition parameters for a jump can be optimized to in(cid:173)
crease the probability of jumping to a "good" page and not to 
a "bad" page.  This effect is obtained by maximizing the cost 
function 

where 
of the jump  parameters. 

We start observing that, 

is the set 

Moreover, the value 
as 

is computed at each iteration 

LEARNING 

577 

Similar relationships can be derived for the negative set. 

The cost function (9) is maximized by gradient ascent. The 
gradient can be computed using equation (10) and the equiv(cid:173)
alent equation for the negative set, yielding 

where we neglected the dependence of x(q 
d \ J) on the pa(cid:173)
rameters.  At each iteration, the parameters are updated using 
the direction of the gradient as, 

being  r;  the  learning  rate,  and  then  normalized to  meet  the 
probabilistic  constraints 

The adapted surfer model can re-surf the Web graph yield(cid:173)
ing a new estimate of equation (12).  Thus, the new gradient 
estimate can be used to update again the parameters, till the 
stop criterion is satisfied. 
3.4  Learning the surfer action bias 
The surfer action bias is represented by the value  of   

which  is the probability of following a link from the cur(cid:173)

rent page rather than jumping to another page  

In order to favor the action leading to good 
pages, we maximize the  following cost function which rep(cid:173)
resents  a  weighted  average  of the  probabilities  of ending  in 
the  layer k  of a  positive context graph  and not  of a negative 
context graph, 

(16) 

(17) 

(18) 

where  we  assumed  that  all  links  out  of a  page  are  equally 
likely to be  followed.  The term 
is the 
probability  of jumping  to  the  layer fc of a  positive  context 
graph from a page of cluster i, and can be computed as 

(19) 

can be  computed  at each  iteration  as  in 
Finally, 
equation (12).  Similar relationships hold for the negative set. 
The gradient of the cost function (16) can be computed us(cid:173)
ing equation  (17) and  the  corresponding relationship for the 
negative set, yielding 

neglecting the dependence of 

I he parameters are updated by gradient ascent as 

where the learning rate must be chosen small enough in order 
to preserve the stochastic model  of the surfer. 

The adapted surfer model can re-surf the Web graph yield-
ing a new estimate of equation (12).  Thus, the new gradient 
estimate can be used to update again the parameters and the 
procedure is iterated till the stop criterion is satisfied. 
4  Experimental  results 
In the following experiments, the learning algorithm was ap(cid:173)
plied  only  to  the  transition  parameters  of the  random  surfer 
model.  The  experiments  were  performed  on  two  datasets, 
each containing  1.000.000 documents, which  were collected 
by focus crawling the Web on the topics "wine" and "playsta-
tion",  respectively.  The  documents  were  clustered  using  a 
hierarchical  version  of  the  k-means  algorithm  [Fukunaga, 
1990]. We considered different numbers of clusters generated 
by the k-means algorithm.  In particular, in two different runs, 
we clustered the pages from the dataset on topic "playstation" 
into 16 and 25 sets, whereas the dataset on topic "wine" was 
clustered into 25 and 100 sets. 

Two topic-specific  search engines  were build  creating the 
inverted lists containing the terms of all the documents. Then, 
the rank of each page in the dataset was computed using the 
model described in section 2 by setting its parameters to re(cid:173)
produce the original PageRank ranking. We tested the quality 
of the ranking  function by submitting a set of queries to this 
focused  search engine.  The  documents  matching the query 
were sorted by the scores assigned by the random surfer.  By 
interacting  with  the  search  engine,  wc  found  pages  which 
were  authorities  on the  specific  topic  but  were  (incorrectly) 
not inserted in the top positions of the result list.  Such pages 
were selected as examples of pages which should get a higher 

578 

LEARNING 

Figure 2:  Plots of the  values of the transition  parameters  for 
each  cluster,  resulting  from  the  training  of the  surfer, 
(a) 
"wine" dataset. (b) "playstation" dataset. 

rank (positive examples).  On the other hand, we found pages 
having a high score which were  not authorities on the topic. 
Such pages were selected as negative examples.  The learn(cid:173)
ing algorithm,  as described in  section 3,  was applied to the 
datasets,  using  the  selected  examples. 
In  the  experimental 
setup  only  a  small  set  of examples  (3-15)  was  typically  la(cid:173)
belled as positive or negative. 

4.1  Analysis  of the  effects  of learning 
In  a  first  group  of experiments  we  aimed  at  demonstrating 
that the surfer model as learnt by the proposed training algo(cid:173)
rithm, could not be generated by a simpler (but less powerful) 
schema as the focused versions of PageRank, proposed in the 
literature. 

In  figure  2-(a)  2-(b),  we  show  the  values  of the  transi(cid:173)
tion  parameters  for  each  cluster,  resulting  from  the  train(cid:173)
ing  sessions  for  a  surfer  on  the  topic  "wine"  and  "playsta(cid:173)
tion",  respectively.  The  learnt parameters  represent a  com(cid:173)
plex  interaction  of the  resulting  surfer  with  the  page  clus(cid:173)
ters.  This  interaction  could  not  be  expressed  by  a  simple 
schema  as,  for  example,  the  focused  versions  of PageRank 
proposed in the literature [Richardson and Domingos, 2002; 
Diligenti et al., 2002].  This demonstrates that the model,  in 
order to  learn the users'  feedback,  needed to exploit hierar(cid:173)
chies  of topics  to  model  the  complex  path  leading  either to 
positive or negative examples. 

In figure 3-(a) and 3-(b), we show for each cluster, the val(cid:173)
ues of the probability of the surfer of being located  in a page 

Figure 3: Plots of the probability of the surfer of being located 
in a page of a given cluster for the topic "wine".  The proba(cid:173)
bilities are normalized with respect to the number of pages in 
the cluster, (a) 25 clusters, (b)  100 clusters. 

of that  cluster. 
In  particular  we  used  the  dataset  on  topic 
"wine" clustered  into  25 and  100 groups,  respectively.  The 
probabilities  arc  normalized  with  respect  to  the  number  of 
pages in the cluster. The learning algorithm is able to increase 
the  likelihood  of the  random  surfer to  visit  pages  belonging 
to a set of clusters, while decreasing its probability of visiting 
pages belonging to other clusters. 

In  figure  4,  we  report  the  rank  of pages after the  learning 
session with respect of their rank before learning.  In particu-
lar, on the x axis the pages are sorted according to their page 
rank  before  the  learning  session.  The  closer a  page  to  the 
origin, the more "relevant" the page is.  On the y axis it is re(cid:173)
ported the corresponding page rank after the learning session. 
The  plot shows  the  generalization  capability  of the  learning 
algorithm,  which  is able to change the ranks of a significant 
percentage of pages,  even  if a small  set of example was pro(cid:173)
vided. 
4.2  Qualitative  results 
After  training  the  surfer  and  computing  the  scores  for  all 
pages in the dataset, we compared the rank before and after 
the training process. 

Figure 5 reports the pages which obtained the largest vari(cid:173)
ation  in their rank.  The pages that obtained a negative rank 
variation  were  either  topic-generic  authoritative pages  (e.g. 
www.netscapc.com,  www.yahoo.com)  or pages  relevant  for 
different  topics  than  "wine"  (e.g.  "www.forgottensoldier-
.org").  Only the page  "www.yahoo.com"  was explicitly pro(cid:173)
vided to the system as a negative example. On the other hand, 
the pages that obtained a positive rank  variation were effec(cid:173)
tively  authoritative pages  on  the  topic  "wine"  (e.g.  "www-
.wine-searcher.com" or  "webwinery.com").  Among the most 

LEARNING 

579 

Mos 

"wine' 

p://www  macromedia.coni/go/getflashplayer 
topic 

http://wwwt  descending  pages  on 
htt.netscape.com 
http://www.macromedia.com/downloads 
http://www.yahoo.com 
http://www.adobe com/products/acrobat/readstep.html 
http://help.yahoo.com 
http://www.nl.placestostay.com/mdex.html 
http://home.netscape.com 
http://www.forgottensoldier.com 
http://www.inntravels.com 

http.//www. winccountry.com 
http://webwinery.com 
http://www.ny-wine.com/winemfo.asp 
http://www.wine-searcher.com 
http.//www. insidcwinc.com 
http.//webwinery.com 
http.//www.vinicsaporidipughacom/en/index.html 
http://www.vinitaly.com/home.en.asp 
http//www. wmeinstitute.org 
http://www.winespectator.com 

Figure  5:  URLs  of the  pages  that  yielded  either  the  largest 
negative or positive rank changes among the pages that were 
initially in the top 1000 scoring documents. 

In  Proceedings  of the  International 
by  context  graphs. 
Conference on  Very Large Databases,  11-15 September 
2000, II Cairo.  Egypt, pages 527-534, 2000. 

[Diligenti etal, 2002]  M.  Diligenti,  M.  Gori,  and  M.  Mag-
gini.  Web Page Scoring Systems for vertical and horizon(cid:173)
tal search engines. In Proceedings of the 11-th World Wide 
Web conference (WWW11), 2002. 

[Fukunaga,  1990]  K.  Fukunaga. 

Introduction  to  Statistical 
Pattern Recognition. Academic Press, San Diego, CA, US, 
1990. 

[Haveliwala, 2002]  T. H. Haveliwala.  Topic-sensitive pager-
In  Proceedings  of the  eleventh  international  con(cid:173)
ank. 
ference on  World Wide Web, pages 517-526. ACM Press, 
2002. 

[Kobayashi and Takeda, 2000]  Mei  Kobayashi  and  Koichi 
Takeda.  Information retrieval on the web.  ACM Comput(cid:173)
ing Surveys, 32(2):144-173, 2000. 

[Lempel and Moran, 2000]  R.  Lempel  and  S.  Moran.  The 
stochatic  approach  for  link-structure  analysis  (SALSA) 
and the TKC effect. In Proceedings of the 9th World Wide 
Web Conference, 2000. 

[Page etal,  1998]  Lawrence  Page,  Sergey  Brin,  Rajeev 
Motwani,  and  Terry  Winograd.  The  PageRank  citation 
ranking:  Bringing  order  to  the  Web.  Technical  report, 
Computer Science Department, Stanford University, 1998. 
[Richardson and Domingos, 2002]  Mathew  Richardson  and 
Pedro  Domingos.  The  Intelligent  Surfer:  Probabilistic 
Combination  of Link  and  Content  Information  in  PageR(cid:173)
In Advances  in  Neural Information  Processing Sys(cid:173)
ank. 
tems 14. MIT Press, 2002. 

Figure 4:  Plot of the page  ranks before  (x  axis)  versus  after 
learning (y axis).  The closer a page is to the origin, the most 
"relevant" it is. 

ascending  documents,  only  the  page  "www.winespectator-
.com" was explicitly provided to the system as a positive ex(cid:173)
ample, whereas all other pages were pushed up in the rank for 
the capability of the system to generalize from a small set of 
training data. 

5  Conclusions 
In  this  paper  we  introduced  a  novel  algorithm  to  learn  the 
ranking  function  over a  collection  of Web  pages.  The  pro(cid:173)
posed  framework  allows  us  to  tune  a  previously  computed 
rank distribution, specifying which pages should get a higher 
or  lower  score.  Like  in  the  original  PageRank,  we  assume 
that the score of a page is proportional to the probability that a 
Web surfer is visiting the page. The learning algorithm adapts 
the parameters of the surfer in order to increase the probabil(cid:173)
ity that the surfer is visiting a "good" page, while decreasing 
the probability that the surfer is  visiting a "bad" page.  The 
parameters  are  estimated  from  the  Context  Graphs,  which 
compactly represent the topological context in which a Web 
page  is  inserted.  The  learning  algorithm  is  fast  since  only 
close ancestors of example pages are considered in the Con(cid:173)
text Graphs. Thus, it can be applied to large scale repositories 
with little overhead (i.e.  the ranking function must be com(cid:173)
puted more  many times).  Further  investigations are needed 
to compare the accuracy of the  learnt ranking functions with 
respect  to  the  focused  versions  of the  PageRank  and  to  ex(cid:173)
ploit the complete learning algorithm which considers all the 
model parameters. 

References 
[Dempster et al,  1977]  A.P.  Dempster,  N.M.  Laird,  and 
D.B.  Rubin.  Maximum  likelihood from  incomplete  data 
via the EM algorithm.  J.  R.  Statist.  Soc.  B, 39:185-197, 
1977. 

[Diligenti et al., 2000]  M. 

Coetzee, 
S.  Lawrence,  L.  Giles,  and  M.  Gori.  Focus  crawling 

Diligenti, 

F. 

580 

LEARNING 


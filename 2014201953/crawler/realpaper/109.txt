Compiling Bayesian Networks with Local Structure

Mark Chavira and Adnan Darwiche

Computer Science Department

University of California, Los Angeles

Los Angeles, CA 90095-1596
{chavira,darwiche}@cs.ucla.edu

Abstract

Recent work on compiling Bayesian networks has
reduced the problem to that of factoring CNF en-
codings of these networks, providing an expressive
framework for exploiting local structure. For net-
works that have local structure, large CPTs, yet no
excessive determinism, the quality of the CNF en-
codings and the amount of local structure they cap-
ture can have a signiﬁcant effect on both the ofﬂine
compile time and online inference time. We ex-
amine the encoding of such Bayesian networks in
this paper and report on new ﬁndings that allow us
to signiﬁcantly scale this compilation approach. In
particular, we obtain order–of–magnitude improve-
ments in compile time, compile some networks
successfully for the ﬁrst time, and obtain orders–
of–magnitude improvements in online inference for
some networks with local structure, as compared to
baseline jointree inference, which does not exploit
local structure.

Introduction

1
It was shown recently that compiling Bayesian networks cor-
responds to factoring multi–linear functions (MLFs) [Dar-
wiche, 2003]. In particular, each Bayesian network can be
characterized by an MLF of exponential size, whose eval-
uation and differentiation solves the exact inference prob-
lem. Moreover, the MLF can be factored into an arith-
metic circuit (AC) whose size is not necessarily exponen-
tial, allowing one to use ACs as a compiled representation
Interestingly, [Park and Darwiche,
of Bayesian networks.
2003] has shown that building a jointree [Jensen et al., 1990;
Shenoy and Shafer, 1986] for a Bayesian network corre-
sponds in a precise sense to a process of factoring the MLF
into an AC, which is embedded by the jointree structure.

These ﬁndings created new possibilities for performing ex-
act inference, as they provided a new computational frame-
work based on factoring MLFs. In fact, a speciﬁc MLF factor-
ing method was proposed in [Darwiche, 2002], which can ex-
ploit the structure inherent in network parameters. According
to this approach, one encodes the MLF using a propositional
theory in Conjunctive Normal Form (CNF), factors the CNF,

and then immediately extracts the AC from the CNF factor-
ization. The beneﬁt of this logical approach is twofold. First,
it allows one to encode local structure in the form of deter-
minism and context speciﬁc independence (CSI) [Boutilier et
al., 1996]. For example, using this approach, it became prac-
tical to compile some Bayesian networks with binary vari-
ables and excessive determinism (induced by relational mod-
els) and having treewidths in excess of 200 by compiling the
AC in minutes and evaluating them in seconds [Chavira et al.,
2004]. The second advantage of this approach is its ability to
accommodate different representations of conditional prob-
ability tables (CPTs) (decision trees, rules, noisy–or, etc.),
without the need for algorithmic change. In this paper, we
consider only tabular representations.

The critical computational step in the above approach is
clearly that of factoring/compiling the CNF, which is done us-
ing an exhaustive and reﬁned version of the DPLL algorithm
[Davis et al., 1962; Darwiche, 2004]. The key observation
underlying this paper is that the efﬁciency of the factoring
step—both factoring time and size of factorization—can be
signiﬁcantly improved through careful CNF encodings which
capture as much local structure as possible, and by passing
additional information about these CNFs to the factoring al-
gorithm. This becomes especially true when handling net-
works that have local structure, large CPTs, yet no excessive
determinism. We do indeed propose a particular CNF en-
coding which appears quite effective on networks with such
properties. We also identify a key semantic property of the
resulting CNFs, which we exploit in the CNF factoring al-
gorithm. By incorporating these ﬁndings, we show dramatic
improvements in the both ofﬂine compile time and online in-
ference. In the ofﬂine compilation phase, we get an order–of–
magnitude improvement in some cases and an ability to com-
pile some networks for the ﬁrst time. In the online phase, we
observe orders–of–magnitude improvements on some well
known benchmarks, such as Pathﬁnder, Munin1, and Water,
over online inference by the baseline jointree algorithm.

2 Factoring Multi–linear Functions
Our investigation is based on three technical observations
[Darwiche, 2003]: that every Bayesian network can be inter-
preted as an exponentially–sized MLF whose evaluation and
differentiation solves the exact inference problem; that such
an MLF can be factored into an AC whose size may not be ex-

row A
1
a1
2
a1
3
a1
4
a1
5
a1
6
a1
7
a2
8
a2
9
a2
10
a2
11
a2
12
a2

B C
c1
b1
b1
c2
c3
b1
c1
b2
c2
b2
c3
b2
b1
c1
c2
b1
c3
b1
c1
b2
c2
b2
b2
c3

Pr(c | a, b)
θc1|a1b1 = 0
θc2|a1b1 = 0.5
θc3|a1b2 = 0.5
θc1|a1b2 = 0.2
θc2|a1b1 = 0.3
θc3|a1b1 = 0.5
θc1|a2b2 = 0
θc2|a2b2 = 0
θc3|a2b1 = 1
θc1|a2b1 = 0.2
θc2|a2b2 = 0.3
θc3|a2b2 = 0.5

Figure 1: A small Bayesian network with one of its CPTs,
showing local structure in the form of determinism and CSI.

Figure 2: An MLF factored into an AC.

ponential; and that the MLF factoring process can be reduced
to factoring a CNF encoding of the MLF.

The MLF for a network contains two types of variables.
For each value x of each network variable X, there is an in-
dicator variable λx. For each network parameter Pr(x|u),
there is a parameter variable θx|u. The MLF contains a term
for each instantiation of the network variables, and the term is
the product of all indicators and parameters that are consistent
with the instantiation. For the network in Figure 1, variables
A and B have two values, and variable C has three values.
The MLF corresponding to this network is as follows:

λa1λb1λc1θa1θb1θc1|a1,b1 + λa1λb1λc2θa1θb1θc2|a1,b1+
. . .
λa2λb2λc2θa2θb2θc2|a2,b2 + λa2λb2λc3θa2θb2θc3|a2,b2

To compute the probability of evidence e, we evaluate the
MLF after setting indicators that contradict e to 0 and other
indicators to 1. For example, to compute Pr(a2, b1), we set
indicators λa1 and λb2 to 0, set other indicators to 1, and eval-
uate the reduced MLF: Pr(a2, b1) =
θa2θb1|a2θc1|a2,b1 + θa2θb1|a2θc2|a2,b1 + θa2θb1|a2θc3|a2,b1
As is obvious from the above example, the MLF has an ex-
ponential size. Yet the MLF can be factored into an AC whose
size may not be exponential, leading one to formulate the ex-
act inference problem as a problem of factoring MLFs into
ACs. An AC is a DAG with internal nodes labeled with mul-
tiplications/additions and leaves labeled with variables and
constants; see Figure 2.1

One way to factor an MLF into an AC is by encoding the
MLF into CNF and then factoring the CNF. To illustrate the
encoding scheme, consider again the MLF f in Figure 2 over
real–valued variables a, b, c, d. The basic idea is to specify
this MLF using a propositional theory that has exactly four
models, one for each term in f. Speciﬁcally, the propositional
theory ∆f = Va ∧ (Vb ⇒ Vd) ∧ (Vc ⇒ Vb) over Boolean
variables Va, Vb, Vc, Vd has exactly four models and encodes
f as follows:

Model
σ1
σ2
σ3
σ4

Va
true
true
true
true

Vb
false
false
true
true

Vc
false
false
false
true

Vd
false
true
true
true

encoded term
a
ad
abd
abcd

That is, model σ encodes term t since σ(Vj) = true precisely
when term t contains the real–valued variable j.

By factoring/compiling the CNF ∆f as discussed in [Dar-
wiche, 2004], one can immediately extract an AC represen-
tation of the MLF f in time and space proportional to the
factored CNF [Darwiche, 2002]. We will discuss this process
later, but we ﬁrst provide more detail on the encoding step.
We start here with the baseline encoding [Darwiche, 2002],
which we refer to as PREV.

The CNF has one Boolean variable Vλ for each indicator
variable λ, and one Boolean variable Vθ for each parameter
variable θ. For brevity though, we will abuse notation and
simply write λ and θ instead of Vλ and Vθ. CNF clauses
fall into three sets. First, for each network variable X with
domain x1, x2, . . . , xn, we have:

Indicator clauses : λx1 ∨ λx2 ∨ . . . ∨ λxn
¬λxi ∨ ¬λxj , for i < j

For example, variable C in Figure 1 generates:
λc1 ∨ λc2 ∨ λc3, ¬λc1 ∨ ¬λc2, ¬λc1 ∨ ¬λc3, ¬λc2 ∨ ¬λc3
These clauses ensure that exactly one indicator variable for
C appears in each term of the MLF. The remaining sets of
clauses correspond to network parameters. In particular, for
each parameter θxn|x1,x2,...,xn−1, we have:

IP clause : λx1 ∧ λx2 ∧ . . . ∧ λxn ⇒ θxn|x1,x2,...,xn−1
PI clauses : θxn|x1,x2,...,xn−1 ⇒ λxi, for each i

For example, parameter θc1|a1,b1 in Figure 1 generates:

λa1 ∧ λb1 ∧ λc1 ⇒ θc1|a1,b1
θc1|a1,b1 ⇒ λa1, θc1|a1,b1 ⇒ λb1, θc1|a1,b1 ⇒ λc1
The models of this CNF are in one–to–one correspondence
with the terms of the MLF. In particular, each model of the
CNF will correspond to a unique network variable instantia-
tion, and will set to true only those indicator and parameter
variables which are compatible with that instantiation.

(1)

3 Encoding Techniques
The PREV encoding as discussed does not encode information
about parameter values (local structure). However, it is quite

1Representations such as ADDs and their variations are more re-

stricted representations of MLFs (they can be unfolded into ACs).

ABCf=a+ad+abd+abcdfactor**abcd1++Table 1: —: jointree ran out of memory.

Table 3: CNFs generated by PREV (determinism encoded).

Network
bm-5-3
stud-3-2
mm-4-8-3
mm-3-8-5
bm-22-3
stud-6-24

Max AC Inference
Time (s)
0.0068
0.0052
0.0516
0.6835
4.7000
13.0000

Cluster
23
25
26
54
104
233

Improvement
Over JT
4,028
1,181
1,114
—
—
—

Network
pathﬁnder
water
mildew
munin1
munin4
diabetes

Vars
55229
6630
38540
9551
48864
113527

Parm Vars
54781
6514
37924
8556
43216
108845

Clauses
300576
49367
683552
49363
247582
814412

Literals
821814
152686
1958952
129358
641839
2196008

easy to encode information about determinism within this en-
coding. Consider Figure 1 and the parameter θc1|a1,b1 = 0,
which generates the four clauses in (1). These clauses en-
sure that the parameter θc1|a1,b1 appears in an MLF term iff
that term contains the indicators λa1, λb1 and λc1. How-
ever, given that this parameter is known to be 0, all terms
that contain this parameter must vanish. Therefore, we can
suppress the generation of a Boolean variable for this param-
eter, and then replace the above clauses by a single clause:
¬λa1 ∨¬λb1 ∨¬λc1. This clause has the effect of eliminating
all CNF models which correspond to vanishing terms, those
containing the parameter θc1|a1,b1.

Armed with determinism, the PREV encoding can produce
impressive results when applied to networks with only bi-
nary variables, that contain small CPTs, and that contain large
numbers of 0 parameters. For example, [Chavira et al., 2004]
reports on networks with such properties, generated from re-
lational models, and Table 1 reviews some of these results. As
is clear from the table, one gets exponential improvements
over the standard jointree method, which does not take ad-
vantage of network determinism.2 Similar results have also
been reported in [Darwiche, 2002] with respect to Bayesian
networks corresponding to digital circuits.

For Bayesian networks with local structure, large CPTs, yet
no excessive determinism, the encoding of determinism alone
may not be so effective. Table 2 lists a set of benchmark net-
works, some having variables with large cardinalities, others
having very large CPTs, and where the amount of determin-
ism is not necessarily excessive. Table 3 provides statistics on
the CNFs generated for some of these networks, according to
the PREV encoding, while also encoding determinism as dis-
cussed above. These CNFs are quite large, but the striking
property they have is the large percentage (up to 99% in some
cases) of Boolean variables that represent parameters versus
those representing indicators. Some of these CNFs proved
challenging to factor, some taking too long and others run-
ning out of memory. There are two key observations, how-
ever, that allowed us to handle these networks successfully,
leading to signiﬁcant improvements in both ofﬂine compile
time and online inference time. We explain each of these in
some detail next, but after providing an overview of the CNF
factoring/compilation algorithm of [Darwiche, 2004].

2The technique of “zero compression” can be employed to ex-
ploit determinism in jointrees, but it requires inference on the full
jointree ﬁrst, which is prohibitive in this case.

3.1 How the CNF factoring/compilation process

works

We provide in this section a sketch of the CNF factoring pro-
cess. We note, however, that this section may be skipped on
a ﬁrst reading of the paper as it is not strictly needed for the
following sections.
Consider again the CNF ∆f = Va ∧ (Vb ⇒ Vd) ∧
(Vc ⇒ Vb) from the previous section, and the MLF f =
a + ad + abd + abcd that it encodes. We now brieﬂy de-
scribe the CNF factoring process which allows us to produce
the AC shown in Figure 2. First, the output of the factor-
ing process is shown on the left of Figure 3: It is a logi-
cal form known as negation normal form (NNF) which sat-
isﬁes decomposability (conjunctions do not share variables),
determinism (disjuncts must be logically incompatible), and
smoothness (disjuncts must mention the same sets of vari-
ables). Such a factorization is generated using an exhaustive
version of the DPLL procedure [Davis et al., 1962]. In par-
ticular, the algorithm will pick a variable x in the CNF, will
factor ∆|x and ∆|¬x separately, and then combine the re-
sults into x ∧ f actor(∆|x) ∨ ¬x ∧ f actor(∆|¬x). To im-
prove performance, the algorithm keeps a cache that stores
CNFs that have been factored and their factorizations and
checks this cache before trying to factor a CNF Γ. Finally, be-
fore picking a variable x to split Γ on, the algorithm checks
the CNF to see if it can be broken into disconnected com-
ponents, say α1 and α2. In that case, the algorithm factors
the components separately and combines their results into
f actor(α1) ∧ f actor(α2). The factoring algorithm we use
[Darwiche, 2004], utilizes a decomposition tree (dtree) to
manage this decomposition process. In particular, a dtree for
a CNF is a binary tree whose leaves correspond to the CNF
clauses. Moreover, each node in the dtree is associated with
a set of variables whose instantiation is guaranteed to decom-
pose the CNF into two independent components.

The above procedure generates NNFs that are decompos-
able and deterministic. Smoothness can be established easily
by a postprocessing step. Given an NNF that satisﬁes the
required properties, we can extract an AC by simply replac-
ing conjunctions with multiplications, disjunctions with ad-
ditions, and negative literals with the constant 1. Positive lit-
erals are replaced by the real–valued variables they encode.
This decoding process is shown in Figure 3; see [Darwiche,
2002] for more details.

This factoring algorithm is indeed a logical version of the
recursive conditioning (RC) algorithm [Darwiche, 2001b].3

3Similar algorithms have been used recently to solve probabilis-

Table 2: The networks with which we experimented.

Network
alarm
bm
diabetes
hailﬁnder
mildew
mm
munin1
munin2
munin3
munin4
pathﬁnder
pigs
students
tcc4f
water

Max Clust
7.2
20.0
17.2
11.7
21.4
23.0
26.8
18.6
17.8
21.4
15.0
17.4
22.0
10.0
19.9

Vars
37
1005
413
56
35
1220
189
1003
1044
1041
109
441
376
105
32

Card Ave Card
2.8
2.0
11.3
4.0
17.6
2.0
5.3
5.4
5.4
5.4
4.1
3.0
2.0
2.0
3.6

2-4
2-2
3-21
2-11
3-100
2-2
1-21
2-21
1-21
1-21
2-63
3-3
2-2
2-2
3-4

Total Parms Max CPT Parms Ave CPT Parms %Det %DP
24.6
100.0
17.6
26.9
25.1
75.0
61.2
69.5
71.3
65.3
5.1
23.9
79.3
35.6
57.0

752
6972
461069
3741
547158
8326
19466
83920
85855
98183
97851
8427
2616
3236
13484

108
8
7056
1188
280000
8
600
600
600
600
8064
27
8
512
3072

20
7
1116
67
15633
7
103
84
82
94
898
19
7
31
421

0.9
99.6
78.2
15.7
93.2
98.7
66.5
63.3
63.1
64.5
56.1
56.2
90.7
0.4
54.0

Figure 3: An NNF (left), its encoded AC (middle) and a simpliﬁcation of the AC (right).

One can in principle use RC to compile a Bayesian network
directly into an AC (bypassing the CNF encoding), but the
approach we use allows us to capitalize on the state of the
art in logical reasoning for handling determinism and CSI,
even though it may incur more overhead in the factoring pro-
cess.
In particular, our CNF factoring algorithm uses unit
resolution to propagate logical constraints; conﬂict directed
backtracking to recover more efﬁciently when conditioning
on variable settings that lead to contradictions; in addition to
clause learning as a means for avoiding such contradictions
early on in future conditioning. It also provides a more ﬂexi-
ble framework for exploiting CSI through the use of two tech-
niques. First, it can detect non–structural decompositions,
that is, subproblems that become independent due to condi-
tioning on speciﬁc variable values—such independence can-
not be detected based only on structural considerations (net-
work topology). This is done by removing clauses that be-
come subsumed due to conditioning, therefore, disconnecting
subsets of the CNF under speciﬁc variable values. Second, it
uses a non–structural caching scheme which allows one to
prove the equivalence of subproblems under speciﬁc variable
values (therefore, avoiding multiple factorings of the same
subproblem). Again, these equivalences cannot be proven if
we were to only use structural considerations. Finally, the re-

tic inference using #SAT [Bacchus et al., 2003].

duction to CNF allows one to more naturally accommodate
other types of CPT structures (e.g., decision trees and rules)
without the need for algorithmic change. The reduction to
CNF factoring and the corresponding techniques lead to quite
a bit of overhead in some cases, but this is justiﬁed since
it will only be done once when compiling the network into
an AC. If the application of these techniques lead to smaller
ACs, the compile time can then be amortized over all online
queries. We will illustrate this beneﬁt more concretely in the
experimental results section.

3.2 Encoding parameter equality / CSI
The CPT depicted in Figure 1 has 12 parameters, yet only 5
of these are distinct. Some of the equalities among parame-
ters imply context–speciﬁc independence; others do not. For
example, the equality between parameters in rows 4 − 6 with
those in rows 10− 12 imply that C is independent of A given
B = b2. However, the equality between parameters in rows 2
and 3 do not imply a CSI.

From a purely encoding viewpoint, one would clearly want
to exploit parameter equality, at least to reduce the number of
Boolean variables one must generate. Table 2 shows the ex-
tent to which parameter equality can help in this regard. In
particular, the table reports as %DP the percentage of distinct
parameters among non–extreme parameters. That is, the per-

VbVcVd¬Vd¬Vc¬Vb^^vv^^^vVabcd111**++***+abcd1+**+aDecodeReducecentage of parameters that would remain if, for each CPT,
we collapsed equal, non–extreme parameters into a single pa-
rameter. The dramatic example here is pathﬁnder: about half
of its parameters are extreme, and among the other half, only
about 5% are distinct within their CPTs. In addition to gener-
ating smaller CNFs, encoding parameter equality allows the
compiler to run with less overhead, and to generate smaller
ACs since parameter equality provides more opportunities for
factoring, which immediately translates to gains in online in-
ference.

A key observation here is that no two parameters in the
same CPT can ever appear in the same MLF term, as they
correspond to incompatible network instantiations. This ob-
servation suggests that we can use the same Boolean variable
to represent multiple parameters, assuming that such param-
eters have equal values and appear in the same CPT. How-
ever, the idea will not work when applied to PREV. Consider
again the CPT in Figure 1. If we use the same variable θ to
represent parameters θc2|a1,b1 and θc3|a1,b2, which are both
equal to .5, we would get the following two PI clauses in the
CNF: θ ⇒ λb1 and θ ⇒ λb2, which is inconsistent with other
clauses. More generally, PI clauses assert that a parameter
implies the corresponding family instantiation. Therefore, if
we simply use the same Boolean variable to represent equal
parameters, we would be implying inconsistent family instan-
tiations.4

The solution we adopt is to drop PI clauses from the encod-
ing! Note that dropping PI clauses introduces additional (un-
intended) models into the CNF, allowing MLF terms which
contain multiple parameters from the same CPT. These unin-
tended models/terms, however, can be easily ﬁltered during
the decoding process given the following.
Theorem 1 Consider a Bayesian network with n variables.
Let ∆ be a CNF encoding which includes the indicator, IP
and PI clause sets, and let Γ be the CNF encoding which in-
cludes the indicator and IP clause sets only. Then ∆’s models
have cardinality 2n and are a subset of Γ’s models. Moreover,
if σ is a model of Γ but not ∆, then σ’s cardinality is > 2n.
Therefore, unintended models have a higher cardinality than
original models (which all have the same cardinality). As it
turns out, if Γ is an NNF which satisﬁes decomposability, de-
terminism and smoothness, one can in linear time obtain an-
other NNF Γ0 whose models are exactly the minimum cardi-
nality models of Γ and which satisﬁes the required properties
[Darwiche, 2001a]. Therefore, we can safely drop PI clauses
as long we minimize the resulting NNF before we decode the
AC.

By including only indicator and IP clauses, we can now
safely represent all equal parameters within the same CPT
by a single Boolean variable in the CNF encoding. For the
Pathﬁnder network for example, this drops the number of
Boolean variables needed to represent non–extreme param-
eters from 42, 946 to 2, 186, a 95% reduction! Similar re-
ductions are obtained for many other networks; see Table 2.
As we show later, not only does this technique improve the
compilation time, but can lead to signiﬁcantly smaller ACs.

4A restricted case of encoding parameter equality was discussed

in [Darwiche, 2002].

3.3 A more informed factoring algorithm
The CNF factoring algorithm employs two key techniques as
discussed earlier. The ﬁrst is variable splitting, which can be
thought of as doing case analysis. The second is caching, so
that one can avoid factoring the same CNF subset multiple
times. Which variables the algorithm ends up splitting on can
very much affect its running time, and the size of factoriza-
tions it generates. Moreover, the complexity of the caching
scheme is proportional to the number of variables appearing
in the cached CNF subset, as the state of such variables are
used to generate keys that uniquely deﬁne CNFs. The fol-
lowing observations state interesting properties of our CNF
encodings, which if passed to the factoring algorithm can sig-
niﬁcantly improve both the splitting and caching processes.

First, if two clauses share a parameter variable, then they
must also share indicators over the same network variable.
This property, and the presence of indicator clauses, allow the
CNF factoring algorithm to restrict its splitting to indicator
variables, which would be sufﬁcient to decompose the prob-
lem into independent components (hence, no splitting/case
analysis is needed on parameter variables). Second, given
the structure of indicator and IP clauses, the state of indicator
variables are sufﬁcient to characterize the state of parameter
variables. This property allows us to only involve indicator
variables when generating CNF keys during the caching pro-
cess. Both of the above optimizations can be exploited by
simply identifying parameter variables to the factoring algo-
rithm.

Another technique that we have used in some of the ex-
periments involves the construction of a decomposition tree
(dtree) for the given Bayesian network, and then converting it
into a dtree for its CNF encoding. A dtree for a Bayesian net-
work is simply a binary tree whose leaves correspond to the
network CPTs [Darwiche, 2001b]. A dtree for a CNF is also
a binary tree, but its leaves correspond to the CNF clauses.
Since each clause in the CNF encoding is generated by a CPT,
we can convert a network dtree into a CNF dtree by simply
unfolding the dtree node corresponding to a CPT into a sub-
tree whose leaves correspond to the clauses generated by that
CPT. The main point of this technique is to more efﬁciently
generate dtrees for very large CNF encodings that are gener-
ated by Bayesian networks with a small number of CPTs (this
happens when the network contains very large CPTs).

3.4 Other Optimizations
Our CNF encodings utilize some additional enhancements,
two of which are described next. First, we deﬁne a new type
of clause, called an eclause, which has the same syntax as a
regular clause but stronger semantics: it asserts that exactly
one of it literals is true. We use eclauses for representing in-
dicator clauses, therefore reducing the size of CNFs consid-
erably in networks having multi–valued variables. Moreover,
we outﬁt the DPLL procedure used in factoring the CNF to
work directly with eclauses, without having to unfold them
into regular clauses. For another optimization example, the
indicators and parameters corresponding to the same state of
a root variable are logically equivalent, making it possible to
delete the parameter variables and the corresponding IP and
PI clauses, which establish the equivalence.

4 Experimental Results
Experiments ran on a 1.6GHz Pentium M with 2GB of RAM,
using the networks in Table 2. Two important columns in Ta-
ble 2 are %Det, which is the percent of parameters that are
equal to 0 or to 1 and %DP, which is the percent of non–
extreme parameters remaining after collapsing equal param-
eters within the same CPT. These two values give an idea
of the amounts of local structure in the form of determinism
and possibly CSI. bm-5-3, mm-3-8-3, and stud-3-2 are net-
works on which PREV was already shown to perform well by
only encoding determinism [Chavira et al., 2004]. These net-
works contain only binary variables, are highly deterministic,
and have small CPTs (no more than two parents per node). In
contrast, the other networks contain variables with higher car-
dinalities and lesser degrees of determinism and, sometimes,
very large CPTs. These networks came from various sources:
http://www2.sis.pitt.edu/∼genie; Hughes Research Labs; and
http://www.cs.huji.ac.il/labs/compbio/Repository.

The experiments serve to demonstrate three points. First,
the new CNF encoding and the additional information we
pass to the CNF factoring algorithm lead to signiﬁcant im-
provement, both in the factoring time and the size of resulting
factorizations. Table 4 (1st three columns) illustrates the im-
provement in factoring time, showing order of magnitude im-
provements in some cases and allowing us to factor some net-
works for the ﬁrst time under the given memory constraints.
The second point illustrated by our experiments concerns
the quality of factorizations (ACs) we obtain, compared to the
ones embedded in jointrees. Recall that every jointree em-
beds an AC [Park and Darwiche, 2003], whose size depends
only on the jointree size and, hence, is not dependent on local
structure. Our experiments are therefore set to illustrate the
extent to which local structure can help the factorization pro-
cess. Table 4 illustrates signiﬁcant improvements in AC size,
as we obtained one to two orders of magnitude improvement
on networks such as Water, Pathﬁnder, Munin1, and Munin4.
For a more direct measure of improvement in online infer-
ence,5 we generated sixteen random sets of evidence, and for
each evidence set, we computed the marginal of each network
variable using jointree propagation and then using AC eval-
uation/differentiation.6 Table 4 shows the obtained improve-
ments in online inference, which are similar to improvements
in AC size.

Note that since the AC is compiled independently of evi-
dence, the improvements apply for computing all marginals
regardless of evidence. This is especially useful for tasks that
apply a massive number of queries to Bayesian networks,
including parameter estimation algorithms (e.g., EM) given
known local structure in the form of determinism and CSI,
and MAP algorithms based on branch&bound search.

[Poole and Zhang, 2003] present another approach for ex-
ploiting local structure, by providing a reﬁnement on variable
elimination (VE). Their approach does not involve a compi-

5Online inference time is affected by the number of AC nodes
too (not reported), and by the structure of ACs which affects locality
of reference (the ACs embedded in jointrees have better locality).

6AC differentiation provides more information that

just

marginals [Darwiche, 2003].

Table 5: Effect of local structure on AC size (edge count).

Local structure
Det/Equal Parms
Det only
Equal Parms only
No local structure
Jointree

Pathﬁnder

42,810
130,380
200,787
784,330
981,178

Water
134,140
138,501

11,111,104
15,305,634
13,777,166

Munin4
5,762,690
9,997,267
17,612,036

—

116,136,985

lation step, and their results are sensitive to the given queries,
so a direct comparison between the two approaches is not too
meaningful. Yet, we mention here that on a re-parameterized
version of Water (to introduce local structure), [Poole and
Zhang, 2003] show a factor of 4 improvement over standard
VE (total time over all queries; the speedup was more or less
depending on the query). Water is the only network from Ta-
ble 4 that [Poole and Zhang, 2003] report on. Note that the
exploitation of local structure can incur overhead that may
not be justiﬁable unless the savings due to local structure are
signiﬁcant enough (not all of the cases in Table 4 lead to sig-
niﬁcant savings). This appears to be less an of an issue in our
approach, since the overhead is pushed into the compilation
step. However, in [Poole and Zhang, 2003] which does not
involve a compilation step, this overhead is incurred in every
query which may or may not lead to overall savings depend-
ing on the query—being query speciﬁc, however, may some-
times pay off quite signiﬁcantly, since work performed can
sometimes be simpliﬁed given speciﬁc evidence and speciﬁc
query variables.

Our ﬁnal point regards the effect of local structure on the
quality of factorizations. Consider Table 5, which shows AC
sizes under different encodings of local structure. First, it is
obvious that encoding local structure is responsible for the
signiﬁcant improvements on these networks. Second, in Wa-
ter, determinism appears to be the main responsible factor.
However, in Pathﬁnder and Munin4, parameter equality alone
(without determinism) is sufﬁcient to bring about most of the
reported improvements, even though determinism alone can
have a similar effect too. Note that there is some overlap be-
tween determinism and parameter equality since by encoding
determinism (0 parameters), one is effectively collapsing all
0 parameters (applying implicit parameter equality). The re-
sults for Pathﬁnder and Munin4 are therefore not surprising,
suggesting possibly that parameter equality is more funda-
mental than determinism for these networks.

5 Conclusion

We considered the problem of compiling Bayesian networks
into ACs. Our aim was to efﬁciently compile networks having
local structure, yet large CPTs and no excessive determinism.
We proposed a new encoding scheme that facilitates the rep-
resentation of local structure in the form of parameter equal-
ity, and identiﬁed some of its properties that improve com-
pile time. Our results demonstrate signiﬁcant improvements
in both ofﬂine/compile and online/inference time, leading to
orders of magnitude improvement in online inference.

Network

alarm
bm-5-3
diabetes
hailﬁnder
mildew
mm-3-8-3
munin1
munin2
munin3
munin4
pathﬁnder
pigs
students-3-2
tcc4f.obfuscated
water

Table 4: Comparing the new and prev encodings with the jointree baseline.
Ofﬂine Compile Time (s)
PREV
0.93
2.51

AC Edge Count

NEW Improv.
2,686
1.8
4,629.1
18,693
3.0
13,585,023
15,687
2.3
2,123,309
7.6
371.6
263,835
34.2
30,620,744
5.1
4,791,974
2,436,598
5.9
20.2
5,762,690
22.8
43,064
1,302,215
1.8
1,385.0
27,292
22,284
1.3
102.7
134,142

Online Inference Time (s)
Jointree
0.07
165.15
37.09
0.13
13.39
248.15
1,321.68
28.32
18.69
137.94
1.68
3.45
55.98
0.14
22.68

NEW Improv.
6.4
0.01
1,814.8
0.09
2.3
16.27
0.06
2.2
4.0
3.35
1,181.6
0.21
29.4
44.91
4.3
6.59
3.65
5.1
17.9
7.70
23.7
0.07
2.2
1.60
799.7
0.07
0.05
2.8
107.5
0.21

2.26

7.44

Jointree
NEW Improv.
4,804
1.8
0.52
86,532,336
2.3
1.11
40,673,307
—
— 2,269.05
36,342
0.86
2.6
16,171,408
—
— 7,483.80
4.0
98,044,208
1.87
— 1,047,211,866
— 1,534.97
24,281,678
225.46
151.72
14,333,412
116,136,985
677.92
981,178
20.36
2,347,299
17.84
37,799,472
0.82
1.15
29,064
13,777,166
4.83

14.4
10.2
3.6
11.1
6.2
1.9
3.6
7.2

3,248.42
1,553.43
2,440.30
226.37
110.10
1.53
4.11
34.82

[Jensen et al., 1990] F. V. Jensen, S.L. Lauritzen, and K.G.
Olesen. Bayesian updating in recursive graphical models
by local computation. Computational Statistics Quarterly,
4:269–282, 1990.

[Park and Darwiche, 2003] James Park and Adnan Dar-
wiche. A differential semantics for jointree algorithms.
NIPS 15, volume 1, pages 299–307. MIT Press, 2003.

[Poole and Zhang, 2003] D. Poole and N.L. Zhang. Exploit-
ing contextual independence in probabilistic inference.
Journal of Artiﬁcial Intelligence, 18:263–313, 2003.

[Shenoy and Shafer, 1986] Prakash P. Shenoy and Glenn
Shafer. Propagating belief functions with local computa-
tions. IEEE Expert, 1(3):43–52, 1986.

Acknowledgments
We thank the reviewers for commenting on an earlier version
of this paper. This work has been partially supported by NSF
grant IIS-9988543 and MURI grant N00014-00-1-0617.

References
[Bacchus et al., 2003] Fahiem Bacchus, Shannon Dalmao,
and Toniann Pitassi. Algorithms and complexity results
for #SAT and Bayesian inference. In FOCS, pages 340–
351, 2003.

[Boutilier et al., 1996] Craig Boutilier, Nir Friedman,
Mois´es Goldszmidt, and Daphne Koller. Context–speciﬁc
independence in bayesian networks.
In UAI’96, pages
115–123, 1996.

[Chavira et al., 2004] M. Chavira, A. Darwiche,

and
M. Yaeger. Compiling relational bayesian networks for
exact inference. In PGM-2004, pages 49–56, 2004.

[Davis et al., 1962] Martin Davis, George Logemann, and
Donald Loveland. A machine program for theorem prov-
ing. Journal of the ACM, (5)7:394–397, 1962.

[Darwiche, 2001a] Adnan Darwiche. Decomposable nega-
tion normal form. Journal of the ACM, 48(4):608–647,
2001.

[Darwiche, 2001b] Adnan Darwiche. Recursive condition-

ing. Artiﬁcial Intelligence, 126(1-2):5–41, 2001.

[Darwiche, 2002] Adnan Darwiche. A logical approach to
In Proceedings of KR, pages

factoring belief networks.
409–420, 2002.

[Darwiche, 2003] Adnan Darwiche. A differential approach
to inference in bayesian networks. JACM, 50(3):280–305,
2003.

[Darwiche, 2004] Adnan Darwiche. New advances in com-
piling CNF to decomposable negational normal form. In
ECAI’2004.


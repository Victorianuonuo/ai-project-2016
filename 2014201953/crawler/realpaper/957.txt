Quality Guarantees on k-Optimal Solutions for Distributed Constraint

Optimization Problems

Jonathan P. Pearce and Milind Tambe∗

University of Southern California
Computer Science Department
{jppearce@usc.edu, tambe@usc.edu}

Abstract

A distributed constraint optimization problem
(DCOP) is a formalism that captures the rewards
and costs of local interactions within a team of
agents. Because complete algorithms to solve
DCOPs are unsuitable for some dynamic or any-
time domains, researchers have explored incom-
plete DCOP algorithms that result in locally opti-
mal solutions. One type of categorization of such
algorithms, and the solutions they produce, is k-
optimality; a k-optimal solution is one that cannot
be improved by any deviation by k or fewer agents.
This paper presents the ﬁrst known guarantees on
solution quality for k-optimal solutions. The guar-
antees are independent of the costs and rewards in
the DCOP, and once computed can be used for any
DCOP of a given constraint graph structure.

Introduction

1
In a large class of multi-agent scenarios, a set of agents
chooses a joint action as a combination of individual ac-
tions. Often, the locality of agents’ interactions means that
the utility generated by each agent’s action depends only
on the actions of a subset of the other agents.
In this
case, the outcomes of possible joint actions can be com-
pactly represented in cooperative domains by a distributed
constraint optimization problem (DCOP) [Modi et al., 2005;
Zhang et al., 2005a]. A DCOP can take the form of a graph in
which each node is an agent and each edge denotes a subset of
agents whose actions, taken together, incur costs or rewards
to the agent team. Applications of DCOP include sensor net-
works [Modi et al., 2005], meeting scheduling [Petcu and
Faltings, 2005] and RoboCup soccer [Vlassis et al., 2004].

Globally optimal DCOP algorithms can incur large com-
putation or communication costs for domains where the num-
ber of agents is large or where time is limited. However, in-
complete algorithms in which agents react on the basis of lo-
cal knowledge of neighbors and constraint utilities can lead
∗This material is based upon work supported by the Defense Ad-
vanced Research Projects Agency (DARPA), through the Depart-
ment of the Interior, NBC, Acquisition Services Division, under
Contract No. NBCHD030010.

to a system that scales up easily and is more robust to dy-
namic environments. Researchers have introduced k-optimal
algorithms in which small groups of agents optimize based
on their local constraints, resulting in a k-optimal DCOP as-
signment, in which no subset of k or fewer agents can im-
prove the overall solution. Some examples include the 1-
optimal algorithms DBA [Yokoo and Hirayama, 1996] and
DSA [Fitzpatrick and Meertens, 2003] for distributed con-
straint satisfaction problems (DisCSPs), which were later ex-
tended to DCOPs [Zhang et al., 2005a], as well as the 2-
optimal algorithms in [Maheswaran et al., 2004], in which
optimization was done by agents acting in pairs. Previous
work has focused on upper bounds on the number of k-
optima in DCOPs [Pearce et al., 2006], as well as experi-
mental analysis of k-optimal algorithms [Zhang et al., 2005a;
Maheswaran et al., 2004].

Unfortunately, the lack of theoretical guarantees on the
quality of solutions obtained by k-optimal algorithms was a
fundamental limitation; until now, we could not guarantee a
lower bound on the quality of the solution obtained with re-
spect to the quality of the global optimum. In this paper, we
introduce such guarantees. These guarantees can help deter-
mine an appropriate k-optimal algorithm, or possibly an ap-
propriate constraint graph structure, for agents to use in sit-
uations where the cost of coordination between agents must
be weighed against the quality of the solution reached. If in-
creasing the value of k will provide a large increase in guaran-
teed solution quality, it may be worth the extra computation or
communication required to reach a higher k-optimal solution.
For example, consider a team of autonomous underwater ve-
hicles (AUVs) [Zhang et al., 2005b] that must quickly choose
a joint action in order to observe some transitory underwa-
ter phenomenon. The combination of individual actions by
nearby AUVs may generate costs or rewards to the team, and
the overall utility of the joint action is determined by their
sum.
If this problem were represented as a DCOP, nearby
AUVs would share constraints in the graph, while far-away
AUVs would not. However, the actual rewards on these con-
straints may not be known until the AUVs are deployed, and
in addition, due to time constraints, an incomplete, k-optimal
algorithm, rather than a complete algorithm, must be used to
ﬁnd a solution.
In this case, worst-case quality guarantees
for k-optimal solutions for a given k, that are independent of
the actual costs and rewards in the DCOP, are useful to help

IJCAI-07

1446

R

12

0

1

0

10

0

1

0

5

R

23

0

1

0

20

1

0

0

11

1

2

3

Figure 1: DCOP example

decide which algorithm to use. Alternatively, the guarantees
can help to choose between diﬀerent AUV formations, i. e.
diﬀerent constraint graphs.

We present two distinct types of guarantees for k-optima.
The ﬁrst, in Sections 3 and 4, is a lower bound on the qual-
ity of any k-optimum, expressed as a fraction of the quality
of the optimal solution. The second, in Section 5, is a lower
bound on the proportion of all DCOP assignments that a k-
optimum must dominate in terms of quality. This type is use-
ful in approximating the diﬃculty of ﬁnding a better solution
than a given k-optimum. For both, we provide general bounds
that apply to all constraint graph structures, as well as tighter
bounds made possible if the graph is known in advance.
2 DCOP and k-optima
We consider a DCOP in which each agent controls a variable
to which it must assign a value. Constraints exist on subsets
of these variables; each constraint generates a cost or reward
to the team based on the values assigned to each variable in
the corresponding subset. Although we assume in this paper
that each agent controls a single variable, all results are valid
for cases in which agents control more than one variable.
Formally, a DCOP is a set of variables (one per agent)
N := {1, . . . , n} and a set of domains A := {A1, . . . ,An},
where the ith variable takes value ai ∈ Ai. We denote the
assignment of the multi-agent team by a = [a1 ··· an]. Val-
ued constraints exist on various subsets S ⊂ N of these vari-
ables. A constraint on S is expressed as a reward function
RS (a). This function represents the reward generated by the
constraint on S when the agents take assignment a; costs are
expressed as negative rewards. θ is the set of all such subsets
S on which a constraint exists, and no S ∈ θ is a subset of any
other S ∈ θ. For convenience, we will refer to these subsets S
as “constraints” and the functions RS (·) as “constraint reward
functions.” The solution quality for a particular complete as-
signment a is the sum of the rewards for that assignment from
all constraints in the DCOP: R(a) =
In [Pearce et al., 2006], the deviating group between two
assignments, a and ˜a, was deﬁned as D(a, ˜a) := {i ∈ N :
ai (cid:2) ˜ai}, i.e.
the set of variables whose values in ˜a diﬀer
from their values in a. The distance between two assignments
was deﬁned as d(a, ˜a) := |D(a, ˜a)| where | · | denotes the size
of the set. An assignment a is classiﬁed as a k-optimum if
R(a) − R(˜a) ≥ 0 ∀˜a such that d(a, ˜a) ≤ k. Equivalently, at
a k-optimum, no subset of k or fewer agents can improve the
overall reward by choosing diﬀerent values; every such subset
is acting optimally given the values of the others.
Example 1 Figure 1 is a binary DCOP in which agents
choose values from {0, 1}, with constraints S 1 = {1, 2} and
S 2 = {2, 3} with rewards shown. The assignment a = [1 1 1]
is 1-optimal because any single agent that deviates reduces
the team reward. However, [1 1 1] is not 2-optimal because if

S∈θ RS (a).

(cid:2)

the group {2, 3} deviated, making the assignment ˜a = [1 0 0],
team reward would increase from 16 to 20. The globally op-
timal solution, a∗ = [0 0 0] is k-optimal for all k ∈ {1, 2, 3}.(cid:2)
In addition to categorizing local optima, k-optimality pro-
vides a natural classiﬁcation for DCOP algorithms. Many
algorithms are guaranteed to converge to k-optima, including
DBA [Zhang et al., 2005a], DSA [Fitzpatrick and Meertens,
2003], and coordinate ascent [Vlassis et al., 2004] for k = 1,
and MGM-2 and SCA-2 [Maheswaran et al., 2004] for k = 2.
Globally optimal algorithms such as Adopt [Modi et al.,
2005], OptAPO [Mailler and Lesser, 2004] and DPOP [Petcu
and Faltings, 2005] converge to a k-optimum for k = n.
3 Quality guarantees on k-optima
This section provides reward-independent guarantees on so-
lution quality for any k-optimal DCOP assignment.
If we
must choose a k-optimal algorithm for agents to use, it is use-
ful to see how much reward will be gained or lost in the worst
case by choosing a higher or lower value for k. We assume
the actual costs and rewards on the DCOP are not known a
priori (otherwise the DCOP could be solved centrally ahead
of time). We provide a guarantee for a k-optimal solution as a
fraction of the reward of the optimal solution, assuming that
all rewards in the DCOP are non-negative (the reward struc-
ture of any DCOP can be normalized to one with all non-
negative rewards as long as no inﬁnitely large costs exist).
Proposition 1 For any DCOP of n agents, with maximum
constraint arity of m, where all constraint rewards are non-
negative, and where a∗ is the globally optimal solution, then,
for any k-optimal assignment, a, where m ≤ k < n,

∗

).

(1)

(cid:3)
(cid:4)
n−m
(cid:3)
(cid:4) R(a
k−m
−
n−m
k

R(a) ≥

(cid:3)

(cid:4)

n
k

Proof: By the deﬁnition of k-optimality, any assignment ˜a
such that d(a, ˜a) ≤ k must have reward R(˜a) ≤ R(a). We call
this set of assignments ˜A. Now consider any non-null subset
ˆA ⊂ ˜A. For any assignment ˆa ∈ ˆA, the constraints θ in the
DCOP can be divided into three discrete sets, given a and ˆa:
• θ1(a, ˆa) ⊂ θ such that ∀S ∈ θ1(a, ˆa), S ⊂ D(a, ˆa).
• θ2(a, ˆa) ⊂ θ s.t. ∀S ∈ θ2(a, ˆa), S ∩ D(a, ˆa) = ∅.
• θ3(a, ˆa) ⊂ θ s.t. ∀S ∈ θ3(a, ˆa), S (cid:3) θ1(a, ˆa) ∪ θ2(a, ˆa).
θ1(a, ˆa) contains the constraints that include only the vari-
ables in ˆa which have deviated from their values in a; θ2(a, ˆa)
contains the constraints that include only the variables in ˆa
which have not deviated from a; and θ3(a, ˆa) contains the con-
straints that include at least one of each. Thus:
(cid:5)
S∈θ3(a,ˆa)
(cid:5)
S∈θ3(a,ˆa)

(cid:5)
And, the sum of rewards of all assignments ˆa in ˆA is:
ˆa∈ ˆA

(cid:5)
S∈θ2(a,ˆa)
(cid:5)
RS (ˆa) +
RS (ˆa) +
(cid:5)
(cid:5)
S∈θ2(a,ˆa)
S∈θ2(a,ˆa)
ˆa∈ ˆA

(cid:5)
S∈θ1(a,ˆa)
(cid:3) (cid:5)
(cid:5)
(cid:5)
(cid:5)
S∈θ1(a,ˆa)
ˆa∈ ˆA
S∈θ1(a,ˆa)
ˆa∈ ˆA

R(ˆa) =
≥

RS (ˆa) +

RS (ˆa).

R(ˆa) =

RS (ˆa) +

RS (ˆa) +

RS (ˆa).

(cid:4)

RS (ˆa)

IJCAI-07

1447

Since R(a) > R(ˆa),∀ˆa ∈ ˆA,

(cid:2)

(cid:2)
ˆa∈ ˆA

R(a) ≥

S∈θ1(a,ˆa) RS (ˆa) +
| ˆA|

(cid:2)

ˆa∈ ˆA

(cid:2)
S∈θ2(a,ˆa) RS (ˆa)

.

(2)

k

(cid:4)

(cid:4)

(cid:4)

k−|S|

k−|S|

k−|S|

d(a,a∗)−|S|

(cid:3)
d(a,a∗)

RS (a∗) ≥

(cid:3)
d(a,a∗)−|S|

Now, if the two numerator terms and the denominator can
be expressed in terms of R(a∗) and R(a), then we have a bound
on R(a) in terms of R(a∗). To do this, we consider the partic-
ular ˆA which contains all assignments ˆa such that d(a, ˆa) = k,
and ∀ˆa ∈ ˆA,∀ˆai ∈ D(a, ˆa), ˆai = a∗
i . This means that exactly
k variables in ˆa have deviated from their value in a, and these
variables are taking the same values that they had in a∗.
assignments ˆa ∈ ˆA. For every constraint
There are
S ∈ θ, there are exactly
diﬀerent assignments ˆa ∈ ˆA
for which S ∈ θ1(a, ˆa). This is because there exists a unique
If S ⊂
ˆa ∈ ˆA for every subset of k variables in D(a, a∗).
D(a, ˆa), as stipulated by the deﬁnition of θ1(a, ˆa), then there
(cid:4)
(cid:3)
are d(a, a∗)−|S| remaining variables from which k −|S| must
d(a,a∗)−|S|
be chosen to complete D(a, ˆa), and so there are
(cid:2)
(cid:2)
possible assignments ˆa for which this is true. For all ˆa, for
(cid:4)
(cid:3)
(cid:3)
all S ∈ θ1(a, ˆa), RS (ˆa) = RS (a∗), so
(cid:2)
S∈θ1(a,ˆa) RS (ˆa) =
ˆa∈ ˆA
(cid:4)
(cid:3)
d(a,a∗)−m
R(a∗).
S∈θ
Similarly, for every constraint S ∈ θ, there are
d(a,a∗)−|S|
diﬀerent assignments ˆa ∈ ˆA for which S ∈ θ2(a, ˆa).
If
S ∩D(a, ˆa) = ∅, as stipulated by the deﬁnition of θ2(a, ˆa), then
(cid:4)
there are d(a, a∗)−|S| remaining variables from which k must
be chosen to complete D(a, ˆa), and so there are
pos-
(cid:2)
sible assignments ˆa for which this is true. For all ˆa, for all
(cid:4)
(cid:3)
S ∈ θ2(a, ˆa), RS (ˆa) = RS (a), and, so
(cid:2)
S∈θ2(a,ˆa) RS (ˆa) =
d(a,a∗)−m
S∈θ
R(a).
(cid:3)
(cid:4)
Therefore, from Equation 2,
d(a,a∗)−m
R(a∗) +
(cid:4) R(a
(cid:4)
(cid:3)
R(a) ≥
)
d(a,a∗)
which is minimized when d(a, a∗) = n, so Equation 1 holds
as a guarantee for a k-optimum in any DCOP. It is possible
that k > n − m; in this case we take
For binary DCOPs (m = 2), Equation 1 simpliﬁes to:
(k − 1)

(cid:4)
RS (a) ≥
d(a,a∗)−|S|
(cid:4)
(cid:3)
d(a,a∗)−m

(cid:3)
d(a,a∗)
(cid:4)

(cid:4)
d(a,a∗)−m

d(a,a∗)−m
(cid:4)
(cid:3)
k−m
−

to be 0. (cid:3)

d(a,a∗)−|S|

n−m
k

(cid:2)

k−m

R(a)

ˆa∈ ˆA

k−m

≥

(cid:3)

(cid:3)

(cid:3)

(cid:3)

∗

k

k

k

k

k

k

k

k

R(a) ≥

(2n − k − 1)

∗

R(a

).

k

The following example illustrates Proposition 1:
Example 2 Consider a DCOP with ﬁve variables numbered
1 to 5, with domains of {0,1}, fully connected with binary
constraints between all variable pairs. Suppose that a =
[0 0 0 0 0] is a 3-optimum, and that a∗ = [1 1 1 1 1]
(cid:3)
(cid:4)
is the global optimum. Then d(a, a∗) = 5, and ˆA contains
d(a,a∗)
= 10 assignments: [1 1 1 0 0], [1 1 0 1 0], [1 1 0 0 1],
[1 0 1 1 0], [1 0 1 0 1], [1 0 0 1 1], [0 1 1 1 0], [0 1 1 0 1],
[0 1 0 1 1], [0 0 1 1 1]. Whatever the values of the rewards are,
every constraint reward RS (a∗) will equal RS (ˆa) for
= 3
assignments in ˆA (e.g. R{1,2}(a∗) = R{1,2}(ˆa) for ˆa = [1 1 1 0 0],
(cid:3)
[1 1 0 1 0], and [1 1 0 0 1]) and similarly, every constraint re-
n−2
= 1 assignment in ˆA. Thus,
ward RS (a) equals RS (ˆa) for
R(a) ≥ 3
k
3 R(a∗).(cid:2)

10−1 R(a∗) = 1

(cid:4)
(cid:3)
n−2
k−2

(cid:4)

We now show that Proposition 1 is tight, i.e. that there exist
DCOPs with k-optima of quality equal to the bound.
Proposition 2 ∀n, m, k such that m ≤ k < n, there exists
some DCOP with n variables, with maximum constraint ar-
ity m with a k-optimal assignment, a, such that, if a∗ is the
globally optimal solution,

(cid:4)
(cid:3)
n−m
(cid:3)
(cid:4) R(a
k−m
−
n−m
k

R(a) =

(cid:3)

(cid:4)

n
k

∗

).

(3)

Proof: Consider a fully-connected m-ary DCOP where the
domain of each variable contains at least two values {0,1} and
every constraint RS contains the following reward function:

RS (a) =

(n−m
k−m)
k)−(n−m
(n
k )
1
0

,∀i ∈ S , ai = 0
,∀i ∈ S , ai = 1
, otherwise
= 1,∀i. If a is deﬁned such
The optimal solution a∗ is a∗
that ai = 0,∀i, then Equation 3 is true. Now we show that a
is k-optimal. For any assignment ˆa, such that d(a, ˆa) = k,

i

⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩

(cid:5)
R(ˆaS ) +
S∈θ1(a,ˆa)
(cid:10)
(cid:11)
(cid:10)
(cid:11)
n − k
(cid:3)
m

k
m

+

R(ˆaS ) +

(cid:5)
(cid:3)
(cid:4)(cid:12)(cid:3)
(cid:4)
S∈θ3(a,ˆa)
n
k
m
k

(cid:5)
(cid:3)
(cid:4)
S∈θ2(a,ˆa)
n−m
n−m
(cid:4)
(cid:3)
k−m
k−m
−
n−m
k
÷ n!(n − m − k)! − (n − m)!(n − k)!

(cid:3)
(cid:4)(cid:13)
n−k
(cid:3)
(cid:4)
+
m
n−m
k

R(ˆaS ).
(cid:3)
−
n−m
(cid:3)
(cid:4)
k
−

(cid:4) + 0 =

(cid:4)(cid:3)

n
k

(cid:4)

n
k

R(ˆa) =

≤

=

=

=

=

n!

k!(n − k)!(n − m − k)!

m!(k − m)!(n − k)!
m!(k − m)![n!(n − m − k)! − (n − k)!(n − m)!]
(cid:10)
(cid:11)
n
(k − m)![n!(n − m − k)! − (n − m)!(n − k)!]
(cid:4)
(cid:11) (cid:3)
(cid:10)
m
n−m
(cid:4) = R(a)
k−m
n!(n−m−k)!
(n−k)!

n!k!(n − m − k)!
(n − m)!k!(n − m − k)!
(cid:3)
n−m
(cid:3)
k−m
−

k!(n − m − k)!
− (n − m)!
(cid:3)

(cid:4)
n−m
k

n
m

n
m

(cid:4)

(cid:10)

(cid:11)

(cid:3)

(cid:4)

=

n
k

n
m

constraints in the DCOP are
because in a, each of the
producing the same reward. Since this can be shown for
d(a, ˆa) = j,∀ j such that 1 ≤ j ≤ k, a is k-optimal. (cid:3)
4 Graph-based quality guarantees
The guarantee for k-optima in Section 3 applies to all possible
DCOP graph structures. However, knowledge of the structure
of constraint graphs can be used to obtain tighter guarantees.
This is done by again expressing the two numerator terms in
Equation 2 as multiples of R(a∗) and R(a). However, for a
sparse graph, if ˆA is chosen as deﬁned in Proposition 1, there
may be many assignments ˆa ∈ ˆA that have few or no con-
straints S in θ1(a, ˆa) because the variables in D(a, ˆa) may not
share any constraints. Instead, exploiting the graph structure
by choosing a smaller ˆA can lead to a tighter bound. We can
take ˆA from Proposition 1, i.e.
ˆA which contains all ˆa such
that d(a, ˆa) = k and ∀ˆa ∈ ˆA,∀ˆai ∈ D(a, ˆa), ˆai = a∗
i . Then,
we restrict this ˆA further, so that ∀ˆa ∈ ˆA, the variables in
D(a, ˆa) form a connected subgraph of the DCOP graph (or
hypergraph), meaning that any two variables in D(a, ˆa) must
be connected by some chain of constraints. This allows us to
again transform Equation 2 to express R(a) in terms of R(a∗);

IJCAI-07

1448

this new method can produce tighter guarantees for k-optima
in sparse graphs. As an illustration, provably tight guaran-
tees for binary DCOPs on ring graphs (each variable has two
constraints) and star graphs (each variable has one constraint
except the central variable, which has n − 1) are given below.
Proposition 3 For any binary DCOP of n agents with a
ring graph structure, where all constraint rewards are non-
negative, and a∗ is the globally optimal solution, then, for
any k-optimal assignment, a, where k < n,

R(a) ≥ k − 1

∗

).

R(a

k + 1

(4)
Proof: Returning to Equation 2, | ˆA| = n because D(a, ˆa)
could consist of any of the n connected subgraphs of k vari-
ables in a ring. For any constraint S ∈ θ, there are k − 1
assignments ˆa ∈ ˆA for which S ∈ θ1(a, ˆa) because there are
(cid:2)
k− 1 connected subgraphs of k variables in a ring that contain
S∈θ1(a,ˆa) RS (ˆa) = (k−1)R(a∗). Also, there
S . Therefore,
are n−k−1 assignments ˆa ∈ ˆA for which S ∈ θ2(a, ˆa) because
there are n − k − 1 ways to choose S in a ring so that it does
(cid:2)
(cid:2)
not include any variable in a given connected subgraph of k
S∈θ2(a,ˆa) RS (ˆa) = (n − k − 1)R(a).
ˆa∈ ˆA
variables. Therefore,
So, from Equation 2,

(cid:2)

ˆa∈ ˆA

R(a) ≥ (k − 1)R(a∗) + (n − k − 1)R(a)

n
and therefore Equation 4 holds. (cid:3)
Proposition 4 For any binary DCOP of n agents with a
star graph structure, where all constraint rewards are non-
negative, and a∗ is the globally optimal solution, then, for
any k-optimal assignment, a, where k < n,

R(a) ≥ k − 1
n − 1
(cid:4)
(cid:3)
Proof: The proof is similar to the previous proof. In a star
n−1
subgraphs of k variables, and there-
graph, there are
k−1
. Every constraint S ∈ θ includes the cen-
fore | ˆA| =
(cid:4)
(cid:3)
n−1
k−1
n−2
tral variable and one other variable, and thus there are
k−2
(cid:2)
connected subgraphs of k variables that contain S , and there-
R(a∗). Finally, there are no
S∈θ1(a,ˆa) RS (ˆa) =
fore
ways to choose S so that it does not include any variable
in a given connected subgraph of k variables. Therefore,

(cid:2)
ˆa∈ ˆA
(cid:2)
S∈θ2(a,ˆa) RS (ˆa) = 0R(a). So, from Equation 2,

n−2
k−2

(cid:2)

ˆa∈ ˆA

R(a

(5)

(cid:3)

(cid:4)

(cid:3)

(cid:4)

∗

).

(cid:4)
(cid:3)
n−2
k−2

R(a) ≥

R(a∗) + 0R(a)
(cid:3)

(cid:4)

n−1
k−1
and therefore Equation 5 holds. (cid:3)

Tightness can be proven by constructing DCOPs on ring
and chain graphs with the same rewards as in Proposition 2;
proofs are omitted for space. The bound for rings can also be
applied to chains, since any chain can be expressed as a ring
where all rewards on one constraint are zero.

Finally, bounds for DCOPs with arbitrary graphs and non-
negative constraint rewards can be found using a linear-
fractional program (LFP). This method gives a tight bound
for any graph, since it instantiates the rewards for all con-
straints, but requires a globally optimal solution to the LFP,

(cid:2)

in contrast to the constant-time guarantees of Equations 1,
4 and 5. An LFP such as this is reducible to a linear pro-
gram (LP) [Boyd and Vandenberghe, 2004]. The objective is
R(a∗) such that ∀˜a ∈ ˜A, R(a) − R(˜a) ≥ 0, given
to minimize R(a)
(cid:2)
˜A as deﬁned in Proposition 1. Note that R(a∗) and R(a) can
S∈θ RS (a∗). We can now
S∈θ RS (a∗) and
be expressed as
transform the DCOP so that every R(˜a) can also be expressed
in terms of sums of RS (a∗) and RS (a), without changing or
invalidating the guarantee on R(a). Therefore, the LFP will
contain only two variables for each S ∈ θ, one for RS (a∗) and
one for RS (a), where the domain of each one is the set of non-
negative real numbers. The transformation is to set all reward
functions RS (·) for all S ∈ θ to 0, except for two cases: when
all variables i ∈ S have the same value as in a∗, or when all
i ∈ S have the same value as in a. This has no eﬀect on R(a∗)
or R(a), because RS (a∗) and RS (a) will be unchanged for all
S ∈ θ. It also has no eﬀect on the optimality of a∗ or the k-
optimality of a, since the only change is to reduce the global
reward for assignments other than a∗ and a. Thus, the tight
lower bound on R(a)
5 Domination analysis of k-optima
In this section we now provide a diﬀerent type of guaran-
tee: lower bounds on the proportion of all possible DCOP as-
signments which any k-optimum must dominate in terms of
solution quality. This proportion, called a domination ratio,
provides a guide for how diﬃcult it may be to ﬁnd a solu-
tion of higher quality than a k-optimum; this metric is com-
monly used to evaluate heuristics for combinatorial optimiza-
tion problems [Gutin and Yeo, 2005].

R(a∗) still applies to the original DCOP.

For example, suppose for some k, the solution quality guar-
antee from Section 4 for any k-optimum was 50% of optimal,
but, additionally, it was known that any k-optimum was guar-
anteed to dominate 95% of all possible assignments to the
DCOP. Then, at most only 5% of the other assignments could
be of higher quality, indicating that it would likely be com-
putationally expensive to ﬁnd a better assignment, either with
a higher k algorithm, or by some other method, and so a k-
optimal algorithm should be used despite the low guarantee
of 50% of the optimal solution quality. Now suppose instead
for the same problem, the k-optimum was guaranteed to dom-
inate only 20% of all assignments. Then it becomes more
likely that a better solution could be found quickly, and so the
k-optimal algorithm might not be recommended.
To ﬁnd the domination ratio, observe that any k-optimum a
must be of the same or higher quality than all ˜a ∈ ˜A as deﬁned
in Proposition 1. So, the ratio is:
1 + | ˜A|
(cid:14)
i∈N |An| .
(cid:3)
(cid:4)

If the constraint graph is fully connected (or not known, and
(cid:14)
so must be assumed to be fully connected), and each variable
i∈N |An| = qn.
has q values, then | ˜A| =
If the graph is known to be not fully connected, then the set
˜A from Equation 6 can be expanded to include assignments of
distance greater than k from a, providing a stronger guarantee
on the ratio of the assignment space that must be dominated
by any k-optimum. Speciﬁcally, if a is k-optimal, then any

(q − 1) j and

(cid:2)

k
j=1

(6)

n
j

IJCAI-07

1449

5
2

5
1

+

+

(cid:3)

(cid:4)

(cid:3)

(cid:4)

(cid:3)

(cid:3)

1 +

(cid:4)(cid:4)

assignment where any number of disjoint subsets of size ≤ k
have deviated from a must be of the same or lower quality as
a, as long as no constraint includes any two agents in diﬀerent
such subsets; this idea is illustrated below:
Example 3 Consider a binary DCOP of ﬁve variables, num-
bered 1 to 5, with domains of two values, with unknown con-
straint graph. Any 3-optimum must be of equal or greater
/25 = 81.25% of all possible
quality than
assignments, i.e. where 0, 1, 2, or 3 agents have deviated.

Now, suppose the graph is known to be a chain with vari-
ables ordered by number. Since a deviation by either the
variables {1,2} or {4,5} cannot increase global reward, and
no constraint exists across these subsets, then neither can a
deviation by {1,2,4,5}, even though four variables are deviat-
ing. The same applies to {1,3,4,5} and {1,2,3,5}, since both
are made up of subsets of three or fewer variables that do
(cid:4)
not share constraints. So, a 3-optimum is now of equal or
/25 = 90.63% of
greater quality than
all assignments. (cid:2)

1 +

+ 3

(cid:3)

(cid:3)

(cid:4)

(cid:3)

(cid:4)

(cid:3)

(cid:4)

5
2

+

+

5
1

5
3

5
3

i=1Di where:

An improved guarantee can be found by enumerating the
set ˜A of assignments ˜a with equal or lower reward than a; this
set is expanded due to the DCOP graph structure as in the
above example. The following proposition makes this possi-
ble; we introduce new notation for it: If we deﬁne n diﬀerent
subsets of agents as Di for i = 1 . . . n, we use Dm = ∪m
i=1Di,
i.e. Dm is the union of the ﬁrst m subsets. The proof is by
induction over each subset Di for i = 1 . . . n.
Proposition 5 Let a be some k-optimal assignment. Let ˜an
be another assignment for which D(a, ˜an) can be expressed
as Dn = ∪n
• ∀Di,|Di| ≤ k. (subsets contain k or fewer agents)
• ∀Di, D j, Di ∩ D j = ∅. (subsets are disjoint)
• ∀Di, D j, (cid:2)i ∈ Di, j ∈ D j such that i, j ∈ S , for any S ∈ θ.
(no constraint exists between agents in diﬀerent subsets)
Then, R(a) ≥ R(˜an).
Proof:
Base case: If n = 1 then Dn = D1 and R(a) ≥ R(˜an) by
deﬁnition of k-optimality.
Inductive step: R(a) ≥ R(˜an−1) ⇒ R(a) ≥ R(˜an).
The set of all agents can be divided into the set of agents in
Dn−1, the set of agents in Dn, and the set of agents not in Dn.
Also, by inductive hypothesis, R(a) ≥ R(˜an−1). Therefore,
(cid:5)
R(a) =
(cid:5)
S∈θ:S∩Dn(cid:2)∅
≥
RS (˜an−1)
) +
(cid:2)
S∈θ:S∩Dn−1(cid:2)∅ RS (a) ≥ (cid:2)
S∈θ:S∩Dn(cid:2)∅
S∈θ:S∩Dn−1(cid:2)∅ RS (˜an−1) because the
so
rewards from agents outside Dn−1 are the same for a and ˜an−1.
Let a(cid:12) be an assignment such that D(a, a(cid:12)) = Dn =
D(˜an−1, ˜an). Because a is k-optimal, R(a) ≥ R(a(cid:12)); therefore,
R(a) =
≥

(cid:5)
RS (a)
(cid:5)
S∈θ:S∩Dn=∅
S∈θ:S∩Dn=∅

RS (a) +
RS (˜an−1

RS (˜an−1) +

(cid:5)
(cid:5)

(cid:5)
(cid:5)

S∈θ:S∩Dn−1(cid:2)∅

S∈θ:S∩Dn−1(cid:2)∅

S∈θ:S∩Dn−1(cid:2)∅

RS (a) +

RS (a) +

RS (a) +

RS (a)

(cid:12)

(cid:12)

(cid:12)

S

(cid:5)
(cid:5)
S∈θ:S∩Dn=∅
S∈θ:S∩Dn=∅

(cid:5)
(cid:5)
S∈θ:S∩Dn(cid:2)∅
S∈θ:S∩Dn(cid:2)∅

S∈θ:S∩Dn−1(cid:2)∅

RS (a

) +

RS (a

) +

RS (a

).

(cid:2)
S∈θ:S∩Dn(cid:2)∅ RS (a) ≥ (cid:2)

We also know that

(cid:2)
S∈θ:S∩Dn=∅ RS (a) =

S∈θ:S∩Dn(cid:2)∅ RS (a(cid:12)) because the
(cid:2)
and so
rewards from agents outside Dn are the same for a and a(cid:12).
S∈θ:S∩Dn=∅ RS (˜an)
because the rewards from agents outside Dn are the same for
a and ˜an; therefore,
R(a) ≥

RS (˜an−1) +

RS (˜an)

RS (a

) +

(cid:12)

(cid:5)
(cid:5)
S∈θ:S∩Dn(cid:2)∅
S∈θ:S∩Dn(cid:2)∅

(cid:5)
(cid:5)
S∈θ:S∩Dn=∅
S∈θ:S∩Dn=∅

(cid:5)
(cid:5)

S∈θ:S∩Dn−1(cid:2)∅

=

S∈θ:S∩Dn−1(cid:2)∅

RS (˜an) +

RS (˜an) +

RS (˜an)

because the rewards from Dn−1 are the same for ˜an−1 and ˜an,
and the rewards from Dn are the same for a(cid:12) and ˜an. There-
fore, R(a) ≥ R(˜an). (cid:3)
6 Experimental results
While the main thrust of this paper is on theoretical guaran-
tees for k-optima, this section gives an illustration of the guar-
antees in action, and how they are aﬀected by constraint graph
structure. Figures 2a, 2b, and 2c show quality guarantees for
binary DCOPs with fully connected graphs, ring graphs, and
star graphs, calculated directly from Equations 1, 4 and 5.
Figure 2d shows quality guarantees for binary-tree DCOPs,
obtained using the LFP from Section 4. The x-axis plots the
value chosen for k, and the y-axis plots the lower bound for k-
optima as a percentage of the optimal solution quality for sys-
tems of 5, 10, 15, and 20 agents. These results show how the
worst-case beneﬁt of increasing k varies depending on graph
structure. For example, in a ﬁve-agent DCOP, a 3-optimum is
guaranteed to be 50% of optimal whether the graph is a star
or a ring. However, moving to k = 4 means that worst-case
solution quality will improve to 75% for a star, but only to
60% for a ring. For fully connected graphs, the beneﬁt of in-
creasing k goes up as k increases; whereas for stars it stays
constant, and for chains it decreases, except for when k = n.
Results for binary trees are mixed.

Figure 3 shows the domination ratio guarantees for k-
optima from Section 5, for DCOPs where variables have do-

Figure 2: Quality guarantees for k-optima with respect to the
global optimum for DCOPs of various graph structures.

IJCAI-07

1450

distributed settings, given the computational and communi-
cation expense of large-scale coordination.
References
[Boyd and Vandenberghe, 2004] S. Boyd and L. Vanden-
berghe. Convex Optimization. Cambridge U. Press, 2004.
and
Distributed coordination through an-
L. Meertens.
archic optimization.
In V. Lesser, C. L. Ortiz, and
M. Tambe, editors, Distributed Sensor Networks: A
Multiagent Perspective, pages 257–295. Kluwer, 2003.

[Fitzpatrick and Meertens, 2003] S.

Fitzpatrick

[Gutin and Yeo, 2005] G. Gutin and A. Yeo. Domination
analysis of combinatorial optimization algorithms and
problems. In M. Golumbic and I. Hartman, editors, Graph
Theory, Combinatorics and Algorithms: Interdisciplinary
Applications. Kluwer, 2005.

[Maheswaran et al., 2004] R. T. Maheswaran, J. P. Pearce,
and M. Tambe. Distributed algorithms for DCOP: A
graphical-game-based approach. In PDCS, 2004.

[Mailler and Lesser, 2004] R. Mailler and V. Lesser. Solving
distributed constraint optimization problems using cooper-
ative mediation. In AAMAS, 2004.

[Modi et al., 2005] P. J. Modi, W. Shen, M. Tambe, and
M. Yokoo. Adopt: Asynchronous distributed constraint
optimization with quality guarantees. Artiﬁcial Intelli-
gence, 161(1-2):149–180, 2005.

[Nair et al., 2005] R. Nair, P. Varakantham, M. Tambe, and
M. Yokoo. Networked distributed POMDPs: A synthesis
of distributed constraint optimization and POMDPs.
In
AAAI, 2005.

[Pearce et al., 2006] J. P. Pearce, R. T. Maheswaran, and
M. Tambe. Solution sets for DCOPs and graphical games.
In AAMAS, 2006.

[Petcu and Faltings, 2005] A. Petcu and B. Faltings. A scal-
able method for multiagent constraint optimization. In IJ-
CAI, 2005.

[Vlassis et al., 2004] N. Vlassis, R. Elhorst, and J. R. Kok.
Anytime algorithms for multiagent decision making using
coordination graphs. In Proc. Intl. Conf. on Systems, Man
and Cybernetics, 2004.

[Yokoo and Hirayama, 1996] M. Yokoo and K. Hirayama.
Distributed breakout algorithm for solving distributed con-
straint satisfaction and optimization problems. In ICMAS,
1996.

[Zhang et al., 2005a] W. Zhang, G. Wang, Z. Xing, and
L. Wittenberg. Distributed stochastic search and dis-
tributed breakout: properties, comparison and applications
to constraint optimization problems in sensor networks.
Artiﬁcial Intelligence, 161(1-2):55–87, 2005.

[Zhang et al., 2005b] Y. Zhang, J. G. Bellingham, R. E.
Davis, and Y. Chao. Optimizing autonomous underwater
vehicles’ survey for reconstruction of an ocean ﬁeld that
varies in space and time. In American Geophysical Union,
Fall meeting, 2005.

Figure 3: Domination ratio guarantees for k-optima for vari-
ous graph structures.
mains of two values. This ﬁgure, when considered with Fig-
ure 2, provides insight into the diﬃculty of ﬁnding a solu-
tion of higher quality than a k-optimum. For example, a 7-
optimum in a fully connected graph of 10 agents (Figure 2a)
is only guaranteed to be 50% of optimal; however this 7-
optimum is guaranteed to be of higher quality than 94.5%
of all possible assignments to that DCOP (Figure 3a), which
suggests that ﬁnding a better solution may be diﬃcult.
In
contrast, a 3-optimum in a ring of 10 agents (Figure 2b) has
the same guarantee of 50% of optimal solution, but this 3-
optimum is only guaranteed to be of higher quality than 69%
of all possible assignments, which suggests that ﬁnding a bet-
ter solution may be easier.
7 Related work and conclusion
This paper contains the ﬁrst guarantees on solution quality for
k-optimal DCOP assignments. The performance of any local
DCOP algorithms can now be compared in terms of worst
case guaranteed solution quality, either on a particular con-
straint graph, or over all possible graphs. In addition, since
the guarantees are reward-independent, they can be used for
any DCOP of a given graph structure, once computed.

In [Pearce et al., 2006], upper bounds on the number of
possible k-optima that could exist in a given DCOP graph
were presented. The work in this paper focuses instead on
lower bounds on solution quality for k-optima for a given
DCOP graph. This paper provides a complement to the ex-
perimental analysis of local optima (1-optima) arising from
the execution of incomplete DCOP algorithms [Zhang et al.,
2005a; Maheswaran et al., 2004]. However, in this paper, the
emphasis is on the worst case rather than the average case.

The results in this paper can help illuminate the re-
lationship between local and global optimality in many
types of multi-agent systems, e. g. networked distributed
POMDPs [Nair et al., 2005]. All results in this paper also
apply to centralized constraint reasoning. However, examin-
ing properties of solutions that arise from coordinated value
changes of small groups of variables is especially useful in

IJCAI-07

1451


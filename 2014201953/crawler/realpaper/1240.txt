THE  LEARNING  OF  PARAMETERS  FOR  GENERATING  COMPOUND  CHARACTERIZERS 

FOR  PATTERN  RECOGNITION 

by 

Leonard  Uhr  and  Sara  Jordan 

University  of  Wisconsin 

Madison,  Wisconsin  U.S.A. 

Abstract 

This  paper  presents  and  describes  a  pattern 
recognition  program  with  a  relatively  simple  and 
general  basic  structure  upon  which  has  been  su­
perimposed  a  rather  wide  variety  of  techniques  for 
learning,  or  self-organization.  The  program  at­
tempts  to  generalize  n-tuple  approaches  to  pattern 
recognition,  in  which  an  n-tuple  is  a  set  of  indi­
vidual  cells  or  small  pieces  of  patterns,  and  each 
n-tuple  is  said  to  characterize  an  input  pattern 
when  these  pieces  match  i t,  as  specified. 

The  program  allows  n-tuples  to  match  when 

only  some  of  their  parts  match,  and  it  allows 
these  parts  to  match  even  though  they  are  not 
precisely  positioned  (See  Uhr,  1969b,  for  some 
simple  example  programs). 
It  further  learns,  in 
It  searches  for  good  weights 
a  variety  of  ways: 
implications,  byre-weight­
on  its  characterizers' 
ing  as  a  function  of  feedback. 
It  generates  and 
discovers  new  characterizers  (and  can  therefore 
begin  with  no  characterizers  at  all),  and  discards 
characterizers  that  prove  to  be  poor  (See  Uhr 
and  Vossler,  1961,  and  Prather  and  Uhr,  1964). 
It 
also  uses  a  set  of  characterizers  of  characterizers, 
to  search  for  good  parameter  values  that  newly-
generated  characterizers  should  have. 

A  detailed  flow-chart-like  "precis"  descrip­
tion  of  the  program  is  given,  along  with  an  ac­
tual  listing. 
It  is  thus  possible  to  examine  ex­
actly  what  the  program  does,  and  how  it  does 
it,  and  therefore  to  see  how  a  wide  variety  of 
learning  mechanisms  have  been  implemented  in 
a  single  pattern  recognition  program.  But  be­
cause  it  was  coded  in  a  "high-level"  pattern-
matching  and  list-processing  language  the  pro­
gram  runs  too  slowly  for  extensive  tests  to  be 
practicable.  Therefore  only  a  brief  listing  of 
output  is  given, 
and  begins  to  learn. 
Descriptors:  Learning,  self-organization,  induc­
tion,  discovery,  pattern  recognition,  learning  to 
learn,  n-tuple  recognition,  characterizing  char­
acterizers. 

to  show  that  the  program,  works 

Introduction 

Programs  that  have  used  n-tuples  as  their 
characterizers  appear  to  perform  with  the  very 
best  of  pattern  recognition  programs  (for  discus­
sions,  see  Uhr,  1963,  1969a;  for  a  good  recent 
example,  see  Andrews,  Atrubin,  and  Hu,  1968). 
This  is  not  surprising, 
for  n-tuples  are  easily 
handled  by  the  digital  computer.  And  although 

they  may  appear  simple,  any  possible  charac-
terizer  can  be  described  as  a  sufficiently  com­
plex  and  detailed  n-tuple.  What  we  don't  know 
is  whether  the  n-tuple  description  of  sufficiently 
powerful  characterizers  would  avoid  being  overly 
cumbersome  and  ridiculously  wasteful  of  storage 
space  and  processing  time. 

itself  continuing  to  choose 

Programs  that  use  n-tuples  either  have  them 
designed  by  human  beings  and  pre-programmed  in 
( e . g .,  Andrews,  Atrubin,  and  Hu,  1968),  or  ran­
domly  generate  a  fixed  set  of  fixed-n-size  n-
tuples  ( e . g .,  Bledsoe  and  Browning,  1959).  An 
interestingly  simple  generalization  of  this  is  the 
following:  Let  the  program  begin  with  no  char­
acterizers,  but  have  it  generate  new  character­
izers  that  are  as  simple  as  possible,  and  only 
when  needed.  Thus  the  program  might  start  by 
generating  one  1-tuple,  continue  generating  more 
1-tuples  as  it  finds 
wrong  names  to  assign  to  input  patterns,  and  at 
some  point  begin  generating  n+1-tuples. 
It  further 
should  be  assessing  how  well  each  characterizer 
is  working,  by  in  effect  conducting  a  running  ex­
periment  that  examines  its  successes  and  failures. 
This  information  should  be  used  a)  to  weight  the 
importance  of  this  characterizer1 s  implications  in 
combining  them  for  the  decisions  as  to  names  to 
choose,  b)  to  decide  whether  a  characterizer  is 
good  and  should  therefore  be  used,  or  is  bad  and 
should  therefore  be  discarded,  to  be  replaced  by 
another,  and  c)  to  gather  information  about  gen­
eral  types  of  characterizers,  so  that  new  charac­
terizers  are  generated  that  are  similar  in  impor­
tant  parameter  values  to  characterizers  that  have 
proved  themselves  good. 

This  paper  describes  a  program  that  is  a 
first  approximation  to  this  simple,  but  hazy, 
scheme  of  generating  as  few  characterizers  as 
needed,  keeping  them  as  simple  as  possible, 
but  using  what  has  been  learned  about  charac­
terizers  to  direct  the  generation  of  new  charac­
terizers,  so  that  they  will  be  similar  in  their 
characteristics  to  good  characterizers  that  have 
been  generated  in  the  past. 

The  program  has  a  second  general  purpose  -
to  puch  deeper  into  techniques  for  learning  char­
acterizers. 

The  basic  structure  of  this  program  seems 
to  us  extremely  simple  -  the  generation,  when 
needed,  of  the  best  new  specific  n-tuple  of  the 
best  general  type  possible,  and  the  learning  of 
as  much  as  possible.  But  when  the  program  is 
described  or  given  in  detail,  as  in  the  following 

- 3 8 1-

for 

to 

forced 

f l e x i b i l i ty 

i n e v i t a b ly  sounds  more  complex  - 

it 
it  is  more  complex  when 

pages, 
the 
indeed 
for  a  discrete  d i g i t al  computer. 
l e v el  of  code 
In  order  to  get 
into  our  n-tuples  so 
that  they  need  not  be  p r e c i s e ly  positioned  and 
can  be  considered 
parts  do  not  always  m a t c h,  extra  details  must 
in  turn  suggest 
be  added  to  the  c o d e. 
additional 
that  w i ll  search 
for  good  values 
threshold  m a t c h i n g. 

These 
learning  mechanisms 

for  this  a l l o w ed  w o b b l i ng  and 

to  match  even 

though  a ll 

There  are  a l so  several  points  at  w h i ch  we 

room 

found 

that  should 

time  a d j u s t i ng 

for  more).  When  the  program 

it  be  of 
This  we 
treating  n - s i ze  as  Just  another  para­

s i m p ly  evade  quite  subtle  decisions 
be  made  by  the  program:  Should  the  program 
spend  more 
the  weights  of  its 
it  g e n­
present  set  of  c h a r a c t e r i z e r s,  or  should 
erate  one  or  more  new  characterizers ? 
This  we 
handle  by  having  the  program  generate  one  new 
characterizer  per  p a t t e r n,  up  to  a 
f i x ed  maximum 
(also  d i s c a r d i ng  characterizers 
to  be  b a d, 
to  make 
generates  a  new  characterizer,  should 
the  same  size 
handle  by 
meter,  so  t h a t,  as  described  b e l o w, 
w i ll  choose  the 
the  goodness  of  the 
t i on  of 
that  have  been  generated  so 
n - s i ze 
i n i t i a l i z ed  to  equal  1; 
n 
is 
keep 
tabs  on 
w i ll  generate 
that  reflects 
this  goodness,  but  w i th  some  p r o b a b i l i ty  w i ll 
o c c a s i o n a l ly  generate  a  new  tuple  of  size 
This  procedure 
c h a r a c t e r i z e r s. 

the  goodness  of  each  n - s i ze  and 
tuples  w i th  an  n - s i ze 

f u n c­
tuples  of  different 
Thus 

for  a  new  tuple  as  a 

for  a ll  parameters  of 

the  program  w i ll 

n,  or  of  size 

the  program 

is  used 

n  +  1? 

n  +  1. 

f a r. 

n 

Precursors 

As  an 

introduction 

the  structure  of  our 
l et  us  consider  the  Bledsoe-Browning 
(1959),  w h i ch  was 

to 

to  use  n-tuples  randomly  s e­
input  grid 

to  recognize 
For  each  n - t u p l e, 

typed  or 
the 

f i r st 
the 

the 
from 

program, 
pattern  recognition  program 
among 
l e c t ed 
handwritten  c h a r a c t e r s. 
possible  pattern  names  having 
as 
comparison 
e r e d, 
tern  most  c l o s e ly  ( i . e .,  having 
of  same-state  n - t u p l e s) 
of  the 

input  p a t t e r n. 

that  matches 

the  unknown 

the  name 

After  a ll 

t a l l y. 

input  pattern  are  added 

the  same  state 
i n to  a 
tuples  are  c o n s i d­
the  unknown  p a t­

is  chosen  as 

the  highest  sum 
the  name 

U s i ng 

the  string  manipulation 

language 

SNOBOL,  Uhr  (1969b)  coded  a  somewhat  extended 
Uhr's 
version  of  the  Bledsoe-Browning  program. 
short  program  uses  w e i g h t ed 
i m p l i c a t i o n s, 
rather 
than  merely  t a l l y i ng  t h e m,  and 
it  a l l o ws  varying 
sizes 
pieces  of  the  n - t u p l e s. 

for  the  n-tuples  and 

i n d i v i d u al 

for  the 

There  are  several  weaknesses 
It  does  not  l e a r n,  so 

this 

in 
type 
its  perform­

remains  o n ly  as  good  as 

the  n-tuple 

of  program: 
ance 

characterizers 
i d ly  p o s i t i o n e d,  and  must  match  e x a c t ly  and 
e n t i r e l y. 

it  starts  w i t h. 

N-tuples  are  r i g­

A  Basic  N-Tuple  Pattern  Recognition  and 

Learning  Program 

Let's 

try 

to  generalize 

the  basic  n-tuple  pro-

Each 

t i m e, 

the  program 

f o u n d.  O p t i o n a l l y,  a 

the  characterizer  tuples 

For  example, 
looked  for  one  part  at  a 

instead  of 
tuple  piece  of  the  character-

information 
its  expected 
the  pattern 
its  s i z e,  and  the  s p e c i f ic  configuration 

gram. 
w i ll  be 
a ll  at  o n c e. 
izer  n-tuple  w i ll  contain  pertinent 
about 
l o c a t i on  w i t h in 
g r i d, 
that  should  be 
tuple  part 
w i ll  have  no  particular  position  s p e c i f i e d,  s i g­
(presently 
naling 
meaning  from 
for 
this 
is  matched, 
its 
found  i m p l i c a t i o n s.  When 
plied  by  several  characterizers, 
weights  of  i m p l i c a t i o ns  are  added 
tuples  are  a l l o w ed 
grid  points 
h a p s, 
the 
several  c h a r a c t e r i z e r s. 

The 
to  be  n o n - e x c l u s i v e,  so  that 
(such  a s,  per­
in 
left  edge  of  the  grid)  may  reappear  in 

tuple  part. 
implied  pattern  names  are  put  on  a 

l i st  of 
is 
the  separate 

its  current  position  on  down) 

If  a  characterizer 

the  same  name 

look  anywhere 

important 

together. 

locations 

i m­

to 

in 

from 

left  as 

the  memory 

implications 

for  reweighting  of 

feedback  name  are 

tuple  configurations  were 

The  weights  of  implications  of  the 

But  a  wrong  answer 
the  char­

the 
feedback  g i v i ng 
If  the  program  gave 
is 
i s,  since 

This  basic  program  w i ll  a l so  have 
learn 

i m p l i­
If 
the  program  w i ll  a l so  g e n­

the  a b i l i­
its  experience,  by  comparing  its 
the  cor­
the  right 
it  p r o­

ty  to 
chosen  answer  w i th 
rect  pattern  name. 
answer, 
duced  s a t i s f a c t o ry  r e s u l t s. 
c a l ls 
acterizers  whose 
matched. 
wrongly  chosen  name  are  decreased,  and 
cations  of  the 
the  answer  was  wrong 
erate  a  new  characterizer  using 
To  do  t h i s,  a  random  n-
named 
tuple 
is  chosen 
d i s t r i b u t i on  of  weights  attached 
values  of  n) 
the 
a  characterizer  w h i ch 
back  name. 
number  of  characterizers  generated, 
to  prevent 
saturation  of  memory  or  unnecessary  s l o w i ng  of 
processing 
c a r d e d,  making  room 
weights  of  a ll 
minimal  acceptable 

Poor  characterizers  are  d i s­
the 
f a ll  below  a 

the  correct 
Each  run  has  an  upper  l i m it 

input  and  assembled 
implies 

input  p a t t e r n. 
(n 

the  generated 
into 

for  new  o n e s,  when 

to  r e f l e ct 
to 

their  i m p l i c a t i o ns 

this  wrongly 

is  extracted 

feed-
to  the 

i n c r e a s e d. 

l e v e l. 

t i m e. 

from 

the 

Characterization  Over  Variations 

Presented  w i th  only  standard,  n o n - v a r y i ng 

type 
it 

instances 
(in  a  single 
repertoire  of  p a t t e r n s, 
a  pattern  recognition  program 
nize  a  set  of  c h a r a c t e r s. 
v a r y,  even  s l i g h t l y, 
time  to  t i m e, 

f o n t,  perhaps)  of  its 
is  no  great  problem  for 
to  r e c o g­
But  if  patterns  can 
from 

to  learn 

in  p o s i t i on  or  shape 

then  problems  mushroom.  Our 

-382-

program  tries  to  handle  this  in  several  ways. 
Wobbly  Patterns 

Each  part  of  a  tuple  is  allowed  to  wobble  a 

(A 

given  horizontal  distance  to  either  side. 
somewhat  more  limited  capability  for  handling 
vertical  wobbling  is  the  "anywhere"  search  men­
tioned  previously,  plus  the  fact  that  all  tuple 
part  addresses  are  given  relative  to  the  last 
position,  wherever  that  may  be.  Uhr  (1969b) 
presents  programs  that  also  allow  vertical  wob­
ble.) 
In  each  characterizer  tuple  part  there  is 
an  explicitly  given  wobble  which  tells  the  pro­
gram  just  how  big  a  hunk  of  the  grid  row  it  can 
look  in  for  the  desired  configuration.  This  a l­
lowable  wobble  may  vary  from  tuple  piece  to 
piece,  as  learning  has  indicated  was  needed  for 
good  performance.  Thus  if  a  desired  configura­
tion  was  not  found  within  the  specified  wobble, 
but  would  have  been  found  were  the  wobble 
slightly  larger, 
then  the  program  remembers  how 
it  almost  found  this  characterizer.  When  feed­
back  shows  it  chose  the  wrong  name, 
program  finds  that  this  almost-matched  charac­
terizer  would  have  implied  the  right  answer, 
it 
increases  the  wobble  allowance  to  improve  per­
formance . 
Threshold  Characterizers  that  Can  Partially  Match 

if  the 

Suppose  that  three  parts  of  a  4-tuple  were 
found,  but  the  other  part  was  not.  We  would 
like  to  allow  use  of  the  implications  of  this 
nearly  matched  characterizer  even  though  the 
program  did  not  find  a  perfect  match. 
In  order 
to  do  this  the  program  uses  threshold  matching, 
where  each  part  of  a  tuple  has  its  own  weight 
to  add  into  the  tuple's  sum  of  "foundness." 
Each  implication  of  the  characterizer  is  preceded 
by  a  threshold  requirement  which  must  be  met  by 
the  tuple  sum  before  the  implication  may  be 
merged  into  the  list  of  possible  pattern  names. 
Thus  one  implication  may  require  all  but  one 
part  of  the  tuple's  configuration  to  be  found, 
where  another  implication  of  the  same  character­
izer  might  require  a  perfect  match  of  all  parts. 

Compound  Characterizers 

Besides  having  a  primitive  sort  of  tuple  con­

sisting  of  a  set  of  0-1  configurations  to  be 
looked  for  at  certain  points  on  the  pattern  grid, 
our  program  can  also  use  compound  characteri­
zers,  where  one  or  more  of  the  tuple  parts  is 
itself  the  name  of  another  characterizer.  The 
program  looks  in  the  stated  position  (or  else 
'anywhere")  for  the  name  of  the  desired  compo­
nent  characterizer  and  treats  this  tuple  part  just 
as  any  other.  Now  the  program  must  add  the 
names  of  found  characterizers  to  the  input  that 
it  is  processing. 

Compound  characterizers  are  currently  gen­
erated  from  primitive  characterizers  that  are  on 
the  list  of  characterizers  found  for  this  input. 
There  must  be  two  or  more  such  component  char­
acterizers  in  order  to  generate  a  compound  char­
acterizer.  When  the  program  decides  to  gener­
ate  a  compound  characterizer, 
maximum  number  of  parts  to  give  the  tuple. 
Then  the  parts  are  pulled  off  the  list  of  charac­
terizers  found  in  this  input,  and  the  characteri­
zer  is  assembled  as  initially  implying  only  the 
feedback. 

it  chooses  the 

These  compound  characterizers  are  more  gen­
eral  than  primitive  characterizers  in  that  a  more 
sophisticated  set  of  pattern  characteristics  can 
be  represented  by  one  tuple. 
Indeed,  with  com­
pound  characterizers  we  approach  a  method  for 
learning  stroke  or  feature  recognition,  where 
primitive  characterizers  might  represent  the  vari­
ous  primary  curves  and  lines,  and  the  compound 
characterizers  could  form  the  desired  combina­
tions  of  strokes  to  imply  the  various  patterns. 
For  example, 
if  CHAR1  is  the  tuple  describing  a 
small  open-left  curve,  CHAR2  is  a  long  vertical 
line,  and  CHAR3  is  a  large  open-left  curve, 
then  CHAR4  compounding  CHAR2  and  CHAR1 
could  imply  the  pattern  "P"  and  CHAR5  coupling 
CHAR2  and  CHAR3  could  imply  " D ". 

Parameters  That  Characterize  Characterizers 
An  important  part  of  learning  in  humans  is 
In  order  to  enable  our  program 

in  effect,  "generalize"  on  what  it  has 

generalization. 
to, 
learned  and  thus  perform  better,  we  have  given 
it  an  expandable  set  of  parameters  or  character­
izer  traits.  For  each  trait  (such  as  the  number 
of  parts,  or  their  closeness,  or  their  maximum 
horizontal  spread),  a  value  can  be  computed  for 
every  characterizer.  With  every  characterizer 
there  is  associated  a  list  of  this  characterizer's 
value  for  each  trait. 
In  addition  we  keep  a 
common  traits  list  of  all  traits  and  all  values 
that  have  been  generated  and  used  for  them.  A 
weight  is  associated  with  each  value  for  each 
trait.  For  example,  suppose  the  program  gives 
a  wrong  name  for  an  input  pattern  on  the  basis 
of  found  characterizer  N. 
Then  after  the  im­
plication  weight  of  the  wrong  name  in  CHAR  N 
is  decreased,  the  program  goes  through  the  trait 
list  of  CHAR  N  and, 
for  every  trait,  downweights 
CHAR  N's  value  for  that  trait  in  the  common 
trait  list. 
if  CHAR  N's  value  for 
VERTSPRED  (vertical  point  spread)  is  " 1 ",  then 
our  program  w i ll  look  for  the  value  " 1"  under 
the  trait  VERTSPRED  in  the  general  characterizer 
traits  list,  and  decrease  the  "goodness  weight" 
of  the  value  " 1 ". 

In  particular, 

When  upweighting  a  good  characterizer's 
the  program  also  enters  (if  not 

trait  values, 

-383-

Detailed  Description  of  Program 

*PRECIS  FOR  NTUPLE 

LEARNING  PROGRAM. 

INITIZE 

Statement 
Number 
17-31 

I n i t i a l i ze  MEMORY  (can  be  n u l l ),  any 
CHARacterizers  and 
l i s t s, 
and  the  general  CHaracterizerTRAITS 
value 

their  TRait 

l i s t, 

PAR 

I N I T i al  WeighT,  GOOD, 

I n i t i a l i ze  PARAMeterS  (values  for 
INCrement,  DECrement, 
I N I t i al 
THRESHold, 
BAD,  PROBability  of  COMPOUND 
characterizer  generation),  EDGEDOTS 
to  a l l ow 
put  p a t t e r n. 

room  around 

, ,wobbling" 

i n­

IN 

READ  in  the  matrix,  ROW  by  ROW, 
to 

putting  an  EDGE  on  each  side 
for  the  maximum  current 
a l l ow 
WOBbLe,  and  maintaining 
the 
current  COLumnSIZE. 
READ  in 

the  FeedBacK  (marked  by 

' * * * ')  and  any  PARAMeter  CHANGE 
(marked  by 
more  c a r d s,  go  to  E N D. 

if  g i v e n. 

If  no 

' $ ' ), 

RECOGNIZE 

I n i t i a l i ze  FOUND 
FOUNDCHARacterizerS 
of  M E M o r y,  ROWSIZE,GRIDSIZE, 

i m p l i c a t i on 

l i s t,  a  copy 

l i s t, 

IMPLIST  of 

i m p l i c a t i o ns 

threshold  requirements 

the  next  CHARacterizer  and 
If  no 

Blank  out 
whose 
are  m e t. 
Get 
its  CNumber  from  M E M. 
more  c h a r a c t e r i z e r s,  go  to  DENY. 
Get  the  DESCRiption  of  CHAR, 
its 
IMPLIED  patterns,  and 
the  C O M­
POUNDS 

is  part  o f. 

that  CHAR 

32-

-38 

39-

-50 

51-

-56 

57 

58-

-59 

60-

-62 

63-

-65 

66-

-69 

70-

-72 

in 

information 

the  general 

already  there)  on  each  trait  value  list  a  slightly 
larger  parameter  value. 
In  this  way  it  broadens 
the  range  of  parameter  values  that  w i ll  be  used 
to  generate  new  characterizers.  The  value  and 
goodness  weight 
traits  list  is  used  when  a  new  characterizer  is 
generated.  The  program  tries  to  generate  the 
new  characterizer  tuple  within  the  framework  of 
what  the  program  has  already  learned;  currently 
it  uses  the  three  traits  necessary  to  control  the 
basic  generation  (tuple  size,  piece  size,  wob­
ble)  plus  a  fourth  chosen  randomly  from 
other  possible  traits  (currently, 
these  are  hori­
zontal  spread,  vertical  spread,  average  close­
ness  of  parts,  number  of  parts  on  the  edge  of 
the  grid,  and  compound  or  primitive).  A  de­
sired  value  for  the  new  characterizer  is  chosen 
with  a  probability  that  reflects  the  weights  as­
sociated  with  the  various  possible  values  of  the 
trait.  The  program  tries  several  times  to  find  a 
randomly  positioned  tuple  which  w i ll  have  this 
same  value.  Thus  the  program  generalizes  on 
what  it  has  learned, 
in  that  if  a  value  of  " 6" 
for  VERTSPRED  has  been  upweighted  several 
times, 
the  program  may  decide  that  this  is  a 
good  value  to  try  for  in  a  new  characterizer. 
(For  further  details,  see  functions  TRAITWT  and 
PROBCHOOSE  and  the  section  labeled  PRIMITIVE 
in  the  precis  and  the  code.) 

the 

The  Complete  Program 

The  preceding  sections  describe  independent 
features,  any  or  all  of  which  could  be  added  to 
a  basic  learning  program  to  create  a  complete 
program.  The  final  program  containing  all  the 
features  is  described  at  the  end  of  this  section. 
As  might  be  expected,  the  characterizers  for 
this  final  program  have  become  fairly  complex. 
As  an  example, 

CHARO 

Rl 

means  the  following: 
"Description=at  row  0,  column  1  look  in  the 
next  1  position  for  the  string  " 0 ",  adding  2  to 
the  tuple  sum  of  weights  on  success;  5  rows 
down  and  3  cols,  over  look  in  the  next  4  posi­
tions  for  the  string  " 0 1 ",  adding  1  to  the  tuple 
sum  on  success/lmplications=if  sum 
imply  I  with  weight  2;  if  sum 
weight 
CHAR4/Trait  list  name=TRO/Last  tuple  part's 
absolute  address  is  row  5,  c o l.  4 / ". 

l/CHARO  is  £art  of  the  compound 

1  imply  T  with 

3  then 

An  outline  of  the  program's  operation  f o l­

lows. 

R3 

Pick  off  THIS  p i e c e, 
and 
no  more  p a r t s,  go  to  R2. 

its  POSition 

its  W e i g h T, 
If 

from  DESCR. 

RR2 

includes 

relativeDROW  and 
If  POS 
DCOL  numbers  and  a  MASK  s i z e, 
compute 
l o c a­
t i on  and  go  to  RR6 
to 
THIS  positioned  p i e c e. 

the  absolute  DROW 

look 

for 

RR3 

look 

Otherwise 
THIS,  starting  at  the  current  Begin 
ROW. 

If  f i nd  THIS,  go  to  R4R. 

'ANYWHERE' 

for 

-384-

Statement 
Number 

73 

DENY 

R3R 

f a i l s,  make 
If  the  ANYWHERE  search 
the  next 
tuple  part  apply  ANYWHERE, 
too  (by  erasing  its  p o s i t i o n ).  Go  to 
R3. 

R4R 

Set  this  Row  as 
future  search. 

the  Begin  ROW 

for 

If  THIS  is 

information  at 

followed  by  BCOLumn 
and  WeiGhT 
the 
RIGHT,  we  are  working  w i th  a 
compound  characterizer;  go  to  RR4. 
the  BeginCOL  for 

Otherwise  compute 

future  search  and  go  to  R3. 

RR4 

threshold  WeighT  requirement 

If  the  WeiGhT  of  THIS  piece  meets 
the 
for 
this  compound  characterizer  part,  add 
the  WeiGhT 
t i o n s.  Go  to  R3. 

the  SUM  of 

implica-

to 

RR6 

Finish  computing 

the  absolute 
look  at 

to 
for 
first  for  THIS 

DCOLumn  position 
THIS  p i e c e. 
Look 
as  a  compound  tuple  part  w i th 
DColumn  and  WeiGhT 
if  don't 

its 
information; 

i t,  go  to  RR7. 

find 

If  DC  was  w i t h in 

the  WobBle  a l l o w- 
reset  BROW  and  BCOL,  add 

a n c e, 
WGT  to  S U M,  and  go  to  R3.  Other­
w i s e, 
allowance  would  have  given  a 
m a t c h,  record  this  NEARMISS.  Go 
to  RR8. 

l i t t le  bigger  WobBLe 

if  a 

RR7 

for  THIS  positioned  primitive 
Look 
tuple  part. 
if  a 
l i t t le  more  wobble  gives  a  NEAR-
MISS. 

If  no  match,  see 

RR8 

Set  new  BROW  and  BCOL  for  next 
search  and  go  to  R3. 

R2 

IMPLIST  of 

Make 
implications  whose 
THReshold  weight  requirements  were 
m e t. 

R5A 

Record  NEARMISSes  on  ALMOSTFOUND 
characterizers 

l i s t. 

R5 

If  IMPLIST 

is  not  empty  put  on 

74 

75 

76 

77 

78-80 

81-84 

85-91 

92-93 

94-95 

96-97 

98 

this  CHAR 

its 

FOUNDCHARacterizerS 
and 

i n f o r m a t i o n. 
If  CHAR  is  part  of  a 
the 

it 

found 

mark 
and  put  the  COMPOUNDS  on  MEM 
to  look  at  l a t e r.  Go  to  R l. 

in 

larger  compound,  99-104 
input  matrix 

If  a ll 

implications  are  erased 

for 

132-134 

this  CHAR,  erase 
and  go 

to  REWEIGHT. 

it  from  MEMORY 

Statement 
Number 
105-106 

107-111 

112-116 

117-122 

123-124 

125-130 

131 

135-136 

137-138 

139-140 

141-150 

151-156 

from  FOUNDCHARS  any 
i m p l i c a­

Erase 
characterizers  whose 
tions  are  DENIED  by  a  compound 
characterizer. 

IMPLY 

the  remaining 
from  FOUNDCHARS 

i m p l i c a­
into  a 

Merge  a ll 
tions 
FOUND 
CHOOSE 

l i s t. 

Choose  as  HINAME 
FOUND  w i th 

the  NAME  on 
the  Highest  WeighT. 

OUT 

PRINT  out  HINAME. 
If  there  was  no  FeedBacK  or  if 
HINAME  was  r i g h t,  go  to  I N. 
Otherwise,  answer  was  w r o n g. 

REWEIGHT 

Pick  off  the  next  CHARacterizer 
from 
the  FCopy  of  FOUNDCHARS. 
If  no  more  CHARs,  go  to  ADJust 
w o b b l e s. 
If  the  wrong  HINAME 

IMPLIED 

is 

by  this  CHAR,  downweight  the 
implication  or  erase 
reduced  weight 

it 
is  BAD. 

if  the 

C7 

t r a i t,  give 

this  bad 

For  each 
CHARacterizer's 
DECrement  on 
TRAITS 

l i s t. 

trait  value  a 

the  main  CHaracter 

C4 

C5 

found  CHAR,  add  it  to 

If  FeedBacK  was  not  IMPLIED  by 
this 
plications  and  go 
O t h e r w i s e, 

to  REWEIGHT. 

i m­

i m p l i c a t i on  of  FBK 
For  each  t r a i t,  give 

upweight  the 
in  CHAR. 
this  good  CHAR'S  value  an  I N C r e-
ment  on 
l i s t. 
Go  to  REWEIGHT. 

the  main  CHTRAITS 

ADJUST 

Pick  off  the  next  CHARacterizer 
which  was  ALMOSTFOUND. 
If 
no  more,  go  to  GENERATE  a  new 
characterizer. 

If 

this  CHARacterizer 

I M P U ED 

the  mask 

for  the  parts 

FBK  but  not  the  wrong  HINAME, 
enlarge 
W H I CH  would  have  given  a  m a t c h. 
Make  sure  this  CHARacterizer*s  TRait 
l i st  contains 
value  of  i ts  p a r t s. 

the  maximum  wobble 

-385-

GENERATE 

If  there  are  already  enough  char­
( i . e.  no  more  TOGEN-

a c t e r i z es 
erate),  go  to  I N. 
Get 
terizer  and  decide 
compound. 
TIVE  g e n e r a t i o n. 

the  number  of  the  new  c h a r a c- 
if  it  should  be 

If  n o t,  go  to  P R I M I­

COMPOUND 

the  number  of  PARTS 

Choose 
new  characterizer  should  h a v e. 
Make  an  FCopy  of  FOUNDCHARS 
to  choose 

f r o m. 

the 

C GI 

the  next 

found  CHaracterizer 

Keep  TUP  count  of  how 

If  no  more,  go  to 

Get 
from  F C. 
C G 2. 
many  parts  are  got 
Get 
and 
the 
DESCRiption  of  the  new  c o m­
pound  characterizer. 

the  LASTPART  l o c a t i on  of  CH 

in  order  into 

from  F C. 

insert 

it 

l i st  of  the  PRIMitive 
in  order  to 

Add  to  the  new  I M P l i c a t i on  LIST 
the  d e n i al  of  this  component 
CHaracterizer's 
i m p l i c a t i o n s. 
Keep  a 
PARTS 
this  new  characterizer 
COMPOUNDS 
Keep  the 
any  part  of  the  compound 
stored  as  W B L. 

later  insert 
their 

largest  WoBble  value  of 

l i s t s. 

in 

largest  Piece  size  of  any 

Keep  the 
part  of  the  compound  stored  as 
P C. 

CG2 

If  there  were 
ponents 

less 

than  2  c o m­

in  F C,  go  to  PRIMITIVE. 

CG3 

the 

Assemble 

f i n al  DESCRiption 
w i th  r e l a t i ve  p o s i t i o n s,  noting 
the  LASTPART. 
I n i t i a l i ze  TRait 

l i st  for  new  c o m- 
Assemble 

pound  characterizer. 
the  c h a r a c t e r i z e r. 

Statement 
Number 
157-158 

159-160 

161-169 

170-171 

172-173 

174 

175 

176-177 

178-180 

181-182 

183-191 

generation  (TUPle  s i z e,  PieCe 
s i z e,  WobBLe). 

I n i t i a l i ze  the  new  TRait  l i st  w i th 

these  v a l u e s. 
Get  a  desired  TRYVALue  for 
another  TRait.  W i ll  TRY  to 
generate  a 
VALue. 

tuple  w i th 

the  same 

GENTUP 

Create  a  r e l a t i v e l y - o r d e r ed  random 
TUP-tuple  ordered  by  r o w s,  and 
calculate 
TRait. 

its  VALue 

the  chosen 

for 

If  VAL  equals 

the  TRYVAL,  or  if  5 

f a i l e d,  go  to  G 4.  Other­

TRYs 
wise  go  to  GENTUP  and  TRY 
a g a i n. 

G4 

Assemble 
i z e r. 

G6 

the  primitive  character­

Complete 
TRait 
a ll  other 

t r a i t s. 

the  new  characterizer's 
for 

l i s t,  computing  values 

Add  the  new  characterizer  to 

MEMORY.  Go  to  I N. 

the  various 

to  c a l c u l a te 
the  r e l a t i v e l y-
in  DESCR. 

for 
tuple 

♦Begin  routines 
trait  values 
addressed 
HOROSPRED 
calculates 
columns  between 
the 
rightmost  parts  of  the 

the  maximum  number  of 
leftmost  and 
t u p l e. 

VERTSPRED 

the  maximum  number  of 

calculates 
rows  between 
bottommost  parts  of  the 

topmost  and 
t u p l e. 

the 

Statement 
Number 

203 

204-207 

208-210 

211-212 

213-216 

217-218 

219-22 7 

228-231 

GRIDEDGE 

tuple  w h i ch 

calculates 
the 
of  the 
input  p a t t e r n. 
normalized  over  10. 

the  number  of  parts 
the  edge 
VAL  is 

l ie  on 

232-242 

in 

192-193 

PROXIM 

243-256 

the  sum  of  absolute 

calculates 
differences  between  corresponding 
d i g i ts 
tuple  p a r t s. 

in  a ll  possible  pairs  of 

COMPOUND 

257-259 

'YES'  or 

returns 
to  whether  the 
pound  or  p r i m i t i ve  characterizer. 

' N O ',  according 

is  a  c o m­

tuple 

CG5 

Mark  each  component  character­
izer  as  part  of  this  compound 
characterizer.  Go  to  G 6. 

PRIMITIVE 

194-196 

197-199 

Make  a  rearranged  COPYTRAITS 
of  CHTRAITS  so  as 
through 
traits  used 
p r i m i t i ve 

tuple  generation. 

to  c y c le 
to 

Influence 

According  to 

the  value  p r o b a b i l i- 
the 

t i es  in  COPYTRAITS,  choose 
necessary  values  for  characterizer 

D i s c u s s i on 

This  paper  b r i e f ly  describes 

features  o f - o ur  program. 
t a i l ed 
number  to  the  a c t u al  program  statements  being 

the  various 
It  then  gives  a  d e­
" p r e c i s"  that  refers  by 

f l o w - c h a r t - l i ke 

200-202 

-386-

described.  The  program  itself  is  given  in  the 
Appendix.  Thus  the  reader  can  examine  exactly 
what  has  been  done  to  implement  any  of  the 
aspects  of  the  program  about  which  he  is  curi­
ous.  This  seems  to  us  of  crucial  importance: 
if  the  program  can  be  used  to  document  itself 
there  is  no  need  for  lengthy  and  usually  mis­
leading  descriptions  and  discussions. 

The  program  listing  is  too  long  and  complex 
to  be  followed  with  ease,  even  by  someone  who 
knows  SNOBOL;  but  it  should  give  an  idea  of 
what's  going  on  to  the  casual  observer,  and 
those  parts  in  which  the  reader  is  interested 
enough  to  make  some  effort  should  become  un­
derstandable.  SNOBOL  is  a  very  simple  lan­
guage  in  its  basic  conception, 
are  built  up  from  sets  of  production  and  re­
placement  statements  (of  the  sort 
"Let  A  =  B; 
Look  for  C  on  A  and, 
if  it's  found,  replace  it 
by  B), 
tied  together  by  labels  and  gotos.  A 
brief  description  of  SNOBOL  is  given  in  the 
Appendix. 

for  its  programs 

This  program  was  written  to  examine 

whether  a  wide  variety  of  learning  methods 
could  be  implemented  together  in  a  single  pat­
tern  recognition  program.  Using  the  language 
SNOBOL  allowed  us  to  code  a  relatively  power­
f u l,  yet  short,  program.  However,  the  program 
runs  too  slowly  to  make  extensive  tests  of  its 
abilities  to  learn  and  achieve  interesting  asymp­
totic  performance  levels.  We  therefore  give 
only  a  brief  listing  of  a  short  run, 
that  the  program  works,  and  that  it  at  least  be­
gins  to  learn.  The  program  will  be  recoded  in 
a  faster  language  if  we  decide  to  make  more 
extensive 

to  indicate 

tests. 

Further  developments  might  be  to  have  the 
program  try  to  learn  good  weights  of  character-
izer  tuple  parts  and  the  thresholds  required  to 
imply  a  pattern  name.  We  would  also  like  it 
to  generate  new  parameters  with  which  to  char­
acterize  its  characterizers  (see  Uhr,  1969b). 

Summary 

The  program  described  in  this  paper  at­

tempts  to  combine  a  very  simple  basic  pattern 
recognition  scheme  with  a  wide  variety  of 
powerful  learning  mechanisms.  The  program  at­
tempts  1)  to  generate  its  own  n-tuple  charac­
terizers  as  needed,  and  to  adjust  their  weights 
as  a  function  of  feedback,  2)  to  decide  what 
type  of  characterizer  to  generate,  and  3)  to 
learn  what  are  good  general  characteristics  of 
characterizers. 
and  how  to  modify  any  particular  characterizer 
that  it  is  evaluating.  These  decisions  are  all 
made  within  the  framework  of  a  program  that 
tries  to  recognize  patterns  with  as  small  a  set 
of  characterizers  that  are  as  simple  as  possible. 

It  can  further  decide  4)  whether 

It  therefore  starts  out  with  no  characterizers, 
and  generates  other  characterizers  which  are  as 
simple  as  it  has  been  able  to  get  away  with  and 
which  fall  within  the  range  of  what  the  program 
conjectures  to  be  optimal  values  for  the  charac­
teristics  of  characterizers. 
In  terms  of  charac­
this  means  the  program  starts  out 
terizer  size, 
generating  1-tuples  and  then, 
to  the  extent  that 
feedback  indicates  that  it  must  improve  upon  its 
performance,  2-tuples,  3-tuples,  and  n+1-tuples. 

Acknowledgements 

This  research  has  been  supported  in  part  by 

NIH  grant  MH-12266  and  NSF  grant  GP-7069. 

Bibliography 

1.  Andrews,  D.R.,  Atrubin,  A . J .,  and  Hu,  K. 
C,  The  IBM  1975  optical  page  reader: 
Part  I I I:  Recognition  logic  development. 
IBM  I.  Research  and  Development,  1968, 
12,  364-372. 

2.  Bledsoe,  W . W.  and  Browning,  I .,  Pattern 

recognition  and  reading  by  machine.  Proc. 
Eastern  Joint  Comp.  Conf.,  1959,  225-
232. 

3.  Prather,  Rebecca  and  Uhr,  L.,  Discovery 

and  learning  techniques  for  pattern  recog­
nition.  Proc.  19th  Annual  Meeting  of  the 
ACM,  1964. 

4.  Uhr,  L.,  Pattern  recognition  computers  as 

models  for  form  perception.  Psychol 
Bull.,  1963,  60.,  40-73. 

5.  Uhr,  L.  &  Vossler,  C,  A  pattern  recogni­
tion  program  that  generates,  evaluates, 
and  adjusts  its  own  operators.  Proc. 
Western  Joint  Computer  Conf.,  1961, 
555-569. 

6.  Uhr,  L.,  A  tutorial  description  of  pattern 

recognition  programs. 
publication,  1969a). 

(Submitted  for 

7.  Uhr,  L,,  Pattern  Recognition,  Problem-Solv­

ing  and  Learning. 
tion) . 

1969b 

(In  prepara­

Appendix 

A  Brief  Description  of  SNOBOL 

SNOBOL  is  a  "pattern  matching"  language 
that  turns  out  to  be  quite  convenient  for  handl­
ing  list  structures  and  networks  of  information, 
using  push-down  stacks, 
indirection,  and  recur­
sive  programming. 
Its  syntax  is  extremely  sim­
ple,  as  follows: 

SNOBOL  programs  are  built  up  of  two  basic 

types  of  statements: 
1)  Assignment  statements  that  assign  a  name  to 
a  pattern  of  strings, 
e.g. 

DESCRIPTION  =  '001100' 
CHARACTERIZER  =  DESCRIPTION  '  =  ' 

IMPIiEDS 

' /' 

-387-

2)  Replacement  statements 
a  string  and 
another  p a t t e r n, 
e . g. 

(if  they  are 

that 

f i nd  patterns  on 
them  by 

found)  replace 

" l i t e r a ls 

is  a  sequence  of  1) 

for  their  c o n t e n t s,  2) 

I M P U E D S,  F O U N D,  THIS)  w h i ch  refer  to 

These  statements  have  several  components: 
the  "name"  of  the  string  to  be  processed,  b) 
"names" 

a) 
the  " p a t t e r n"  w h i ch 
( e . g. 
and  stand 
( e . g. "' = ' ")  w h i ch  stand  for  t h e m s e l v e s,  and  3) 
"variable  names" 
assigned  contents  during 
statement, 
the  pattern  somewhere 
variable  name  can  be  subscripted  w i th  a  number 
that 
♦THIS/SIZE*  where  SIZE  contains  an 

" * S U M * " ),  w h i ch  are 
the  execution  of  the 

if  the  program  succeeds 

in  matching 
A 

in  the  named  s t r i n g. 

i n t e g e r ). 
two  examples  of  assignment  s t a t e­

length 

In 

the 

( e . g. 

( e . g. 

fixes 

its 

l i t e r al  contents  are 
is  put  at 

ments  above,  DESCRIPTION  is  made  the  name  of 
the  string  whose 
' 0 0 1 1 0 0 ', 
the  beginning  of  the 
and 
then  001100 
string  named  CHARACTERIZER,  since 
the  name 
to 
DESCRIPTION  refers 
its  c o n t e n t s. 
assignment  statement, 
c o d e d, 
sign  ($)  preceding  the  name  $ DESCRIPTION 
w o u ld  put  EDGE,  not  001100,  on  CHARACTERIZER. 

If  another 
were 
indirect  reference  symbol  dollar-

then 

the 

i t,  and 

this  a l so  marks 
The 

The  end  of  the  p a t t e r n - t o - b e - m a t c h ed 

marked  by  the  equal  sign 
around 
the  replacement  p a t t e r n. 
statement  is  a l w a ys 
strings  up  to 
t o - b e - m a t c h e d,  and  a ll  strings  after 
sign 
hand  pattern  succeeded). 

is 
w i t h o ut  quotes 
the  beginning  of 
first  string  of  a 
the  name;  a ll  subsequent 
the  p a t t e r n-
the  equal 

the  replacement  pattern 

the  equal  sign 

(if  the  l e f t-

form 

form 

A  statement  can  be  surrounded  by  " l a b e l s" 
and  'tjotos"  w h i ch  control  the  f l ow  of  the  program. 
A  ' l a b e l"  is  a  string  that  always  begins  in  column 
1.  A  "goto"  comes  after  the  statement,  is  s i g­
naled  by  a  s l a s h,  and  is  of  the  form  /(INPUT)  or 
/S(INPUT)  or  /F(INPUT)  or  /S(INPUT)F(PROCESS), 
where  S  means  transfer  on  s u c c e s s,  F  means 
transfer  on  f a i l u r e,  and  no 
means  unconditional 
c o u r s e,  a l w a ys  refer  to  a 
are  no  g o t o s, 
statement 

letter  after  the  slash 

the  program  goes 

l a b e l .)  When 

in  sequence. 

to  the  next 

transfer. 

(The  goto  must,  of 

there 

Arithmetic 

is  performed  w i t h in 

these  s t a t e­

and  /  (and  ** 

for  e x­
Numbers  must  be  referred  to 
(as 
' 1'  above,  w h i ch  w i ll  add  one  to  the 

l i t e r a ls  or  as  contents  of  l i s ts 

in 

A  number  of  b u i l t - in 
test  for  i n e q u a l i t i e s: 

ments  by  using  
p o n e n t i a t i o n ). 
either  as 
SUM  + 
number  stored 
functions  can  be  used 
. G T ( A , B ), 
.EQ(A,B)  and  EQUALS(A,B) 
matching  e q u a l i t y ). 
read 

in  S U M ). 

to 

. L T ( A , B ) , . G E ( A , B ), 

.LE(A,B), 

(which 

is  s t r i n g-

The  command  ".READ"  w i ll 

in  one  data  c a r d,  and  ".PRINT  ="  w i ll 

IN 

ASK 

END 

-388-

print  out 

the  pattern 

that 

f o l l o w s. 

An  asterisk  (*) 

in  column  1  denotes  a  c o m­

indicates 

the  compiler  w i ll  i g n o r e. 

in  column  1 
the  statement  on 

A 
ment  c a r d,  w h i ch 
that  this  card 
period  (.) 
continues 
the  preceding  card 
(statements  can  use  o n ly  72  columns,  whereas 
the  data  cards 
that  f o l l ow  the  program  can  use 
a ll  80  columns).  A  program  ends  w i th  an  END 
card  (END  starts 
the 
label  of  the 
The  basic  pattern  match  goes 

in  column  1) 
f i r st  statement  to  be  e x e c u t e d. 
l e ft  to 

from 

that  also  contains 

- 

looks 

found, 

l ie  between 

for  the  next  match  of 

it  backtracks 

The  compiler 

(ignoring  variable 

the  matched  elements 

to  the  strings 
the 

r i g h t. 
each  element  of  the  pattern 
names,  w h i ch  w i ll  be  assigned 
that 
l i t e r a ls  and  names  w i th  c o n t e n t s ). 
is 
of  the  previously  matched  element,  and 
for  its  next  m a t c h,  continuing 
the 
f a i l s. 
tingent  upon  either  this  match  or  one  of  the 
f u n c t i o n s. 
his  own  f u n c t i o n s,  and  do  a  number  of  other 
powerful 

things  not  discussed  h e r e. 

last  element  matches  or 

Success  or  failure 

f i r st  element 
is  c o n­

looks 
this  u n t il  either 

If  no  match 
the  assignment 

The  programmer  can  define  and  code 

in  the  gotos 

to  break 

the 

A  simple  program 

for 

information  retrieval 

IN 
ASK 

f o l l o w s. 
♦EXAMPLE  PROGRAM.  A  SIMPLE  PROGRAM  TO 
♦ DO 
GO 

'INFORMATION  RETRIEVAL"  FOLLOWS: 
DOCUMENTS  =  ' R I V E R S = D 1 , D 3 , D 8 , /' 
'LAKES=D3, D5, /SPAIN= D 3,  D8, Dl 1 ,' 
• D 1 7 , /' 
.READ  *QUERY*  ' 
QUERY  ♦DESCRIPTORS 
DOCUMENTS  DESCRIPTOR  ' = ' 
♦PERTINENT* 
.PRINT  =  DESCRIPTOR  ' 
'DISCUSSED  IN 
GO 

'  PERTINENT/(ASK) 

' , ' = / F ( I N) 

7F(END) 

/F(ASK) 

IS 

' /' 

' 

END 
RIVERS, SPAIN, MOUNTAINS, 
((Query  to  be 
♦ PRECIS  -  AN  ENGLISH  DESCRIPTION  OF 
♦ABOVE  INFORMATION  RETRIEVAL  PROGRAM. 
GO 

i n p u t,  on  data  card)) 

l i st  of  descriptors) 

Let  DOCUMENTS  contain  the  d e- 
s c r i p t o r s, 
f o l l o w ed  by  pertinent 
documents. 
READ  in  the  next  QUERY  (which  is 
a 
Get  the  next  DESCRIPTOR  from  the 
QUERY. 
(If  no  more,  Fail  to  ASK.) 
From  DOCUMENTS,  get  PERTINENT 
ones 
f o u n d. 
PRINT  out  the  DESCRIPTOR  and  the 
PERTINENT  documents. 
GO 

if  the  DESCRIPTOR  is 

Ml 

1 
2 
3 

4 

Dl 

Ml 

1 

2 

3 

4 

-397-


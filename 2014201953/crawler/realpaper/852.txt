Feature Selection and Kernel Design via Linear Programming

Glenn Fung, Romer Rosales, R. Bharat Rao

Siemens Medical Solutions, 51 Valley Stream Parkway, Malvern, PA, USA.

glenn.fung@siemens.com, romer.rosales@siemens.com, bharat.rao@siemens.com

Abstract

The deﬁnition of object (e.g., data point) similar-
ity is critical to the performance of many machine
learning algorithms, both in terms of accuracy and
computational efﬁciency. However, it is often the
case that a similarity function is unknown or chosen
by hand. This paper introduces a formulation that
given relative similarity comparisons among triples
of points of the form object i is more like object j
than object k, it constructs a kernel function that
preserves the given relationships. Our approach is
based on learning a kernel that is a combination of
functions taken from a set of base functions (these
could be kernels as well). The formulation is based
on deﬁning an optimization problem that can be
solved using linear programming instead of a semi-
deﬁnite program usually required for kernel learn-
ing. We show how to construct a convex problem
from the given set of similarity comparisons and
then arrive to a linear programming formulation by
employing a subset of the positive deﬁnite matrices.
We extend this formulation to consider represen-
tation/evaluation efﬁciency based on formulating a
novel form of feature selection using kernels (that
is not much more expensive to solve). Using pub-
licly available data, we experimentally demonstrate
how the formulation introduced in this paper shows
excellent performance in practice by comparing it
with a baseline method and a related state-of-the art
approach, in addition of being much more efﬁcient
computationally.

1 Introduction and Related Work
The deﬁnition of a distance or a similarity function over the
input or data space is fundamental in the performance of ma-
chine learning algorithms, both in terms of accuracy and ef-
ﬁciency. This can be easily veriﬁed by looking at the role
that the distance or similarity plays in common algorithms
(e.g., k-means, nearest neighbors, support vector machines or
any other kernel method, etc.). However, since the concept
of similarity depends on the task of interest (objects can be
similar or dissimilar in many ways depending on the appli-
cation), a similarity function that is appropriate for one task

may not be appropriate for another. This task dependency has
been noted in a number of approaches that seek to build these
functions for a particular classiﬁcation or regression model
given some training data (e.g., [Cohen et al., 1998; Xing et
al., 2002; Schultz and Joachims, 2003; Lanckriet et al., 2004;
Athitsos et al., 2004]).

In this paper, we are interested in automatically ﬁnd-
ing good similarity functions after some basic information
about the task of interest has been provided (e.g., by an
expert user). Unlike approaches that rely on class labels
(e.g., [Lanckriet et al., 2004]) or on building sets of points
that are globally similar (or dissimilar) [Xing et al., 2002;
Wagstaff et al., 2001], here we explore a simpler-to-specify
form of user supervision: that provided by statements like ob-
ject i is more similar to object j than to object k1. We remark
that we are not only interested on building functions that are
consistent with this information provided by the user, but we
are also interested in obtaining functions that are efﬁcient to
compute. Thus, in addition we focus on functions that can
be evaluated by looking only at part of the features or input
dimensions; therefore, implying some form of feature selec-
tion.

More formally,

let us represent the objects of interest
, with k =
as points xk in a D-dimensional space (cid:2)D
{1, 2, ..., N }. We would like to obtain a function K :
(cid:2)D × (cid:2)D → (cid:2) that satisﬁes the above similarity com-
parisons and that in addition can be well approximated by
˜K : (cid:2)d × (cid:2)d → (cid:2) which only uses d < D components of
xk. For this we will rely on (Mercer) Kernel representations
[Cristianini and Shawe-Taylor, 2000] and deﬁne K (similarly
j αjKj(x, y) where α ∈ (cid:2);
for
thus, K is a mixture of kernel functions2. Throughout this pa-
per, we will work with this representation of similarity func-
tions, deﬁne convex problems for obtaining K, and later ex-
˜K. Some of the motivation for the
tend this idea to focus on
framework in this paper is related to earlier work in metric
learning [Rosales and Fung, 2006]; however, here the focus
is on a different problem, kernel design.

˜K) as a kernel K(x, y) =

(cid:2)

1Example objects of interest include database records, user opin-

ions, product characteristics, etc.

2Note that K is any afﬁne combination of kernels not just a con-
vex combination. Also, we are not restricting Kj to be itself a ker-
nel.

IJCAI-07

786

In this paper, we ﬁrst show how relative constraints, involv-
ing triples of points of the form, K(xi, xj) > K(xi, xk) can
be used to learn K. One can think of this ﬁrst formulation as
parallel to other approaches for learning K that rely on super-
vised learning with class labels (and, unlike the one presented
here, are classiﬁcation-algorithm dependent). These formu-
lations involve solving a semideﬁnite programming problem
(SDP) [Graepel, 2002; Lanckriet et al., 2004], but we show
how a different Linear Programming formulation, introduced
in this paper, is also sufﬁcient in practice and much more
efﬁcient computationally (e.g., it can be use to solve much
larger problems, faster). In addition, we extend this formu-
lation to consider the issue of representation/evaluation efﬁ-
ciency based on a form of feature selection. This formulation
leads to a linear programming approach for solving the ker-
nel design problem that also optimizes for succinct feature
representations.

1.1 Kernel Design

In recent years the idea of kernel design for learning algo-
rithms has received considerable attention in the machine
learning community. Traditionally, an appropriate kernel is
found by choosing a parametric family of well-known ker-
nels, e.g., Gaussian or polynomial, and then learning a gen-
erally sub-optimal set of parameters by a tuning procedure.
This procedure, although effective in most cases, suffers from
important drawbacks: (1) calculating kernel matrices for a
large range of parameters may become computationally pro-
hibitive, especially when the training data is large and (2) very
little (to none) prior knowledge about the desired similarity
can be easily incorporated into the kernel learning procedure.
More recently, several authors [Bennett et al., 2002;
Lanckriet et al., 2004; Fung et al., 2004] have considered the
use of a linear combination of kernels that belong to a fam-
ily or superset of different kernel functions and parameters;
this transforms the problem of choosing a kernel model into
one of ﬁnding an optimal linear combination of the members
of the kernel family. Using this approach there is no need to
predeﬁne a kernel; instead, a ﬁnal kernel is constructed ac-
cording to the speciﬁc classiﬁcation problem to be solved. In
this paper we also use a mixture model representation3; how-
ever, our approach does not depend on and is not attached
to any speciﬁc machine learning algorithm (for classiﬁca-
tion, regression, inference, etc). Instead, inspired by the fact
that most kernels can be seen as similarity functions, we rely
on explicit conditions on similarity values among subset of
points in the original training dataset; this is, our formulation
allows the user to explicitly ask for conditions that have to
be satisﬁed or reﬂected in the desired kernel function to be
learned.

1.2 Relative Similarity Constraints

As explained above, we concentrate on examples of proxim-
ity comparisons among triples of objects of the type object i is
more like object j than object k. In choosing this type of rel-
ative relationships, we were inspired primarily by [Athitsos

3But note that the elements of the mixture could be functions that

are not kernels.

et al., 2004; Cohen et al., 1998; Schultz and Joachims, 2003;
Rosales and Fung, 2006], where these relationships are de-
ﬁned with respect to distances or rankings. The use of rela-
tive similarity constraints of this form for kernel design offers
different challenges. Note that we are not interested in pre-
serving absolute similarities, which are in general much more
difﬁcult to obtain (absolute similarities imply relative similar-
ities, but the converse is not true).

One important observation for kernel design is that the ker-
nel conditions implied by these relationships are linear rela-
tionships among individual kernel entries in the kernel matrix
(deﬁned over the points of interest) and can be expressed as
linear constraints. In addition to this, unlike any of the meth-
ods referred to above, by restricting the kernel family to have
only kernels that depend in one original feature at the time,
we are able to learn kernels that depend on a minimal set of
the original features. This can be seen as implicitly perform-
ing feature selection with respect to the original data space.
In combination with the beneﬁts of a new linear program-
ming approach, the formulation proposed in this paper has a
unique set of attributes with signiﬁcant advantages over re-
cent approaches for kernel design.

1.3 Notation and background

In the following, vectors will be assumed to be column vec-
tors unless transposed to a row vector by a superscript (cid:5).
The scalar (inner) product of two vectors x and y in the d-
will be denoted by x(cid:2)y. The 2-
dimensional real space (cid:2)d
2 and (cid:6)x(cid:6)
norm and 1-norm of x will be denoted by (cid:6)x(cid:6)
1
respectively. A column vector of ones of arbitrary dimension
will be denoted by (cid:3)e, and one of zeros will be denoted by (cid:3)0. A
kernel matrix for a given set of points will be denoted by K,
individual components by Kij, and kernel functions by K()
or simply K.

2 A Linear programming formulation for

kernel design

2.1 Using a linear combination of Kernels
Let us say we are given a set of points xk ∈ (cid:2)D
with
k = {1, ..., N } for which an appropriate similarity function is
unknown or expensive to compute. In addition we are given
information about a few relative similarity comparisons. For-
mally, we are given a set T = {(i, j, k)|K(xi, xj ) >
K(xi, xk)} for some kernel function K. The kernel K is not
known explicitly, instead a user may only be able to provide
these similarity relationships sparsely or by example. We are
interested in ﬁnding a kernel function K() that satisﬁes these
relationships.

For the rest of the paper, let us suppose that instead of
the kernel K being deﬁned by a single kernel mapping (e.g.,
Gaussian, polynomial, etc.), the kernel K is instead com-
posed of a linear combination of kernel functions Kj, j =
{1, . . . , k}, as below:

K α(x, y) =

k(cid:3)

j=1

αjKj(x, y).

(1)

IJCAI-07

787

As pointed out in [Lanckriet et al., 2004], the set Ω =
{K1(x, y), . . . , Kk(x, y)} can be seen as a predeﬁned set of
initial guesses of the kernel matrix. Note that the set Ω could
contain very different kernel matrix models, all with different
parameter values. The goal is then to ﬁnd a kernel function
K α
that satisﬁes relative kernel value constraints (speciﬁed
by the user as a set T ). A general formulation for achieving
this is given by:

(cid:2)

t t + γh(Kα)

minα,
s.t.

∀t

t ≥ 0
Kα (cid:9) 0,

∀(i, j, k) ∈ T K α(xi, xj ) + t > K α(xi, xk)

(2)

where t are slacks variables, t indexes T ,
the function
h(Kα) is a regularizer on the kernel matrix Kα
(or alter-
natively the kernel function K α
) for capacity control, and
the parameter γ ≥ 0 controls the trade-off between con-
straint satisfaction and regularization strength (usually ob-
tained by tuning). Note that this formulation seems sufﬁ-
cient, we can optimize the set of values αi in order to obtain a
positive semideﬁnite (PSD) linear combination K α(x, y) =
(cid:2)k
αjKj(x, y) suitable for the speciﬁc task at hand. This
formulation, however requires solving a relatively expensive
semideﬁnite program (SDP). Enforcing αi ≥ 0, ∀i, [Fung et
al., 2004; Lanckriet et al., 2004] and moreover restricting all
the kernels in the set Ω to be PSD results in Kα
being PSD.
However this strongly limits the space of attainable kernel
functions K α

j=1

.

Instead of using these restrictions, we explore an alterna-
tive, general deﬁnition that allows us to consider any kernel
function in Ω and any αk ∈ (cid:2) without compromising compu-
tational efﬁciency (e.g., without requiring us to solve a SDP).
For this we will explicitly focus on a well characterized sub-
family of the PSD matrices:
the set of diagonal dominant
matrices. In order to provide a better understanding of the
motivation for our next formulation we present the following
theorem as stated in [Golub and Van Loan, 1996]:

Theorem 1 Diagonal Dominance Theorem Suppose that
M ∈ (cid:2)D×D
is symmetric and that for each i = 1, . . . , n ,
we have:

(cid:3)

Mii ≥

|Mij| ,

j(cid:3)=i

then M is positive semi-deﬁnite (PSD). Furthermore, if the
inequalities above are all strict, then M is positive deﬁnite.

Based on the diagonal dominance theorem (for matrices with
positive diagonal elements) above, by simplifying the nota-
ij ≡ K α(xi, xj), we arrive to the following
tion letting K α
alternative formulation:
(cid:2)T

t + γ (cid:6)α(cid:6)

1

t=1

minα,
s.t
∀t = (i, j, k) ∈ T
∀i ∈ T
∀t

K α

ij − K α

(cid:2)
ik + t ≥ 1

K α

ii ≥
t ≥ 0,

j∈S,j(cid:3)=i |K α
ij|

(3)

(cid:2)

Ki∈Ω

ij =

αiKi(xi, xj) and S is a set of train-
where K α
ing points (e.g., those points used to deﬁne the triples in T ).
Finally, deﬁning auxiliary variables rij ∈ (cid:2)+, γ1 ≥ 0, and
γ2 ≥ 0, formulation (3) can be rewritten as a linear program-
ming problem in the following way:

(cid:2)T

(cid:2)

t + γ1

t=1

i si + γ2

i K α

ii

(cid:2)

min
α,,s,r
s.t
∀t = (i, j, k) ∈ T
∀i (cid:11)= j|i, j ∈ S
∀i, j ∈ S
∀j ∈ {1, ..., k}
∀t

K α

K α

ij − K α
(cid:2)

−rij ≤ K α

ik + t ≥ 1
ij ≤ rij
j(cid:3)=i rij ≥ 0
−sj ≤ αj ≤ sj
t ≥ 0,

ii −

(4)
(same dimensionality as α) and sj ≥ 0. Note
where s ∈ (cid:2)k
that to simplify notation we have overloaded the variables
i, j, k (but their meaning should be clear from the context).

In order to better understand the motivation for formulation

(4) it is important to note that:
(cid:2)k
is

(i) Minimizing

(cid:2)

(cid:2)

rij since K α

i K α
ii
ii ≥

j

j(cid:3)=i

equivalent
rij , ∀i.
(cid:2)

j=1
j(cid:3)=i

to minimizing

(ii) Since we are implicitly minimizing

rij, at the op-
timal solution {α∗, r∗, ∗, s∗} to problem (4), we have
that:

j(cid:3)=i

j

(cid:4)(cid:4)(cid:4)K α∗

ij

(cid:4)(cid:4)(cid:4) , ∀(i, j ∈ S, i (cid:11)= j)

0 ≥ r∗

ij =

(iii) Combining (i) and (ii) we obtain:

∀(i)K α∗

ii ≥

r∗
ij =

(cid:3)

(cid:3)

|K α∗
ij |

j

j

which implies that K α∗
positive semideﬁnite.

is diagonal dominant and hence

2.2 Learning kernels that depend on fewer input

features (

˜K)

Next, we will modify formulation (4) in order to learn kernels
that depend in a small subset of the original input features.
As far as we know, all of the existing direct objective opti-
mization methods for feature selection with kernels perform
feature selection in the implicit feature space induced by the
kernel and not with respect to the original feature space. Do-
ing this using traditional kernels, leads to nonlinear, noncon-
vex optimization problems that are difﬁcult to solve due to the
nonlinearity relations among the original features introduced
by the kernel mappings.

In order to overcome this difﬁculty we propose a simple but
effective idea. Instead of using kernels that depend on all the
¯Ω comprised of weak ker-
features we will only consider a set
nels that depend on only one feature at a time. For example, if
i ) and xj = (x1
xi = (x1
j , . . . , xD
j )
are two vectors on (cid:2)D
, then a weak Gaussian kernel only
depending on feature f is deﬁned by:

i , . . . , xD

i , . . . , xf

j , . . . , xf

K(f )(xi, yj ) = exp(−μ

)

(5)

(cid:5)(cid:5)(cid:5)xf
i − xf

j

(cid:5)(cid:5)(cid:5)2

2

IJCAI-07

788

Let us denote by If the set of indices i of kernels Ki ∈ ¯Ω
that only depend on feature f for f ∈ {1, . . . , D}. Then, any
linear combination of weak kernels in
(cid:3)

¯Ω can be written as:

D(cid:3)

K α

ij = K α(xi, xj) =

αpKp(xi, xj )

(6)

f =1

p∈If

Note that if αp = 0, ∀p ∈ If for a given f , this implies that
ij does not depend on the original feature f . This motivates
K α
our next formulation for feature selection that uses weak one-
dimensional kernels:

(cid:2)

min
α,,s
s.t
∀t = (i, j, k) ∈ T
∀i (cid:11)= j
∀i
∀f, ∀p ∈ If

(cid:2)T

(cid:2)

t + γ1

t=1

f sf + γ2

i K α

ii

K α

K α

ij − K α
ik + t ≥ 1
(cid:2)
−rij ≤ K α
ij ≤ rij
j rij ≥ 0
ii −
−sf ≤ αp ≤ sf
t ≥ 0,

(7)
where now s is indexed by the feature number f rather
than the kernel number alone. It is interesting to note that
for each feature f , formulation (7) is minimizing Mf =
max {|αp| /p ∈ If }. This is appropriate since:

Mf = 0 ⇒ |αp| ≤ 0, ∀p ∈ If
⇒ αp = 0, ∀p ∈ If
⇒ ∀i, j, K α

ij does not depend on featuref.

(8)

3 Numerical Evaluation
For our experimental evaluation we used a collection of nine
publicly available datasets, part of the UCI repository4. A
summary of these datasets is shown in Table 1. These datasets
are commonly used in machine learning as a benchmark for
performance evaluation. Our choice of datasets is motivated
primarily by their use in evaluating a competing approach
[Xing et al., 2002] aimed to learn distance functions.

We evaluate our approach against single kernels by com-
paring against the standard Gaussian (at various widths), lin-
ear, and polynomial kernels. These kernels are also the ones
used as the basis for our mixture kernel design; thus it is a rea-
sonable baseline comparison. We compare both of our formu-
lations. One formulation attempts to perform implicit feature
selection by deﬁning weak kernels, while the other uses full
kernel matrices (that depend on all input dimensions).

We have also chosen to compare our formulation against
that proposed in [Xing et al., 2002]. This obeys various rea-
sons. In addition to being a state-of-the-art method, the pri-
mary reason for our choice is that it uses a similar (but not
identical) type of supervision, as explained below. Unlike
other related approaches, computer code and data has been
made public for this method5. In addition, this method out-
performed a constrained version of K-means [Wagstaff et al.,
2001] in the task of ﬁnding good clusterings.

4http://www.ics.uci.edu/∼mlearn/MLRepository.html.
5Data for all experiments and code for [Xing et al., 2002] was
The
downloaded from http://www.cs.cmu.edu/∼epxing/papers/.
class for dataset 1 was obtained by thresholding the median value
attribute to 25K.

Table 1: Benchmark Datasets

Name

Pts (N ) Dims (D) Classes

1 Housing-Boston
2 Ionosphere
3 Iris
4 Wine
5 Balance Scale
6 Breast-Cancer Wisc.
7 Soybean Small
8 Protein
9 Pima Diabetes

506
351
150
178
625
569
47
116
768

13
34
4
13
4
30
35
20
8

2
2
3
3
3
2
4
6
2

3.1 Evaluation Settings
The datasets employed in these experiments are generally
used for classiﬁcation since class labels are available. How-
ever, the various methods to be compared here do not re-
quire explicit class labels. The method introduced in this pa-
per requires relative similarity information among a subset of
points (clearly class labels provide more information). We
use the available class labels to generate a set of triples with
similarity comparisons that respect the classes. More explic-
itly, given a randomly chosen set of three points (from the
training set), if two of these belong to the same class and a
third belongs to a different class, then we place this triple in
our set T (i.e., i and j are the points in the same class, k is
the remaining point). For the case of [Xing et al., 2002], the
supervision is in the form of two sets, one called a similar set
and the other a dissimilar set. In order to identify these sets,
we can again use the class labels, now to build a similar set of
pairs (likewise for a dissimilar set of pairs). Given this level
of supervision, this method attempts to ﬁnd an optimal Ma-
halanobis distance matrix to have same-class points closer to
each other than different-class points (see [Xing et al., 2002]
for details).

For every triple (i, j, k) ∈ T used in our approach for
learning, we use (i, j) ∈ S and (i, k) ∈ D for learning in
[Xing et al., 2002]; where S and D are the similar and dis-
similar sets. We believe this provides a fair level of supervi-
sion for both algorithms since roughly the same information
is provided. It is possible to obtain a superset of T from S
and D, and by construction S and D can be obtained from T .
In order to evaluate performance for the various methods,
we use a 0.85/0.15 split of the data into training and testing
(for the methods where training is required). From the train-
ing portion, we generate 1500 triples, as explained above, for
actual training. This information is provided, in the appropri-
ate representation, to both algorithms. For testing, we repeat-
edly choose three points at random, and if their class labels
imply that any two points are more similar to each other than
to a third (i.e., again if two points have the same class and a
third has a different class label), then we check that the correct
relationships were learned. That is, whether the two points in
the same class are more similar (or closer) to each other than
any of these points (chosen at random) to the third point. This
same measure is used for all algorithms. Thus, we deﬁne the

IJCAI-07

789

percentage correct simply as the proportion of points from
the test set (sampled at random) that respect the class-implied
similarity or distance relationship.

Our method requires setting two balancing parameters. We
set them by using cross validation by splitting the training
set in two halves. The values tested (for both parameters)
were {10−4, 10−2, 10−1, 1, 10, 102}. These parameters have
an effect on the number of dimensions employed since a
higher value for γ1 favors using fewer kernels (or dimen-
sions). Likewise, larger values for γ2 favors kernel matrices
with a smaller trace.

3.2 Discussion and Results

Fig. 1 shows the performance of our approach compared
against various Gaussian and polynomial kernels. A linear
kernel performed almost identically to the polynomial kernel
of degree two and it is omitted in the graph. The mean and
standard deviation (of the performance measure) for each in-
dividual kernel was computed from 10 random samplings of
the dataset. In order to be consistent across approaches, the
number of samples used for testing was set to 1000 triples.

One could expect an optimal mixture of kernels to provide
a higher performance in both cases (full and weak kernel mix-
tures) when compared with single kernels. This is generally
the case. It can be seen in the ﬁgure that in most cases single
predetermined kernels are suboptimal. In the case of weak
kernels, the performance is always better than single kernels.
However, for the case of mixtures of full kernels there are
cases where a single kernel provides a higher performance in
average. This can be at least due to two reasons: (1) simply
unlucky selection (sampling) of test points. In datasets 1, 3,
and 9, the standard deviation is large enough to justify this
possibility. A much more interesting reason is (2) the im-
posed restrictions on the solution space of the mixture kernel
(e.g., dataset 6). Recall that we concentrated our solution on
a subspace of the PSD matrices, that of diagonal dominant
matrices. If the full cone of PSD matrices were to be incor-
porated as solution space, the kernel mixture could perform
as good as any base kernel (since a mixture of a single kernel
is a valid solution). Note however that, although this is possi-
ble, one would be required to pay much higher computational
costs (e.g., for large datasets or number of constraints).

Interestingly, the performance for the mixture of weak ker-
nels is superior. This can be due to the larger number of
degrees of freedom (the number of α mixing parameters is
larger) or also a feature selection effect on overﬁtting. How-
ever, even though this is the case for the datasets considered
in this paper (and results suggest that this may often be the
case), we remark that this result is not theoretically guaran-
teed since non-linear interactions among multiple dimensions
or features may not be representable using a linear combina-
tion of single features.

Fig. 2 shows the average optimal number of dimensions
found by this process in a 10-fold cross validation experiment
and the corresponding one-standard-deviation error bars. The
number of dimensions was identiﬁed by counting the number
of α’s larger than 0.01. This automatic choice of dimension-
ality was done using cross-validation as explained above and
is a valuable property of the method presented. Note that the

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

)
0
0
1
 
×
(
 

i

p
h
s
n
o

i
t

l

a
e
r
 

e
c
n
a

t
s
d

i

 
t
c
e
r
r
o
c
 
%

Gauss μ=1
Gauss μ=0.1

Poly degree 2

Mix Full Kernels

Mix Weak Kernels

0

0

1

2

3

5

4
6
Dataset index

7

8

9

10

Figure 1: Performance of single kernels and our approachs in the
nine UCI datasets. We show both instances of our approach (us-
ing full kernel matrices and weak kernels). Bars show performance
results on 10 random splits of training/test points (training only ap-
plicable to kernel mixtures). Performance is measured in terms of
the percentage of randomly chosen points (1000) from test set whose
distance relationship respect the class labels. The number of triples
used for training for all runs was 1500. Error bars show one standard
deviation.

method effectively reduces the number of features for all but
one dataset.

Fig. 3 shows the comparison of the present formulation
(using weak kernels) against [Xing et al., 2002]. We show
percentage correct averaged over 10 random splits of the data
along with one-standard-deviation bars. For each of the 10
splits, 1000 triples from the test set are randomly chosen.
When comparing the performance of both methods, we note
that, except for dataset 5, our method clearly outperforms the
competing approach. Interestingly, this dataset was the same
for which the optimal number of dimensions was determined
to always be equal to the original dimensionality.

In addition to the implicit non-linear representations im-
plied by the kernels employed, we believe that a key reason
for the superior performance of the mixture of weak kernels is
the automatic identiﬁcation of relevant dimensions. This re-
duction in dimensionality appears to provide an important ad-
vantage at the time of generalization. It is generally accepted
that a simpler representation is preferable (e.g., [Blumer et
al., 1987]) and it can reduce overﬁtting in practice.

From a computational efﬁciency perspective at test time,
being able to represent the original data more succinctly is
especially advantageous. In particular, when similarities can
be calculated directly using a low-dimensional representa-
tion, computational time savings can be signiﬁcant for on-line
applications. The projection step in this approach can be pre-
computed off-line. In retrieval applications (e.g., query-by-
example), the objects can be stored in their low-dimensional
representation. From a conceptual point of view, this formu-
lation also has the advantage of providing a more effective
tool for understanding the data since it can identify whether
variables (dimensions) are of high or low relevance for a task
of interest.

IJCAI-07

790

#Original dims
#Used dims

quest the kernel to preserve local distances.

i

s
n
o
s
n
e
m
d

i

 
f

o
 
r
e
b
m
u
N

35

30

25

20

15

10

5

0

1

2

3

5

4
6
Dataset index

7

8

9

We believe this paper has potential implications in other
problems. Our results suggest that the SDP problem could,
in some cases, be replaced by a linear programming problem.
We suggested one way to achieve this goal; this has the poten-
tial to be applied to other problems whose solution involves
SDP.

References
[Athitsos et al., 2004] V. Athitsos, J. Alon, S. Sclaroff, and G. Kol-
lios. Boostmap: A method for efﬁcient approximate similarity
rankings. In Computer Vision and Pattern Recognition, 2004.

[Bennett et al., 2002] K. Bennett, M. Momma, and M. Embrechts.
Mark: a boosting algorithm for heterogeneous kernel models. In
Proceedings Knowledge Discovery and Data Mining, 2002.

[Blumer et al., 1987] A. Blumer, A. Ehrenfeucht, D. Haussler, and
M. K. Warmuth. Occam’s razor. Information Processing Letters,
24:377–380, 1987.

[Cohen et al., 1998] W. Cohen, R. Schapire, and Y. Singer. Learn-
ing to order things. In Advances in Neural Information Process-
ing Systems 10, 1998.

[Cristianini and Shawe-Taylor, 2000] N. Cristianini and J. Shawe-
Taylor. An Introduction to Support Vector Machines. Cambridge
University Press, 2000.

[Fung et al., 2004] G. Fung, M. Dundar, J. Bi, and B. Rao. A fast it-
erative algorithm for ﬁsher discriminant using heterogeneous ker-
nels. In Proceedings of the twenty-ﬁrst international conference
on Machine learning. ACM Press, 2004.

[Golub and Van Loan, 1996] G. H. Golub and C. F. Van Loan. Ma-
trix Computations. The John Hopkins University Press, Balti-
more, Maryland, 3rd edition, 1996.

[Graepel, 2002] T. Graepel. Kernel matrix completion by semidef-
inite programming. In Proceedings of the International Confer-
ence on Neural Networks, ICANN, 2002.

[Lanckriet et al., 2004] G. Lanckriet, N. Cristianini, P. Bartlett,
L. El Ghaoui, and M. Jordan. Learning the kernel matrix with
semideﬁnite programming. Journal of Machine Learning Re-
search, 5:27–72, 2004.

[Rosales and Fung, 2006] R. Rosales and G. Fung. Learning sparse
metrics via linear programming. In Proceedings Knowledge Dis-
covery and Data Mining, 2006.

[Schultz and Joachims, 2003] M. Schultz and T. Joachims. Learn-
ing a distance metric from relative comparisons. In Advances in
Neural Information Processing Systems, 2003.

[Wagstaff et al., 2001] K. Wagstaff, C. Cardie, S. Rogers, and
S. Schroedl. Constrained k-means clustering with background
knowledge. In International Conference on Machine Learning,
2001.

[Weinberger et al., 2004] K. Weinberger, F. Sha, and L. Saul.
Learning a kernel matrix for nonlinear dimensionality reduction.
In Proceedings of the Twenty First International Confernence on
Machine Learning, 2004.

[Xing et al., 2002] E.P. Xing, A.Y. Ng, M.I. Jordan, and S. Russell.
Distance metric learning, with application to clustering with side
information. In Advances in Neural Information Processing Sys-
tems, 2002.

Figure 2: Dimensionality reduction. Total number of dimen-
sions and average number of dimensions (along with one-standard-
deviation error bars) found by our algorithm for each dataset using
the optimal parameters γ1, γ2. Averages are computed over 10 ran-
dom splits of training/test points, and 1500 triples per run.

)
0
0
1
 
×
(
 
p
h
s
n
o

i

i
t

l

a
e
r
 

e
c
n
a

t
s
d

i

 
t
c
e
r
r
o
c
 
%

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1

2

3

Xing et al.

This method

7

8

9

4

Dataset number

5

6

Figure 3: Performance comparison between competing approach
and our weak kernel approach in the nine UCI datasets. Bars show
performance results on 10 random splits of training/test points. Per-
formance is measured in terms of the percentage of randomly cho-
sen points (1000) from test set whose distance relationship respect
the class labels. The number of triples used for training for all runs
was 1500. Error bars show one standard deviation.

4 Conclusions and Future Work

We presented a novel formulation for learning kernel func-
tions from relative similarity information. Our general formu-
lation consisted of a convex optimization problem requiring
semideﬁnite programming to be solved. We designed a prac-
tical approximate problem, requiring linear programming in-
stead. In particular, we showed how the diagonal dominance
constraint on the kernel matrix leads to a general problem
that can be solved very efﬁciently.
In addition to this, we
have shown how to extend the formulation to allow for im-
plicit feature selection using a combination of kernels. Ex-
periments indicated that our weak kernel formulation outper-
forms a state-of-the-art approach that uses similar informa-
tion to learn a distance (rather than a similarity function).

Although relative similarity constraints are used in this pa-
per, other constraints could potentially be imposed. For ex-
ample, we could have sign or magnitude restrictions on cer-
tain kernel components, we can impose local isometry con-
straints (similar to [Weinberger et al., 2004]), or we can re-

IJCAI-07

791


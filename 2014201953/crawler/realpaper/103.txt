A Novel Approach to Model Generation for Heterogeneous Data Classification 

*Dept. of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824  

† Department of Computer Science and Engineering, Arizona State University, Tempe, AZ85287-8809

Rong Jin*, Huan Liu† 

rongjin@cse.msu.edu 

hliu@asu.edu 

Abstract 

Ensemble  methods  such  as  bagging  and  boosting 
have  been  successfully  applied  to  classification 
problems. Two important issues associated with an 
ensemble approach are: how to generate models to 
construct  an  ensemble,  and how  to  combine  them 
for  classification.  In  this  paper,  we  focus  on  the 
problem  of  model  generation  for  heterogeneous 
data classification. If we could partition heteroge-
neous  data  into  a  number  of  homogeneous  parti-
tions, we will likely generate reliable and accurate 
classification models over the homogeneous parti-
tions.  We  examine  different  ways  of  forming  ho-
mogeneous  subsets  and  propose  a  novel  method 
that  allows  a  data  point  to  be  assigned  multiple 
times in order to generate homogeneous partitions 
for ensemble learning. We present the details of the 
new algorithm and empirical studies over the UCI 
benchmark datasets and datasets of image classifi-
cation, and show that the proposed approach is ef-
fective for heterogeneous data classification. 

1  Introduction 

Ensemble approaches such as bagging and boosting have 
been  successfully  applied  to  many  classification  problems 
[Dietterich, 2000; Bauer and Kohavi, 1999]. The basic idea 
of ensemble methods is to construct a number of classifiers 
over training data and then classify new data points by tak-
ing a (weighted) vote of their predictions. Thus, two impor-
tant  issues  associated  with  an  ensemble  approach  are:  1) 
how to generate accurate yet diverse classification models, 
and 2) how to combine the models for ensemble classifica-
tion.  Diverse  classifiers  ensure  good  ensembles  [Quinlan, 
1996]. In this paper, we focus on the first issue with an em-
phasis on heterogeneous data classification. Heterogeneous 
data classification refers to the problem when input data of a 
single  class  are  widely  distributed  into  multiple  modes.  It 
arises when training data are collected under different envi-
ronments or through different sources. An example of het-
erogeneous  data  classification  is  image  classification,  in 
which labeled images are acquired from multiple resources 
and  exhibit  disparate  characteristics.  For  instance,  some 
images are black and white, and others are colorful.  

A widely used approach for constructing an ensemble of 
models is to sample different subsets from the training data 
and  create  a  classification  model  for  each  subset.  Bagging 
[Briemann,  1996]  and  AdaBoost  [Schapire  and  Singer, 
1999] are two representative methods in this category. Bag-
ging  randomly  draws  samples  from  the  training  data  with 
replacement and AdaBoost samples training data according 
to a dynamically changed distribution, which is updated by 
putting  more  weight  on  the  misclassified  examples  and 
smaller  weights  on  the  correctly  classified  examples. 
Clearly,  both  methods  do  not  treat  homogeneous  data  and 
heterogeneous data differently.  

For  ensemble  methods  to  work  effectively  on  heteroge-
neous data, one intuitive solution is to first divide the het-
erogeneous  data  into  a  set  of  homogeneous  partitions  and 
then  to  create  a  model  for  each  partition  of  data.  Member 
classifiers  built  with  different  homogeneous  partitions  will 
likely result in good diversity of an ensemble. One way to 
realize this homogeneity-based partition is to employ stan-
dard clustering algorithms, such as K-means [Hartigan and 
Wong, 1979] and the EM clustering algorithm [Celeux and 
Govaert, 1992]. An example is the Gaussian Mixture Model 
(GMM).  But,  in  general,  there  are  two  problems  with  this 
simple clustering approach:  

•  Single  cluster  membership.  Most  clustering  algo-
rithms  assume  that  cluster  membership  is  mutually  exclu-
sive and each data point can only belong to a single cluster. 
Even though the EM clustering algorithm allows soft mem-
bership for a data point, in the resulting clusters, each data 
point  still  only  belongs  to  a  single  cluster  [Witten  and 
Frank, 2000]. Therefore, when we use these clustering algo-
rithms to partition data, if the number of clusters is large and 
the subsets of training data formed by a clustering algorithm 
are mutually disjoint, some clusters may have a very small 
number of data points, which can lead to unreliable classifi-
cation  models.  This  is  similar  to  the  data  fragmentation 
problem  occurred  in  decision  tree  induction  [Quinlan, 
1993]. In contrast, the subsets of training data produced by 
Bagging and AdaBoost are not mutually disjoint. For exam-
ple,  in  bootstrap  sampling,  each  subset  contains  around 
63.2% of the original training data.  

•  Unbalanced  cluster  sizes.  Since  most  clustering  al-
gorithms do not have control over cluster sizes, unbalanced 
cluster sizes resulting from clustering cannot be easily cor-

rected. When the resulting clusters have very different sizes, 
a classifier built over a small cluster can be unreliable and 
thus  degrade  the  performance  of  the  ensemble  in  forming 
final ensemble classification. On the contrary, both Bagging 
and  AdaBoost  have  data  samples  of  similar  sizes  when 
learning different models. Note that there have been previ-
ous efforts on balancing the sizes of different clusters, par-
ticularly for spectral clustering algorithms (e.g., the normal-
ized cut algorithm [Melta & Shi, 2001]). But, since the con-
trol of cluster size comes indirectly from the objective func-
tion, the resulting clusters can still have unbalanced sizes.  

In sum, a clustering approach may produce homogeneous 
data  partitions,  but  cannot  ensure  similar  sizes  of different 
partitions; methods like Bagging can produce equally sized 
partitions,  but  partitions  are  not  homogeneous.  Therefore, 
we need a novel approach to partitioning data into homoge-
neous subsets of similar sizes in ensemble learning for het-
erogeneous data classification. 

The goal of this work is to divide heterogeneous data into 
homogeneous  subsets  of  similar  sizes  in  order  to  generate 
reliable and accurate classification models. By focusing on 
homogeneous subsets, we do not require that each data point 
belong to one subset; by ensuring similar sizes of data sub-
sets,  each  classification  model  can  be  built  with  a  similar 
number  of  data  points.  In  this  paper,  we  propose  a  HISS 
(Homogeneous  data  In  Similar  Size)  algorithm  specially 
designed  for  the  above  purposes  for  heterogeneous  data 
classification. Specifically, HISS allows the user to specify 
the size of a subset. For example, the user can ask the algo-
rithm to create 20 subsets with each containing 40% of the 
original data. This algorithm is similar to the bootstrap sam-
pling procedure in that both the number of subsets and the 
percentage of training data covered by each cluster can be 
specified  and  varied.  However,  it  differs  from  the  simple 
bootstrap sampling procedure in that it puts the similar data 
points  into  a  single  subset  while  bootstrap  sampling  ran-
domly selects data to form a subset. This property is impor-
tant in ensemble learning for classifying heterogeneous data. 
We will use strata for the homogeneous data partitions, and 
subsets for data partitions resulting from random sampling. 

2  Related Work 
There have been many previous studies on how to create an 
ensemble  of  models.  The  methods  for  constructing  an  en-
semble of models can be categorized into five groups [Diet-
terich, 2000]: 1)  Bayesian  methods,  which  creates  an  en-
semble of model by sampling them from a estimated poste-
rior model distribution; 2)  Sampling 
training  examples, 
which  creates  multiple  subsets  of  training  examples  and 
trains a classifier for each of the subsets; 3)  Sampling input 
features,  which  creates  a  number  of  subsets  of  the  input 
features and a classifier is built for each subset of input fea-
tures;  4)  Error correct output code (ECOC), which convert 
a multiple class problem into a set of binary class problems; 
5) Injecting randomness, that generates ensembles of classi-
fiers by injecting randomness into the learning algorithm.  

Among the five categories, our work is closely related to 
the  second  one,  which  creates  multiple  classifiers  by  sam-

pling  training  examples.  Important  methods  in  this  group 
include  Bagging  [Brieman,  1996]  and  AdaBoost  [Schapire 
and  Singer,  1999].  Although  these  methods  have  been 
shown  to  be  effective  for  classification,  they  are  not  de-
signed to take into account characteristics of heterogeneous 
data.  In  this  paper,  we  propose  HISS  –an  algorithm  that 
constructs  homogeneous  strata  from  heterogeneous  data 
while maintains the nice property of boostrap sampling pro-
cedure  -  each  stratum  contains  a  similar  number  of  data 
points.  

Another line of research closely related to this work is the 
study  of  clustering  algorithms.  In  general,  clustering  algo-
rithms  can  be  categorized  into  parametric  approaches  and 
non-parametric  approaches.  The  parametric  approach  is  to 
find a parametric model that minimizes a cost function asso-
ciated with instance-cluster assignments. Such methods in-
clude  the  Mixture  Model  [Celeux  and  Govaert,  1992]  and 
K-means  algorithm.  For  the  non-parametric  approaches,  a 
cost function is  minimized by either  merging two separate 
clusters  into  a  larger  one  or  dividing  a  cluster  into  two 
smaller ones. The representative examples of this category 
are the agglomerative approach and the divisive approach.  

Most  clustering  approaches  assume  that  each  data  point 
only belongs to a single cluster. This assumption may not be 
appropriate since the ultimate goal of clustering is to group 
similar data points together. When it is uncertain to assign a 
data point to a single cluster, it is better off assigning it to 
multiple  clusters.  Although  the  traditional  probabilistic 
model and the fuzzy clustering algorithm allow for multi- or 
soft-memberships, the uncertainty of cluster membership is 
only  exploited  during  the  process  of  estimation.  In  the  re-
sulting clusters, each data point is assigned to only a single 
cluster. Furthermore, most clustering algorithms do not have 
any  control  over  the  size  of  clusters.  Hence,  the  resulting 
clusters can be very unbalanced in size and the clusters of 
too small sizes could be useless in learning.  

3  The HISS Algorithm for Model Generation  

3.1  From Probabilistic Clustering to HISS 

We  first  describe  the  traditional  probabilistic  clustering 

algorithm, and then introduce algorithm HISS. 

The general idea of probabilistic clustering is to describe 
data with a mixture of generative models. Optimal parame-
ters  are  usually  obtained  by  maximizing  the  likelihood  of 
data using the mixture model. Let n be the number of input 
data points, K be the number clusters, 
 be the 
m   be  the  underlying  models 
input  data,  and 
}K
that generate the data. By assuming that each data point is 
m ,  we 
generated  from  a  mixture  of  models
}K
have the likelihood of the data written as: 

m m
{ ,
1
2

m m
{ ,
1
2

x x
{ ,
1
2

x
}n

,...,

,...,

,...,

τ
l m
({ } , )
i

K
j
1
=

=

∑ ∑
j
τ
i

log

n

i

1
=





K

j

1
=

p x m
(

|

i

)

j

 





(1) 

i

j

|

p x m  is the likelihood of generating 
)
(
j
jm , and 
iτ  is the likelihood for data point 

ix from the 
where 
ix  to be 
model 
in the j-th cluster. Based on the assumption that each data 
point can only belong to a single cluster, we have constraint 
∑
.  An  example  of  probabilistic  clustering  is  the 
Gaussian  Mixture  Model  (GMM),  in  which  both 
iτ and 
p x m  are parameterized as: 
(

j τ=

=

1

)

K

|

1

j

i

j

i

j

ing a reliable and accurate ensemble for heterogeneous data. 
By  setting  γ  to  be  a  reasonably  large  value  (0.4  in  this 
work), we ensure that each stratum has a sufficiently large 
number  of  examples  for  building  a  statistical  learning 
model. For later reference, we refer this new clustering ap-
proach as “HISS”, which stands for Homogeneous data In 
Similar Size.  
3.2  Optimization for HISS 
Putting Equations (3) and (4) together, we have: 

p x m
(
j

|

i

)

=

d

(

)
2
π

1
/2
|

Σ

j

|
1/ 2

j

τ θ
i
j

=

, and
(

−



exp

x
i

−

µ
j

)
T

(

x
i

−

µ
j

)

1
−
Σ
j
2

 






(2) 

jθ  denotes the prior for the j-th cluster, and 

jµ  and 
where 
jΣ   are  the  mean  and  variance  matrix  for  the  j-th  cluster, 
respectively. Expectation and Maximization algorithm (EM) 
(Dempster et al, 1977) can be used to search for the optimal 
parameters.  

By  removing  the  constraint 

,  we  allow  each 
data point to belong to multiple homogeneous clusters, or in 
short, strata. Hence, the optimization problem becomes 

j τ=

=

1

1

j

i

K

∑

=

∑ ∑
j
τ
i

log





K

j

1
=

n

i

1
=

p x m
(

|

i

)

j





 

(3) 

j
,
τ
i

K
j
1
=

l m
i

τ
max ({ } , )
m
i
subject to 
0

1  for 

i

j

=

=

≤

≤

K

1,...,

1,...,

n
j
,  and, 

j
τ
i
iτ  are constrained to between 0 and 1 to maintain 
where all 
the probability interpretation. It is easy to see that the opti-
mal solution is to set all 
iτ  to be 1, which means that each 
data point is included in every stratum. 

j

To avoid the trivial solution for 

iτ , we choose to enforce 
the  percentage  of  training  data  that  are  covered  by  each 
cluster to be a predefined constantγ, i.e., 

j

1
n

∑

j

n
τ γ
i
i
1
=

=

,  0

≤ ≤
γ

j
1,  for 

=

1,...,

K

 

(4) 

With the above constraint, we guarantee that the number of 
data points that support each stratum is around nγ .  

Compared to the single membership constraint, this new 
constraint has the following two advantages: 1) It does not 
assume that each data point has to belong to one stratum. 
For this new stratifying method, on average each data point 
can belong to  Kγ  number of strata. Therefore, when  Kγ  is 
larger  than  one,  each  data  point  is  allowed  to  be  in  more 
than one stratum simultaneously. 2) It ensures that differ-
ent strata have balanced numbers of data points. In  con-
trast  to  most  clustering  algorithms,  the  new  algorithm  en-
sures almost the same size for each stratum. This is particu-
larly important to the research goal of this paper - generat-

j
,
τ
i

K
j
1
=

l m
i

τ
max ({ } , )
m
i
subject to
1
∑
n
0
j
≤
τ
i

n
τ γ
i
i
1
=

1  for 

,  0

=

≤

i

j

=

∑ ∑
j
τ
i

log





K

j

1
=

n

i

1
=

p x m
(

|

i

)

j





 

(5) 

≤ ≤
γ

j
1,  for 

=

1,...,

K

 

=

1,...,

n
j
,  and, 

=

1,...,

K

|

p x m , i.e., 
Let us assume the Gaussian distribution for
(
N µ σ . Following the idea of the EM algo-
p x m
(
(
rithm, the difference in the likelihood of data between two 
consecutive iterations is bound by: 

) ~

)

)

,

|

i

j

j

j

j

( )} , ( ))

τ

t

K
i
1
=

l m t
({
(

i

+

τ
t
1)} , (

K
i
1
=

+

≥

∑ ∑

n
i
1
=

K
j
υ
i
j
1
=

t
( )log

∑ ∑

n
i
1
=

K
j
υ
i
j
1
=

t
( )log






+

l m t
({
1))
−
i
j
+

t
(
1)
τ
i


j
t
( )
τ




i
p x m t
(
(
|
1))
+
i
p x m t
( ))
(

|

j

i

j

where 

j

iυ  is defined as 
t p x m t
( )
j
τ
i
∑
K
k
τ=
i
j

(
( ))
t p x m t
( )

j
υ
i

t
( )

|
(

j
|

=

1

k

i

i

( ))

 

(6) 

(7) 






 

Thus, the optimal solutions for the mean and variance of 
Gaussian distribution can be obtained as follows: 

t
(

t
(

µ
j

,
2
σ
j

1)
+ =

1)
+ =

∑
∑

n
t x
( )
j
υ
i
i
i
1
=
n
t
( )
j
υ
i
i
1
=

n
t x
( )
j
2
υ
i
i
i
1
=
n
t
( )
j
υ
i
i
1
=

∑
∑
iτ   is  rather  difficult  to 
However,  the  optimal  solution  for 
. 
obtain  because  of  the  inequality  constraints  0
Directly optimizing the Equation (6) with only the equality 
tτ + : 
constraint will result in the following solution for 

j
iτ≤

2
µ
j

1)

1)

1 

t
(

≤

−

+

(

j

j

i

j
τ
i

t
(

+

1)

=

i

j

t n
( )
υ γ
∑
t
( )
j
υ=

n
i

1

i

 

(8) 

j

Apparently, the above solution will always be nonnegative 
tτ  is nonnegative. However, it does not guarantee that  
if  
i
tτ +  is not greater than 1.  
(
j

( )
1)

i

Finding Optimal 

j

tτ +  
1)
i

(

 

j

i

n

=

1,...,

 and 

Inputs: 
tυ  for 
( )
i
Outputs: 
tτ +  that maximizes Equation (7).  
(
1)
j
i
Initialization:  
1)

 and 

1,...,

1,...,

1,...,

K

K

=

n

=

=

j

j

 

 

 

j

i

(

tτ +
i

=  for 

0
for each cluster j 
   do 
        For all examples i,  

j

(

set 

tτ +
i

(
        Compute the probability mass 
1) 1}
+ =

=  if 

1) 1

n
γ

i
{ |

tτ + >  
1) 1
i

t
(

−

 

j
τ
i

j

s
=
       Re-compute  

    

j
∀

 s.t. 

j
τ
i

t
(

1) 1
+ <

 

t
( )

j
τ
i

t
(

1)
+ =

t
( )

s

j
υ
i
∑
j
t
i
( 1) 1}
{ |
+ <
τ
i
j
tτ
(
 s.t. 
+
i

j
υ
i

1) 1

> ) 

while (

j
∃

end 
 

j

j

j

(

≤

≤

1 

1 

1)

j
iτ≤
1)

tτ +  
1)
i
j
iτ≤

Figure 1: Algorithm for finding optimal 

(
In order to satisfy the inequality constraints  0

tτ + . The basic idea is to reset 
i

, 
we  use  the  KKT  conditions  [Fletcher,  1987]  to  efficiently 
iτ  to 
adjust the value of 
be  1  whenever  the  output  from  Equation  (10)  violates  the 
constraint  0
.  After  the  adjustment,  we  will  re-
tτ +   that  are  less  than  1  using  Equation  (8). 
compute 
(
j
i
tτ +  will 
The procedure of adjusting and recomputing 
tτ +  violates the constraint. Figure 1 
continue until no 
shows the detailed steps for finding the optimal solution for 
tτ + . Due to the space limit, the proof for the optimality 
i
of the algorithm in Figure 1 is not provided here.  
3.3 Classifying Heterogeneous Data 

1)

1)

1)

(

(

(

j

i

j

i

j

For  classification  problems,  heterogeneous  data  can  be 

found in many applications and in experiments:  
1)  Data  acquired  from  multiple  sources.  In  many  cases, 
training  data  are  acquired  from  multiple  sources.  Because 
each source has its own data distribution that may be differ-
ent from others, the data merged from multiple sources are 
therefore  heterogeneous.  For  example,  consider  building  a 
classification model for outdoor scenes. The training images 
are collected from several different types of videos. Some of 
the videos are news stories and some of them are of adver-
tisement.  Some  of  them  are  of  high  quality  and  some  of 
them  are  not.  Thus,  the  widely  disparate  characteristics  in 
videos cause the merged data to be heterogeneous. 
2)  Data by converting a multiple class problem into a set 
of binary class problems. In order to apply the binary class 
classification  algorithm  to  multiple  class  case,  we  need  to 

Data Set 

# Examples  #Class  # Features

Ecoli 
Pendigit 
Glass 
Yeast 
Vehicle 

327 
2000 
204 
1479 
946 
3500 
3500 

5 
10 
5 
10 
4 
2 
2 

7 
16 
10 
8 
17 
190 
126 

Image/Indoor
Image/Outdoor
Table  1:  Description  of  datasets  for  the  experi-
ment for heterogeneous data classification. 

convert the classification problem of multiple classes into a 
set  of  binary  class  problems.  The  representative  examples 
include the one-against-all approach and error correct output 
coding  (ECOC)  method  [Dietterich,  1995].  During  this 
process,  multiple  classes  are  grouped  into  two  subsets  of 
classes. Data points from one subset of classes are used as 
positive  examples  and  the  remaining  are  used  as  negative 
examples. Because both the positive and negative pools can 
be comprised of examples from multiple classes, it will cre-
ate data heterogeneity for each of the binary classes. 

As discussed, an intuitive solution to classifying hetero-
geneous data is to create a set of classification models with 
each classifier built on a homogeneous partition (stratum) of 
the  data,  and  then  combine  classifiers  for  the  final  predic-
tion.  The traditional clustering algorithms are not designed 
for  this  task  because  of  the  potential  unbalanced  cluster-
sizes  and  the  data  fragmentation  problem.  With  the  pro-
posed algorithm HISS, we can avoid these two problems by 
setting the parameter to be large (0.4 in the experiment).  

In  sum,  to  classify  heterogeneous  data,  we  first  apply 
HISS to obtain homogeneous strata and then create a classi-
fication  model  for  each  stratum  to  form  an  ensemble.  We 
will refer to this model generation method as ‘HISS-based 
Model  Generation’  in  our  empirical  study  next.  Finally,  a 
stacking approach [Wolpert, 1992] is used to combine mod-
els that are generated by the HISS-based model generation 
method for the final prediction of the ensemble. 

4.  Experimental Study 
The experimental study is designed to answer the following 
questions: 
Is the proposed model generation method effective for 
1) 
classifying  heterogeneous  data?  To  this  end,  we  compare 
the  proposed  model  generation  method  to  Bagging  and 
AdaBoost in classifying heterogeneous datasets.  
Is the proposed HISS algorithm effective for generating 
2) 
reliable  models?  To  address  this  question,  we  will  apply 
both  the  proposed  HISS  algorithm  and  the  probabilistic 
clustering algorithm to partition the training data and build a 
classification model for each partition.  
4.1  Experimental Design 
Seven  different  datasets  are  used  in  the  experiments:  five 
multiple class datasets from the UCI Machine Learning re-
pository [Blake and Merz, 1998] and two binary class data-

Baseline 

Data Set 

AdaBoost     
(Standard) 

Bagging     
(Standard) 
0.047 (0.012)  0.046 (0.006)  0.057 (0.014) 
0.010 (0.003)  0.013 (0.003)  0.012 (0.002) 
0.382 (0.027)  0.385 (0.081)  0.379 (0.046) 
0.314 (0.012)  0.320 (0.023)  0.313 (0.013) 
0.103 (0.020)  0.163 (0.048)  0.131 (0.024) 
0.153 (0.008)  0.140 (0.007)  0.156 (0.014) 
0.116 (0.008)  0.111 (0.017)  0.120 (0.011) 

AdaBoost  
(Stacking) 
0.059 (0.006) 
Ecoli 
0.012 (0.003) 
Pendigit 
0.379 (0.027) 
Glass 
0.315 (0.008) 
Yeast 
0.085 (0.033) 
Vehicle 
0.144 (0.007) 
Image/Indoor 
Image/Outdoor 
0.112 (0.007) 
Table 2: Classification errors for the baseline model (SVM), AdaBoost, Bagging and the propose model generation 
method (‘HISS-based Ensemble’). The column ‘Bagging (Stacking)’ refers to the case when the ensemble of mod-
els is created by the Bagging algorithm but combined through the stacking approach using an SVM. The same is 
for the column ‘AdaBoost (Stacking)’.  The variance of classification error is listed in parenthesis. 

HISS-based 
Ensemble  
0.037 (0.006) 
0.008 (0.002) 
0.161 (0.044) 
0.313 (0.013) 
0.048 (0.012) 
0.140 (0.013) 
0.088 (0.005) 

Bagging  
(Stacking) 
0.046 (0.006) 
0.012 (0.001) 
0.379 (0.027) 
0.315 (0.012) 
0.100 (0.017) 
0.157 (0.011) 
0.114 (0.006) 

sets  for  image  classification.  The  characteristics  of  these 
seven datasets are listed in Table 1. 
   For the multiple class datasets, we introduce the hetero-
geneity  into  the  data  by  converting  the  original  multiple-
class problem into a binary one. Similar to the one-against-
all approach, examples from the most popular class are used 
as the positive instances and examples from the remaining 
classes  are  assigned  to  the negative  class. Because data  of 
the negative class are from multiple classes, we would ex-
pect some degree of heterogeneity inside the negative class. 
For  the  two  datasets  of  image  classification,  they  both  are 
binary classification problems. The heterogeneity of data is 
due  to  the  fact  that  images  are  from  seven  different  video 
clips and each video clip provides 500 images. Since each 
video  clip  is  of  different  type  (e.g.,  varied  quality  in  im-
ages),  we  would  expect  certain  amount  of  heterogeneity 
within the data.   

The baseline algorithm used in this experiment is support 
vector machine [Burger, 1998]. In all the experiments, each 
ensemble  method generates  20 different  SVMs;  a  stacking 
approach [Wolpert, 1992] that also uses a SVM is employed 
to combine the outputs from all 20 models to form the final 
prediction  of  the  ensemble.  For  each  experiment,  we  ran-
domly select 70% of the data as training and the remaining 
30% as testing. The experiment is repeated 10 times and the 
average  classification  error  of  the  ten  runs  is  used  as  the 
final result with the variance of classification errors.  
4.2  Heterogeneous Data Classification 
Table 2 shows classification errors for the baseline support 

Data Set 

HISS 

EM 

EM 

(3 Clusters) 

(10 Clusters)
0.037(0.006)  0.448 (0.021)  0.448 (0.021)
Ecoli 
0.008(0.002)  0.081 (0.043)  0.110 (0.023)
Pendigit 
0.161(0.044)  0.292 (0.101)  0.353 (0.017)
Glass 
0.313 (0.013)  0.314 (0.013)  0.314 (0.019)
Yeast 
0.048 (0.012)  0.219 (0.068)  0.052 (0.026)
Vehicle 
Image/Indoor  0.140(0.013)  0.184 (0.022)  0.203 (0.014)
Image/Outdoor  0.088(0.005)  0.156 (0.031)  0.182 (0.036)
Table 3: Classification error for using different cluster-
ing algorithms for model generation. ‘EM’ refers to us-
ing Expectation-Maximization algorithm to cluster data.  

vector  machine,  the  proposed  HISS-based  ensemble  learn-
ing  approach,  standard  Bagging  and  standard  AdaBoost. 
First,  we  can  see  that  the  baseline  model  performs  well 
comparing with both standard Bagging and AdaBoost. This 
observation  indicates  that  these  seven  heterogeneous  data-
sets  are  rather  difficult  for  the  standard  ensemble  ap-
proaches to learn. In contrast, the proposed HISS-based en-
semble method performs better than the baseline model and 
the  two  standard  ensemble  methods.  For  the  datasets 
‘Glass’,  ‘Vehicle’,  and  ‘Image/Outdoor’,  the  improvement 
is  substantial,  from  38.2%  to  16.1%  for  ‘Galss’,  10.3%  to 
4.8%  for  ‘Vehicle’,  and  from  11.6%  to  8.8%  for  ‘Im-
age/Outdoor’.  

Since the HISS-based ensemble method uses the stacking 
approach for combining different models, it is different from 
the combination method that is used by AdaBoost and Bag-
ging.  To  address  this  difference,  we  conduct  the  experi-
ments that apply a stacking method to combine the models 
generated  by  both  Bagging  and  AdaBoost.  The  results  are 
listed  in  Table  2  on  the  right  side  of  the  HISS-based  ap-
proach,  titled  as  ‘Bagging  (Stacking)’  and  ‘AdaBoost 
(Stacking)’, respectively. Compared these results to the re-
sults  of  ‘Bagging  (Standard)’  and  ‘AdaBoost  (Standard)’, 
we  see  that  there  is  no  substantial  change  in  classification 
errors when using a stacking approach to combine models in 
ensemble learning. For all the seven datasets, the ensemble 
of models generated by HISS performs the best. The reason 
why a stacking approach is useful for the HISS-based model 
generation  method  but  not  to  the  other  two  is  that  models 
generated by the HISS-based algorithm are much more di-
verse  than  the  ones  generated  by  both  Bagging  and 
AdaBoost. As a result, applying another layer of classifica-
tion model to combine the outputs from the distinguishable 
models (or stacking) will be able to take full advantage of 
all the models and obtain the best performance. 

Based  on  the  above  discussion,  we  conclude  that  the 
HISS-based ensemble model is more effective for classify-
ing heterogeneous data than existing ensemble approaches. 
4.3  Comparison with Other Clustering-based En-

semble Methods 

The advantage of HISS versus the traditional clustering al-
gorithms is that HISS allows each data point to be in multi-
ple  different  strata.  Thus  it  can  ensure  that  the  number  of 

data  points  distributed  over  each  stratum  is  of  similar  size 
and sufficiently large. 

In this experiment, we use both the traditional clustering 
algorithm and the proposed HISS algorithm for model gen-
eration and see how different they are in classifying the het-
erogeneous datasets. To observe the effect due to the trade-
off  between  the  number  of  strata  and  the  number  of  data 
points  in  each  stratum,  we  consider  two different numbers 
of strata (or clusters) for the traditional clustering algorithm: 
10 and 3( 1/γ≈
). We did not use 20 clusters in the compari-
son because for some datasets the traditional clustering al-
gorithm  is  unable  to  produce  the  full  twenty  clusters.  The 
traditional clustering algorithm used in the experiment is the 
probabilistic EM clustering algorithm. Similar to the HISS-
based  ensemble  approach,  a  stacking  method  is  used  to 
combine models generated by the EM clustering algorithm. 
The  results  for  using  EM  clustering  algorithms  for  model 
construction  are  listed  in  Table  3,  titled  ‘EM  (3  clusters)’ 
and  ‘EM  (10  clusters)’.  As  suggested  by  Table  3,  the  in-
creasing number of clusters can lead to degraded perform-
ance. This is because a large number of clusters will form 
clusters with  a  small  number  of data points,  which  can be 
insufficient for building a reliable classification model. On 
the  other  hand,  as  already  indicated  in  the  previous  study 
[Ditterich,  2000],  being  able  to  generate  a  relatively  large 
number of models is critical to the success of the ensemble 
approach.  The  proposed  HISS  algorithm  can  satisfy  both 
needs  by  introducing  the  substantial  overlapping  between 
different  clusters.  As  shown  in  Table  3,  the  HISS-based 
method  outperforms  the  EM_clustering-based  ensemble 
approaches  substantially  for  almost  all  datasets  except  for 
‘Yeast’ (similar). The most noticeable cases are ‘Ecoli’ and 
‘Pendigit’, for  which  the  classification  errors of  EM-based 
clustering  approaches  are  one  order  more  than  that  of  the 
HISS-based ensemble algorithm. 

Based  on  the  above  experiments  and  analysis,  we  con-
clude that the HISS-based model generation is an effective 
method for model generation in ensemble learning for het-
erogeneous data classification. 

5.  Conclusion and Future Work 

In this paper, we propose and examine a new method for 
generating an ensemble of models, which is to first partition 
data into homogeneous subsets and then create a model for 
each  subset.  A  traditional  clustering  algorithm  like  EM  is 
not suitable for the task of partitioning data due to potential 
size-unbalanced  clusters  and  the  data  fragmentation  prob-
lem.  To  address  these  two  problems,  we  propose  a  novel 
algorithm HISS, which allows for data overlapping between 
different  clusters  (strata)  and  promises  size-balanced  clus-
ters.  Empirical  studies  over  seven  different  heterogeneous 
datasets have shown that this new HISS-based model gen-
eration  method  performs  very  well  for  heterogeneous  data 
classification.  Currently,  the  proposed  HISS  algorithm  as-
sumes  equal  size  for  each  stratum  (cluster).  One  possible 
extension is to examine alternatives to balance sizes of clus-
ters.  For  example,  instead  of  enforcing  all  the  clusters  to 
have one size, we can constrain the sizes of the clusters into 

a  specified  range  to  allow  some  flexibility  in  maintaining 
high homogeneity of clusters. 

References 
[Hartigan and Wong, 1979] Hartigan, J.A. and Wong, M.A., 
A K-means Clustering Algorithm, Applied Statistics 28: 
100-108 

[Briemann,  1996]  Briemann,  L.,  Bagging  Predicator,  Ma-

chine Learning 26, 123-140, 1996 

[Schapire  and  Singer,  1999]Schapire,  R.E,  and  Singer,  Y., 
Improved  boosting  algorithms  using  confidence-rated 
predictions, Machine Learning 37 (3): 291-336, 1999 

[Celeux and Govaert, 1992] Celeux, G. and Govaert, G., A 
Classification  EM  Algorithm  for  Clustering  and  Two 
Stochastic  Versions,  Computational  Statistics  &  Data 
Analysis, vol. 14, pp. 315-332, 1992  

[Dietterich,  2000]  Dietterich,  T.G.,  Ensemble  Methods  in 
Machine  Learning.  In  Multiple  Classier  Systems, 
Cagliari, Italy, 2000 

[Dempster  et  al.,  1977]  Dempster,  A.P.,  Laird,  N.M.,  and 
Rubin,  D.B.,  Maximum  Likelihood  from  Incomplete 
Data via the EM Algorithm, Journal of the Royal statis-
tical Society, Series B, 39(1): 1-38, 1977 

[Fletcher, 1987] Fletcher, R., Practical Methods of Optimi-

zation. John Wiley and Sons, Inc., 2nd edition, 1987. 

[Dietterich  and  Bakiri,  1995]  Dietterich,  T.G.  and  Bakiri, 
G.,  Solving  Multiclass  Learning  Problems  via  Error-
Correcting  Output  Codes.  Journal  of  Artificial  Intelli-
gence Research, (2):263–286, 1995. 

[Wolpert,  1992]  Wolpert,  D.H.,  Stacked  Generalization. 

Neural Networks, 5:241-259, Pergamon Press, 1992.  

[Burges, 1998] Burges, C.J.C.,  A Tutorial on Support Vec-
tor Machines for Pattern Recognition. Data Mining and 
Knowledge Discovery, 2(2):955-974, 1998. 

[Witten and Frank, 2000] Witten, I.H. and Frank, E., Data 
Mining:  Practical  Machine  Learning  Tools  with  Java 
Implementations. Morgan Kaufmann, 2000. 

[Blake and Merz, 1998] Blake, C. and Merz, C., UCI reposi-
databases. 

tory 
http://www.ics.uci.edu/mlearn/MLRepository.html. 

ma-chine 

of 

learning 

[Quinlan, 1993] Quinlan, R.J., C4.5: Programs for Machine 

Learning, Morgan Kaufmann, San Mateo, 1993. 

[Quinlan, 1996] Quinlan R.J., Bagging, boosting, and C4.5. 
In Proceedings of the Thirteenth National Conference on 
Artificial Intelligence (AAAI 96), 1996. 

[Shi and Malik, 2000] Shi, J., and Malik, J., Normalized Cut 
and Image Segmentation. IEEE Transactions on Pattern 
Analysis and Machine Intelligence, 22(8):888-905, 2000 
[Bauer  and  Kohavi,  1999]  Bauer,  E.  and  Kohavi,  R.,  An 
Empirical  Comparison  of  Voting  Classification  Algo-
rithms:  Bagging,  Boosting,  and  Variants.  Machine 
Learning, 36(1):105-139, 1999. 


Exploiting Background Knowledge for

Knowledge-Intensive Subgroup Discovery

Martin Atzmueller and Frank Puppe

Department of Computer Science

University of W¨urzburg

Hans-Peter Buscher

DRK-Kliniken Berlin-K¨openick,
Clinic for Internal Medicine II,

97074 W¨urzburg, Germany

{atzmueller, puppe}@informatik.uni-wuerzburg.de

12559 Berlin, Germany
buscher.dhp@t-online.de

Abstract

In general, knowledge-intensive data mining meth-
ods exploit background knowledge to improve the
quality of their results. Then, in knowledge-rich
domains often the interestingness of the mined pat-
terns can be increased signiﬁcantly.
In this paper we categorize several classes of back-
ground knowledge for subgroup discovery, and
present how the necessary knowledge elements can
be modelled. Furthermore, we show how subgroup
discovery methods beneﬁt from the utilization of
background knowledge, and discuss its application
in an incremental process-model. The context of
our work is to identify interesting diagnostic pat-
terns to supplement a medical documentation and
consultation system. We provide a case study in
the medical domain, using a case base from a real-
world application.

Introduction

1
Knowledge-intensive learning methods usually exploit back-
ground knowledge, because this can improve the quality of
[Richardson and Domingos,
their results signiﬁcantly (c.f.
2003]). In this paper we exploit background knowledge for
subgroup discovery [Wrobel, 1997; Kl¨osgen, 2002], applied
for exploration or descriptive induction, to discover ”inter-
esting” subgroups concerning a certain property of interest.
Background knowledge can help to focus the algorithm for
subgroup search on the relevant patterns according to the
goals of the user, thus reducing uninteresting patterns and re-
stricting the search space. This can increase the quality of the
discovered set of subgroups signiﬁcantly, as well as the efﬁ-
ciency of the search method. It is obvious, that the amount
of available background knowledge depends on the particu-
lar application domain. For example, in the medical domain
usually a lot of background knowledge is available.

Besides (simple) constraints the background knowledge for
the presented knowledge-intensive process consists of two
categories: a task-speciﬁc subset of derived attributes and
general ontological background knowledge, that can be re-
ﬁned incrementally. Certain knowledge classes can also be
used to derive new (lower-level) knowledge. The knowledge

classes can potentially be used in other rule-based learning
methods in a straightforward manner. Similar to the applied
knowledge, subgroup discovery results can also be formal-
ized incrementally and can be provided as input to the search
method. We will introduce this approach in this paper.

Our

implementation and evaluation is based on the
knowledge-based documentation and consultation system for
sonography SONOCONSULT [Huettig et al., 2004], which is
in routine use in the DRK-hospital in Berlin/K¨openick.

The rest of the paper is organized as follows: First we de-
scribe the knowledge-intensive process for subgroup discov-
ery. We introduce the subgroup discovery method, present
the different types of suitable background knowledge and dis-
cuss their characteristics in detail. After that, an experimental
evaluation of the impact of the proposed method is demon-
strated by a case study in the medical domain. Finally, we
conclude the paper with a discussion of the presented work,
and we show promising directions for future work.

2 Knowledge-Intensive Subgroup Discovery
In this section, we ﬁrst give an overview of the subgroup dis-
covery method and deﬁne the basic concepts of our knowl-
edge representation schema. Furthermore, we present the
knowledge-intensive subgroup discovery process, and de-
scribe the background knowledge in detail.

2.1 Subgroup Discovery
Subgroup discovery [Wrobel, 1997; Kl¨osgen, 2002] is a
method to discover ”interesting” subgroups of individuals,
e.g., ”the subgroup of patients who are treated in a small, un-
derstaffed hospital are signiﬁcantly more likely to suffer from
complications in the future than the patients in the reference
population”. Subgroups are described by relations between
independent (explaining) variables and a dependent (target)
variable, rated by a certain interestingness measure. For ex-
ample, two possible criteria are the difference in the distrib-
ution of the target variable concerning the subgroup and the
general population, and the subgroup size. The main appli-
cation areas of subgroup discovery are exploration and de-
scriptive induction, to obtain an overview of the relations
between a target variable and usually many explaining vari-
ables. Subgroup discovery does not necessarily focus on ﬁnd-
ing complete relations; instead partial relations, i.e., (small)
subgroups with ”interesting” characteristics can be sufﬁcient.

Before deﬁning the subgroup discovery task, we ﬁrst intro-
duce the necessary notions concerning our knowledge repre-
sentation schema. Let ΩA the set of all attributes. For each
attribute a ∈ ΩA a range dom(a) of values is deﬁned. Fur-
thermore, we assume VA to be the (universal) set of attribute
values of the form (a : v), where a ∈ ΩA is an attribute and
v ∈ dom(a) is an assignable value.

A subgroup discovery task mainly relies on the following
four main properties:
the target variable, the subgroup de-
scription language, the quality function, and the search strat-
egy. The target variable may be binary, nominal or numeric.
Depending on its type, there are different analytic questions,
e.g., for a numeric target variable we can search for signiﬁ-
cant deviations of the mean of the target variable.

A subgroup discovery problem encapsulates the target vari-
able, the search space of independent variables, the general
population, and additional constraints.
Deﬁnition 1 (Subgroup Discovery Problem). A subgroup dis-
covery problem SP is deﬁned as the tuple
SP = (T, A, C, CB) ,

where T ∈ ΩA∪VA is a target variable. A ⊆ ΩA is the set of
attributes to be included in the subgroup discovery process.
CB is the case base representing the general population used
for subgroup discovery. C speciﬁes (optional) constraints for
the discovery method. We deﬁne ΩSP as the set of all possible
subgroup discovery problems.

The deﬁnition above allows for arbitrary target variables.
However, for our analytic questions we will focus on binary
target variables, i.e., T ∈ VA.
The description language speciﬁes the individuals from the
reference population belonging to the subgroup.
Deﬁnition 2 (Subgroup Description). A subgroup description
sd = {ei} consists of a set of selection expressions (selectors)
ei = (ai, Vi) which are selections on domains of attributes,
i.e., ai ∈ ΩA, Vi ⊆ dom(ai). A subgroup description is de-
ﬁned as the conjunction of its contained selection expressions.
We deﬁne Ωsd as the set of all possible subgroup descriptions.
A quality function measures the interestingness of the sub-

group (c.f., [Kl¨osgen, 2002] for examples).
Deﬁnition 3 (Quality Function). A quality function

q : Ωsd × ΩSP → R

evaluates a subgroup description sd ∈ Ωsd given a sub-
group discovery problem SP ∈ ΩSP . It is used by the search
method to rank the discovered subgroups during search.

the quality function qBT , the quality function qRG only com-
pares the target shares of the subgroup and the total popula-
tion measuring the relative gain. Therefore, a suitable support
threshold is necessary to discover signiﬁcant subgroups.

Considering the subgroup search strategy an efﬁcient
search is necessary, since the search space is exponential con-
cerning all the possible selectors of a subgroup description.
Commonly, a beam search strategy is used because of its ef-
ﬁciency [Kl¨osgen, 2002]. For our search strategy, we use
a modiﬁed beam search strategy, where an initial subgroup
description can be selected as the initial value for the beam.
Beam search adds a selector to the k best subgroup descrip-
tions in each iteration. Iteration stops if the quality as evalu-
ated by the quality function q does not improve any further.
2.2 The Knowledge-Intensive Process Model
Often suitable background knowledge can help both to focus
the subgroup discovery task on the relevant patterns concern-
ing the given analysis goals, and to restrict the search space.
We propose an incremental approach in which background
knowledge can be applied initially at the start, but also dur-
ing the discovery process. The proposed knowledge-intensive
process for subgroup discovery is depicted in Figure 1.

We start with a deﬁned population given as a case base
CB and existing background knowledge, if available. For the
analysis task deﬁned by a subgroup discovery problem the
subgroup discovery method generates a set of subgroups. If
these are considered as interesting by the user, the results are
presented, and the process is ﬁnished. Otherwise the sub-
groups are analyzed, and background knowledge including
constraints can be added to the subgroup discovery problem.
Additionally, selected subgroups can be used as the basis for
further reﬁnement. Then the process continues with a new
iteration: all constraints of the current subgroup discovery
problem are applied for the search process. In summary, the
knowledge-intensive process consists of three main steps:
1. DISCOVER: Discover subgroups – result set SG
2. INSPECT: If SG is interesting – STOP and present the

subgroup discovery results; otherwise

3. REFINE: Analyze the discovered subgroups; adapt the
subgroup discovery problem,
i.e., extend or modify
background knowledge and/or use interesting subgroups
as a starting point for step 1. GOTO step 1

are given by

pp0 · (1 − p0)
(p − p0) · √

n

·

r N

For binary target variables, examples for quality functions

p − p0

qBT =

qRG =

,

N − n

p0 · (1 − p0) ,
where p is the relative frequency of the target variable in the
subgroup, p0 is the relative frequency of the target variable in
the total population, N = |CB| is the size of the total popu-
lation, and n denotes the size of the subgroup. In contrast to

Figure 1: The Knowledge-Intensive Process Model

Step 3 is performed by the domain specialist according to the
analysis goals. Examples for the adaptation of the subgroup
discovery problem are given in the case study in Section 3.

2.3 Background Knowledge for Subgroup

Discovery

We want to make the acquisition of helpful background
knowledge as easy as possible. Therefore, we try utilize
knowledge known from other knowledge systems, that the
domain specialist is already familiar with. There are different
classes of background knowledge which can be used in the
knowledge-intensive process for subgroup discovery: con-
straints, ontological knowledge, abstraction knowledge, and
pattern knowledge. Knowledge acquisition is always expen-
sive, so its costs should be minimized. Sometimes knowledge
can be derived from already formalized knowledge, e.g., we
can derive constraints from ontological knowledge, and thus
reduce its acquisition costs.

In the following table, we summarize the different classes
and types of background knowledge (CK = constraint knowl-
edge, OK = ontological knowledge, PK = pattern knowledge,
AK = abstraction knowledge). We show their characteris-
tics in terms of the ’derivable knowledge’ if applicable, their
costs, and their potential contribution to restricting the search
space and/or focusing the search process. Considering the
costs and the impact of the knowledge types on the search
space, the label - indicates no cost/impact; the labels +, ++,
and +++ indicate increasing costs and impact. A +(+) signi-
ﬁes, that the respective element has low costs if it can be de-
rived/learned, and moderate costs otherwise. Similarly ++(+)
indicates this for moderate and high costs, respectively.

Knowledge
Class
CK
CK
CK
CK
CK
OK
OK

Type
Syntactical Constr.
Quality Constr.
Attr. Values Constr.
Meta Values Constr.
Attributes Constr.
Normality Info
Abnormality Info

Similarity Info
Ordinality Info
Attr. Weights
Derived Attributes
Subgroup Pattern
Priority Groups

OK
OK
OK
AK
PK
PK
The most important types of background knowledge with
an especially good cost/beneﬁt ratio are indicated in bold
type. Derived attributes are a special case with potentially
high beneﬁt as discussed below. Then, the need for derived
attributes depends on the speciﬁc application domain.
Constraint Knowledge Constraints are a class of back-
ground knowledge, which is especially simple to apply. The
different constraint types can be described as follows:
• Attribute values constraints: the attribute values can be
• Meta values constraints: speciﬁc value groups deﬁning
an abstracted ’meta value’ can be speciﬁed, e.g., inter-
vals for ordinal values. Meta values are not restricted to
intervals, but can cover any combination of values.
• Constraints for attributes: speciﬁc attributes and/or com-
• General constraints restricting the syntactical form of the
discovered subgroups, or quality constraints for the dis-
covered subgroups can be applied.

binations of attributes can be excluded.

restricted to the set of relevant values.

Derivable
Knowledge
–
–
–
–
–
Attr. Val. Constr.
Attr. Val. Constr.
Meta Val. Constr.
Meta Val. Constr.
Meta Val. Constr.
Attr. Constraints
Derived Attributes
Derived Attributes
–

Cost

+
+
+(+)
+(+)
+(+)
+
++

++(+)
+
+(+)
+++
+(+)
++

Search Space
Focus
Restr.
+
+
++
++
+
+
++
–
++
++
++
+
++
+
–
++
++
–
+++
++
++
+
+++
+++
+
–
–
+

Constraints can signiﬁcantly restrict the search space and fo-
cus the search process. The knowledge acquisition cost for
constraints is moderate, depending on the number of relations
that need to be modeled; the costs can also be decreased uti-
lizing ontological knowledge as described below.

Pattern Knowledge In general, pattern knowledge speciﬁes
the kinds of patterns that the user is especially interested in.
For example, pattern knowledge can be used to deﬁne already
known subgroups, that can be directly applied in the discov-
ery process for subgroup reﬁnement. As another kind of pat-
tern knowledge, the user can specify priority groups which
are partial disjunctive sets of attributes, that have an assigned
priority, e.g., depending on the association strength between
an attribute and the target variable. Then, we can start with
the set of attributes with the highest priority. If the discovered
subgroups cannot be improved any further by the attributes in
the current set, then we take the attributes in the next priori-
tized attribute set into account, in the next iteration. The costs
for pattern knowledge are not too high, if the patterns are dis-
covered automatically. For priority knowledge, the cost is
encoded in the partially disjunctive separation of attributes.
Pattern knowledge does not really restrict the search space,
but can help to focus the search.

Ontological Knowledge We can utilize ontological knowl-
edge which is commonly used in the development of knowl-
edge systems, e.g., in case-based reasoning systems. The fol-
lowing elements need to be deﬁned by the domain specialist if
they cannot be learned (semi-)automatically, e.g., [Baumeis-
ter et al., 2002].

• weights of attributes denoting their importance
• similarity information about the relative similarity be-
• abnormality information about attribute values
• ordinality information about attributes

tween attribute values

Abnormality/Normality Information If abnormality or
normality information about attribute values is available, then
each value v ∈ dom(a) of an attribute a is attached with
a label that explains, if v is describing a normal or an ab-
normal state of the attribute. Normality information only
requires a binary label while abnormality information de-
ﬁnes several categories. For example, consider the attribute
temperature with the value range dom(temperature) =
{normal, marginal, high, very high}. The values normal and
marginal denote normal states of the attribute, while the val-
ues high and very high describe abnormal states. Several cate-
gories can be deﬁned according to the degree of abnormality.
We use ﬁve degrees of abnormality given by the symbolic
values A1, A2, A3, A4, A5. Category A1 denotes a normal
value; the categories {A2, A3, A4, A5} denote abnormal val-
ues in ascending order. Using abnormality/normality knowl-
edge we can constrain the value range of an attribute: e.g., ei-
ther all values corresponding to selected abnormal categories
or the values marked with the normal category can be ex-
cluded from the domain of an attribute used in the subgroup
discovery method, by a global exclusion condition. Then, at-
tribute values constraints are derived accordingly.

Meta Values Abnormality information and similarity infor-
mation concerning attribute values can be used to deﬁne ad-
ditional meta values: e.g., if the similarity between two at-
tribute values is very high, then they can potentially be ana-
lyzed as one value, thus forming a disjunctive selection ex-
pression on the value range of an attribute. Likewise, global
abnormality groups can be deﬁned by providing groups of ab-
normality degrees that specify which values should be com-
bined. This is especially relevant in the medical domain with
attribute values such as probable, possible, and unveriﬁable.
In diagnostics the value probable contributes more evidence
to the represented concept than the value possible. However,
for analysis often the values probable and possible have an al-
most equivalent meaning and can be considered as one value.
Ordinality information that speciﬁes if an attribute is or-
dinal can be used to deﬁne meta values relating to ordinal
groups: we can generate meta values covering all adjacent
combinations of attribute values, or all ascending/descending
combinations of values, starting with the minimum or max-
imum, respectively. If abnormality information is available,
then we partition the value range by the given normal value
and only start with the most extreme value. For example,
for the ordinal attribute liver size with the values 1:smaller
than normal, 2:normal, 3:marginally increased, 4:slightly in-
creased, 5:moderately increased and 6:highly increased, we
partition by the normal value (2) and obtain the following
meta values: (1), (3, 4, 5, 6), (4, 5, 6), (5, 6) and (6).

We summarize how constraints can be derived from on-
tological knowledge: The set of relevant attributes can be
constrained using attribute weights, with a suitable thresh-
old. Given similarity and abnormality/normality information
about attribute values we can model/restrict the value ranges
of attributes. Ordinality information about attribute values
can be easily used to construct ordinal grouped meta values,
which are often more meaningful for the domain specialist.
Then, the original values can be replaced, optionally.

Abstraction Knowledge This class of background knowl-
edge is given by derived attributes that are inferred from ba-
sic attributes or other derived attributes. Derived attributes
often correspond to certain known dependencies between at-
tributes, or known intermediate concepts, that are not stored
as basic attributes. For example, in the medical domain, we
can infer the derived attribute body mass index, given the at-
tributes height and weight. Additionally, if there are a lot of
basic attributes in the case base, (known) multi-correlations
between basic attributes can cause unstructured subgroup dis-
covery results. Then, data abstraction can increase the in-
terpretability of the subgroup discovery results signiﬁcantly.
Simple concepts can be aggregated to intermediate concepts
to form more potentially meaningful and interesting selec-
tors. A nominal derived attribute a ∈ ΩA is deﬁned us-
ing abstraction rules, which are utilized to derive the val-
ues via ∈ VA concerning attribute a. A rule of the form
rva = cond(rva) → va is used for a value va of attribute
a, where the rule condition cond(rva) contains conjunctions
and/or disjunctions of (negated) attribute values vi ∈ VA.
Furthermore, derived attributes with a numerical value range
can be deﬁned by algebraic formula expressions.

Improving the Handling of Missing Values Considering
the quality functions abstraction knowledge contributes to
one major point – handling missing values. Missing values
in cases are a signiﬁcant problem for subgroup discovery and
machine learning in general. For example, in medical case
bases for a speciﬁc patient usually only a subset of the possi-
ble examinations is performed, given a structured data gath-
ering strategy in real-world applications. Then, only the rel-
evant questions for the diagnostic tasks are presented to the
user. This results in reduced costs for the examiner, however
then a speciﬁc instance of the data set concerning the basic
attributes may be quite sparse.

There exist several strategies for dealing with missing val-
ues: a common strategy [Tsumoto, 2002] removes objects
(cases) with missing values from the set of analyzed objects.
Other strategies try to ﬁll in the missing values according to
statistical evaluations, or try to model the distribution [Ragel
and Cr´ewmilleux, 1999]. The subgroup quality functions ba-
sically perform a kind of statistical hypothesis testing given a
subgroup description, the target variable and the general pop-
ulation. For such a test only the cases of the population can
be considered in which all variables have deﬁned values. The
power of the test is decreased signiﬁcantly, if many analysis
objects are removed due to missing values.

In general, we cannot simply apply the ”closed-world as-
sumption”, i.e., that missing values of a concept indicate the
non-existence or negation of the concept. For example, in the
medical domain, a diagnosis may be missing, because either
all its relevant observations are missing or they are known
but denote the normal, i.e., the non-pathological state. Conse-
quently the diagnosis is not inferred. If we construct a derived
attribute to indicate the cases when the relevant observations
are missing, then we can use this derived ”helper” attribute to
indicate when the diagnosis really has a missing value. Ad-
ditionally, derived attributes besides the described ”helper”
attributes can also be constructed accordingly to minimize
missing values themselves, such that a certain default value
is provided which denotes the normal category. So, the de-
rived attributes serve three purposes:
• They focus the subgroup discovery method on the rele-
• They decrease multi-correlations between attributes that
• Derived attributes can reduce missing values for a given
concept, since they can be constructed such that a de-
ﬁned value is more often computed if the respective con-
cept would have a missing value otherwise.

vant analysis objects,

are not interesting,

The derived attributes can either be constructed based on ex-
pert knowledge, or on speciﬁc discovered subgroups. A sub-
group description is a set of selectors for a speciﬁc target con-
cept that are highly correlated with the concept. If the selec-
tors can be abstracted into a derived attribute, then it can be
used as potential background knowledge as well. Further-
more, derived attributes can potentially be reﬁned according
to the subgroup discovery results, by specialization or gener-
alization. Abstraction knowledge is probably the most costly
class of background knowledge in the knowledge-intensive
process. If the abstractions are not based on discovery results,
then they have to be formalized manually by the expert.

2.4 Related Work
Using background knowledge to constrain the search space
and pruning hypotheses during the search process has been
proposed in ILP approaches. [Weber, 2000] proposes require-
and exclude-constraints for attribute – value pairs, in order to
prune the search space. [Zelezny et al., 2003] integrate con-
straints into an ILP approach as well; the used constraints are
mainly concerned with syntactical restrictions and constraints
relating to the quality of the discovered subgroups.

The main difference between the presented approach and
the existing approaches is the fact, that we are able to inte-
grate several new types of additional background knowledge.
This additional background knowledge can be reﬁned incre-
mentally according to the requirements of the discovery task,
and can additionally be used to infer new background knowl-
edge on the ﬂy, e.g., constraints. As a major point we apply
special abstraction knowledge, which can be deﬁned by the
expert, or can be constructed semi-automatically using the
subgroup discovery results. This type of knowledge can be
applied dynamically in the process and does not rely on a sta-
tic data-preprocessing and cleaning task, for example.

3 Case Study
In this section we describe a case study for the application of
the knowledge-intensive process. We use cases taken from
the SONOCONSULT system [Huettig et al., 2004] – a med-
ical documentation and consultation system for sonography
– which has been developed with the knowledge system D3
[Puppe, 1998]. The system is in routine use in the DRK-
hospital in Berlin/K¨openick and documents an average of
about 300 cases per month. These are detailed descriptions
of ﬁndings of the examination(s), together with the inferred
diagnoses (binary attributes). The derived diagnoses are usu-
ally correct as shown in a medical evaluation (c.f. [Huettig
et al., 2004]), resulting in a high-quality case base with de-
tailed case descriptions. The applied SONOCONSULT case
base contains 4358 cases. The domain ontology contains 427
basic attributes with about 5 symbolic values on average, 133
symptom interpretations, which are rule-based abstractions
of the basic attributes, and 221 diagnoses. This indicates the
potentially huge search space for subgroup discovery.
using

the
[VIKAMINE, 2005] (Visual, Interactive and Knowledge-
Intensive Analysis and Mining Environment)
system,
developed at the Department of Computer Science VI of the
University of W¨urzburg. We used beam search with a beam
size of 10 as the search strategy, and the quality function
qRG as deﬁned in Section 2.1. The discovered subgroups
were evaluated by domain specialists according to (clinical)
novelty, interestingness, and actionability aspects.

Subgroup

discovery was

performed

First, we performed subgroup discovery only using basic
attributes and general background knowledge. We used at-
tribute weights for feature subset selection. The subgroup dis-
covery algorithm presented many signiﬁcant subgroups, that
supported the validity of the subgroup discovery techniques.
However the results at this stage were not really novel, since
they indicated mostly already known dependencies, e.g., the
relation between relative body weight and body-mass index.

In the next stage, the expert decided to deﬁne new at-
tributes, i.e., abstracted attributes which described interest-
ing concepts for analysis. The expert provided 45 derived at-
tributes, that consist of symptom interpretations directly indi-
cating a diagnosis and intermediate concepts which are used
in clinical practice, for example pleura-effusion, portal hy-
pertension, or pathological gallbladder status. Furthermore,
constraints were formalized, that prevented the combination
of certain attributes, to restrict the search space. Some ex-
amples are depicted in the Table 1, where the dependent and
independent variables are directly related to each other. The

Target Variable
Chronic Pancreatitis
Pancreas Disease
Body Mass Index

Independent Variable
Pancreas Disease
Carcinoma of the Pancreas
Relative Body Weight

Table 1: Examples of known/uninteresting relations

newly deﬁned abstraction knowledge was applied extending
the search space to the expert-deﬁned attributes. For each
attribute a in the set of derived attributes and each value
vi ∈ dom(a), a subgroup discovery problem SP ai ∈ ΩSP
was generated using the binary target variable (a = vi).

The impact of the added background knowledge was
proven by a greater acceptance of the subgroup discovery re-
sults by the expert. However still too many subgroups were
not interesting for the expert, because too many normal val-
ues were included in the results, e.g., liver-size = normal, or
fatty liver = unlikely. This motivated the application of ab-
normality information to constrain the value space to the set
of abnormal values of the attributes. Additionally, the ex-
pert suggested to group sets of values into disjunctive value
sets deﬁned by abnormality groups, e.g., grouping the values
possible and probable for some attributes. Furthermore, or-
dinality information was applied to construct meta values of
ordinal attributes like age or liver size.

Target Variable
Aorta-sclerosis = calciﬁed
Aorta-sclerosis = calciﬁed

Def. Pop.
1418
75

Subgroup Description
Pancreas Disease={probable;possible}
Pancreas Disease={probable;possible}
AND Ascites = present

Table 2: Example: Missing Value Problem

Further investigation showed that missing values play a
central role in the discovery process. Sometimes the deﬁned
population signiﬁcantly decreased, when adding a selection
expression to a subgroup description, compared to the parent
subgroup. An example is given in Table 2; the deﬁned popu-
lation is indicated in the second column. In this example, the
attribute ascites has many missing values. This problem was
solved by adapting the derived attributes to indicate when a
missing value corresponds to the value ”disease not present”.
Then, the ﬁnal set of interesting subgroups was obtained.

Figure 2 shows exemplary discovered subgroups concern-
ing the target variable gallstones, that were considered as in-
teresting for clinical practice (lines 1-6). All these subgroups
were at least signiﬁcant at the 10−6 level. The individual sub-
groups are shown in the rows of the table. Subgroup parame-
ters given in the columns are: (Subgroup) Size, TP (true pos-
itives), FP (false positives), Pop. (deﬁned population size),
RG (relative gain) and the value of the binomial quality func-
tion qBT (Bin. QF), c.f., Section 2.1. The domain specialists

evaluated the discovered subgroups concerning interesting-
ness for clinical practice: for them, the relative gain was the
primary criterion to rank the subgroups in a ﬁrst step. Then,
as a combined (helper) measure for subgroup size and gain
quality, the value of the binomial quality function qBT was
used for post-processing the set of subgroups.

In the future we are planning to consider appropriate qual-
ity measures concerning the simplicity of the discovered sub-
groups. Primary work for learning rule bases was presented
e.g., in [Atzmueller et al., 2004]. We will especially focus on
quality measures which are easy to interpret and tunable to
the analysis goals.

References
[Atzmueller et al., 2004] Martin

Atzmueller,

Joachim
Baumeister, and Frank Puppe. Quality Measures for
Semi-Automatic Learning of Simple Diagnostic Rule
Bases. In Proc. 15th Intl. Conference on Applications of
Declarative Programming and Knowledge Management
(INAP 2004), pages 203–213, Potsdam, Germany, 2004.

Figure 2: Examples of discovered subgroups (clinically interest-
ing: lines 1-6): e.g., the ﬁrst line depicts the subgroup (89 cases) de-
scribed by Age ≥ 70 AND Sex=female AND Liver size={slightly or
moderately or highly increased} and Aorta sclerosis=calciﬁed with
a target share (gallstones) of 41.6% (p) compared to 17.2% (p0) in
the general population, with a relative gain of 171% (RG).

Especially interesting for the expert proved the situations,
when a specialization of a subgroup signiﬁcantly improved
its quality (#5 vs. #2). Additionally, an important criterion to
determine sound subgroups was the comparison of possible
value combinations for ordinal and nominal attributes (e.g.,
#6 vs. #2 or #3 vs. #2).

For clinical practice, the expert preferred small subgroup
descriptions if the subgroups were comparable concerning
the relative gain measure. This is in line with the heuristic of
preferring simpler knowledge for actionability. However, sig-
niﬁcant improvements in a subgroup specialization countered
the increase in the length of the subgroup description, e.g., #7,
#9, #10 vs. #1. Furthermore, the baseline qualities of known
factors (e.g., age, gender) (c.f., #6 to #10) were also very im-
portant for the domain specialist as a reference for subgroup
comparison. In summary, the interpretation and judgment of
the subgroup discovery results ultimately depends on the ex-
pert: even if a suitable quality function is applied, the expert
still needs to semantically interpret the subgroup descriptions
for the ﬁnal assessment of a subgroup.

4 Summary and Future Work
In this paper we presented how exploiting background knowl-
edge can help to improve subgroup discovery results in a
knowledge-intensive approach. We described several classes
of background knowledge in detail, that can potentially be
used in other rule-based learning methods besides subgroup
discovery as well. A case study using cases from a real-
world application showed that applying background knowl-
edge helped to focus the discovery algorithm on the interest-
ing subspace of subgroup hypotheses, increasing the quality
of the discovery results. Furthermore, we discussed how ap-
plying abstraction knowledge can help to handle the problem
of missing values.

[Baumeister et al., 2002] Joachim Baumeister, Martin Atz-
mueller, and Frank Puppe. Inductive Learning for Case-
Based Diagnosis with Multiple Faults.
In Advances in
Case-Based Reasoning, volume 2416 of LNAI, pages 28–
42, Berlin, 2002. Springer Verlag.

[Huettig et al., 2004] Matthias Huettig, Georg Buscher,
Thomas Menzel, Wolfgang Scheppach, Frank Puppe, and
Hans-Peter Buscher. A Diagnostic Expert System for
Structured Reports, Quality Assessment, and Training of
Residents in Sonography. Medizinische Klinik, 99(3):117–
122, 2004.

[Kl¨osgen, 2002] Willi Kl¨osgen. Handbook of Data Mining
and Knowledge Discovery, chapter 16.3: Subgroup Dis-
covery. Oxford University Press, New York, 2002.

[Puppe, 1998] Frank Puppe. Knowledge Reuse among Diag-
nostic Problem-Solving Methods in the Shell-Kit D3. Intl.
Journal of Human-Computer Studies, 49:627–649, 1998.
[Ragel and Cr´ewmilleux, 1999] Arnaud Ragel and Bruno
Cr´ewmilleux. MVC–a Preprocessing Method to Deal with
Missing Values. Knowledge-Based Systems, 12(5–6):285–
291, October 1999.

[Richardson and Domingos, 2003] Matthew Richardson and
Pedro Domingos. Learning with Knowledge from Mul-
tiple Experts. In Proc. 20th International Conference on
Machine Learning (ICML-2003), 2003.

[Tsumoto, 2002] Shusaku Tsumoto. Handbook of Data Min-
ing and Knowledge Discovery, chapter Chapter 43: Medi-
cine. Oxford University Press, New York, 2002.

[VIKAMINE, 2005] http://ki.informatik.uni-

wuerzburg.de/projects/dm, 2005.

[Weber, 2000] Irene Weber. Levelwise search and Pruning
Strategies for First-Order Hypothesis Spaces. Journal of
Intelligent Information Systems, 14:217–239, 2000.

[Wrobel, 1997] Stefan Wrobel. An Algorithm for Multi-
Relational Discovery of Subgroups. In Proc. 1st European
Symposion on Principles of Data Mining and Knowledge
Discovery, pages 78–87, Berlin, 1997. Springer Verlag.

[Zelezny et al., 2003] Filip Zelezny, Nada Lavrac, and Saso
Dzeroski. Using Constraints in Relational Subgroup Dis-
covery. In International Conference on Methodology and
Statistics, pages 78–81. University of Ljubljana, 2003.

Target Variable: Gallstones#AgeSexLiver sizeAorta sclerosis123mf123456ncSizeTPFPPop.p0pRGBin. QF1XXXXXX89375231710.1720.4161.716.172XXXXXXX119467331710.1720.3871.56.313XXXXXXX132518131710.1720.3861.56.664XXXXXX1906812231770.1720.3581.36.995XXXXX2077213531710.1720.3481.236.926XXXXXXX64224231710.1720.3441.23.677X1651414123737430.1770.2510.5110.578XX1334310102437490.1770.2320.386.669X1776408136837490.1770.230.378.110XXX89417871631770.1720.1990.192.52Age:Sex:Liver size:Aorta sclerosis:1 = <50m = male1 = smaller than normal4 = slightly increasedn = not calcified2 = 50-69f = female2 = normal5 = moderately increasedc = calcified3 = >=703 = marginally increased6 = highly increased
Learning and Inference over Constrained Output

Vasin Punyakanok

Dan Roth

Wen-tau Yih

Dav Zimak

Department of Computer Science

University of Illinois at Urbana-Champaign

{punyakan, danr, yih, davzimak}@uiuc.edu

Abstract

We study learning structured output in a discrimi-
native framework where values of the output vari-
ables are estimated by local classiﬁers.
In this
framework, complex dependencies among the out-
put variables are captured by constraints and dictate
which global labels can be inferred. We compare
two strategies, learning independent classiﬁers and
inference based training, by observing their behav-
iors in different conditions. Experiments and theo-
retical justiﬁcation lead to the conclusion that using
inference based learning is superior when the lo-
cal classiﬁers are difﬁcult to learn but may require
many examples before any discernible difference
can be observed.

1 Introduction
Making decisions in real world problems involves assigning
values to sets of variables where a complex and expressive
structure can inﬂuence, or even dictate, what assignments are
possible. For example, in the task of identifying named enti-
ties in a sentence, prediction is governed by constraints like
“entities do not overlap.” Another example exists in scene in-
terpretation tasks where predictions must respect constraints
that could arise from the nature of the data or task, such as
“humans have two arms, two legs, and one head.”

There exist at least three fundamentally different solutions
to learning classiﬁers over structured output.
In the ﬁrst,
structure is ignored; local classiﬁers are learned and used
to predict each output component separately.
In the sec-
ond, learning is decoupled from the task of maintaining struc-
tured output. Estimators are used to produce global out-
put consistent with the structural constraints only after they
are learned for each output variable separately. Discrimina-
tive HMM, conditional models [Punyakanok and Roth, 2001;
McCallum et al., 2000] and many dynamic programming
based schemes used in the context of sequential predictions
fall into the this category. The third class of solutions in-
corporates dependencies among the variables into the learn-
ing process to directly induce estimators that optimize a
global performance measure. Traditionally these solutions
were generative; however recent developments have pro-
duced discriminative models of this type, including condi-

tional random ﬁelds [Lafferty et al., 2001], Perceptron-based
learning of structured output [Collins, 2002; Carreras and
M`arquez, 2003] and Max-Margin Markov networks which
allow incorporating Markovian assumptions among output
variables [Taskar et al., 2004].

Incorporating constraints during training can lead to solu-
tions that directly optimize the true objective function, and
hence, should perform better. Nonetheless, most real world
applications using this technique do not show signiﬁcant ad-
vantages, if any. Therefore, it is important to discover the
tradeoffs of using each of the above schemes.

In this paper, we compare three learning schemes.

In
the ﬁrst, classiﬁers are learned independently (learning only
(LO)), in the second, inference is used to maintain struc-
tural consistency only after learning (learning plus inference
(L+I)), and ﬁnally inference is used while learning the pa-
rameters of the classiﬁer (inference based training (IBT)). In
semantic role labeling (SRL), it was observed [Punyakanok
et al., 2004; Carreras and M`arquez, 2003] that when the lo-
cal classiﬁcation problems are easy to learn, L+I outperforms
IBT. However, when using a reduced feature space where the
problem was no longer (locally) separable, IBT could over-
come the poor local classiﬁcations to yield accurate global
classiﬁcations.

Section 2 provides the formal deﬁnition of our problem.
For example, in Section 3, we compare the three learning
schemes using the online Perceptron algorithm applied in the
three settings (see [Collins, 2002] for details). All three set-
tings use the same linear representation, and L+I and IBT
share the same decision function space. Our conjectures of
the relative performance between different schemes are pre-
sented in Section 4. Despite the fact that IBT is a more pow-
erful technique, in Section 5, we provide an experiment that
shows how L+I can outperform IBT when there exist accu-
rate local classiﬁers that do not depend on structure, or when
there are too few examples to learn complex structural depen-
dencies. This is also theoretically justiﬁed in Section 6.

2 Background
Structured output classiﬁcation problems have many ﬂavors.
In this paper, we focus on problems where it is natural both
to split the task into many smaller classiﬁcation tasks and to
solve directly as a single task. In Section 5.2, we consider the

semantic role-labeling problem, where the input X are nat-
ural language features and the output Y is the position and
type of a semantic-role in the sentence. For this problem,
one can either learn a set of local functions such as “is this
phrase an argument of ’run’,” or a global classiﬁer to pre-
dict all semantic-roles at once. In addition, natural structural
constraints dictate, for example, that no two semantic roles
for a single verb can overlap. Other structural constraints, as
well as linguistic constraints yield a restricted output space in
which the classiﬁers operate.

In general, given an assignment x ∈ X nx to a collec-
tion of input variables, X = (X1, . . . , Xnx),
the struc-
tured classiﬁcation problem involves identifying the “best”
assignment y ∈ Y ny to a collection of output variables
Y = (Y1, . . . , Yny ) that are consistent with a deﬁned struc-
ture on Y. This structure can be thought of as constraining
the output space to a smaller space C(Y ny ) ⊆ Y ny , where
C : 2Y ∗
→ 2Y ∗ constrains the output space to be structurally
consistent.

In this paper, a structured output classiﬁer is a function
h : X nx → Y ny ,
that uses a global scoring function,
f : X nx × Y ny → IR to assign scores to each possible ex-
ample/label pair. Given input x, it is hoped that the correct
output y achieves the highest score among consistent outputs:

ˆy = h(x) = argmax
0∈C(Y ny )

y

f (x, y0),

(1)

where nx and ny depend on the example at hand. In addition,
we view the global scoring function as a composition of a set
of local scoring functions {fy(x, t)}y∈Y , where fy : X nx ×
{1, . . . , ny} → IR. Each function represents the score or
conﬁdence that output variable Yt takes value y:

f (x, (y1, . . . , yny )) =

ny

Xt=1

fyt(x, t)

Inference is the task of determining an optimal assign-
ment y given an assignment x. For sequential structure
of constraints, polynomial-time algorithms such as Viterbi
or CSCL [Punyakanok and Roth, 2001] are typically used
for efﬁcient inference. For general structure of constraints,
a generic search method (e.g., beam search) may be ap-
plied. Recently, integer programming has also been shown
to be an effective inference approach in several NLP applica-
tions [Roth and Yih, 2004; Punyakanok et al., 2004].

y · Φy(x, t), where α

In this paper, we consider classiﬁers with linear rep-
resentation. Linear local classiﬁers are linear functions,
y ∈ IRdy is a weight
fy(x, t) = α
vector and Φy(x, t) ∈ IRdy is a feature vector. Then,
it
is easy to show that the global scoring function can
be written in the familiar form f (x, y) = α · Φ(x, y),
t=1 Φyt (x, t)I{yt=y} is an accumula-
tion over all output variables of features occurring for class
y, α = (α1, . . . , α|Y|) is concatenation of the α
y’s, and
Φ(x, y) = (Φ1(x, y), . . . , Φ|Y|(x, y)) is the concatenation
of the Φy(x, y)’s. Then, the global classiﬁer is

where Φy(x, y) = Pny

h(x) = ˆy = argmax
0∈C(Y ny )

y

α · Φ(x, y0).

3 Learning
We present several ways to learn the scoring function pa-
rameters differing in whether or not the structure-based in-
ference process is leveraged during training. Learning con-
sists of choosing a function h : X ∗ → Y ∗ from some hy-
pothesis space, H. Typically, the data is supplied as a set
D = {(x1, y1), . . . , (xm, ym)} from a distribution PX ,Y
over X ∗ × Y ∗. While these concepts are very general, we fo-
cus on online learning of linear representations using a variant
of the Perceptron algorithm (see [Collins, 2002]).

Learning Local Classiﬁers: Learning stand-alone local
classiﬁers is perhaps the most straightforward setting. No
knowledge of the inference procedure is used. Rather, for
each example (x, y) ∈ D, the learning algorithm must en-
sure that fyt(x, t) > fy 0(x, t) for all t = 1, . . . , ny and all
y0 6= yt. In Figure 3(a), an online Perceptron-style algorithm
is presented where no global constraints are used. See [Har-
Peled et al., 2003] for details and Section 5 for experiments.
Learning Global Classiﬁers: We seek to train classiﬁers
so they will produce the correct global classiﬁcation. To this
end, the key difference from learning locally is that feed-
back from the inference process determines which classiﬁers
to modify so that together, the classiﬁers and the inference
procedure yield the desired result. As in [Collins, 2002;
Carreras and M`arquez, 2003], we train according to a global
criterion. The algorithm presented here is an online proce-
dure, where at each step a subset of the classiﬁers are up-
dated according to inference feedback. See Figure 3(b) for
details of a Perceptron-like algorithm for learning with infer-
ence feedback.

Note that in practice it is common for problems to be mod-
eled in such a way that local classiﬁers are dependent on part
of the output as part of their input. This sort of interaction can
be incorporated directly to the algorithm for learning a global
classiﬁer as long as an appropriate inference process is used.
In addition, to provide a fair comparison between LO, L+I,
and IBP in this setting one must take care to ensure that the
learning algorithms are appropriate for this task. In order to
remain focused on the problem of training with and without
inference feedback, the experiments and analysis presented
concern only the local classiﬁers without interaction.

4 Conjectures
In this section, we investigate the relative performance of
classiﬁer systems learned with and without inference feed-
back. There are many competing factors. Initially, if the lo-
cal classiﬁcation problems are “easy”, then it is likely that
learning local classiﬁers only (LO) can yield the most accu-
rate classiﬁers. However, an accurate model of the structural
constraints could additionally increase performance (learning
plus inference (L+I)). As the local problems become more
difﬁcult to learn, an accurate model of the structure becomes
more important, and can, perhaps, overcome sub-optimal lo-
cal classiﬁers. Despite the existence of a global solution, as
the local classiﬁcation problems become increasingly difﬁ-
cult, it is unlikely that structure based inference can ﬁx poor
classiﬁers learned locally. In this case, only training with in-
ference feedback (IBT) can be expected to perform well.

Algorithm ONLINELOCALLEARNING

INPUT: DX,Y ∈ {X ∗ × Y ∗}m
OUTPUT: {fy}y∈Y ∈ H

Initialize α
Repeat until converge

y ∈ IR|Φy | for y ∈ Y

for each (x, y) ∈ DX,Y do

for t = 1, . . . , ny do

ˆyt = argmaxy
if ˆyt 6= yt then

α

y · Φy(x, t)

α

α

yt = α
ˆyt = α

yt + Φyt (x, t)
ˆyt − Φˆyt (x, t)

(a) Without inference feedback

Algorithm ONLINEGLOBALLEARNING

INPUT: DX,Y ∈ {X ∗ × Y ∗}m
OUTPUT: {fy}y∈Y ∈ H

Initialize α ∈ IR|Φ|
Repeat until converge

for each (x, y) ∈ DX,Y do

ˆy = argmaxy∈C(Y ny ) α · Φ(x, y)
if ˆy 6= y then

α = α + Φ(x, y) − Φ(x, ˆy)

(b) With inference feedback

Figure 1: Algorithms for learning without and with infer-
ence feedback. The key difference lies in the inference step
(i.e. argmax).
Inference while learning locally is trivial
and the prediction is made simply by considering each la-
bel locally. Learning globally uses a global inference (i.e.
argmaxy∈C(Y ny )) to predict global labels.

As a ﬁrst attempt to formalize the difﬁculty of classiﬁca-
tion tasks, we deﬁne separability and learnability. A classi-
ﬁer, f ∈ H, globally separates a data set D iff for all exam-
ples (x, y) ∈ D, f (x, y) > f (x, y0) for all y0 ∈ Y ny \ y
and locally separates D iff for all examples (x, y) ∈ D,
fyt(x, t) > fy(x, t) for all y ∈ Y \ yt, and all y0 ∈ Y ny \ y.
A learning algorithm A is a function from data sets to a H.
We say that D is globally (locally) learnable by A if there
exists an f ∈ H such that f globally (locally) separates D.

The following simple relationships exist between local and
global learning: 1. local separability implies global separa-
bility, but the inverse is not true; 2. local separability implies
local and global learnability; 3. global separability implies
global learnability, but not local learnability. As a result, it is
clear that if there exist learning algorithms to learn global sep-
arations, then given enough examples, IBT will outperform
L+I. However, learning examples are often limited either be-
cause they are expensive to label or because some learning
algorithms simply do not scale well to many examples. With
a ﬁxed number of examples, L+I can outperform IBT.

Claim 4.1 With a ﬁxed number of examples:

1. If the local classiﬁcation tasks are separable, then L+I

outperforms IBT.

2. If the task is globally separable, but not locally sepa-
rable then IBT outperforms L+I only with sufﬁcient ex-
amples. This number correlates with the degree of the
separability of the local classiﬁers.

5 Experiments
We present experiments to show how the relative performance
of learning plus inference (L+I) compares to inference based
training (IBT) when the quality of the local classiﬁers and
amount of training data varies.

5.1 Synthetic Data
In our experiment, each example x is a set of c points in d-
dimensional real space, where x = (x1, x2, . . . , xc) ∈ IRd ×
. . . × IRd and its label is a sequence of binary variable, y =
(y1, . . . , yc) ∈ {0, 1}c, labeled according to:

y = h(x) = argmax

y∈C(Y c) Xi

yifi(xi) − (1 − yi)fi(xi),

where C(Y c) is a subset of {0, 1}c imposing a random con-
straint1 on y, and fi(xi) = wixi +θi. Each fi corresponds to
a local classiﬁer yi = gi(xi) = Ifi(xi)>0. Clearly, the dataset
generated from this hypothesis is globally linearly separable.
To vary the difﬁculty of local classiﬁcation, we generate ex-
amples with various degree of linear separability of the lo-
cal classiﬁers by controlling the fraction κ of the data where
h(x) 6= g(x) = (g1(x1), . . . , gc(xc))—examples whose la-
bels, if generated by local classiﬁers independently, violate
the constraints (i.e. g(x) /∈ C(Y c)).

Figure 2 compares the performance of different learning
strategies relative to the number of training examples used.
In all experiments, c = 5, the true hypothesis is picked at ran-
dom, and C(Y c) is a random subset with half of the size of
Y c. Training is halted when a cycle complete with no errors,
or 100 cycles is reached. The performance is averaged over
10 trials. Figure 2(a) shows the locally linearly separable case
where L+I outperforms IBT. Figure 2(c) shows results for the
case with the most difﬁcult local classiﬁcation tasks(κ = 1)
where IBT outperforms L+I. Figure 2(b) shows the case
where data is not totally locally linearly separable(κ = 0.1).
In this case, L+I outperforms IBT when the number of train-
ing examples is small. In all cases, inference helps.

5.2 Real-World Data
In this section, we present experiments on two real-world
problems from natural language processing – semantic role
labeling and noun phrase identiﬁcation.
Semantic-Role Labeling
Semantic role labeling (SRL) is believed to be an important
task toward natural language understanding, and has imme-
diate applications in tasks such Information Extraction and

1Among the total 2c possible output labels, C(·) ﬁxes a random

fraction as legitimate global labels.

y
c
a
r
u
c
c
A

1

0.95

0.9

0.85

0.8

0.75

0.7
0

0.85

0.8

0.75

y
c
a
r
u
c
c
A

0.7

1000

y
c
a
r
u
c
c
A

1

0.9

0.8

0.7

0.6
0

LO
L+I
IBT

2000

4000

6000

# Training examples

8000

(a) κ = 0, d = 100

2000

# Training examples

3000

4000

10000

LO
L+I
IBT

5000

(b) κ = 0.15, d = 300

LO
L+I
IBT

2000

4000

6000

8000

10000

# Training examples

(c) κ = 1, d = 100

Figure 2: Comparison of different learning strategies in various
degrees of difﬁculties of the local classiﬁers. κ = 0 implies locally
linearly separability. Higher κ indicates harder local classiﬁcation.

Question Answering. The goal is to identify, for each verb in
the sentence, all the constituents which ﬁll a semantic role,
and determine their argument types, such as Agent, Patient,
Instrument, as well as adjuncts such as Locative, Temporal,
Manner, etc. For example, given a sentence “ I left my pearls
to my daughter-in-law in my will”, the goal is to identify dif-
ferent arguments of the verb left which yields the output:

[A0 I] [V left ] [A1 my pearls] [A2 to my daughter-in-law]

80
80

75
75

70
70

1
1
F
F

65
65

60
60

55
55

50
50
103
103

104
106
104
106
Separability (# Features)
Separability (# Features)

105
105

LO
LO
L+I
L+I
IBT
IBT

107
107

Figure 3: Results on the semantic-role labeling (SRL) problem. As
the number of features increases, the difﬁculty of the local classiﬁ-
cation problem becomes easier, and the independently learned clas-
siﬁers (LO) perform well, especially when inference is used after
learning (L+I). Using inference during training (IBT) can aid perfor-
mance when the learning problem is more difﬁcult (few features).

[AM-LOC in my will].

Here A0 represents leaver, A1 represents thing left, A2 rep-
resents benefactor, AM-LOC is an adjunct indicating the lo-
cation of the action, and V determines the verb.

We model the problem using classiﬁers that map con-
stituent candidates to one of 45 different types, such as
fAO and fA1 [Kingsbury and Palmer, 2002; Carreras and
M`arquez, 2003]. However, local multiclass decisions are in-
sufﬁcient. Structural constraints are necessary to ensure, for
example, that no arguments can overlap or embed each other.
In order to include both structural and linguistic constraints,
we use a general inference procedure based on integer linear
programming [Punyakanok et al., 2004]. We use data pro-
vided in the CoNLL-2004 shared task [Carreras and M`arquez,
2003], but we restrict our focus to sentences that have greater
than ﬁve arguments. In addition, to simplify the problem, we
assume the boundaries of the constituents are given – the task
is mainly to assign the argument types.

The experiments clearly show that IBT outperforms lo-
cally learned LO and L+I when the local classiﬁers are in-
separable and difﬁcult to learn. The difﬁculty of local learn-
ing was controlled by varying the number of input features.
With more features, the linear classiﬁer are more expressive
and can learn effectively and L+I outperforms IBT. With less
features the problem becomes more difﬁcult and IBT outper-
forms L+I. See Figure 3.
Noun Phrase Labeling
Noun phrase identiﬁcation involves the identiﬁcation of
phrases or of words that participate in a syntactic relation-
ship. Speciﬁcally, we use the standard base Noun Phrases
(NP) data set [Ramshaw and Marcus, 1995] taken from the
Wall Street Journal corpus in the Penn Treebank [Marcus et
al., 1993].

The phrase identiﬁer consists of two classiﬁers: one that

1
F

100

80

60

40

20

0
100

102

104

Separability (# Features)

L+I
IBT
106

Figure 4: Results on the noun phrase (NP) identiﬁcation prob-
lem.

detects the beginning, f[, and a second that detects the end,
f] of a phrase. The outcome of these classiﬁers are then com-
bined in a way that satisﬁes structural constraints constraints
(e.g. non-overlapping), using an efﬁcient constraint satisfac-
tion mechanism that makes use of the conﬁdence in the clas-
siﬁers’ outcomes [Punyakanok and Roth, 2001].

In this case, L+I trains each classiﬁer independently, and
only during evaluation, the inference is used. On the other
hand, IBT incorporates the inference into the training. For
each sentence, each word position is processed by the clas-
siﬁers, and their outcomes are used by the inference process
to infer the ﬁnal prediction. The classiﬁers are then updated
based on the ﬁnal prediction not on their own prediction be-
fore the inference.

As in the previous experiment, Figure 4 shows perfor-
mance of two systems varied by the number of features. Un-
like the previous experiment, the number of features in each
experiment was determined by the frequency of occurrence.
Less frequent features are pruned to make the task more difﬁ-
cult. The results are similar to the SRL task in that only when
the problem becomes difﬁcult IBT outperforms L+I.

6 Bound Prediction
In this section, we use standard VC-style generalization
bounds from learning theory to gain intuition into when learn-
ing locally (LO and L+I) may outperform learning globally
(IBT) by comparing the expressivity and complexity of each
hypothesis space. When learning globally, it is possible to
learn concepts that may be difﬁcult to learn locally, since the
global constraints are not available to the local algorithms.
On the other hand, while the global hypothesis space is more
expressive, it has a substantially larger representation. here
we develop two bounds—both for linear classiﬁers on a re-
stricted problem. The ﬁrst upper bounds the generalization
error for learning locally by assuming various degrees of sep-
arability. The second provides an improved generalization
bound for globally learned classiﬁers by assuming separabil-
ity in the more expressive global hypothesis space.

We begin by deﬁning the growth function to measure the

effective size of the hypothesis space.
Deﬁnition 6.1 (Growth Function) For a given hypothesis
class H consisting of functions h : X → Y, the growth func-
tion, NH(m), counts the maximum number of ways to label
any data set of size m:

NH(m) =

sup

x1,...,xm∈X m

|{(h(x1), . . . , h(xm)) |h ∈ H}|

The well-known VC-style generalization bound expresses
expected error, , of the best hypothesis, hopt on unseen data.
In the following theorem adapted from [Anthony and Bartlett,
1999][Theorem 4.2], we directly write the growth function
into the bound,
Theorem 6.2 Suppose that H is a set of functions from a set
X to a set Y with growth function NH(m). Let hopt ∈ H
be the hypothesis that minimizes sample error on a sample of
size m drawn from an unknown, but ﬁxed probability distri-
bution. Then, with probability 1 − δ

 ≤ opt +r 32(log(NH(2m)) + log(4/δ))

m

.

(2)

For simplicity, we ﬁrst describe the setting in which a sep-
arate function is learned for each of a ﬁxed number, c, of
output variables (as in Section 5.1). Here, each example has
c components in input x = (x1, . . . , xc) ∈ IRd × . . . × IRd
and output y = (y1, . . . , yc) ∈ {0, 1}c.

Given a dataset D, the aim is to learn a set of linear scor-
ing functions fi(xi) = wixi, where wi ∈ IRd for each
i = 1, . . . , c. For LO and L+I, the setting is simple: ﬁnd
a set of weight vectors that, for each component, satisfy
yiwixi > 0 for all examples (x, y) ∈ D. For IBT, we ﬁnd
iwixi for all

a set of classiﬁers such that Pi yiwixi > Pi y0

y0 6= y (and that satisfy the constraints, y0 ∈ C(Y c)).

As previously noted, when learning local classiﬁers inde-
pendently (LO and L+I), one can only guarantee convergence
when each local problem is separable – however, it is often
the case that global constraints render these problems insepa-
rable. Therefore, there is a lower bound, opt, on the optimal
error achievable. Since each component is a separate learning
problem, the generalization error is thus
Corollary 6.3 When H is the set of separating hyperplanes
in IRd,

 ≤ opt +r 32(d log((em/d)) + log(4/δ))

m

.

(3)

Proof sketch: We show that NH(m) ≤ (em/d)d when H
is the class of threshold linear functions in d dimensions.
NH(m) is precisely the maximum number of continuous
regions an arrangement of m halfspaces in IRd, which is

2Pd

i=1(cid:0)m−1

i (cid:1) ≤ 2(e(m − 1)/d)d. For m > 1, the result

holds. See [Anthony and Bartlett, 1999][Theorem 3.1] for
details.

On the other hand, when learning collectively with IBT,
examples consist of the full vector x ∈ IRcd.
In this set-
ting, convergence is guaranteed (if, of course, such a function

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

y
c
a
r
u
c
c
A

0
0

2

IBT
e
opt
e
opt
e
opt

 =  0
 = 0.1
 = 0.2

8

10
x 105

4

6

Number of Examples

Figure 5: The VC-style generalization bounds predict that IBT will
eventually outperform LO if the local classiﬁers are unable to ﬁnd
consistent classiﬁcation (opt > 0.0, accuracy < 1). However, if
the local classiﬁers are learnable (opt = 0.0, accuracy = 1), LO
will perform well.

exists). Thus, the optimal error when training with IBT is
opt = 0. However, the output of the global classiﬁcation is
now the entire output vector. Therefore, the growth function
must account for exponentially many outputs.
Corollary 6.4 When H is the set of decision functions over
i=1 yiwixi, where
{0, 1}c, deﬁned by argmaxy
w = (w1, . . . , wc) ∈ IRcd,

0∈C({0,1}c)Pc

 ≤ r 32(cd log(em/cd) + c2d + log(4/δ))

m

.

(4)

In this setting, we must count the effective
Proof sketch:
hypothesis space – which is the effective number of differ-
ent classiﬁers in weight space, IRcd. As before, this is done
by constructing an arrangement of halfspaces in the weight
space. Speciﬁcally, each halfspace is deﬁned by a single

((x, y), y0) pair that deﬁnes the region where Pi yiwixi >
iwixi. Because there are potentially C(2c) ≤ 2c out-
Pi y0
put labels and the weight space is cd-dimensional, the growth
function is the size of the arrangement of c2c halfspaces in
IRcd. Therefore NH(m) ≤ (em2c/cd)cd.

Figure 5 shows a comparison between these two bounds,
where the generalization bound curves on accuracy are shown
for IBT (Corollary 6.4) and for LO and L+I (Corollary 6.3)
with opt ∈ {0.0, 0.1, 0.2}. One can see that when separa-
ble, the accuracy=1 curve (opt = 0.0) in the ﬁgure outper-
forms IBT. However, when the problems are locally insepara-
ble, IBT will eventually converge, whereas LO and L+I will
not – these results match the synthetic experiment results in
Figure 2. Notice the relationship between κ and opt. When
κ = 0, both the local and global problems are separable and
opt = 0 As κ increases, the global problem remains separa-
ble and the local problems are inseparable (opt > 0).
7 Conclusion
We studied the tradeoffs between three common learning
schemes for structured outputs, i.e.
learning without the
knowledge about structure (LO), using inference only after
learning (L+I), and learning with inference feedback (IBT).

We provided experiments on both real-world and synthetic
data as well as a theoretical justiﬁcation that support our main
clams. – ﬁrst, when the local classiﬁcation is linearly sepa-
rable, L+I outperforms IBT, and second, as the local prob-
lems become more difﬁcult and are no longer linearly sepa-
rable, IBT outperforms L+I, but only with sufﬁcient number
of training examples. In the future, we will seek a similar
comparison for the more general setting where nontrivial in-
teraction between local classiﬁers is allowed, and thus, local
separability does not imply global separability.

8 Acknowledgments
We grateful Dash Optimization for the free academic use of Xpress-
MP. This research is supported by the Advanced Research and De-
velopment Activity (ARDA)’s Advanced Question Answering for
Intelligence (AQUAINT) Program, a DOI grant under the Reﬂex
program, NSF grants ITR-IIS-0085836, ITR-IIS-0085980 and IIS-
9984168, and an ONR MURI Award.

References
[Anthony and Bartlett, 1999] M. Anthony and P. Bartlett. Neural
Network Learning: Theoretical Foundations. Cambridge Uni-
versity Press, 1999.

[Carreras and M`arquez, 2003] X. Carreras and Llu´ıs M`arquez. On-
line learning via global feedback for phrase recognition. In Ad-
vances in Neural Information Processing Systems 15, 2003.

[Collins, 2002] M. Collins. Discriminative training methods for
hidden Markov models: Theory and experiments with perceptron
algorithms. In Proceedings of EMNLP, 2002.

[Har-Peled et al., 2003] S. Har-Peled, D. Roth, and D. Zimak. Con-
straint classiﬁcation: A new approach to multiclass classiﬁcation
and ranking. In Advances in Neural Information Processing Sys-
tems 15, 2003.

[Kingsbury and Palmer, 2002] P. Kingsbury and M. Palmer. From

Treebank to PropBank. In Proceedings of LREC, 2002.

[Lafferty et al., 2001] J. Lafferty, A. McCallum, and F. Pereira.
Conditional random ﬁelds: Probabilistic models for segmenting
and labeling sequence data. In Proc. ICML-01, pages 282–289,
2001.

[Marcus et al., 1993] M.

M. Marcinkiewicz.
English: The Penn Treebank.
19(2):313–330, June 1993.

P. Marcus,

and
Building a large annotated corpus of
Computational Linguistics,

Santorini,

B.

[McCallum et al., 2000] A. McCallum, D. Freitag, and F. Pereira.
Maximum entropy Markov models for information extraction
and segmentation.
In Proceedings of ICML-00, Stanford, CA,
2000.

[Punyakanok and Roth, 2001] V. Punyakanok and D. Roth. The use
of classiﬁers in sequential inference. In Advances in Neural In-
formation Processing Systems 13, 2001.

[Punyakanok et al., 2004] V. Punyakanok, D. Roth, W. Yih, and
D. Zimak. Semantic role labeling via integer linear programming
inference. In Proceedings of COLING-04, 2004.

[Ramshaw and Marcus, 1995] L. A. Ramshaw and M. P. Marcus.
Text chunking using transformation-based learning. In Proceed-
ings of the Third Annual Workshop on Very Large Corpora, 1995.
[Roth and Yih, 2004] D. Roth and W. Yih. A linear programming
In

formulation for global inference in natural language tasks.
Proceedings of CoNLL-2004, pages 1–8, 2004.

[Taskar et al., 2004] B. Taskar, C. Guestrin, and D. Koller. Max-
In Advances in Neural Information

margin markov networks.
Processing Systems 16, 2004.


Explanation-Based Feature Construction

Shiau Hong Lim, Li-Lun Wang and Gerald DeJong

Dept. of Computer Science

University of Illinois, Urbana-Champaign
{shonglim,lwang4,mrebl}@cs.uiuc.edu

Abstract

Choosing good features to represent objects can be
crucial to the success of supervised machine learn-
ing algorithms. Good high-level features are those
that concentrate information about the classiﬁca-
tion task. Such features can often be constructed as
non-linear combinations of raw or native input fea-
tures such as the pixels of an image. Using many
nonlinear combinations, as do SVMs, can dilute the
classiﬁcation information necessitating many train-
ing examples. On the other hand, searching even
a modestly-expressive space of nonlinear functions
for high-information ones can be intractable. We
describe an approach to feature construction where
task-relevant discriminative features are automati-
cally constructed, guided by an explanation-based
interaction of training examples and prior domain
knowledge. We show that in the challenging task of
distinguishing handwritten Chinese characters, our
automatic feature-construction approach performs
particularly well on the most difﬁcult and complex
character pairs.

1 Introduction
Sensitivity to appropriate features is crucial to the success of
supervised learning. The appropriateness of a feature set de-
pends on its relevance to the particular task. Various notions
of relevance have been proposed [Blum and Langley, 1997;
Kohavi and John, 1997] and statistical tools to evaluate and
select relevant feature sets are available (e.g. hypothesis
testing, random probes, cross-validation). Unfortunately,
most feature construction strategies focus on feature sub-
set selection (e.g. ﬁlters [Almuallim and Dietterich, 1991;
Kira and Rendell, 1992], wrappers [Kohavi and John, 1997],
embedded methods [Guyon et al., 2006]) from either a pre-
determined set or a space deﬁned by some feature construc-
tion operators [Markovitch and Rosenstein, 2002] and require
many training examples to evaluate.

In challenging domains, the “native”, observable features
(e.g. pixel values of an image) are high-dimensional and en-
code a large amount of mostly irrelevant information. There
are standard statistical techniques such as principal compo-
nents analysis (PCA) and linear discriminant analysis (LDA)

to reduce the dimensionality of the input features. But of-
ten the high-level features that simplify the task are complex
nonlinear combinations of the native features. We believe the
most effective approach is to incorporate additional informa-
tion in the form of prior world knowledge. In this direction,
one approach [Gabrilovich and Markovitch, 2005] utilizes the
large repository of the Open Directory Project to aid in natural
language classiﬁcation. We address a very different problem
of handwritten Chinese character recognition. Our additional
world knowledge is derived from a relatively small, imper-
fect, and abstract prior domain theory.

Explanation-based learning (EBL) is a method of dynam-
ically incorporating prior domain knowledge into the learn-
ing process by explaining training examples. In our charac-
ter recognition task for example, we know that not all pixels
in the input image are equally important. [Sun and DeJong,
2005] used this observation to learn specialized Feature Ker-
nel Functions for support vector machines. These embody
a specially constructed distance metric that is automatically
tailored to the learning task at hand. That approach automati-
cally discovers the pixels in the images that are more helpful
in distinguishing between classes; the feature kernel function
magniﬁes their contribution.

However, we know that it is not the raw pixels that intrin-
sically distinguish one character from one another. Rather,
the pixels are due to strokes of a writing implement and these
strokes in relation to one another distinguish the characters.
Not telling the learner of these abstract relationships means
that it must perform the moral equivalent of inventing the no-
tion of strokes from repeated exposure to patterns of raw pix-
els, making the learning task artiﬁcially difﬁcult. Our system
learns a classiﬁer that operates directly on the native features
but which appreciates the abstractions of the domain theory
as illustrated by the training examples.

In section 2, we describe what makes a good high-level fea-
ture. We then detail the role of explanations and give our gen-
eral algorithm in section 3. In section 4 we illustrate our ap-
proach with detailed feature construction algorithms for Chi-
nese character recognition, and in section 5 we show some
experimental results. Section 6 concludes.

2 High Level Features and Generalization
The criteria for feature construction are usually based on
how well a particular set of features retains the information

IJCAI-07

931

about the class while discarding as much irrelevant and re-
dundant information as possible. Information-theoretic meth-
ods, based on mutual information, channel coding and rate-
distortion theorems have been applied to this problem [Battiti,
1994; Tishby et al., 1999; Torkkola, 2003]. Let X and Y be
random variables that represent the input and the label respec-
tively, and there is an underlying joint-distribution on (X, Y ).
It can be shown [Feder and Merhav, 1994] that the optimal
1
2 H(Y |X). Therefore, a fea-
Bayes error is upper bounded by
ture F (X) that retains as much information as possible about
the label will have H(Y |F (X)) ≤ H(Y |X)+, where  > 0
represents the amount of information loss that we are willing
to tolerate. On the other hand, discarding irrelevant infor-
mation can be achieved by minimizing H(F (X)|Y ) while
satisfying the condition on H(Y |F (X)). Intuitively, this im-
plies that most information about the class label is preserved,
while irrelevant information (e.g. within-class variations) is
discarded. We show how this bounds the actual risk for the
binary case in the Appendix, regardless of what hypothesis
space is used.

When comparing alternative features, each satisfying
H(Y |F (X)) ≤ H(Y |X) + , we therefore prefer the one
with the smallest H(F (X)|Y ). Alternatively, we may aim at
minimizing the following functional:
1

J(F ) = H(F (X)|Y ) +

H(Y |F (X))



Unfortunately, without additional knowledge about the un-
derlying probability distribution, it is impossible to accu-
rately estimate the conditional entropy empirically from the
data when the training set is small. For example, when all
the training examples have different X, one can simply de-
ﬁne a feature F based on the nearest neighbor rule (itself a
classiﬁer) and empirically, with a naive estimator, achieve
ˆJ(F ) = 0. There is no reason to believe that the feature
F (X) and the resulting classiﬁer will generalize to unseen
examples. In this sense, building a right feature is as hard as
building the right classiﬁer, that is, it does not work without
any inductive bias.

However, it is possible that for some tasks, there exist fea-
tures that are known a priori to have J(F ) ≈ 0. Such features
capture our knowledge about patterns that are unique to a par-
ticular class of objects. What is not known, however, is a re-
liable way to detect or extract the feature from any unlabeled
input. The problem of constructing a good feature can there-
fore be viewed as the problem of building a good detector for
such “high-level” features, based on the training data. If we
can ensure that the detector produces the right output for the
right reason, i.e., it detects the intended high-level feature,
then we will have high conﬁdence that the resulting feature
will generalize well.

How do we verify that a detector produces the right output
for the right reason? Doing so statistically, if possible at all,
will require too many training examples for many tasks. In-
stead, if available domain knowledge can be used to build ex-
planations for the training examples, then they can be used to
verify whether a feature’s output is consistent with our prior
knowledge. We show how to use this idea to automatically
construct features that focus on the most informative part of

any input object with regard to the particular task.

3 Explanation-based Feature Construction
In classical EBL, an “explanation” is a logical proof that
shows how the class label of a particular labeled example
can be derived from the observed inputs. But our version is
weaker. We only require the explanation to identify potential
low level evidence for the assigned classiﬁcation label. We
then use training data to calibrate and evaluate the strength of
that evidence.

Our prior knowledge includes ideal stroke models of the
characters of interest (roughly of the sort one obtains from a
vector font) and the model of a stroke as a connected straight
line of ﬁnite width.

Feature construction is performed by the following steps
which we state abstractly and describe speciﬁcally in the con-
text of the Chinese character domain.

1. Explain each training example to obtain the associa-
tion between the assigned label and the observed na-
tive features mediated by high-level domain theory
concepts. After this step, each pixel is associated with
a stroke (the line that most likely made it) constrained
so that the line conﬁgurations match the stroke require-
ments of the assigned character.

2. Using the prior knowledge, identify 1) high-level fea-
tures that are similar in both categories, and 2) high-
level features that are different between the cate-
gories. The ﬁrst set form our reference strokes. These
can be conﬁdently found in new test images since a simi-
lar stroke should be present for either instance type. The
second set are information-bearing; they are character
dependent.

3. With the generated explanations, evaluate each po-
tential similarity statistically using the training set,
keeping the ones that can be detected efﬁciently and
with high conﬁdence. These are strokes that are easily
found in an unlabeled image and form a frame of refer-
ence to guide us to the high class-distinguishing infor-
mation.

4. Using the training examples, optimize the process
of ﬁnding detection features from the reference fea-
tures. This identiﬁes the high information image regions
w.r.t.
the reference strokes. The regions chosen to be
larger if over the training set there is greater variance of
the location of the detection strokes w.r.t. the reference
strokes and tighter otherwise.

Generally, ﬁnding lines in an image is problematic. Many
lines will be missed and often non-lines will be found. The
process can be expensive. However, this is only done for la-
beled examples during the learning phase. Thus, we know
what lines we should ﬁnd and their approximate geometrical
conﬁguration. This greatly improves the line-ﬁnder’s perfor-
mance. But what if we can ﬁnd no reference strokes? If there
are no easily-found similarities between the categories, then
the two classes must be very different; the classiﬁcation task
should be simple. Many features and algorithms should be
able to differentiate them, and our knowledge-based approach

IJCAI-07

932

is unnecessary. Next we examine this process in more detail
for Chinese characters.

serve as “reference strokes.” Finding them allows the target
region to be more accurately identiﬁed.

4 Classifying Handwritten Chinese

Characters

Ofﬂine handwritten Chinese character recognition remains a
challenging task due to the large variability in writing styles
and the similarity of many characters. Approaches are ei-
ther structural or feature-based [Suen et al., 2003]. The for-
mer extract strokes from a new input image and try to match
the extracted strokes to the known stroke-level models. Ex-
tracting strokes is unreliable and consequently the model-
matching process is problematic. Feature-based approaches
utilize statistics on well-designed features and have proven to
be more robust. However, the features are hand-crafted and
it is not easy to exploit prior knowledge during the learning
process; similar characters are difﬁcult to differentiate using
such globally-deﬁned features.

In this paper, we focus our attention to the task of distin-
guishing pairs of similar characters. In our approach automat-
ically constructed features are tailored to best differentiate the
characters.

Consider the pair of Chinese characters in Figure 1.

4.1 Building Explanations
Our prior model of each character is a graph, where nodes
represent a stroke and edges represent the relationship be-
tween strokes. Each stroke is itself modeled as a line seg-
ment with 5 parameters (x, y, θ, l, t) denoting its center, di-
rection, length and thickness. Such models can either be
hand-speciﬁed, or obtained from existing character-stroke
database.1 The model need not be highly accurate as the ex-
planation process relies primarily on its structural informa-
tion. The model is used to explain each training example by
ﬁnding the most likely parameters for each requisite character
stroke.

In general, searching for the best set of parameters is a
combinatorial problem for the general graph. This may still
be acceptable since the size of the graph is small and the pro-
cess is done only once for each character during training.

For efﬁciency, we structure these graphs into trees to em-
ploy a dynamic programming approach to produce the expla-
nations. We use an algorithm based on [Felzenszwalb and
Huttenlocher, 2000]. Our implementation uses two automati-
cally generated trees, focusing on horizontally and vertically
oriented strokes separately. Figure 3 shows a character model
and an example explanation.

Figure 1: Two very similar Chinese characters

They are almost identical except the leftmost radical. Ex-
tracting a global feature that summarizes the whole character
dilutes the class-relevant information concentrated on the far
left of the image.

Once the informative region has been identiﬁed, there is
still much variability in the exact location of the informa-
tive region. Figure 2 illustrates the variability among the ﬁrst
character of the pair.

Figure 2: Within-class Variability

Attempting to deﬁne an absolute set of pixels (say, one 3rd
of the image from the left) would result in noisy features.
Too small the region we risk missing the important stroke for
some of the characters, too large the region our advantage of
focused feature is lost. This is where we utilize our knowl-
edge about similarities between the two characters. The three
long, roughly vertical strokes present in both characters may

Figure 3: A Character Model and an Explained Example

Identifying Potential Similarities

4.2
Given the models for a particular pair of characters, we per-
form graph matching to identify strokes that are similar in
terms of location, direction and length. The result of this pro-
cess is the identiﬁcation of a set of strokes M which have a
match in both characters. We refer to this set as the matching
set M. These are the candidates for reference strokes. Figure
4 shows an example.

4.3 Finding Efﬁcient Reference Stroke Detector
Any efﬁcient feature extractor can be used in this step. Since
we are concerned with lines, we use a Hough transform
[Forsyth and Ponce, 2002]. In particular, we perform a Hough
transform on an input image, and look for a local minimum in
a speciﬁed region which reﬂects the variability of the match-
ing set stroke in the training data. We refer to this as the
“Hough detector”. Not every stroke in M can be reliably de-
tected by the Hough detector. We use the following algorithm

1For example, the Wen Quan Yi Project, http://wenq.org/

IJCAI-07

933

Figure 4: The strokes in M are shown as dotted lines

to select from the matching set a set of reference strokes that
can be reliably detected. The explanation for each training
example is used to measure how accurately the Hough detec-
tor detects a particular stroke.

1. Initialize the set of reference stroke R to empty
2. For each stroke S in M

(a) Find the range of directions and offsets for this
stroke among all the training examples (from the
explanations), namely, ﬁnd the smallest bounding
rectangle with the center (¯
θ, ¯ρ), the width Δθ, and
the height Δρ.

(b) For each α ∈ {0.8, 1, 1.2, 1.4, 1.6} and each β ∈

{0.8, 1, 1.2, 1.4, 1.6}
i. Deﬁne the bounded region in Hough space as a
θ, ¯ρ) with width αΔθ and

rectangle centered at (¯
height βΔθ.

ii. For each training example
A. Search for the highest peak in the region
B. Check whether the peak is within a threshold

distance τ from the actual stroke orientation

iii. Record the hit ratio H(α, β) (percentage of ref-
erence strokes correctly detected using the spec-
iﬁed parameters)

(c) Find α∗
(d) if H(α∗, β∗) > H0 then add S to R

with the highest H(α, β)

and β∗

The range of the detector window (α and β) in the above
algorithm is chosen for simplicity. The thresholds τ (distance
in Hough space) and H0 are optimized using cross-validation.

4.4 Learning the ﬁnal Feature

Once reference strokes are identiﬁed, the system estimates
the informative region relative to the parameters of the refer-
ence strokes. We use a simple deﬁnition for our “informative
region”. In each character, every stroke that is not in M is
considered a potentially informative stroke. From the expla-
nations, we know the location of these strokes in the training
examples. Using these locations, we ﬁnd the smallest rectan-
gle that includes each stroke. Whenever there are overlapping
strokes, the two rectangles are combined into a single larger
rectangle bounding both strokes. Figure 5 illustrates this.

Feature points, which can be the center or endpoints of a
reference stroke, receive a distance score with respect to each
edge of the target window, where the score is deﬁne as aδ +
bσ. This combines the mean distance (δ) to the window edge
and its standard deviation given the reference strokes. The

Figure 5: The ideal “target” rectangles

feature point with the smallest score is selected. If none of
the feature points qualiﬁes, then the edge of the image is used
as the deﬁnition of the target window. Figure 6 illustrates this.

top anchor

right anchor

Figure 6: The actual “target” rectangles with respect to the
feature-points. Note that there are no feature-points for the
left and the bottom edge.

We assume a joint-distribution on the location of the ref-
erence strokes and the target “window” as deﬁned by the
feature-points. Each reference stroke is parameterized by
the direction and the offset Ri = (ρi, θi) obtained from the
Hough detector. Each target window is parameterized by 4
parameters L = (l1, l2, l3, l4) which correspond to left, top,
right and the bottom of the window. For example, k refer-
ence strokes and one target window form a joint-distribution
on (R, L) where R = (R1, R2, . . . , Rk). The joint distribu-
tion is estimated from the training examples, since we know
both R and L from the explanations. For unlabeled examples,
we ﬁrst apply the Hough detector to localize the reference
strokes, r = (r1, . . . , rk), then ﬁnd the maximum-likelihood
location of the window according to the conditional proba-
bility P (L|R = r). Assuming that the joint-distribution is
Gaussian with mean

(cid:2)

(cid:3)

and covariance

(cid:2)

μR
μL

ΣRR ΣRL
ΣLR ΣLL

(cid:3)

,

the conditional is itself Gaussian with mean

μL|R = μL + ΣLRΣ−1

RR(r − μR)

and covariance

ΣL|R = ΣLL − ΣLRΣ−1

RRΣRL .

IJCAI-07

934

Pair

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

SVM SVM(EBL)
18.0
17.8
14.3
13.0
13.0
11.0
8.5
8.0
7.8
7.5
7.5
7.0
7.0
7.0
7.0

18.0
17.0
14.3
5.8
13.0
8.3
8.5
7.0
7.8
8.8
8.5
3.0
5.8
2.0
3.8

Pair
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30

SVM SVM(EBL)

6.8
6.8
6.5
6.5
6.3
6.3
6.3
6.0
5.8
5.8
5.8
5.5
5.5
5.5
5.5

6.8
6.8
6.5
6.5
5.3
6.3
6.3
6.0
5.8
5.0
5.3
3.5
5.5
5.5
5.3

Table 1: Error Rate (%) (5-fold cross-validation)

5 Experiments

We evaluate our system on pairwise classiﬁcations between
difﬁcult pairs of characters. We use the ETL9B database (a
popular database of more than 3000 Chinese and Japanese
characters, each with 200 examples). We ﬁrst
learn a
literature-standard multiclass classiﬁer using linear discrim-
inants to identify 100 most difﬁcult pairs (i.e. pairs of charac-
ters that result in greatest confusion). These are, as expected,
pairs of very similar characters. We use the weighted direc-
tion code histogram (WDH) [Kimura et al., 1997] as features.
These features are generally the best or among the best for
handwritten Chinese character recognition [Ding, 2005].

For each pair of characters, we learn a classiﬁer using a
linear support vector machine. We observe that the support
vector machine performs signiﬁcantly better than those with
linear discriminants. For our system, we use the same classi-
ﬁer, but the input to the SVM are the WDH features extracted
only from the target feature window found with respect to the
detected reference strokes. Table 1 shows the results of the
experiment on the 30 most challenging pairs of characters.
Even though the SVM is generally robust in the presence of
irrelevant features, our system managed to achieve signiﬁcant
improvement on many of these pairs.2

6 Conclusion

We believe that the key to generalization is to incorporate
available domain knowledge into the learning process. We
show that tailored discriminative features can be constructed
from comparisons of generative models built according to the
prior domain knowledge. This approach is particularly attrac-
tive in problems where training examples are few but high
level domain knowledge is available, as demonstrated in the
task of classifying handwritten Chinese characters.

2We note that in several cases the examples in the database are
mislabaled and our algorithm could achieve better results if these
are corrected, but we decided not to modify the original database to
retain its integrity.

Acknowledgements

This material is based upon work supported by the National
Science Foundation under Award NSF IIS 04-13161. Any
opinions, ﬁndings, and conclusions or recommendations ex-
pressed in this publication are those of the authors and do not
necessarily reﬂect the views of the National Science Founda-
tion.

Appendix
We show how a feature f (X) with small H(f (X)|Y ) and
small H(Y |f (X)) leads to better generalization bound in
terms of Rademacher complexity, for the speciﬁc case of bi-
nary classiﬁcation. We make use of the following theorem:

Theorem 1. [Bartlett and Mendelson, 2002] Let P be a
probability distribution on X × {±1}, let F be a set of {±1}-
valued functions deﬁned on X , and let (Xi, Yi)n
i=1 be train-
ing samples drawn according to P n
. With probability at least
(cid:4)
1 − δ, every function f in F satisﬁes
P (Y (cid:5)= f (X)) ≤ ˆPn(Y (cid:5)= f (X)) + Rn(F )

ln(1/δ)

+

2

2n

where
complexity of F , given by

ˆPn is the empirical risk. Rn(F ) is the Rademacher
(cid:8)

(cid:5)

n(cid:6)

(cid:7)(cid:7)(cid:7)X1, . . . , Xn

Rn(F ) = Eσ1...n EX1...n

|

sup
f ∈F

2

n

i=1

σif (Xi)|

where σ1, . . . , σn are independent {±1}-valued random
variables.

The following lemma shows that Rn(G) is bounded if

1. Given

H(f (X)|Y ) is bounded.
Lemma
with
H(f ∗(X)|Y ) ≤ β < 1, where Y takes values from
{±1}, the Rademacher complexity of a set G of {±1}-valued
functions is bounded as follows

discriminative

feature

f ∗

Rn(G) ≤ β +

4

n

Proof. From the concavity of entropy, it can be shown that
given the label Y = y, there exists v such that
Pv|y = P r(f

∗(x) = vy|y) = max

∗(x) = u|y)

P r(f

u:f ∗(x)=u

≥ 1 − β
2

.

(1)
We will call this vy the “typical” value of f ∗(x) given y.
Given the training samples (X1, Y1), . . . , (Xn, Yn), and the
Rademacher random variables σ1, . . . , σn, let n−
de-
note the expected number of examples with Y = −1 and
Y = +1 according to P (Y = y). The expected num-
ber of examples where f ∗(x) is typical is then given by
−
t = n−Pv|−1 and n
n
Consider the Rademacher average deﬁned in Theorem 1.
Suppose that the hypothesis space G contains all {±1}-valued
functions, that is, there is always a hypothesis g∗ ∈ G that
attains the maximum Rademacher average. However, regard-
less of the function g∗
chosen, only one label can be assigned

+
t = n+Pv|+1 respectively.

and n+

IJCAI-07

935

to Xi with the same value, in particular, those with the typical
value. The value σi = ±1 is assigned with equal probability
to all Xi, therefore the expected number of examples that are
wrongly labeled is N = n−
. Since there are at least
N other examples with typical value that will be correctly la-
beled, the expected sum of these will cancel each other. From
2 ). The
equation (1), n
Rademacher average is therefore

+n+

−2

2

t

t

(cid:10)

−
t ≥ n−(1 − β
(cid:9)
(cid:9)
n − 2N
n − (n− + n+)(1 − β

2 ) and n
(cid:9)
n − 2( n−

= 2
n

t

2
2 ) + 2

t

(cid:10)

+
t ≥ n+(1 − β
(cid:10)

+n+

−2

)

Rn(G) ≤ 2
n
≤ 2
n

= β + 4

n

.

discriminative

Theorem 2. Given
with
H(f ∗(X)|Y ) ≤ β < 1, where Y takes values from
{±1}. Let G be a set of {±1}-valued functions deﬁned on
{f ∗(x) : x ∈ X }. With probability at least 1 − δ, every
function g in G satisﬁes

feature

f ∗

(cid:4)

P (Y (cid:5)= g(X)) ≤ ˆPn(Y (cid:5)= g(X)) + β
2

+

2

n

+

Proof. Apply Lemma 1 to Theorem 1.

Corollary 1. Given discriminative
with
H(Y |f ∗(X)) ≤ α and H(f ∗(X)|Y ) ≤ β < 1, where
Y takes values from {±1}. Given a sufﬁciently rich space of
hypothesis space G, the risk bound will be minimized if both
α and β are minimized.

feature f ∗

Proof. From the risk bound in Theorem 2, the ﬁrst term on
the right hand side, which is the empirical risk, can be min-
imized (given the assumption that G is sufﬁciently rich) by
minimizing H(Y |f ∗(X)) , or equivalently, by minimizing α.
The second term on the right hand side can be minimized by
minimizing β.

We note that the bounds in this section are only meaning-
ful when H(f ∗(X)|Y ) < 1. In practice, this cannot be easily
achieved while preserving a near zero H(Y |f ∗(X)), and this
suggests that the interaction between domain knowledge and
data should play an important role in the construction of fea-
tures that may eventually approach this ideal quality. This
motivates our algorithm.

References

[Almuallim and Dietterich, 1991] Almuallim, H., Diet-
terich, T.G. Learning with many irrelevant features, in
Ninth National Conference on AI, MIT Press, pp.547-552,
1991.

[Bartlett and Mendelson, 2002] Bartlett, P.L., Mendelson, S.
Rademacher and Gaussian complexities: Risk bounds and
structural results. J. of Machine Learning Research, 3:463-
482, 2002.

[Battiti, 1994] Battiti, R. Using mutual information for se-
lecting features in supervised neural net learning. Neural
Networks, 5(4):537-550, 1994.

ln(1/δ)

2n

[Guyon et al., 2006] Guyon, I., Gunn, S., Nikravesh, M.,
Zadeh, L. Feature Extraction, oundations and Applica-
tions. Springer 2006.

[Blum and Langley, 1997] Blum, A., Langley, P. Selection
of relevant features and examples in machine learning. Ar-
tiﬁcial Intelligence, 97(1-2):245-271, 1997.

[Ding, 2005] Ding, X., Chinese Character Recognition, in
Handbook of Pattern Recognition and Computer Vision,
3rd ed., pp. 241-257, World Scientiﬁc, 2005.

[Felzenszwalb and Huttenlocher, 2000] Felzenszwalb,

P., Huttenlocher, D. Efﬁcient Matching of Pictorial
Structures, CVPR 2000, pp. 66-73, 2000.

[Feder and Merhav, 1994] Feder, M., Merhav, N. Relations
between entropy and error probability. IEEE Trans. on In-
formation Theory, 40:259-266, 1994.

[Forsyth and Ponce, 2002] Forsyth, D.A., Ponce, J. Com-

puter Vision: A Modern Approach, Prentic Hall, 2002.

[Gabrilovich and Markovitch, 2005] Gabrilovich,

E.,
Markovitch, S. Feature Generation for Text Categoriza-
tion Using World Knowledge, IJCAI 2005, pp. 1048-1053,
2005.

[Kimura et al., 1997] Kimura, F., Wakabayashi, T., Tsu-
ruoka, S., Miyake, Y. Improvement of Handwritten
Japanese Character Recognition using Weighted Direction
Code Histogram, Pattern Recognition, Vol. 30, No. 8, pp.
1329-1337, 1997.

[Kira and Rendell, 1992] Kira, K, Rendell, L.A. A practical
approach to feature selection, in Proc. of the 9th ICML,
1992.

[Kohavi and John, 1997] Kohavi, R., John, G. Wrappers for
feature selection. Artiﬁcial Intelligence, 97(1-2):273-324,
December 1997.

[Lu et al., 1991] Lu, S.W., Ren, Y., Suen, C.Y. Hierarchical
attributed graph representation and recognition of hand-
written chinese characters. Pattern Recognition, Vol. 24,
No. 7, pp. 617-632, 1991.

[Markovitch and Rosenstein, 2002] Markovitch, S., Rosen-
stein, D. Feature Generation Using General Constructor
Functions, Machine Learning, Vol. 49, No. 1, pp. 59-98,
2002.

[Suen et al., 2003] Suen, C.Y., Mori, S., Kim, S.H., Leung,
C.H. Analysis and Recognition of Asian Scripts - the State
of the Art, ICDAR ’03, pp. 866-878, 2003.

[Sun and DeJong, 2005] Sun, Q., DeJong, G. Feature Kernel
Functions: Improving SVMs Using High-Level Knowl-
edge. CVPR (2) 2005: 177-183, 2005.

[Tishby et al., 1999] Tishby, N., Pereira, F., Bialek, W. The
information bottleneck method. In Proc. of the 37th Ann.
Allerton Conf. on Communication, Control and Comput-
ing, pp. 368-377, 1999.

[Torkkola, 2003] Torkkola, K. Feature extraction by non-
parametric mutual information maximization. J. of Ma-
chine Learning Research, 3:1415-1438, 2003.

IJCAI-07

936


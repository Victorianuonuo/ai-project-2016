Using core beliefs for point-based value iteration

Masoumeh T. Izadi
McGill University

School of Computer Science

mtabae@cs.mcgill.ca

Ajit V. Rajwade

University of Florida

CISE Department
avr@cise.uﬂ.edu

Doina Precup

McGill University

School of Computer Science

dprecup@cs.mcgill.ca

Abstract

Recent research on point-based approximation al-
gorithms for POMDPs demonstrated that good so-
lutions to POMDP problems can be obtained with-
out considering the entire belief simplex. For in-
stance, the Point Based Value Iteration (PBVI) al-
gorithm [Pineau et al., 2003] computes the value
function only for a small set of belief states and it-
eratively adds more points to the set as needed. A
key component of the algorithm is the strategy for
selecting belief points, such that the space of reach-
able beliefs is well covered. This paper presents
a new method for selecting an initial set of repre-
sentative belief points, which relies on ﬁnding ﬁrst
the basis for the reachable belief simplex. Our ap-
proach has better worst-case performance than the
original PBVI heuristic, and performs well in sev-
eral standard POMDP tasks.

Introduction

1
Partially Observable Markov Decision Processes (POMDPs)
provide a general framework for planning under uncertainty
[Kaelbling et al., 1998; Sondik, 1971]. One of the standard
algorithms used to provide solutions to POMDPs is value it-
eration, which associates values to probability distributions
over states. Because exact value iteration is intractable, a lot
of recent work has focused on approximate algorithms, which
rely on compressing the belief space or considering a ﬁnite set
of reachable beliefs. A representative algorithm for this class
is point-based value iteration (PBVI) [Pineau et al., 2003]. In
PBVI, a ﬁnite set of reachable belief points is selected heuris-
tically, and values are computed only for these points. The
success of PBVI depends crucially on the selection of the be-
lief points. In particular, these points should cover the space
of reachable beliefs as evenly as possible. The main PBVI
heuristic uses one-step simulated trajectories in order to ﬁnd
reachable beliefs. The belief that is “farthest away” from the
points already included is greedily added to the set. We ex-
plore an alternative heuristic for choosing belief points. We
aim to ﬁnd core beliefs, i.e. beliefs which form a basis for the
space of all reachable beliefs. This approach is aimed at cov-
ering more quickly the space of beliefs that are reachable, by
looking farther into the future. This improves the worst-case

behavior of the PBVI algorithm. We discuss how core beliefs
can be found, and we illustrate the behavior of the algorithm
on several standard POMDP benchmarks.

2 POMDPs and point-based value iteration
A POMDP is described by a tuple hS ,A,O, T,W
, R,g , b0i,
where S is a ﬁnite set of states, A is a ﬁnite set of actions
and O is a ﬁnite set of observations. The probability of any
state at t = 0, is described by the initial belief, b0. The tran-
sition function T (s, a, s0) describes the probability of going
from state s to state s0 given that action a is chosen. The
observation function W (o, s, a) describes the probability of
observation o given that action a was taken and state s was
reached. The reward function, R(s, a), speciﬁes the expected
immediate reward, obtained after executing action a in state
s. The discount factor g ∈ (0,1) is used to weigh less delayed
rewards. The goal of an agent acting in a POMDP is to ﬁnd
a way of choosing actions (policy) maximizing the future re-
t g tR(st , at)]. In order to achieve this goal, the agent
turn: E[(cid:229)
must keep either a complete history of its actions and obser-
vations, or a sufﬁcient statistic of the history. The sufﬁcient
statistic in a POMDP is the belief state, b, a vector with |S|
components, which represents a probability distribution over
states:

bt(s) = P(st = s|b0, a0o1 . . . at−1ot)

On each time step, after taking action a and making observa-
tion o, the agent updates its belief state using Bayes rule:

W (o, s0, a)(cid:229)

s b(s)T (s, a, s0)

P(o|a, b)

b(s0) ← P(s0|a, o, b) =

,

(1)

where the denominator is just a normalizing constant.

In value-based POMDP methods, the agent computes a
value function, which is maintained over belief states. A lot
of recent research has been devoted to computing such values
approximately. Compression-based methods [Roy and Gor-
don, 2003; Poupart and Boutilier, 2004] aim to compress the
belief space, and maintain a value function only on this com-
pressed version. Point-based value iteration (PBVI)[Pineau
et al., 2003] maintains values and gradients (a -vectors, in
POMDP terminology) for a selected set of belief points, B.
The idea is that much of the belief simplex will not be reach-
able in general. The set B is expanded over time, in order to
cover more of the reachable belief space, and these expan-
sions are interleaved with computing new value estimates.

3 Belief selection
In order to extend the belief set B, the standard PBVI heuristic
considers, for each b ∈ B, all possible actions a and samples
one observation o for each action. Among these beliefs reach-
able from b, the belief b0 that is farthest away from B is picked
and added to B. This heuristic is motivated by an analytical
upper bound on the approximation error, which depends on
the maximum L1 distance from any reachable belief to B:

e B = max
b0∈ ¯D

min
b∈B

||b− b0||1

where ¯D
is the set of all reachable beliefs. PBVI does not
keep an upper bound on the value function when expanding
its belief set. However Smith and Simmons [Smith and Sim-
mons, 2004] have developed an alternative anytime solution
method, HSVI, based on maintaining an upper and a lower
bound of the optimal value function to guide the local value
updates. They choose the belief points to update using for-
ward heuristic search. Vlassis and Spaan [Vlassis and Spaan,
2004] instead sample a large set of reachable beliefs B dur-
ing a random walk, but then only update a subset of of B,
sufﬁcient to improve values overall.

We propose an approach for covering the space of reach-
able beliefs more quickly, by considering longer courses of
action, as well as by removing the strictly greedy charac-
ter of the PBVI algorithm. We will use core beliefs, which
form a basis for the belief simplex. The idea of core be-
liefs is inspired by work on predictive state representations
(PSRs) [Littman et al., 2002], an alternative way of represent-
ing stochastic dynamical systems. In order to reason about
the space of reachable beliefs, one can consider the initial
belief vector, b0, and all possible sequences of actions and
observations following it. These sequences are called tests.
James and Singh (2004) introduced the concept of an inﬁ-
nite matrix, whose rows correspond to all histories and whose
columns correspond to all tests. In this paper we use the same
inﬁnite matrix but the rows correspond to all reachable be-
liefs from a given initial belief point. For a row b and a test
a0o1, . . . an−1on, the content of the corresponding element in
the matrix is the probability that o1, . . . on is observed given
that action sequence a0, . . . an−1 is executed starting from b.
This matrix has ﬁnite rank, at most equal to the number of
states in the POMDP. We deﬁne core beliefs as the set of lin-
early independent rows of this matrix.

The computation of core beliefs can be done exactly using
the POMDP model, as shown in Algorithm 1. It is similar to
the computation of core tests in [James and Singh, 2004], ex-
cept that here we compute the elements of the matrix having
the POMDP model while they estimate these elements from
data without the model. But in large environments, consid-
ering all one-step extensions for all tests is prohibitive. We
experimented with two heuristics for the extensions:

1. Random: pick a random action, but consider all obser-

vations.

2. e -greedy: consider actions that are likely to be picked
by a good policy. We generate ﬁrst a subset of the core
beliefs based on all one-step tests. Then, we run PBVI
on this subset, for a ﬁxed number of iterations, in order

Algorithm 1 Finding core beliefs

Initialize the matrix with b0 and rows corresponding to all
beliefs reachable by one-step tests a, o. One column is all
1’s and the others correspond to one-step tests a, o.
for all elements (b, ao) do

Compute P(o|b, a), using (1) and store it in row b, col-
umn a, o

end for
repeat

Compute a set of linearly independent rows and columns
Add the linearly independent rows to the set of core be-
liefs
Eliminate all linearly dependent rows and columns
Add all one-step extensions to the matrix and compute
the rank of the extended matrix

until The rank is unchanged

to get a ﬁrst approximation of the value function. We
extend the core belief matrix by picking one action, in
e -greedy fashion. The next observation can be sampled
as well.

Note that for PBVI, the maximum error value for e B that can
be achieved is 2, attained for the case in which two beliefs are
non-zero over different states. This case cannot occur when
all core beliefs are part of B. Hence, the worst-case error
when using core beliefs is strictly smaller.

4 Experimental results
We performed experiments on several standard POMDP do-
mains used in the literature. For the smaller three domains
(4x4 grid, 4x3 grid and Cheese) we computed the core be-
liefs exactly (column “CBVI-all” of Table 1). The proposed
heuristics are run using all possible observations in the one-
step extensions. We also ran standard PBVI with a similar
number of initial beliefs. In all cases we performed 5 itera-
tions of PBVI to obtain a value function. Then, we ran 251
trials, with each trial cut at 251 steps. We averaged the results
obtained over these trials, and over 5 independent runs.

All versions of the CBVI algorithm yield improvements
over the PBVI heuristic. Surprisingly, in the 4x4 and Cheese
domains, the best result is achieved by the e -greedy heuristic.
We conjecture that the reason is that the e -greedy heuristic
actually focuses the computation on good actions, that are
likely to be used in the optimal policy.
In the larger domains, we only used the e -greedy heuris-
tic with sampled observations, as the other versions are too
expensive. The experimental setup is as above. In this case,
we ﬁnd a ﬁrst set of core beliefs, then run PBVI for 5 it-
erations. We take the resulting belief set, and add one-step
extensions using e -greedy actions and sampled observations,
as above. This results in a new matrix, which yields a new set
of core beliefs. This process is repeated at most 5 times; if no
more core beliefs are detected in a new iteration, the process
stops. The results are presented in the columns CBVI(1) to
CBVI(5) in Table 2. Note that for the Coffee domain, all core
beliefs are found after two such repetitions. Hence, there is
no data for the CBVI(3)-CBVI(5). The reward for standard

Domain
4x4 grid
4x3 grid
Cheese

PBVI

3.17 ± 0.22
1.58 ± 0.35
3.53 ± 0.13

CBVI-all
3.80 ± 0.11
2.45 ± 0.44
3.53 ± 0.08

CBVI-rand
3.45 ± 0.22
2.10 ± 0.09
3.65 ± 0.22

CBVI-egr
3.96± 0.28
2.07 ± 0.11
3.81 ± 0.35

Table 1: Small domains

Domain
Maze33
Hallway1
Hallway2

Coffee

PBVI-orig

2.25
0.53
0.35
-3.00

VS
2.34
0.51
0.31

-

PBVI
2.248
0.503
0.379

-2.759 ±0.93

CBVI(1)
2.185 ± 0.3
0.535 ± 0.03
0.333 ± 0.04
0.024 ± 2.14

CBVI(2)
2.285 ± 0.31
0.617 ± 0.08
0.334 ± 0.04
0.58 ±2.72

CBVI(3)
2.165 ± 0.38
0.556 ± 0.03
0.392 ± 0.05

-

CBVI(4)
2.234 ± 0.25
0.574 ± 0.04
0.403 ± 0.04

-

CBVI(5)
2.301 ± 0.26
0.582 ± 0.08
0.400 ± 0.03

-

Table 2: Large domains

[Kaelbling et al., 1998] Leslie P. Kaelbling, Michael L.
Littman, and Anthony R. Cassandra. Planning and act-
ing in partially observable stochastic domains. Artiﬁcial
Intelligence, 101:99–134, 1998.

[Littman et al., 2002] Michael Littman, Richard S. Sutton,
and Satinder Singh. Predictive representations of state. In
NIPS 14, pages 1555–1561, 2002.

[Pineau et al., 2003] Joelle Pineau, Geoff Gordon, and Se-
bastian Thrun. Point-based value iteration: An anytime
algorithms for POMDPs. In Proceedings of IJCAI, pages
1025–1032, 2003.

[Poupart and Boutilier, 2004] Pascal Poupart

and Craig
Boutilier. VDCBPI: an approximate scalable algorithm
for large scale POMDPs. In NIPS 17, 2004.

[Roy and Gordon, 2003] Nicholas Roy and Geoffrey Gor-
don. Exponential family PCA for belief compression in
POMDPs. In NIPS 15, pages 1635–1642, 2003.

[Smith and Simmons, 2004] Tery Smith and Reid Simmons.
Heuristic Search Value Iteration for POMDPs. In UAI 20,
pages 520-527, 2004.

[Sondik, 1971] Edward J. Sondik. The optimal control of
Partially Observable Markov Decision Processes. PhD
Thesis, Stanford University, 1971.

[Vlassis and Spaan, 2004] Nikos Vlassis

and Matthijs
A point-based POMDP algorithm for robot

Spaan.
planning. In Proceedings of ICRA, 2004.

PBVI using a similar number of initial beliefs is in the col-
umn PBVI in Table 2. The results reported in the original
PBVI paper (PBVI-orig)[Pineau et al., 2003] and the results
by Vlassis and Spaan (VS)[Vlassis and Spaan, 2004] are in
the columns PBVI-org and VS respectively. As shown in Ta-
ble 2, for Maze33 all algorithms obtain very similar results.
The Coffee domain is a very favorable case for CBVI, be-
cause it has a nice structure, and the reachable space of beliefs
lies in a very low dimensional part of the entire belief sim-
plex. Generating core beliefs can be done in 2-3 seconds for
this domain, because there are only two core beliefs for this
problem, although its POMDP representation has 32 states.
Our experiments show a huge value improvemnet for this par-
ticular case over PBVI. For the larger domains, Hallway1 and
Hallway2, CBVI also has a deﬁnite advantage, especially in
the later iterations, when the initial subset of core beliefs is
larger. The time complexity of CBVI is the same as PBVI
if generating the core beliefs can be done in a preprocessing
phase. In small domains and large domains with small intrin-
sic dimensionality (e.g. Coffee) this can be done in a few sec-
onds. However, generating all core beliefs in large domains
in general is very expensive. In our experiments, we reached
the maximum number of core beliefs in at most ﬁve exten-
sions of the matrix in algorithm 1; however, the computation
time, taking into account the process of belief generation is
two orders of magnitude longer than for PBVI. Although this
approach improves the value function and the policy that is
obtained, it is signiﬁcantly more expensive, at the moment,
unless the problem at hand has special structure. We are in-
vestigating different ways to compute a reasonably large por-
tion of the space of core beliefs at a lower computation cost.

Acknowledgments
This research was supported in part by funding from NSERC
and CFI. We thank Joelle Pineau for very helpful discussions
and insights into point-based algorithms, as well as for con-
structive feedback. We thank Matthijs Spaan for providing
code for his point-based value iteration algorithm.

References
[James and Singh, 2004] Michael James and Satinder Singh.
Predictive State Representations: A New Theory for Mod-
eling Dynamical Systems. ICML 21, pages 417-424, 2004.


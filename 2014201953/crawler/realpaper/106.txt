Semantic Argument Classiﬁcation Exploiting Argument Interdependence

Zheng Ping Jiang 1 and Jia Li 2 and Hwee Tou Ng 1

;

2

1. Singapore-MIT Alliance, E4-04-10, 4 Engineering Drive 3, Singapore 117576

2. Department of Computer Science, National University of Singapore,

3 Science Drive 2, Singapore 117543

fjiangzp, lijia1, nghtg@comp.nus.edu.sg

Abstract

This paper describes our research on automatic se-
mantic argument classiﬁcation, using the PropBank
data [Kingsbury et al., 2002]. Previous research
employed features that were based either on a full
parse or shallow parse of a sentence. These fea-
tures were mostly based on an individual seman-
tic argument and the relation between the predi-
cate and a semantic argument, but they did not cap-
ture the interdependence among all arguments of a
predicate. In this paper, we propose the use of the
neighboring semantic arguments of a predicate as
additional features in determining the class of the
current semantic argument. Our experimental re-
sults show signiﬁcant improvement in the accuracy
of semantic argument classiﬁcation after exploit-
ing argument interdependence. Argument classiﬁ-
cation accuracy on the standard Section 23 test set
improves to 90.50%, representing a relative error
reduction of 18%.

1 Introduction
Deriving the semantic representation of a sentence is im-
portant in many natural language processing tasks, such as
information extraction and question answering. The re-
cent availability of semantically annotated corpora, such as
FrameNet [Baker et al., 1998]1 and PropBank [Kingsbury
et al., 2002]2, prompted research in automatically producing
the semantic representations of English sentences. For Prop-
Bank, the semantic representation annotated is in the form of
semantic roles, such as ARG0, ARG1, etc. of each predicate
in a sentence, and the process of determining these semantic
roles is known as semantic role labeling.

Most previous research treats the semantic role labeling
task as a classiﬁcation problem, and divides the task into two
subtasks: semantic argument identiﬁcation and semantic ar-
gument classiﬁcation. Semantic argument identiﬁcation in-
volves classifying each syntactic element in a sentence into
either a semantic argument or a non-argument. A syntac-
tic element can be a word in a sentence, a chunk in a shal-

1http://framenet.icsi.berkeley.edu/
2http://www.cis.upenn.edu/˜ace

low parse, or a constituent node in a full parse tree. Seman-
tic argument classiﬁcation involves classifying each seman-
tic argument identiﬁed into a speciﬁc semantic role, such as
ARG0, ARG1, etc.

Various features based on either a shallow parse or full
parse of a sentence have been proposed. These features are
then used by some machine learning algorithm to build a clas-
siﬁer for semantic argument classiﬁcation. While these fea-
tures capture the syntactic environment of the semantic argu-
ment currently being classiﬁed, they overlook the semantic
context of the argument.

We propose a notion of semantic context, which consists of
the already identiﬁed or role-classiﬁed semantic arguments in
the context of an argument that is currently being classiﬁed.
Semantic context features are deﬁned as features extracted
from the neighboring semantic arguments, and used in clas-
sifying the current semantic argument. The use of these fea-
tures explicitly captures the interdependence among the se-
mantic arguments of a predicate.

In this paper, we focus on the semantic argument classiﬁ-
cation task using PropBank[Kingsbury et al., 2002]. Our se-
mantic context features are derived from full parse trees. Our
experimental results show signiﬁcant improvement in the ac-
curacy of semantic argument classiﬁcation after adding the
semantic context features, when compared to using only fea-
tures of the current semantic argument.

The rest of this paper is organized as follows: Section 2
explains in detail the application of semantic context features
to semantic argument classiﬁcation; Section 3 gives a spe-
ciﬁc example to illustrate how semantic context features can
improve semantic argument classiﬁcation; Section 4 presents
our experimental results; Section 5 reviews related work; and
Section 6 concludes the paper.

2 Semantic Argument Classiﬁcation with

Semantic Context Features

Similar to [Pradhan et al., 2005], we divide the task of se-
mantic role labeling into two sequential subtasks: semantic
argument identiﬁcation and semantic argument classiﬁcation,
and treat each subtask as a separate classiﬁcation problem.
Our investigation focuses on semantic argument classiﬁca-
tion with the assumption that the semantic arguments have
been correctly identiﬁed by the semantic argument identiﬁca-

Description

Feature
Sentence level features
predicate (Pr)

voice (Vo)

subcat (Sc)

predicate lemma in the predicate-argument struc-
ture
grammatical voice of the predicate, either active
or passive
grammar rule that expands the predicate’s parent
node in the parse tree

Argument-speciﬁc features
phrase type (Pt) syntactic category of the argument constituent
head word (Hw) head word of the argument constituent
Argument-predicate relational features
path (Pa)

syntactic path through the parse tree from the ar-
gument constituent to the predicate node
relative position of the argument consitituent with
respect to the predicate node, either left or right

position (Po)

Table 1: Baseline features

S

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

HHHHHHHHHHHHHHHHHH

NP

(ARG1)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

PPPPP

The Nasdaq index

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:0)

PPPPPPPPPP

@

VP
(cid:0)(cid:0)
@@

(cid:0)
NP

@
PP

VBD

(predicate)

(ARG2)

added

1.01

(ARG4)

(cid:16)(cid:16)(cid:16) PPP
to 456.64

(ARGM-ADV)

PP
PPP
(cid:16)(cid:16)(cid:16)
on . . . shares

.

.

Figure 1: Semantically labeled parse tree, from dev set 00

Pr

Vo

Sc

Pt Hw

Pa

Po

Ar

add active VP:VBD NP PP PP NP index NP^S VP VBD L
add active VP:VBD NP PP PP NP 1.01 NP^VP VBD R
add active VP:VBD NP PP PP PP
PP^VP VBD R
add active VP:VBD NP PP PP PP
PP^VP VBD R ARGM-ADV

ARG1
ARG2
ARG4

to
on

tion module in the ﬁrst step.

Table 2: Semantic context features based on Figure 1

2.1 Baseline Features
By treating the semantic argument classiﬁcation task as a
classiﬁcation problem using machine learning, one of the
most important steps in building an accurate classiﬁer is the
choice of appropriate features. Most features used in prior
research, such as [Gildea and Jurafsky, 2002; Pradhan et al.,
2005; Bejan et al., 2004], can be categorized into three types:
(cid:15) Sentence level features, such as predicate, voice, and

predicate subcategorization.

(cid:15) Argument-speciﬁc features, such as phrase type, head
word, content word, head word’s part of speech, and
named entity class of the argument.

(cid:15) Argument-predicate relational features, such as an ar-
gument’s position with respect to the predicate, and its
syntactic path to the predicate.

The above features attempt to capture the syntactic envi-
ronment of the semantic argument currently being classiﬁed.
They are entirely determined by the underlying shallow parse
or full parse of the sentence. They carry no information about
the semantic arguments that have already been identiﬁed or
classiﬁed. As such, the classiﬁcation of each semantic argu-
ment is done independently from other semantic arguments.
It assumes that the semantic arguments of the same predicate
do not inﬂuence each other.

We use the baseline features in [Pradhan et al., 2005] as

our baseline. These features are explained in Table 1.

2.2 Semantic Context Features
For a semantic argument, its semantic context features are de-
ﬁned as the features of its neighboring semantic arguments.
The combination of the features of the current semantic ar-
gument and its neighboring semantic arguments encodes the
interdependence among the semantic arguments.

To illustrate, the parse tree in Figure 1 is annotated with the
semantic arguments of the predicate “added”. These semantic
arguments are ARG1, ARG2, ARG4, and ARGM-ADV .

Each row of Table 2 contains the baseline features (as deﬁned
in Table 1) extracted for each semantic argument.

In Figure 1, suppose the semantic argument identiﬁcation
module has correctly identiﬁed the constituents “The Nasdaq
index”, “1.01”, “to 456.64”, and “on : : : shares” as semantic
arguments of the predicate “added”. Assume further that the
semantic argument “The Nasdaq index” has been assigned the
semantic role ARG1. Now consider the task of assigning a
semantic role to the semantic argument “1.01”. Without using
semantic context features, the baseline features used for the
semantic argument “1.01” appear in row 2 of Table 2.

If we augment with the semantic context features of all
neighboring arguments,
then the features P t, Hw, P a,
and P o of the neighboring arguments ARG1, ARG4, and
ARGM-ADV are added as additional features. Also, since
“The Nasdaq index” has been assigned a semantic role, its
argument class ARG1 is also used as an additional semantic
context feature Ar.

2.3 Various Ways of Incorporating Semantic

Context Features

There are many possible subsets of the semantic context fea-
tures in Table 2 that one can choose to add to the baseline
features of the current semantic argument being classiﬁed. In
addition, since the semantic role labels of arguments that are
already classiﬁed (like ARG1 in the previous example) are
available as additional context features, the order in which
we process the semantic arguments during semantic argument
classiﬁcation can affect the accuracy of the semantic role la-
bels assigned. A good classiﬁer should incorporate an infor-
mative set of semantic context features, as well as adopt an
argument ordering that helps to give the best overall accuracy
for semantic argument classiﬁcation.

We use context feature acronyms with subscripts to denote
particular types of context features at speciﬁc locations rel-
ative to the current argument being classiﬁed. For example,
Hw1 refers to the head word feature of the argument imme-

Feature Description
P ti

the syntactic category of the
ith context semantic argu-
ment
the head word of the ith con-
text semantic argument
the path of the ith context
semantic argument
the position of the ith con-
text semantic argument
the semantic role of the ith
previous semantic argument

H wi

P ai

P oi

Ari

Examples
P t(cid:0)1=NP ; P t1=PP

;

H w(cid:0)1=index
H w1=to
P a(cid:0)1=NP^S VP VBD
; P a1=PP^VP VBD
P o(cid:0)1=L ; P o1=R

Ar(cid:0)1=ARG1

Table 3: Examples of semantic context features. The current
semantic argument being classiﬁed is “1.01” as shown in Fig-
ure 1.

diately to the right of the current argument being classiﬁed.
More examples are given in Table 3. We also use set notation
f(cid:0)i::ig to denote the set of context features with subscript
index j 2 f(cid:0)i; : : : ; ig. For example, Hwf(cid:0)1::1g denotes the
context features Hw(cid:0)1 and Hw1.
Adding All Types of Context Features
We could choose to add all types of context features within
a certain window size. To illustrate with the example in Fig-
ure 1, suppose the current semantic argument being classiﬁed
is “1.01”. If we add all types of context features from the
set f(cid:0)1::1g (i.e., within a window size of one), then the ad-
ditional context features added are those in Table 4 that are
darkly shaded. The baseline features for the current argument
“1.01” is lightly shaded in Table 4.

Pr

Vo

Sc

Pt Hw

Pa

Po Ar

add active VP:VBD NP PP PP NP index NP^S VP VBD L ARG1
add active VP:VBD NP PP PP NP 1.01 NP^VP VBD R
add active VP:VBD NP PP PP PP
PP^VP VBD R
add active VP:VBD NP PP PP PP
PP^VP VBD R

to
on

*

Table 4: Adding all types of context features from the set
f(cid:0)1::1g

Adding a Single Type of Context Features
Alternatively, we could add only a speciﬁc type of context
features within a certain window size, since it may be the case
that certain types of context features are more effective than
others. To illustrate again with the same example. If we add
the head word features Hwf(cid:0)2::2g (i.e., within a window size
of two), then the additional context features added are those
in Table 5 that are darkly shaded. The baseline features for
the current argument “1.01” is lightly shaded in Table 5.

Different Argument Ordering
So far we have implicitly assumed that the semantic argu-
ments are processed in a linear ordering, i.e., according to
the natural textual order in which the semantic arguments ap-
pear in a sentence. In PropBank, the semantic arguments of a
single predicate in a sentence do not overlap, so this ordering

Pr

Vo

Sc

Pt Hw
nil

Pa

Po Ar

add active VP:VBD NP PP PP NP index NP^S VP VBD L ARG1
add active VP:VBD NP PP PP NP 1.01 NP^VP VBD R
add active VP:VBD NP PP PP PP
PP^VP VBD R
add active VP:VBD NP PP PP PP
PP^VP VBD R

*

to
on
nil

Table 5: Adding one type of context features Hwf(cid:0)2::2g

is well-deﬁned. Using a linear ordering, the semantic argu-
ments in Figure 1 are processed in the order ARG1, ARG2,
ARG4, and ARGM-ADV .

Experiments reported in [Lim et al., 2004] indicate that
semantic arguments spanned by the parent of the predicate
node (the immediate clause) can be more accurately classi-
ﬁed. Hence, if we process these semantic arguments ﬁrst,
their semantic role labels Ari may be more accurately de-
termined and so may help in the subsequent classiﬁcation of
the remaining semantic arguments. Inspired by [Lim et al.,
2004], we view a parse tree as containing subtrees of increas-
ing size, with each subtree rooted at each successive ances-
tor node of the predicate node. In subtree ordering, the se-
mantic arguments spanned by a smaller subtree are processed
ﬁrst before the arguments of the next larger subtree. Using
a subtree ordering, the semantic arguments in Figure 1 are
processed in the order ARG2, ARG4, ARGM-ADV , and
ARG1.

3 An Illustrating Example of the Utility of

Semantic Context Features

In this section, we shall motivate the utility of semantic con-
text features by considering the example in Figure 1. The
predicate “add” has three rolesets deﬁned in PropBank, as
shown in Table 6. Each roleset corresponds to a different ar-
gument structure. Depending on the particular roleset that
a speciﬁc instance of “add” belongs to, different argument
classes are assigned to its semantic arguments.

Roleset Arguments
add.01 ARG0(speaker), ARG1(utterance)
add.02 ARG0(adder), ARG1(thing being added),

ARG2(thing being added to)

add.03 ARG1(logical

subject,

thing
rising / gaining / being added to),
ARG2(amount risen), ARG4(end point),
ARGM-LOC(medium)

patient,

Table 6: The rolesets of “add”, as deﬁned in PropBank

The roleset for “add” in our example is “add.03”, where
the ﬁrst argument to the left of “added”, “The Nasdaq index”,
is the “logical subject, patient, thing rising / gaining / being
added to”. This contrasts with the second roleset “add.02”,
such as “added” in the sentence “Judge Curry added an addi-
tional $55 million to the commission’s calculations.”, which
is an illustrating example used in PropBank’s description of
the roleset “add.02”.

During classiﬁcation of the semantic arguments “1.01” and
“to 456.64” in this example, using only the baseline fea-

tures of an individual semantic argument results in “1.01” be-
ing misclassiﬁed as ARG1 and “to 456.64” being misclassi-
ﬁed as ARG2, when their correct semantic argument classes
should be ARG2 and ARG4 respectively. Since the two role-
sets “add.02” and “add.03” have similar argument structures
(add something to something), and since “add.02” occurs
more frequently than “add.03”, the semantic argument clas-
siﬁcation module has assigned the semantic argument classes
of the roleset “add.02” to “1.01” and “to 456.64”, which is
incorrect.

However, our experiments in section 4 show that a clas-
siﬁer augmented with the context features Hwf(cid:0)2::2g as-
signs the correct semantic argument classes to “1.01” and
“to 456.64”. This is because in the sentence “index added
something to something”, that “index” is the head word of
the ﬁrst argument gives a strong clue that the correct roleset
is “add.03”, since “index” cannot ﬁll the adder (agent) role of
“add.02”. In fact, occurrence counts from PropBank section
02-21 indicate that of the three occurrences of “index” as the
head word of the ﬁrst argument of predicate “add”, all three
appear in the roleset “add.03”. This example illustrates the
importance of neighboring semantic arguments in the context
of the current semantic argument in determining its correct
semantic argument class.

4 Experimental Results
Our semantic argument classiﬁcation module uses sup-
port vector learning and is implemented with yamcha3 and
tinysvm4 [Kudo and Matsumoto, 2001]. Polynomial kernel
with degree 2 is used in a one-vs-all classiﬁer. The cost per
unit violation of the margin C = 1, and the tolerance of the
termination criterion e = 0:001.

The training, development, and test data sections follow
the conventional split of Section 02-21, 00, and 23 of Prop-
Bank release I respectively. In Section 4.1 to Section 4.4, we
present the accuracy scores when evaluated on development
section 00. In Section 4.5, we present the accuracy scores on
test section 23.

The semantic argument classiﬁcation accuracy when eval-
uated on section 00 using baseline features is 88.16%, assum-
ing that semantic argument identiﬁcation is done correctly
(i.e., “gold argument identiﬁcation”). In Section 4.1 to Sec-
tion 4.4, accuracy scores that show a statistically signiﬁcant
improvement ((cid:31)2 test with p = 0:05) over the baseline score
of 88.16% are marked by “*”. The highest score of each row
in the score tables is boldfaced.

4.1 Results of Linear Ordering
Results Using All Types of Context Features
Table 7 contains accuracy scores after augmenting baseline
features with all types of semantic context features. Ar fea-
ture is only present within context feature sets that contain
neighboring arguments to the left of the current argument.
We observe that the accuracy scores have a signiﬁcant im-
provement from the baseline if context features on both sides

3http://chasen.org/˜taku/software/yamcha/
4http://chasen.org/˜taku/software/TinySVM/

of the current argument are used together. Context features to
the left of the current argument are not effective when used
alone.

Accuracy of increasing context window size

89.27*

89.32*

feature f(cid:0)1::1g f(cid:0)2::2g f(cid:0)3::3g f(cid:0)4::4g f(cid:0)5::5g
all
88.20
feature f(cid:0)1::0g f(cid:0)2::0g f(cid:0)3::0g f(cid:0)4::0g f(cid:0)5::0g
86.70
all
feature
f0::5g
all
88.08

87.88
f0::1g
88.74

87.41
f0::2g
88.82

87.14
f0::3g
88.48

86.84
f0::4g
88.32

88.91

88.52

Table 7: Accuracy based on adding all types of semantic con-
text features, with increasing window size, assuming correct
argument identiﬁcation and linear ordering of processing ar-
guments

Results Using a Single Type of Context Features
Accuracy scores of semantic argument classiﬁcation after
augmenting baseline features with a single type of context
features are shown in Table 8. The best improvement results
from the use of head word features Hw. The use of features
Ar reduces accuracy. This is likely due to semantic argument
classes not being accurately determined, and errors commit-
ted in a position in linear ordering might affect classiﬁcation
accuracy at subsequent positions. However, a better argument
ordering may improve the effectiveness of Ar.

4.2 Results of Subtree Ordering
We repeat the experiments of the last subsection, but this time
with the use of subtree ordering. The accuracy scores are
given in Table 9 and Table 10. The most prominent differ-
ence from the results of linear ordering is the better accuracy
scores with the use of the Ar features, as shown in Table 10
compared with Table 8. The improvement observed is consis-
tent with the ﬁndings of [Lim et al., 2004], that the argument
class of the semantic arguments spanned by the parent of the
predicate node can be more accurately determined.

4.3 More Experiments with the Ar Feature
Unlike other semantic context features that are completely de-
termined once argument identiﬁcation is completed, the Ar
feature is dynamically determined during argument classiﬁ-
cation. It may be possible to improve the overall accuracy of

Accuracy of increasing context window size

89.01*
89.49*
89.17*
88.63

feature f(cid:0)1::1g f(cid:0)2::2g f(cid:0)3::3g f(cid:0)4::4g f(cid:0)5::5g
Pt
88.55
89.32*
Hw
88.55
Pa
Po
88.24
feature f(cid:0)1::0g f(cid:0)2::0g f(cid:0)3::0g f(cid:0)4::0g f(cid:0)5::0g
87.75
Ar

89.06*
89.82*
89.04*
88.74

88.89
89.56*
88.89
88.58

88.81
89.42*
88.66
88.54

87.98

87.72

87.73

87.78

Table 8: Accuracy based on adding a single type of semantic
context features, with increasing window size, assuming cor-
rect argument identiﬁcation and linear ordering of processing
arguments

Accuracy of increasing context window size

Accuracy of increasing context window size

88.96*

88.62

88.99*

feature f(cid:0)1::1g f(cid:0)2::2g f(cid:0)3::3g f(cid:0)4::4g f(cid:0)5::5g
all
88.33
feature f(cid:0)1::0g f(cid:0)2::0g f(cid:0)3::0g f(cid:0)4::0g f(cid:0)5::0g
all
88.40
feature
f0::5g
all
87.63

88.92
f0::1g
88.41

88.83
f0::2g
88.38

88.87
f0::3g
88.08

88.66
f0::4g
87.80

88.43

Table 9: Accuracy based on adding all types of semantic con-
text features, with increasing window size, assuming correct
argument identiﬁcation and subtree ordering of processing ar-
guments

Accuracy of increasing context window size

89.03*
89.29*
89.25*
88.77

feature f(cid:0)1::1g f(cid:0)2::2g f(cid:0)3::3g f(cid:0)4::4g f(cid:0)5::5g
Pt
88.46
89.08*
Hw
88.74
Pa
Po
88.21
feature f(cid:0)1::0g f(cid:0)2::0g f(cid:0)3::0g f(cid:0)4::0g f(cid:0)5::0g
Ar
88.83

88.98*
89.60*
89.25*
88.96*

88.83
89.29*
89.14*
88.72

88.77
89.20*
88.95
88.37

88.90

88.84

88.85

88.87

Table 10: Accuracy based on adding a single type of seman-
tic context features, with increasing window size, assuming
correct argument identiﬁcation and subtree ordering of pro-
cessing arguments

assigning argument classes to all the semantic arguments of a
predicate if we use a beam search algorithm to keep the best
k combinations of argument classes assigned to all semantic
arguments processed so far.

Ar Feature with Gold Semantic Argument Class
To determine the maximum accuracy improvement possible
with the use of the Ar feature, we assume that the argument
classes of all previous semantic arguments have been accu-
rately determined and used as semantic context features. The
experimental results are presented in Table 11, titled “linear
gold” and “subtree gold” respectively for linear and subtree
ordering.

1

Ar Feature with Beam Search
Tinysvm’s output scores are converted to conﬁdence scores
between 0 and 1, by applying the sigmoid function y =
1+e(cid:0)x . We implemented a beam search algorithm to ﬁnd the
overall best sequence of argument classes to be assigned to
all semantic arguments. At each iteration, the beam search
algorithm processes one semantic argument and determines
the top 3 argument classes with the highest conﬁdence scores
for each semantic argument. It then ﬁnds and keeps the best
k (k = 10 in our implementation) argument class sequences
overall. Each argument class sequence is assigned a conﬁ-
dence score, which is the product of the conﬁdence scores of
all arguments in the sequence. Finally, the sequence of argu-
ment classes for all semantic arguments with the highest score
is chosen. Experimental results are presented under “linear
beam” and “subtree beam” in Table 11, and show improve-
ments after using beam search with Ar feature for subtree
ordering.

Ar with
87.98
linear
88.83
linear beam
89.37*
linear gold
subtree
88.85
subtree beam 89.13*
subtree gold
89.87*

f(cid:0)1::0g f(cid:0)2::0g f(cid:0)3::0g f(cid:0)4::0g f(cid:0)5::0g
87.75
88.67
89.58*
88.83
89.20*
90.11*

87.73
88.64
89.59*
88.90
89.18*
90.09*

87.78
88.66
89.60*
88.87
89.19*
90.07*

87.72
88.46
89.54*
88.84
89.20*
89.84*

Table 11: More experiments with the Ar feature, using beam
search and gold semantic argument class history

4.4 Combining Multiple Semantic Context

Features

The best accuracy score we have obtained so far is 89.82%,
given by Hw(cid:0)2::2 in Table 8. This score is a statistically sig-
niﬁcant improvement over the baseline score of 88.16% ob-
tained with only baseline features. We want to further lever-
age on the other types of semantic context features. Error
analysis of experiments in Table 8 and 10 on development
section 00 shows that classiﬁers using different semantic con-
text features make different classiﬁcation mistakes. Thus we
would like to explore the combination of multiple semantic
context features. This provides a basis for combining multi-
ple semantic context features. Here we more carefully select
the context features than simply using all available features
as in Table 7 and 9.
A Single Classiﬁer with Multiple Context Features
The best context features from each row of Table 8,
P tf(cid:0)2::2g, Hwf(cid:0)2::2g, P af(cid:0)1::1g, P of(cid:0)2::2g, and Arf(cid:0)1::0g
are combined to form a new classiﬁer based on linear order-
ing. Similarly P tf(cid:0)1::1g, Hwf(cid:0)2::2g, P af(cid:0)2::2g, P of(cid:0)2::2g,
and Arf(cid:0)3::0g from Table 10 are combined to form a new
classiﬁer based on subtree ordering. Evaluation on develop-
ment section 00 gives accuracy scores of 89.54% and 89.19%,
for linear and subtree ordering, respectively. The lack of ac-
curacy improvement over that of Hw(cid:0)2::2 shows that such a
simple combination might accumulate the errors introduced
by each additional context feature.
Voting with Multiple Classiﬁers
Instead of building a new classiﬁer with multiple semantic
context features, one can combine the classiﬁcation results
of multiple classiﬁers and hope to achieve better accuracy
through consensus and mutual error correction. The voting
process combines k different classiﬁers each belonging to
one row in Table 8 or Table 10. Currently only classiﬁers
based on the same ordering are combined. Classiﬁers are
chosen from different context feature types, but can be of
the same context feature window size. For a particular se-
mantic argument A, each candidate argument class will accu-
mulate its conﬁdence score assigned by all the voting classi-
ﬁers. The candidate argument class with the highest aggre-
gate score will be output as the argument class for A. Ex-
haustive experiments are carried out with k = 2; 3; 4; 5. On
the development section 00, the best voting classiﬁer in linear
ordering is fP tf(cid:0)1::1g; Hwf(cid:0)4::4g; P af(cid:0)2::2g; Arf(cid:0)1::0gg,
with accuracy 90.32%. The best for subtree ordering is

Linear ordering

Subtree ordering

Feature
P tf(cid:0)2::2g
H wf(cid:0)2::2g
P af(cid:0)1::1g
P of(cid:0)2::2g
Arf(cid:0)1::0g
Arbeam
f(cid:0)1::0g
Allf(cid:0)2::2g
V otelinear

Accuracy Feature
89.26* P tf(cid:0)1::1g
89.92* H wf(cid:0)2::2g
89.20* P af(cid:0)2::2g
88.96 P of(cid:0)2::2g
88.14 Arf(cid:0)3::0g
88.80 Arbeam
f(cid:0)2::0g
89.61* Allf(cid:0)1::1g
90.15* V otesubtree

Accuracy

88.88
89.66*
89.34*
89.20*
88.88
89.13*
89.33*
90.50*

Table 12: Semantic argument classiﬁcation accuracy on test
section 23. Baseline accuracy is 88.41%.

fHwf(cid:0)2::2g; P af(cid:0)1::1g; Arf(cid:0)4::0gg with accuracy 90.62%.
We denote the two voting classiﬁers as V otelinear and
V otesubtree respectively.

Note that

the accuracy score of 90.62% achieved by
V otesubtree is a statistically signiﬁcant improvement over
the best score we have obtained without voting, 89.82% of
Hwf(cid:0)2::2g in Table 8.

4.5 Testing on Section 23

Based on the experiments on development section 00, we
choose the best classiﬁers and apply them for semantic argu-
ment classiﬁcation on test section 23. The baseline classiﬁca-
tion accuracy for section 23 is 88.41%, with the use of only
baseline features. Subtree voting improves the accuracy to
90.50%, representing a relative error reduction of 18%. The
detailed scores are in Table 12.

5 Related Work

To overcome the inadequate assumption of semantic argu-
ment independence during classiﬁcation, [Punyakanok et al.,
2004] employed integer programming to impose global con-
straints in semantic role labeling. These global constraints
can be viewed as an introduction of semantic context knowl-
edge, e.g., no argument classes should be duplicated. How-
ever, these constraints are rule-based and may not always be
applicable due to exceptions. Our proposed approach can be
viewed as inducing the constraints via statistical learning.

The methods proposed in [Hacioglu et al., 2004; Kouch-
nir, 2004; Park et al., 2004] used features such as the phrase
type and head word of neighboring syntactic chunks. Ex-
periments in [Pradhan et al., 2005; Fleischman et al., 2003;
Kwon et al., 2004] used the semantic role labels of previous
semantic arguments as dynamic context features and showed
improved accuracy. This demonstrates the utility of limited
semantic context features. The semantic context features pro-
posed in this paper capture more extensively the interdepen-
dence among arguments.

Our proposed subtree ordering can be viewed as a general-
ization of the two-level incremental approach of [Lim et al.,
2004] in processing arguments, in which a sentence is divided
into immediate clause and upper clauses.

6 Conclusion
In this paper, we successfully used additional features of the
neighboring semantic arguments in determining the class of
the current semantic argument. Our experimental results have
shown signiﬁcant improvement in the accuracy of semantic
argument classiﬁcation after exploiting argument interdepen-
dence. Argument classiﬁcation accuracy on the standard Sec-
tion 23 test set yields a relative error reduction of 18%.

Acknowledgements
We thank Taku Kudo and Sameer Pradhan for their help in
answering our questions related to the work in this paper.

References
[Baker et al., 1998] C. F. Baker, C. J. Fillmore, and J. B.
Lowe. The Berkeley FrameNet Project. Proceedings of
COLING, 1998.

[Bejan et al., 2004] C. A. Bejan, A. Moschitti, P. Morarescu,
G. Nicolae, and S. Harabagiu. Semantic Parsing Based on
FrameNet. Proceedings of SENSEVAL-3, 2004.

[Fleischman et al., 2003] M. Fleischman, N. Kwon, and
E. Hovy. Maximum Entropy Models for FrameNet Clas-
siﬁcation. Proceedings of EMNLP, 2003.

[Gildea and Jurafsky, 2002] D. Gildea and D. Jurafsky. Au-
tomatic Labeling of Semantic Roles. Computational Lin-
guistics, 2002.

[Hacioglu et al., 2004] K. Hacioglu, S. Pradhan, W. Ward,
J. H. Martin, and D. Jurafsky. Semantic Role Labeling by
Tagging Syntactic Chunks. Proceedings of CoNLL, 2004.
and
M. Marcus. Adding Semantic annotation to the Penn Tree-
bank. Proceedings of HLT, 2002.

[Kingsbury et al., 2002] P. Kingsbury, M. Palmer,

[Kouchnir, 2004] B. Kouchnir. A Memory-based Approach
for Semantic Role Labeling. Proceedings of CoNLL, 2004.
[Kudo and Matsumoto, 2001] T. Kudo and Y. Matsumoto.
Chunking with Support Vector Machines. Proceedings of
NAACL, 2001.

[Kwon et al., 2004] N. Kwon, M. Fleischman, and E. Hovy.
FrameNet-based Semantic Parsing using Maximum En-
tropy Models. Proceedings of COLING, 2004.

[Lim et al., 2004] J.-H. Lim, Y.-S. Hwang, S.-Y. Park, and
H.-C. Rim. Semantic Role Labeling using Maximum En-
tropy Model. Proceedings of CoNLL, 2004.

[Park et al., 2004] K.-M. Park, Y.-S. Hwang, and H.-C. Rim.
Two-Phase Semantic Role Labeling based on Support Vec-
tor Machines. Proceedings of CoNLL, 2004.

[Pradhan et al., 2005] S. Pradhan, K. Hacioglu, V. Krugler,
W. Ward, J. H. Martin, and D. Jurafsky. Support Vector
Learning for Semantic Argument Classiﬁcation. To appear
in Machine Learning, 2005.

[Punyakanok et al., 2004] V. Punyakanok, D. Roth, W.-T.
Yih, and D. Zimak. Semantic Role Labeling via Integer
Linear Programming Inference. Proceedings of COLING,
2004.


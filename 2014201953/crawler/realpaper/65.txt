Semi-Supervised Regression with Co-Training

Zhi-Hua Zhou and Ming Li

National Laboratory for Novel Software Technology

Nanjing University, Nanjing 210093, China

{zhouzh, lim}@lamda.nju.edu.cn

Abstract

In many practical machine learning and data min-
ing applications, unlabeled training examples are
readily available but labeled ones are fairly expen-
sive to obtain. Therefore, semi-supervised learn-
ing algorithms such as co-training have attracted
much attention. Previous research mainly focuses
on semi-supervised classiﬁcation. In this paper, a
co-training style semi-supervised regression algo-
rithm, i.e. COREG, is proposed. This algorithm
uses two k-nearest neighbor regressors with differ-
ent distance metrics, each of which labels the unla-
beled data for the other regressor where the label-
ing conﬁdence is estimated through consulting the
inﬂuence of the labeling of unlabeled examples on
the labeled ones. Experiments show that COREG
can effectively exploit unlabeled data to improve
regression estimates.

1 Introduction
In many practical machine learning and data mining appli-
cations such as web user proﬁle analysis, unlabeled training
examples are readily available but labeled ones are fairly ex-
pensive to obtain because they require human effort. There-
fore, semi-supervised learning methods that exploit unlabeled
examples in addition to labeled ones have attracted much at-
tention.

Many current semi-supervised learning methods use a gen-
erative model for the classiﬁer and employ Expectation-
Maximization (EM) [Dempster et al., 1977] to model the
label estimation or parameter estimation process. For ex-
ample, mixture of Gaussians [Shahshahani and Landgrebe,
1994], mixture of experts [Miller and Uyar, 1997], and naive
Bayes [Nigam et al., 2000] have been respectively used as the
generative model, while EM is used to combine labeled and
unlabeled data for classiﬁcation. There are also many other
methods such as using transductive inference for support vec-
tor machines to optimize performance on a speciﬁc test set
[Joachims, 1999], constructing a graph on the examples such
that the minimum cut on the graph yields an optimal labeling
of the unlabeled examples according to certain optimization
functions [Blum and Chawla, 2001], etc.

A prominent achievement in this area is the co-training
paradigm proposed by Blum and Mitchell [1998], which
trains two classiﬁers separately on two sufﬁcient and redun-
dant views, i.e. two attribute sets each of which is sufﬁcient
for learning and conditionally independent to the other given
the class label, and uses the predictions of each classiﬁer on
unlabeled examples to augment the training set of the other.
Dasgupta et al. [2002] have shown that when the require-
ment of sufﬁcient and redundant views is met, the co-trained
classiﬁers could make few generalization errors by maxi-
mizing their agreement over the unlabeled data. Unfortu-
nately, such a requirement can hardly be met in most sce-
narios. Goldman and Zhou [2000] proposed an algorithm
which does not exploit attribute partition. This algorithm re-
quires using two different supervised learning algorithms that
partition the instance space into a set of equivalence classes,
and employs cross validation technique to determine how to
label the unlabeled examples and how to produce the ﬁnal
hypothesis. Although the requirement of sufﬁcient and re-
dundant views is quite strict, the co-training paradigm has al-
ready been used in many domains such as statistical parsing
and noun phrase identiﬁcation [Hwa et al., 2003][Pierce and
Cardie, 2001][Sarkar, 2001][Steedman et al., 2003].

It is noteworthy that previous research mainly focuses on
classiﬁcation while regression remains almost untouched. In
this paper, a co-training style semi-supervised regression al-
gorithm named COREG, i.e. CO-training REGressors, is
proposed. This algorithm employs two k-nearest neighbor
(kNN) regressors, each of which labels the unlabeled data
for the other during the learning process. In order to choose
appropriate unlabeled examples to label, COREG estimates
the labeling conﬁdence through consulting the inﬂuence of
the labeling of unlabeled examples on the labeled examples.
The ﬁnal prediction is made by averaging the regression es-
timates generated by both regressors. Since COREG utilizes
different distance metrics instead of requiring sufﬁcient and
redundant views, its applicability is broad. Moreover, experi-
mental results show that this algorithm can effectively exploit
unlabeled data to improve regression estimates.

The rest of this paper is organized as follows. Section 2
proposes the COREG algorithm. Section 3 presents an anal-
ysis on the algorithm. Section 4 reports on the experimental
results. Finally, Section 5 concludes and raises several issues
for future work.

2 COREG
Let L = {(x1, y1),··· , (x|L|, y|L|)} denote the labeled ex-
ample set, where xi is the i-th instance described by d at-
tributes, yi is its real-valued label, i.e.
its expected real-
valued output, and |L| is the number of labeled examples;
let U denote the unlabeled data set, where the instances are
also described by the d attributes, whose real-valued labels
are unknown, and |U| is the number of unlabeled examples.
Two regressors, i.e. h1 and h2, are generated from L, each
of which is then reﬁned with the help of unlabeled exam-
ples that are labeled by the latest version of the other re-
gressor. Here the kNN regressor [Dasarathy, 1991] is used
as the base learner to instantiate h1 and h2, which labels a
new instance through averaging the real-valued labels of its
k-nearest neighboring examples.

The use of kNN regressor as the base learner is due to the
following considerations. First, the regressors will be reﬁned
in each of many learning iterations. If neural networks or re-
gression trees are used, then in each iteration the regressors
have to be re-trained with the labeled examples in addition to
the newly labeled ones, the computational load of which will
be quite heavy. Since kNN is a lazy learning method which
does not hold a separate training phase, the reﬁnement of the
kNN regressors can be efﬁciently realized. Second, in order
to choose appropriate unlabeled examples to label, the label-
ing conﬁdence should be estimated. In COREG the estimation
utilizes the neighboring properties of the training examples,
which can be easily coupled with the kNN regressors.

It is noteworthy that the initial regressors should be diverse
because if they are identical, then for either regressor, the
unlabeled examples labeled by the other regressor may be
the same as these labeled by the regressor for itself. Thus,
the algorithm degenerates to self-training [Nigam and Ghani,
2000] with a single learner.
In the standard setting of co-
training, the use of sufﬁcient and redundant views enables the
learners be different. Previous research has also shown that
even when there is no natural attribute partitions, if there are
sufﬁcient redundancy among the attributes then a fairly rea-
sonable attribute partition will enable co-training to exhibit
advantages [Nigam and Ghani, 2000]. While in the extended
co-training algorithm which does not require sufﬁcient and
redundant views, the diversity among the learners is achieved
through using different learning algorithms [Goldman and
Zhou, 2000]. Since COREG does not assume sufﬁcient and
redundant views and different learning algorithms, the diver-
sity of the regressors should be sought from other channels.

Here the diversity is achieved through utilizing different
distance metrics.
In fact, a key of kNN learner is how to
determine the distances between different instances. The
Minkowsky distance shown in Eq. 1 is usually used for this
purpose. Note that different concrete distance metrics can be
generated through setting different values to the distance or-
der, p. Roughly speaking, the smaller the order, the more
robust the resulting distance metric to data variations; while
the bigger the order, the more sensitive the resulting distance
metric to data variations. Therefore, the vicinities identiﬁed
for a given instance may be different using the Minkowsky
distance with different orders. Thus, the kNN regressors h1

and h2 can be diverse through instantiating them with differ-
ent p values. Such a setting can also bring another proﬁt, that
is, since it is usually difﬁcult to decide which p value is better
for the concerned task, the functions of these regressors may
be somewhat complementary to be combined.

(cid:195)
d(cid:88)

(cid:33)1/p

M inkowskyp(xr, xs) =

|xr,l − xs,l|p

(1)

l=1

In order to choose appropriate unlabeled examples to la-
bel, the labeling conﬁdence should be estimated such that the
most conﬁdently labeled example can be identiﬁed. In classi-
ﬁcation this is relatively straightforward because when mak-
ing classiﬁcations, many classiﬁers can also provide an esti-
mated probability (or an approximation) for the classiﬁcation,
e.g. a Naive Bayes classiﬁer returns the maximum posteriori
hypothesis where the posterior probabilities can be used, a
BP neural network classiﬁer returns thresholded classiﬁcation
where the real-valued outputs can be used, etc. Therefore, the
labeling conﬁdence can be estimated through consulting the
probabilities of the unlabeled examples being labeled to dif-
ferent classes. For example, suppose the probability of the
instance a being classiﬁed to the classes c1 and c2 is 0.90
and 0.10, respectively, while that of the instance b is 0.60 and
0.40, respectively. Then the instance a is more conﬁdent to
be labeled (to class c1).

Unfortunately, in regression there is no such estimated
probability that can be used directly. This is because in con-
trast to classiﬁcation where the number of class labels to be
predicted is ﬁnite, the possible predictions in regression are
inﬁnite. Therefore, a key of COREG is the mechanism for
estimating the labeling conﬁdence.

Heuristically, the most conﬁdently labeled example of a
regressor should be with such a property, i.e.
the error of
the regressor on the labeled example set should decrease the
most if the most conﬁdently labeled example is utilized. In
other words, the most conﬁdently labeled example should be
the one which makes the regressor most consistent with the
labeled example set. Thus, the mean squared error (MSE)
of the regressor on the labeled example set can be evaluated
ﬁrst. Then, the MSE of the regressor utilizing the information
provided by (xu, ˆyu) can be evaluated on the labeled exam-
ple set, where xu is an unlabeled instance while ˆyu is the
real-valued label generated by the original regressor. Let ∆u
denote the result of subtracting the latter MSE from the for-
mer MSE. Note that the number of ∆u to be estimated equals
to the number of unlabeled examples. Finally, (xu, ˆyu) as-
sociated with the biggest positive ∆u can be regarded as the
most conﬁdently labeled example.

Since repeatedly measuring the MSE of the kNN regres-
sor on the whole labeled example set in each iteration will be
time-consuming, considering that kNN regressor mainly uti-
lizes local information, COREG employs an approximation.
That is, for each xu, COREG identiﬁes its k-nearest neighbor-
ing labeled examples and uses them to compute the MSE. In
detail, let Ω denote the set of k-nearest neighboring labeled
examples of xu, then the most conﬁdently labeled example
is identiﬁed through maximizing the value of ∆xu in Eq. 2,
where h denotes the original regressor while h(cid:48) denotes the

Table 1: Pseudo-code describing the COREG algorithm

ALGORITHM: COREG
INPUT: labeled example set L, unlabeled example set U,

number of nearest neighbors k,
maximum number of learning iterations T ,
distance orders p1, p2

PROCESS:

L1 ← L; L2 ← L
Create pool U(cid:48) by randomly picking examples from U
h1 ← kN N(L1, k, p1); h2 ← kN N(L2, k, p2)
Repeat for T rounds:
for j ∈ {1, 2} do
for each xu ∈ U(cid:48) do
ˆyu ← hj(xu)
Ω ← N eighbors(xu, k, Lj)
j ← kN N(Lj ∪ {(xu, ˆyu)}, k, pj)
h(cid:48)

((yi − hj(xi))2 −(cid:161)

∆xu ← (cid:80)

yi − h(cid:48)

j(xi)

(cid:162)2)

xi∈Ω

end of for
if there exists an ∆xu > 0
then ˜xj ← arg max
xu∈U(cid:48)

∆xu; ˜yj ← hj(˜xj)
πj ← {(˜xj, ˜yj)}; U(cid:48) ← U(cid:48) − πj

else πj ← ∅
end of for
L1 ← L1 ∪ π2; L2 ← L2 ∪ π1
if neither of L1 and L2 changes then exit
else
h1 ← kN N(L1, k, p1); h2 ← kN N(L2, k, p2)
Replenish U(cid:48) by randomly picking examples from U

end of Repeat
OUTPUT: regressor h∗(x) ← 1

2 (h1(x) + h2(x))

reﬁned regressor which has utilized the information provided
by (xu, ˆyu). Note that ˆyu = h(xu).

∆xu =

((yi − h(xi))2 − (yi − h(cid:48)(xi))2)

(2)

(cid:88)

xi∈Ω

The pseudo code of COREG is shown in Table 1, where
the function kN N(Lj, k, pj) returns a kNN regressor on the
labeled example set Lj, whose distance order is pi. The learn-
ing process stops when the maximum number of learning it-
erations, i.e. T , is reached, or there is no unlabeled exam-
ple which is capable of reducing the MSE of any of the re-
gressors on the labeled example set. According to Blum and
Mitchell [1998]’s suggestion, a pool of unlabeled examples
smaller than U is used. Note that in each iteration the unla-
beled example chosen by h1 won’t be chosen by h2, which
is an extra mechanism for encouraging the diversity of the
regressors. Thus, even when h1 and h2 are similar, the exam-
ples they label for each other will still be different.

3 Analysis
This section attempts to analyze whether the learning process
of COREG can use the unlabeled examples to improve the

regression estimates. In order to simplify the discussion, here
the effect of the pool U(cid:48) is not considered as in [Blum and
Mitchell, 1998]. That is, the unlabeled examples are assumed
as being picked from the unlabeled example set U directly.

In each learning iteration of COREG, for each unlabeled ex-
ample xu, its k-nearest neighboring labeled examples are put
into the set Ω. As mentioned before, the newly labeled exam-
ple should make the regressor become more consistent with
the labeled data set. Therefore, a criterion shown in Eq. 3 can
be used to evaluate the goodness of xu, where h is the origi-
nal regressor while h(cid:48) is the one reﬁned with (xu, ˆyu). If the
value of ∆u is positive, then utilizing (xu, ˆyu) is beneﬁcial.
(yi − h(cid:48)(xi))2

(yi − h(xi))2 − 1
|L|

(cid:88)

(cid:88)

1
|L|

∆u =

(3)
In the COREG algorithm, the unlabeled example which
maximizes the value of ∆xu is picked to be labeled. There-
fore, the question is, whether the unlabeled example chosen
according to the maximization of ∆xu will result in a positive
∆u value or not.

First, assume that (xu, ˆyu) is among the k-nearest neigh-
bors of some examples in Ω, and is not among the k-nearest
neighbors of any other examples in L. In this case, it is obvi-
ous that utilizing (xu, ˆyu) will only change the regression es-
timates on the examples in Ω, therefore Eq. 3 becomes Eq. 4.
Comparing Eqs. 2 with 4 it can be found that the maximiza-
tion of ∆xu also results in the maximization of ∆u.

xi∈L

xi∈L

∆u =

1
k

(yi − h(xi))2 − 1
k

(yi − h(cid:48)(xi))2

(4)

(cid:88)

xi∈Ω

(cid:88)

xi∈Ω

(cid:180)2

(cid:179)

(cid:180)2 −

Second, assume that (xu, ˆyu) is not among the k-nearest
neighbors of any example in Ω. In this case, the value of ∆xu
is zero, therefore (xu, ˆyu) won’t be chosen in COREG.

Third, assume that (xu, ˆyu) is among the k-nearest neigh-
bors of some examples in Ω as well as some examples
in L − Ω, and assume these examples in L − Ω are
(cid:48)
(x
1, y

(cid:48)
m). Then Eq. 3 becomes Eq. 5.
((yi − h(xi))2 − (yi − h(cid:48)(xi))2)+

(cid:48)
m, y

(cid:48)

(cid:88)
1),··· , (x
(cid:179)
(cid:88)
∆u =

xi∈Ω

1
k

(cid:48)

(cid:48)

)

(

(5)

q∈{1,···,m}

q − h(x
(cid:48)
q)
y

q − h(cid:48)(x
(cid:48)
q)
y

1
m
Maximizing ∆xu will maximize the ﬁrst sum term of Eq. 5,
but whether it can enable ∆u be positive should also refer the
second sum term. Unfortunately, the value of this sum term
is difﬁcult to be measured except that the neighboring rela-
tionships between all the labeled examples and (xu, ˆyu) are
evaluated. Therefore, there may exist cases where the unla-
beled example chosen according to the maximization of ∆xu
may decrease ∆u, which is the cost COREG takes for using
∆xu that can be more efﬁciently computed to approximate
∆u. Nevertheless, experiments show that in most cases such
an approximation is effective.

It seems that using only one regressor to label the unlabeled
examples for itself might be feasible, where the unlabeled ex-
amples can be chosen according to the maximization of ∆xu.

While considering that the labeled example set usually con-
tains noise, the use of two regressors can be helpful to reduce
overﬁtting.

Let Λ denote the subset of noisy examples in L. For the
unlabeled instance xu, either of the regressors h1 and h2 will
identify a set of k-nearest neighboring labeled examples for
xu. Assume these sets are Ω1 and Ω2, respectively. Since h1
and h2 use different distance orders, Ω1 and Ω2 are usually
different, and therefore Ω1 ∩ Λ and Ω2 ∩ Λ are also usually
different. Suppose xu is labeled by h1 and then (xu, h1(xu))
is put into L1, where h1(xu) suffers from Ω1 ∩ Λ. For an-
other unlabeled instance xv which is very close to xu, its
k-nearest neighbors identiﬁed by h1 will be very similar to
Ω1 except that (xu, h1(xu)) has replaced a previous neigh-
bor. Thus, h1(xv) will suffer from Ω1 ∩ Λ more seriously
than h1(xu) does. While, if the instance xu is labeled by h2
and (xu, h2(xu)) is put into L1, then h1(xv) will suffer from
Ω1 ∩ Λ only once, although xu is still very close to xv.
4 Experiments
Experiments are performed on ten data sets listed in Table 2
where “# attribute” denotes the number of input attributes.
These data sets have been used in [Zhou et al., 2002] where
the detailed descriptions of the data sets can be found. Note
that the input attributes as well as the real-valued labels have
been normalized to [0.0, 1.0].

Table 2: Experimental data sets

Data set
2-d Mexican Hat
3-d Mexican Hat
Friedman #1
Friedman #2
Friedman #3
Gabor
Multi
Plane
Polynomial
SinC

# attribute Size
5,000
3,000
5,000
5,000
3,000
3,000
4,000
1,000
3,000
3,000

1
2
5
4
4
2
5
2
1
1

For each data set, 25% data are kept as the test set, while
the remaining 75% data are partitioned into the labeled and
unlabeled sets where 10% (of the 75%) data are used as la-
beled examples while the remaining 90% (of the 75%) data
are used as unlabeled examples.

In the experiments, the distance orders used by the two
kNN regressors in COREG are set to 2 and 5, respectively,
the k value is set to 3, the maximum number of iterations T is
set to 100, and the pool U(cid:48) contains 100 unlabeled examples
randomly picked from the unlabeled set in each iteration.

A self-training style algorithm is tested for comparison,
which is denoted by SELF. This algorithm uses a kNN re-
gressor and in each iteration, it chooses the unlabeled exam-
ple which maximizes the value of ∆xu in Eq. 2 to label for
itself. Moreover, a co-training style algorithm, denoted by
ARTRE, is also tested. Since the experimental data sets are
with no sufﬁcient and redundant views, here an artiﬁcial re-
dundant view is developed through deriving new attributes

from the original ones. For example, on 3-d Mexican Hat two
new attributes, i.e. x3 and x4, are constructed from x1 + x2
and x1 − x2, and then a kNN regressor is built on x1 and x2
while the other is built on x3 and x4. In each iteration, each
kNN regressor chooses the unlabeled example which maxi-
mizes the value of ∆xu in Eq. 2 to label for the other regres-
sor. The ﬁnal prediction is made by averaging the regression
estimates of these two reﬁned regressors. Besides, a kNN re-
gressor using only the labeled data is tested as a baseline for
the comparison, which is denoted by LABELED.

All the kNN regressors used in SELF, ARTRE, and LA-
BELED employ 2nd-order Minkowski distance, and the k
value is set to 3. The same pool, U(cid:48), as that used by COREG is
used in each iteration of SELF and ARTRE, and the maximum
number of iterations is also set to 100.

One hundred runs of experiments are carried out on each
data set. In each run, the performance of all the four algo-
rithms, i.e. COREG, SELF, ARTRE, and LABELED, are eval-
uated on randomly partitioned labeled/unlabeled/test splits.
The average MSE at each iteration is recorded. Note that
the learning processes of the algorithms may stop before the
maximum number of iterations is reached, and in that case,
the ﬁnal MSE is used in computing the average MSE of the
following iterations.

The improvement on average MSE obtained by exploiting
unlabeled examples is tabulated in Table 3, which is com-
puted by subtracting the ﬁnal MSE from the initial MSE and
then divided by the initial MSE.

Table 3: Improvement on average mean squared error

Data set
2d Mexican Hat
3d Mexican Hat
Friedman #1
Friedman #2
Friedman #3
Gabor
Multi
Plane
Polynomial
SinC

SELF ARTRE COREG
19.6%
9.2% 12.8%
5.7%
3.7%
3.9%
-1.8%
-4.0%
0.5%
2.1%
-4.3%
-1.3%
0.0%
-3.6%
-0.9%
9.0%
3.8%
4.0%
1.4%
-1.9%
-4.4%
-3.8%
-3.5%
-1.6%
22.0%
15.1% 17.4%
13.0% 16.4%
26.0%

Table 3 shows that SELF and ARTRE improve the regres-
sion estimates on only ﬁve data sets, while COREG improves
on eight data sets. Moreover, Table 3 discloses that the im-
provement of COREG is always bigger than that of SELF and
ARTRE. These observations tell that COREG can effectively
exploit unlabeled examples to improve regression estimates.
For further studying the compared algorithms, the average
MSE of different algorithms at different iterations are plotted
in Fig 1, where the average MSE of the two kNN regressors
used in COREG are also depicted. Note that in each subﬁg-
ure, every curve contains 101 points corresponding to the 100
learning iterations in addition to the initial condition, where
only 11 of them are explicitly depicted for better presentation.
Fig. 1 shows that COREG can exploit unlabeled examples
to improve the regression estimates on most data sets, except
that on Friedman #3 there is no improvement while on Plane

(a) 2-d Mexican Hat

(b) 3-d Mexican Hat

(c) Friedman #1

(d) Friedman #2

(e) Friedman #3

(f) Gabor

(g) Multi

(h) Plane

(i) Polynomial

(j) SinC

Legend

Figure 1: Comparisons on average mean squared error of different algorithms at different iterations

the performance is degenerated. While, SELF and ARTRE de-
generate the regression estimates on ﬁve data sets, i.e. Fried-
man #1 to #3, Multi, and Plane. Moreover, the average MSE
of the ﬁnal prediction made by COREG is almost always the
best except on Friedman #1 where ARTRE is slightly better
and on Plane where LABELED is the best while all the semi-
supervised learning algorithms degenerate the performance.
These observations disclose that COREG is apparently the
best algorithm in the comparison.

Pairwise two-tailed t-tests with 0.05 signiﬁcance level re-
veal that the ﬁnal regression estimates of COREG are signif-
icantly better than its initial regression estimates on almost
all the data sets except that on Plane the latter is better while
on Friedman #3 there is no signiﬁcant difference. Moreover,

the ﬁnal regression estimates of COREG are signiﬁcantly bet-
ter than these of ARTRE on almost all the data sets except
on Friedman #1 where the latter is better. Furthermore, the
ﬁnal regression estimates of COREG are signiﬁcantly better
than these of SELF and LABELED on almost all the data sets
except on Plane where LABELED is better and on Fried-
man #3 where there is no signiﬁcant difference. These re-
sults of t-tests conﬁrm that COREG is the strongest among
the compared algorithms, which can effectively exploit unla-
beled data to improve the regression estimates.

5 Conclusion
This paper proposes a co-training style semi-supervised re-
gression algorithm COREG. This algorithm employs two k-

0204060801004.44.64.855.25.4x 10−3Mean Squared ErrorIterations0204060801000.0550.0560.0570.0580.0590.06Mean Squared ErrorIterations0204060801000.0630.0640.0650.0660.0670.0680.0690.070.0710.072Mean Squared ErrorIterations0204060801000.0450.050.0550.060.0650.070.0750.08Mean Squared ErrorIterations0204060801000.10.1050.110.1150.120.1250.13Mean Squared ErrorIterations0204060801000.0360.0370.0380.0390.040.041Mean Squared ErrorIterations0204060801000.05150.0520.05250.0530.05350.0540.05450.0550.05550.0560.0565Mean Squared ErrorIterations0204060801000.3040.3060.3080.310.3120.3140.3160.318Mean Squared ErrorIterations0204060801003.63.844.24.44.64.8x 10−3Mean Squared ErrorIterations0204060801003.23.43.63.844.24.44.6x 10−3Mean Squared ErrorIterationsnearest neighbor regressors using different distance metrics.
In each learning iteration, each regressor labels the unlabeled
example which can be most conﬁdently labeled for the other
learner, where the labeling conﬁdence is estimated through
considering the consistency of the regressor with the labeled
example set. The ﬁnal prediction is made by averaging the
predictions of both the reﬁned kNN regressors. Experiments
show that COREG can effectively exploit unlabeled data to
improve the regression estimates.

In contrast to standard co-training setting, COREG does not
require sufﬁcient and redundant views, which enables it have
broad applicability. However, this forces COREG generate di-
verse initial regressors with speciﬁc mechanisms. In this pa-
per the diversity is obtained by instantiating the Minkowski
distance with different distance orders. It is obvious that us-
ing completely different distance metrics may be more help-
ful. Moreover, trying to obtain the diversity of the initial
regressors from channels other than using different distance
metrics is an issue to be investigated in future work. Note that
although this paper uses kNN regressor as the base learner,
an important idea of COREG, i.e. regarding the labeling of
the unlabeled example which makes the regressor most con-
sistent with the labeled example set as with the most conﬁ-
dence, can also be used with other base learners. Therefore,
designing semi-supervised regression algorithms with other
base learners along the way of COREG is another interest-
ing issue to be explored in the future. Furthermore, designing
semi-supervised regression algorithms outside the co-training
framework is also well-worth studying.

Acknowledgments
Supported by NSFC (60325207), FANEDD (200343), and
JIANGSUSF (BK2004001).

References
[Blum and Chawla, 2001] A. Blum and S. Chawla. Learn-
ing from labeled and unlabeled data using graph mincuts.
In Proceedings of the 18th International Conference on
Machine Learning, pages 19–26, Williamston, MA, 2001.
Morgan Kaufmann.

[Blum and Mitchell, 1998] A. Blum and T. Mitchell. Com-
bining labeled and unlabeled data with co-training.
In
Proceedings of the 11th Annual Conference on Compu-
tational Learning Theory, pages 92–100, Madison, WI,
1998. ACM Press.

[Dasarathy, 1991] B. V. Dasarathy.

Nearest Neighbor
Norms: NN Pattern Classiﬁcation Techniques. IEEE Com-
puter Society Press, Los Alamitos, CA, 1991.

[Dasgupta et al., 2002] S. Dasgupta, M. Littman,

and
D. McAllester. PAC generalization bounds for co-training.
In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,
Advances in Neural Information Processing Systems 14,
pages 375–382. MIT Press, Cambridge, MA, 2002.

[Dempster et al., 1977] A. P. Dempster, N. M. Laird, and
D. B. Rubin. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical So-
ciety, Series B, 39(1):1–38, 1977.

[Goldman and Zhou, 2000] S. Goldman and Y. Zhou. En-
hancing supervised learning with unlabeled data. In Pro-
ceedings of the 17th International Conference on Machine
Learning, pages 327–334, San Francisco, CA, 2000. Mor-
gan Kaufmann.

[Hwa et al., 2003] R. Hwa, M. Osborne, A. Sarkar, and
M. Steedman. Corrected co-training for statistical parsers.
In Working Notes of the ICML’03 Workshop on the Contin-
uum from Labeled to Unlabeled Data in Machine Learning
and Data Mining, Washington, DC, 2003.

[Joachims, 1999] T. Joachims. Transductive inference for
text classiﬁcation using support vector machines. In Pro-
ceedings of the 16th International Conference on Machine
Learning, pages 200–209, Bled, Slovenia, 1999. Morgan
Kaufmann.

[Miller and Uyar, 1997] D. J. Miller and H. S. Uyar. A mix-
ture of experts classiﬁer with learning based on both la-
belled and unlabelled data. In M. Mozer, M. I. Jordan, and
T. Petsche, editors, Advances in Neural Information Pro-
cessing Systems 9, pages 571–577. MIT Press, Cambridge,
MA, 1997.

[Nigam and Ghani, 2000] K. Nigam and R. Ghani. Analyz-
ing the effectiveness and applicability of co-training.
In
Proceedings of the 9th ACM International Conference on
Information and Knowledge Management, pages 86–93,
Washington, DC, 2000. ACM Press.

[Nigam et al., 2000] K. Nigam, A. K. McCallum, S. Thrun,
and T. Mitchell. Text classiﬁcation from labeled and un-
labeled documents using EM. Machine Learning, 39(2–
3):103–134, 2000.

[Pierce and Cardie, 2001] D. Pierce and C. Cardie. Limi-
tations of co-training for natural language learning from
large data sets.
In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Processing,
pages 1–9, Pittsburgh, PA, 2001.

[Sarkar, 2001] A. Sarkar. Applying co-training methods to
statistical parsing. In Proceedings of the 2nd Annual Meet-
ing of the North American Chapter of the Association for
Computational Linguistics, pages 95–102, Pittsburgh, PA,
2001.

[Shahshahani and Landgrebe, 1994] B. Shahshahani

and
D. Landgrebe. The effect of unlabeled samples in re-
ducing the small sample size problem and mitigating the
hughes phenomenon.
IEEE Transactions on Geoscience
and Remote Sensing, 32(5):1087–1095, 1994.

[Steedman et al., 2003] M.

Steedman, M. Osborne,
A. Sarkar, S. Clark, R. Hwa, J. Hockenmaier, P. Ruhlen,
S. Baker, and J. Crim. Bootstrapping statistical parsers
from small data sets.
In Proceedings of the 11th Con-
ference on the European Chapter of the Association for
Computational Linguistics, pages 331–338, Budapest,
Hungary, 2003.

[Zhou et al., 2002] Z.-H. Zhou, J. Wu, and W. Tang. Ensem-
bling neural networks: many could be better than all. Ar-
tiﬁcial Intelligence, 137(1–2):239–263, 2002.


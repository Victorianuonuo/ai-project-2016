Planning under Risk and Knightian Uncertainty

Felipe W. Trevizan

Instituto de Matem´atica e Estat´ıstica

F´abio G. Cozman
Escola Polit´ecnica

Leliane N. de Barros

Instituto de Matem´atica e Estat´ıstica

Universidade de S˜ao Paulo

Universidade de S˜ao Paulo

Universidade de S˜ao Paulo

Rua do Mat˜ao, 1010
S˜ao Paulo, SP, Brazil
trevisan@ime.usp.br

Av. Prof. Mello Moraes, 2231

S˜ao Paulo, SP, Brazil

fgcozman@usp.br

Rua do Mat˜ao, 1010
S˜ao Paulo, SP, Brazil
leliane@ime.usp.br

Abstract

Two noteworthy models of planning in AI are prob-
abilistic planning (based on MDPs and its gener-
alizations) and nondeterministic planning (mainly
based on model checking). In this paper we: (1)
show that probabilistic and nondeterministic plan-
ning are extremes of a rich continuum of prob-
lems that deal simultaneously with risk and (Knigh-
tian) uncertainty; (2) obtain a unifying model for
these problems using imprecise MDPs; (3) derive
a simpliﬁed Bellman’s principle of optimality for
our model; and (4) show how to adapt and analyze
state-of-art algorithms such as (L)RTDP and LDFS
in this unifying setup. We discuss examples and
connections to various proposals for planning un-
der (general) uncertainty.

1 Introduction
Planning problems can be classiﬁed, based on the effects of
actions, in deterministic, probabilistic or nondeterministic. In
this paper we are concerned with action dynamics under gen-
eral forms of uncertainty; indeed, we are interested in plan-
ning under both risk and Knightian uncertainty. We show
how to use these concepts to express probabilistic and non-
deterministic planning (and combinations thereof) as Markov
decision processes with set-valued transitions (MDPSTs).

Similar generalizations of Markov decision processes
(MDPs) have appeared before in research on artiﬁcial intel-
ligence. For example, Givan et al. [2000] use intervals to
encode a set of exact MDPs, which is used to conduct space
state reduction of MDPs. Their bounded-parameter MDPs
(BMDPs) form neither a superset nor a subset of MDP-
STs. Buffet and Aberdeen [2005] use BMDPs to produce
robust policies in probabilistic planning. They also show
that Real-Time Dynamic Programming (RTDP) can be used
in BMDPs. Our perspective is different: we wish to unify
various strands of planning that have proven practical value,
using a theory that has a behavioral basis on preferences and
beliefs — otherwise, we do follow a similar path to Buf-
fet and Aberdeen’s in that we exploit RTDP in our models.
Another recent work that should be mentioned is Perny et
al.’s [2005], where transitions must only satisfy a few alge-
braic properties. Our models are a strict subset of their Al-

gebraic MDPs (AMDPs); we forgo some generality because
we want to employ models with solid behavioral justiﬁcation,
and to exploit the speciﬁc structure of combined probabilistic-
nondeterministic planning. Even though general algorithms
such as AMDP-based value iteration are useful theoretically,
we ﬁnd that a more speciﬁc approach based on Real-Time
Dynamic Programming leads to encouraging results on com-
putational complexity.

This necessarily brief review of closest literature should in-
dicate the central motivation of this work: we strive to work
with decision processes that have solid behavioral founda-
tion and that can smoothly mix problems of practical signif-
icance. A similar approach has been proposed by Eiter and
Lukasiewicz [2003], using nonmonotonic logic and causal se-
mantics to deﬁne set-valued transitions in Partial Observable
MDPs (and leaving algorithms for future work). We offer a
model that assumes full observability, and we obtain algo-
rithms and complexity analysis for our model.

The remainder of this paper is organized as follows.

In
Section 3 we discuss how MDPSTs capture the continuum of
planning problems from “pure” probabilistic to “pure” non-
deterministic planning. In Section 4 we show that MDPSTs
are Markov decision processes with imprecise probabilities
(MDPIPs), a model that has received attention in operations
research and that displays a solid foundation. We also com-
ment on the various relationships between MDPSTs and other
models in the literature. In Section 5 we show that MDPSTs
lead to important simpliﬁcations of their “minimax” Bellman-
style equation (we note that such simpliﬁcations are men-
tioned without a proof by Buffet and Aberdeen [2005] for
BMDPs). We obtain interesting insights concerning compu-
tational complexity of MDPSTs and related models. Section
6 investigates algorithms that produce minimax policies for
MDPSTs. Although our results yield easy variants of value
and policy iteration for MDPSTs, we are interested in more
efﬁcient algorithms based on RTDP. In Section 6 we derive
the conditions that must be true for RTDP to be applied. Sec-
tion 7 brings a few concluding remarks.

2 Background

2.1 Varieties of planning

We start reviewing a few basic models of planning problems,
attempting to unify them as much as possible as suggested by

IJCAI-07

2023

recent literature [Bonet and Geffner, 2006]:
M1 a discrete and ﬁnite state space S,
M2 a nonempty set of initial states S0 ⊆ S,
M3 a goal given by a set SG ⊆ S,
M4 a nonempty set of actions A(s) ⊆ A representing the

actions applicable in each state s,

M5 a state transition function F (s, a) ⊆ S mapping state s
and action a ∈ A(s) into nonempty sets of states, i.e.
|F (s, a)| ≥ 1, and

M6 a positive cost C(s, a) for taking a ∈ A(s) in s.

Adapting M2, M5 and M6, one can produce:

• Deterministic models (DET), where the state transition
function is deterministic: |F (s, a)| = 1. In “classical”
planning, the following constraints are added: (i) |S0| =
1; (ii) SG (cid:5)= ∅; and (iii) ∀s ∈ S, a ∈ A(s) : C(s, a) = 1.
• Nondeterministic models (NONDET), where the ac-
tions may result in more than one successor state without
preferences among them.

• Probabilistic models (MDPs), where actions have
probabilistic consequences. Not only the function
|F (s, a)| ≥ 1 is given, but also the model includes:
(MDP1) a probability distribution P0(·) over S0; and
(MDP2) a probability distribution P (·|s, a) over F (s, a)
for all s ∈ S, a ∈ A(s).

For any of these models, we expect that a solution (e.g. a
policy) is evaluated on its long-term costs. The cost of a so-
lution can be evaluated in a ﬁnite-horizon, in which the max-
imum number of actions to be executed is limited to k ∈ R+.
An alternative is to consider discounted inﬁnite-horizon, in
which the number of actions is not bounded and the cost of
actions is discounted geometrically using a discount factor
0 < γ < 1. Since it is difﬁcult to ﬁnd an appropriate k
for each problem, in this paper we assume the discounted
inﬁnite-horizon framework.1

Due to the assumption of full observability and discounted
inﬁnite-horizon cost, a valid solution is a stationary policy,
that is, a function π mapping states s ∈ S into actions
a ∈ A(s). Bellman’s principle of optimality deﬁnes the op-
timal cost function V ∗(s) = mina∈A(s) QV ∗ (s, a) [Bellman,
1957], where:

C(s, a) + γV (s(cid:2)), s(cid:2) ∈ F (s, a) for DET,
C(s, a) + γ max
X

V (s(cid:2)) for NONDET, and
P (s(cid:2)|s, a)V (s(cid:2)) for MDPs.

C(s, a) + γ

(1)

8>>><
>>>:

QV (s, a)=

s(cid:3)∈F (s,a)

s(cid:3)∈F (s,a)

This principle characterizes V ∗
(also called optimal value
function) and induces the optimal policy for each model:
π∗(s) = argmina∈A(s) QV ∗ (s, a).
The deﬁnition of
QV (s, a) clariﬁes the guarantees of each model.
In DET,
guarantees given by π∗
do not depend on its execution; in
NONDET guarantees are on the worst-case cost; and in
MDPs guarantees are on expected cost. There are algorithms

1Results presented here are also applicable to ﬁnite-horizon, and

can be easily adapted to address partial observability.

that compute the optimal policies for each one of these mod-
els, and algorithms that can be specialized to all of them
[Bonet and Geffner, 2006]. However, we should emphasize
that previous unifying frameworks do not intend to handle
smooth “mixtures” of these planning problems. In fact, one
of our goals in this paper is to provide a framework where
NONDET and MDPs are the extreme points of a continuum
of planning problems.

2.2 Varieties of uncertainty

Probability theory is often based on decision theory [Berger,
1985], a most appropriate scheme in the realm of planning.
Thus a decision maker contemplates a set of actions, each
one of which yields different rewards in different states of
nature. Complete preferences over actions imply that a pre-
cise probability value is associated with each state — a situa-
tion of risk [Knight, 1921; Luce and Raiffa, 1957]. An obvi-
ous example of sequential decision making under “pure” risk
is probabilistic planning. However, often preferences over
actions are only partially ordered (due to incompleteness in
beliefs, or lack of time/resources, or because several experts
disagree), and then it is not possible to guarantee that precise
probabilities represent beliefs. In those cases, a set of proba-
bility measures is the adequate representation for uncertainty;
such sets are often referred to as credal sets [Levi, 1980;
Kadane et al., 1999; Walley, 1991]. Even though terminology
is not stable, this situation is said to contain Knightian uncer-
tainty (other terms are ambiguity or simply uncertainty). An
extreme case is nondeterministic planning, where no proba-
bilities are speciﬁed.2

Note that actual decision making is rarely restricted to ei-
ther “pure” risk nor “pure” Knightian uncertainty; in fact
the most realistic scenario mixes elements of both. Not sur-
prisingly, such combinations are well studied in economics,
psychology, statistics, and philosophy. We note that credal
sets have raised steady interested in connection with arti-
ﬁcial intelligence, for example in the theory of probabilis-
tic logic [Nilsson, 1986], in Dempster-Shafer theory [Shafer,
1976], in theories of argumentation [Anrig et al., 1999],
and in generalizations of Bayesian networks [Cozman, 2005;
Fagiuoli and Zaffalon, 1998].

The usual prescription for decision making under risk is to
select an action that maximizes expected utility. In the pres-
ence of Knightian uncertainty, matters become more com-
plex, as now a decision maker carries a set of probability
measures and consequently every action is associated with
an interval of expected costs [Walley, 1991]. Thus a decision
maker may choose one of several criteria, such as minimax-
ity, maximality, E-admissibility [Troffaes, 2004]. In this pa-
per we follow a minimax approach, as we are interested in
actions that minimize the maximum possible expected cost;
we leave other criteria for future work.

2The term “nondeterministic” is somewhat unfortunate as non-
determinism is often equated to probabilism; perhaps the term plan-
ning under pure Knightian uncertainty, although longer, would offer
a better description.

IJCAI-07

2024

Drug d1

0.8

cardiopathy

Drug d2

cardiopathy

0.8

Heart transplant

cardiopathy

severe

cardiopathy

0.4

controlled
cardiopathy

0.6

unrecorverable

cardiopathy

0.7

0.2

0.3

death

stroke

controlled
cardiopathy
with sequels

severe

cardiopathy

0.7

0.2

unrecorverable

cardiopathy

controlled
cardiopathy

severe

cardiopathy

controlled
cardiopathy
with sequels

unrecorverable

cardiopathy

0.8

0.6

0.7

controlled
cardiopathy

controlled
cardiopathy
with sequels

stroke

0.3

0.4

0.6

death

0.2

0.3

0.4

death

stroke

Cost of actions

State

cardiopathy
severe
cardiopathy
unrecoverable
cardiopathy
controled
cardiopathy
controled
cardiopathy
with sequels
stroke
death

d1
30

30

40

d2
20

25

30

HT
75

70

70

−−−

−−−

−−−

−−−

−−−

−−−

Noop
−−−

−−−

−−−

0

2

−−−
−−−

−−−
−−−

−−−
−−−

85
100

Figure 1: An MDPST representing the Example 1. Dotted lines indicate each one of the reachable sets. Cost of taking actions
d1, d2 and HT in the Example 1. States with “–” indicates the action is not applicable. Action Noop represents the persistence
action for the absorbing states.

3 Markov decision processes with set-valued

transitions

In this section we develop our promised synthesis of proba-
bilistic and nondeterministic planning. We focus on the tran-
sition function; that is, on M5. Instead of taking F (s, a) ⊆ S,
we now have a set-valued F(s, a) ⊆ 2S\∅; that is, F(s, a)
maps each state s and action a ∈ A(s) into a set of nonempty
subsets of S. We refer to each set k ∈ F(s, a) as a reachable
set. A transition from state s given action a is now asso-
ciated with a probability P (k|s, a); note that there is Knight-
ian uncertainty concerning P (s(cid:4)|s, a) for each successor state
s(cid:4) ∈ k. We refer to the resulting model as a Markov deci-
sion process with set-valued transitions (MDPSTs):
transi-
tions move probabilistically to reachable sets, and the prob-
ability for a particular state is not resolved by the model. In
fact, there is a close connection between probabilities over
F(s, a) and the mass assignments that are associated with the
theory of capacities of inﬁnite order [Shafer, 1976]; to avoid
confusion between P (k|s, a) and P (s(cid:4)|s, a), we refer to the
former as mass assignments and denote them by m(k|s, a).
Thus an MDPST is given by M1, M2, M3, M4, M6, MDP1,
MDPST1 a state transition function F(s, a) ⊆ 2S\∅ mapping
states s and actions a ∈ A(s) into reachable sets of
S, and

MDPST2 mass assignments m(k|s, a) for all s, a ∈ A(s),

and k ∈ F(s, a).

There are clearly two varieties of uncertainty in a MDPST:
a probabilistic selection of a reachable set and a nondetermin-
istic choice of a successor state from the reachable set. An-
other important feature of MDPSTs is that they encompass
models discussed in Section 3:

• DET: There is always a single successor state: ∀s ∈
S, a ∈ A(s) : |F(s, a)| = 1 and ∀s ∈ S, a ∈ A(s), k ∈
F(s, a) : |k| = 1.

• NONDET: There is always a single reachable set, but
selection within this set is left unspeciﬁed (nondeter-
ministic): ∀s ∈ S, a ∈ A(s) :
|F(s, a)| = 1, and
∃s ∈ S, a ∈ A(s), k ∈ F(s, a) : |k| > 1.

• MDPs: Selection of k ∈ F(s, a) is probabilistic and it
resolves all uncertainty: ∀s ∈ S, a ∈ A(s) : |F(s, a)| >
1, and ∀s ∈ S, a ∈ A(s), k ∈ F(s, a) : |k| = 1.

Example 1 A hospital offers three experimental treatments
to cardiac patients: drug d1, drug d2 and heart transplant
(HT ). State s0 indicates patient with cardiopathy. The effects
of those procedures lead to other states: severe cardiopathy
(s1), unrecoverable cardiopathy (s2), cardiopathy with se-
quels (s3), controlled cardiopathy (s4), stroke (s5), and death
(s6). There is little understanding about drugs d1 and d2, and
considerable data on heart transplants. Consequently, there
is “partial” nondeterminism (that is, there is Knightian un-
certainty) in the way some of the actions operate. Figure 1
depicts transitions for all actions, indicating also the mass
assignments and the costs. For heart transplant, we suppose
that all transitions are purely probabilistic.

4 MDPSTs, MDPIPs and BMDPs
In this section we comment on the relationship between
MDPSTs and two existing models in the literature: Markov
decision processes with imprecise probabilities (MDPIPs)
[White III and Eldeib, 1994; Satia and Lave Jr, 1973] and
bounded-parameter Markov decision processes (BMDPs)
[Givan et al., 2000].

An MDPIP is a Markov decision process where transitions
are speciﬁed through sets of probability measures; that is, the
effects of an action are modelled by a credal set K over the
state space. An MDPIP is given by M1, M2,M3, M4, M6,
MDP1 and
MDPIP1 a nonempty credal set Ks(a) for all s ∈ S and
a ∈ A(s), representing probability distributions
P (s(cid:4)|s, a) over successor states in S.

In this paper we assume that a decision maker seeks a min-
imax policy (that is, she selects a policy that minimizes the
maximum cost across all possible probability distributions).
This adopts an implicit assumption that probabilities are se-
lected in an adversarial manner; other interpretations for MD-
PIPs are possible [Troffaes, 2004]. Under the minimax inter-
pretation, the Bellman principle of optimality is [Satia and
Lave Jr, 1973]:
V ∗(s) = min
a∈A(s)

P (·|s, a)V ∗(s(cid:2))};

X

s(cid:3)∈S

max

{C(s, a) + γ

P (·|s,a)∈Ks(a)

(2)
moreover, this equation always has a unique solution that
yields the optimal stationary policy for the MDPIP. To inves-
tigate the relationship between MDPSTs and MDPIPs, the
following notation is useful: when k ∈ F(s, a), we denote by

IJCAI-07

2025

MDPST

.5

.4

.1

(a)

s0

.7

p

(c)

s

1−p

s1

.3

s1

s2

3s

s4

BMDP

[.5,.9]

[.7,.7]

[.1,.5]

s1
[.3,.3]

(b)

s0

(d)

s

[0,p]

[0,p]

[0,1]

[0,1−p]

s1

s2

3s

s4

Figure 2: This ﬁgure illustrates two examples of planning un-
der uncertainty modeled through MDPSTs and BMDPs. Ex-
ample 1 is the Heart example from Perny et al. (2005). Ex-
ample 2 is a simple example in which none of the models can
express the problem modelled by the other one.
(cid:4)
D(k, s, a) the set of states such that k \
.
Thus D(k, s, a) represents all nondeterministic effects of k
that belong only to k. We now have:
Proposition 1 Any MDPST p = (cid:9)S, S0, SG, A, F, C, P0, m(cid:10)
is expressible by an MDPIP q = (cid:9)S, S0, SG, A, F, C, P0, K(cid:10).

k(cid:3)∈F(s,a)(cid:5)=k k(cid:4)

(cid:2)(cid:3)

Proof (detailed in [Trevizan et al., 2006]) It is enough to
prove that ∀s ∈ S, a ∈ A(s), F(s, a) (MDPST1) and m(s, a)
(MDPST2) imply Ks(a) (MDPIP1). First, note that MDPST2
bounds for all s(cid:4) ∈ S the probability of being in state s(cid:4)
after
applying action a in state s as follows
(cid:5)

m({s(cid:4)}|s, a) ≤ P (s(cid:4)|s, a) ≤

m(k|s, a) ≤ 1.

(3)

k∈F(s,a)∧s(cid:3)∈k

(To see that, use the deﬁnition of reachable sets:
F(s, a); if s(cid:4)
nondeterministic effect of a.)

(cid:5)∈ k, then it is not possible to select s(cid:4)

let k ∈
as a

From MDPST1 and MDPST2 it is possible to bound the sum
of the probabilities of each state in a reachable set k ∈ F(s, a)
and in the associated set D(k, s, a):

0 ≤

P (s(cid:4)|s, a) ≤ m(k|s, a) ≤

P (s(cid:4)|s, a) ≤ 1 (4)

s(cid:3)∈D(k,s,a)

s(cid:3)∈k

The set of inequalities (3) and (4) for s ∈ S and a ∈ A(s)
describe a possible credal set Ks(a) for MDPIP1.
(cid:12)(cid:13)
Deﬁnition 1 The MDPIP q obtained through Proposition 1
is called the associated MDPIP of p.

As noted in Section 1, BMDPs are related to MDPSTs. In-
tuitively, BMDPs are Markov decision processes where tran-
sition probabilities and rewards are speciﬁed by intervals [Gi-
van et al., 2000]. Thus BMDPs are not comparable to MD-
PIPs due to possible imprecision in rewards; here we only
consider those BMDPs that have real-valued rewards. Clearly
these BMDPs form a strict subset of MDPIPs. The relation-
ship between such BMDPs and MDPSTs is more complex.
Figure 2.a and 2.b presents an MDPST and an BMDP that
are equivalent (that is, they represent the same MDPIP). Fig-
ure 2.c and 2.d presents an MDPST that cannot be expressed

(cid:5)

(cid:5)

MDPIP

DET MDP

NON−DET

BMDP

MDPST

Figure 3: Relationships between models (BMDPs with pre-
cise rewards).

as an BMDP, and an BMDP that cannot be expressed as an
MDPST. As a technical aside, we note that the F(s, a) de-
ﬁne Choquet capacities of inﬁnite order, while transitions
in BMDPs deﬁne Choquet capacities of second order [Wal-
ley, 1991]; clearly they do not have the same representational
power.

The results of this section are captured by Figure 3.

In
the next two sections we present our main results, where we
explore properties of MDPSTs that make these models rather
amenable to practical use.

5 A simpliﬁed Bellman equation for MDPSTs
We now present a substantial simpliﬁcation of the Bellman
principle for MDPSTs. The intuition behind the following
result is this. In Equation (2), both minima and maxima are
taken with respect to all combinations of actions and possible
probability distributions. However, it is possible to “pull” the
maximum inside the summation, so that less combinations
need be considered.
Theorem 2 For any MDPST and its associated MDPIP, (2)
is equivalent to:
V ∗(s) = min
a∈A(s)

m(k|s, a) max
s(cid:3)∈k

V ∗(s(cid:2))} (5)

{C(s, a) + γ

X

k∈F(s,a)

IP (s) and V ∗

Proof Deﬁne V ∗
ST (s) as a shorthand for the val-
ues obtained through, respectively, (2) and (5). We want to
prove that for all MDPST p = (cid:9)S, S0, SG, A, F, C, P0, m(cid:10),
its associated MDPIP q = (cid:9)S, S0, SG, A, F, C, P0, K(cid:10), and
∀s ∈ S, V ∗
ST (s). Due to the Proposition 1, we have
that the probability measure induced by maxs(cid:3)∈k V ∗(s(cid:4)) for
k ∈ F(s, a) in (5) is a valid choice according to Ks(a), there-
fore ∀s ∈ S, V ∗
IP (s). Now, it is enough to show
that ∀s ∈ S, V ∗

IP (s) to conclude this proof.

ST (s) ≥ V ∗
ST (s) ≤ V ∗

IP (s) = V ∗

ST (s) ≤ V ∗

(cid:6)
IP (s) and suppose that V ∗
ST (s) > V ∗
(cid:6)

For all ˆs ∈ S, we denote by Fˆs(s, a) the set of reach-
ST (s(cid:4))}. The
able sets {k ∈ F(s, a)| ˆs = argmaxs(cid:3)∈k V ∗
proof of ∀s ∈ S, V ∗
IP (s) proceeds by contra-
diction as follows. For all s ∈ S and all a ∈ A(s), let
P (·|s, a) be the probability measure chosen by the operator
max in V ∗
IP (s). There-
fore, there is a s ∈ S such that
k∈Fs(s,a) m(k|s, a) >
P (s|s, a); as P (·|s, a) is a probability measure, there is also a
s ∈ S s.t.
IP (s) >
V ∗
IP (s). Now, let P (·|s, a) be a probability measure deﬁned
by: P (s(cid:4)|s, a) = P (s(cid:4)|s, a) ∀s(cid:4) ∈ S \ {s, s}, P (s|s, a) =
P (s|s, a) +  and P (s|s, a) = P (s|s, a) − , for  > 0. Note
IP , a con-
that
tradiction by the deﬁnition of P (·|s, a). Thus, the rest of this
proof shows that P (s(cid:4)|s, a) satisﬁes Proposition 1.

k∈Fs(s,a) m(k|s, a) < P (s|s, a) and V ∗

s(cid:3)∈S P (s(cid:4)|s, a)V ∗

s(cid:3)∈S P (s(cid:4)|s, a)V ∗

IP >

(cid:6)

(cid:6)

IJCAI-07

2026

Due to the deﬁnition of P (·|s, a), we have that the left side
and the right side of (3) are trivially satisﬁed, respectively, by
P (s|s, a) and P (s|s, a). To treat the other case for both s and
s, it is sufﬁcient to deﬁne  as follows:

X

X

 = min{

m(k|s, a)−P (s|s, a), P (s|s, a)−

m(k|s, a)}.

k∈F

s

(s,a)

k∈Fs(s,a)

Using this deﬁnition, we have that  > 0 by hypothesis;
since (4) gives a lower and an upper bound to the sum of
P (·|s, a) over, respectively, k ∈ F(s, a) and D(k, s, a) ⊆ k.
If {s, s} ∈ D(k, s, a) or {s, s} (cid:5)∈ D(k, s, a), then noth-
ing changes and these bounds remain valid. There is one
more case for each bound that its satisfaction is trivial too:
(i) for the upper bound when s (cid:5)∈ D(k, s, a); and (ii) for
the lower bound when s (cid:5)∈ k. A nontrivial case is for
the lower bound in (4) when s (cid:5)∈ k and s ∈ k. This
bound still holds because m({s}|s, a) ≤ P (s|s, a) (by hy-
(cid:6)
s(cid:3)∈k m({s(cid:4)}|s, a) +
pothesis) and
(cid:6)
s(cid:3)∈k\{s} m({s(cid:4)}|s, a) + δ + P (s|s, a) ≤
δ ≤
s(cid:3)∈k\{s} P (s(cid:4)|s, a) + P (s|s, a), δ ≥ 0 (using Proposition

s(cid:3)∈k P (s(cid:4)|s, a) =

(cid:6)

(cid:6)

1 for P (·|s, a)).

The last remaining case (to prove that Equation (4) is
true for P (·|s, a)) happens when s ∈ D(k, s, a) and s (cid:5)∈
D(k, s, a) for the upper bound in (4). This case is valid
because there is no k(cid:4) ∈ F(s, a) such that k(cid:4)
(cid:5)= k and
(cid:6)
s ∈ k(cid:4)
by the deﬁnition of reachable set, thus P (s|s, a) ≤
k∈Fs(s,a) m(k|s, a) = m(k|s, a) by hypothesis. If there
is not a s(cid:4) ∈ k \ {s} s.t. P (s(cid:4)|s, a) > 0 this up-
per bound still holds, else, choosing s(cid:4)
as s will validate
Since P (·|s, a) respects Proposition 1,
all
(cid:6)
s(cid:3)∈S P (s(cid:4)|s, a)V (s(cid:4)) >
we get a contradiction because
s(cid:3)∈S P (s(cid:4)|s, a)V (s(cid:4)) but by hypothesis P (·|s, a) =
Therefore,
(cid:12)(cid:13)

argmaxP (·|s,a)∈Ks(a)
ST (s) ≤ V ∗
∀s ∈ S, V ∗

IP (s), what completes the proof.

s(cid:3)∈S P (s(cid:4)|s, a)V ∗(s).

the bounds.

(cid:6)

(cid:6)

An immediate consequence of Theorem 2 is a decrease
of the worst case complexity order of MDPIPs algorithms
used for solving MDPSTs. Consider ﬁrst one iteration of the
Bellman principle of optimality for each s ∈ S (one round)
using Equations (2). Deﬁne an upper bound of |F(s, a)|
for all s ∈ S and a ∈ A(s) of an MDPST instance by
F = maxs∈S{maxa∈A(s) |F(s, a)|} ≤ 2|S|
. In the MDPIP
obtained through Proposition 1, computation of V ∗(s) con-
sists of solving a linear program induced by the max operator
on (2). Because this linear program has |S| variables and
its description is proportional to F, the worst case complex-
ity of one round is O(|A||S|p+1
), for p ≥ 2 and q ≥ 1.
The value of p and q is related to the algorithm used to solve
this linear program (for instance, using the interior point al-
gorithm [Kojima et al., 1988] leads to p = 6 and q = 1, and
the Karmarkar’s algorithm [Karmarkar, 1984] leads to p to
3.5 and q to 3).

F

q

However, the worst case complexity for one round using
Equation (5) is O(|S|2|A|F). This is true because the prob-
ability measure that maximizes the right side of Equation
(2) is represented by the choice maxs(cid:3)∈k V (s(cid:4)) in Equation
(5), avoiding the cost of a linear program.
In the special

case of an MDP modelled as an MDPST, i.e. ∀s ∈ S, a ∈
A(s), |F(s, a)| ≤ |S| and ∀k ∈ F(s, a), |k| = 1, this worst
case complexity is O(|S|2|A|), the same for one round using
the Bellman principle for MDPs [Papadimitriou, 1994].

6 Algorithms for MDPSTs

Due to Proposition 1, every algorithm that ﬁnds the optimal
policy for MDPIPs can be directly applied to MDPSTs. In-
stances of algorithms for MDPIP are: value iteration, pol-
icy iteration [Satia and Lave Jr, 1973], modiﬁed policy iter-
ation [White III and Eldeib, 1994], and the algorithm to ﬁnd
all optimal policies presented in [Harmanec, 1999]. How-
ever, a better approach is to use Theorem 2. This proposi-
tion gives a clear path on how to adapt algorithms from the
realm of MDPs — algorithms such as (L)RTDP [Bonet and
Geffner, 2003] and LDFS [Bonet and Geffner, 2006]. These
algorithms are deﬁned for Stochastic Shortest Path problems
(SSPs) [Bertsekas, 1995] (SSPs are a special case of MDPs,
in which there is only one initial state (M2) and the set of
goal states is nonempty (M3)). To ﬁnd an optimal policy, an
additional assumption is required: the goal must be reachable
from every state with nonzero probability (the reachability as-
sumption). For MDPSTs, this assumption can be generalized
by requirement that the goal be reachable from every state
with nonzero probability for all probability measures in the
model. The following proposition gives a sufﬁcient, however
not necessary, condition to prove the reachability assumption
for MDPSTs.
Proposition 3 If, for all s ∈ S, there exists a ∈ A(s) such
that, for all k ∈ F(s, a) and all s(cid:4) ∈ k, P (s(cid:4)|s, a) > 0, then it
is sufﬁcient to prove that the reachability assumption is valid
using at least one probability measure for each s ∈ S and
a ∈ A.

Proof If the reachability assumption is true for a speciﬁc se-
quence of probability measures P = (cid:9)P 1, P 2, . . . .P n(cid:10), then
there exists a policy π and a history h, i.e. sequence of vis-
ited states and executed actions, induced by π and P such
(cid:7)i≤n
that h = (cid:9)s0 ∈ S0, π(s0), s1, . . . , π(sn−1), sn ∈ SG(cid:10) is min-
i=1 P i(si|si−1, π(si−1)) > 0.
imum and ∀s ∈ S, P0(s0)
Since si+1
can always be reached, because there exists an ac-
tion a ∈ A(si) such that P (si+1|si, a) > 0, then, for any
sequence of probability measures in the model, every history
h(cid:4)
(cid:12)(cid:13)

induced by π contains h, i.e., reaches sn ∈ SG.

Example 2 Consider the planning problem in the Example
1 and the cost of actions in Figure 1. We have obtained the
following optimal policy for this MDPST:

π∗ =

s0
d1

s1
s2
d2 HT

s3

s4

s5

s6

Noop Noop Noop Noop

7 Conclusion

In this paper we have examined approaches to planning across
many dimensions: determinism, nondeterminism, risk, un-
certainty. We would like to suggest that Markov decision
processes with set-valued transitions represent a remarkable

IJCAI-07

2027

entry in this space of problems. MDPSTs are quite general,
as they not only capture the main existing planning models
of practical interest, but also they can represent mixtures of
these models — we have emphasized throughout the paper
that MDPSTs allow one to combine nondeterminism of ac-
tions with probabilistic effects. It is particularly important to
note that MDPSTs specialize rather smoothly to DET, NON-
DET or MDP; if an MDPST belongs to one of these cases, its
solution inherits the complexity of the special case at hand.
Such a “smooth” transition to special cases does not obtain if
one takes the larger class of MDPIPs; the general algorithms
require one to perform bilevel programming (linear programs
are nested, as one linear program is needed to compute the
value) and do not treat efﬁciently the special cases.

In fact, MDPSTs are remarkable not only because they are
rather general, but because they are not overly general — they
are sufﬁciently constrained that they display excellent compu-
tational properties. Consider the computation of an iteration
of the Bellman equation for a state s (a round). This is an es-
sential step both in versions of value and policy iteration and
in more sophisticated algorithms such as (suitably adapted)
RTDP. As discussed in Section 6, rounds in MDPSTs have
much lower complexity than rounds in general MDPIPs —
in essence, the simpliﬁcation is the replacement of a linear
program by a fractional knapsack problem.

Finally, we would like to emphasize that MDPSTs inherit
the pleasant conceptual aspects of MDPIPs. They are based
on solid decision theoretic principles that attempt to repre-
sent, as realistically as possible, risk and Knightian uncer-
tainty. We feel that we have only scratched the surface of this
space of problems; much remains to be done both on theoret-
ical and practical fronts.

Acknowledgements

We thank FAPESP (grant 04/09568-0) and CNPq (grants
131403/05-2, 302868/04-6, 308530/03-9) for ﬁnancial sup-
port and the four anonymous reviewers for the suggestions
and comments.

References
[Anrig et al., 1999] B. Anrig, R. Bissig, R. Haenni, J. Kohlas, and
N. Lehmann. Probabilistic argumentation systems: Introduction
to assumption-based modeling with ABEL. Technical Report 99-
1, Institute of Informatics, University of Fribourg, 1999.

[Bellman, 1957] R. E. Bellman. Dynamic Programming. Princeton

University Press, Princeton, New Jersey, 1957.

[Berger, 1985] J.O. Berger.

Statistical Decision Theory and

Bayesian Analysis. Springer-Verlag, 1985.

[Bertsekas, 1995] D.P. Bertsekas. Dynamic programming and opti-

mal control. Athena Scientiﬁc Belmont, Mass, 1995.
[Bonet and Geffner, 2003] B. Bonet and H. Geffner.

Labeled
RTDP: Improving the convergence of real-time dynamic pro-
gramming.
In Proc. of the 13th ICAPS, pages 12–21, Trento,
Italy, 2003. AAAI Press.

[Bonet and Geffner, 2006] B. Bonet and H. Geffner.

Learning
Depth-First Search: A uniﬁed approach to heuristic search in de-
terministic and non-deterministic settings, and its application to
MDPs. In Proc. of the 16th ICAPS, 2006.

[Buffet and Aberdeen, 2005] O. Buffet and D. Aberdeen. Robust
planning with (L)RTDP. In Proc. of the 19th IJCAI, pages 1214–
1219, 2005.

[Cozman, 2005] F.G. Cozman. Graphical models for imprecise
probabilities. International Journal of Approximate Reasoning,
39(2-3):167–184, 2005.

[Eiter and Lukasiewicz, 2003] T. Eiter and T. Lukasiewicz. Proba-
bilistic reasoning about actions in nonmonotonic causal theories.
In Proc. of the 19th UAI, pages 192–199, 2003.

[Fagiuoli and Zaffalon, 1998] E. Fagiuoli and M. Zaffalon. 2U: An
exact interval propagation algorithm for polytrees with binary
variables. Artiﬁcial Intelligence, 106(1):77–107, 1998.

[Givan et al., 2000] R. Givan, S. M. Leach, and T. Dean. Bounded-
parameter Markov decision processes. Artiﬁcial Intelligence,
122(1-2):71–109, 2000.

[Harmanec, 1999] D. Harmanec. A generalization of the concept of
Markov decision process to imprecise probabilities. In ISIPTA,
pages 175–182, 1999.

[Kadane et al., 1999] J.B. Kadane, M.J. Schervish, and T. Seiden-
feld. Rethinking the Foundations of Statistics. Cambridge Uni-
versity Press, 1999.

[Karmarkar, 1984] N. Karmarkar. A new polynomial-time algo-
rithm for linear programming. In Procs of the 16th annual ACM
symposium on Theory of computing, pages 302–311. ACM Press
New York, NY, USA, 1984.

[Knight, 1921] F.H. Knight. Risk, Uncertainty, and Proﬁt. Hart,

Schaffner & Marx; Houghton Mifﬂin Company, Boston, 1921.

[Kojima et al., 1988] M. Kojima, S. Mizuno, and A. Yoshise. A
primal-dual interior point algorithm for linear programming. In
Progress in Mathematical Programming: Interior-point and re-
lated methods, pages 29–47. Springer-Verlag, 1988.

[Levi, 1980] I. Levi. The Enterprise of Knowledge. MIT Press,

1980.

[Luce and Raiffa, 1957] D. Luce and H. Raiffa. Games and Deci-

sions. Dover edition, Mineola, 1957.

[Nilsson, 1986] N.J. Nilsson. Probabilistic logic. Artiﬁcial Intelli-

gence, 28:71–87, 1986.

[Papadimitriou, 1994] Christos H. Papadimitriou. Computational

Complexity. Addison-Wesley, 1994.

[Perny et al., 2005] P. Perny, O. Spanjaard, and P. Weng. Algebraic
In Proc. of the 19th IJCAI, pages

Markov decision processes.
1372–1377, 2005.

[Satia and Lave Jr, 1973] J. K. Satia and R. E. Lave Jr. Markovian
decision processes with uncertain transition probabilities. Oper-
ations Research, 21(3):728–740, 1973.

[Shafer, 1976] G. Shafer. A Mathematical Theory of Evidence.

Princeton University Press, 1976.

[Trevizan et al., 2006] F. W. Trevizan, F. G. Cozman, and L. N.
de Barros. Unifying nondeterministic and probabilistic planning
through imprecise markov decision processes.
In Proc. of the
10th IBERAMIA/18th SBIA, pages 502–511, 2006.

[Troffaes, 2004] M. Troffaes. Decisions making with imprecise
probabilities: a short review. The SIPTA Newsletter, 2(1):4–7,
2004.

[Walley, 1991] P. Walley.

Statistical Reasoning with Imprecise

Probabilities. Chapman and Hall, London, 1991.

[White III and Eldeib, 1994] C. C. White III and H. K. Eldeib.
Markov decision processes with imprecise transition probabili-
ties. Operations Research, 42(4):739–749, 1994.

IJCAI-07

2028


Deictic Option Schemas

Balaraman Ravindran

Dept. of Computer Science and Engineering

Andrew G. Barto
Dept. of Computer Science

Vimal Mathew

Dept. of Computer Science and Engineering

IIT Madras, India

ravi@cse.iitm.ac.in

University of Massachusetts, Amherst

barto@cs.umass.edu

IIT Madras, India

vml@cse.iitm.ac.in

Abstract

a

is

representation

that allows an agent

Deictic
representational
paradigm, based on selective attention and point-
to learn and reason
ers,
about rich complex environments.
In this article
we present a hierarchical reinforcement learning
framework that employs aspects of deictic repre-
sentation. We also present a Bayesian algorithm
for learning the correct representation for a given
sub-problem and empirically validate it on a
complex game environment.

1 Introduction
Agre [1987; 1988] introduced deictic representations to the
AI community as a way of reasoning and learning in large
complex domains. It is a representational paradigm based on
using a small set of pointers that lets the agent focus on parts
of the state space relevant to solving the problem at hand.

Deictic pointers might be simple physical locators or they
might encode complex semantics. Agre and Chapman [1987]
employ pointers that let the agent precisely locate important
components of the system, but their method needs substan-
tial pre-processing and domain knowledge. While solving
the arcade game Pengo, their agent Pengi employs complex
pointers such as bee-attacking-me, ice-cube-next-to-me, etc.
The actions of the agent are then deﬁned with respect to these
pointers, for example, push ice-cube-next-to-me toward bee-
attacking-me. In general, deictic representations can be used
in rich environments with incredible amounts of detail. They
are also useful in systems where there are physical limita-
tions on the sensory capabilities so that some form of atten-
tional mechanism has to be employed [Minut and Mahade-
van, 2001].

There has been considerable interest in employing deictic
representations in a reinforcement learning (RL) framework
[Whitehead and Ballard, 1991; McCallum, 1995; Finney et
al., 2002]. The attractions of such a synthesis are obvious and
can lead to a trial-and-error approach that can work in large
environments. In this article we develop a hierarchical deictic
RL framework based on relativized options [Ravindran and
Barto, 2003a]. Speciﬁcally we extend our earlier approach to
factored MDPs and show that certain aspects of deixis can be
modeled as ﬁnding structured homomorphic reductions of the

problem. We present a Bayesian algorithm for learning the
correct conﬁguration of pointers and validate our approach
on a simulated game domain inspired by Agre and Chapman
[1987].

We employ Markov Decision Processes (MDPs) as our
basic modeling paradigm. First we present some notation
regarding factored MDPs (Section 2), a brief summary of
relativized options (Section 3) and introduce deictic option
schemas( Section 4). Then we present our Bayesian algo-
rithm to choose the right pointer conﬁguration to apply to a
sub-task (Section 5). We relate our approach to an existing
family of deictic algorithms in Section 6. In Section 7 we de-
scribe some related work, and we conclude in Section 8 by
discussing some future directions for research.

2 Notation
A structured (ﬁnite) MDP is described by the tuple
(cid:2)S, A, Ψ, P, R(cid:3), where S is a ﬁnite set of states, A is a ﬁnite
set of actions, Ψ ⊆ S × A is the set of admissible state-action
pairs, P : Ψ×S → [0, 1] is the transition probability function
(cid:2)) being the probability of transition from state
with P (s, a, s
under action a, and R : Ψ → IR is the expected
s to state s
reward function, with R(s, a) being the expected reward for
performing action a in state s. The state set S is given by
M features or variables, S ⊆
i=1 Si, where Si is the set
of permissible values for feature i. Thus any s ∈ S is of the
form s = (cid:2)s1, . . . , sM (cid:3), where si ∈ Si for all i.

(cid:2)M

(cid:2)

(cid:2)
M

(cid:2)
1, . . ., s

The transition probability function P is usually described
by a family of two-slice temporal Bayesian networks (2-
TBNs), with one TBN for each action. A 2-TBN is a two
layer directed acyclic graph whose nodes are {s1, . . . , sM}
}. Here si denotes the random variable repre-
and {s
(cid:2)
senting feature i at the present state and s
i denotes the random
variable representing feature i in the resulting state. Many
classes of structured problems may be modeled by a 2-TBN
in which each arc is restricted to go from a node in the ﬁrst
set to a node in the second. The state-transition probabilities
can be factored as:

M(cid:3)

P (s, a, s

(cid:2)) =

Prob(s

(cid:2)
i|Parents(s

(cid:2)
i, a)),

i=1

(cid:2)
the parents of node s
where Parents(s
i
in the 2-TBN corresponding to action a and each

(cid:2)
i, a) denotes

IJCAI-07

1023

(cid:2)
i|Parents(s

(cid:2)
i, a)) is given by a conditional probability
Prob(s
(cid:2)
i. In computing the condi-
table (CPT) associated with node s
tional probabilities it is implicitly assumed that the nodes in
Parents(s

(cid:2)
i,a) are assigned values according to s.

An option (or a temporally extended action) [Sutton et al.,
1999] in an MDP M = (cid:2)S, A, Ψ, P , R(cid:3) is deﬁned by the
tuple O = (cid:2)I, π, β(cid:3), where the initiation set I ⊆ S is the
set of states in which the option can be invoked, π is the op-
tion policy, and the termination function β : S → [0, 1] gives
the probability of the option terminating in any given state.
The option policy can in general be a mapping from arbitrary
sequences of state-action pairs (or histories) to action proba-
bilities. We restrict attention to Markov sub-goal options in
which the policies are functions of the current state alone,
and an option terminates on reaching a set of pre-deﬁned
(sub)goal states. The states over which the option policy is
deﬁned is known as the domain of the option. In such cases
the option policy can be deﬁned in a sub-MDP, known as the
option MDP, consisting of the states in the domain of the op-
tion.

3 Relativized Options

such that a solution to M(cid:2)

A relativized option combines MDP homomorphisms [Ravin-
dran and Barto, 2003b] with the options framework to com-
pactly represent a family of related options. An MDP ho-
momorphism is a transformation from an MDP M to a re-
duced model M(cid:2)
yields a solu-
tion to M. Notationally, an MDP homomorphism, h, is de-
ﬁned as a surjection from Ψ to Ψ(cid:2)
, given by a tuple of surjec-
tions (cid:2)f, {gs|s ∈ S}(cid:3), with h((s, a)) = (f (s), gs(a)), where
f : S → S
is called
the homomorphic image of M under h. An optimal policy
in M(cid:2)

when lifted to M yields an optimal policy in M.

(cid:2)
f (s) for s ∈ S. M(cid:2)
and gs : As → A

(cid:2)

In a relativized option, the policy for achieving the op-
tion’s sub-goal is deﬁned in the image of a partial homo-
morphism deﬁned only over a subset of S. This image
is called the option MDP. When the option is invoked, the
current state is projected onto the option MDP, MO =
(cid:2)SO, AO, ΨO, PO, RO(cid:3), and the policy action is lifted to the
original MDP based on the states in which the option is in-
voked. A relativized option is deﬁned as follows:
Deﬁnition: A relativized option of an MDP M =
(cid:2)S, A, Ψ, P, R(cid:3) is the tuple O = (cid:2)h, MO, I, β(cid:3), where I ⊆
S is the initiation set, β : SO → [0, 1] is the termination
function and h = (cid:2)f, {gs|s ∈ S}(cid:3) is a partial homomorphism
(cid:2)(cid:3) to the option MDP MO with
from the MDP (cid:2)S, A, Ψ, P, R
R
In other words, the option MDP MO is a partial homomor-
phic image of an MDP with the same states, actions and tran-
sition dynamics as M but with a reward function chosen
based on the option’s sub-task. The homomorphism condi-
tions hold only for states in the domain of the option O. The
option policy π : ΨO → [0, 1] is obtained by solving MO
by treating it as an episodic task. When lifted to M, π is
suitably transformed into policy fragments over Ψ.

chosen based on the sub-task.

(cid:2)

A relativized option can be viewed as an option schema
where a template of an option policy is speciﬁed using a
parameterized representation of a family of sub-problems.

When the option is invoked, a particular instantiation of the
schema is chosen by binding to the appropriate resources.
Given a set of possible bindings, [Ravindran and Barto,
2003a] presented a Bayesian approach to choosing the right
binding to apply in a given context based on experience gath-
ered in solving the task. It assumed that the set of bindings
were given by a family of transformations, H, applied to Ψ
and maintained a heuristic weight vector, wn(h, ψ(s)), which
is a measure of the likelihood of transformation h ∈ H being
the right transformation in the context represented by ψ(s).1
The weight vectors are updated using:

wn(h, ψ(s)) = PO((f (s), gs(a), f (s
K

(cid:2)))wn−1(h, ψ(s))

(1)

(cid:2)(s), g

(cid:2)
s(a), f

(cid:2)

(cid:2)(s

(cid:2)))wn−1(h

h(cid:2)∈H PO((f

where PO(s, a, s
(cid:4)

(cid:2)) = max (ν, PO(s, a, s

(cid:2))), and K =
, ψ(s)) is a nor-
malizing factor. Since the projected transition probability is
lower bounded by ν, this approach works even when the ho-
momorphic image is not exact.
In this article we extend
our earlier work to cases where the bindings are speciﬁed by
deictic pointers.

4 Deictic Option Schemas
The set of candidate transformations, H, can be speciﬁed by
a set of deictic pointers together with their possible conﬁg-
urations. The agent learns to place the pointers in speciﬁc
conﬁgurations to effect the correct bindings to the schema.
We call such option schema together with the set of pointers
a deictic option schema. Formally a deictic option schema is
deﬁned as follows:

Deﬁnition: A deictic option schema of a factored MDP
M = (cid:2)S, A, Ψ, P, R(cid:3) is the tuple (cid:2)D, O(cid:3), where O =
(cid:2)h, MO, I, β(cid:3),
is a relativized option in M, and D =
{D1, D2, · · · , DK} is the set of admissible conﬁgurations of
the deictic pointers, with K being the number of deictic point-
ers available. For all i, Di ⊆ 2{1,···,M}
is the collection of all
possible subsets of indices of the features that pointer i can
project onto the schema, where M is the number of features
used to describe S.

The set Di indicates the set of objects that pointer i can
point to in the environment. For example, in a blocks world
domain this is the set of all blocks. In our example in the next
section, it is the set of all possible adversaries.

Once a deictic option schema is invoked the decision mak-
ing proceeds in two alternating phases. In the ﬁrst phase the
agent picks a suitable pointer conﬁguration, using a heuris-
tic weight function similar to the one given in Section 3. In
the second phase, the agent picks an action, based on the per-
ceptual information obtained from the pointers, using a Q-
function. This calling semantics is similar to that followed by
Whitehead and Ballard [1991].
(cid:2)K

Each member of H has a state transformation of the form
i=1 ρJi , where Ji ∈ Di for all i and ρJ , is the projec-
tion of S onto the subset of features indexed by J . If h is

1Frequently, ψ(s) is some simple function of the state space, like

a projection onto a few features.

IJCAI-07

1024

known a priori then the pointer conﬁgurations can be chosen
appropriately while learning. In the absence of prior knowl-
edge the Bayesian algorithm developed in [Ravindran and
Barto, 2003a] can be used to determine the correct bindings to
the schema from among the possible pointer conﬁgurations.
But, the algorithm is not entirely suitable for deictic option
schemas for the following reason.

1
n(·, ·), w

The algorithm assumes that the candidate transformations
are not structured and maintains a monolithic weight vector,
wn(·, ·). In the case of deictic option schemas the transfor-
mations are structured and it is advantageous to maintain a
2
“factored” weight vector, wn(·, ·) = (cid:2)w
n(·, ·), · · ·(cid:3).
Ideally each component of the weight vector should be the
likelihood of the corresponding pointer being in the right con-
ﬁguration. But usually there is a certain degree of dependence
among the pointers and the correct conﬁguration of some
pointers might depend on the conﬁguration of other pointers.
Therefore, three cases need to be considered. Assume that
there are only two pointers, i and j, for the following dis-
cussion, but the concepts generalize to arbitrary number of
pointers.

1. Independent pointers: For every J ∈ Di, ρJ satisﬁes
the homomorphism condition on transition probabilities.
Then, the right assignment for pointer i is independent
of the other pointers and there is one component of the
weight vector corresponding to pointer i and the updates
for this components depends only on the features in-
dexed by some J ∈ Di.

2. Mutually dependent pointers: For each J ∈ Di and
(cid:2) ∈ Dj, ρJ × ρJ (cid:2) satisﬁes the homomorphism condi-
J
tions. But ρJ and ρJ (cid:2) do not satisfy the homomorphism
(cid:2) ∈ Dj. Thus, they
conditions for some J ∈ Di and J
cannot be treated separately and the composite projec-
tions given by their cross-products has to be considered.
There is one component of the weight vector that corre-
sponds to this cross-product projection. The update for
this component will depend on the features indexed by
some J ∈ Di and J

(cid:2) ∈ Dj.

3. Dependent pointer: For each J ∈ Di and J

(cid:2) ∈ Dj, ρJ ×
ρJ (cid:2) satisﬁes the homomorphism conditions, as does ρJ .
But ρJ (cid:2) does not satisfy the homomorphism conditions
(cid:2) ∈ Dj. This means pointer
for at least some value of J
i is an independent pointer, while j is a dependent one.
There is a separate component of the weight vector that
corresponds to pointer j, but whose update depends on
the features indexed by some J ∈ Di and J

(cid:2) ∈ Dj.

The weight vector is chosen such that there is one com-
ponent for each independent pointer, one for each dependent
pointer and one for each set of mutually dependent pointers.
Let the resulting number of components be L. A modiﬁed
version of the update rule in [Ravindran and Barto, 2003a] is
used to update each component l of the weight independently
of the updates for the other components:

l
n(h

w

i

, ψ(s)) = P l

O((f i(s), gi

s(a), f i(s
K

(cid:2))) · wl

n−1(hi, ψ(s))

(2)

Figure 1: A game domain with interacting adversaries and
stochastic actions. The task is to collect the black diamond.
The adversaries are of three types—benign (shaded), retriever
(white) and delayers (black). See text for more explanation.

(cid:5)
ν, P l

(cid:6)
(cid:2))

(cid:4)

(cid:2)) = max

O(s, a, s

O(s, a, s

, ψ(s) is again
where P l
a function of s that captures the features of the states
necessary to distinguish the particular sub-problem under
(cid:2)))
consideration, and K =
(cid:2)) is
wn−1(h
(cid:2)) computed as follows. Let
a “projection” of PO(s, a, s
J be the set of features that
is required in the com-
putation of wl
This is determined as de-
(cid:2)
(cid:2)) =
scribed above for the various cases. Then P l

(cid:2)i(s), g
(cid:2)i
s (a), f
(cid:2)i, ψ(s)) is the normalizing factor. P l
O(s, a, s

n(hi, ψ(s)).

h(cid:2)i∈H P l

O(f

O(s, a, s

(cid:2)i(s

Prob(s

(cid:2)
j |Parents(s

(cid:2)
j, a)).

j∈J

5 Experimental Illustration in a Game

Environment

We now apply a deictic option schema to learning in a mod-
iﬁed version of the game environment introduced in [Ravin-
dran and Barto, 2003a]. The layout of the game is shown in
Figure 1. The environment has the usual stochastic gridworld
dynamics. There is just one room in the world and the goal
of the agent is to collect the diamond in the room and exit it.
The agent collects a diamond by occupying the same square
as the diamond. A Boolean variable have indicates posses-
sion of the diamond.

The room also has 8 autonomous adversaries. The adver-
saries may be of three types—benign, delayer or retriever. If
the agent happens to occupy the same square as the delayer it
is captured and is prevented from moving for a random num-
ber of time steps determined by a geometric distribution with
parameter hold. When not occupying the same square, the
delayer pursues the agent with probability chase. The benign
robots execute random walks in the room and act as mobile
obstacles. The retriever behaves like the benign adversary till
the agent picks up the diamond. Once the agent picks up the
diamond, the retriever’s behavior switches to that of the de-
layer. The main difference is that once the retriever occupies
the same square as the agent, the diamond is returned to the
original position and the retriever reverts to benign behavior.
The retriever returns to benign behavior if the agent is also
“captured” by the delayer. None of the adversaries leave the
room, and thus the agent can “escape” from the room by exit-
ing to the corridor. The agent is not aware of the types of the

IJCAI-07

1025

Figure 2: The option MDP corresponding to the sub-task get-
object-and-leave-room for the domain in Figure 1. There is
just one delayer and one retriever in this image MDP.

individual adversaries.

The option MDP (Figure 2) is a symmetrical room with just
two adversaries—a delayer and a retriever with ﬁxed chase
and hold parameters. The features describing the state space
of the option MDP consists of the x and y coordinates relative
to the room of the agent and of the adversaries and a boolean
variable indicating possession of the diamond. The room in
the world does not match the option MDP exactly and no ad-
versary in the world has the same chase and hold parameters
as the adversaries here.

The deictic agent has access to 2 pointers: a delayer pointer
that projects one of the adversaries onto the delayer in the im-
age MDP and a retriever pointer that projects one of the ad-
versaries onto the retriever in the image MDP. The delayer
pointer is an independent pointer and the retriever pointer
is dependent on the delayer pointer. The sets Ddelayer and
Dretriever are given by the 8 pairs of features describing the
adversary coordinates.

In addition to the pointers the agent also has access to some
background information, such as its own location (which can
be formalized as a self pointer) and whether it has the dia-
mond or not. Note that since the option MDP is an approxi-
mate homomorphic image, the homomorphism conditions are
not strictly met by any of the projections. Therefore, in com-
puting the weight updates, the inﬂuence of the features not
used in the construction of the image MDP are ignored by
marginalizing over them.

5.1 Experimental Results

The performance of the deictic agent is compared with a rela-
tivized agent (monolithic agent) that employs the same option
MDP but chooses from a set H of 64 monolithic transforma-
tions, formed by the cross product of the 8 conﬁgurations of
the deictic pointers. Both agents employ hierarchical SMDP
Q-learning, with the learning rates for the option and the main
task set to 0.1. The agents are both trained initially in the
option MDP to acquire an approximate initial option policy
that achieves the goal some percentage of the trials, but is not
optimal. Both agents use  greedy exploration. The results
reported are averaged over 10 independent runs.

On learning trials both agents perform similarly (Figure 3),
but the monolithic agent has a marginally better initial perfor-
mance. To understand this we look at the rates at which the
transformation weights converge (Figures 4, 5, and 6 are for

Figure 3: Average number of steps per episode taken by both
agents for solving the task shown in Figure 1.

Transformation 21

 1

 0.8

 0.6

 0.4

 0.2

s
t
h
g
i
e
w
 
n
o
i
t
a
m
r
o
f
s
n
a
r
T

 

6
1
n
o
i
t
a
m
r
o
f
s
n
a
r
T

2
2
 
n
o
i
t
a
m
r
o
f
s
n
a
r
T

 0

 0

 500

 1000
Steps

 1500

 2000

Figure 4: Typical evolution of a subset of weights of the
monolithic agent on the task shown in Figure 1.

a single typical run). Figure 5 shows that typically the deic-
tic agent identiﬁes the delayer quite rapidly. In fact it takes
only an average of 52 update steps to identify the delayer,
over the 10 independent runs. The monolithic agent takes
much longer to identify the right transformation (number 21
in our encoding), as seen from Figure 4. On an average it
takes around 2007 update steps. As Figure 6 shows, identi-
fying the retriever is harder and the deictic agent takes about
3050 update steps on an average.

This result is not surprising, since the correct position
for the retriever depends on position of the delayer pointer.
Therefore, while the delayer is being learned, the weights for
the retriever receive inconsistent updates and it takes a while
for the weights to get back on track. For much of the time we
are interested in only identifying the delayer correctly and
hence the deictic agent seems to have lower variability in its
performance. Overall a single run with the composite agent
takes around 14 hours of CPU time on a Pentium IV, 3.4 GHz
machine, while a single run of the deictic agent takes around
4 hours. Further, the monolithic agent considers all possi-
ble combinations of pointer conﬁgurations simultaneously.
Therefore, while it takes fewer update steps to converge to
the right weights, the deictic agent makes far fewer number
of weight updates: 130,000 vs. 85,000 on an average.

IJCAI-07

1026

s
t
h
g
i
e

W

 
r
e
y
a
l
e
D

4

 
t
o
b
o
R

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

Robot 2

Robot 5

 20

 40

 60

 80

 100

Steps

Figure 5: Typical evolution of a subset of the delayer weights
of the deictic agent on the task shown in Figure 1.

Robot 5

Robot 0

1

 
t
o
b
o
R

s
t
h
g
i
e

W

 
r
e
v
e
i
r
t
e
R

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

phases: in the perceptual phase the agent looks around the
environment to ﬁnd a consistent representation of the under-
lying state. A consistent representation [Whitehead and Bal-
lard, 1991] of a state is one such that all states that map to
the representation have the same optimal action-value func-
tion. In the overt phase the agent picks an action to apply to
the environment based on the current sensory input. Learning
takes place in both phases. The Lion algorithm [Whitehead
and Ballard, 1991] is an example of a consistent representa-
tion algorithm. Here Q-learning is used in the overt phase and
a simple learning rule based on one step error information is
used to train the sensory phase. If the one step error in the
Q update rule for a particular conﬁguration is negative then
that representation is considered perceptually aliased and is
ignored in the future. This simple rule limits the applicability
of this algorithm to deterministic settings alone.

If the representation used is a homomorphic image then
it is a consistent representation, as mentioned in [Ravindran
and Barto, 2003b]. By restricting the deﬁnition of deictic op-
tion schema to employ partial homomorphic images as option
MDPs, it is guaranteed that a consistent representation is al-
ways employed. In the absence of knowledge of the option
homomorphism, ﬁnding the right transformation to employ
constitutes the search for a consistent representation and we
employ Bayesian learning in this phase. As with the Lion
algorithm, a form of Q-learning is used in the overt phase.

 200

 400

 600

 800

 1000

Steps

7 Related Work

Figure 6: Typical evolution of a subset of the retriever
weights on the task shown in Figure 1.

Comment

The algorithm used above updates the weights for all the
transformations after each transition in the world. This is
possible since the transformations are assumed to be math-
ematical operations and the agent could use different trans-
formations to project the same transition onto to the option
MDP. But deictic pointers are often implemented as physi-
cal sensors. In such cases, this is equivalent to sensing ev-
ery adversary in the world before making a move and then
sensing them after making the move, to gather the data re-
quired for the updates. Since the weights converge fairly
rapidly, compared to the convergence of the policy, the time
the agent spends “looking around” would be a fraction of the
total learning time.

6 Perceptual Aliasing and Consistent

Representations

The power of deixis arises from its ability to treat many per-
ceptually distinct states in the same fashion, but it is also the
chief difﬁculty in employing deictic representations. This
phenomenon is known as perceptual aliasing [Whitehead and
Ballard, 1991]. One approach to overcome perceptual alias-
ing is a class of methods known as Consistent Representation
Methods. These methods split the decision making into two

Deixis originates from the Greek word deiknynai which
means to show or to point out. It is employed by linguists
to denote the pointing function of certain words, like here
and that, whose meaning could change depending on the con-
text. Deixis was introduced to the AI community by Agre.
As mentioned earlier, [Agre and Chapman, 1987] used deic-
tic representations to design an agent, Pengi, that plays the
arcade game Pengo. Pengi was designed to play the game
from the view point of a human player and hence used visu-
als from a computer screen as input.

[Whitehead and Ballard, 1991] were the ﬁrst to use de-
ictic representations in a RL system, with their Lion algo-
rithm. Unfortunately, the method the Lion algorithm employs
to determine consistency works only in deterministic environ-
ments. [McCallum, 1995] takes a more direct approach to
overcoming perceptual aliasing. He employs deixis to solve
a car driving task and models the problem as a partially ob-
servable MDP. He uses a tree structure, known as U-trees,
for representing “states” and identiﬁes the necessary distinc-
tions so that the resulting representation is consistent. But
his approach is not divided into explicit perceptual and overt
phases. There has not been much work on using hierarchical
RL and deixis. The only work we are aware of is by [Minut
and Mahadevan, 2001]. They develop a selective attention
system that searches for a particular object in a room. It oper-
ates by identifying the most salient object in the agent’s visual
ﬁeld and shifting its visual ﬁeld to center and focus on that
object. They employ an option to identify the most salient
object in the current visual ﬁeld. Though they do not state it
thus, this is a “deictic” option, whose effect depends on the

IJCAI-07

1027

current visual ﬁeld.

A systematic study on using deictic representations with
RL was reported by [Finney et al., 2002]. They employ a
straightforward deictic representation with two pointers on a
blocks world task. They use the G-algorithm to represent past
information as a tree. They report that their approach does
not work well for a variety of reasons. First the tree grows
very large rapidly. The deictic commands are deﬁned with
respect to the two focus pointers. When long sequences of
actions are required with a small number of pointers, it is easy
to lose focus. While they try to address this by redesigning
the pointers, they do not have much success. One way to
alleviate this problem is by adopting a hierarchical approach
as we do in this work. If the number of pointers required by
each deictic level to maintain focus is not large, we can avoid
some of the problems encountered by [Finney et al., 2002].

8 Discussion and Future Work

While deixis is a powerful paradigm ideally suited for situa-
tions that are mainly reactive, it is difﬁcult to employ a purely
deictic system to solve complex tasks that require long-range
planning. Our hierarchical deictic framework allows us to
employ deictic representations in lower levels of the problem
to leverage their power and generalization capacities, while
at the higher levels we retain global context information in a
non-deictic fashion. Mixing such representations allows us to
exploit the best of both worlds and to solve tasks that require
maintaining long term focus. It is our belief that there is no
pure deictic system in nature. While it has been established
that humans employ deixis in a variety of settings, we cer-
tainly maintain some higher level context information. While
gathering ingredients for making tea, we might be using de-
ictic pointers for accessing various containers [Land et al.,
1998], but we also are continuously aware of the fact that we
are making tea.

The Bayesian approach we outlined requires that we have
a complete model of the option MDP, which we assumed was
available apriori. To learn this would require a lot more ex-
perience with the option MDP than we would need to learn
a reasonable policy. Currently we are experimenting with
learning a partial model of the option MDP, such that it is
sufﬁcient to identify the correct conﬁguration of the deictic
pointers.

The various approaches to learning with deictic pointers
[Whitehead and Ballard, 1991; Finney et al., 2002] usually
employ simple physical locators.[Agre, 1988] uses complex
pointers, but hand codes the policy for maintaining the focus
of these pointers. For example, a set of rules are used to deter-
mine which is the bee-attacking-me and the pointer is moved
suitably. Our approach falls somewhere in between. We start
by deﬁning a set of simple pointers. As learning progresses
the agent learns to assign these pointers consistently such that
some of them take on complex roles. We can then assign se-
mantic labels to these pointers such as robot-chasing-me.

It should be noted that a homomorphic image implies a
consistent representation but the reverse is not true. The
notion of a consistent representation is a more general con-
cept than a homomorphic image and corresponds to optimal-

action value equivalence discussed in [Givan et al., 2003]. By
restricting ourselves to homomorphic images we are limiting
the class of deictic pointers that we can model. Further work
is needed to extend our framework to include richer classes
of deictic pointers and to employ memory based methods.
Nevertheless in this article we have taken the ﬁrst steps in
accommodating deictic representations in a hierarchical deci-
sion theoretic framework.

References
[Agre and Chapman, 1987] Philip E. Agre and David Chap-
man. Pengi: An implementation of a theory of activity. In
Proceedings of AAAI-87, 1987.

[Agre, 1988] Philip E. Agre. The dynamic structure of ev-
eryday life. Technical Report AITR-1085, Massachusetts
Institute of Technology, 1988.

[Finney et al., 2002] S. Finney, N. H. Gardiol, L. K. Kael-
bling, and T. Oates. That thing we tried didn’t work very
well: Deictic representation in reinforcement learning. In
Proceedings of the 18th UAI, 2002.

[Givan et al., 2003] R. Givan, T. Dean, and M. Greig. Equiv-
alence notions and model minimization in Markov deci-
sion processes. Artiﬁcial Intelligence, 147(1–2):163–223,
2003.

[Land et al., 1998] Michael F. Land, N. Mennie,

and
J. Rusted. Eye movements and the roles of vision in ac-
tivities of daily living: making a cup of tea. Investigative
Ophthalmology and Visual Science, 39(S457), 1998.

[McCallum, 1995] A. McCallum. Reinforcement Learning
with Selective Perception and Hidden State. PhD thesis,
Computer Science Department, University of Rochester,
1995.

[Minut and Mahadevan, 2001] Silviu Minut and Sridhar Ma-
hadevan. A reinforcement learning model of selective vi-
sual attention.
In Proceedings of the Fifth International
Conference on Autonomous Agents, Montreal, 2001.

[Ravindran and Barto, 2003a] Balaraman Ravindran

and
Andrew G. Barto. Relativized options: Choosing the right
transformation.
In Proceedings of ICML 2003), pages
608–615, August 2003. AAAI Press.

[Ravindran and Barto, 2003b] Balaraman Ravindran and
Andrew G. Barto. SMDP homomorphisms: An algebraic
approach to abstraction in semi-Markov decision pro-
cesses.
In Proceedings of the Eighteenth IJCAI, pages
1011–1016, August 2003. AAAI Press.

[Sutton et al., 1999] Richard S. Sutton, Doina Precup, and
Satinder Singh. Between MDPs and Semi-MDPs: A
framework for
temporal abstraction in reinforcement
learning. Artiﬁcial Intelligence, 112:181–211, 1999.

[Whitehead and Ballard, 1991] Steven D. Whitehead and
Dana Ballard. Learning to perceive and act by trial and
error. Machine Learning, 7:45–83, 1991.

IJCAI-07

1028


All Common Subsequences

Hui Wang

School of Computing and Mathematics

University of Ulster, Northern Ireland, UK

h.wang@ulster.ac.uk

Abstract

Time series data abounds in real world problems.
Measuring the similarity of time series is a key
to solving these problems. One state of the art
measure is the longest common subsequence. This
measure advocates using the length of the longest
common subsequence as an indication of simi-
larity between sequences, but ignores information
contained in the second, third, ..., longest subse-
quences.
In order to capture the common information in se-
quences maximally we propose a novel measure of
sequence similarity – the number of all common
subsequences. We show that this measure satisﬁes
the common properties of similarity functions. Cal-
culating this measure is not trivial as a brute force
approach is exponential in time. We present a novel
dynamic programming algorithm to calculate this
number in polynomial time. We also suggest a dif-
ferent way of extending a class of such measures
to multidimensional, real-valued time series, in the
spirit of probabilistic metric spaces.
We conducted an experimental study on the new
similarity measure and the extension method for
classiﬁcation. It was found that both the new sim-
ilarity and the extension method are consistently
competitive.

1 Introduction
A time series is a sequence 1 of symbols or real numbers,
representing the measurements of a variable at, usually, equal
time intervals [Gunopulos and Das, 2000]. When more than
one variable are involved we get a multidimensional time se-
ries. Examples of time series include: stock price move-
ments, volume of sales over time, daily temperature read-
ings, ECG signals, and gene expressions.
In tasks involv-
ing time series we need to consider the issues of indexing
and searching [Agrawal et al., 1995; Goldin et al., 2004] and
similarity measuring [Gunopulos and Das, 2000] – the latter
being the focus of this paper. One state of the art measure is

1Throughout this paper we use the terms “time series” and “se-

quence” interchangeably.

the longest common subsequence (LCS) [Hirschberg, 1977;
Bergroth et al., 2000].

A subsequence is obtained from a sequence by removing
0 or more symbols, and a common subsequence of a set of
sequences is a subsequence of every sequence in the set. LCS
advocates the use of the longest common subsequence as an
indication of the similarity relationship between sequences.
However the common information shared between sequences
is more than that contained in the longest common subse-
quence. The second longest, third longest and, in general, all
common subsequences contain some common information to
a certain degree. The use of all possible common information
between sequences is intuitively appealing and may provide
a fuller picture about the similarity relationship between the
sequences. How to measure all such common information
between two sequences?

The above question can be answered by addressing a new
problem, all common subsequences (ACS) – counting the
number of all common subsequences. We expect that two
sequences are more similar to each other if they have more
common subsequences. ACS is conceptually novel as far as
we know. To motivate ACS, we consider three sequences
{α, β, γ} = {cbabca, bcabac, abcade}. The set of all com-
mon subsequences of α and β is (ignoring the empty se-
quence): {a, aa, ab, aba, abc, ac, b, ba, baa, bab, baba, babc,
bac, bb, bba, bbc, bc, bca, c, ca, caa, cab, caba, cabc, cac,
cb, cba, cbac, cbc, cc}. The set of all common subsequences
of α and γ is: {a, aa, ab, aba, abc, abca, ac, aca, b, ba, bc,
bca, c, ca}. Clearly lcs(α, β) = lcs(α, γ) = 4. However
acs(α, β) = 31, acs(α, γ) = 15 2, suggesting there is a rela-
tively higher degree of commonality in α and β than in α and
γ. However this difference can not be revealed by LCS.

Solving the ACS problem is not trivial. A brute-force ap-
proach has an exponential time complexity as we need to ver-
ify all subsequences. We take a dynamic programming ap-
proach, and develop a novel algorithm to calculate ACS be-
tween two sequences in polynomial time.

In order to evaluate ACS as a similarity measure we need
to extend ACS, as well as LCS, to real-valued, multidimen-
sional time series as both LCS and ACS are deﬁned in terms

2We use a convention here: when talking about a concept like
LCS or ACS, we use capital letters. When talking about the same
concept as a measure function, we use lower case letters.

IJCAI-07

635

of symbolic sequences. We suggest a novel method for this
purpose, which requires determination of equality/inequality.
This is done in the spirit of probabilistic metric spaces, where
we consider the probability that two data points are the same.
This extension method is different from the threshold-based
extension method [Vlachos et al., 2003], where two data
points in a multidimensional time series are deemed equal
if they are close enough (judged by a threshold) in every di-
mension. Under this extension the ACS similarity is evalu-
ated in an experimental study with some real-valued, multi-
dimensional time series. The study shows that ACS is favor-
ably comparable to LCS in general, and offers some property
that is absent from LCS. Additionally the extension method
is sound and competitive consistently.

The rest of the paper is organized as follows. The longest
common subsequence concept is brieﬂy reviewed in next sec-
tion as it inspired the work presented here. Our all common
subsequence concept is presented in Section 3, along with a
dynamic programming algorithm to calculate ACS. A method
is presented in Section 4, which can be used to extend ACS
as well as LCS to multidimensional, real-valued time series.
Evaluation is discussed in Section 5 and related work is dis-
cussed in Section 6, followed by some concluding remarks.

2 Longest common subsequence

The longest common subsequence, as a computer science
problem, is concerned with the search for an efﬁcient method
of ﬁnding the longest common subsequence of two or more
sequences. A milestone in the study of LCS is the devel-
opment of dynamic programming algorithms [Hirschberg,
1977; Bergroth et al., 2000], which calculates LCS in poly-
nomial time.

The dynamic programming algorithm for calculating LCS
was originally designed to work for one dimensional se-
quences of discrete values. It was extended in [Vlachos et al.,
2003] to work for multidimensional, real-valued sequences as
follows:

Let A and B be two d-dimensional sequences with length
n and m respectively: A = (a1, · · · , an) and B =
(b1, · · · , bm), where ai = (ai,1, · · · , ai,d), and bj =
(bj,1, · · · , bj,d).
Let Head(A) = (a1, · · · , an−1) and
Head(B) = (b1, · · · , bm−1). Then the LCS similarity be-
tween A and B is [Vlachos et al., 2003]:
(1)

⎧⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩

0,
1 + lcsδ,(Head(A), Head(B)),

if A or B is empty

if |an,i − bm,i| <  for 1 ≤ i ≤ d
and |n − m| ≤ δ

max(lcsδ,(Head(A), B),

lcsδ,(A, Head(B))), otherwise

lcsδ,(A, B) =

where δ is an integer that controls matching in time, and
0 <  < 1 is a real number that controls matching in space.
The LCS algorithm, thus restricted, has a computational com-
plexity of O(δ(n + m)) if we only allow a matching window
δ in time [Das et al., 1997]. The advantage of LCS is that it
supports elastic and imprecise matches.

3 All common subsequence
Here we consider sequences of discrete symbols. In the next
section we will consider sequences of multidimensional, real-
valued sequences.

Let A be a ﬁnite set of symbols. A sequence α is an ordered
set {si}n
= {s0, s1, · · · , sn}, where si ∈ A. For simplicity
we also write the above sequence as s0s1 · · · sn. The empty
sequence is an empty set and is denoted by . The data space
V is then the set of all sequences.

0

Consider two symbols a, b in a sequence α. Looking from
left to right, if a comes before b we write a ≤α b. Consider
two sequences α and β.
If α can be obtained by deleting
some symbols from β, we say α is a subsequence of β (or,
equivalently, β is a supersequence of α) and we write α (cid:4) β.
It is obvious that the (cid:4) relation is reﬂexive and transitive.

A common subsequence of two sequences is a subsequence
of both sequences. For two identical sequences, they have the
maximal number of common subsequences relative to their
length. For two different sequences, they have the minimal
number of common subsequences. Therefore the number of
common subsequences is an indication of similarity. Whether
or not it is a good similarity indicator is unclear, but will be
evaluated in a later section. We call this all common subse-
quences (ACS) similarity. For two sequences α and β, we
write acs(α, β) for the ACS similarity of the two sequences.
Lemma 1. The ACS similarity has the following properties:

• acs(α, β) ≥ 0.
• acs(α, β) ≤ acs(α, α) and acs(α, β) ≤ acs(β, β).
• ACS is symmetric, i.e., acs(α, β) = acs(β, α);
• acs(α, β) = acs(αr, βr), where αr

is the reverse se-

quence of α.

The ﬁrst two properties are common to all similarity mea-
sures [Osborne and Bridge, 1997], but the last two are ad-
ditional. The proof of the lemma follows directly from the
deﬁnition of ACS. The following lemma shows how many
subsequences there are in one sequence, which can be proved
by mathematical induction.

Lemma 2. For a sequence of length m, the number of its
, which includes the empty
subsequences is
sequence.

= 2m

i
m

The relationship between sequence and subsequence is
shown in the following lemmas. Let N (α|γ) be the set of
subsequences of α that are supersequences of γ, and N (α)
be the set of all subsequences of α.
Lemma 3. Consider γ1, γ2 (cid:4) α. Then

γ1 (cid:4) γ2 ⇐⇒ N (α|γ2) ⊆ N (α|γ1)

Proof. ⇒: Suppose γ1 (cid:4) γ2. Consider x ∈ N (α|γ2). Then
γ2 (cid:4) x. Since the relation (cid:4) is clearly transitive we have
γ1 (cid:4) x. Therefore x ∈ N (α|γ1). We then have N (α|γ2) ⊆
N (α|γ1).
⇐: Suppose N (α|γ2) ⊆ N (α|γ1). By deﬁnition we have
γ2 (cid:4) x for all x ∈ N (α|γ2), and γ1 (cid:4) y for all y ∈ N (α|γ1).
To show that γ1 (cid:4) γ2, we assume it is not true. Then γ2 (cid:9)∈
N (α|γ1), but γ2 ∈ N (α|γ2). Therefore it is not possible
N (α|γ2) ⊆ N (α|γ1), contradicting the assumption.

(cid:7)

(cid:8)

(cid:6)m

i=0

IJCAI-07

636

Lemma 4. α (cid:4) β if and only if N (α) ⊆ N (β).

Proof. ⇒: Suppose α (cid:4) β. Consider x ∈ N (α). Then x is
a subsequence of α, i.e., x (cid:4) α. By the transitivity of (cid:4) we
have x (cid:4) β, so x is a subsequence of β. Therefore we have
x ∈ N (β). This proves N (α) ⊆ N (β).
⇐: Suppose N (α) ⊆ N (β). To show that α (cid:4) β we assume
it is not true. Then α (cid:9)∈ N (β). However α ∈ N (α). There-
fore it is not possible that N (α) ⊆ N (β), contradicting the
assumption. This proves α (cid:4) β.

3.1 Calculating the number of all common

subsequences

Now we discuss how to calculate the number of all common
subsequences, acs(A, B), of any two sequences A and B.
Since the number of subsequences of a sequence is exponen-
tial in its length, a straightforward brute force approach is
clearly not feasible. We turn our attention to dynamic pro-
gramming.

“Dynamic programming can be thought of as being the re-
verse of recursion. Recursion is a top-down mechanism – we
take a problem, split it up, and solve the smaller problems that
are created. Dynamic programming is a bottom-up mecha-
nism – we solve all possible small problems and then combine
them to obtain solutions for bigger problems. The reason that
this may be better is that, using recursion, it is possible that
we may solve a small subproblem many times. Using dynamic
programming, we solve it once. ” [Hirschberg, 2005]

Theorem 1. Consider two sequences, A = (A1, · · · , Am)
and B = (B1, · · · , Bn). Let N (i, j) be the number of com-
mon subsequences of (A1, · · · , Ai) and (B1, · · · , Bj), i.e.,
the preﬁxes of sequences A and B of lengths i and j. Then

N (i, j) = N (i − 1, j − 1) × 2, if Ai = Bj
N (i, j) = N (i − 1, j) + N (i, j − 1) − N (i − 1, j − 1),

if Ai (cid:9)= Bj

Consequently acs(A, B) = N (m, n).

Proof. Let S(i − 1, j − 1) be the set of all common subse-
quences between (A1, · · · , Ai−1) and (B1, · · · , Bj−1). So
N (i − 1, j − 1) = |S(i − 1, j − 1)|.

If Ai = Bj, then S(i, j) = S(i−1, j−1)∪S(i−1, j−1)Ai.
Here S(i − 1, j − 1)Ai = {xAi : x ∈ S(i − 1, j − 1)}.
Therefore N (i, j) = N (i − 1, j − 1) × 2.

If Ai (cid:9)= Bj, then some new common subsequences may be
added to S(i, j − 1) or S(i − 1, j) on top of S(i − 1, j − 1).
Therefore S(i, j) = S(i, j − 1) ∪ S(i − 1, j) − S(i − 1, j − 1).
Consequently we have N (i, j) = N (i, j − 1) + N (i − 1, j) −
N (i − 1, j − 1).

Algorithm 3.1: calACS – calculate all common subse-
quence

Input: Two sequences A and B.
Output: The number of all common subsequences

acs(A, B).

begin

for i = 0 to |A| do N (i, 0) = 1
for j = 0 to |B| do N (0, j) = 1
for i = 1 to |A| do

for j = 1 to |B| do
if Ai = Bj then
N (i, j) = N (i − 1, j − 1) × 2
else N (i, j) =
N (i − 1, j) + N (i, j − 1) − N (i − 1, j − 1)

end

end
acs(A, B) = N (|A|, |B|)

end

1

2

3

4

5

6

7

8

9

10

11

Note that the δ parameter as discussed in Section 2 can also
be used together with the above calACS algorithm.

The acs(A, B) is unbounded. We can normalize it by the
number of all subsequences, which can be calculated as in
Lemma 2.

4 Extension of LCS and ACS
The ACS is presented in the previous section on the assump-
tion that a sequence is an ordered set of symbols or dis-
crete values. When a sequence is real-valued or multidimen-
sional, the dynamic programming algorithm for calculating
ACS must be extended.

This is also the case for LCS, but a method is presented in
[Vlachos et al., 2003] to extend LCS. This extension method
can be directly used for ACS but, as discussed in Section 2,
the threshold  is hard to choose and is usually different for
different data. This is not desirable [Keogh et al., 2004].

Here we present a different solution for the problem
of extending LCS and ACS, possibly some other similar-
ity/distance measures as well, where we need to decide if two
points are equal (equality checking). Our solution is in the
spirit of Probabilistic Metric Spaces [Schweizer and Sklar,
1983], where the line between ’equal’ and ’not equal’ is prob-
abilistic. Consider two sequences A = (A1, · · · , Am) and
B = (B1, · · · , Bn). For any pair Ai and Bj we need to de-
cide, at some point in the LCS and ACS algorithms, if they
are equal. The answer to this question may not be determin-
istic, especially when real numbers are involved. In general
the question can be answered by measuring the probability,
P rob(Ai = Bj), that they are equal 3.

A general template for both LCS and ACS is the following.
The distance/similarity between a sequence (A1, · · · , Ai)
and another sequence (B1, · · · , Bj) is

(cid:9)

(2)

DS(i, j) =

DS1
DS0

if Ai = Bj
otherwise

3Probabilistic Metric Spaces are concerned with a more general
question: what is the probability that the distance between Ai and
Bj is less than a given threshold δ?

IJCAI-07

637

In the case of LCS, DS1 = 1 + DS(i − 1, j − 1), and DS0 =
max(DS(i−1, j), DS(i, j −1)). In the case of ACS, DS1 =
DS(i − 1, j − 1) × 2, and DS0 = DS(i, j − 1) + DS(i −
1, j) − DS(i − 1, j − 1).

We propose to use the following template instead:

(3)
DS(i, j) = DS1 × P rob(Ai = Bj) + D0 × P rob(Ai (cid:9)= Bj)

where P rob(Ai (cid:9)= Bj) = 1 − P rob(Ai = Bj). This ap-
proach avoids using the threshold .

The question now is how to determine P rob(Ai = Bj). In
the case of symbols or discrete values, we can simply and
reasonably take P rob(Ai = Bj) = 1 if Ai = Bj, and
P rob(Ai = Bj) = 0 otherwise. In the case of real num-
bers, either 1-dimensional or multidimensional, we can esti-
mate P rob(Ai = Bj) by the distance between Ai and Bj
normalized to the range of [0, 1]. Note that the distance can
be the Euclidean distance or other distance. This is a sim-
plistic approach, but a more involved approach is beyond the
scope of this paper.

5 Experimental evaluation

We conducted an experimental study to evaluate the ACS
similarity measure along with the extension method. In this
study we demonstrate the utility of ACS for classiﬁcation
in an experiment on some public datasets. We consider the
following multidimensional, real-valued time series: ECG2c,
ECG3c and Auslan.

The ECG datasets are from the BIDMC Conges-
The original database
tive Heart Failure Database.
[Keogh
contains two ECG signals, but Keogh et al
et al., 2004]
separated the signals and create two
datasets (ECG2c – ecg clustering.txt and ECG3c –
ecg clustering 3class.txt) of one-dimensional time series.
Each instance of 3,200 contiguous data points (about 20
heartbeats) of each signal is randomly extracted from each
long ECG signals of each patient (class). Twenty instances
are extracted from each class, resulting in eighty total in-
stances for each dataset.

The Australian Sign Language (Auslan) data is available
from [Keogh and Folias, 2002]. We consider a clean, pre-
processed version of Auslan. There are 10 classes: ‘come’,
‘girl’, ‘man’, ‘maybe’, ‘mine’, ‘name’, ‘read’, ‘right’, ‘sci-
ence’, ‘thank’. There are 20 examples in each class, and all
classes come from one signer. All time series are of the same
length (by interpolation).

Auslan is an interesting and challenging dataset as “it is
multivariate, and without exploiting the multivariate property
it is hard to do much better than random guessing.” [Keogh
and Folias, 2002] We use the following similarity/distance
measures on the datasets: Euclidean (euc), Longest Common
Subsequence (lcs) and All Common Subsequences (acs). Be-
cause all of the datasets are real-valued and some are mul-
tidimensional, these distance/similarity measures need to be
extended. We consider the following combinations:

• euc:

the standard Euclidean distance, used in our ex-

tended similarity measures (lcs w euc, acs w euc).

• ncm:

the NCM similarity (see [Wang and Dub-
itzky, 2005]), used in our extended similarity measures
(lcs w ncm, acs w ncm).

• lcs vhgk: the extended LCS presented in [Vlachos et

al., 2003] (see Section 2).

• acs vhgk: ACS extended in a way similar to lcs vhgk.
• lcs w euc: LCS extended as in Eq.3, where the prob-
ability is estimated (simplistically) as the normalized
Euclidean distance.

• acs w euc: ACS extended as in Eq.3, where the proba-
bility is estimated as the normalized Euclidean distance.
• lcs w ncm: where the probability is the normalized

NCM.

• acs w ncm: where the probability is the normalized

NCM.

We used the leave-one-out evaluation strategy in the k-
nearest neighbors (kNN) classiﬁcation framework, meaning
that we classify every time series based on all other time se-
ries and then we measure the classiﬁcation accuracy. There
are two parameters in the experiment: δ and , which are
required by acs and lcs. For simplicity we set δ = 3
for all measures, without loss of generality. For  we test-
experimented on samples of the data with various values and
set it to a value that gives the best overall result, that is,
 = 0.13. Additionally we set parameter k (required for kNN)
to 1, 4, 7, 10, 13, 16, 19.

Experimental results are presented in Tables 1, 2, 3, and 4,

from which we observe the following:

• when used as part of acs or lcs, ncm gives consistently
better results than euc: acs w ncm > acs w euc and
lcs w ncm > lcs w euc.

• acs has an edge over lcs, especially for relatively
large k: acs w euc > lcs w euc and acs w ncm >
lcs w ncm.

• The extension method presented in this paper (see Sec-
tion 4) has an edge over that suggested in [Vlachos et
al., 2003], especially for small k values: lcs w euc >
lcs vhgk.

We believe that the superior performance of ACS over LCS
is due to the fact that ACS is able to capture more common
information in a pair of sequences than LCS.

6 Related work
Paper [Lin et al., 2005] presents a novel measure of dissimi-
larity between two time sequences – the degree of difference
for pattern p, weighted by the conﬁdence of the pattern, in
two time series. Given two time series, s1 and s2, they are
ﬁrst of all represented as symbolic SAX sequences. A pattern
is a subsequence in the SAX representation of a time series.
Without weighting, the dissimilarity is

DSim(s1, s2) ∝

|f1(p) − f2(p)|

max(f1(p), f2(p))

(cid:10)

p

where fi(p) is the frequency of pattern p in sequence si.

IJCAI-07

638

Paper [Yang et al., 2003] introduces a new measure of sim-
ilarity/distance between symbolic sequences. The measure is
based on counting the occurrences of m-bit (i.e., consisting
of m symbols) words (i.e., subsequences) in a symbolic se-
quence. Without weighting by the probability of each word
in a sequence, the distance between two sequences s1 and s2
is deﬁned as follows:

2m(cid:10)

Dm(s1, s2) ∝

|R1(wk) − R2(wk)|

k=1

where Ri(wk) is the rank of subsequence wk
si.

4 in sequence

DSim and Dm are both related to ACS in the sense they
consider all common subsequences in s1 and s2. However
there are signiﬁcant differences:

• Conceptually both DSim and Dm measure the average
(weighted) difference of the frequency of a subsequence
p in s1 and the frequency of p in s2, while ACS counts
the number of common subsequences of two sequences.
• They all need to look through a large, exponential search
space for subsequences, but ACS has a dynamic pro-
gramming algorithm to calculate the similarity in poly-
nomial time.

• ACS can be applied to sequences of real numbers with-
out the need of transformation, while Dm can not. To
use DSim for sequences of real numbers, the symbol
SAX representation (a type of transformation) is needed.

7 Conclusion

This paper presents a conceptually novel similarity measure
for time series data – all common subsequences or ACS. It
is intended to maximally capture the common information
shared by two time series. A novel dynamic programming
algorithm is presented to calculate the number of all common
subsequences.

To use ACS and other equality-checking based sequence
similarity measures (e.g., LCS) to handle multidimensional,
read-valued time series, we need to extend these measures.
This paper presents an extension method, which is inspired
by the theory of probabilistic metric spaces. We seek to use
the probability that two points are equal in calculating the
similarity when a time series is real-valued or multidimen-
sional.

We conducted an experimental study to evaluate the new
similarity measure and the extension method. Our exper-
imental results show that ACS is favorably comparable to
LCS, and it displays a distinctive feature – stable perfor-
mance as more data are considered for decision (i.e., as k gets
larger), making ACS suitable for those tasks whose aim is to
ﬁnd a set of ranked sequences (e.g., information retrieval).
The results also show that the extension method presented in
this paper works well, and appears better than that discussed
in [Vlachos et al., 2003].

4The frequency of occurrence is calculated for every words, and

is used as the basis for ranking all the words.

We feel that the way in which we estimate the probability
of two data points being equal is a bit crude. In our future
work we intend to develop more involved methods in order to
make it more effective and efﬁcient.

References
[Agrawal et al., 1995] R. Agrawal, K.-I. Lin, H.S. Sawhney,
and K. Shim. Fast similarity search in the presence of
noise, scaling, and translation in time-series databases. In
Proc. VLDB95, 1995.

[Bergroth et al., 2000] L. Bergroth, H. Hakonen,

and
T. Raita. A survey of longest common subsequence
algorithms. In SPIRE’00: Proceedings of the Seventh In-
ternational Symposium on String Processing Information
Retrieval, page 39. IEEE Computer Society, 2000.

[Das et al., 1997] G. Das, D. Gunopulos, and H. Mannila.
Finding similar time series. In Proc. of PKDD97: Prin-
ciples of Data Mining and Knowledge Discovery in Data-
bases, pages 88–100, 1997.

[Goldin et al., 2004] Dina Goldin, Todd Millstein,

and
Ayferi Kutlu. Bounded similarity querying for time-series
data, 2004.

[Gunopulos and Das, 2000] Dimitrios Gunopulos and Gau-
tam Das. Time series similarity measures. 2000. Tutorial
notes of the sixth ACM SIGKDD international conference
on Knowledge discovery and data mining.

[Hirschberg, 1977] Daniel S. Hirschberg. Algorithms for the
longest common subsequence problem. Journal of ACM,
24(4):664–675, 1977.

[Hirschberg, 2005] Dan Hirschberg. Dynamic program-
http://www.ics.uci.edu/∼dan/

ming, 2005.
class/161/notes/6/Dynamic.html.

[Keogh and Folias, 2002] E. Keogh and T. Folias. The UCR
time series data mining archive, 2002. http://www.
cs.ucr.edu/∼eamonn/TSDMA/index.html.

[Keogh et al., 2004] Eamonn Keogh, Stefano Lonardi, and
Chotirat Ann Ratanamahatana. Towards parameter-free
data mining. In Proc. SIGKDD04, 2004.

[Lin et al., 2005] Jessica Lin, Eamonn Keogh, and Stefano
Lonardi. Visualizing and discovering non-trivial patterns
in large time series databases. Information Visualization,
4(2):61–82, 2005.

[Osborne and Bridge, 1997] Hugh Osborne

and Derek
Bridge. Models of similarity for case-based reasoning.
In Procs. of the Interdisciplinary Workshop on Similarity
and Categorisation, pages 173–179, 1997.

[Schweizer and Sklar, 1983] B. Schweizer and A. Sklar.

Probabilistic metric spaces. North-Holland, 1983.

[Vlachos et al., 2003] Michail Vlachos, Marios Hadjieleft-
heriou, Dimitrios Gunopulos, and Eamonn Keogh. Index-
ing multi-dimensional time-series with support for mul-
tiple distance measures.
In Proceedings of SIGKDD03:
ACM International Conference on Data Mining August
24-27, 2003, Washington, DC, USA, 2003.

IJCAI-07

639

[Wang and Dubitzky, 2005] H. Wang and W. Dubitzky. A
ﬂexible and robust similarity measure based on contex-
tual probability. In Proceedings of IJCAI’05, pages 27–32,
2005.

[Yang et al., 2003] Albert C. C. Yang, Shu-Shya Hseu,
Huey-Wen Yien, Ary L. Goldberger, and C. K. Peng. Lin-
guistic analysis of the human heartbeat using frequency
and rank order statistics. Phsical Review Letters, 90(10),
2003.

A Tables

1
k
97.5
acs vhgk
97.5
lcs vhgk
100
acs w euc
lcs w euc
100
acs w ncm 100
lcs w ncm 100

4
97.5
97.5
100
100
100
100

7
97.5
97.5
100
100
100
100

10
97.5
97.5
97.5
95.0
100
100

13
97.5
97.5
90.0
87.5
100
95.0

16
97.5
97.5
87.5
85.0
100
95.0

19
97.5
97.5
87.5
85.0
100
95.0

Table 1: Leave-one-out classiﬁcation accuracy (%) on a sam-
ple (’come’ and ’girl’) of Auslan.

1
k
83.5
acs vhgk
83.5
lcs vhgk
90.5
acs w euc
lcs w euc
90.5
acs w ncm 93.0
lcs w ncm 94.5

4
84.0
85.0
90.0
90.0
93.5
94.0

7
85.0
84.5
89.0
90.5
93.5
94.0

10
85.5
85.0
86.5
88.0
93.0
93.0

13
85.5
85.0
85.5
86.0
93.0
92.0

16
85.5
84.5
84.5
86.0
93.5
90.5

19
85.5
84.0
84.5
84.5
93.0
90.5

Table 2: Leave-one-out classiﬁcation accuracy (%) on all of
Auslan.

1
k
90.0
acs vhgk
90.0
lcs vhgk
95.0
acs w euc
lcs w euc
95.0
acs w ncm 95.0
lcs w ncm 95.0

4
90.0
80.0
95.0
95.0
95.0
95.0

7
90.0
65.0
95.0
80.0
95.0
80.0

10
90.0
50.0
95.0
75.0
95.0
75.0

13
90.0
50.0
95.0
70.0
95.0
65.0

16
90.0
50.0
95.0
65.0
95.0
65.0

19
90.0
60.0
95.0
60.0
95.0
65.0

Table 3: Leave-one-out classiﬁcation accuracy (%) on Ecg2c.

1
k
75.0
acs vhgk
80.6
lcs vhgk
91.7
acs w euc
91.7
lcs w euc
acs w ncm 91.7
lcs w ncm 91.7

4
75.0
80.6
91.7
88.9
91.7
88.9

7
75.0
80.6
91.7
86.1
91.7
83.3

10
75.0
75.0
91.7
80.6
88.9
80.6

13
75.0
75.0
91.7
77.8
86.1
77.8

16
75.0
75.0
91.7
72.2
86.1
72.2

19
75.0
75.0
91.7
69.4
86.1
72.2

Table 4: Leave-one-out classiﬁcation accuracy (%) on Ecg3c.

B Acknowledgements
Thanks to Eamonn Keogh for providing all of the datasets
used in this paper, for his comments on the paper, and for
pointing out the work in [Lin et al., 2005; Yang et al., 2003].

IJCAI-07

640


Searching for Interacting Features

Zheng Zhao and Huan Liu

Department of Computer Science and Engineering

Arizona State University

{zheng.zhao, huan.liu}@asu.edu

Abstract

Feature interaction presents a challenge to feature
selection for classiﬁcation. A feature by itself may
have little correlation with the target concept, but
when it is combined with some other features, they
can be strongly correlated with the target concept.
Unintentional removal of these features can result
in poor classiﬁcation performance. Handling fea-
ture interaction can be computationally intractable.
Recognizing the presence of feature interaction, we
propose to efﬁciently handle feature interaction to
achieve efﬁcient feature selection and present ex-
tensive experimental results of evaluation.

1 Introduction
The high dimensionality of data poses a challenge to learning
tasks such as classiﬁcation. In the presence of many irrele-
vant features, classiﬁcation algorithms tend to overﬁt train-
ing data [Guyon and Elisseeff, 2003; Dash and Liu, 1997].
Many features can be removed without performance dete-
rioration [Gilad-Bachrach et al., 2004]. Feature selection
is one effective means to remove irrelevant features [Blum
and Langley, 1997]. Optimal feature selection requires an
exponentially large search space (O(2N ), where N is the
number of features) [Almuallim and Dietterich, 1994]. Re-
searchers often resort to various approximations to determine
relevant features (e.g., relevance is determined by correla-
tion between individual features and the class) [Hall, 2000;
Yu and Liu, 2003]. However, a single feature can be con-
sidered irrelevant based on its correlation with the class; but
when combined with other features, it becomes very rele-
vant. Unintentional removal of these features can result in
the loss of useful information and thus may cause poor clas-
siﬁcation performance. This is studied in [Jakulin and Bratko,
2003] as attribute interaction. For example, MONK1 is a data
set involving feature interaction. There are six features in
MONK1 and the target concept of MONK1 is: (A1 = A2)
or (A5 = 1). Here A1 and A2 are two interacting features.
Considered individually, the correlation between A1 and the
class C (similarly for A2 and C) is zero, measured by mu-
tual information. Hence, A1 or A2 is irrelevant when each
is individually evaluated. However, if we combine A1 with
A2, they are strongly relevant in deﬁning the target concept.

An intrinsic character of feature interaction is its irreducibil-
ity [Jakulin and Bratko, 2004], i.e., a feature could lose its
relevance due to the absence of its interacting feature(s).

Existing efﬁcient feature selection algorithms usually as-
sume feature independence [Dash and Liu, 1997; Hall, 2000].
Because of the irreducible nature of feature interactions, these
algorithms cannot select interacting features such as A1 and
A2 in MONK1. Others attempt to explicitly address feature
interactions by ﬁnding some low-order interactions (2- or 3-
way). In [Jakulin and Bratko, 2003], the authors suggest to
use interaction gain as a practical heuristic for detecting at-
tribute interaction. Using interaction gain, their algorithms
can detect if datasets have 2-way (one feature and the class)
and 3-way (two features and the class) interactions. They fur-
ther provide in [Jakulin and Bratko, 2004] a justiﬁcation of
the interaction information, and replace the notion of ‘high’
and ‘low’ in [Jakulin and Bratko, 2003] with statistical sig-
niﬁcance and illustrate the signiﬁcant interactions in the form
of interaction graph. Below we apply four feature selection
algorithms to synthetic data with known interaction and ob-
serve how they fare: FCBF [Yu and Liu, 2003], CFS [Hall,
2000], ReliefF [Kononenko, 1994], and FOCUS [Almuallim
and Dietterich, 1994], all available in WEKA [Witten and
Frank, 2005].

Motivating examples: Synthetic data with known fea-
ture interaction. Four synthetic data sets are used to exam-
ine how various algorithms deal with known feature interac-
tions in feature selection. The ﬁrst data set is Corral [John et
al., 1994], having six boolean features A0, A1, B0, B1, I, R.
The class Y is deﬁned by Y = (A0 ∧ A1) ∨ (B0 ∧ B1) and
features A0, A1, B0, B1 are independent of each other. Fea-
ture I is irrelevant to Y and its values have a uniform ran-
dom distribution; and feature R is correlated with Y 75% of
the time and is redundant. The other three training data sets
are MONKs data. Their target Concepts are: (1) MONK1:
(A1 = A2) or (A5 = 1); (2) MONK2: Exactly two of
A1 = 1, A2 = 1, A3 = 1, A4 = 1, A5 = 1, A6 = 1; and
(3) MONK3: (A5 = 3 and A4 = 1) or (A5 (cid:4)= 4 and A2 (cid:4)= 3)
(5% class noise added to the training data).

Results are presented in Table 1. For Corral, all four algo-
rithms remove the irrelevant feature I, but only FOCUS re-
moves the redundant feature R. Features A0, A1 and B0, B1
interact with each other to determine the class label of an in-
stance. CFS, FCBF and ReliefF cannot remove R because R

IJCAI-07

1156

Table 1: Features selected by each algorithm on artiﬁcial data, and the

(cid:2)

(cid:2)

indicates a missing relevant feature.

Corral
Monk1
Monk2

Monk3

Relevant Features
A0, A1, B0, B1

A1, A2, A5

A1, A2, A3, A4,

A5, A6

A2, A4, A5

FCBF

A0, A1, B0, B1, R A0,
A1, , A3, A4, A5

, , , A4,
A5, A6

A2, , A5, A6

CFS
,
,

ReliefF

FOCUS

, R

A0, A1, B0, B1, R

A0, A1, B0, B1

,

, A5
, , , A4,
A5, A6
A2,
, A5

A1, A2, A5

A1, A2, A5

A1, A2, A3, A4,

A1, A2, A3, A4,

A5, A6

A2, A4, A5

A5, A6

A1, A2, A4, A5

is strongly correlated (75%) with Y . For all the three MONKs
data sets, ReliefF can ﬁnd true relevant features1 as seen in
Table 1. Both FCBF and CFS perform similarly, and FCBF
ﬁnds more features. FOCUS can handle feature interaction
when selecting features. However, as an exhaustive search
algorithm, FOCUS ﬁnds irrelevant feature A1 for MONK3
due to 5% noise in the data. In a sense, it overﬁts the training
data. FOCUS is also impractical because ﬁnding moderately
high-order interactions can be too expensive, as
can be too large, when dimensionality N is large.

(cid:2)m

(cid:3)

(cid:4)

N
i

i=1

In this work, we design and implement an efﬁcient ap-
proach to deal with feature interactions. Feature interactions
can be implicitly handled by a carefully designed feature eval-
uation metric and a search strategy with a specially designed
data structure, which together take into account interactions
among features when performing feature selection.

2 Interaction and Data Consistency

One goal of feature selection is to remove all irrelevant fea-
tures. First, we deﬁne feature relevance as in [John et al.,
1994]. Let F be the full set of features, Fi be a feature,
Si = F − {Fi} and P denote the conditional probability of
class C given a feature set.

Deﬁnition 1 (Feature Relevance) A feature Fi is relevant iff

∃S

(cid:2)
i ⊆ Si, such that P (C|Fi, S

(cid:2)
i) (cid:4)= P (C|S

(cid:2)
i).

Otherwise, feature Fi is said to be irrelevant.

Deﬁnition 1 suggests that a feature can be relevant only
when its removal from a feature set will reduce the prediction
power. From Deﬁnition 1, it can be shown that a feature is
relevant due to two reasons: (1) it is strongly correlated with
the target concept; or (2) it forms a feature subset with other
features and the subset is strongly correlated with the target
concept. If a feature is relevant because of the second reason,
there exists feature interaction. Feature interaction is charac-
terized by its irreducibility [Jakulin and Bratko, 2004]. A kth
feature interaction can be formalized as:

Deﬁnition 2 (kth order Feature Interaction) F is a feature
subset with k features F1, F2, . . . , Fk. Let C denote a metric
that measures the relevance of the class label with a feature or
a feature subset. Features F1, F2, . . ., Fk are said to interact
with each other iff: for an arbitrary partition F = {F1, F2,
F3, . . ., Fl} of F, where l ≥ 2 and Fi (cid:4)= φ, we have

∀i ∈ [1, l], C (F) > C (Fi)

Identifying either relevant features or a kth order feature
interaction requires exponential time. Deﬁnitions 1 and 2

cannot be directly applied to identify relevant or interact-
ing features when the dimensionality of a data set is high.
Many efﬁcient feature selection algorithms identify relevant
features based on the evaluation of the correlation between
the class and a feature (or a selected feature subset). Rep-
resentative measures used for evaluating relevance include:
distance measures [Kononenko, 1994; Robnik-Sikonja and
Kononenko, 2003], information measures [Fleuret, 2004],
and consistency measures [Almuallim and Dietterich, 1994],
to name a few. Using these measures, feature selection algo-
rithms usually start with an empty set and successively add
“good” features to the selected feature subset, the so-called
sequential forward selection (SFS) framework. Under this
framework, features are deemed relevant mainly based on
their individually high correlations with the class, and rele-
vant interacting features of high order may be removed [Hall,
2000; Bell and Wang, 2000], because the irreducible nature
of feature interaction cannot be attained by SFS.

Recall that ﬁnding high-order feature interaction using rel-
evance (Deﬁnitions 1 and 2) entails exhaustive search of all
feature subsets. In order to avoid exponential time complex-
ity, we derive a feature scoring metric based on the con-
sistency hypothesis proposed in [Almuallim and Dietterich,
1994] to approximate the relevance measure as in Deﬁni-
tion 1. With this metric, we will design a fast ﬁlter algorithm
that can deal with feature interaction in subset selection.

Let D be a data set of m instances, D = {d1, d2, . . . , dm},
and F be the feature space of D with n features, F = {F1, F2,
. . . , Fn}, we have the following:
two instances di
Deﬁnition 3 (Inconsistent Instances) If
and dj in D have the same values except for their class la-
bels, di and dj are inconsistent instances or the two matching
instances are inconsistent.
Deﬁnition 4 (Inconsistent-Instances Set) D ⊆ D, D is an
inconsistent-instances set iff ∀ di, dj ∈ D , i (cid:4)= j, either di
and dj are inconsistent or they are duplicate. D is a maximal
inconsistent-instances set, iff ∀d ∈ D and d /∈ D , D ∪ {d}
is not an inconsistent-instances set.

Deﬁnition 5 (Inconsistency Count) Let
an
inconsistent-instances set with k elements d1, d2,. . . , dk,
and c1, c2, . . . , ct are the class labels of D , we partition
D into t subsets S1, S2, . . . , St by the class labels, where
Si = {dj| dj has label ci}. The inconsistency count of D is:

be

D

inconsistencyCount(D) = k − max
1≤i≤t

(cid:11)Si(cid:11)

Deﬁnition 6 (Inconsistency Rate) Let D1, D2, . . ., Dp de-
note all maximal inconsistent-instances sets of D, the incon-
sistency rate (ICR) of D is:

(cid:2)

1Interestingly, using the full data sets ReliefF missed A1, A5 for

MONK2, A1 for MONK3.

ICR(D) =

IJCAI-07

1157

1≤i≤p inconsistencyCount(Di)

m

Deﬁnition 7 (Consistency Contribution) or c-contribution
Let π denote the projection operator which retrieves a sub-
set of columns from D according to the feature subset, the
c-contribution (CC) of feature Fi for F is deﬁned as:

CC(Fi, F) = ICR(πF −{Fi}(D)) − ICR(πF (D))

(D)) ≥ ICR(πSj

It is easy to verify that the inconsistency rate is monotonic
in terms of the number of features, i.e., ∀Si, Sj, Si ⊆ Sj
⇒ ICR(πSi
(D)). Hence, c-contribution
of a feature is always a non-negative number with the zero
meaning no contribution. C-contribution of a feature Fi is a
function of F − {Fi}, where F is the set of features for D.
C-contribution of a feature is an indicator about how signiﬁ-
cantly the elimination of that feature will affect consistency.
C-contribution of an irrelevant feature is zero. C-contribution
can be considered as an approximation of the metric in Def-
inition 1 by using inconsistency rate as an approximation of
P , the conditional probability of class C given a feature set.
The monotonic property of inconsistency rate suggests that
the backward elimination search strategy ﬁts c-contribution
best in feature selection. That is, one can start with the full
feature set and successively eliminating features one at a time
based on their c-contributions. Backward elimination allows
every feature to be evaluated with the features it may inter-
act with. Hence, backward elimination with c-contribution
should ﬁnd interacting features. However, backward elimina-
tion using inconsistency rate or c-contribution has two prob-
lems. The ﬁrst problem is that it is very costly as it needs
to calculate inconsistency rate for each potentially remov-
able feature. As in the work of FOCUS [Almuallim and
Dietterich, 1994], FOCUS relies on exhaustive search. It is
impractical to do so when the dimensionality is reasonably
large, which separates this work from FOCUS. We will de-
sign a speciﬁc data structure next to achieve efﬁcient calcu-
lation of c-contribution for our algorithm INTERACT. The
second problem is that c-contribution measure is sensitive to
which feature is selected to compute ﬁrst, the so-called the
feature order problem. This is because features evaluated ﬁrst
for their consistency are more likely to be eliminated ﬁrst.

Solutions to the two problems will enable c-contribution to
be used in building an efﬁcient algorithm of backward elimi-
nation. We present our solutions and the algorithm next.

3 Eliminating Irrelevant Features

We ﬁrst present our solutions that form two pillar components
for the algorithm INTERACT and then discuss its details.

3.1 Efﬁcient update of c-contribution

C-contribution relies on the calculation of inconsistency rate.
With the monotonicity of inconsistency rate, the following
two properties are true after a feature fi is eliminated from
a set {f1, .., fi, ..., fn} where i = 1, 2, ..., n: (I) all inconsis-
tent instances sets are still inconsistent; and (II) each maximal
inconsistent instances set will be either of equal size or big-
ger. Based on these two properties, we implement a hashing
mechanism to efﬁciently calculate c-contribution: Each in-
stance is inserted into the hash table using its values of those
features in Slist as the hash key, where Slist contains the

(D). Hence, the inconsistency rate of πSlist

ranked features that are not yet eliminated (Slist initialized
with the full set of features). Instances with the same hash
key will be insert into the same entry in the hash table. And
the information about the labels is recorded. Thus each en-
try in the hash table corresponds to a maximal inconsistency
(D)
set of πSlist
can be obtained by scanning the hash table. Property (I) says
that in order to generate an entry in the hash table for a new
Slist (after eliminating a feature), it is not necessary to scan
the data again, but only the current hash table. Property (II)
suggests that after each iteration of elimination, the number of
entries of the hash table for new Slist should decrease. There-
fore, the hashing data structure allows for efﬁcient update of
c-contribution iterative feature elimination.

3.2 Dealing with the feature order problem
We now consider the feature order problem in applying c-
contribution. If we can keep removing the current, most irrel-
evant feature, we will likely retain the most relevant ones in
the remaining subset of selected features. Assuming that a set
of features can be divided into subset S1 including relevant
features, and subset S2 containing irrelevant ones. By consid-
ering to remove features in S2 ﬁrst, features in S1 are more
likely to remain in the ﬁnal set of selected features. Therefore,
we apply a heuristic to rank individual features using symmet-
rical uncertainty (SU) in an descending order such that the
(heuristically) most relevant feature is positioned at the begin-
ning of the list. SU is often used as a fast correlation measure
to evaluate the relevance of individual features [Hall, 2000;
Yu and Liu, 2003]. This heuristic attempts to increase the
chance for a strongly relevant feature to remain in the se-
lected subset. Let H(X) and H(X, C) denote entropy and
joint entropy respectively, and M (X, C) = H(C) + H(X) −
H(X, C) the mutual information measuring the common in-
formation shared between the two variables. SU between the
class label C and a feature Fi is:

(cid:6)

(cid:5)

SU (Fi, C) = 2

M (Fi, C)

H(Fi) + H(C)

This ranking heuristic cannot guarantee that the interact-
ing features are ranked high. For MONK1, for example,
SU (A1, C) = SU (A6, C) = 0. Either one can be evaluated
ﬁrst for its c-contribution. Since CC(A1, F) > CC(A6, F),
A6 is eliminated. We will experimentally examine the rank-
ing effect in Section 4.2.

INTERACT - An algorithm

3.3
The above solutions pave the way for c-contribution to be
used in feature selection. We present an algorithm, INTER-
ACT, that searches for interacting features. It is a ﬁlter al-
gorithm that employs backward elimination to remove those
features with no or low c-contribution. The details are shown
in Figure 1: Given a full set with N features and a class at-
tribute C, it ﬁnds a feature subset Sbest for the class concept.
The algorithm consists of two major parts. In the ﬁrst part
(lines 1-6), the features are ranked in descending order based
on their SU values. In the second part (lines 7-16), features
are evaluated one by one starting from the end of the ranked
feature list. Function getLastElement() returns the feature

IJCAI-07

1158

input:

F: the full feature set with features F1,

F2, . . . , FN

C: the class label
δ: a predeﬁned threshold

output:

Sbest: the best subset

calculate SUFi,c for Fi
append Fi to Slist

1 Slist = NULL
2 for i=1 to N do
3
4
5 end
6 order Slist in descending values of SUi,c
7 F = getLastElement(Slist)
8 repeat
9
10
11
12
13
14
15
16 until F == N U LL
17 Sbest = Slist
18 return Sbest

end
F = getN extElement(Slist, F )

end

if F <>N U LL then

p = CC(F, Slist) // c-contribution
if p ≤ δ then

remove F from Slist

Figure 1: Algorithm INTERACT

in the end of the list, Slist. If c-contribution of a feature is less
than δ, the feature is removed, otherwise it is selected. Func-
tion getN extElement(Slist, F ) returns the next unchecked
feature just preceding F in the ranked feature list (line 15).
The algorithm repeats until all features in the list are checked.
δ is a predeﬁned threshold (0 < δ < 1). Features with their
c-contribution < δ are considered immaterial and removed.
A large δ is associated with a high probability of removing
relevant features. Relevance is deﬁned by c-contribution: the
higher value of CC(F, Slist) indicates that F is more rele-
vant. δ = 0.0001 if not otherwise mentioned. The parameter
can also be tuned using the standard cross validation.
Time complexity of INTERACT: The ﬁrst part of the algo-
rithm has a linear time complexity of O(N M ), where N is
the number of features and M is the number of instances of
a given data set. For the second part of the algorithm, the
calculation of a feature’s c-contribution using a hash table
takes also O(N M ). For N features, the time complexity
of INTERACT is O(N 2M ). This is the worst case analy-
sis. Its average time complexity is less because (1) we only
use the hash table of current Slist in the calculation of c-
contribution, and (2) the number of the entries in the hash
table decreases after each iteration. If we assume it decreases
to a percentage of the initial size, where 0 < a < 1, then
in N iterations, the overall time complexity of INTERACT
is O(N M (1 − aN +1)/(1 − a)) (the proof is omitted).
In
other words, INTERACT is expected to be comparable with
heuristic algorithms such as FCBF [Yu and Liu, 2003].

4 Empirical Study

We empirically evaluate the performance of INTERACT in
search of interacting features. For the four synthetic data sets
with known feature interaction (Table 1), INTERACT ﬁnds

Table 2: Summary of the benchmark data sets. F: number of
features; I: number of instances and C: number of classes.

Data Set
lung-cancer
zoo
wine
soy-large
cmc

F
56
16
13
35
9

I
32
101
178
306
1473

C
3
7
3
19
3

Data Set
vehicle
kr-vs-kp

F
18
36

internet-ads
45×4026+2C

1558
4026

I

846
3196

3278

45

C
4
2

2
2

only the relevant features (δ = 0.05). Now we evaluate
INTERACT using benchmark data sets in comparison with
some representative feature selection algorithms with the fol-
lowing aspects: (1) number of selected features, (2) predictive
accuracy with feature selection, and (3) run time. We also ex-
amine how effective the two solutions (given in Sections 3.1
and 3.2) are through a lesion study by removing one of them
at a time from INTERACT.

4.1 Experiment setup

In our experiments, we choose four representative feature se-
lection algorithms for comparison. They are FCBF [Yu and
Liu, 2003], CFS [Hall, 2000], ReliefF [Kononenko, 1994],
and FOCUS [Almuallim and Dietterich, 1994]. All are avail-
able in the WEKA environment [Witten and Frank, 2005].
INTERACT is implemented in the WEKA’s framework. It
will be made available upon request. All the experiments
were conducted in the WEKA environment.

From 28 data sets, [Jakulin, 2005] identiﬁed 10 data sets
having feature interactions without selecting interacting fea-
tures. Here we focus on our discussion on the 10 data sets2.
The datasets are from the UCI ML Repository [Blake and
Merz, 1998]. We also include another two datasets in the
experiment: the ‘internet-ads’ data from the UCI ML Repos-
itory, and the ‘45×4026+2C’ data from [Alizadeh and et al.,
2000]. The information about the 9 data sets (without 3
MONKS data sets) is summarized in Table 2. For each data
set, we run all 5 feature selection algorithms and obtain se-
lected feature subsets of each algorithm. For data sets con-
taining features with continuous values, if needed, we apply
the MDL discretization method (available in WEKA). We re-
move all index features if any. In order to evaluate whether
the selected features are indeed good, we apply two effec-
tive classiﬁcation algorithms C4.5 and a linear Support Vector
Machine (SVM)3 (both available from Weka) before and af-
ter feature selection and obtain prediction accuracy by 10-fold
cross-validation (CV). Although C4.5 itself evaluates features
(one at a time), its performance is sensitive to the given sets of
features (e.g., its accuracy rates on MONK1 are 88.88% and
75.69% for (A1, A2, and A5) and for all 6 features, respec-
tively). Because of FOCUS’s exponential time complexity,
we only provide those results of FOCUS obtained in 3 hours
of dedicated use of the PC. INTERACT, FCBF, CFS and Re-

2In which 3 data sets are MONKs data. We consider them syn-

thetic data and discussed them earlier separately.

3Since Naive Bayes Classiﬁer (NBC) assumes conditional inde-
pendence between features [Irina Rish, 2001], selecting interacting
features or not has limited impact on it. Our experimental results of
NBC conform to the analysis and will be presented elsewhere due to
the space limit.

IJCAI-07

1159

Table 3: Number of selected features for each algorithm (IN:
INTERACT, FC: FCBF, RE: ReliefF, FO: FOCUS, FS: Full Set).
NA denotes not available.

Data Set
lung-cancer
zoo
wine
soy-large
cmc
vehicle
kr-vs-kp
internet-ads
45×4026+2C
average

IN
6
5
5
13
9
18
29
49
3

FC
6
8
10
13
2
4
7
38
64

CFS
11
9
8
21
3
11
3
11
42

RE
22
14
11
32
5
7
8
2
15

15.22

16.89

13.22

12.89

FO
4
5
5
NA
9
18
NA
NA
NA
8.20

FS
56
16
13
35
9
18
36

1558
4026
640.78

liefF all complete their runs in seconds. This is consistent
with our understanding and expectation of these algorithms.

4.2 Results and discussion
Number of selected features. Table 3 presents the numbers
of features selected by the ﬁve algorithms. All algorithms sig-
niﬁcantly reduced the number of features in many cases (e.g.,
from 56 to as few as 6). The average numbers of selected
features for the 9 data sets are 15.22 (INTERACT), 16.89
(FCBF), 13.22 (CFS), 12.89 (ReliefF), and 640.78 (Full Set).
For four data sets (indicated by NA in the table), FOCUS did
not ﬁnish after 3 hours. For the 4 synthetic data sets with
known relevant features (Table 1), INTERACT selected only
the relevant ones. Next we examine the effect of this reduc-
tion on accuracy.

Predictive accuracy before and after feature selection.
For the 9 data sets with known interactions, we obtained pre-
dictive accuracy rates by 10-fold cross validation using C4.5
and SVM. The results are shown in the two sub-tables of Ta-
ble 5. For both classiﬁers, the reduction of features by IN-
TERACT obtains results comparable with using all features:
average accuracy 83.58% (INTERACT) vs. 79.74% (Full
Set) for C4.5, and 82.88% (INTERACT) vs. 81.04% (Full
Set) for SVM. Comparing INTERACT with other feature se-
lection algorithms, INTERACT performs consistently better
for C4.5 with better average accuracy. For SVM, INTER-
ACT is comparable with other feature selection algorithms.
One exception is the ‘soy-large’ data for the result of SVM.
We notice that the data set has 35 features, 306 instances, and
19 classes (Table 2); INTERACT identiﬁes 13 features (Ta-
ble 3) - the smallest number of selected features (FCBF also
selected 13 features). We surmise that it may be too easy for
the inconsistency measure to be satisﬁed with a small feature
subset when each class has a small number of instances. In
sum, for both classiﬁers, INTERACT can help achieve bet-
ter or similar accuracy, and hence, INTERACT is effective in
search of interacting features.

The effect of feature ranking. INTERACT ranks features
before backward elimination of features begins. As a part
of the lesion study, we remove the ranking component from
INTERACT to form a version INTERACT\R. We summa-
rize the results here due to the space limit: INTERACT is
always better than or equivalent to INTERACT\R; the aver-
age 10-fold CV accuracy for C4.5 and SVM for (INTERACT,
INTERACT\R) are (83.58,78.69) and (82.88 78.54), respec-
tively. Noticing that INTERACT ranks features using SU and

16

14

12

10

8

6

4

2

0

z
o
o

l

u
n
g
-
c
a
n
c
e
r

c
m
c

i

w
n
e

s
o
y
-
l
a
r
g
e

v
e
h
c
e

i

l

k
r
-
v
s
-
k
p

4
5
x
4
0
2
6
+
2
C

i

n

t

e
r
n
e

t
-
a
d

Figure 2: TIN T ERACT\D /TIN T ERACT

Table 4: Run time (in second) for each algorithm.

Data Set
lung-cancer
zoo
wine
soy-large
cmc
vehicle
kr-vs-kp
internet-ads
45×4026+2C
average

IN
0.09
0.08
0.09
0.14
0.14
0.20
1.31
150.86
20.30
19.25

FC
0.02
0.01
0.02
0.06
0.02
0.05
0.27
60.484
1.02
6.88

CFS
0.05
0.01
0.02
0.05
0.03
0.06
0.31
54.344
323.25
42.01

Re
0.02
0.02
0.02
0.09
0.11
0.11
0.61
31.359
1.50
3.76

FO
2.31
0.81
0.33
NA
2.81
976.58
NA
NA
NA
196.57

FCBF employs SU, we observe in Table 5 that FCBF does
not perform as well as INTERACT in selecting interacting
features. The results suggest that the combination of SU and
c-contribution helps INTERACT to achieve its design goal.
Clearly, using SU is a heuristic. There could be other alter-
natives to rank the features. Studying the effects of different
ranking algorithms is one line of our on-going research.

The effect of the data structure. We devised a hashing
data structure to speed up the time consuming calculation
of c-contribution during feature selection. Here we examine
how effective the data structure is by comparing the run time
of INTERACT with that of INTERACT\D which does not
employ the hashing data structure. Since data size is a deter-
mining factor for run time, we ﬁrst reorganize the 9 data sets
according to their sizes (approximated by N ∗ M - number
of features multiplied by number of instances without con-
sidering feature types such as nominal or continuous). Fig-
ure 2 shows the ratios using time of INTERACT as the base:
TIN T ERACT\D divided by TIN T ERACT . It shows that the
run time difference between INTERACT and INTERACT\D
is more pronounced for larger data sets.

Run time comparison. Table 4 records the run time for
each feature selection algorithm. Except for FOCUS, all al-
gorithms ﬁnished their runs within the given time. The algo-
rithms are ordered as ReliefF, FCBF, INTERACT, CFS and
FOCUS: ReliefF is the fastest, and FOCUS is the slowest if it
ﬁnishes the run within 3 hours.

IJCAI-07

1160

75.00
91.09
96.63
82.03
52.14
73.05
99.19
96.46
86.67
83.58

62.50
93.07
96.63
83.33
48.74
73.88
95.90
96.34
95.56
82.88

50
92.08
93.82
84.31
51.93
57.68
94.02
95.85
80.00
77.74

4/0

53.13
92.08
98.31
87.91
44.87
42.08
94.06
95.97
100.00
78.71

0.01+
0.72
0.05+
0.25
0.89
0.00+
0.00+
0.08
0.23

0.32
0.72
0.08
0.01-
0.05+
0.00+
0.00+
0.08
0.12

Acc

43.75
91.09
93.82
84.31
54.11
69.62
90.43
95.76
77.78
77.85

C4.5

0.00+
1.00
0.05+
0.17
0.08
0.00+
0.00+
0.03+
0.14

5/0
SVM

62.50
96.04
96.63
92.16
49.97
57.21
90.43
95.94
100.00
82.32

1.00
0.19
1.00
0.00-
0.37
0.00+
0.00+
0.02+
0.12

4/0

0.06
1.00
0.00+
0.31
0.10
0.00+
0.00+
0.00+
1.00

1.00
0.19
0.01+
0.00-
0.02+
0.00+
0.00+
0.00+
1.00

65.63
91.09
83.15
84.31
50.17
67.73
90.43
91.52
86.67
78.97

62.50
96.04
84.83
92.16
44.33
50.35
90.43
90.57
95.56
78.53

Acc

37.5
91.09
93.26
NA
52.27
73.05
NA
NA
NA
69.43

31.25
93.07
89.89
NA
48.68
73.88
NA
NA
NA
67.35

2/0

0.01+
1.00
0.05+
NA
0.17
1.00
NA
NA
NA

0.01+
1.00
0.04+
NA
0.34
1.00
NA
NA
NA

Acc

46.88
92.08
93.82
85.95
52.27
73.05
99.44
96.4
77.78
79.74

37.50
96.04
97.75
91.18
48.68
73.88
95.93
97.28
91.11
81.04

2/0

0.02+
0.78
0.05+
0.11
0.17
1.00
0.31
0.79
0.19

0.03+
0.19
0.17
0.00-
0.34
1.00
0.94
0.01-
0.39

lung-cancer
zoo
wine
soy-large
cmc
vehicle
kr-vs-kp
internet-ads
45×4026+2C
Average
Win/Loss

lung-cancer
zoo
wine
soy-large
cmc
vehicle
kr-vs-kp
internet-ads
45×4026+2C
Average
Win/Loss

Table 5: Accuracy of C4.5 and SVM on selected features: ‘Acc’ denotes 10-fold CV accuracy(%) and p-Val obtained from a
two-tailed t-Test. The symbols “+” and “-” respectively identify statistically signiﬁcant (at 0.05 level) if INTERACT wins over
or loses to the compared algorithm. NA denotes the result is not available.

Data Set

INTERACT

Acc

Acc

FCBF

p-val

CFS

p-val

Acc

ReliefF

p-val

FOCUS

p-val

WholeDS

p-val

3/1

3/1

5/1

2/0

1/2

5 Conclusions
We recognize the need to study feature interaction in sub-
set selection using some synthetic data sets, and propose to
search for interacting features employing c-contribution to
measure feature relevance. We investigated the key issues
hindering the use of consistency measures and developed IN-
TERACT that handles feature interaction and efﬁciently se-
lects relevant features.
It ranks features to overcome the
feature order problem.
INTERACT implements a special
hashing data structure to avoid repeated scanning of a data
set by taking advantage of the intrinsic properties of the c-
contribution measure, which results in a signiﬁcant speed-up
of the algorithm. The efﬁciency and effectiveness of INTER-
ACT are demonstrated through theoretical analysis as well as
extensive experiments comparing with representative feature
selection algorithms using the synthetic and real-world data
sets with known feature interactions. Experimental results
show that INTERACT can effectively reduce the number of
features, and maintain or improve predictive accuracy in deal-
ing with interacting features. This work presents promising
initial efforts on searching efﬁciently for interacting features.

References
[Alizadeh and et al., 2000] A. Alizadeh and et al. Distinct types of
diffuse large B-cell lymphoma identiﬁed by gene expression pro-
ﬁling. Nature, 403:503–511, 2000.

[Almuallim and Dietterich, 1994] H. Almuallim and T. G. Diet-
terich. Learning boolean concepts in the presence of many ir-
relevant features. Artiﬁcial Intelligence, 69(1-2):279–305, 1994.

[Bell and Wang, 2000] D. A. Bell and H. Wang. A formalism for
relevance and its application in feature subset selection. Machine
Learning, 41(2):175–195, 2000.

[Blake and Merz, 1998] C.L. Blake and C.J. Merz. UCI repository

of machine learning databases, 1998.

[Blum and Langley, 1997] A. L. Blum and P. Langley. Selection
of relevant features and examples in machine learning. Artiﬁcial
Intelligence, 97:245–271, 1997.

[Dash and Liu, 1997] M. Dash and H. Liu. Feature selection for
classiﬁcation. Intelligent Data Analysis: An International Jour-
nal, 1(3):131–156, 1997.

[Fleuret, 2004] Franois Fleuret. Fast binary feature selection with
conditional mutual information. J. Mach. Learn. Res., 5:1531–
1555, 2004.

[Gilad-Bachrach et al., 2004] Ran Gilad-Bachrach, Amir Navot,
and Naftali Tishby. Margin based feature selection - theory and
algorithms. In ICML 2004.

[Guyon and Elisseeff, 2003] I. Guyon and A. Elisseeff. An intro-
duction to variable and feature selection. J. Mach. Learn. Res.,
3:1157–1182, 2003.

[Hall, 2000] M. A. Hall. Correlation-based feature selection for dis-

crete and numeric class machine learning. In ICML, 2000.

[Irina Rish, 2001] Jayram Thathachar Irina Rish, Joseph L. Heller-
stein. An analysis of data characteristics that affect naive bayes
performance. Technical report, IBM T.J. Watson, 2001.

[Jakulin and Bratko, 2003] Aleks Jakulin and Ivan Bratko. Analyz-

ing attribute dependencies. In PKDD, 2003.

[Jakulin and Bratko, 2004] Aleks Jakulin and Ivan Bratko. Testing

the signiﬁcance of attribute interactions. In ICML, 2004.

[Jakulin, 2005] Aleks Jakulin. Machine Learning Based on At-

tribute Interactions. PhD thesis, University of Ljubljana, 2005.

[John et al., 1994] G. H. John, R. Kohavi, and K. Pﬂeger. Irrelevant

feature and the subset selection problem. In ICML, 1994.

[Kononenko, 1994] I. Kononenko. Estimating attributes : Analysis

and extension of RELIEF. In ECML, 1994.

[Robnik-Sikonja and Kononenko, 2003] M. Robnik-Sikonja and
I. Kononenko. Theoretical and empirical analysis of Relief and
ReliefF. Machine Learning, 53:23–69, 2003.

[Witten and Frank, 2005] I. H. Witten and E. Frank. Data Mining
- Pracitcal Machine Learning Tools and Techniques with JAVA
Implementations. Morgan Kaufmann Publishers, 2 edition, 2005.
[Yu and Liu, 2003] L. Yu and H. Liu. Feature selection for high-
In

dimensional data: a fast correlation-based ﬁlter solution.
ICML, 2003.

IJCAI-07

1161


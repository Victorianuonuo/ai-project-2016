Reinforcement Learning in POMDPs Without Resets

Eyal Even-Dar

Sham M. Kakade

Yishay Mansour

School of Computer Science

Computer and Information Science

School Computer Science

Tel-Aviv University
Tel-Aviv, Israel 69978
evend@post.tau.ac.il

University of Pennsylvania

Philadelphia, PA 19104

skakade@linc.cis.upenn.edu

Tel-Aviv University
Tel-Aviv, Israel 69978
mansour@post.tau.ac.il

Abstract

We consider the most realistic reinforcement learn-
ing setting in which an agent starts in an unknown
environment (the POMDP) and must follow one
continuous and uninterrupted chain of experience
with no access to “resets” or “ofﬂine” simula-
tion. We provide algorithms for general connected
POMDPs that obtain near optimal average reward.
One algorithm we present has a convergence rate
which depends exponentially on a certain horizon
time of an optimal policy, but has no dependence
on the number of (unobservable) states. The main
building block of our algorithms is an implemen-
tation of an approximate reset strategy, which we
show always exists in every POMDP. An interest-
ing aspect of our algorithms is how they use this
strategy when balancing exploration and exploita-
tion.

Introduction

1
We address the problem of lifelong learning in a partially ob-
servable Markov decision process (a POMDP). We consider
the most general setting where an agent begins in an unknown
POMDP and desires to obtain near optimal reward. In this
setting, the agent is forced to obey the dynamics of the envi-
ronment, which, in general, do not permit resets.

The problem of lifelong learning has been well studied
for observable MDPs. Kearns and Singh (1998) provide the
E3 algorithm, which has ﬁnite (polynomial) time guarantees
until the agent obtains near optimal reward. Unfortunately,
such an algorithm is not applicable in the more challenging
POMDP setting. In fact, none of the guarantees in the litera-
ture for learning in the limit for MDPs apply to POMDPs, for
reasons which are essentially due to the partially observabil-
ity.

For POMDPs, the problem of balancing exploitation with
exploration has received rather little attention in the literature
— typically most results in POMDPs are on planning (see for
example Sondik (1971); Lovejoy (1991a, 1991b); Hauskrecht
(1997); Cassandra (1998)). Most of the existing learning
algorithms such as Parr and Russell (1995); Peshkin et al.
(2000) either assume a goal state or assume a reset button.
In fact, to the best of our knowledge, the literature does not

contain even asymptotic results for general POMDPs which
guarantee that the average reward of an agent will be near
optimal in the limit.

Part of the technical difﬁculty is that there are currently no
general results for belief state tracking with an approximate
model showing that divergence in the belief state does not
eventually occur. Crudely, the issue is that if a belief state
is being tracked in an approximate manner, then it is impor-
tant to show that this approximation quality does not contin-
ually degrade with time — otherwise the agent will eventu-
ally loose track of the belief state in the inﬁnite horizon (this
of course is not an issue in an MDP where the current state
is observable). Boyen and Koller (1998) address the issue
of approximate belief state tracking, but in their setting the
model is known perfectly and their goal is to keep a compact
representation of the belief state. Note that approximate be-
lief state tracking is much simpler if the agent is only acting
over a ﬁxed ﬁnite horizon, since then one can bound the error
accumulation as a function of the horizon.

We present new algorithms for learning in POMDPs which
guarantee that the agent will obtain the optimal average re-
ward in the limit. Furthermore, we provide a ﬁnite time con-
vergence rates for one of our algorithms which has an expo-
nential dependence on a certain horizon time (of an optimal
strategy) but has no dependence on the number of states in
the POMDP. This result is reminiscent of the trajectory tree
algorithm of Kearns et al. (1999) which has similar depen-
dencies (though there they assumed access to a generative
model, which allowed simulation of the POMDP). Given the
plethora of complexity results in the literature on planning in
POMDPs (see Lusena et al.
(2001)), we feel these depen-
dencies are the best one could hope for in the most general
setting.

Central to our algorithms is the implementation of an ap-
proximate reset strategy or a homing strategy. The idea of a
reset strategy is not new to the literature — homing sequences
were used in the learning of deterministic ﬁnite automata (see
Rivest and Schapire (1993), though there the sequence pro-
vided exact resets). Here, the agent follows a homing strat-
egy in order to move approximately towards a reset. We show
that such a strategy always exists, and our ﬁnite convergence
rates also depend on a characteristic time it takes to approx-
imately reset. However, note that existence of such a strat-
egy alone does not imply that such a strategy will be useful.

The reason is that the agent must take actions to reset, which
might otherwise be better spent exploring or exploiting.
It
turns out that our algorithms use the homing strategy while
both exploring and exploiting. In fact, they use the homing
strategies inﬁnitely often, which, unfortunately, detracts from
exploiting. However, we are able to show that the ratio of the
time these homing strategies are used compared to the time
spent exploiting is decreasing sufﬁciently rapidly, such that
near optimal average reward can be obtained.

2 Preliminaries
A Partially Observable Markov Decision Process (POMDP)
is deﬁned by a ﬁnite set of states S, an initial state s0, a set
of actions A, a set of observations O, with an output model
Q, where Q(o, r|s, a) is the probability of observing o and
reward r after performing action a at state s (we assume that
r ∈ [0, 1]), and a set of transitions probabilities P , where
P (s0|a, s) is the transition probability from s to s0 after per-
forming action a. We deﬁne r(s, a) as the expected reward
under Q(·|s, a) after performing action a in state s.
actions,
i.e.

rewards
h =
and observations of some ﬁnite length,
{(a1, r1, o1), ..., (at, rt, ot)}.
A strategy or policy in
a POMDP is deﬁned as a mapping from histories to
actions. We deﬁne a belief state B to be a distribu-
tion over states.
Given an initial belief state B0 let
Pr[h|B0] = Pr[r1, o1. . . . , rt, ot|a1, . . . , at, B0] be the
probability of observing the sequence of reward-observations
(r1, o1, . . . rt, ot) after performing the actions a1 . . . at.

A history h is

sequence of

a

t (B).

(1/t)Eh∼π[Pt

For each strategy π we deﬁne its

pected reward from a belief state B as Rπ

t-horizon ex-
t (B) =
i=1 r(si, ai)|B0 = B]. A t-Markov strategy
is a strategy that depends only on the last t observations. The
optimal t-Markov strategy’s expected return from initial be-
lief state B is deﬁned as R∗
The only assumption we make is that the POMDP is con-
nected, i.e. for all states s, s0, there exists a strategy π which
can reach s0 with positive probability starting from s. (We do
not make any ergodicity assumptions, since strategies are by
deﬁnition non-stationary). Note that if the POMDP is discon-
nected, then the best statement we could hope for is to obtain
the optimal average reward for one of its connected compo-
nents.
Connectivity implies that there exists a strategy π∗ that
maximizes the average reward. More formally, there exists
a π∗ such that: i) for every B, limt→∞ Rπ∗
t (B) exists and
does not depend on B, which we denote by R∗, and ii) for all
π and B, R∗ ≥ limt→∞ sup Rπ
t (B). Hence, for all  > 0
there exist a τ , such that for all B and t ≥ τ :

|R∗ − Rπ∗

t (B)| ≤ 

and we refer to τ as the -horizon time of the optimal strategy.
Essentially, τ is the timescale in which the optimal strategy
achieves close to its average reward.

When we say that we restart a t-Markov strategy π from
a belief state B we mean speciﬁcally that we reset the his-
tory, i.e., h = ∅, and run π starting from a state s distributed
according to B.

3 Homing Strategies

Clearly, having an action which resets the agent to some des-
ignated state would be useful, as it would allow us to test and
compare the performance of various policies, starting at the
same start state. However, in general, such an action is not at
our disposal.

Instead our algorithms utilize an approximate reset, which
we show always exists. There are a few subtle points when
designing such a reset. First, we must select actions to
achieve the approximate reset, i.e.
the approximate reset is
done through the use of a homing strategy. Hence, while
homing, the agent is neither exploring nor exploiting. Sec-
ond, rather than moving to a ﬁxed state, the homing strategy
can only hope to move to toward a ﬁxed (unknown) belief
state. Third, as we shall see, since the POMDP might be
periodic the stopping time must be a random variable1. To
implement this randomized stopping time, we introduce ﬁc-
titious ’stay’ actions, in which the agent does not take an ac-
tion that period. By this, we mean that if the homing strategy
decides to take a ’stay’ action at some time — which may not
be possible if the true POMDP does not permit ’stay’ actions
— then the agent just ignores this ’stay’ action and obtains
another action from the homing strategy to execute. Hence,
after the agent has taken t homing actions (which are either
real or ’stay’ actions), the agent has taken t − m real actions
in the POMDP and m stay actions. We now deﬁne an approx-
imate reset strategy.

Deﬁnition 3.1 H is an (, k)-approximate reset (or homing)
strategy if for every two belief states B1 and B2, we have
kHE(B1) − HE(B2)k1 ≤ , where HE(B) is the expected
belief state reached from B after k homing actions of H (so at
most k real actions have been taken) and H(B) is a random
variable such that HE(B) = EH(B)∼H,B[H(B)].

The above deﬁnition only states that H will approximately
reset, but this approximation quality could be poor. We now
show how to amplify the accuracy of an approximate hom-
ing strategy, and then we show that an approximate homing
strategy always exists.

Lemma 3.1 Suppose that H is an (, k) approximate reset
then H ‘ is an (‘, k‘) approximate reset, where H ‘ consec-
utively implements H for ‘ times. Furthermore, this implies
there exists a unique belief state BH such that HE(BH) =
BH.

Proof: The proof is a standard contraction argument, and
we use induction. For l = 1, the claim follows by deﬁnition.
Assume now that kH ‘−1
E (B2)k1 ≤ ‘−1. Let
s |Q1(s) −
H l−1
E (B1) = Q1 and H l−1
Q2(s)| ≤ ‘−1. For an arbitrary state s0, and using the fact

E (B2) = Q2, so P

E (B1) − H ‘−1

1Without randomizing over the stopping times (i.e. allowing
’stay’ actions), the state transition matrix may be periodic and no
stationary distribution may exist, e.g. if the states deterministically
alternate between states 1 and 2.

that H is a linear operator, we have

kH ‘

E(B1) − H ‘
= kHE(Q1) − HE(Q2)]k1

E(B2)]k1

s

= kX
= kX
+kX
= 0 +X

s

s

(Q1(s) − Q2(s))HE(s)k1
(Q1(s) − Q2(s))HE(s0)k1
(Q1(s) − Q2(s))(HE(s) − HE(s0))k1
|Q1(s) − Q2(s)|

s

≤ ‘

P
where the ﬁrst term is 0 since for any two distributions
s Q1(s) − Q2(s) = 1 − 1 = 0 (and the vector H(s0)
is constant in this sum), and we have used the fact that
kH(s) − H(s0)k1 ≤  (by deﬁnition of H).
(cid:3)
We now show that the random walk strategy (includ-
ing ’stay’ actions) is an approximate reset strategy in every
POMDP (including periodic ones), though with prior knowl-
edge we might have better approximate reset strategies at our
disposal.
Lemma 3.2 For all POMDPs, the random walk strategy (us-
ing ’stay’ actions) constitutes an (, k) approximate reset
strategy for some k ≥ 1 and 0 <  < 1
2 .

Proof: By our connectivity assumption, for all states s and
s0, there exists some strategy that reaches s0 from s with pos-
itive probability. This implies that under H (the random walk
strategy), there is positive probability of moving from one
state to another, i.e.
the Markov chain is irreducible. Fur-
thermore, since H performs ’stay’ actions, then the Markov
chain is aperiodic. Thus, there exists a unique stationary dis-
tribution. We choose k to be the time at which the error in
convergence is less than 1/2, from all starting states. Hence,
by linearity of expectation, the error is less than 1/2 from all
(cid:3)
belief states in k steps.

4 Reinforcement Learning with Homing
We now provide two algorithms which demonstrate how
near-optimal average reward can be obtained, with different
rates of convergence. The key to the success of these algo-
rithms is their use of homing sequences in both exploration
and in exploitation. For exploration, the idea is that each
time we attempt some exploration trajectory we do it after
implementing our reset strategy — hence our information is
(approximately) grounded with respect to the belief state BH
(recall HE(BH) = BH). The idea of exploration is to ﬁnd
a good t-Markov strategy ˆπ∗
t from BH. During exploitation,
the goal is to use this t-Markov strategy. Unfortunately, we
have only guaranteed that ˆπ∗
t performs well starting from BH
and only for t steps. Hence, after each time we exploit with
ˆπ∗
T , we run our homing sequence to get back close to BH (and
then we rerun ˆπ∗

t ). We gradually increase t in the process.

The problem is that while homing, we are wasting time and
neither exploiting nor exploring. Furthermore, since we use

: H /*a (1/2, KH) approximate reset strategy */

Input
for t = 1 to ∞ do

(cid:17)

(cid:16) 1

/*Exploration in Phase t */;
1 = O
kt
;
foreach Policy π in Πt do

log(t2|Πt|)

2
t

for i = 1 to kt

1 do

Run π for t steps;
Repeatedly run H for log(1/t) times;

end
Let vπ be the average return of π in from these
kt
1 trials;

end
/*Exploitation in Phase t */;
Let ˆπ∗
t = arg maxπ∈Πt vπ;
2 = O
([current time T]
kt

(cid:16) 1

t
+[time in t + 1-th exploration phase])

for i = 1 to kt

2 do

Run ˆπ∗
Repeatedly run H for log(1/t) times;

t for t steps;

(cid:17)

;

end

end

Algorithm 1: Policy Search

the homing sequence between every run of ˆπ∗
t , asymptotically
we never stop homing. Nonetheless, we able to show that
there exists an algorithm which obtains near optimal reward
in a POMDP, since the ratio of the time spent exploiting vs.
homing decreases sufﬁciently fast.
Theorem 4.1 There exists an algorithm A, such that in any
connected POMDP, A obtains the optimal average reward in
the limit, with probability 1.

We later provide an algorithm with a better convergence
rate (see Theorem 4.5). However, we start with a simpler pol-
icy search algorithm which establishes the above Theorem.

4.1 Policy Search
Algorithm 1 takes as input a (1/2, KH)-approximate reset
strategy, which could be the random walk strategy with a
very crude reset. The algorithm works in phases, interleav-
ing exploration phases with exploitation phases. Let us start
by describing the exploration phases. Let Πt be the set of
all t-Markov strategies. An estimate of the value of a policy
π ∈ Πt can be found by ﬁrst resetting and then running π for t
steps. The exploration phase consists of obtaining an estimate
vπ of the return of each policy π ∈ Πt, where each estimate
consists of an average of k1 trials (followed by approximate
resets).

These estimates have both bias and variance. The variance
is just due to the stochastic nature of the POMDP. The bias is
due to the fact that we never can exactly reset to BH. How-
ever, if we run H for log 1/t times (where t is an error pa-
rameter in the t-th phase, which will be ﬁxed latter) then, by
lemma 3.1, all expected belief states we could approach will
be (1/2)log 1/t = t close to BH. The following lemma
shows that accurate estimates can be obtained.

(cid:16) 1

(cid:17)

log(t2|Πt|)

1 = O

Lemma 4.2 In phase t, if kt
and each
reset consists of using the homing sequence log 1/t times,
then for all policies π ∈ Πt, the estimated t-horizon reward,
vπ, satisﬁes |Rπ
t (BH) − vπ| ≤ 2t with probability greater
than 1 − 1
2t2 .

2
t

Hk1 ≤ . This directly implies that |Rπ

Proof: First, let us deal with the bias. Any expected belief
states b that results from using the homing sequence log 1/t
times must satisfy kb − BHk ≤ (1/2)log 1/t ≤ t. Now it is
straightforward to see that if b and BH are belief states such
that kb − BHk1 ≤ , then for every strategy π, |Rπ
t (b) −
t (BH)| ≤ . To see this, let the belief states at time t be bt
Rπ
and Bt
H, which result from following either π starting from
b or BH, respectively. By linearity of expectation, it follows
t (b) −
that kbt − Bt
t (BH)| ≤ .
Rπ
For the variance, the Hoeffding bound and our choice of kt
1
imply that the average return of each policy is t close to its
expectation (the expectation is both over the initial state and
on the policy trajectory) with probability 1 − 1/(2t2).
(cid:3)
Now during exploitation, the algorithm uses the policy ˆπ∗
t
which had the highest return in the exploration phase (and by
the previous lemma this is close to the policy with largest re-
turn). Note that we have only guaranteed a large return for
executing ˆπ∗
t from BH for t steps. However, we would like to
exploit for a longer period of time than t. The key is that we
again reset log(1/t) times after each time we run ˆπ∗
t , which
resets us close t close to BH. Unfortunately, this means we
spend KH log 1/t steps between each run of ˆπ∗. Hence, our
t KH log 1/t) less than we would
average return could be O( 1
like, since O( 1
t KH log 1/t) is the fraction of time we spend
resetting. Note that this fraction could be large if we desire
that t be very small (thought this would guarantee very ac-
curate resets).

Now, when we do exploit (and reset), we run the exploita-
tion phase long enough (for kt
2 time) such that our overall
average reward is comparable to the average reward in the
last exploitation phase.
Lemma 4.3 At any time T after phase t, the average reward
t (BH) −
from time 1 to time T satisﬁes: 1
T
O(t + 1

PT
i=1 ri ≥ R∗
PT
i=1 ri ≥ R∗
) with probability at least 1 − 1
t2 .

which Theorem 4.1 follows.
Corollary 4.4 Let t = 1/t. Then 1
T
O( KH log(t)

t KH log(1/t)) with probability at least 1 − 1
t2 .

Before proving this lemma, let us state a corollary from

t (BH) −

t

Importantly, note the loss term goes to 0 as t goes to inﬁn-
ity. Furthermore, for a large enough phase t, we know that
t (BH) will approach the optimal average reward R∗ (since
R∗
R∗ is independent of the starting B). Theorem 4.1 follows.
Essentially, although we have to home inﬁnitely often, the ra-
tio of time spent homing to the time spent using our t-step
exploitation policies is going as O( log t
let us show that

the average reward,
t (BH), is no less than t from the average
1
T
reward obtained in the t-th exploitation phase. To do this, we
2, to be 1/t times greater
set the time of exploitation phase, kt

PT
First,
i=1 ri ≥ R∗

t ), which goes to 0.

Proof:

than previous amount of time spent in the MDP time plus the
amount of time that will be spent in the next exploration phase
(this latter factor accounts for the case in which time T lies in
the exploration phase immediately after t).

t

t

t

, satisﬁes R∗

t (BH) − Rˆπ∗

Now we bound the average reward obtained in the exploita-
tion phase. First. let us show that the t-average reward of the
policy used, Rˆπ∗
t (BH) ≤ 4t with
probability at least 1 − 1
2t2 . By Lemma 4.2 for each policy
π ∈ Πt, we have |Rπ
t (BH) − vπ| ≤ 2t with probability at
least 1 − 1
t is 4t-optimal with probabil-
ity 1− 1/(2t2). Now the observed average return of ˆπ∗
t in the
exploitation period is 2t close to R∗
t (BH) with probability at
least 1− 1
2t2 , since our observed average return in exploitation
is least as good as those used to ﬁnd v ˆπ∗

2|Πt|t2 . Therefore, ˆπ∗

t (since kt

2 > kt

1).

the observed average return of ˆπ∗

However, the average return during the exploitation phase
t , since we re-
is not
set after each t exploitation steps for a number of steps
that is KH log(1/t). The resets in the exploitation pe-
riod can change the average reward by at most a fraction
(cid:3)
t KH log(1/t).
1

4.2 A Model Based Algorithm
The previous algorithm was the simplest way to demonstrate
Theorem 4.1. However, it is very inefﬁcient, since it is testing
all t-Markov policies — there are doubly exponential, in t,
such polices2. Here, we provide a more efﬁcient model based
algorithm, which resembles the algorithms given in Kearns
et al. (1999); McAllester and Singh (1999), and is exponen-
tial in the horizon time, yet it still has no dependence on the
number of states in the POMDP.

We now state a convergence rate in terms of τ , the -
horizon time of an optimal policy (see Section 2) and and
in terms of the homing time KH (recall, such a time exists
for every POMDP using a random walk policy).
Theorem 4.5 There exists an algorithm A, such that in any
connected POMDP and with probability greater than 1 − δ,
A achieves an average reward that is 2 close to the optimal
average reward in a number of steps in the POMDP which is
polynomial in |A|,|O|,KH and log(1/δ) and exponential in
τ . Furthermore the computational runtime of this algorithm
is polynomial in |A|,|O|, and log(1/δ) and exponential τ .

We provide such an algorithm in the next page. In explo-
ration phase, the algorithm builds an approximate model of
the transition probabilities after some history has occurred
starting from BH. In the t-th phase, it builds a model with
respect to the set of all t-length histories, which we denote by
Ht. In the exploitation phase, it uses the best t Markov strat-
egy with respect to this model. The use of homing strategies
is similar to that in the previous algorithm.

Let L = |A||O|, and note that 2Lt ≥ |Ht|.

In the ex-
ploration phase, the algorithm takes actions uniformly at ran-
dom for t steps and then resets (running the homing strategy

2The number of histories of length t is exponential in t, and the
number of t-Markov polices is exponential in the number of t-length
histories

: H /*an (1/2, KH) approximate reset strategy */

Input
Let L = |A| · |O|;
for t = 1 to ∞ do

(cid:16) L4t

1 = O
kt
for kt

log(t2|Ht|)

;

2
t

(cid:17)

1 times do
Run RANDOM for t steps;
Run H for KH log(Lt/t) steps.

end
for h ∈ Ht, a ∈ A and o ∈ O do

ˆPr[o|h, B0, a] = 0;
if ˆPr[h(a, o)|B0] ≥ t

Lt then
ˆPr[o|h, B0, a] = ˆPr[h(a,o)|B0]
ˆPr[h|B0] ˆPr[a]

end

(cid:16) 1

end
Compute ˆπ∗
2 = O
kt

t using ˆPr[o|B0, h, a];
([current time T]

t
+[time in t + 1-th exploration phase])

(cid:17)

;

for kt

2 times do
Run ˆπ∗
Run H for KHlog(1/t) steps;

t for t steps;

end

end

Algorithm 2: Model based

for log(Lt/t) times). This is done kt
1 times.3 Then using
the empirical frequencies in these trajectories the algorithm
forms estimates ˆPr[o|h, BH , a], which is just the empirical
probability of observing o conditioned on history h followed
by taking action a. For histories h which are unlikely, these
empirical estimates could be very bad, though, as we shall
see, we do not need accurate estimates of ˆPr[o|h, BH , a] for
such histories. Let h(a, o) be a history with h followed by
(a, o).

(cid:16) L4t

(cid:17)

log(t2|Ht|)

2
t

1 = O

Lemma 4.6 In phase t, if kt
and
each reset consists of using the homing sequence log(Lt/t)
times, then: (1) | ˆPr[h|B0] − Pr[h|B0]| ≤ t
L2t , and (2) for
every h(a, o) ∈ Ht such that Pr[h(a, o)|B0] ≥ t
Lt , we have
| ˆPr[o|h, B0, a] − Pr[o|h, B0, a]| ≤ 2|A|
Lt , with probability at
least 1 − 1
t2 .

Proof: We ﬁrst note that, with probability 1 − 1/t2, for
every history h ∈ Ht we have | ˆPr[h|B0] − Pr[h|B0]| ≤ t
(using the Hoeffding bound). The error, | Pr[o|h, B0, a] −

L2t

3We can use in the algorithm any approximate homing strategy
H. However, if H is simply the random policy, then the reset and
exploration would both use the same policy, and the algorithm would
slightly simplify.

ˆPr[o|h, B0, a]|, is then

− ˆPr[h(a, o)|B0]
ˆPr[h|B0] ˆPr[a]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Pr[h(a, o)|B0]
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)Pr[h(a, o)|B0] + t
(cid:12)(cid:12)(cid:12)(cid:12)

Pr[h|B0] ˆPr[a]
≤

Pr[h|B0] − t

Pr[h|B0] − t

ˆPr[a]

t
L2t

L2t

+

L2t

1

(cid:12)(cid:12)(cid:12)(cid:12)

− Pr[h(a, o)|B0]
Pr[h|B0]
2 Pr[h(a, o)|B0] t

(cid:12)(cid:12)(cid:12)(cid:12)

L2t

L2t

Pr[h|B0](Pr[h|B0] − t
L2t )

1
=
ˆPr[a]
≤ 2|A|
Lt ,
where the ﬁrst inequality holds with probability 1 − 1
in the last inequality we used the fact that Pr[h|B0] ≥ t

t2 , and
Lt . (cid:3)
The exploitation policy can be found using dynamic pro-
gramming with the model. Note that the POMDP is equiva-
lent to an MDP where the histories are states. In the exploita-
tion phase, the algorithm uses the best t-Markov policy, ˆπ∗
t ,
(with respect to the approximate model) interleaving it with
KH log(1/t) homing steps.
Lemma 4.7 In phase t, the exploitation policy ˆπ∗
|R∗
probability at least 1 − 1
t2 .

t (BH)| ≤ t(t + 2|A|

t , satisﬁes
Lt ) with

t (BH) − Rˆπ∗

Lt ) + (2t + 2t

t

Lt + t

t has return not less than k2(t + 2|A|

Proof: (sketch) We observe that by ignoring all histories
(which we view as nodes in a tree) such that ˆPr(h|B0) ≤ t
Lt ,
the return of an optimal strategy in this empirical model is
Lt ), due to the fact that the true
decreased by at most 2t(t + t
history probability is bounded by t
L2t , the return from
each node is bounded by t and the total number of such nodes
is bounded by 2Lt. Next we prove that the return of the opti-
mal policy in the empirical model loses at most t2(t + 2|A|
Lt )
due to the tree approximation on the other nodes (the other
histories). Using backward induction, we show that the pol-
icy ˆπ∗
Lt ) in comparison to
the true optimal value, starting from (t − k + 1)-length his-
tories. The base case, for the leaves (the t-length histories),
holds since the reward (which is encoded through the obser-
vations) is within t + 2|A|
Lt , where the ﬁrst error is due to the
imperfect reset and the second is due to the marginal distribu-
tion error that is bounded by 2|A|
Lt by Lemma 4.6. Assume the
induction assumption holds for k − 1. There are two sources
of error, the ﬁrst is due to the current estimation error (of both
the marginal distribution and the immediate reward) which is
bounded by (t + 2|A|
Lt )k and the second is due to errors from
the previous levels and is bounded by (k − 1)2(t + 2|A|
Lt ) by
the induction assumption. Summing the terms completes the
(cid:3)
induction step.
Similarly to Subsection 4.1, we exploit long enough such
that the overall average reward is essentially the average re-
ward in the last exploitation period.
Lemma 4.8 At any time T after phase t, the average reward
t (BH) −
from time 1 to time T satisﬁes: 1
T
O(tt + |A|t
Lt + (1/t)KH log(1/t)) with probability at least
1 − 1
t2 .

PT
i=1 ri ≥ R∗

R. Parr and S. Russell. Approximating optimal policies for
partially observable stochastic domains.
In Proceedings
of the International Joint Conference on Artiﬁcial Intelli-
gence, 1995.

L. Peshkin, K. Kim, N. Meuleau, and L.P. Kaelbling. Learn-
ing to cooperate via policy search. In 16th Proceedings of
UAI, pages 307–314, 2000.

R. Rivest and R. Schapire. Inference of ﬁnite automata using
homing sequences. Information and Computation, 103(2):
299–347, 1993.

E. Sondik. The optimal control of partially observable pro-
cesses over a ﬁnite horizon. PhD thesis, Stanford Univer-
sity, Stanford, California, 1971.

Using the above lemma, Theorem 4.5 follows immediately

if we set t = 1/t2.

t (BH) − Rˆπ∗

t (BH)| ≤ t(t + 2|A|

Proof: (sketch) We ﬁrst note that all exploitations and ex-
plorations from phases 1 to T and from the next, (T + 1)-th,
exploration phase can effect the average reward by at most
t. By Lemma 4.7, the exploitation policy is near optimal and
satisﬁes, |R∗
Lt )
Lt ) + (t + t
with probability 1 − 1/(2t2). As in Lemma 4.2, we observe
that the bias of the exploitation policy is t and the variance
due to Hoeffding’s bound and the large exploitation time is
bounded by t with probability 1 − 1/(2t2). The last source
for loss is the resets in the exploitation period and its effect
(cid:3)
can be bounded by KH log(1/t)
A direct and simple corollary from which Theorem 4.1 fol-

lows as well.
Corollary 4.9 Let t = 1/t. Then 1
T
O( KH log(t)

PT
i=1 ri ≥ R∗
) with probability at least 1 − 1
t2 .

t

t (BH) −

t

t

.

Acknowledgements
This work was supported in part by the IST Programme of
the European Community, under the PASCAL Network of
Excellence, IST-2002-506778, and by a grant from the Israel
Science Foundation and an IBM faculty award. This publica-
tion only reﬂects the authors’ views.

References
X. Boyen and D. Koller. Tractable inference for complex

stochastic processes. In UAI, 1998.

A. Cassandra. Exact and approximate algorithms for par-
tially observable markov decision processes. PhD thesis,
Brown University, 1998.

M. Hauskrecht. A heuristic variable-grid solution method for

pomdps. In AAAI-97, pages 734–739, 1997.

M. Kearns, Y. Mansour, and A. Ng. Approximate planning in
large pomdps via reusable trajectories. In NIPS 12, 1999.
M. Kearns and S. Singh. Near-optimal reinforcement learning

in polynomial time. Proceedings of ICML, 1998.

W. S. Lovejoy. Computationally feasible bounds for par-
tially observed markov decision processes. Operations Re-
search, 39(1):162–175, 1991a.

W. S. Lovejoy. A survey of algorithmic methods for partially
observed markov decision processes. Annals of Operations
Research, 28:47–66, 1991b.

C. Lusena, J. Goldsmith, and M. Mundhenk. Nonapprox-
imability results for partially observable markov decision
processes. Journal of Artiﬁcial Intelligence Research, 14:
83–103, 2001.

D. McAllester and S. Singh. Approximate planning for
factored pomdps using belief state simpliﬁcation.
In In
Proceedings of the Fifteenth Annual Conference on Un-
certainty in Artiﬁcial Intelligence (UAI), pages 409–416,
1999.


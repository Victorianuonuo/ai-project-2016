Information Extraction from Web Documents Based on Local Unranked Tree 

Automaton Inference 

Raymond Kosala1, Maurice Bruynooghe1, Jan Van den Bussche2, Hendrik Blocked1 

1 K.U.Leuven, Dept. of Computer Science, Celestijnenlaan 200A, B-3001  Leuven 

{raymond,maurice,hendrik}@cs.kuleuven.ac.be 

2 University of Limburg, Dept.  WNI, Universitaire Campus, B-3590 Diepenbcek 

jan.vandenbussche@luc.ac.bea 

Abstract 

Information extraction (IE) aims at extracting spe(cid:173)
cific  information  from  a  collection  of documents. 
A  lot of previous work on 10 from semi-structured 
documents (in XML or HTML) uses learning tech(cid:173)
niques  based  on  strings.  Some  recent work  con(cid:173)
verts the document to  a ranked tree and  uses tree 
automaton induction.  This paper introduces an al(cid:173)
gorithm that uses unranked trees to induce an auto(cid:173)
maton.  Experiments show that this  gives the best 
results obtained so  far for  IE from semi-structured 
documents based on learning. 

Introduction 

1 
Information extraction aims at extracting specific information 
from a collection of documents. One can distinguish between 
IE from unstructured and from (semi-) structured texts [Mus-
lea,  1999].  Extracting information from web documents be(cid:173)
longs to the latter category and gains importance [Levy et al, 
1998].  These documents are not written in natural language, 
but rather involve explicit annotations such  as  HTML/XML 
tags  to  convey the  structure  of the  information,  making the 
methods tuned towards natural language unusable. 

While  special  query  languages  exist  [Bry  and  Schaffert, 
2002;  XQL,  2002],  their  use  is  time  consuming  and  re(cid:173)
quires  nontrivial  skill.  As  argued  in  [Muslea  et  al,  2001; 
Kushmerick, 2000], there is a need for systems that learn to 
extract  information  from  a  few  annotated  examples  {wrap(cid:173)
per  induction).  Several  machine  learning techniques  for  in(cid:173)
ducing wrappers have been proposed.  Examples are multi-
strategy approaches [Freitag, 2000] and various grammatical 
inference  techniques  that  induce  a  kind  of delimiter-based 
patterns [Muslea et al,  2001;  Freitag and McCallum,  1999; 
Freitag  and  Kushmerick,  2000;  Soderland,  1999;  Freitag, 
1997;  Hsu and  Dung,  1998; Chidlovskii  et a/.,  2000].  All 
these methods treat the document as a string of characters. 
Structured  documents  such  as  HTML  and  XML  docu(cid:173)
ments,  however,  have  an  explicit tree  structure.  In  [Kosala 
et  al,  2002b; 2002a], it is argued that one can better exploit 
this tree structure and use tree automata [Comon et  al,  1999]. 
The document tree is converted in a ranked binary tree and k-
testable  tree  automata  [Rico-Juan  et  al.,  2000]  are  induced 
and then used for the extraction task.  Typically in a IE task 

from structured documents,  there  is  some structural  context 
close to the target. After linearisation in a string, this context 
can be arbitrarily far away, making the learning task very dif(cid:173)
ficult for string based methods.  While binarisation may also 
increase the distance between the context and the target, they 
remain  closer,  and the  learning task  should be  easier.  This 
is  confirmed  by  the  experiments  in  [Kosala  et  al,  2002b; 
2002a]. 
If  distance  between  the  relevant  context  and  the 
target  is  indeed  a  main  factor  determining  the  ability  to 
learn an appropriate automaton, then an algorithm inducing 
a  wrapper  directly  from  the  unranked  tree  should  perform 
even  better.  This  path  is  pursued  in  the  current paper.  As 
in  [Kosala  et al. , 2002b;  2002a]  user  intervention  is  lim(cid:173)
ited  to  annotating  the  field  to  be  extracted  in  a  few  rep(cid:173)
resentative  examples.  String  based  methods  require  sub(cid:173)
stantially  more  user  intervention,  such  as  splitting the  doc(cid:173)
ument  into  small  fragments,  and  selecting  some  of  them 
for  use  as  a  training  example,  e.g.  [Soderland,  1999];  the 
manual  specification  of the  length  of a  window  for the  pre(cid:173)
fix, suffix and target fragments [Freitag and McCallum, 1999; 
Freitag and Kushmerick, 2000], and of the special tokens or 
landmarks such as  ''>"  or''  [Freitag and Kushmerick, 2000; 
Muslea et al., 2001]. 

The  rest  of the  paper is  organized as  follows.  Section  2 
provides  some  background  on  unranked  tree  automata  and 
their  use  for  IE.  Section  3  describes  our  methodology  and 
introduces our unranked tree inference algorithm. Results are 
described in Section 4, related work in Section 5.  Section 6 
concludes. 

inference  and 

2  Preliminaries 
Grammatical 
information  extraction. 
Grammatical  inference  (also  called  automata  induction, 
grammar induction, or automatic language acquisition) refers 
to  the  process  of  learning  rules  from  a  set  of  labeled  ex(cid:173)
amples.  The  target  domain  is  a  formal  language  (a  set  of 
strings  over  some  alphabet 
and  the  hypothesis space  is 
a  family  of grammars.  The  inference  process  aims  at find(cid:173)
ing a minimum automaton (the canonical automaton) that is 
compatible with the examples.  There is a large body of work 
on grammar induction, for a survey see e.g.  [Murphy,  1996; 
Parekh and Honavar, 1998]. 

In grammar induction, we have a  finite  alphabet 
.  Given a set of examples in 

formal language 

and a 

INFORMATION  EXTRACTION 

403 

field of interest.  E.g., the text "Organization:" may be relev(cid:173)
ant for the extraction, hence this text should not be changed 
into CDATA. In each data set, at most one text field is identi(cid:173)
fied as distinguishing context. It is found automatically using 
the approach described in [Kosala et al., 2002b]. 

and rejects those in 

and  a  (possibly  empty)  set  of examples  not  in 
, the 
task is to infer a deterministic finite automaton (DFA) that ac(cid:173)
cepts the examples in 
. In [Freitag, 
1997], an IE task is mapped into a grammar induction task. 
A document is converted into a sequence of tokens (from 
Examples are transformed by  replacing the  token to be ex(cid:173)
tracted by the special  token x.  Then a DFA  is inferred that 
accepts all  the transformed examples.  In our case,  a docu(cid:173)
ment is converted into a tree and the token to be extracted (at 
a leaf) is replaced by the special token x.  Then a tree auto(cid:173)
maton is  inferred that accepts all the transformed examples. 
When using the learned automaton, a similar transformation 
is  done.  Each  token that  is  a candidate  for extraction  is  in 
turn  replaced by x.  The token replaced by x  is extracted  iff 
the transformed document is accepted by the automaton. 

Unranked  tree  automata.  Some  existing  algorithms  for 
string automaton induction have been upgraded to ranked tree 
automaton  induction  (e.g.  [Rico-Juan et al,  2000;  Abe  and 
Mamitsuka,  1997])1.  By  converting documents  (which  are 
unranked)  into  binary  trees  (which  are  ranked),  tree  auto(cid:173)
maton  induction can  be  used  for IE as shown in  [Kosala  et 
al,  2002b].  The  present  work  avoids binarisation  and  uses 
unranked trees.  Unranked tree  automata have been studied 
since the late 60's, see e.g.  [Bruggemann-Klein  et al, 2001] 
for a survey. To our knowledge, algorithms for inducing them 
do not yet exist. This paper is a first step in this direction. 

An unranked label  is  a  label  with a  variable rank (arity). 
Thus the number of children is not fixed by the label.  Given 
a set V of labels in an unranked alphabet, we can define 
, 
the set of all (unranked) trees, as follows: 

•  a is a tree where 
•  a(u1,..., 

un)  is  a  tree,  where 

tree. 

and each 

is a 

An  unranked  tree  automaton  (UTA)  is  a  quadruple 
,  where V  is a  set of unranked labels,  Q  is a 
is a set of final (accepting) states, 
is  a  set of transitions  where each  transition is  of the 
and e is a regular 

finite set of states, 
and 
form 
expression over Q. 

A bottom up UTA processes trees bottom up.  When a leaf 
node is labeled v and there is a transition 
such that 
e  matches the empty string,  then the node  is assigned state 
q.  When an internal node is labeled v, its children have been 
assigned states  qi,..., qn,  and  there  is  a  transition 
such that the string 
matches the regular expression 
e, then the node is assigned state q.  A tree is accepted if the 
state of its root is assigned an accepting state 

3  Approach and algorithm 
Preprocessing.  Fig.  1  shows  a  representative  task.  For 
dealing  with text  nodes,  we  follow  the  approach  described 
in  [Kosala  et  al.,  2002b]:  we  replace  most  text  nodes  by 
CDATA2,  making  an  exception  for  so-called  distinguishing 
context, specific text that is useful for the identification of the 

'Ranked: the label determines the number of children. 
2 See Fig. 2 for an example. 

Figure  1:  The fields to be extracted are the  fields  following 
the  A l t . N a me  and  O r g a n i z a t i on  fields.  A  document 
consists of a variable number of records.  The number of oc(cid:173)
currences of the  fields  to be extracted is variable (from zero 
to many). Also the position is not fixed. 

Figure 2:  The left figure is an HTML tree.  The right one  is 
the same tree after abstracting the text nodes 

Approach.  Our approach for information extraction has the 
following characteristics: 

•  Strings stored at the text nodes are treated as single la(cid:173)

bels.  If extracted, the whole string is returned. 

•  One automaton is learned for one type of field to be ex(cid:173)

tracted, e.g., the field following "Organization". 

•  In examples used during learning, one target field (a text 
node) is replaced by x.  A document gives rise to several 
examples when several targets occur. 

The learning phase proceeds as follows: 

•  Replace  in  the  examples  the  target by  "a:",  the  distin(cid:173)
and all  other 

guishing  context(s)  (if present)  by 
text fields by CDATA. 

•  Map examples to trees and learn a tree automaton. 

The extraction phase repeats for all candidate targets: 

•  Map  the  document to  a tree  and replace  the  candidate 
target by  uz",  the  distinguishing context(s) (if present) 

404 

INFORMATION  EXTRACTION 

by "ctx" and all  other text fields by  CDATA. 

• Run the tree automaton; if the tree is accepted, then out(cid:173)

put the original text of the node labeled with x. 

3.1 

Local unranked tree  automaton inference 
algorithm 

Preliminaries.  A  partition  of  a  set  5  is  a  set  of  dis(cid:173)
the  union  of 
joint  nonempty  subsets  of  S  such 
the  subsets 
is  defined 
a

The  height  of  a  tree 
+ 

is  S. 
s

that 

:1 

before these steps, the node labels of each input tree t are re(cid:173)
written  using  the  function  convert Jabels(t).  This  function 
(Algorithm 2) rewrites node labels.  The label v of the root of 
a subtree is changed into vx  if that subtree contains a " x" and 
into vctx if that subtree contains a "ctx". The effect is that the 
special labels are remembered up to the root3. 

then 
the  ti  are called subtrees oft, and subtrees of ti are also sub(cid:173)
trees of t.  When a tree t is "cut off" at level k, this means all 
subtrees at level k become leaves. Thus the height of the cut-
off tree can be at most k. Given a tree t, the set of roots 
is the singleton containing t cut off at A;; the set of forks 
contains all subtrees of t of height at least k, cut off at k\ and 
the set ofsubtrees 
contains all  subtrees oft  of height at 
most k.  Roughly, these sets respectively collect all subtrees 
of height k at the top,  in the middle, and at the bottom oft. 
See [Rico-Juan et al., 2000] for a formal definition. 

Next,  the  states  are  collected  (the  1-root,  the  1-subtrees, 
and the  1-roots of the  forks)  in  Q,  the transitions are  initial(cid:173)
ized  with  one  transition  for  each  1-subtree  and  the  2-forks 
are partitioned according to the label of the forks' root.  The 
latter  results  in  a  set  of pairs  (v, Str)  with  Str  a  set  of se(cid:173)
quences,  each  sequence  being  the  children of a  fork.  E.g., 
{a, {(b,c), (b,c, c)}}  represents two forks with root label a. 
In  the  second  for-loop,  the  k.contextual  algorithm  (Al(cid:173)
gorithm 3) [Muggleton, 1990; Ahonen, 1996] is used to learn 
a deterministic finite automaton (DFA) that can be used as the 
representation of the regular expressions e  to be  used in the 
transitions v(e)  ->  q of the UTA. As an illustration, consider 
an input string ab for k  =  3.  The value of 
ind 
one  obtains 
-¥ 
, where #  is a distinguished label that is not 
in 
.  It is capable of identifying in the limit any k-contextual 
string automaton, a subset of the finite automata, from posit(cid:173)
ive examples only. In principle, we could use any string auto(cid:173)
maton inference algorithm for this purpose (see e.g. [Murphy, 
1996;  Parekh  and  Honavar,  1998]  for  others).  We  choose 
the k.contextual algorithm because it is efficient, simple, and 
works well in practice. 

As a result, the tree automaton is not purely local. 

The  algorithm.  We  have  designed a  procedure,  which  is 
shown in Algorithm  1, to learn an unranked tree automaton 
from a set T of positive examples.  The inferred automaton is 
"local" in the sense that it identifies a tree with its 2-forks as 
defined above. Note that DTDs for XML do the same [Murata 
et al,, 2001],  In addition to the input T, it takes as additional 
parameter a positive integer 
which is the parameter 
for the k.contextual subroutine that it calls. 

In  a  first  for-loop,  our  algorithm  collects  all  2-forks,  1-
subtreesand 1-roots. The latter become final states.  However, 

INFORMATION  EXTRACTION 

405 

An  element  (v, Str)  of the  partition  gives  all  positive  ex(cid:173)
amples for a particular label v.  The regular expression e cap(cid:173)
tures the regularity in these examples (the document content 
model  in  XML terminology).  To obtain sufficient generaliz(cid:173)
ation, we decided, after some experimentation, to distinguish 
If all  children  of a  vx  node  are  long  enough, 
three  cases. 
we  construct  a  DFA  using  the  k-contextual  algorithm  with 
k-value  kc,  otherwise  with  value  2.  For other labels (either 
a Vetx label  or  an  original  label),  we  ignore  the  content  of 
the children and accept any sequence (the regular expression 

Using  a  result  of [Muggleton,  1990],  it  is  quite  straight-
forward to make an incremental version of Algorithm  1.  We 
omit  it  here  due  to  lack  of space.  Using  a  basic  result  of 
[Angluin, 1980], we can prove: 
Theorem  1  Every unranked tree language that  is definable 
by a local unranked tree automaton with  k-contextual regular 
expressions is identifiable in the limit, from positive examples 
only,  by our algorithm. 

4  Experimental results 
We  evaluated  our  method  on  two  semi-structured  data 
sets  commonly  used  in  IE  research  (available  online  from 
http://www.isi. edu/-muslea/RISE/): a collection of web pages 
containing  people's  contact  addresses  (the  Internet  Address 
Finder (IAF)  database)  and  a  collection of web pages  about 
stock  quotes  (the  Quote  Server  (QS)  database).  For  each 
dataset,  there  are  two  tasks;  they  are  the  extraction  of al(cid:173)
ternative  and  organization  fields  in  the  IAF  dataset  and  of 
the  date  and  volume  fields  in  the  QS  dataset.  Each  data-
set  consists  of  10  documents.  The  number  of  fields  to  be 
extracted  is  respectively  94  (IAF-organization),  12  (IAF-
alt.name), 24 (QS-date), and 25 (QS-vol).  We choose these 
datasets because they are  benchmark datasets that are com(cid:173)
monly used for research in  IE; hence they allow us to com(cid:173)
pare  results. 
In  order  to  provide  a  close  comparison,  we 
use  the  same  train  and  test  splits  as  in  [Freitag  and  Kush-
merick,  2000].  In  addition,  they  require the  extraction of a 
whole  leaf node  (our algorithms are  designed for that task). 

Moreover,  the  results  obtained  so  far  [Muslea  et al.,  1999; 
Hsu and Chang,  1999] indicate that they are difficult tasks.  In 
fact  one  of the  authors  in  [Muslea et al.,  1999]  has tried  to 
build a hand-crafted extractor given all  available documents 
from the QS dataset and achieved only 88% accuracy (or re(cid:173)
call in our criteria below). We also test our method on a signi(cid:173)
ficantly  reduced  Shakespeare X ML  dataset (available online 
from  http://www.ibiblio.org/bosak/).  We use the same train(cid:173)
ing and test set as in [Kosala et al., 2002b].  The task on this 
dataset is to extract the second scene from one act in a partic(cid:173)
ular play. 

We  apply  the  commonly  used  criteria  of IE  research  for 
evaluating  our  method.  Precision  P  is  the  number  of cor(cid:173)
rectly extracted objects divided by the total number of extrac(cid:173)
tions,  while recall R is the  number of correct extractions di(cid:173)
vided  by  the  total  number of objects  present  in  the  answer 
template.  The  Fl  score  is  defined  as  2PR/(P  +  R),  the 
harmonic  mean  of P  and  R.  Table  1  shows  the  results  we 
obtained as well as those obtained by some current state-of-
the-art string-based methods:  an algorithm based on Hidden 
Markov Models (HMMs) [Freitag and McCallum,  1999], the 
Stalker wrapper induction algorithm [Muslea et al., 2001 ] and 
BWI  [Freitag and Kushmcrick,  2000].  We  also  include the 
results  of the  k-testable  algorithm  in  [Kosala  et  al.,  2002b] 
which works on ranked trees.  The results of 11MM,  Stalker 
and BWI are taken from [Freitag and Kushmerick, 2000]. All 
tests  are  performed  with  10-fold  cross  validation  following 
the splits used in  [Freitag and Kushmerick, 2000], except in 
the small Shakespeare dataset which uses 2-fold cross valida(cid:173)
tion. Each split has 5 documents for training and 5 for testing. 
We refer to Section 5 for a description of these methods. 

Table 1 shows the best results of the unranked method with 
a  certain  kc  (cross-validation  on  one  fold  of 50%  random 
training and  test examples.)  As can  be seen,  our method  is 
the only one giving optimal results. 

Table  2  shows  the  value  of  k  and  kc  used  respectively 
by  the  k-testable  and  the  unranked  algorithm. 
It  is  well-
known that when learning from positive examples only, there 
is  a  problem  of  over-generalization.  Our  algorithm  re(cid:173)
quires  a  cross-validation  on  the  value  of 
to  avoid  over-
generalization. 

Algorithm 1 runs in time 

, where n is the total num(cid:173)
ber of nodes in the training examples and c is a constant.  It 
takes  an  average time  between  19  and  26  ms  in  a  1.7  Ghz 
Pentium 4 PC for Algorithm 1 to learn an example in the IAF 
and QS datasets. 

5  Related work 
The IE work  for (semi-) structured texts can be divided into 
systems  built  manually  using  a  knowledge  engineering  ap(cid:173)
proach, e.g.  [Hammer et al.,  1997] and systems built (semi-) 
automatically using machine learning techniques or other al(cid:173)
gorithms.  The  latter  are  called  wrapper induction methods. 
We briefly survey them. 

The  three  systems  referred to  in  Table  1  learn  wrappers 
based on regular expressions. BWI [Freitag and Kushmerick, 
2000]  uses  a  boosting  approach  in  which  the  weak  learner 
learns a simple regular expression with high precision but low 

406 

INFORMATION  EXTRACTION 

recall.  The  HMM  approach  [Freitag  and  McCallum,  1999] 
learns a hidden Markov model;  it  solves the problem of es(cid:173)
timating probabilities  from sparse data  by using a statistical 
technique called  shrinkage.  This  model  has been  shown  to 
achieve  state-of-the-art performance on  a  range  of IE tasks. 
Stalker  [Muslea  et  al,  2001]  induces  extraction  rules  that 
are  expressed  as  simple  landmark  grammars,  a  class  of fi(cid:173)
nite automata; it performs hierarchical extraction guided by a 
manually built embedded catalog tree that describes the struc(cid:173)
ture of the fields to be extracted. 

Several techniques based on naive-Bayes, two regular lan(cid:173)
guage  inference  algorithms,  and  their  combinations  for  IE 
from  unstructured  texts  are  described  in  [Freitag,  1997]. 
WHISK  fSoderland,  1999]  learns  extraction  rules based  on 
a form of regular expression patterns with a top-down rule in(cid:173)
duction technique.  [Chidlovskii et ai,  2000] describe an  in(cid:173)
cremental grammar induction approach;  they use a subclass 
of deterministic finite automata that do not contain cyclic pat(cid:173)
terns.  The  SoftMcaly  system  [Hsu  and  Dung,  1998]  learns 
separators that identify the boundaries of the fields of interest. 
iHsu and Chang,  1999] propose two classes of SoftMealy ex(cid:173)
tractors:  single pass,  which  is biased  for tabular documents 
such  as  QS  data  (they  reach  up  to  97%  recall),  and  multi 
pass,  which is  biased  for tagged-list document such  as  IAF 
data (they reach up to 57% recall). We cannot really compare 
results because the experimental setting is different. 

All  above methods use algorithms for learning string lan(cid:173)
guages  and  require  some  manual  intervention.  HMMs  and 
BWI require to specify a window length for the prefix, suffix 
and  the  target  fragments.  Stalker and  BWI  require  to  spe(cid:173)
cify  special  tokens  or  landmarks  such as 
or  ";".  Soft(cid:173)
Mcaly extractors in [Hsu and Chang, 1999] requires to choose 
between single and multi pass bias. 

In  [Kosala  et al,  2002b],  the  document is  converted in  a 
ranked (binary) tree and an algorithm is used that induces a 
k-testable tree automaton. However, as binarisation increases 
the distance between target and distinguishing context, large 
k are needed and the resulting automaton is precise but does 
not  generalize  enough  (Table  1).  In  [Kosala  et al,  2002a], 
the  same  authors  generalize  the  obtained  automaton by  se(cid:173)
lectively introducing wild-card labels. This gives some mod(cid:173)
est improvement in recall but does not solve the problem. Our 
unranked tree automaton induction algorithm does. 

The  most  apparent limitation  of our method  is  that  it  can 
only output a whole text node.  To overcome this, it could be 
extended with a second step where string based methods are 
used to extract part of the text node.  For example, to extract 
the  substring "the web" from  the  whole string "Data on the 
web"  (Fig.  2).  Another  limitation  is  that  our method  only 
output a single field (slot) in one run. 

Finally,  a  disadvantage that  is not  apparent from the  res(cid:173)
ults  reported  above,  is  that  when the  identification of target 
fields does not require dependencies between nodes in the tree 
but can rely on a local pattern (e.g., the field to be extracted 
is  always  surrounded by  specific  delimiters),  our tree  based 
method  needs  more  examples  to  learn  the  same  extraction 
rule as methods that automatically focus on local patterns. In(cid:173)
tuitively, more variations in further-away nodes need to be ob(cid:173)
served before these variations are considered irrelevant. This 
is simply an instance of the well-known trade-offbetween the 
generality of a hypothesis space and the efficiency with which 
the correct hypothesis can be extracted from it. 

Some other approaches that exploit the structure of the doc(cid:173)
uments are:  WL2  [Cohen et al, 2002], a logic-based wrapper 
learner that uses multiple (string, tree, visual, and geometric) 
representations of the HTML documents. In fact, WL2 is able 
to extract all four tasks in the IAF and QS datasets with 100% 
recall;  and  wrappers  [Sakamoto  et al,  2002]  that identify  a 
field with  a  path  from  root  to  leaf,  imposing  conditions  on 
each node in the path. 

6  Conclusion 
We have presented an algorithm for the inference of a  local 
unranked  tree  automaton  with  k-contextual  regular  expres(cid:173)
sions and have shown that it can be used for IE from struc(cid:173)
tured documents.  Our results confirm the claim of [Kosala et 
al, 2002b] that utilizing the tree structure of the documents is 
worthwhile  for structured IE tasks.  Whereas the  latter work 
transforms the positive examples into binary ranked trees, we 
use them directly as unranked trees.  Our results are optimal 
for the previously considered benchmarks,  substantially  im(cid:173)
proving  upon  the  published  results  of other  string  and  tree 
based methods and are a strong indication that unranked tree 
automata are much better suited than ranked ones for struc(cid:173)
tured IE tasks. 

INFORMATION  EXTRACTION 

407 

Possible  future  work  includes  experiments  with  larger and 
more  difficult  datasets,  adapting  tree  automata  for  multi  slot 
extraction  in  one  run,  and  a  more  formal  analysis  of the  al(cid:173)
gorithm. 

Acknowledgements 
We  thank  Nicholas  Kushmerick  for  providing  us  with  the 
datasets used for B WI  experiments and anonymous reviewers 
for their  very valuable  comments.  This  work  is  supported by 
the  FWO  project  query  languages  for  data  mining.  Hendrik 
Blocked  is  a  post-doctoral  fellow  of the  Fund  for  Scientific 
Research  of Flanders. 

References 
[Abe and Mamitsuka,  1997]  N. Abe and H. Mamitsuka.  Predicting 
protein secondary structure using stochastic tree grammars.  Ma-
chine Learning, 29:275  301,  1997. 

[Ahonen,  1996]  H.  Ahonen.  Generating grammars for structured 
documents  using grammatical  inference  methods.  PhD  thesis, 
University of Helsinki, Department of Computer Science,  1996. 
Inductive  inference  of formal  lan(cid:173)
guages from positive data. Information and Control, 45:117-135, 
1980. 

[Angluin,  1980]  D.  Angluin. 

[Briiggemann-Klein  etal., 2001]  A. 

Briiggemann-Klein, 
M.  Murata,  and  D.  Wood,  newblock  Regular  tree  and  regu(cid:173)
lar  hedge  languages  over  unranked  alphabets,  TR  2001-05, 
HKUST-TCSC2001. 

[Bry and Schaffert, 2002]  F.  Bry and  S.  Schaffert.  Towards  a de(cid:173)
clarative query and transformation language for XML and semis-
tructured data:  Simulation unification.  In ICLP 2002, pages 255— 
270, 2002. 

[Chidlovskii et a/.,2000]  B.  Chidlovskii, 

and 
M.  de  Rijkc.  Wrapper  generation  via  grammar  induction. 
In ECML 2000, pages 96- 108, 2000. 

J.  Ragetli, 

[Cohen et al, 2002]  W. Cohen, M. Hurst, and L. S. Jensen.  A flex(cid:173)
ible learning system for wrapping tables and lists in HTML doc(cid:173)
uments.  In  The Eleventh International  World Wide  Web Confer-
ence (WWW2002), 2002. 

[Comon et al., 1999]  H.  Comon,  M.  Dauchet,  R.  Gilleron,  F.  Jac-
Tree 
Available  on: 

quemard,  D.  Lugiez,  S.  Tison,  and  M.  Tommasi. 
Automata  Techniques  and  Applications. 
http://www.grappa.univ-lille3.fr/tata,  1999. 

[Freitag and Kushmerick, 2000]  D.  Frcitag  and  N.  Kushmerick. 
Boosted  wrapper induction.  In AAAI/IAAI2000,  pages  577-583. 
AAAI Press, 2000. 

[Freitag and McCallum,  1999]  D.  Frcitag  and  A.  McCallum. 

In(cid:173)
In  AAAI-
formation  extraction  with  HMMs  and  shrinkage. 
99  Workshop  on  Machine  Learning for  Information  Extraction, 
1999. 

[Frcitag,  1997]  D. Freitag. Using grammatical inference to improve 
In  ICML-97  Workshop  on 
precision  in  information  extraction. 
Automata  Induction,  Grammatical Inference,  and Language Ac(cid:173)
quisition,  1997. 

[Freitag, 2000]  D.  Freitag.  Machine  learning  for information  ex(cid:173)
traction  in  informal  domains.  Machine Learning,  39(2/3): 169-
202, 2000. 

[Hammer etal,  1997]  J.  Hammer,  H.  Garcia-Molina,  J.  Cho, 
A. Crespo, and R. Aranha. Extracting semistructured information 
from  the  Web.  In  Workshop  on  Management  of  Semistructured 
Data, pages 18-25,  1997. 

[Hsu and Chang,  1999]  C-N.  Hsu  and  C-C.  Chang.  Finite-state 
transducers  for semi-structured text mining.  In  IJCAI-99  Work(cid:173)
shop on  Text Mining:  Foundations,  Techniques and Applications, 
1999. 

[Hsu and Dung,  1998]  C-N. Hsu and M-T. Dung. Generating finite-
state  transducers  for  semi-structured  data  extraction  from  the 
Web.  Information Systems, 27 (8);52\-SW, 1998. 

[Kosala et al., 2002a]  R. Kosala, M. Bruynooghe, H. Blocked, and 
J. Van den Bussche.  Information extraction by means of a gener(cid:173)
alized  k-tcstable  tree automata inference  algorithm.  In Informa(cid:173)
tion  Integration  and  Web-based Applications  & Services, HWAS, 
Proc, pages  105-109,  2002. 

[Kosala et al., 2002b]  R.  Kosala, 

den  Bussche, 
M.  Bruynooghe,  and  H.  Blocked. 
Information  extraction 
in structured documents using tree automata induction.  In PKDD 
2002, pages 299-310, 2002. 

J.  Van 

[Kushmerick, 2000]  N.  Kushmerick.  Wrapper  induction:  Effi(cid:173)
ciency  and  expressiveness.  Artificial  Intelligence,  118:15-68, 
2000. 

[Levy etal.,  1998]  A.  Levy,  C.  Knoblock,  S.  Minton,  and W.  Co(cid:173)
hen.  Trends  and  controversies:  Information  integration.  IEEE 
Intelligent Systems,  13(5),  1998. 

[Mugglcton,  1990]  S.  Mugglcton.  Inductive Acquisition  of Expert 

Knowledge.  Addison-Wesley,  1990. 

[Murata et al., 2001]  M. Murata, D.  Lee, and M. Mani.  Taxonomy 
of XML schema languages using formal  language theory.  In Ex(cid:173)
treme Markup Languages, 2001. 

[Murphy,  1996]  K. Murphy.  Learning  finite  automata.  TR 96-04-

017, Santa Fe Institute, 1996. 

[Musicalal.,  1999]  I.  Muslca,  S.  Minton,  and  C.  Knoblock.  A 
In  Int.  Conf  on 

hierarchical  approach  to  wrapper  induction. 
Autonomous Agents, 1999. 

[Muslea et al.,2001] 1.  Muslca,  S.  Minton,  and  C.  Knoblock. 
Hierarchical  wrapper  induction  for  semistructured  information 
sources.  Journal  of Autonomous  Agents  and  Multi-Agent  Sys-
tems, 4:93-114, 2001. 

[Muslca,  1999]  I.  Muslea.  Extraction patterns  for  information  ex(cid:173)
In  AAAI-99  Workshop  on  Machine 

traction  tasks:  A  survey. 
Learning  for  Information  Extraction,  1999. 

[Parekh and Honavar,  1998]  R.  Parekh  and  V.  Honavar.  Auto(cid:173)
mata Induction,  Grammar Inference,  and Language Acquisition. 
Handbook  of Natural  Language  Processing.  New  York:  Marcel 
Dckkcr,  1998. 

[Rico-Juan etal., 2000]  J.R.  Rico-Juan,  J.  Calcra-Rubio,  and  R.C. 
Carrasco.  Probabilistic  k-testable  tree-languages.  In  Grammat(cid:173)
ical  Inference:  Algorithms  and  Applications  ICG  I  2000,,  pages 
221-228,2000. 

[Sakamoto et al., 2002]  H. Sakamoto, H. Arimura, and S. Arikawa. 
Knowledge discovery from semistructured texts.  In Progress in 
Discovery Science - Final Report of the Japanese Discovery Sci(cid:173)
ence Project, pages 586-599. Springer, 2002. 

[Soderland,  1999]  S.  Soderland.  Learning  information  extraction 
rules for semi-structured and free text.  Machine Learning, 34(1-
3):233-272,  1999. 

[XQL, 2002]  Xquery  1.0:  An xml  query language.  W3C Working 

Draft  16 August 2002.  www.w3 . o r g / T R / x q u e r y /. 

408 

INFORMATION  EXTRACTION 


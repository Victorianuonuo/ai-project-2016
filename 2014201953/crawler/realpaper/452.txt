Responsibility and Blame: A Structural-Model Approach 

Hana Chockler 

Joseph Y. Halpern* 

School of Engineering and Computer Science 

Department of Computer Science 

Hebrew University 

Jerusalem 91904, Israel. 
Email: hanac@cs.huji.ac.il 

Cornell University 

Ithaca, NY 14853, U.S.A. 

Email: halpern@cs.cornell.edu 

Abstract 

Causality is typically treated an all-or-nothing con(cid:173)
cept; either A is a cause of B or it is not. We extend 
the  definition  of causality  introduced by  Halpern 
and Pearl 2001a to take into account the degree of 
responsibility of A for B. For example, if someone 
wins an election 11-0, then each person who votes 
for him is less responsible for the victory than if he 
had won 6-5.  We then define a notion of degree 
of blame, which takes into account an agent's cpis-
temic state. Roughly speaking, the degree of blame 
of A  for D  is the expected degree of responsibil(cid:173)
ity of A for B, taken over the epistemic state of an 
agent. 

Introduction 

1 
There have been many attempts to define causality going back 
to Hume 1739, and continuing to the present (see, for exam(cid:173)
ple, [Collins et al, 2003; Pearl, 2000] for some recent work). 
While many definitions of causality have been proposed, all 
of them treat causality is treated as an all-or-nothing concept. 
That is, A is either a cause of B or it is not. As a consequence, 
thinking only in terms of causality docs not at times allow us 
to make distinctions that we may want to make. For example, 
suppose that Mr. B wins an election against Mr. G by a vote 
of 11-0. Each of the people who voted for Mr. B is a cause of 
him winning. However, it seems that their degree of respon(cid:173)
sibility should not be as great as in the case when Mr. B wins 
6-5. 

In this paper, we present a definition of responsibility that 
takes this distinction into account.  The definition is an ex(cid:173)
tension of a definition of causality introduced by Halpern and 
Pearl 2001a.  Like many other definitions of causality going 
back to Hume 1739, this definition is based on counterfactual 
dependence.  Roughly speaking,  A  is a cause of B if, had 
A not happened (this is the counterfactual condition, since A 
did in fact happen) then B would not have happened.  As is 
well known, this naive definition does not capture all the sub(cid:173)
tleties involved with causality. In the case of the 6-5 vote, it 

* Supported in part by NSF under grant CTC-0208535 and by 
the DoD Multidisciplinary University Research Initiative (MURI) 
program administered by ONR under grant N00014-01-1-0795. 

is clear that, according to this definition, each of the voters 
for Mr. B is a cause of him winning, since if they had voted 
against Mr. B, he would have lost. On the other hand, in the 
case of the  11-0 vote, there are no causes according to the 
naive counterfactual definition.  A change of one vote does 
not makes no difference.  Indeed, in this case, we do say in 
natural language that the cause is somewhat "diffuse". 

While  in this case the  standard counterfactual definition 
may not seem quite so problematic, the following example 
(taken from [Hall, 2003]) shows that things can be even more 
subtle.  Suppose that Suzy and Billy both pick up rocks and 
throw them at a bottle. Suzy's rock gets there first, shattering 
the bottle.  Since both throws are perfectly accurate, Billy's 
would have shattered the bottle had Suzy not thrown.  Thus, 
according to the naive counterfactual definition, Suzy's throw 
is not a cause of the bottle shattering.  This certainly seems 
counter to intuition. 

Both problems are dealt with the same way in [Halpern and 
Pearl, 2001a]. Roughly speaking, the idea is that A is a cause 
of B if B counterfactually depends on C under some con(cid:173)
tingency. For example, voter 1 is a cause of Mr. B winning 
even if the vote is  11-0 because, under the contingency that 
5 of the other voters had voted for Mr. G instead, voter 1 's 
vote would have become critical; if he had then changed his 
vote, Mr. B would not have won.  Similarly, Suzy's throw is 
the cause of the bottle shattering because the bottle shattering 
counterfactually depends on Suzy's throw, under the contin(cid:173)
gency that Billy doesn't throw.  (There are further subtleties 
in the definition that guarantee that, if things are modeled ap(cid:173)
propriately, Billy's throw is not a cause. These are discussed 
in Section 2.) 

It is precisely this consideration of contingencies that lets 
us define degree of responsibility.  We take the degree of re(cid:173)
sponsibility  of A  for  B  to be  1/(N  +  1),  where  N  is  the 
minimal number of changes that have to be made to obtain a 
contingency where B counterfactually depends on A.  (If A 
is not a cause of B, then the degree of responsibility is 0.) In 
particular, this means that in the case of the 11-0 vote, the de(cid:173)
gree of responsibility of any voter for the victory is 1/6, since 
5 changes have to be made before a vote is critical. If the vote 
were 1001-0, the degree of responsibility of any voter would 
be  1/501.  On the other hand,  if the vote is 5-4, then the 
degree of responsibility of each voter for Mr. B  for Mr.  B's 
victory is 1; each voter is critical. As we would expect, those 

CAUSALITY 

147 

merman,  1988]).  Our  definitions,  by  design,  do  not  take 
into  account  intentions  or  possible  alternative  actions,  both 
of which  seem  necessary  in  dealing  with  moral  issues.  For 
example, there is no question that Truman was in part respon(cid:173)
sible and to blame for the deaths resulting from dropping the 
atom bombs on Hiroshima and Nagasaki. However, to decide 
whether this is a morally reprehensible act, it is also necessary 
to consider the  alternative actions he could have performed, 
and their possible outcomes.  While our definitions do not di(cid:173)
rectly address these moral issues, we believe that they may be 
helpful in elucidating them. 

The  rest  of this  paper  is  organized  as  follows. 

In  Sec(cid:173)
tion 2 we review the basic definitions of causal models based 
on  structural  equations,  which  are  the  basis  for  our  defi(cid:173)
nitions  of responsibility  and  blame. 
In  Section  3,  we  re(cid:173)
view  the  definition  of  causality  from  [Halpern  and  Pearl, 
2001a],  and  show  how  it  can  be  modified  to  give  a  defi(cid:173)
nition  of  responsibility. 
In  Section  3.3,  we  give  our  def(cid:173)
inition  of  blame. 
In  Section  4,  we  discuss  the  com(cid:173)
plexity  of  computing  responsibility  and  blame.  Proofs  of 
the  theorems  can  be  found  in  the  full  paper,  available  at 
http://www.cs.cornell.edu/home/halpern/papers/blame.ps. 

voters  who  voted  for  Mr.  G  have  degree  of responsibility  0 
for Mr.  B's  victory,  since they  are  not causes  of the  victory. 
Finally,  in  the  case  of Suzy  and  Billy,  even though  Suzy  is 
the  only  cause  of the  bottle  shattering,  Suzy's  degree  of re(cid:173)
sponsibility  is  1/2,  while  Billy's  is  0.  Thus,  the  degree  of 
responsibility  measures to  some  extent whether or not  there 
are other potential causes. 

When determining responsibility, it is assumed that every(cid:173)
thing relevant about the facts of the world and how the world 
works  (which  we  characterize  in  terms  of what  are  called 
structural equations)  is  known.  For  example,  when  saying 
that  voter  1  has degree of responsibility  1/6 for Mr.  B's win 
when the vote is  11-0, we assume that the vote and the pro(cid:173)
cedure  for  determining  a  winner  (majority  wins)  is  known. 
There is no uncertainty about this. Just as with causality, there 
is no difficulty in talking about the probability that someone 
has a certain degree of responsibility by putting a probability 
distribution on the way the world could be and how it works. 
But  this  misses  out  on  important  component  of determining 
what we call here blame: the epistemic state.  Consider a doc(cid:173)
tor who treats a patient with a particular drug resulting in the 
patient's death.  The doctor's treatment is a cause of the pa(cid:173)
tient's death;  indeed,  the doctor may well bear degree of re(cid:173)
sponsibility  1  for the  death.  However,  if the  doctor had  no 
idea  that  the  treatment  had  adverse  side  effects  for  people 
with high blood pressure,  he should perhaps not be held to 
blame for the death.  Actually, in legal arguments, it may not 
be so relevant what the  doctor actually did or did not know, 
but what he should have known. Thus, rather than considering 
the doctor's actual epistemic state, it may be more important 
to consider what his epistemic  state should have been.  But, 
in any case, if we are trying to determine whether the doctor 
is to blame for the patient's death, we must take into account 
the doctor's epistemic state. 

We  present  a  definition  of blame  that  considers  whether 

agent a  performing  action b is  to  blame  for an  outcome 
The definition is relative to an epistemic state for a, which is 
taken,  roughly speaking,  to be a set of situations before  ac(cid:173)
tion b is performed, together with a probability on them.  The 
degree of blame is then essentially the expected degree of re(cid:173)
sponsibility of action b for 
(except that we ignore situations 
where  was  already  true  or b was  already performed).  To 
understand the difference between  responsibility and blame, 
suppose that there  is  a  firing  squad consisting  of ten  excel(cid:173)
lent marksmen.  Only one of them has live bullets in his rifle; 
the  rest have  blanks.  The  marksmen  do not know which  of 
them has the live bullets. The marksmen shoot at the prisoner 
and he dies.  The only marksman that is the cause of the pris(cid:173)
oner's death is the one with the live bullets.  That marksman 
has degree of responsibility  1  for the death; all the rest have 
degree  of responsibility  0.  However,  each  of the  marksmen 
has degree of blame  1/10.1 

While we believe that our definitions  of responsibility and 
blame  are  reasonable,  they  certainly  do  not  capture  all  the 
connotations of these words as  used in the literature.  In the 
philosophy  literature,  papers  on  responsibility  typically  are 
concerned with moral responsibility (see, for example,  [Zim-

1 We thank Tim Williamson for this example. 

148 

CAUSALITY 

While the equations  for a given problem are typically ob(cid:173)
vious, the choice of variables may not be.  For example, con(cid:173)
sider  the  rock-throwing  example  from  the  introduction. 
In 
this  case,  a naive  model  might  have  an  exogenous  variable 
U that encapsulates whatever background factors cause Suzy 
and Billy to decide to throw the rock (the details of U do not 
matter, since we are interested only in the context where U's 
value is such that both Suzy and Billy throw), a variable ST 
for  Suzy throws (ST  =  1  if Suzy throws,  and  ST  =  0  if she 
doesn't), a variable  DT  for Billy throws,  and  a  variable  BS 
for bottle shatters.  In the naive model, whose graph is given 
in Figure  1, BS is  1  if one of ST and  BT  is  1.  (Note that the 
graph omits the exogenous variable U, since it plays no role. 
In the graph, there is an arrow from variable X to variable Y 
if the  value  of Y  depends on  the  value  of X.) 

3  Causality and Responsibility 
3.1  Causality 
We start with the definition of cause from [Halpern and Pearl, 
2001a]. 

AC1 just says that A cannot be a cause of B unless both A 
and  B are true,  while AC3  is a minimality condition to pre(cid:173)
vent, for example, Suzy throwing the rock and sneezing from 

CAUSALITY 

149 

being a cause of the bottle shattering.  Eiter and Lukasiewicz 
2002b showed that one consequence of AC3 is that causes can 
always be taken to be single conjuncts. Thus, from here on in, 

3.2  Responsibility 
The definition of responsibility  in causal  models extends  the 
definition  of causality. 

Example 3.4  It is easy to see that Suzy's throw has degree of 
responsibility  1/2  for the  bottle  shattering  both  in  the  naive 
model  described  in  Figure  1  and the "better" model  of Fig(cid:173)
ure 2 in the context where both Suzy and Billy throw.  In both 
cases,  we  must  consider  the  contingency  where  Billy  docs 
not throw.  Although Suzy is the only cause of the bottle shat(cid:173)
tering  in the latter model,  her degree of responsibility  is still 
only  1 / 2,  since  the  bottle  would  have  shattered even  if she 
hadn't thrown.  This  is a subtlety  not captured by the notion 
of causality. 

Interestingly, in a companion paper [Chockler et al, 2003] 
we apply our notion of responsibility to program verification. 
The  idea  is  to  determine  the  degree  of responsibility  of the 
setting of each state for the satisfaction of a specification in a 
given system.  For example, given a specification of the form 
Op (eventually p is true),  if/;  is true  in  only one  state of the 
verified system, then that state has degree of responsibility  1 
for the  specification.  On the other hand,  if p is true  in three 
states, each state only has degree of responsibility 1/3.  Expe(cid:173)
rience has shown that if there are many states with low degree 
of responsibility for a specification, then either the specifica(cid:173)
tion is incomplete (perhaps p really did have to happen three 
times, in which case the specification should have said so), or 
there is a problem with the system generated by the program, 
since it has redundant states. 

The  degree  of responsibility  can  also  be  used  to  provide 
a  measure  of the  degree  of fault-tolerance  in  a  system.  If a 
component  is  critical  to  an  outcome,  it  will  have  degree  of 
responsibility  1.  To ensure fault tolerance, we need to make 
sure  that  no  component  has  a  high  degree  of responsibility 
for an outcome. Going back to the example of O P,, the degree 
of responsibility  of 1/3  for  a  state  means  that  the  system  is 
robust to the simultaneous failures of at most two states. 

150 

CAUSALITY 

3.3  Blame 
The  definitions  of both  causality  and  responsibility  assume 
that the context and the structural  equations are given; there 
is no uncertainty.  We are often  interested in assigning a de­
gree of blame to an action.  This assignment depends on the 
epistemic state of the agent before the action  was performed. 
Intuitively, if the agent had no reason to believe that his action 
would result in a certain outcome, then he is not to blame for 
the outcome (even if in fact his action caused the outcome). 
To  deal  with  the  fact  that  we  are  considering  two  points 
in time—before the action was performed and after—we add 
superscripts to variables.  We use a superscript 0 to denote the 
value of the random variable before the action was performed 
and the superscript  1  to denote the value of the random vari­
able  after.  Thus,  Y0  =  1  denotes  that  the  random  variable 
Y has value  1  before the action is performed, while Y1  =  2 
denotes that it had value  1  afterwards.  If ψ is a Boolean com­
bination  of (unsuperscripted)  random  variables,  we  use  ψ° 
and  ψ1  to denote the value of ψ before and after the action is 
performed, respectively. 

There  arc  two  significant  sources  of  uncertainty  for  an 

agent who is contemplating performing an action: 

•  what the true situation is;  for example, a doctor may be 
uncertain  about  whether a patient has  high  blood pres­
sure. 

•  how the world works; for example, a doctor may be un­

certain about the side effects of a given medication. 

In  our  framework,  the  "true  situation"  is  determined  by 
the context and "how the world works" is determined by the 
structural equations.  Thus,  we model an agent's uncertainty 
by a pair (AC, Pr), where AC is a set of pairs of the form (A/, a), 
where  M  is  a  causal  model  and  u  is  a  context,  and  Pr  is 
a  probability  distribution  over  AC.  Following [Hlalpern and 
Pearl,  2001b],  who used such epistemic  states in the defini­
tion of explanation, we call a pair (A/,  u)  a situation. 

Roughly  speaking,  the  degree  of blame  that  setting  X  to 
x  has  for  ψ  is  the  expected  degree  of responsibility  of X  = 
x  for  ψ,  taken  over  the  situations 
,  where 
Our actual  definition  of blame  is just 
this definition, except that, when computing the expectation, 
we do not count situations in AC where ψ was already true or 
X  was  already  x.  To  understand  why,  suppose  that  we  are 
trying to compute the degree of blame of Suzy's throwing the 
rock  for the bottle shattering.  Assume that we are interested 
in a period in a bottle in the period between time 0 and time 1, 
and the bottle was actually shattered at time  1.  We certainly 
don't  want  to  say  that  Suzy's  throw  was  to  blame  if  Suzy 
didn't throw between  time 0  and  time  1  or if the bottle  was 
already shattered at time 0.  So suppose that Suzy does in fact 
throw between time 0 and time 1, and at time 0, she considers 
the following four situations to be equally likely: 

•  (M1,u1), where the bottle was already shattered before 

Suzy's throw; 

•  (M2,u2),  where  the  bottle  was  whole  before  Suzy's 
throw,  and  Suzy  and  Billy  both  hit  the  bottle  simulta­
neously (as described in the model in Figure  1); 

•  (M3,u3),  where  the  bottle  was  whole  before  Suzy's 
throw, and Suzy's throw hit before Billy's throw (as de­
scribed in the model in Figure 2); and 

•  (M4,u4),  where  the  bottle  was  whole  before  Suzy's 

throw, and Billy did not throw. 

To compute the degree of blame assigned to Suzy's throwing 
the rock for the bottle shattering, we ignore (A/,  u1),  because 
the  bottle  is  already  shattered  in  (M,u1)  before  Suzy's  ac­
tion.  The  degree  of responsibility  of  Suzy's  throw  for  the 
bottle  shattering  is  1/2  in  (M2,u2)  and  (M3,u3),  and  is  1 
in  (M4,7/4).  It  is  easy  to  see  that  the  degree  of blame  is 

Example 3.6  Consider again the example of the firing squad 
with  ten  excellent  marksmen.  Suppose  that  marksman  1 
knows that exactly one marksman has a live bullet in his rifle. 
Thus, he considers  10 situations possible, depending on who 
has the bullet.  Let pL  be his prior probability that marksman 
i  has the live bullet.  Then the degree of blame of his shot for 
the death  is  pi.  The degree of responsibility is cither  1  or 0, 
depending on whether or not he  actually  had the  live bullet. 
Thus, it is possible for the degree of responsibility to be 1 and 
the degree of blame to be 0 (if he ascribes probability 0 to his 
having the live bullet,  when in fact he does), and it is possi­
ble  for the degree of responsibility to  be 0 and the degree of 
blame to be  1  (if he mistakenly ascribes probability  1  to his 
having the bullet when he in fact does not). 

Example 3.7  The  previous  example  suggests  that  both  de­
gree of blame and degree of responsibility may be relevant in 
a legal setting.  Another issue that is relevant in legal settings 
is  whether to  consider  actual  epistemic  state  or  to  consider 
what the epistemic state should have been.  The former is rel­
evant  when  considering  intent.  To  see  the  relevance  of the 
latter, consider a patient who dies as a result of being treated 
by  a doctor with  a particular drug.  Assume that  the patient 
died  due  to  the  drug's  adverse  side  effects  on  people  with 
high blood pressure and, for simplicity, that this was the only 
cause of death.  Suppose that the doctor was not aware of the 
drug's adverse side effects. (Formally, this means that he docs 
not consider possible  a situation  with a causal  model  where 
taking the drug causes death.)  Then, relative to the doctor's 
actual  epistemic  state,  the  doctor's  degree  of blame  will  be 
0.  However,  a  lawyer  might  argue  in  court  that  the  doctor 
should  have  known  that  treatment  had  adverse  side  effects 
for  patients  with  high  blood  pressure  (because  this  is  well 
documented  in  the  literature)  and thus  should have  checked 
the patient's blood pressure.  If the doctor had performed this 
test, he would of course have known that the patient had high 
blood pressure.  With respect to the resulting epistemic state, 
the  doctor's degree  of blame  for the  death  is  quite  high.  Of 
course, the lawyer's job is to convince the court that the lat­
ter  epistemic  state  is  the  appropriate  one  to  consider  when 
assigning degree of blame. 

CAUSALITY 

151 

4  The Complexity of Computing 

Responsibility and Blame 

In  this  section  we  present  complexity  results  for computing 
the  degree  of responsibility  and  blame  for general  recursive 
models. 

4.1  Complexity  of responsibility 
Complexity  results  for  computing  causality  were  presented 
by  Eiter and  Lukasiewicz 2002a;  2002b.  They showed that 
the problem  of detecting  whether X  —  x  is an actual  cause 
of ψ is 
-complete  for general  recursive  models  and  NP-
complete  for binary models  [Liter and Lukasiewicz, 2002b]. 
(Recall that 
is the second level of the polynomial hierar­
chy and that binary models are ones where all  random vari­
ables can take on exactly two values.)  There is a similar gap 
between the complexity of computing the degree of responsi­
bility and blame in general models and in binary models. 

For a  complexity class  A, 

consists  of all  func­
tions that can be computed by a polynomial-time Turing ma­
chine with an oracle for a problem in A, which on input x asks 
a total  of 
queries (cf.  [Papadimitriou,  1984]).  We 
show  that  computing  the  degree  of responsibility  of X  =  x 
for ψ in arbitrary models is 
-complete.  In [Chock-
ler et  al.,  2003],  we  show  that  computing  the  degree  of re­
sponsibility in binary models is 

-complete. 

Since  there  are  no  known  natural 

-complete 
problems,  the  first  step  in  showing  that  computing  the  de­
gree  of responsibility  is 
-complete is to define an 
-complete problem.  We start by defining one that 

we call MAXQSAT2. 

Recall  that  a  quantified  Boolean  formula  [Stock  meyer, 
1977] (QBF) has the form 
.  .ψ.  ,  where  X1,  X2,... 
are propositional variables and ψ is a propositional formula. 
A  QBF  is  closed  if  it  has  no  free  propositional  variables. 
TQBF consists of the closed QBF formulas that are true.  For 
As  shown by  Stock­
example, 
meyer 1977, the following problem QSAT2 is 
-complete: 

A  witness  f  for  a  true  closed 

That is,  QSAT2  is the  language of all  true QBFs of the  form 
where ψ is a Boolean formula in 3-CNF. 
is  an  as­
signment  /  to  X  under  which 
is  true.  We  define 
MAXQSAT2 as the problem of computing the maximal num­
ber  of variables  in  X  that  can  be  assigned  1  in  a  witness 
for 

Formally,  given 

ne 

a

f

i

d

e

to  be  k  if there  exists  a  witness  for 

that 
assigns exactly k of the variables in X the value  1, and every 
other witness for  assigns at most k'  <  k variables in A'  the 
value 1. 
Theorem 4.1 

The proof of Theorem 4.1  shows explicitly how to reduce 
the computation of any 
problem to MAXQSAT2. 
Given  a  polynomial-time  Turing  machine  M  with  oracle  in 

that on an input of size 7/. makes 0(log u) oracles queries, 
is a Boolean 
we construct a formula 
formula  in  3-CNF  such  that given 
we can 
compute  the  output  of  M  in  polynomial  time.  Essentially, 
the  formula ψ describes the  output of M  for all  possible  se­
quences  of answers  for  oracle  queries  and 
gives the correct sequence of answers. Since the total number 
of oracle  queries  is 
,  the number of all  possible  se­
quences of answers is polynomial in the size of the input, and 
thus the size  of 
is also polynomial  in the size of the input. 
The construction is somewhat complicated; see the full paper 
for details. 
Similarly 

define 
the  minimum  number  of 
variables  in  X  that  can  be  assigned  1  in  a  witness  for 
otherwise.  It 
is  easy to  see  that MINQSAT2  has  the  same  complexity  as 
MAXQSAT2, since 

MAXQSAT2, 
to  be 

if there is such a witness, and 

we 

to 

where (0) is obtained from ψ by replacing each propositional 
variable 

with its negation. 

Using a reduction from MINQSAT2, we can prove the de­

sired complexity result. 

Theorem 4.2  The  degree  of responsibility  is 
complete for general recursive causal models. 

Proof:  Due to lack of space, we present the proof here in a 
somewhat abridged form.  Membership in 
can be 
proved using an argument similar to the one used in [Chockler 
et al,  2003]  to prove membership of the  degree  of responsi­
bility for binary causal models  in 

The  proof  that  computing  the  degree  of  responsibility 
is 
-hard  essentially  follows  from  an  argument  in 
[Eiter and Lukasiewicz, 2002a] showing that QSAT2  can be 
reduced to the problem of detecting causality. In fact, their ar­
gument actually provides a reduction from MINQSAT2 to the 
degree of responsibility.  Given a QBF of the form 
ψ 
Eiter and Lukasiewicz construct a causal model M whose en­
dogenous  variables  include  X',  Y,  and  a  fresh  variable  X*. 
They  consider  a  context  u  in  which  all  variables  in  X'  get 
the  value  0.  They  show  that 
is  true  iff  X'*  =  0 
is  a  cause  of  ψ  in  ( M, u). 
If  X*  =  0  is  indeed  a  cause, 
then  the  set  W  in  AC2  must  be  a  subset  of X.  That  is,  if 
there  is  a  partition  (W,Z)  and  a  setting 
(x'.w')  satisfying 
AC2  showing  that  X*  =  0  is  a  cause  of  ψ,  then  W  must 
be  a subset of X.  Moreover, the  variables  in  W  that change 
value from 0 to  1  in  w'  are precisely those such that an as-

152 

CAUSALITY 

. 

,  , 

4.2  Complexity  of blame 
Given an epistemic state (K, Pr), where K consists of N pos(cid:173)
sible  situations,  each  with  at  most  n  random  variables,  the 
straightforward way to compute 
is by 
for  each 
computing 
such that  , 
and then computing the 
expected degree of responsibility with respect to these situa(cid:173)
tions,  as in Definition 3.5.  Recall  that the degree of respon(cid:173)
sibility in each model is determined by using a binary search 
thus uses 
queries  in  each  model  in  K.  Since  the 
number of models is  N,  we get a polynomial  time algorithm 
with  N log n oracle  queries.  The  type  of oracle  depends  on 
whether the models are binary or general.  For binary models 
it is enough to have an NP oracle, whereas for general mod(cid:173)
els we need a 
We do not have a matching lower 
bound  but,  as  we  show  in  the  full  paper,  any  binary-search 
style  algorithm  for  computing  the  degree  of blame  requires 

[Papadimitriou,  1984]  C.H.  Papadimitriou.  The complexity 
of unique solutions. Journal of ACM, 31:492-500, 1984. 
[Pearl, 2000]  J.  Pearl.  Causality:  Models,  Reasoning,  and 
Inference.  Cambridge University Press, New York, 2000. 
[Stockmeyer,  1977]  L. J.  Stockmeyer.  The polynomial-time 

hierarchy.  Theoretical Computer Science, 3:1-22,  1977. 

[Zimmerman,  1988]  M.  Zimmerman.  An  Essay  on  Moral 
Responsibility.  Rowman  and  Littlefield,  Totowa,  N.J., 
1988. 

oracle queries,  for N  —  o(n). 

Acknowledgment  We  thank  Michael  Ben-Or  and  Orna 
Kupferman for helpful discussions. 

References 
[Chockler et al., 2003]  H.  Chockler,  J.  Y.  Halpern,  and 
O.  Kupferman.  Causality  and  responsibility  in  temporal 
logic model checking.  Unpublished manuscript. Available 
at 
http://www.cs.cornell.edu/home/halpcni/papers/resp.ps, 
2003. 

[Collins et ai, 2003]  J. Collins, N.  Hall, and L. A.  Paul, ed(cid:173)
itors.  Causation  and Counter/actuals.  MIT  Press,  Cam(cid:173)
bridge, Mass., 2003. 

[Eiter and Lukasiewicz, 2002a]  T. Eiter and T. Lukasiewicz. 
Causes and explanations in the structural-model approach: 
tractable  cases. 
In  Proc.  Eighteenth  Conference on  Un(cid:173)
certainty  in  Artificial  Intelligence  (UAI2002),  pages  146— 
153,2002. 

[Eiter and Lukasiewicz, 2002b]  T. Eiter and T.  Lukasiewicz. 
Complexity results  for structure-based  causality.  Artificial 
Intelligence, 142( 1 ):53-89, 2002. 

[Hall, 2003]  N.  Hall. 

In 
J. Collins, N. Hall, and L. A. Paul, editors, Causation and 
Counter/actuals. MIT Press, Cambridge, Mass., 2003. 

Two  concepts  of  causation. 

[Halpern and Pearl, 2001a]  J.  Y.  Halpern  and  J.  Pearl. 
Causes  and  explanations:  A  structural-model  approach. 
Part 1: Causes.  In Proc.  Seventeenth Conference on  Un(cid:173)
certainty  in  Artificial Intelligence  (UAI  2001),  pages  194-
202,  2001.  The  full  version  of the  paper  is  available  at 
http://www.cs.cornell.edu/home/halpern. 

[Halpern and Pearl, 2001b]  J.  Y.  Halpern  and  J.  Pearl. 
Causes  and  explanations:  A  structural-model  approach. 
Part II:  Explanations.  In Proc.  Seventeenth  International 
Joint  Conference  on  Artificial  Intelligence  (IJCAI 
'01), 
pages 27-34, 2001.  The full version of the paper is avail(cid:173)
able at http://www.cs.cornell.edu/home/halpern. 

[Hume,  1739]  D.  Hume.  A  Treatise  of  Human  Nature.  John 

Noon, London, 1739. 

CAUSALITY 

153 


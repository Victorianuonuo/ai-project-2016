AUC: a Statistically Consistent and more Discriminating Measure than Accuracy 

Charles X. Ling 
Jin Huang 
Department of Computer Science 
The University of Western Ontario 
London, Ontario, Canada N6A 5B7 

{ling,jhuang}@csd.uwo.ca 

Harry  Zhang 

Faculty  of Computer Science 
University  of New  Brunswick 

Fredericton, NB, Canada E3B 5A3 

hzhang@unb.ca 

Abstract 

Predictive accuracy has been used as the main and 
often  only  evaluation  criterion  for  the  predictive 
performance  of classification  learning  algorithms. 
In recent years,  the area  under the ROC (Receiver 
Operating  Characteristics)  curve,  or simply  AUC, 
has been proposed as an alternative single-number 
measure for evaluating learning algorithms.  In this 
paper, we prove that AUC is a better measure than 
accuracy.  More  specifically,  we  present rigourous 
definitions  on  consistency  and  discriminancy  in 
comparing two evaluation measures for learning al(cid:173)
gorithms.  We  then  present  empirical  evaluations 
and a formal proof to establish that AUC is indeed 
statistically consistent and more discriminating than 
accuracy.  Our  result  is  quite  significant  since  we 
formally prove that, for the first time, AUC is a bet(cid:173)
ter measure than accuracy in the evaluation of learn(cid:173)
ing algorithms. 

Introduction 

1 
The  predictive ability of the  classification  algorithm is  typi(cid:173)
cally measured by its predictive accuracy (or error rate, which 
is  1  minus the accuracy) on the testing examples.  However, 
most  classifiers  (including C4.5  and  Naive  Bayes)  can  also 
produce  probability estimations  or  "confidence"  of the  class 
prediction.  Unfortunately, this information is completely ig(cid:173)
nored in accuracy.  This is often taken for granted since the 
true probability is unknown for the testing examples anyway. 
In many applications, however, accuracy is not enough. For 
example,  in direct marketing,  we often need to promote the 
top X% of customers during gradual roll-out, or we often de(cid:173)
ploy different promotion strategies  to customers  with differ(cid:173)
ent likelihood of buying some products.  To accomplish these 
tasks, we need more than a mere classification of buyers and 
non-buyers. We need (at least) a ranking of customers in terms 
of their likelihoods of buying. 
If we want to achieve a more 
accurate ranking from a classifier, one might naturally expect 
that we  must need the true ranking in  the training examples 
[Cohen et a/.,  1999].  In most scenarios, however, that is not 
possible.  Instead, what we are given is a dataset of examples 
with class  labels only.  Thus, given only classification labels 

in training and testing sets, are there better methods than ac(cid:173)
curacy to evaluate classifiers that also produce rankings? 

The  ROC  (Receiver  Operating  Characteristics)  curve  has 
been  recently  introduced to evaluate  machine  learning algo(cid:173)
rithms  [Provost  and Fawcett,  1997; Provost et  al,  1998].  It 
compares the classifiers'  performance across the entire range 
of class distributions and error costs. However, often there is 
no clear dominating relation between two ROC curves in the 
entire range; in those situations, the area under the ROC curve, 
or simply AUC, provides a single-number "summary" for the 
performance  of the  learning  algorithms.  Bradley  [Bradley, 
1997]  has compared popular machine learning algorithms us(cid:173)
ing AUC, and found that AUC exhibits several desirable prop(cid:173)
erties compared to accuracy.  For example, AUC has increased 
sensitivity  in  Analysis  of Variance  (ANOVA)  tests,  is  inde(cid:173)
pendent  to  the  decision  threshold,  and  is  invariant to a pri(cid:173)
ori class  probability distributions [Bradley,  1997].  However, 
no formal arguments or criteria have been established.  How 
can  we  compare two evaluation  measures  for learning algo(cid:173)
rithms? How can we establish that one measure is better than 
another? In this paper, we give formal definitions on the con(cid:173)
sistency and discriminancy for comparing two measures.  We 
show, empirically and formally, that AUC is indeed a statisti(cid:173)
cally consistent and more discriminating measure than accu(cid:173)
racy. 

One  might ask  why  we need  to care  about anything more 
than accuracy, since by definition, classifiers only classify ex(cid:173)
amples (and do not care about ranking and probability).  We 
answer this question from two aspects.  First, as we discussed 
earlier, even with labelled training and testing examples, most 
classifiers  do  produce  probability  estimations  that  can  rank 
training/testing examples.  Ranking is very important in most 
real-world applications.  As we establish that AUC is a better 
measure than accuracy,  we can choose classifiers with better 
AUC, thus producing better ranking.  Second, and more im(cid:173)
portantly,  if we  build classifiers  that  optimize  AUC  (instead 
of accuracy),  it has been shown [Ling and Zhang, 2002] that 
such classifiers produce not only better AUC (a natural con(cid:173)
sequence), but also better accuracy (a surprising result), com(cid:173)
pared  to classifiers  that  optimize the accuracy.  To  make  an 
analogy, when we train workers on a more complex task, they 
will do better on a simple task than workers who are trained 
only on the simple task. 

Our  work  is  quite  significant  for  several  reasons.  First, 

LEARNING 

519 

we  establish  rigourously,  for  the  first  time,  that  even  given 
only labelled examples, AUC is a better measure (defined in 
Section 2.2) than accuracy.  Second, our result suggests that 
AUC  should  replace  accuracy  in  comparing  learning  algo(cid:173)
rithms in the future.  Third, our results prompt and allow us 
to  re-evaluate  well-established  results  in  machine  learning. 
For example, extensive experiments have been conducted and 
published on comparing,  in  terms of accuracy,  decision tree 
classifiers to Naive Bayes classifiers.  A well-established and 
accepted  conclusion  in  the  machine  learning  community  is 
that those learning algorithms are  very  similar as  measured 
by accuracy [Kononenko, 1990;Langley et  al,  1992;Domin-
gos and Pazzani,  1996].  Since we have established that AUC 
is a better measure,  are  those learning  algorithms still  very 
similar as measured  by AUC?  We  have shown [Ling et a/., 
2003] that, surprisingly, this is not true:  Naive Bayes is sig(cid:173)
nificantly better than decision tree  in terms of AUC.  These 
kinds of new conclusions are very useful to the machine learn(cid:173)
ing community,  as well  as to machine  learning applications 
(e.g., data mining). Fourth, as a new measure (such as AUC) 
is discovered and  proved to be  better than  a previous mea(cid:173)
sure (such as accuracy),  we can re-design most learning al(cid:173)
gorithms  to  optimize  the  new  measure  [Ferri  et  al,  2002; 
Ling and Zhang,  2002].  This would produce classifiers that 
not only perform well in the new measure, but also in the pre(cid:173)
vious measure,  compared  to the classifiers that optimize the 
previous measure, as shown in [Ling and Zhang, 2002]. This 
would further improve the performance of our learning algo(cid:173)
rithms. 

2  Criteria for Comparing Evaluation 

Measures for Classifiers 

We start with some  intuitions in  comparing AUC  and  accu(cid:173)
racy,  and  then  we  present  formal  definitions  in  comparing 
evaluation measures for learning algorithms. 

2.1  A UC vs Accuracy 
Hand  and Till  [Hand  and Till,  2001]  present a simple ap(cid:173)
proach to calculating the AUC of a classifier G below. 

(1) 

where 

where n0 and  n1  are the numbers of positive and negative ex(cid:173)
amples  respectively, and 
is the rank 
of ith  positive example in the ranked list.  Table  1  shows an 
example of how to calculate  AUC  from  a ranked  list with 5 
positive examples and 5 negative examples.  The AUC of the 
ranked list in Table 1 is 
which is 24/25. 
It is clear that AUC obtained by Equation 1 is a measure for the 
quality of ranking, as the more positive examples are ranked 
higher (to the right of the list), the larger the term 
.  AUC 
is shown to be equivalent to the Wilcoxon statistic rank test 
[Bradley, 1997]. 

Intuitively, we can see why AUC is a better measure than 
accuracy  from the following example.  Let  us consider two 
classifiers, Classifier 1 and Classifier 2, both producing prob(cid:173)
ability estimates for a set of 10 testing examples.  Assume that 
both classifiers classify 5 of the  10 examples as positive, and 

Table  1:  An example for calculating AUC with rz 

Table 2:  An Example in which two classifiers have the same 
classification accuracy,  but different AUC values 

the other 5  as negative.  If we rank the testing examples ac(cid:173)
cording to increasing probability of being + (positive), we get 
the two ranked lists as in Table 2. 

Clearly, both classifiers produce an error rate of 20% (one 
false positive and one false negative), and thus the two classi(cid:173)
fiers are equivalent in terms of error rate.  However, intuition 
tells us that Classifier 1 is better than Classifier 2, since overall 
positive examples are ranked higher in Classifier 1  than 2.  If 
we calculate AUC according to Equation 1, we obtain that the 
AUC of Classifier  1  is 
(as seen  in Table  1), and the AUC 
of Classifier 2 is 
. Clearly, AUC does tell us that Classifier 
1  is indeed better than Classifier 2. 

Unfortunately,  "counter examples"  do exist,  as  shown  in 
Table 3 on two other classifiers:  Classifier 3 and Classifier 4. 
It is easy to obtain that the AUC of Classifier 3 is 
, and the 
AUC  of Classifier 4 is 
.  However, the error rate of Classi(cid:173)
fier 3 is 40%, while the error rate of Classifier 4 is only 20% 
(again we assume that the threshold for accuracy is set at the 
middle so that 5 examples are predicted as positive and 5 as 
negative).  Therefore,  a larger AUC does not always imply a 
lower error rate. 

Table 3:  A counter example in which one classifier has higher 
AUC but lower classification accuracy 

Another intuitive argument for why AUC is better than ac(cid:173)
curacy is that AUC is more discriminating than accuracy since 
it has more possible values.  More specifically, given a dataset 
with n examples, there is a total of only n +  1  different clas(cid:173)
sification accuracies 
.  On the other hand, 
assuming there are no positive examples and n1 negative ex-
different AUC val-
,  generally more  than 
n  +  1.  However,  counterexamples  also exist in this regard. 
Table 4 illustrates that classifiers with the same AUC can have 
different accuracies.  Here, we see that Classifier 5 and Clas(cid:173)
sifier 6 have the same 
but different accuracies (60% 
and 40% respectively).  In general, a measure with more val(cid:173)
ues is not necessarily more discriminating.  For example, the 
weight of a  person  (having  infinitely many  possible values) 
has nothing to do with the number of siblings (having only a 
small number of integer values) he or she has. 

520 

LEARNING 

Table  4:  A  counter  example  in  which  two  classifiers  have 
same  A UC  but  different  classification  accuracies 

How  do  we  compare  different  evaluation  measures  for 
learning  algorithms?  Some  general  criteria  must  be  estab(cid:173)
lished. 
2.2 

(Strict)  Consistency and  Discriminaney of Two 
Measures 

Intuitively speaking,  when  we discuss  two different  measures 
/  and  g  on  evaluating  two  learning  algorithms  A  and  B,  we 
want  at  least  that  /  and  g  be  consistent  with  each  other.  That 
is,  when  /  stipulates  that  algorithm  A  is  (strictly)  better  than 
B,  then  g  will  not say  B  is  better than  A.  Further,  i f/  is  more 
discriminating  than  g,  we  would  expect  to  see  cases  where  / 
can  tell  the difference  between  algorithms A  and  B  but g  can(cid:173)
not. 

This  intuitive  meaning  of  consistency  and  discriminaney 

Figure  1:  Illustrations  of  two  measures  /  and  g.  A  link  be(cid:173)
tween  two  points  indicates  that  the  function  values  are  the 
.  In  (a),  /  is  strictly  consistent  and  more 
same  on  domain  
discriminating  than  g. 
In  (b),  /  is  not  strictly  consistent  or 
more discriminating than g.  Counter examples on consistency 
(denoted by  X  in the  figure)  and discriminaney (denoted by Y) 
exist  here 

,  /,  g  are  (strictly)  consistent  if  there  exist  no  a,  b 

can  be  made precise  as  the following definitions. 
Definition  1  (Consistency)  For  two  measures  f,  g  on  do(cid:173)
main 
such  that  f (a)  > 
Definition  2  (Discriminaney)  For  two  measures  f,  g  on  do(cid:173)
f  is  (strictly)  more  discriminating  than  g  if there  exist 
main 
a, b 
such  that  f (a)  > 
-  g(b),  and  there 
exist  no  a,  b 

f(b)  and  g(a)  <  g(b). 

, 

f(b)  and  g(a) 

such  that  g(a)  >  g(b)  and  f(a)  — 

f(b). 

As an  example,  let us think about numerical  marks  and let(cid:173)
ter marks  that  evaluate  university students.  A numerical  mark 
gives  100,  99,  98 
1,  or 0  to  students,  while  a  letter mark-
gives  A,  B,  C,  D,  or F to  students.  Obviously,  we  regard  A  > 
B  >  C  >  D  >  F.  Clearly,  numerical  marks  are  consistent  with 
letter  marks  (and  vice  versa). 
In  addition,  numerical  marks 
are  more  discriminating than  letter marks,  since  two students 
who  receive  91  and  93  respectively  receive  different  numer(cid:173)
ical  marks  but  the  same  letter  mark,  but  it  is  not  possible  to 
have students with different letter marks (such as A and B) but 
with the same  numerical  marks.  This  ideal  example of a  mea(cid:173)
sure  /  (numerical  marks)  being  strictly  consistent  and  more 
discriminating than  another g  (letter  marks)  can  be  shown  in 
the  figure  1(a). 
2.3  Statistical Consistency and  Discriminaney of 

Two Measures 

As  we  have already  seen  in  Section  2.1, counter examples  on 
consistency  and  discriminaney  do  exist  for  A UC  and  accu(cid:173)
racy.  Therefore,  it  is  impossible to prove  the consistency  and 
discriminaney  on  A UC  and  accuracy  based  on  Definitions  1 
and  2.  Figure  1(b)  illustrates  a  situation  where  one  measure 
/  is not completely  consistent  with  g,  and  is  not strictly more 
discriminating than g.  In this case,  we must consider the prob(cid:173)
ability of being consistent and  degree  of being  more  discrimi(cid:173)
nating.  What  we  w i ll  define  and  prove  is  the probabilistic ver(cid:173)
sion  of the  two  definitions  on  strict  consistency  and  discrim(cid:173)
inaney.  That  is,  we  extend  the  previous  definitions to  degree 
of consistency  and  degree  of discriminaney,  as  follows: 

There  are  clear  and  important  implications  of these  defini(cid:173)
tions of measures  /  and  g  in  evaluating  two  machine  learning 
algorithms, say  A  and B.  If /  and g  are  consistent to degree  C, 
then  when  /  stipulates that  A  is  better than  B,  there  is a  prob(cid:173)
ability  C  that  g  will  agree  (stipulating  A  is  better  than  B).  If 
/  is  D  times  more  discriminating  than  g,  then  it  is  D  times 
more  likely that  /  can  tell  the  difference  between  A  and  B  but 
g  cannot  than  that  g  can  tell  the  difference  between  A  and  B 
but  /  cannot.  Clearly,  we  require  that  C  >  0.5  and  D  >  1  if 
we  want  to  conclude  a  measure  /  is  "better"  than  a  measure 
g.  This  leads  to  the following definition: 

Definition  5  The  measure 
is  statistically  consistent  and 
more  discriminating  than  g  if  and  only  if  C  >  0.5  and  D  >  1. 
In  this  case,  we  say,  intuitively,  that  f  is  a  better  measure  than 
9> 

f 

The  statistical  consistency  and  discriminaney  is  a  special 
case  of the strict consistency  and  more  discriminaney.  For the 
example  of numerical  and  letter  marks  in  the  student  evalua(cid:173)
tion discussed  in  Section  2.2,  we can  obtain that  C  =  1.0 and 
D  —  oc, as the former is strictly consistent and more discrim(cid:173)
inating than  the  latter. 

'it  is easy to prove that this definition is  symmetric; that  is, the 
degree of consistency o f/ and g is same as the degree of consistency 
of g  and  /. 

LEARNING 

521 

To prove AUC is statistically consistent and more discrim­
inating than accuracy, we substitute / by AUC and g by accu­
racy in the definition above. To simplify our notation, we will 
use AUC to represent AUC values, and ace for accuracy.  The 
domain Ω is the ranked lists of testing examples (with n0 pos­
itive and  n1  negative examples).  Since we require C  >  0.5 
and D  >  1  we will essentially need to prove: 

3  Empirical Verification on AUC and 

Accuracy 

Before we present a formal proof that AUC is statistically con­
sistent and more discriminating than accuracy, we first present 
an empirical  verification.  To simplify our notation, we will 
use AUC to represent AUC values, and ace for accuracy. 

We  conduct  experiments  with  (artificial)  testing  sets  to 
verify  the statistical  consistency  and discriminancy between 
AUC  and  accuracy.  The  datasets  are  balanced  with  equal 
numbers of positive and negative examples.  We test datasets 
with 4, 6, 8,  10,  12,  14, and  16 testing examples.  For each 
number of examples,  we enumerate  all  possible ranked  lists 
of positive and negative examples. For the dataset with 2n ex­
amples, there are (2n) such ranked lists. 

We exhaustively compare all pairs of ranked lists to see how 
they  satisfy  the consistency  and  discriminating propositions 
probabilistically.  To obtain degree  of consistency,  we count 
the  number  of  pairs  which  satisfy  "AUC  (a)  >  AUC(b) 
and  acc(a)  >  acc(b)  and  the  number of pairs  which  sat­
isfy  "AUC(a)  >  AUC(b)  and  ace(a)  <  acc(b)".  We  then 
calculate the percentage of those cases;  that is, the degree of 
consistency.  To obtain degree of discriminancy, we count the 
number  of  pairs  which  satisfy  "AUC(a)  >  AUC(b)  and 
acc(a)  =  acc(by\  and  the  number  of  pairs  which  satisfy 
"AUC(a)  =  AUC(b)  and  aec{a)  >  orc(6)". 

Tables 5 and 6 show the experiment results for the balanced 
dataset.  For consistency,  we can  see (Table 5) that for vari­
ous numbers of balanced testing examples, given AUC (a) > 
AUC(b),  the  number  (and  percentage)  of cases  that  satisfy 
aee(a)  >  ace(b)  is  much  greater  than  those  that  satisfy 
ace(a)  <  aee(b).  When  n  increases,  the degree of consis­
tency (C)  seems  to approach 0.94,  much  larger than the re­
quired 0.5. For discriminancy, we can see clearly from Table 6 
that the number of cases that satisfy. 
and 
acc(a)  =  acc(b) is much more (from 15.5 to 18.9 times more) 
than  the  number of cases  that  satisfy  ace(a)  >  acc(b)  and 
AUC(a)  =  AUC(b).  When  n  increases,  the  degree  of dis­
criminancy (D) seems  to approach  19,  much larger than the 
required threshold 1. 

These  empirical  experiments verify empirically that  AUC 
is  indeed  a  statistically  consistent  and  more  discriminating 

Table 5:  Experimental results for verifying statistical consis­
tency between AUC and accuracy for the balanced dataset 

Table  6:  Experimental  results  for  verifying  AUC  is  statis­
tically  more  discriminating  than  accuracy  for  the  balanced 
dataset 

measure than accuracy for the balanced datasets, as a measure 
for learning algorithms. 

4  Formal Proof 

In our following discussion, we assume that the domain  ^  is 
the set  of all  the ranked  lists  with no  positive and  n1  nega­
tive examples.  In  this paper,  we only study the cases  where 
a ranked list contains an equal number of positive and nega­
tive examples, and we assume that the cutoff for classification 
is at the exact middle of the ranked list (each classifier classi­
fies exactly half examples into the positive class and the other 
half into the negative class).  That is, n0  =  n1,  and we use n 
instead of  n0  and  n1. 

Lemma  1  gives the maximum and minimum AUC values 

for a ranked list with a fixed accuracy rate. 

Proof:  Assume that there are np  positive examples correctly 
classified. Then there are n—np positive examples in negative 
section.  According to Equation  1, AUC value is determined 
by 5o,  which is the sum of indexes of all positive examples. 
So when all n—np positive examples are put on the highest po­
sitions in negative section (their indexes are n, n - 1, • • •, np + 
1), and np positive examples are put on the highest positions in 
positive section (their indexes are 

522 

LEARNING 

AUC reaches the maximum value. So 

Similarly when all positive examples are put on the lowest po­
sitions in both positive and negative sections, AUC reaches 
the minimum value. Thus 

Now let us explore the discri mi nancy of AUC and accuracy. 
Lemma 4 to Lemma 5 are proposed as a basis to prove Theo­
stands for the number of 
rem 2. In the following lemmas, 
examples in ranked list r, and 
stands for the number of 
positive examples in r. 

Proof:  For any ranked list r  E  R1,  we switch the examples 
on position i and n  — i, to obtain another unique ranked list 
s. Next we replace all positive examples in .s by negative ex­
amples and all negative examples in s by positive examples to 
obtain ranked list s'. It is obvious that. 
S
we can prove 

T h u s ,. 
Therefore 

Similarly 
.  □ 

i

n

c

e. 

Proof:  (a) It is straightforward to prove it from Lemma 1. 
(b) For any ranked  list r  £  R1  we can convert it to another 
unique ranked  list  v'  G  R2  by exchanging the positions of 
examples.  In the negative section, for any positive example 
whose position is r7, we exchange it with the example in the 
position of n +  1  —  ri  In the positive section, for any posi­
tive example in position rt, we exchange it with the example 
in the position of n — (r, — n) + 1 + n. It's easy to obtain that 
AUC(r')  =  AUCm ax  +  AUCmin. 
Sor'  E  R2.  Thus, we 
have that  \Ri\  <  |R2|.  Similarly, we can prove  \r2\  <  lR1l-
Therefore  1R1|  =  |R2|-  ° 

LEARNING 

523 

Therefore,  from  inequality  2 and equation  3,  we  w i ll get 

that A UC  is  a better  measure  than  accuracy  in evaluating  and 
comparing classification learning algorithms.  This conclusion 
has  many  important  implications  in  evaluating,  comparing, 
and designing learning algorithms.  In  our future research,  we 
w i ll  extend  the  results  in  this  paper  to  cases  with  unbalanced 
class  distribution and  multiple classes. 

References 
[Bradley,  1997]  A.  P.  Bradley.  The  use  of  the  area  under 
the ROC curve  in  the evaluation  of machine  learning  algo(cid:173)
rithms.  Pattern  Recognition,  30:1145-1159,  1997. 

[ C o h e n s a / .,  1999]  W.  W.  Cohen,  R.  E.  Schapire,  and 
Y.  Singer.  Learning  to  order  things.  Journal  of Artificial 
Intelligence  Research, 

10:243-270,1999. 

[Domingos and Pazzani,  1996]  P. DomingosandM. Pazzani. 
Beyond  independence:  conditions  for  the  optimality  of 
In  Proceedings  of  the 
the  simple  Bayesian  classifier. 
Thirteenth 
International  Conference  on  Machine  Learn-
ing,  pages  1 0 5 - 1 1 2,  1996. 

[Ferri  et  al,  2002]  C.  Ferri,  P.  A.  Flach,  and  J.  Hernandcz-
learning  decision  trees  using  the  area  under 
In  Proceedings  of the  Nineteenth  Inter(cid:173)

Orallo. 
the  ROC  curve. 
national  Conference  on  Machine  Learning 
pages  139-146,2002. 

(ICML  2002), 

[Hand  and T i l l, 2001 ]  D. J. Hand and R. J. T i l l. A simple gen(cid:173)
eralisation  of  the  area  under  the  ROC  curve  for  multiple 
class  classification  problems.  Machine  Learning,  4 5 : 1 7 1-
186,2001. 

[Kononenko,  1990]  I.  Kononenko.  Comparison  of  induc(cid:173)
tive  and  naive  Bayesian  learning  approaches  to  automatic 
In  B.  Wielinga,  editor,  Current 
knowledge  acquisition. 
Trends  in  Knowledge  Acquisition.  IOS  Press,  1990. 

fLangley et  al,  19921  P. Langley, W. Iba, and K. Thomas.  An 
In  Proceedings  of  the 
Intelligence,  pages 

analysis  of  Bayesian  classifiers. 
Tenth  National  Conference  of Artificial 
223-228. A A AI  Press,  1992. 

[Ling  and Zhang,  2002]  C.  X.  Ling  and  H.  Zhang.  Toward 
In  Pro-

Bayesian  classifiers  with  accurate  probabilities. 
ceedings  of  the  Sixth  Pacific-Asia  Conference  on  KDD. 
Springer,  2002. 

[Ling et  al,  2003]  C.  X.  Ling,  J.  Huang,  and  H.  Zhang. 
A U C: A  better measure  than  accuracy  in  comparing  learn(cid:173)
In  Proceedings  of 16th  Canadian  Confer-
ing  algorithms. 
ence  on  Artificial  Intelligence.  Springer,  2003.  To  appear. 
[Provost and Fawcett,  1997]  F.  Provost  and  T.  Fawcett. 
Analysis  and  visualization  of  classifier  performance: 
comparison  under  imprecise  class  and  cost  distribution. 
In  Proceedings  of  the  Third 
International  Conference  on 
Knowledge  Discovery  and  Data  Mining,  pages  43-48. 
A A AI  Press,  1997. 

[Provost  etal,  1998]  F.  Provost,  T.  Fawcett,  and  R.  Kohavi. 
The case against accuracy  estimation for comparing induc(cid:173)
In  Proceedings  of the  Fifteenth  Interna(cid:173)
tion  algorithms. 
tional  Conference  on  Machine  Learning,  pages  445-453. 
Morgan  Kaufmann,  1998. 

5  Conclusions 
In  this paper,  we  give  formal  definitions of discriminancy  and 
consistency in comparing evaluation measures for learning al(cid:173)
gorithms.  We  prove  that  A UC  is  statistically consistent  and 
more discriminating than  accuracy.  Our result is  quite signif(cid:173)
icant since  we have established  rigourously, for the  first  time, 

524 

LEARNING 


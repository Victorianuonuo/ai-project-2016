Bayesian Tensor Inference for Sketch-based Facial Photo Hallucination

†
Wei Liu

Xiaoou Tang

†‡

†
Jianzhuang Liu

†Dept. of Information Engineering, The Chinese University of Hong Kong, Hong Kong

‡Visual Computing Group, Microsoft Research Asia, Beijing, China

{wliu5, jzliu}@ie.cuhk.edu.hk and xitang@microsoft.com

Abstract

This paper develops a statistical
inference ap-
proach, Bayesian Tensor Inference, for style trans-
formation between photo images and sketch im-
ages of human faces. Motivated by the rationale
that image appearance is determined by two co-
operative factors: image content and image style,
we ﬁrst model the interaction between these fac-
tors through learning a patch-based tensor model.
Second, by introducing a common variation space,
we capture the inherent connection between photo
patch space and sketch patch space, thus building
bidirectional mapping/inferring between the two
spaces. Subsequently, we formulate a Bayesian ap-
proach accounting for the statistical inference from
sketches to their corresponding photos in terms
of the learned tensor model. Comparative ex-
periments are conducted to contrast the proposed
method with state-of-the-art algorithms for facial
sketch synthesis in a novel face hallucination sce-
nario: sketch-based facial photo hallucination. The
encouraging results obtained convincingly validate
the effectiveness of our method.

1 Introduction
Recently, machine learning becomes more and more popu-
lar applied to the computer vision community. Various ap-
plications and methods that learn low-level vision have been
proposed in the classical literature [Freeman et al., 2000]. In
this paper, we focus on a fascinating vision topic: automatic
image sketching which automatically generates alike sketch
images from photo images. Sketches are the simplest form
of drawings because they consist of only drawing lines. The
artists can distill the identifying characteristics of a photo and
highlight them with a small number of critical strokes.

Implementing the great idea of learning vision, success-
ful image sketching techniques try to observe and learn from
the artist’s works, and hence generate vivid and expressive
sketches. As a result, example-based methods are widely
studied in the latest years. Given a set of training images
and their associated sketches drawn by artists, it is of interest
to generate a sketch image automatically from an input image
with the help of machine learning techniques.

groundtruth 

photo-sketch pair

sketching

hallucinating

(a) 

(b) 

Figure 1: Bidirectional transforms on photo-sketch pairs. (a)
Forward transform: synthesizing a sketch image from a photo
image; (b) backward transform: hallucinating a photorealistic
image from a sketch image.

For the particular image class of human faces, the trans-
form between photo-realistic faces and their associative
sketch drawings has shown promising applications in the fu-
ture.
In recent years, some works have been done to ad-
dress the issues of synthesizing sketches from photos [Chen
et al., 2001; Tang and Wang, 2004; Liu et al., 2005a]
and sketch-based face recognition [Tang and Wang, 2004;
Liu et al., 2005a]. However, to the best of our knowledge,
a more difﬁcult issue - the backward transform from sketches
to photos - has not been seriously addressed. In this paper,
we develop a novel research topic: hallucinating photorealis-
tic faces from sketches, which is termed as sketch-based fa-
cial photo hallucination. We design a new face hallucination
technique to fulﬁll the intractable backward transform. We
also consider the forward and backward transforms together
in order to explore the inherent relation between the bidirec-
tional transforms shown in Fig. 1.

IJCAI-07

2141

Considering the complexity of image spaces and the con-
spicuous distinction between photos and sketches, global lin-
ear models such as [Tang and Wang, 2004; Liu et al., 2005a]
tend to oversimplify this problem. Therefore, we try to ex-
tract the local relations by explicitly establishing the connec-
tion between two feature spaces formed by a patch-based ten-
sor model. To hold the statistical dependencies between pair-
wise patches with two styles more precisely and ﬂexibly, we
present a Bayesian Tensor Inference approach that incorpo-
rates the advantages of multilinear analysis techniques based
on tensor into Bayesian statistics.

The rest of this paper is organized as follows. In Section 2,
we elicit a tensor model for a facial image ensemble. The de-
tailed rationale and algorithm for Bayesian Tensor Inference
are presented in Section 3. Experimental results are shown in
Section 4 and conclusions are drawn in Section 5.

2 Tensor Model

Recently, multilinear algebra and tensor modeling have at-
tracted considerable attention in both computer vision and
computer graphics communities. Research efforts apply-
ing tensor cover a broad range of topics including face
modeling and synthesis [Vasilescu and Terzopoulos, 2002;
Wang and Ahuja, 2003; Vlasic et al., 2005], super-resolution
[Liu et al., 2005b], etc.

Motivated by previous multilinear approaches, we make
use of a novel tensor model to exclusively account for the
representation of images with two styles: photo-style and
sketch-style. As small image patches can account for high-
level statistics involved in images, we take patches as con-
stitutive elements of the tensor model. Based on a corpus
containing image patches with photo- and sketch-styles, we
arrange all these patches into a high-order tensor which will
sufﬁce to encode the latent connection between the two styles.

2.1 TensorPatches

Based on the observation that both styles, i.e. photo- and
sketch-styles, share some common characteristics and each
style possesses its special traits, we assume the existence
of decomposition of the patch feature space into the com-
mon variation space, which reﬂects the commonalities shared
by both styles, and the special variation space. Relying on
this rationale, we employ multilinear algebra to perform ten-
sor decomposition on one large patch ensemble carrying two
modalities.

Let us divide training pairwise images into overlapping
small square patches which are assumed to be of the same
size. m pairs of patches within a spatial neighborhood located
in images are collected to form a high-order tensor. Follow-
ing the non-parametric sampling scheme [Chen et al., 2001],
we allow m to be smaller than the length of each patch fea-
ture vector d. Resulting from the conﬂuence of three modes
related to patch examples, patch styles and patch features, a
3-order tensor D ∈ (cid:3)m×2×d
is built by grouping pairwise
patches pertaining to the same neighborhood.

This paper adopts the High-Order SVD [Lathauwer et al.,
2000] or N -mode SVD [Vasilescu and Terzopoulos, 2002],
both of which are the generalizations of SVD, to decompose

the higher-order tensor D as follows1
D = C ×1 U1 ×2 U2 ×3 U3

= C ×1 Upatches ×2 Ustyles ×3 Uf eatures,

(1)
where C, known as the core tensor, governs the interaction
between the mode matrices U1, · · · , UN . Mode-n matrix Un
contains the orthonormal vectors spanning the column space
of matrix D(n) resulting from mode-n ﬂattening (unfolding)
D. Deﬁning a tensor T = C ×3 Uf eatures as TensorPatches,
then we have

D = T ×1 Upatches ×2 Ustyles.

(2)

So far, we succeed in decomposing the two-modal patch
feature space into the common variation space spanned by
Upatches ∈ (cid:3)m×m
and the special variation space spanned
by Ustyles ∈ (cid:3)2×2
.

For any patch whether it is photo- or sketch-style, its cor-

responding tensor representation is
P = T ×1 wT ×2 sT

k , k = 1, 2

(3)
where w contains patch parameters encoding the common
characteristics of pairwise patches, and sk, capturing style pa-
rameters reﬂecting the special properties of style k (1 denotes
photo-style, 2 denotes sketch-style), is the k-th row vector of
is the i-th row vector of Upatches, the ten-
Ustyles. When wT
sor representation of the i-th training patch with k-th style is
obtained. Its vector representation can be derived via mode-
1(or 2, 3) ﬂattening the subtensor P, that is, (f1(P))T

For a new patch pair (x, y), x represents the photo-style
and y denotes the sketch-style. Their tensor representations
can be learned in line with eq. (3) by
P(x) = T ×1 wT ×2 sT
P(y) = T ×1 wT ×2 sT

1 = A1 ×1 wT
2 = A2 ×1 wT ,

(4)

.

where both A1 = T ×2 sT
tensors. Mode-1 ﬂattening eq. (4) results in

1 and A2 = T ×2 sT

2 are constant

x = (f1(A1))
y = (f1(A2))

T w = Axw
T w = Ayw.

(5)

and Ay ∈ (cid:3)d×m

Ax ∈ (cid:3)d×m
are called inferring matrices.
The shared parameter vector w, expected to be solved, exists
in the common variation space. We solve it through mapping
pairwise patches into the common variation space as follows

w = Bxx
w = Byy,
x and By = (AT

(6)

where Bx = (AT
y are
called mapping matrices. Fig. 2 illustrates the relations
among x, y, w.

x Ax)−1AT

y Ay)−1AT

When one of (x, y) is unknown, inferring the counterpart
style from the known style of a patch is the objective of image
sketching or hallucination. Combining eq. (5) and eq. (6), a
coarse estimation for y can be derived as

y ≈ AyBxx,

(7)

1Details for tensor decomposition can be found in [Lathauwer et
al., 2000; Vasilescu and Terzopoulos, 2002], we will not elaborate.

IJCAI-07

2142

Photo Patch Space

Bx: mapping

Ax: inferring

Common
Variation

Space

 By: mapping

Ay: inferring

Sketch Patch Space

Figure 2: Illustration of relations among common variation
space, photo patch space and sketch patch space.

which coincides with the path shown in Fig. 2: mapping the
“Photo Patch Space” to the “Common Variation Space” from
which inferring the “Sketch Patch Space” .

Importantly, the coarse estimation offered by eq.

(7) is
consistent with sketch synthesis methods [Tang and Wang,
2004][Liu et al., 2005a] under particular conditions. Consid-
ering (x, y) as the holistic photo-sketch image pair of faces,
when Ax and Ay are sample matrices whose columns are vec-
tors recording the training sketch and photo images, eq. (7)
is equivalent to the eigentransform method [Tang and Wang,
2004] where w contains linear combination coefﬁcients w.r.t.
training faces. In the case that Ax contains K nearest neigh-
bors of input photo patch x in the training set and Bx collects
associative sketches of these neighbors, eq. (7) resembles the
local geometry preserving method [Liu et al., 2005a]. Fur-
thermore, eq. (7) can be thought as the unconstrained solution
of LLE [Roweis and Saul, 2000] where w is the weighting
vector for locally linear reconstruction.

2.2 TensorPatches for Facial Image Ensembles
Due to the structural characteristics of face images, we adopt
a higher-order tensor rather than the 3-order tensor modeled
on generic images. For the facial image ensemble, it necessi-
tates modeling both global structures and local details of face
images together.

Given the multi-factor (people, spatial positions of patches)
and multi-modal (styles) natures of these patches, we develop
a 4-order tensor model to explicitly account for both styles
and constituent factors of facial patches. Suppose we have n
pairs of face images available for training, of which each is di-
vided into m overlapping square patches. With two styles, the
length of each patch feature vector is d. Therefore, patches
are inﬂuenced by four modes: people, patches, styles and fea-
tures. A 4-order tensor D ∈ (cid:3)n×m×2×d
is naturally built by
grouping all photo-sketch patches sampled from the training
faces. Perform tensor decomposition on D
D = C ×1 Upeople ×2 Upositions ×3 Ustyles ×4 Uf eatures

= T ×1 Upeople ×2 Upositions ×3 Ustyles,

where the core tensor C governs the interaction between 4
modes encoded in 4 mode matrices: Upeople ∈ (cid:3)n×n
,
Upositions ∈ (cid:3)m×m
and Uf eatures ∈

, Ustyles ∈ (cid:3)2×2

, and TensorPatches T is obtained by forming the prod-

(cid:3)d×d
uct C ×4 Uf eatures.

Signiﬁcantly, the two factors (people, position) are cru-
cial to determine a concrete patch and encoded in row vec-
tor spaces of mode matrices Upeople and Upositions. To
remove factor (people, position) redundancies and suppress
the effects of noise, a truncation of the two mode matrices
is needed. In this paper, we adopt the N -mode orthogonal
iteration algorithm in [Vasilescu and Terzopoulos, 2003] to
perform factor-speciﬁc dimensionality reduction, outputting
converged truncated mode matrices ˆUpeople ∈ (cid:3)n∗r1 (r1 <
n), ˆUpositions ∈ (cid:3)m∗r2 (r2 < m) along with the rank-reduced
approximation of tensor D

ˆD = ˆT ×1 ˆUpeople ×2 ˆUpositions ×3 ˆUstyles.

(8)

In analogy to the last subsection, for a new patch pair
(xj , yj) residing in the j-th spatial position of face images,
their tensor representations can be derived with the similar
form as eq. (4)

P(xj) = ˆT ×1 wT ×2 vT
P(yj) = ˆT ×1 wT ×2 vT

j ×3 sT
j ×3 sT

1 = A1j ×1 wT
2 = A2j ×1 wT ,

(9)

j and sT

k is the j-th and k-th row vector of the mode
where vT
matrix ˆUpositions and ˆUstyles, respectively. Both A1j =
ˆT ×2 vT
2 are constant
tensors. The people parameter vector w ∈ (cid:3)r1×1
maintains
to be solved for new patch pairs.

1 and A2j = ˆT ×2 vT

j ×3 sT

j ×3 sT

Mode-1 ﬂattening eq. (9) results in

xj = (f1(A1j))
yj = (f1(A2j))

T w = Aj
T w = Aj

xw
yw,

(10)

x, Aj

y ∈ (cid:3)d×r1 are position-dependent inferring ma-
where Aj
trices. Note that the people parameter vector w is shared
in all patch pairs (xj , yj)m
j=1 appearing in the same per-
son. Deﬁning a concatenated photo feature vector Ix =
[x1; · · · ; xm] ∈ (cid:3)md×1
whose sketch counterpart is Iy =
[y1; · · · ; ym] ∈ (cid:3)md×1
and two enlarged md × r1 matrices
Ax = [A1

x ] and Ay = [A1

x; · · · ; Am

y; · · · ; Am

y ], we have

Ix = Axw
Iy = Ayw.

(11)

So we can solve the parameter vector w by the least square
method

(cid:2)
(cid:2)

w =

w =

T
x Ax
A

T
A
y Ay

(cid:3)
(cid:3)

−1

−1

T
x Ix = BxIx
A

T
y Iy = ByIy,
A

(12)

where both A
r1 which is always satisﬁed throughout this paper.

T
T
y Ay are invertible because md >>
x Ax and A

3 Bayesian Tensor Inference

The learned tensor representations eq. (9) model the latent
connection between Ix and w, or Iy and w. The concluded

IJCAI-07

2143

relations eq. (5), (6), (11), and (12) originate from factor de-
composition which is implemented through tensor decompo-
sition. Although eq. (7) gives a direct inference from x to y,
we lack analysis in statistical dependencies between x and y.
Utilizing the merits of super-resolution techniques [Free-
man et al., 2000; Baker and Kanade, 2002], we incorporate
the learned relations eq. (11) and (12) in which the tensor
model entails into a Bayesian framework. The entire infer-
ence process is thus called the Bayesian Tensor Inference.

3.1 Formulation
We fulﬁll the backward transform Iy → Ix through the peo-
ple parameter vector w by taking these quantities as a whole
into a global optimization formulation. Our inference ap-
proach is still deduced from canonical Bayesian statistics, ex-
ploiting PCA to represent the photo feature vector Ix to be
hallucinated. The advantage of our approach is to take into
account the statistics between the latent variable a ∈ (cid:3)l
and
Ix. We write the eigenface-domain representation after per-
forming PCA on the training photo vectors {I(i)

x }n

i=1

Ix = Ua + μ + ε ≈ Ua + μ,

(13)
where PCA noise ε may be ignored. The prior p(a) is easier
to acquire

p(a) ∝ exp{−aT Λ−1a},

(14)
where Λ is a l × l diagonal matrix with eigenvalues at its
diagonal.

Due to the Bayesian MAP (maximum a posterior) criterion,
we ﬁnd the MAP solution a∗ for hallucinating the optimal I∗
x
as follows

a∗ = arg max
w, a

p(w, a|Iy)

= arg max
w, a

= arg max
w, a

p(Iy|w, a)p(w, a)

p(Iy|w)p(w|a)p(a).

(15)

We take the statistics between w and Iy into considera-
tion. Due to the learned relation (see eq.(11)) Iy = Ayw,
we assume the representation error of Iy w.r.t w is i.i.d. and
Gaussian. The conditional PDF p(Iy|w) is thus obtained as
follows

(cid:5)

(cid:4)

p(Iy|w) ∝ exp

−

,

(16)

(cid:7)Iy − Ayw(cid:7)2

λ1

where λ1 scales the variance in the photo space. In analogy
to above, we obtain the condition PDF p(w|a) in doing w =
BxIx and using eq. (13)

(cid:5)

(cid:4)

p(w|a) ∝ exp

−

(cid:7)w − Bx(Ua + μ)(cid:7)2

λ2

,

(17)

where λ2 scales the variance in common variation space.

Substituting PDFs given in eq. (14), (16), and (17) into eq.
(15), maximizing p(w, a|Iy) is equivalent to minimizing the
following object function

E(w, a) =

(cid:7)Iy − Ayw(cid:7)2

λ1

+

(cid:7)w − Bx(Ua + μ)(cid:7)2

λ2

+ aT Λ−1a.

(18)

Signiﬁcantly, function (18) is quadratic, and consequently
minimizing it is a quadratic optimization problem (uncon-
strained) where a globally optimal solution exists.

Taking the derivative of E with respect to w and a respec-

tively, the partial gradients of E can be calculated as

(cid:7)

(cid:6)

(cid:7)

∂E
∂w = 2

T
A
y Ay
λ1

(cid:6)

+

I
λ2

w − 2

T
y Iy
A
λ1

+

BxUa + Bxμ

λ2

,

(cid:6)
(cid:7)

∂E
∂a = 2

UT B

T
x BxU
λ2

+ Λ−1

a − 2UT B

T
x (w − Bxμ)

λ2

.

By ∂E/∂w = 0 and ∂E/∂a = 0, we obtain a pair of solu-
tions {w∗, a∗} that is unique, and so must be globally opti-
mal. The optimal MAP solution a∗ is hereby given as

(cid:2)

T

(cid:6)

a∗ =

UT B

x C BxU + λ2Λ−1

(cid:3)

−1

UT B

T
x

(cid:7)

(cid:7)−1

− C Bxμ

,

(19)

T
λ2(I − C)A
y Iy

λ1

(cid:6)

T
λ2A
y Ay
λ1

where the r1 × r1 matrix C is predeﬁned as

C = I −

+ I

.

(20)

So far, we accomplish the task of backward transform, i.e.
hallucinating the photorealistic photo feature vector I∗
x =
Ua∗ +μ given a sketch feature vector Iy based on an available
set of myriad training photo-sketch image pairs.

3.2 Algorithm
Our algorithm tackles the intractable problem: sketch-based
facial photo hallucination with a learning-based scheme.
There have been several successful relevant efforts on facial
sketch synthesis. Example-based sketch generation [Chen et
al., 2001] takes pixels as rendering elements, so the computa-
tional efﬁciency is somewhat low. Linear methods [Tang and
Wang, 2004; Liu et al., 2005a] tend to trivialize the synthe-
sized results without any statistical consideration. The fact
that our work improves upon the existing techniques is that
patch-based tensor learning and Bayesian inference are coor-
dinated in our algorithm, which is illustrated in Fig. 3.

Collect a large corpus of n training facial image pairs
{I (n)
x , I (n)
n=1 with two styles, and divide each image into
m overlapping patches. Because the sketch image space and
photo image space are disparate in terms of gray-level inten-
sities, to bring the two spaces into correspondence, we use
the horizontal and vertical image derivatives as features of
both photo- and sketch-style patches for learning the Tensor-
Patches model. Concretely, we envision Bayesian Tensor In-
ference involving two phases: the training and testing phase.

y }N

Training Phase
1. Based on the derivative feature representation, build a 4-
order tensor on n ∗ m photo-sketch patch pairs sampled from
n training image pairs and learn the matrices Ay and Bx.
2. Construct concatenated photo feature vectors {I(i)
x }n

i=1,
and run PCA on them to achieve the eigen-system (U, Λ) and
the mean vector μ.

IJCAI-07

2144

training sketch -photo pairs

Local Geometry 

Preserving

(2)

testing sketch face

initial result

(1)

TensorPatches

Gradient 
Correction 

(4)

hallucinated face

Bayesian 

Tensor 
Inference

(3)

input image derivatives

inferred image derivatives

Figure 3: Architecture of our sketch-based facial photo hallu-
cination approach. (1) Learn the TensorPatches model taking
image derivatives as features in the training phase; (2) ob-
tain the initial result applying the local geometry preserving
method; (3) infer image derivatives of the target photo us-
ing the Bayesian Tensor Inference method, given input image
derivatives extracted from the test sketch face; (4) conduct
gradient correction to hallucinate the ﬁnal result.

Testing Phase
1. For a new sketch image Iy, divide it into m patches the
same way as in the training phase. Using grayscale intensities
for image features, apply the local geometry method [Liu et
al., 2005a] to estimate an initial hallucinated result I 0
x.

2. Construct the concatenated sketch feature vector Iy us-
ing m overlapping patches, and exploit it and Ay, Bx, U, Λ, μ
to infer the target photo feature vector I∗
x according to eq.(19).
x into m patches and afterward integrate these
x , with pix-

patches into two holistic derivative maps I h
els in the overlapping area blended.

3. Break I∗

x and I v

4. Conduct gradient correction on I 0

x using the derivative
x (i.e. gradient ﬁeld) to render the ﬁnal result I f
x .

x , I v

maps I h

position of centers of eyes and mouths. Each 160 × 100 face
image is divided into 160 overlapping 13 × 13 patches in the
same way, and the overlap between adjacent patches is three
pixels.

To display the effectiveness of our Bayesian Tensor Infer-
ence, we compare it with the representative methods, which
have been applied in facial sketch synthesis, including the
global method– eigentransform [Tang and Wang, 2004] and
the local method– local geometry preserving (LGP) [Liu et
al., 2005a]. Fig. 4 and Fig. 5 illustrates the results of sketch-
based face hallucination for Asians and Europeans, respec-
tively. In both the two groups of results, patch-based meth-
ods (c), (d) consistently and notably outperform the global
method (b) in high ﬁdelity of local details, which indicates
that a local model is more suitable for modeling complex dis-
tributions such as images. By comparing the results yielded
by LGP (c) and those produced by our method (d), we can
clearly see that our inference algorithm performs sharper and
more realistic with higher image quality than LGP. One rea-
son is that employing the patch-based tensor model can re-
cover both the common face structure and local details.

We set the scale parameters λ1 and λ2 to be 0.02 and 0.2,
respectively. We use standard SVD to do PCA on the train-
ing photo feature vectors, with 97% of the eigenvalue total
retained. When applying the LGP method to obtain the initial
hallucinated result in step 1 of the testing phase, we choose
K = 5 to be the number of nearest neighbors.

From the quantitative perspective, Table 1 lists the average
root-mean-square pixel errors (RMSE) for 4 different meth-
ods. The baseline is the nearest neighbor method. As might
be expected, the performance of our method does improve
as the number of training sample pairs increases from 250
(Training Set I) to 500 (Training Set II). Although the RMSE
of our method is larger than that of LGP, the hallucinated im-
ages look better, as detailed as that in the groundtruth face im-
ages. To sum up, our sketch-based face hallucination method
acquires the best visually perceptual quality.

Table 1: Comparison of Hallucination Methods.

Hallucination Method

Nearest Neighbor
Eigentransfrom

Local Geometry Preserving
Bayesian Tensor Inference

Training Set I
RMSE Red.

Training Set II
RMSE Red.

52.49
52.18
50.74
51.47

-

47.20
0.59% 43.14
3.33% 44.24
1.94% 44.90

-

8.60%
6.27%
4.87%

4 Experiments
Sketches often have many styles, for example, a cartoon
sketch often represents a person very exaggeratedly. In this
paper, we only focus on sketches of plain styles without ex-
aggeration, as for applications in law enforcement or photo
search, no much exaggeration is allowed in sketch images so
that the sketches can realistically describe the real subjects.
Some sketch examples are shown in Fig. 4 and Fig. 5.

We conduct experiments on a database over 600 persons of
which 500 persons are for training and the rest is for testing.
Each person has a frontal face photo image and a sketch im-
age with a plain style drawn by an artist. All the training and
testing images are normalized by afﬁne transform to ﬁx the

5 Conclusions

In this paper, we present a statistical inference approach
called Bayesian Tensor Inference between the photo patch
space and the sketch patch space. Based on the principle that
only commonalities contribute to the inference process and by
virtue of the patch-based tensor model, we capture and learn
the inter-space dependencies. The Bayesian MAP framework
integrates tensor modeling and statistical optimization to en-
sure the inference performance of the difﬁcult vision prob-
lem: sketch-based facial photo hallucination. The potency of
our method is sufﬁciently shown in the comparative experi-
ments, achieving surprising photo rendering quality.

IJCAI-07

2145

(a)

(b)

(c)

(d)

(e)

(a)

(b)

(c)

(d)

(e)

Figure 4: Photo hallucination results for Asian faces.
(a) Input sketch images, (b) eigentransform method, (c)
local geometry preserving method, (d) our method, (e)
groundtruth face photos.

Figure 5: Photo hallucination results for European faces.
(a) Input sketch images, (b) eigentransform method, (c)
local geometry preserving method, (d) our method, (e)
groundtruth face photos.

Acknowledgement

The work described in this paper was fully supported by
grants from the Research Grants Council of the Hong
Kong Special Administrative Region and a joint grant
(N CUHK409/03) from HKSAR RGC and China NSF. The
work was done at the Chinese University of Hong Kong.

References

[Baker and Kanade, 2002] S. Baker and T. Kanade. Limits
on super-resolution and how to break them. IEEE Trans.
on PAMI, 24(9):1167–1183, 2002.

[Chen et al., 2001] H. Chen, Y. Xu, H. Shum, S-C. Zhu, and
N. Zheng. Example-based facial sketch generation with
non-parametric sampling. In Proc. of ICCV, 2001.

[Freeman et al., 2000] W. Freeman,

E. Pasztor,

O. Carmichael.
40(1):25–47, 2000.

Learning low-level vision.

and
IJCV,

[Lathauwer et al., 2000] L. D. Lathauwer, B. D. Moor, and
J. Vandewalle. A multilinear singular value decomposi-
tion. SIAM Journal on Matrix Analysis and Applications,
21(4):1253–1278, 2000.

[Liu et al., 2005a] Q. Liu, X. Tang, H. Jin, H. Lu, and S. Ma.
A nonlinear approach for face sketch synthesis and recog-
nition. In Proc. of CVPR, 2005.

[Liu et al., 2005b] W. Liu, D. Lin, and X. Tang. Halluci-
nating faces: Tensorpatch super-resolution and coupled
residue compensation. In Proc. of CVPR, 2005.

[Roweis and Saul, 2000] S. T. Roweis and L. K. Saul. Non-
linear dimensionality reduction by locally linear embed-
ding. Science, 290(5500):2323–2326, 2000.

[Tang and Wang, 2004] X. Tang and X. Wang. Face sketch

recognition. IEEE Trans. on CSVT, 14(1):50–57, 2004.

[Vasilescu and Terzopoulos, 2002] M. Vasilescu and D. Ter-
zopoulos. Muiltilinear analysis of image ensembles: Ten-
sorfaces. In Proc. of ECCV, 2002.

[Vasilescu and Terzopoulos, 2003] M. Vasilescu and D. Ter-
zopoulos. Multilinear subspace analysis for image ensem-
bles. In Proc. of CVPR, 2003.

[Vlasic et al., 2005] D. Vlasic, M. Brand, H. Pﬁster, and
J. Popovic. Face transfer with multilinear models. In Proc.
of ACM SIGGRAPH 2005, pp. 426–433, 2005.

[Wang and Ahuja, 2003] H. Wang and N. Ahuja. Facial ex-

pression decomposition. In Proc. of ICCV, 2003.

IJCAI-07

2146


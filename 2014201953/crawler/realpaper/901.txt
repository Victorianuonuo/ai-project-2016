First Order Decision Diagrams for Relational MDPs

Chenggang Wang and Saket Joshi and Roni Khardon

Department of Computer Science

Tufts University

161 College Avenue

Medford, MA 02155, USA

{cwan|sjoshi01|roni}@cs.tufts.edu

Abstract

Dynamic programming algorithms provide a basic
tool identifying optimal solutions in Markov Deci-
sion Processes (MDP). The paper develops a rep-
resentation for decision diagrams suitable for de-
scribing value functions, transition probabilities,
and domain dynamics of First Order or Relational
MDPs (FOMDP). By developing appropriate op-
erations for such diagrams the paper shows how
value iteration can be performed compactly for
such problems. This improves on previous ap-
proaches since the representation combines com-
pact form with efﬁcient operations. The work also
raises interesting issues on suitability of different
representations to different FOMDPs tasks.

1 Introduction

In the past years there has been an increased interest in de-
veloping relational or ﬁrst order MDPs. Some examples in-
clude symbolic dynamic programming (SDP) [Boutilier et
al., 2001], the relational Bellman algorithm (ReBel) [Ker-
sting et al., 2004], approximate linear programming for
FOMDPs [Guestrin et al., 2003; Sanner and Boutilier, 2005],
approximate policy iteration [Fern et al., 2003], and induc-
tive policy selection using ﬁrst order regression [Gretton and
Thiebaux, 2004].

Among these, only SDP and ReBel are exact solution
methods. To our knowledge there is no working implemen-
tation of SDP because it is hard to keep the state formulas
consistent and of manageable size in the context of situation
calculus. Compared with SDP, ReBel provides a more prac-
tical solution. ReBel uses simpler language (a probabilistic
STRIPS-like language) to represent FOMDPs, so that reason-
ing over formulas is easier to perform.

Inspired by the successful application of Algebraic Deci-
sion Diagrams (ADD) [Bryant, 1986; Bahar et al., 1993] in
solving propositionally factored MDPs [Hoey et al., 1999;
St-Aubin et al., 2000] we lift propositional ADDs to handle
relational structure and use them in the solution of FOMDPs.
The intuition behind this idea is that the ADD representation
allows information sharing, e.g., sharing between state parti-
tions. If there is sufﬁcient regularity in the model, ADDs can

be very compact, allowing problems to be represented and
solved efﬁciently.

First order decision trees and even decision diagrams
have already been considered in the literature [Blockeel and
De Raedt, 1998; Groote and Tveretina, 2003] and several se-
mantics for such diagrams are possible. In particular Groote
and Tveretina [2003] provide a notation for ﬁrst order BDDs
that can capture formulas in Skolemized conjunctive normal
form and then provide a theorem prover based on this repre-
sentation. In this paper we adapt and extend their approach to
handle ﬁrst order MDPs. In particular, we extend the deﬁni-
tions to handle existential quantiﬁcation and numerical leaves
through the use of an aggregation function. This allows us to
capture value functions using algebraic diagrams in a natural
way. We also provide additional reduction transformations
for algebraic diagrams that help keep their size small, and al-
low the use of background knowledge in reductions. We then
develop appropriate representation and algorithms showing
how value iteration can be performed using the decision dia-
grams.

It is useful to compare our solutions to the propositional
ones. The main difﬁculty in lifting the ideas is that in rela-
tional domains the transition function speciﬁes schemas for
conditional probabilities. The propositional solution uses the
concrete conditional probability to calculate the regression
function. But this is not possible with schemas. While one
can ﬁrst ground the domain and problem at hand and only
then perform the reasoning (e.g. [Sanghai et al., 2005]) this
does not allow for solutions abstracting over domains and
problems. Like SDP and ReBel our constructions do perform
general reasoning and they do so by using decision diagrams.
Due to space constraints most proofs and some details are

omitted from the paper.

2 Markov Decision Processes
We assume familiarity with standard notions of MDPs and
value iteration [Puterman, 1994]. In the following we intro-
duce some of the notions and our notation. A MDP can be
characterized by a state space S, an action space A, a state
transition function P r(sj|si, a) denoting the probability of
transition to state sj given state si and action a, and an imme-
diate reward function r(s), specifying the immediate utility
of being in state s. A solution to a MDP is an optimal policy
that maximizes expected discounted total reward as deﬁned

IJCAI-07

1095

by the Bellman equation. The value iteration algorithm uses
the Bellman equation to iteratively reﬁne an estimate of the
value function:

(cid:2)

Vn+1(s) = maxa∈A[r(s) + γ
where Vn(s) represents our current estimate of the value

s(cid:2)∈S P r(s(cid:3)|s, a)Vn(s(cid:3))]

function and Vn+1(s) is the next estimate.

The main observation used by Hoey et al. [1999] is that if
we can represent each of r(s), P r(s(cid:3)|s, a), and Vk(s) com-
pactly using ADDs then value iteration can be done directly
using this representation, avoiding the need to enumerate the
state space which is implicit in the equation above.

Taking the next step, the SDP approach [Boutilier et al.,
2001] was developed in the context of the situation calcu-
lus. Stochastic actions are speciﬁed as a non-deterministic
choice among deterministic alternatives. In this way one can
separate the regression over action effects, which is now de-
terministic, from the probabilistic choice of action. On each
regression step during value iteration, the value of a stochastic
action A((cid:3)x) parameterized with free variables (cid:3)x is determined
in the following manner:

QV (A((cid:3)x), s) = rCase(s)⊕

γ[⊕jpCase(nj((cid:3)x), s) ⊗ regr(vCase(do(nj ((cid:3)x), s)))]
where rCase(s) and vCase(s) denote reward and value
functions in the compact “case notation” of Boutilier et
al. [2001], nj((cid:3)x) denotes the possible outcomes of A((cid:3)x), and
pCase(nj((cid:3)x), s) the choice probabilities for nj((cid:3)x).

After the regression, we need to maximize over the action
parameters of each Q-function to get the maximum value that
could be achieved by using an instance of this action. In SDP,
this is done by adding the negation of higher value partitions
into the description of lower value partitions, leading to com-
plex formulas and reasoning. Finally, to get the next value
function we maximize over the choice of action.

The solution of ReBel [Kersting et al., 2004] follows the
same outline but uses a probabilistic STRIPS-like language
for representing FOMDPs. More importantly the paper uses
a decision list [Rivest, 1987] style representation for value
functions. The decision list gives us an implicit maximization
operator since rules higher on the list are evaluated ﬁrst. As
a result the object maximization step is very simple. On the
other hand regression in ReBel requires that one enumerate
all possible matches between a subset of a state partition and
action effects and reason about each of these separately.

3 First Order Decision Diagrams

An Algebraic Decision Diagram is a labeled directed acyclic
graph where non-leaf nodes are labeled with propositional
variables, each non-leaf node has exactly two children cor-
responding to true and false branches, and leaves are
labeled with numerical values. Ordered decision diagrams
specify a ﬁxed order on propositions and require that node la-
bels respect this order on every path in the diagram. In this
case every function has a unique canonical representation and
diagrams have efﬁcient manipulation algorithms, leading to
successful applications [Bryant, 1986; Bahar et al., 1993].

There are various ways to generalize ADDs to capture rela-
tional structure. One could use closed or open formulas in the
nodes, and in the latter case we must interpret the quantiﬁca-

Figure 1: A simple FODD.

tion over the variables. We focus on the following syntactic
deﬁnition which does not have any explicit quantiﬁers.

Deﬁnition of First Order Decision Diagrams:

(1) We assume a signature with a ﬁxed set of predicates and
constant symbols, and an enumerable set of variables. We
also allow to use an equality between any pair of terms (con-
stants or variables).
(2) A First Order Decision Diagram (FODD) is a labeled di-
rected acyclic graph, where each non-leaf node has exactly
two children. The outgoing edges are marked with values
true and false.
(3) Each non-leaf node is labeled with: an atom P (t1, . . . , tn)
or an equality t1 = t2 where ti is a variable or a constant.
(4) Leaves are labeled with numerical values.

Figure 1 shows a FODD with binary leaves. Left going
edges represent true branches. To simplify diagrams in the
paper we draw multiple copies of the leaves 0 and 1 but they
represent the same node in the FODD.

The semantics of ﬁrst order formulas are given relative to
interpretations. An interpretation has a domain of elements,
a mapping of constants to domain elements, and for each
predicate a relation over the domain elements which speci-
ﬁes when the predicate is true. There is more than one way
to deﬁne the meaning of FODD B on interpretation I. In the
following we discuss two possibilities.

Semantics based on a single path: A semantics for deci-
sion trees is given by Blockeel and De Raedt [1998] that can
be adapted to FODDs. The semantics deﬁne a unique path
that is followed when traversing B relative to I. All variables
are existential and a node is evaluated relative to the path lead-
ing to it. For example, if we evaluate the diagram in Figure 1
on the interpretation I1 with domain {1, 2, 3} and relations
{p(1), q(2), h(3)} then we follow the true branch at the
root since ∃x, p(x) is satisﬁed, but we follow the false
branch at q(x) since ∃x, p(x) ∧ q(x) is not satisﬁed. Since
the leaf is labeled with 0 we say that B does not satisfy I.
This is an attractive approach, since it builds mutually exclu-
sive partitions over states, and various FODD operations can
be developed for it. However, for reasons we discuss later
this semantics is not well suited to value iteration, and it is
therefore not used in the paper.

Semantics based on a multiple paths: Following Groote
and Tveretina [2003] we deﬁne the semantics ﬁrst relative to
a variable valuation ζ. Given a FODD B over variables (cid:3)x and
an interpretation I, a valuation ζ maps each variable in (cid:3)x to a
domain element in I. Once this is done, each node predicate
evaluates either to true or false and we can traverse a
single path to a leaf. The value of this leaf is denoted by
MAPB(I, ζ).

We next deﬁne MAPB(I) = aggregateζ {MAPB(I, ζ)} for
some aggregation function. That is, we consider all possible

IJCAI-07

1096

valuations ζ, for each we calculate MAPB(I, ζ) and then we
In [Groote and Tveretina,
aggregate over all these values.
2003] leaf labels are in {0, 1} and variables are universally
quantiﬁed; this is easily captured by using minimum as the
aggregation function. In this paper we use maximum as the
aggregation function. This corresponds to existential quan-
tiﬁcation in the binary case, and gives useful maximization
for value functions in the general case. We therefore deﬁne:
MAPB(I) = maxζ {MAPB(I, ζ)}.

Consider evaluating the diagram in Figure 1 on the inter-
pretation I1. The valuation {x/2, y/3} leads to a leaf with
value 1 so the maximum is 1 and we say that I satisﬁes B.

We deﬁne node formulas (NF) and edge formulas (EF) re-
cursively as follows. For a node n labeled l(n) with incom-
ing edges e1, . . . , ek, the node formula NF(n) = (∨iEF(ei)).
Denote the true branch of a node n by n↓t and the false
branch by n↓f . The edge formula for the true outgoing
edge of n is EF(n↓t) = NF(n) ∧ l(n). The edge formula for
the false outgoing edge of n is EF(n↓f ) = NF(n)∧¬l(n).
These formulas, where all variables are existentially quanti-
ﬁed, capture reachability conditions for the node or edge.

Basic Reduction of FODDs:

and
Tveretina [2003] deﬁne several operators that reduce a
diagram into “normal form”. A total order over open predi-
cates (node labels) is assumed. We describe these operators
brieﬂy and give their main properties.

Groote

(R1) Neglect operator: if both children of a node p in the
FODD lead to the same node q then we remove p and link all
parents of p to q directly. (R2) Join operator: if two nodes p, q
have the same label and point to the same two children then
we can join p and q (remove q and link q’s parents to p). (R3)
Merge operator: if a node and its child have the same label
then the parent can point directly to the grandchild. (R4) Sort
operator: If a node p is a parent of q but the label ordering is
violated (l(p) > l(q)) then we can reorder the nodes locally
using two copies of p and q such that labels of the nodes do
not violate the ordering.

Deﬁne a FODD to be reduced if none of the four operators

can be applied. We have the following:

Theorem 1 [Groote and Tveretina, 2003]
(1) Let O ∈ {Neglect, Join, Merge, Sort} be an operator and
O(B) the result of applying O to FODD B, then for any ζ,
MAPB(I, ζ) = MAPO(B)(I, ζ)
(2) if B1, B2 are reduced and satisfy ∀ζ, MAPB1
MAPB2

(I, ζ) then they are identical.

(I, ζ) =

Property (1) gives soundness, and property (2) shows that re-
ducing a FODD gives a normal form. This only holds if the
maps are identical for every ζ and this condition is stronger
than normal equivalence. However, this weak normal form
sufﬁces for Groote and Tveretina [2003] who use it to pro-
vide a theorem prover for ﬁrst order logic.

Combining FODDs: Given two algebraic diagrams we
may need to add the corresponding functions, take the max-
imum or use any other binary operation op over the values
represented by the functions. Here we adopt the solution
from the propositional case [Bryant, 1986] in the form of
the procedure Apply(p, q, op) where p and q are the roots of
two diagrams. This procedure chooses a new root label (the

lower among labels of p, q) and recursively combines the cor-
responding sub-diagrams, according to the relation between
the two labels (<, =, or >).

Additional Reduction Operators:

In our context, es-
pecially for algebraic FODDs we may want to reduce the
diagrams further. We distinguish strong reduction that pre-
serves MAPB(I, ζ) for all ζ and weak reduction that only
preserves MAPB(I).
In the following let B represent any
background knowledge we have about the domain. For exam-
ple in the Blocks World we may know that ∀x, y, [on(x, y) →
¬clear(y)].

(R5) Strong Reduction for Implied Branches: Consider
any node n with label l(n). Let (cid:3)x be the variables in EF(n↓t).
If B |= ∀(cid:3)x, [NF(n) → l(n)] then whenever node n is reached
then the true branch is followed. In this case we can re-
move n and connect its parent directly to the true branch.
It is clear that the map is preserved for any valuation. A sim-
ilar reduction can be formulated for the false branch.

(R6) Weak Reduction Removing Dominated Siblings:
Consider any node n such that if we can reach n we can also
reach n↓t. If n↓t always gives better values than n↓f then we
should be able to remove n↓f from the diagram. We start by
giving a special case of this condition.

Let (cid:3)x be the variables that appear in NF(n), and (cid:3)y the vari-
ables in l(n) and not in NF(n). Consider the condition (I1):
B |= ∀(cid:3)x, [NF(n) → ∃(cid:3)y, l(n)] which requires that every valu-
ation reaching n can be extended to reach n↓t.

Let min(n↓t) be the minimum leaf value in n↓t, and
max(n↓f ) be the maximum leaf value in n↓f . Consider next
the additional condition (V1): min(n↓t) ≥ max(n↓f ).
In
this case regardless of the valuation we know that it is better
to follow n↓t and not n↓f . If both I1 and V1 hold then ac-
cording to maximum aggregation the value of MAPB(I) will
never be determined by the false branch. Therefore we
can safely replace n↓f with any constant value between 0 and
min(n↓t) without changing the map. A symmetric operation
can be applied exchanging the roles of n↓t and n↓f .

In some cases we can also drop the node n completely and
connect its parents directly to n↓t. This can be done if (I1A):
B |= ∀(cid:3)u, [∃(cid:3)v, NF(n)] → [∃(cid:3)v, (cid:3)w, EF(n↓t)] where (cid:3)u are the
variables that appear in (the sub-diagram of) n↓t, (cid:3)v the vari-
ables that appear in NF(n) but not in n↓t, and (cid:3)w the variables
in l(n) and not in (cid:3)u or (cid:3)w. This condition requires that for ev-
ery valuation ζ1 that reaches n↓f there is a valuation ζ2 that
reaches n↓t and such that ζ1 and ζ2 agree on all variables in
n↓t. It is easy to see that I1A follows from I1 if (I1B): no
variable in (cid:3)y appears in the sub-diagram of n↓t.

An important special case of R6 occurs when l(n) is an
equality t1 = y where y is a variable that does not occur on
the FODD above node n. In this case, the condition I1 holds
since we can choose the value of y. Therefore if V1 holds
we can remove the node n connecting its parents to n↓t and
substituting t1 for y in the diagram n↓t.

(R6) General Case: The conditions for replacing n↓f with
a constant and for dropping n completely can be relaxed. Due
to space constraints, we only sketch the details.
I1 can be
relaxed to (I2): B |= [∃(cid:3)x, NF(n)] → [∃(cid:3)x, (cid:3)y, EF(n↓t)], which
requires that if n is reachable then n↓t is reachable but does
not put any restriction on the valuations (in contrast with I1).

IJCAI-07

1097

next state when A((cid:3)a) has been performed in the current state.
We call (cid:3)a action parameters, and (cid:3)x predicate parameters. No
other variables are allowed in the TVD.

Notice that the TVD simultaneously captures the truth val-
ues of all instances of p((cid:3)x) in the next state. Notice also that
TVDs for different predicates are separate and independent.
This can be safely done even if an action has correlated ef-
fects (not conditionally independent) since the actions are de-
terministic.

For any domain, a TVD for predicate p((cid:3)x) can be deﬁned
generically as in Figure 2. The idea is that the predicate is
true if it was true before and is not “undone” by the action or
was false before and is “brought about” by the action. TVDs
for the logistics domain in our running example are given in
Figure 3. All the TVDs omitted in the ﬁgure are trivial in the
sense that the predicate is not affected by the action. In order
to simplify the presentation we give the TVDs in their generic
form and did not sort the diagrams.

Notice how we utilize the multiple path semantics with
maximum aggregation. A predicate is true if it is true ac-
cording to one of the paths speciﬁed so we get a disjunction
over the conditions for free. If we use the single path seman-
tics then a single path in a TVD must capture all possibilities
for a predicate to become true in a state. Thus different con-
ditions must be tested sequentially and their bindings must be
combined so the corresponding notion of TVD is signiﬁcantly
more complicated.

Probabilistic Action Choice: Multiple path semantics
makes it hard to specify mutually exclusive conditions us-
ing existentially quantiﬁed variables and in this way specify
a distribution. We therefore restrict the conditions to be ei-
ther propositional or depend directly on the action parame-
ters. Notice that under this restriction any interpretation fol-
lows exactly one path (since there are no variables and thus
only the empty valuation) so the aggregation function does
not interact with the probabilities assigned. A diagram show-
ing the choice probability for unloadS in our logistics exam-
ple is given in Figure 3.

Reward and value functions: can be represented directly

using algebraic FODDs. An example is given in Figure 3.

5 Value Iteration with FODDs
The general ﬁrst order value iteration algorithm works as fol-
lows: given as input the reward function R and the action
model, we set V0 = R, n = 0 and perform the following
steps until termination:
(1) For each action type A((cid:3)x), compute:

n+1.

Vn+1 = maxA QA
Regression by Block Replacement: Consider Vn and
the nodes in its FODD. For each such node take a copy of the
corresponding TVD, where predicate parameters are renamed
so that they correspond to the node’s arguments and action
parameters are unmodiﬁed. Block Replacement Regression
BR-regress(Vn, Aj((cid:3)x)) is the FODD resulting from replacing
each node in Vn with the corresponding TVD, with outgoing

T

A((cid:3)x)
n+1 (Vn) = ⊕j(prob(Aj ((cid:3)x)) ⊗ Regr(Vn, Aj((cid:3)x))).
n+1 = R ⊕ [γ ⊗ obj-max(T

(2) QA
(3) Vn+1 is obtained by maximizing over Qn+1:

A((cid:3)x)
n+1 (Vn))].

Figure 2: A template for the TVD

Let D = n↓t − n↓f which we can calculate using Apply.
V1 can be relaxed to (V2): all leaves in D have non-negative
values. But using V2 requires a condition stronger than I2.

(R7) Weak Reduction Removing Dominated Edges:
Consider a FODD with two nodes p, q where q is in the
sub-FODD of p↓f and their formulas satisfy that if we can
follow q↓t then we can also follow p↓t.
In this case, if
min(p↓t) ≥ max(q↓t) then MAPB(I) will never be deter-
mined by q↓t so we can replace q↓t with a constant between 0
and min(p↓t). If in addition we have min(p↓t) ≥ max(q↓f )
then it is also safe to remove q completely. Due to space con-
straints, we omit the details and the general case of R7.

(R8) Weak Reduction by Uniﬁcation: Consider a FODD
B and two sets of variables (cid:3)x and (cid:3)y of the same cardinality.
By B{(cid:3)x/(cid:3)y} we denote the FODD resulting from replacing
variables in (cid:3)x by the corresponding variables in (cid:3)y. Now con-
sider the FODD B{(cid:3)x/(cid:3)y} − B which we can calculate using
Apply.
If all leaves in this diagram are non negative then
∀I, MAPB(I) = MAPB{(cid:3)x/(cid:3)y}(I) so we can safely replace B.

4 Decision Diagrams for MDPs
We follow Boutilier et al. [2001] and specify stochastic ac-
tions as a non-deterministic choice among deterministic al-
ternatives. We therefore need to use FODDs to represent the
deterministic domain dynamics of actions, the probabilistic
choice among actions, and value functions.

Example Domain: We use the logistics problem vari-
ant from [Boutilier et al., 2001] to illustrate our constructions
for MDPs. The domain includes boxes, trucks and cities,
and predicates are Bin(Box, City), T in(T ruck, City), and
On(Box, T ruck) with their obvious meaning. The reward
function, capturing a planning goal, awards a reward of 10 if
the formula ∃b, Bin(b, P aris) is true, that is if there is any
box in Paris.

The domain includes 3 actions load, unload, and drive.
Actions have no effect if their preconditions are not met. Ac-
tions can also fail with some probability. When attempting
load, a successful version loadS is executed with probability
0.99, and an unsuccessful version loadF (effectively a no-
operation) with probability 0.01. The drive action is executed
deterministically. When attempting unload, the probabilities
depend on whether it is raining or not. If it is not raining (rain-
ing) then unloadS is executed with probability 0.9 (0.7), and
unloadF with probability 0.1 (0.3).

The domain dynamics:

are deﬁned by truth value dia-
grams (TVDs). For every action schema A((cid:3)a) and each pred-
icate schema p((cid:3)x) the TVD T (A((cid:3)a), p((cid:3)x)) is a FODD with
{0, 1} leaves. The TVD gives the truth value of p((cid:3)x) in the

IJCAI-07

1098

≠

(a)(b) The TVDs for
Figure 3: Logistics domain: TVDs, action choice probabilities, reward function, and regression.
Bin(B, C) and On(B, T ) under action choice unloadS(b∗, t∗). (c)(d) The TVDs for Bin(B, C) and On(B, T ) under ac-
tion choice loadS(b∗, t∗, c∗). Note that c∗
must be an action parameter so that (d) is a valid TVD. (e) The TVD for T in(T, C)
under action drive(t∗, c∗). (f) The probability FODD for the action choice unloadS(b∗, t∗). (g) The reward function R and
the value function V0. (h) Regression of V0 through unloadS(b∗, t∗). (i) Multiply (h) with the choice probability FODD. (j)
Regression of V0 over unloadF (b∗, t∗) multiplied with the probability. (k) The unreduced result of adding two outcomes for
unload(b∗, t∗). (l) The reduced result after addition. Notice that the left branch reduces to 10 by using both the recursive part of
Apply and R6. The middle part is dropped by R7. (m) Multiply by γ = 0.9, perform object maximization, and add the reward
to get Qunload
. Notice that in object maximization we have also dropped the equality on the right branch by the special case
of R6. It turns out that V1, the value function after ﬁrst iteration, is the same as Qunload
. In this case the diagram for unload
dominates the other actions (not shown). (n) Block replacement in computing V2 through action unloadS(b∗, t∗) .

1

1

IJCAI-07

1099

edges connected to the 0, 1 leaves of the TVD. One can show
that block replacement preserves the map for any valuation
w.r.t. corresponding states.

However, naive implementation of block replacement may
not be efﬁcient.
If we use block replacement for regres-
sion then the resulting FODD is not necessarily reduced or
sorted. Reducing and sorting the results may be an expen-
sive operation.
Instead we calculate the result as follows.
We traverse Vn using postorder traversal, where in the re-
cursive step we combine the TVD block of the correspond-
ing node (which is a TVD with binary leaves) with the pro-
cessed versions of its children (which are general FODDs).
If we call the parent B, the true branch child Bt and the
false branch child Bf then their combination is equivalent
to [B × Bt] + [(1 − B) × Bf ]. The result can be calculated by
several calls to the Apply procedure. We can use strong re-
duction during block combination; weak reductions can only
be applied after all blocks have been combined.

Object Maximization: As mentioned above we get max-
imization over action parameters for free. We simply rename
the action parameters using new variable names (to avoid rep-
etition between iterations) and consider them as variables.
The aggregation semantics provides the maximization. Since
constants are turned into variables additional reduction is typ-
ically possible at this stage. Any combination of weak and
strong reductions can be used.

Adding and Maximizing Over Actions:

These can
be done directly using the Apply procedure. Recall that
prob(Aj ((cid:3)x)) is restricted to include only action parameters
and cannot include variables. We can therefore calculate
prob(Aj ((cid:3)x)) ⊗ Regr(Vn, Aj((cid:3)x)) in step (1) directly. How-
ever, the different regression results are independent func-
tions so that in the sum ⊕j(prob(Aj ((cid:3)x))⊗Regr(Vn, Aj((cid:3)x)))
we must standardize apart the different regression results be-
fore adding the functions (note that action parameters are
still considered constants at this stage). Similarly the max-
imization Vn+1 = maxA QA
n+1 in step (3) must ﬁrst stan-
dardize apart the different diagrams. The need to standardize
apart complicates the diagrams and often introduces struc-
ture that can be reduced. In each of these cases we ﬁrst use
the propositional Apply procedure and then follow with weak
and strong reductions.

Figure 3 traces several steps in the application of value iter-
ation to the logistics domain. In order to simplify the presen-
tation the diagrams are not completely sorted allowing equal-
ities in arbitrary locations.

6 Discussion

ADDs have been used successfully to solve propositional
factored MDPs. Our work gives one proposal of lifting
these ideas to FOMDPs. While the general steps are simi-
lar the technical details are signiﬁcantly more involved than
the propositional case. It is easy to see that our approach can
capture probabilistic STRIPS style formulations as in ReBel,
allowing for more ﬂexibility in representation. However, it is
more limited than SDP since we cannot use arbitrary formu-
las for rewards, transitions, and probabilistic choice (e.g. no
universal quantiﬁcation).

An implementation and empirical evaluation are obvious
next steps. Also it would be interesting to investigate con-
ditions that guarantee a normal form for a useful set of re-
duction operators, and improvements of the representation to
achieve further compression.

Acknowledgments
This work has been partly supported by NSF Grant IIS-0099446, and
by a Research Semester Fellowship Award from Tufts University.

References
[Bahar et al., 1993] R. I. Bahar, E. A. Frohm, C. M. Gaona, G. D.
Hachtel, E. Macii, A. Pardo, and F. Somenzi. Algebraic decision
diagrams and their applications. In Proceedings of the Interna-
tional Conference on Computer-Aided Design, 1993.

[Blockeel and De Raedt, 1998] H. Blockeel and L. De Raedt. Top
down induction of ﬁrst order logical decision trees. Artiﬁcial In-
telligence, 101:285–297, 1998.

[Boutilier et al., 2001] C. Boutilier, R. Reiter, and B. Price. Sym-
bolic dynamic programming for ﬁrst-order MDPs. In Proceed-
ings of the International Joint Conf. on Artiﬁcial Intelligence,
2001.

[Bryant, 1986] R. E. Bryant. Graph-based algorithms for boolean
IEEE Transactions on Computers, C-

function manipulation.
35(8):677–691, 1986.

[Fern et al., 2003] A. Fern, S. Yoon, and R. Givan. Approximate
policy iteration with a policy language bias. In International Con-
ference on Neural Information Processing Systems, 2003.

[Gretton and Thiebaux, 2004] C. Gretton and S. Thiebaux. Exploit-
In Pro-
ing ﬁrst-order regression in inductive policy selection.
ceedings of the International Conf. on Machine Learning, 2004.

[Groote and Tveretina, 2003] J. F. Groote and O. Tveretina. Binary
decision diagrams for ﬁrst-order predicate logic. The Journal of
Logic and Algebraic Programming, 57:1–22, 2003.

[Guestrin et al., 2003] C. Guestrin, D. Koller, C. Gearhart, and
N. Kanodia. Generalizing plans to new environments in relational
MDPs. In Proceedings of the International Joint Conference of
Artiﬁcial Intelligence, 2003.

[Hoey et al., 1999] J. Hoey, R. St-Aubin, A. Hu, and C. Boutilier.
SPUDD: Stochastic planning using decision diagrams. In Pro-
ceedings of Uncertainty in Artiﬁcial Intelligence, 1999.

[Kersting et al., 2004] K. Kersting, M. V. Otterlo, and L. D. Raedt.
Bellman goes relational. In Proceedings of the International Con-
ference on Machine Learning, 2004.

[Puterman, 1994] M. L. Puterman. Markov decision processes:

Discrete stochastic dynamic programming. Wiley, 1994.

[Rivest, 1987] R. L. Rivest. Learning decision lists. Machine

Learning, 2(3):229–246, 1987.

[Sanghai et al., 2005] S. Sanghai, P. Domingos, and D. Weld. Re-
lational dynamic Bayesian networks. Journal of Artiﬁcial Intelli-
gence Research, 24:759–797, 2005.

[Sanner and Boutilier, 2005] S. Sanner and C. Boutilier. Approxi-
mate linear programming for ﬁrst-order MDPs. In Proceedings
of the Workshop on Uncertainty in Artiﬁcial Intelligence, 2005.

[St-Aubin et al., 2000] R. St-Aubin, J. Hoey, and C. Boutilier.
APRICODD: Approximate policy construction using decision di-
agrams. In International Conference on Neural Information Pro-
cessing Systems, 2000.

IJCAI-07

1100


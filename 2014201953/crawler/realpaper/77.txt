Generalization Error of Linear Neural Networks

in an Empirical Bayes Approach

Shinichi Nakajima †‡ and Sumio Watanabe †

† Tokyo Institute of Technology

Mailbox R2-5, 4259 Nagatsuda, Midori-ku, Yokohama, Kanagawa, 226-8503 Japan

nakajima.s@cs.pi.titech.ac.jp, swatanab@pi.titech.ac.jp

‡ Nikon Corporation

201-9 Oaza-Miizugahara, Kumagaya, Saitama, 360-8559 Japan
Abstract

It is well known that in unidentiﬁable models, the
Bayes estimation has the advantage of generaliza-
tion performance to the maximum likelihood esti-
mation. However, accurate approximation of the
posterior distribution requires huge computational
costs. In this paper, we consider an empirical Bayes
approach where a part of the parameters are re-
garded as hyperparameters, which we call a sub-
space Bayes approach, and theoretically analyze
the generalization error of three-layer linear neu-
ral networks. We show that a subspace Bayes ap-
proach is asymptotically equivalent to a positive-
part James-Stein type shrinkage estimation, and be-
haves similarly to the Bayes estimation in typical
cases.

1 Introduction
Unidentiﬁable parametric models, such as neural networks,
mixture models, and so on, have a wide range of applica-
tions. These models have singularities in the parameter space,
hence the conventional learning theory of the regular sta-
tistical models does not hold. Recently, generalization per-
formance of some unidentiﬁable models has been theoreti-
cally clariﬁed. In the maximum likelihood (ML) estimation,
which is asymptotically equivalent to the maximum a poste-
rior (MAP) estimation, the generalization error of linear neu-
ral networks was proved to be greater than that of the reg-
ular models whose dimension of the parameter space is the
same when the model is redundant to learn the true distri-
bution [Fukumizu, 1999]. On the other hand, in the Bayes
estimation, the generalization error of neural networks, lin-
ear neural networks, mixture models, and so on was proved
to be less than that of the regular models [Watanabe, 2001;
Aoyagi and Watanabe, 2004; Yamazaki and Watanabe, 2003].
However, the Bayes posterior distribution can seldom be
exactly realized. Furthermore, Markov chain Monte Carlo
(MCMC) methods, often used for approximation of the pos-
terior distribution, require huge computational costs. As an
alternative, the variational Bayes approach, where the corre-
lation between parameters and the other parameters, or the
correlation between the parameters and the hidden variables

is neglected, was proposed [Hinton and van Camp, 1993;
MacKay, 1995; Attias, 1999; Ghahramani and Beal, 2000].1
In this paper, we consider another alternative, which we
call a subspace Bayes (SB) approach. An SB approach is an
empirical Bayes (EB) approach where a part of the parame-
ters of a model are regarded as hyperparameters. If we re-
gard the parameters of one layer as hyperparameters, we can
analytically calculate the marginal likelihood in some three-
layer models. Consequently, what we have to do is only to
ﬁnd the hyperparameter value maximizing the marginal like-
lihood. The computational costs of an SB approach is thus
much less than that of posterior distribution approximation by
MCMC methods. At ﬁrst in this paper, we prove that in three-
layer linear neural networks, an SB approach is equivalent
to a positive-part James-Stein (JS) type shrinkage estimation
[James and Stein, 1961]. Then, we clarify its generalization
error, also considering delicate situations, the most important
situations in model selection problems and in statistical tests,
when the Kullback-Leibler divergence of the true distribution
from the singularities is comparable to the inverse of the num-
ber of training samples.2 We conclude that an SB approach
provides as good performance as the Bayes estimation in typ-
ical cases.

In Section 2, neural networks and linear neural networks
are brieﬂy introduced. The framework of the Bayes estima-
tion, that of an EB approach, and that of an SB approach are
described in Section 3. The signiﬁcance of singularities for
generalization performance and the importance of analysis of
delicate situations are explained in Section 4. The SB solu-
tion and its generalization error are derived in Section 5. Dis-
cussions and conclusions follow in Section 6 and in Section 7,
respectively.

M be an input (column) vector, y ∈ R

2 Linear Neural Networks
Let x ∈ R
N an output
vector, and w a parameter vector. A neural network model
can be described as a parametric family of maps {f(·; w) :
N}. A three-layer neural network with H hidden
M (cid:2)→ R
1We have just derived the variational Bayes solution of linear
neural networks and clariﬁed its generalization error and training
error in [Nakajima and Watanabe, 2005b].

R

2We have also clariﬁed the training error, which is put into

[Nakajima and Watanabe, 2005a].

(cid:1)H
M × R

units is deﬁned by

hx),

h=1 bhψ (at

f(x; w) =
(1)
where w = {(ah, bh) ∈ R
N ; h = 1, . . . , H} summa-
rizes all the parameters, ψ(·) is an activation function, which
is usually a bounded, non-decreasing, antisymmetric, nonlin-
ear function like tanh(·), and t denotes the transpose of a
matrix or vector. Assume that the output is observed with a
noise subject to NN (0, σ2IN ), where Nd(µ, Σ) denotes the
d-dimensional normal distribution with average vector µ and
covariance matrix Σ, and Id denotes the d×d identity matrix.
Then, the conditional distribution is given by

(cid:2)
−(cid:4)y − f(x; w)(cid:4)2

(cid:3)

2σ2

.

(2)

p(y|x, w) =

1

(2πσ2)N/2 exp

In this paper, we focus on linear neural networks, whose ac-
tivation function is linear, as the simplest multilayer models.3
A linear neural network model (LNN) is deﬁned by

f(x; A, B) = BAx,

(3)
where A = (a1, . . . , aH)t is an H × M input parameter ma-
trix and B = (b1, . . . , bH) is an N × H output parameter
matrix. Because the transform (A, B) (cid:2)→ (T A, BT −1) does
not change the map for any non-singular H × H matrix T ,
the parameterization in Eq.(3) has trivial redundancy. Ac-
cordingly, the essential dimension of the parameter space is
given by

K = H(M + N) − H 2.

(4)

We assume that H ≤ N ≤ M throughout this paper.
3 Framework of Learning Methods
3.1 Bayes Estimation
Let X n = {x1, . . . , xn} and Y n = {y1, . . . , yn} be arbitrary
n training samples independently and identically taken from
the true distribution q(x, y) = q(x)q(y|x). The marginal con-
ditional likelihood of a model p(y|x, w) is given by
(cid:5)n
i=1p(yi|xi, w)dw,
(cid:5)n
i=1 p(yi|xi, w)
Z(Y n|X n)

where φ(w) is the prior distribution. The posterior distribu-
tion is given by

Z(Y n|X n) =

φ(w)

φ(w)

(cid:4)

(5)

(6)

,

and the predictive distribution is deﬁned as the average of the
model over the posterior distribution as follows:

p(w|X n, Y n) =
(cid:4)

p(y|x, X n, Y n) =

p(y|x, w)p(w|X n, Y n)dw.

(7)

The generalization error, a criterion of generalization perfor-
mance, is deﬁned by

G(n) = (cid:6)G(X n, Y n)(cid:7)q(X n,Y n),

(8)

3A linear neural network model is not a toy but an useful model,
known as a reduced-rank regression model, in many applications
[Reinsel and Velu, 1998].

(cid:3)

(cid:2)
−(cid:4)w(cid:4)2
2τ 2
1

where

(cid:4)

G(X n, Y n) =

q(x)q(y|x) log

q(y|x)

p(y|x, X n, Y n)

dxdy (9)

is the Kullback-Leibler (KL) divergence of the predictive dis-
tribution from the true distribution, and (cid:6)·(cid:7)q(X n,Y n) denotes
the expectation value over all sets of n training samples.
3.2 Empirical Bayes Approach

and Subspace Bayes Approach

We often have little information about the prior distribution,
with which an EB approach was originally proposed to cope.
We can introduce hyperparameters in the prior distribution;
for example, when we use a prior distribution that depends
on a hyperparameter τ1 such as

φ(w) =

1
1 )K/2 exp
(2πτ 2

,

(10)

the marginal likelihood, Eq.(5), also depends on τ1. In an EB
approach, τ1 is estimated by maximizing the marginal likeli-
hood or by a slightly different way [Efron and Morris, 1973;
Akaike, 1980; Kass and Steffey, 1989]. Extending the idea
above, we can introduce hyperparameters also in a model dis-
tribution. What we call an SB approach is an EB approach
where a part of the parameters of a model are regarded as hy-
perparameters. In the following sections, we analyze two ver-
sions of SB approach: in the ﬁrst one, we regard the output
parameter matrix B of the map, Eq.(3), as a hyperparame-
ter and then marginalize the likelihood in the input parameter
space (MIP); and in the other one, we regard the input pa-
rameter matrix A, instead of B, as a hyperparameter and then
marginalize in the output parameter space (MOP).

4 Unidentiﬁability and Singularities
We say that a parametric model is unidentiﬁable if the map
from the parameter to the probability distribution is not one-
to-one. A neural network model, Eq.(1), is unidentiﬁable be-
cause the model is independent of ah when bh = 0, or vice
versa. The continuous points denoting the same distribution
are called the singularities, because the Fisher information
matrix on them degenerates. When the true model is not on
the singularities, asymptotically they do not affect prediction,
and therefore, the conventional learning theory of the regular
models holds. On the other hand, when the true model is on
the singularities, they signiﬁcantly affect generalization per-
formance as follows: in the ML estimation, the extent of the
set of the points denoting the true distribution increases its
neighborhoods and hence the ﬂexibility of imitating noises,
and therefore, accelerates overﬁtting; while in the Bayes es-
timation, the large entropy of the singularities increases the
weights of the distributions near the true one, and therefore,
suppresses overﬁtting. In LNNs, the former property appears
as acceleration of overﬁtting by selection of the largest singu-
lar value components of a random matrix, and in the SB ap-
proaches of LNNs, the latter property appears as James-Stein
type shrinkage, as shown in the following sections.

Suppression of overﬁtting accompanies insensitivity to the
true components with small amplitude. There is a trade-off,

which would, however, be ignored in asymptotic analysis if
we would consider only situations when the true model is dis-
tinctly on the singularities or not. Therefore, in this paper, we
also consider delicate situations when the KL divergence of
the true distribution from the singularities is comparable to
the inverse of the number of training samples, n−1, which
are important situations in model selection problems and in
statistical tests with ﬁnite number of samples for the follow-
ing reasons: ﬁrst, that there naturally exist a few true compo-
nents with amplitude comparable to n−1/2 when neither the
smallest nor the largest model is selected; and secondly, that
whether the selected model involves such components essen-
tially affects generalization performance.
5 Theoretical Analysis
5.1 Subspace Bayes Solution
By (cid:4) we, hereafter, distinguish the hyperparameter τ from
the parameter w, for example, p(y|x, w(cid:4)τ). Assume that the
variance of a noise is known and equal to unity. Then, the
conditional distribution of an LNN in the MIP version of SB
approach is given by
p(y|x, A(cid:4)B) =

(cid:3)

(11)

1

.

(2π)N/2 exp

We use the following prior distribution:

(cid:2)
−(cid:4)y − BAx(cid:4)2
(cid:2)
(cid:3)
− tr (AtA)

2

1

.

2

(cid:6)

φ(A) =

(2π)HM/2 exp

(12)
Note that we can similarly prepare p(y|x, B(cid:4)A) and φ(B)
for the MOP version. We assume that the true conditional
distribution is p(y|x, A∗(cid:4)B∗), where B∗A∗ is the true map
with rank H∗ ≤ H. We denote by ∗ the true value of a
parameter as above, and by a hat an estimator of a parameter,
for example, ˆA, ˆbh, etc.. For simplicity, we assume that the
xxtq(x)dx = IM .
input vector is orthonormalized so that
Consequently, the central limit theorem leads to the following
two equations:

i=1 xxt = IM + O(n−1/2),
i=1 yxt = B∗A∗ + O(n−1/2),

(13)
(14)
where Q(X n) is an M × M symmetric matrix and
R(X n, Y n) is an N × M matrix. Hereafter, we abbreviate
Q(X n) as Q, and R(X n, Y n) as R.
Let γh be the h-th largest singular value of the matrix
RQ−1/2, ωah the corresponding right singular vector, and
ωbh the corresponding left singular vector. We ﬁnd from
Eq.(14) that γh for H∗ < h ≤ H is of order O(n−1/2).
Hence, combining with Eq.(13), we get

Q(X n) = n−1(cid:1)n
R(X n, Y n) = n−1(cid:1)n

for H∗ < h ≤ H,

ωbh

RQρ = ωbh

(15)
where −∞ < ρ ∈ R < ∞ is an arbitrary constant. The
SB estimator, deﬁned as the expectation value over the SB
posterior distribution, is given by the following theorem:
Theorem 1 Let L = M in the MIP version or L = N in the
MOP version, and L(cid:2)
h). The SB estimator of
the map of an LNN is given by

h = max(L, nγ2

R + O(n−1)

(cid:1)H
h=1(1 − L(cid:2)−1

ˆB ˆA =

h L)ωbh

ωt
bh

RQ−1 + O(n−1).

(16)

(The proof is given in Appendix A.)

Because the independence between A and B makes the

posterior distribution localized, the following lemma holds.
Lemma 1 The predictive distribution in the SB approaches
can be written as follows:
(cid:9)
p(y|x, X n, Y n) =
−(y− ˆV ˆB ˆAx)t ˆV −1
· exp

(cid:10)
(y− ˆV ˆB ˆAx)

+O(n−3/2), (17)

(cid:8)−1/2

(2π)N| ˆV |

(cid:7)

2

(cid:11)

where ˆV = IN + O(n−1), and | · | denotes the determinant
of a matrix.
(Proof) The predictive distribution is written as follows:

p(y|x, X n, Y n) = (cid:6)p(y|x, A(cid:4)B)(cid:7)p(A|X n,Y n(cid:3)B)

(cid:7)
yt( ˆB ˆA − B∗A∗

(cid:8)(cid:12)

∝ q(y|x)

)x

exp

,
p(A|X n,Y n(cid:3)B)

(18)
where (cid:6)·(cid:7)p denotes the expectation value over a distribution p.
Since ( ˆB ˆA − B∗A∗) = O(n−1/2) in the SB approaches, we
can expand the predictive distribution as follows:
p(y|x, X n, Y n) ∝ q(y|x)
1 + yt( ˆB ˆA − B∗A∗
ytvvty

(cid:13)

(cid:11)

)x
+ O(n−3/2),
p(A|X n,Y n(cid:3)B)

(19)

+

2n

n( ˆB ˆA− B∗A∗)x is an N-dimensional vector of
where v =
order O(1). Calculating the expectation value and expanding
the logarithm of Eq.(19), we arrive at Lemma 1. (Q.E.D.)

Comparing Eq.(16) with the ML estimator
RQ−1

ˆB ˆAMLE =

h=1ωbh

ωt
bh

(cid:1)H

(20)

[Baldi and Hornik, 1995], we ﬁnd that the SB estimator of
each component is asymptotically equivalent to a positive-
part JS type shrinkage estimator. Moreover, by virtue of
Lemma 1, we can substitute the model at the SB estimator
for the predictive distribution with asymptotically insigniﬁ-
cant impact on generalization performance. Therefore, we
conclude that the SB approach is asymptotically equivalent to
the shrinkage estimation. Note that the variance of the prior
distribution, Eq.(12), asymptotically has no effect upon pre-
diction and hence upon generalization performance, as far as
it is a positive, ﬁnite constant. We call L the degree of shrink-
age. Remember that we can modify all the theorems in this
paper for the ML estimation only by letting L = 0.

5.2 Generalization Error
Using the singular value decomposition of the true map
B∗A∗, we can transform arbitrary A∗ and B∗ without change
of the map into a matrix with its orthogonal row vectors and
another matrix with its orthogonal column vectors, respec-
tively. Accordingly, we assume the above orthogonalities
without loss of generality. Then, Lemma 1 implies that the
KL divergence, Eq.(9), with a set of n training samples is

√

given by

G(X n, Y n) =

=

where

(cid:15)

(cid:14)
(cid:4)(B∗A∗ − ˆB ˆA)x(cid:4)2
(cid:1)H
h=1 Gh(X n, Y n) + O(n−3/2),
(cid:7)

q(x)

2

+ O(n−3/2)

(21)

(cid:8)

1
2

(b∗
ha∗t

h − ˆbhˆat

h)t(b∗

ha∗t

h − ˆbhˆat
h)

tr

Gh(X n, Y n)=
(22)
is the contribution of the h-th component. Here tr(·) denotes
the trace of a matrix. We denote by Wd(m, Σ, Λ) the d-
dimensional Wishart distribution with m degrees of freedom,
scale matrix Σ, and noncentrality matrix Λ, and abbreviate as
Wd(m, Σ) the central Wishart distribution.
Theorem 2 The generalization error of an LNN in the SB
approaches can be asymptotically expanded as

G(n) = λn−1 + O(n−3/2),

where the coefﬁcient of the leading term, called the general-
ization coefﬁcient in this paper, is given by
2λ = (H∗

(cid:14)

(M + N) − H∗2)
θ(γ(cid:2)2

H−H∗(cid:16)

(cid:2)
1 − L
γ(cid:2)2

(cid:15)

(cid:3)2
γ(cid:2)2

h

h

+

h=1

h > L)

.
(23)
h })
q({γ(cid:2)2
Here θ(·) is the indicator function of an event, γ(cid:2)2
h is
the h-th largest eigenvalue of a random matrix subject to
WN−H∗(M − H∗, IN−H∗), and (cid:6)·(cid:7)q({γ(cid:2)2
h }) denotes the ex-
pectation value over the distribution of the eigenvalues.
(Proof) According to Theorem 1, the difference between the
SB and the ML estimators of a true component with a posi-
tive singular value is of order O(n−1). Furthermore, the gen-
eralization error of the ML estimator of the component is the
same as that of the regular models because of its identiﬁabil-
ity. Hence, from Eq.(4), we obtain the ﬁrst term of Eq.(23)
as the contribution of the ﬁrst H∗ components. On the other
hand, we ﬁnd from Eq.(15) and Theorem 1 that for a redun-
dant component, identifying RQ−1/2 with R affects the SB
estimator only of order O(n−1), which, hence, does not affect
the generalization coefﬁcient. We say that U is the general
diagonalized matrix of an N × M matrix T if T is singular
value decomposed as T = ΩbU Ωa, where Ωa and Ωb are
an M × M and an N × N orthogonal matrices, respectively.
Let D be the general diagonalized matrix of R, and D(cid:2) the
(N − H∗) × (M − H∗) matrix created by removing the ﬁrst
H∗ columns and rows from D. Then, the ﬁrst H∗ diagonal
elements of D correspond to the positive true singular value
components and D(cid:2) consists only of noises. Therefore, D(cid:2) is
the general diagonalized matrix of n−1/2R(cid:2), where R(cid:2) is an
(N − H∗) × (M − H∗) random matrix whose elements are
independently subject to N1(0, 1), so that R(cid:2)R(cid:2)t is subject
to WN−H∗(M − H∗, IN−H∗). The redundant components
imitate n−1/2R(cid:2). Hence, using Theorem 1 and Eq.(22), we
obtain the second term of Eq.(23) as the contribution of the
last (H − H∗) components. Thus, we complete the proof of
Theorem 2. (Q.E.D.)

5.3 Large Scale Approximation
In a similar fashion to the analysis of the ML estimation in
[Fukumizu, 1999], the second term of Eq.(23) can be analyt-
ically calculated in the large scale limit when M, N, H, and
H∗ go to inﬁnity in the same order. We deﬁne the following
scalars: α = N(cid:2)/M(cid:2) = (N−H∗)/(M−H∗), β = H(cid:2)/N(cid:2) =
(H − H∗)/(N − H∗), and κ = L/M(cid:2) = L/(M − H∗).
Let W be a random matrix subject to WN(cid:2)(M(cid:2), IN(cid:2)), and
{u1, . . . , uN(cid:2)} the eigenvalues of M(cid:2)−1W . The measure of
the empirical distribution of the eigenvalues is deﬁned by

p(u)du = N(cid:2)−1 {δ(u1) + δ(u2) + ··· + δ(uN(cid:2))} ,

(24)
where δ(u) denotes the Dirac measure at u. In the large scale
limit, the measure, Eq.(24) , converges almost everywhere to

2παu

p(u)du =

θ(um < u < uM)du, (25)

(u − um)(uM − u)
√
√
α − 1)2 and uM = (
α + 1)2 [Watcher,
where um = (
1978]. Calculating moments of Eq.(25), we obtain the fol-
lowing theorem:
Theorem 3 The generalization coefﬁcient of an LNN in the
large scale limit is given by

(cid:17)

(cid:18)
2λ = (H∗

(M − H∗)(N − H∗)

(cid:19)
(M + N) − H∗2) +
2πα
J(st; 1) − 2κJ(st; 0) + κ2J(st;−1)

,

(26)

(cid:17)
(cid:17)
1 − s2 + cos
J(s; 1) = 2α(−s
−1 s),
√
J(s; 0) = −2
1 − s2 + (1 + α) cos
−1 s
α
√
√
− (1 − α) cos
α(1 + α)s + 2α
α(1 + α)

2αs +

−1

,

where


J(s;−1)
√
(cid:24)
2
α

=

2

√
√
2

1−s2
αs+1+α

− cos−1s+ 1+α

1−α cos−1

√

√
(0 < α < 1)

α(1+α)s+2α
α(1+α)
,

2αs+

1+s − cos−1 s
1−s

(cid:25)
(κ − (1 + α))/2

(cid:26)
(α = 1)
√
α, J−1(2παβ; 0)

. Here

and st = max
J−1(·; k) denotes the inverse function of J(s; k).
5.4 Delicate Situations
In ordinary asymptotic analysis, one considers only situations
when the amplitude of each component of the true model is
zero or distinctly-positive. Also Theorem 2 holds only in such
situations. However, as mentioned in the last paragraph of
Section 4, it is important to consider delicate situations when
the true map B∗A∗ has tiny but non-negligible singular values
h <∞. Theorem 1 still holds in such situa-
such that 0 <
tions by replacing the second term of Eq.(16) with o(n−1/2).
We regard H∗ as the number of distinctly-positive true singu-
lar values such that γ∗
n). Without loss of general-
ity, we assume that B∗A∗ is a non-negative, general diagonal
matrix with its diagonal elements arranged in non-increasing
order. Let R(cid:2)(cid:2)∗ be the true submatrix created by removing the

√
−1 = o(

√
nγ∗

h

K

 
/
 

a
d
b
m
a

l
 

2

2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0

SB(MIP)
SB(MOP)
ML
Bayes
Regular

0

2

4

6

8

10
H*

12

14

16

18

20

Figure 1: Generalization error.

ﬁrst H∗ columns and rows from B∗A∗. Then, D(cid:2), deﬁned in
the proof of Theorem 2, is the general diagonalized matrix of
n−1/2R(cid:2)(cid:2), where R(cid:2)(cid:2) is a random matrix such that R(cid:2)(cid:2)R(cid:2)(cid:2)t is
subject to WN−H∗(M − H∗, IN−H∗ , nR(cid:2)(cid:2)∗R(cid:2)(cid:2)∗). Therefore,
we obtain the following theorem:
Theorem 4 The generalization coefﬁcient of an LNN in the
general situations when the true map B∗A∗ may have delicate
singular values such that 0 <
(M +N) −H∗2) +
(cid:3)2
h −2
γ(cid:2)(cid:2)2

h < ∞ is given by
θ(γ(cid:2)(cid:2)2
nγ∗2
(cid:28)(cid:15)

√
nγ∗
H(cid:16)
(cid:3)
h=H∗+1
γ(cid:2)(cid:2)
hω(cid:2)(cid:2)t

(cid:14)
H−H∗(cid:16)

(cid:2)
1 − L
γ(cid:2)(cid:2)2

h +
√

nR(cid:2)∗ω(cid:2)(cid:2)

h

bh

, (27)
q(R(cid:2)(cid:2))
where γ(cid:2)(cid:2)
bh are the h-th largest singular value of
R(cid:2)(cid:2), the corresponding right singular vector, and the corre-
sponding left singular vector, respectively, of which (cid:6)·(cid:7)q(R(cid:2)(cid:2))
denotes the expectation value over the distribution.

ah, and ω(cid:2)(cid:2)

ah

2λ=(H∗
(cid:27)(cid:2)
1 − L
γ(cid:2)(cid:2)2
h, ω(cid:2)(cid:2)

h

h > L)

h=1

6 Discussions
6.1 Comparison with the ML Estimation

and with the Bayes Estimation

Figure 1 shows the generalization coefﬁcients of an LNN with
M = 50 input, N = 30 output, and H = 20 hidden units.
The horizontal axis indicates the true rank H∗. The verti-
cal axis indicates the coefﬁcients normalized by the param-
eter dimension K, given by Eq.(4). The lines correspond to
the generalization coefﬁcients of the SB approaches, clariﬁed
in this paper, that of the ML estimation, clariﬁed in [Fuku-
mizu, 1999], that of the Bayes estimation, clariﬁed in [Aoyagi
and Watanabe, 2004], and that of the regular models, respec-
tively.4 The results in Fig. 1 have been calculated in the large
scale approximation, i.e., by using Theorem 3. We have also
numerically calculated them by creating samples subject to
the Wishart distribution and then using Theorem 2, and thus
found that the both results almost coincide with each other so
that we can hardly distinguish. We see in Fig. 1 that the SB
approaches provide as good performance as the Bayes esti-
mation, and that the MIP, moreover, has no greater general-
ization coefﬁcient than the Bayes estimation for arbitrary H∗,
4In the regular models, the normalized generalization coefﬁcient
is always equal to one, which leads to the penalty term of Akaike’s
information criterion [Akaike, 1974].

K

 
/
 

a
d
b
m
a

l
 

2

K

 
/
 

a
d
b
m
a

l
 

2

2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0

2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0

SB(MIP)
SB(MOP)
ML
Regular

0

2

4

6

8

10

12

Figure 2: With delicate true components.

sqrt(n) gamma*

SB(MIP)
SB(MOP)
Bayes
ML(=Regular)

0

2

4

6

8

10

12

sqrt(n) gamma*

Figure 3: Single-output LNN.

which might seem to be inconsistent with the proved superior-
ity of the Bayes estimation to any other learning method when
we use the true prior distribution. This suspicion is cleared by
consideration of delicate situations in the following.

√
nγ∗, where γ∗

Using Theorem 4, we can numerically calculate the SB,
as well as the ML, generalization error in delicate situa-
tions when the true distribution is near the singularities. Fig-
ure 2 shows the coefﬁcients of an LNN with M = 50 input,
N = 30 output, and H = 5 hidden units on the assump-
tion that the true map consists of H∗ = 1 distinctly-positive
component, three delicate components whose singular val-
ues are identical to each other, and the other one null com-
h = γ∗
ponent. The horizontal axis indicates
for h = 2, . . . , 4. The Bayes generalization error in deli-
cate situations was previously clariﬁed [Watanabe and Amari,
2003], but unfortunately, only in single-output (SO) LNNs,
i.e., N = H = 1.5 Figure 3 shows the coefﬁcients of an
SOLNN with M = 5 input units on the assumption that
H∗ = 0 and the true singular value of the one component,
indicated by the horizontal axis, is delicate. We see in Fig. 3
that the SB approaches have a property similar to the Bayes
estimation, suppression of overﬁtting by the entropy of the
singularities. We also see that in some delicate situations, the
MIP is worse than the Bayes estimation, which shows consis-
tency with the superiority of the Bayes estimation. We con-
clude that in typical cases, the suppression by the singularities
in the MIP is comparable to, or sometimes stronger than, that

5An SOLNN is regarded as a regular model at a view point of the
ML estimation because the transform b1a1 (cid:1)→ w ∈ (cid:0)
M makes the
model linear and hence identiﬁable, and therefore, the ML general-
ization error is identical to that of the regular models. Nevertheless,
an SOLNN has a property of unidentiﬁable models at a view point
of the Bayesian learning methods, as shown in Fig. 3.

in the Bayes estimation.

It would be more fortunate if any of the SB approaches,
which require much less computational costs than MCMC
methods, would always provide comparable generalization
performance to the Bayes estimation. However, the SB ap-
proaches have also a property similar to the ML estimation,
acceleration of overﬁtting by selection of the largest singular
values of a random matrix. Because of selection from a large
number of random variables subject to non-compact sup-
port distribution, the (H − H∗) largest eigenvalues of a ran-
dom matrix subject to WN−H∗(M − H∗, IN−H∗) are much
greater than L when (M − H∗) > (N − H∗) (cid:12) (H − H∗).
h } in Theorem 2 go out of
Therefore, the eigenvalues {γ(cid:2)2
the effective range of shrinkage, and consequently, the SB
approaches approximate the ML estimation in such atypical
cases. Actually, the generalization coefﬁcient of an LNN in
the case that M = N = 80, H = 1, H∗ = 0 is 2λ/K ∼ 1.04,
which is greater than that of the regular models, though in
the Bayes estimation, the generalization coefﬁcient never ex-
ceeds that of the regular models [Watanabe, 2001].
6.2 Relation to Shrinkage Estimation
The relation between an EB approach in a linear model and
the JS estimator, into which the SB estimator, Eq.(16), in an
SOLNN is changed by letting L = (M − 2), was discussed
in [Efron and Morris, 1973]. Based on the EB approach, the
JS estimator can be derived as the solution of an equation
with respect to an unbiased estimator of the hyperparameter
τ−2
1 , introduced in Section 3.2. The similarity between the JS
and the SB estimators is natural because in an SOLNN, the
transform b1a1 (cid:2)→ w ∈ R
M makes not only the model linear
but also the prior distribution as Eq.(10).

√
nγ∗
(
1)

We focus on SOLNNs in this paragraph. In Fig. 3, the SB
approaches and the Bayes estimation seem to be superior to
the ML estimation regardless of the true distribution. The fol-
lowing asymptotic expansion of the generalization coefﬁcient
with respect to

√
nγ∗
√
2λ = M − ξ(
nγ∗
−2 + o
1)

1 provides a clue when it occurs:
,

(28)
where ξ is the coefﬁcient of the leading term when γ∗
1 in-
creases to be distinctly-positive. The sign of ξ indicates the
direction of approach to the line 2λ = M, which corre-
sponds to the generalization coefﬁcient of the regular mod-
els. It was found that ξ = (M − 1)(M − 3) in the Bayes
estimation, which leads to the conjecture that the Bayes esti-
mation would be superior to the ML estimation when M ≥ 4
[Watanabe and Amari, 2003]. Similarly expanding Eq.(27),
we get ξ = M(M − 4) in the MIP, as well as ξ = (2M − 5)
in the MOP, which leads to the conjecture that the MIP when
M ≥ 5, as well as the MOP when M ≥ 3, would be supe-
rior to the ML estimation. We have found that the numeri-
cal calculation by using Theorem 4 supports the conjecture
above [Nakajima and Watanabe, 2005a]. We also ﬁnd that
ξ = (M − 2)2 in the JS estimation, which is consistent with
its proved superiority to the ML estimation when M ≥ 3.
6.3 Relation to Variational Bayes Approach
The generalization error of the variational Bayes (VB) ap-
proach in LNNs has just been clariﬁed [Nakajima and Watan-
abe, 2005b]. In the parameter subspace corresponding to the

(cid:26)

(cid:25)

−2

redundant components, the VB posterior distribution extends
with its variance of order 1 in the larger dimension parameter
subspace either input one or the output one; while the SB pos-
terior distribution extends with its variance of order 1 in the
parameter space, not in the hyperparameter space, as we ﬁnd
from Eqs. (30) and (37) in Appendix A. Consequently, the
VB approach is asymptotically equivalent to the MIP version
of SB approach.

6.4 Future Works
A future work can be consideration of the effect of non-
linearity of the activation function, ψ(·) in Eq.(1). We expect
that the non-linearity would extend the range of basis selec-
tion and hence increase the generalization error.

7 Conclusions
We have introduced a subspace Bayes (SB) approach, an em-
pirical Bayes approach where a part of the parameters are re-
garded as hyperparameters, and derived the solution of two
versions of SB approach in three-layer linear neural networks.
We have also clariﬁed its generalization error and concluded
that the SB approaches have a property similar to the Bayes
estimation and provide as good performance as the Bayes es-
timation in typical cases.

Acknowledgments
The authors would like to thank the reviewers who gave us
meaningful advice, which motivates us to add Section 4 and
some comments in other sections. They also would like to
thank Kazuo Ushida, Masahiro Nei, and Nobutaka Magome
of Nikon Corporation for encouragement to research.

A Proof of Theorem 1
First, we will prove in the MIP version. Given an arbitrary
map BA, we can have A with its orthogonal row vectors and
B with its orthogonal column vectors by using the singular
value decomposition. Just in that case, the prior probabil-
ity, Eq.(12), is maximized. Accordingly, we assume without
loss of generality that the optimum value of B consists of
its orthogonal column vectors. Consequently, the marginal
(conditional) likelihood, as well as the posterior distribution,
factorizes as

Z(Y n|X n(cid:4)B) =
p(A|X n, Y n(cid:4)B) =

(cid:5)H
(cid:5)H
h=1 Z(Y n|X n(cid:4)bh),
h=1 p(ah|X n, Y n(cid:4)bh).

(cid:2)

Then, substituting Eqs.(11) and (12) into Eq.(5), as well as
Eq.(6), we can easily derive the marginal likelihood, as well
as the posterior distribution, of each component as follows:
Z(Y n|X n(cid:4)bh)∝|Sh|−1/2 exp
p(ah|X n, Y n(cid:4)bh)
(cid:26)t nSh
∝ exp

hRS−1
h Rtbh
2
(cid:25)
ah−S−1

(cid:2)
−(cid:25)

ah−S−1

(cid:26)(cid:3)

, (30)

h Rtbh

h Rtbh

(cid:3)

nbt

,

(29)

2

where Sh = ((cid:4)bh(cid:4)2Q + n−1IM ). Let F (cid:2)(Y n|X n(cid:4)bh) =
F (Y n|X n(cid:4)bh)+const., where F (Y n|X n(cid:4)bh) is the stochas-
tic complexity, i.e., the negative log marginal likelihood, of
the h-th component. Then, we get

(Y n|X n(cid:4)bh) = −2 log Z(Y n|X n(cid:4)bh) + const.

2F (cid:2)

= log |Sh| − nbt

hRS−1

h Rtbh

(31)
Hereafter, separately considering the components imitating
the positive true ones and the redundant components, we will
minimize Eq.(31). We abbreviate F (cid:2)(Y n|X n(cid:4)bh) as F (cid:2)(bh).
For a positive true component, h ≤ H∗, the correspond-
ing observed singular value γh of RQ−1/2 is of order 1 with
probability 1. Then, from Eq.(31), we get
2F (cid:2)

(bh) = M log (cid:4)bh(cid:4)2− n(cid:4)bh(cid:4)−2bt

hRQ−1Rtbh

hRQ−2Rtbh+ O(n−1).

(32)
To minimize Eq.(32), the leading, second term dominates the
determination of the direction cosine of bh and leads to bh =
(cid:4)bh(cid:4)(ωbh + O(n−1)). The ﬁrst and the third terms determine
the norm of bh because the second term is independent of it.
Thus, we get the optimal hyperparameter value as follows:

+ (cid:4)bh(cid:4)−4bt

(cid:29)

ˆbh =

RQ−2Rtωbh

ωt
bh

M

ωbh + O(n−1).

(33)

ˆbhˆat

Because the average of ah over the posterior distribution,
Eq.(30), is ˆah = S−1
h Rtbh, we obtain the SB estimator for
the positive true component of the map BA as follows:

RQ−1 + O(n−1).

h = ˆωbh ˆωt
bh

(34)
On the other hand, for a redundant component, h > H∗,

Eq.(15) allows us to approximate Eq.(31) as follows:
2F (cid:2)

(cid:26)− nbt
hRRtbh
(cid:4)bh(cid:4)2+n−1 +O(n−1/2).

(cid:25)(cid:4)bh(cid:4)2+n−1

(bh) = M log

0 = 2

((cid:4)bh(cid:4)2+n−1)2

∂F (cid:2)(bh)
∂(cid:4)bh(cid:4)2 =

(35)
Then, we ﬁnd that the direction cosine of bh, determined by
the second term of Eq.(35), is approximated by ωbh with ac-
h(cid:4)bh(cid:4)2(1+ O(n−1/2))
curacy O(n−1/2). After substituting γ2
(cid:2)
(cid:3)
for bt
hRRtbh, we get the following extreme condition by par-
tial differentiation of Eq.(35) with respect to the norm of bh:
h − M
(cid:4)bh(cid:4)2− nγ2
nM
(cid:17)
+O((cid:4)bh(cid:4)−2n−1/2).
(36)
We ﬁnd that Eq.(36) has no solution if γh is less than
M/n.
Therefore, using the fact that Eq.(35) diverges to inﬁnity with
arbitrary n if (cid:4)bh(cid:4) → ∞, we get the optimum hyperparame-
ter value as follows:
ˆbh =

(37)
Thus, we obtain the SB estimator of the redundant component
as follows:
ˆbhˆat

(38)
Selecting the largest singular value components minimizes
Eq.(31). Hence, combining Eqs.(34) and (38), and then using
Eq.(15), we obtain the SB estimator in Theorem 1. We can
also derive the SB estimator in the MOP version in exactly
the same way. (Q.E.D.)

h = (1 − (L(cid:2)
−1M)ωbh
h)

ωbh + O(n−1).

R + O(n−1).

h − M
L(cid:2)

(cid:30)

ωt
bh

nM

M

References
[Akaike, 1974] H. Akaike. A new look at statistical model. IEEE

Trans. on Automatic Control, 19:716–723, 1974.

[Akaike, 1980] H. Akaike. Likelihood and bayes procedure. In J.
M. Bernald, Bayesian statistics, pages 143–166, Valencia, Italy,
1980. University Press.

[Aoyagi and Watanabe, 2004] M. Aoyagi and S. Watanabe. The
generalization error of reduced rank regression in bayesian esti-
mation. In Proc. of ISITA, pages 1068–1073, Parma, Italy, 2004.
[Attias, 1999] H. Attias. Inferring parameters and structure of latent

variable models by variational bayes. In Proc. of UAI, 1999.

[Baldi and Hornik, 1995] P. F. Baldi and K. Hornik. Learning in lin-
ear neural networks: a survey. IEEE Trans. on Neural Networks,
6:837–858, 1995.

[Efron and Morris, 1973] B. Efron and C. Morris. Stein’s estima-
tion rule and its competitors—an empirical bayes approach. J. of
Am. Stat. Assoc., 68:117–130, 1973.

[Fukumizu, 1999] K. Fukumizu. Generalization error of linear neu-
ral networks in unidentiﬁable cases. In Proc. of ALT, pages 51–
62. Springer, 1999.

[Ghahramani and Beal, 2000] Z. Ghahramani and M. J. Beal.
Graphical models and variational methods. In Advanced Mean
Field Methods. MIT Press, 2000.

[Hinton and van Camp, 1993] G. E. Hinton and D. van Camp.
Keeping neural networks simple by minimizing the description
length of the weights. In Proc. of COLT, 1993.

[James and Stein, 1961] W. James and C. Stein. Estimation with
In Proc. of the 4th Berkeley Symp. on Math.
quadratic loss.
Stat. and Prob., pages 361–379, Berkeley:University of Califor-
nia, 1961.

[Kass and Steffey, 1989] R. E. Kass and D. Steffey. Approxi-
mate bayesian inference in conditionally independent hierarchi-
cal models (parametric empirical bayes models). J. of the Am.
Stat. Assoc., 84:717–726, 1989.

[MacKay, 1995] D. J. C. MacKay. Developments in probabilistic
modeling with neural networks—ensemble learning. In Proc. of
the 3rd Ann. Symp. on Neural Networks, pages 191–198, 1995.

[Nakajima and Watanabe, 2005a] S. Nakajima and S. Watanabe.
Generalization performance of subspace bayes approach in lin-
ear neural networks. Submitted to IEICE Trans., 2005.

[Nakajima and Watanabe, 2005b] S. Nakajima and S. Watanabe.
Variational bayes solution of linear neural networks and its gen-
eralization error and training error. Submitted to ICML, 2005.

[Reinsel and Velu, 1998] G. C. Reinsel and R. P. Velu. Multivariate

Reduced-Rank Regression. Springer, 1998.

[Watanabe and Amari, 2003] S. Watanabe and S. Amari. Learning
coefﬁcients of layered models when the true distribution mis-
matches the singularities. Neural Computation, 15:1013–1033,
2003.

[Watanabe, 2001] S. Watanabe. Algebraic information geometry
for learning machines with singularities. Advances in NIPS,
13:329–336, 2001.

[Watcher, 1978] K. W. Watcher. The strong limits of random matrix
spectra for sample matrices of independent elements. Ann. Prob.,
6:1–18, 1978.

[Yamazaki and Watanabe, 2003] K. Yamazaki and S. Watanabe.
Singularities in mixture models and upper bounds of stochastic
complexity. Neural Networks, 16(7):1029–1038, 2003.


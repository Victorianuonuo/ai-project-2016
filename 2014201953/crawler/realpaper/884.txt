Neighborhood MinMax Projections

∗

Feiping Nie1, Shiming Xiang2, and Changshui Zhang2

State Key Laboratory of Intelligent Technology and Systems,

Department of Automation, Tsinghua University, Beijing 100080, China

1nfp03@mails.tsinghua.edu.cn; 2{xsm, zcs}@mail.tsinghua.edu.cn

Abstract

A new algorithm, Neighborhood MinMax Projec-
tions (NMMP), is proposed for supervised dimen-
sionality reduction in this paper. The algorithm
aims at learning a linear transformation, and fo-
cuses only on the pairwise points where the two
points are neighbors of each other. After the trans-
formation, the considered pairwise points within
the same class are as close as possible, while those
between different classes are as far as possible. We
formulate this problem as a constrained optimiza-
tion problem, in which the global optimum can be
effectively and efﬁciently obtained. Compared with
the popular supervised method, Linear Discrimi-
nant Analysis (LDA), our method has three signif-
icant advantages. First, it is able to extract more
discriminative features. Second, it can deal with
the case where the class distributions are more com-
plex than Gaussian. Third, the singularity problem
existing in LDA does not occur naturally. The per-
formance on several data sets demonstrates the ef-
fectiveness of the proposed method.

1 Introduction

Linear dimensionality reduction is an important method when
facing with high-dimensional data. Many algorithms have
been proposed during the past years. Among these algo-
rithms, Principal Component Analysis (PCA) [Jolliffe, 2002]
and Linear Discriminant Analysis (LDA) [Fukunaga, 1990]
are two of the most widely used methods. PCA is an unsu-
pervised method, which does not take the class information
into account. LDA is one of the most popular supervised di-
mensionality reduction techniques for classiﬁcation. How-
ever, there exist several drawbacks in it. One drawback is that
it often suffers from the Small Sample Size problem when
dealing with high dimensional data. In this case, the within-
class scatter matrix Sw may become singular, which makes
LDA difﬁcult to be performed. Many approaches have been
proposed to address this problem [Belhumeur et al., 1997;

∗This paper is Funded by Basic Research Foundation of Ts-
inghua National Laboratory for Information Science and Technol-
ogy (TNList).

Chen et al., 2000; Yu and Yang, 2001]. However, these vari-
ants of LDA discard a subspace and thus some important dis-
criminative information may be lost. Another drawback in
LDA is its distribution assumption. LDA is optimal in the
case that the data distribution of each class is Gaussian, which
can not always be satisﬁed in real world applications. When
the class distribution is more complex than Gaussian, LDA
may fail to ﬁnd the optimal discriminative directions. More-
over, the number of available projection directions in LDA is
smaller than the class number [Duda. et al., 2000], but it may
be insufﬁcient for many complex problems, especially when
the number of class is small.

For the distance metric based classiﬁcation methods, such
as the nearest neighbor classiﬁer, learning an appropriate
distance metric plays a vital role. Recently, a number of
methods have been proposed to learn a Mahalanobis dis-
tance metric [Xing et al., 2003; Goldberger et al., 2005;
Weinberger et al., 2006]. Linear dimensionality reduction
can be viewed as a special case of learning a Mahalanobis dis-
tance metric(see section 5). This viewpoint can give a reason-
able interpretation for the fact that the performance of nearest
neighbor classiﬁer can always be improved after performing
linear dimensionality reduction.

In this paper, we propose a new supervised linear di-
mensionality reduction method, Neighborhood MinMax Pro-
jections (NMMP). The method is largely inspired by the
classical supervised linear dimensionality reduction method,
i.e., LDA, and the recent proposed distance metric learning
method, large margin nearest neighbor (LMNN) classiﬁca-
tion [Weinberger et al., 2006].
In our method, we focus
only on the pairwise points where the two points are neigh-
bors of each other. After the transformation, we try to pull
the considered pairwise points within the same class as close
as possible, and take those between different classes apart.
This goal can be achieved by formulating the task as a con-
strained optimization problem, in which the global optimum
can be effectively and efﬁciently obtained. Compared with
LDA, our method avoids the three drawbacks in LDA dis-
cussed in above. Compared with the LMNN method, our
method is computationally much more efﬁcient. The perfor-
mance on several data sets demonstrates the effectiveness of
our method.

IJCAI-07

993

Nw(i)

Nw(i)

Nb(i)

Nb(j)

A

B

A

B

where

˜Sb =

(cid:2)

i,j:xi∈Nb(Cj)&xj ∈Nb(Ci)

(xi − xj)(xi − xj)T

(5)

(a)

(b)

Figure 1: In the left ﬁgure, point A and B belong to the same
class i, and the two circles denote the within-class neighbor-
hood of A and B respectively. A is B’s within-class neighbor-
hood and B is A’s within-class neighborhood. After the trans-
formation, we try to pull the two points as close as possible;
In the right ﬁgure, point A belongs to class i, point B be-
longs to class j, and the two circles denote the between-class
neighborhood of A and B respectively. A is B’s between-class
neighborhood and B is A’s between-class neighborhood. Af-
ter the transformation, we try to push the two points as far as
possible.

2 Problem Formulation
Given the data matrix X = [x1, x2, ..., xn], xi ∈ Rd
, our
goal is to learn a linear transformationW : Rd → Rm
, where
and WTW = I. I is m × m identity matrix.
W ∈ Rd×m
Then the original high-dimensional data x is transformed into
a low-dimensional vector:

y = W

T x

(1)
Let each data point of class i have two kinds of neighbor-
hood: within-class neighborhood Nw(i) and between-class
neighborhood Nb(i), where Nw(i) is the set of the data’s
kw(i) nearest neighbors in the same class i and Nb(i) is the
set of the data’s kb(i) nearest neighbors in the class other than
i. Obviously, 1 ≤ kw(i) ≤ ni − 1, and 1 ≤ kb(i) ≤ n − ni,
where ni is the data number of class i.

Here, we focus only on the pairwise points where the two
points are neighbors of each other. After the transformation,
we hope that the distance of the considered pairwise points
within the same class will be minimized, while the distance
of those between different classes will be maximized. (see
Figure 1).

After the transformation W, the sum of the Euclidean dis-
tances of the pairwise points within the same class can be
formulated as:

where tr(·) denotes the trace operator of matrix, and

T ˜SwW)

sw = tr(W
(cid:2)

(2)

˜Sw =

i,j:xi∈Nw(Cj )&xj ∈Nw (Ci)

(xi − xj)(xi − xj)T

(3)

Here, Ci denote the class label of xi, and Cj denote the
class label of xj. Obviously, Ci = Cj, and ˜Sw is positive
semi-deﬁnite.

Similarly, the sum of the Euclidean distances of the pair-

wise points between different classes is:

Here, Ci (cid:6)= Cj, and ˜Sb is positive semi-deﬁnite too.
To achieve our goal, we should maximize sb while mini-
mize sw. The following two function can be used as objec-
tive:

(cid:3)

(cid:4)

M1(W) = tr

W

T (˜Sb − λ˜Sw)W

M2(W) =

tr(WT ˜SbW)
tr(WT ˜SwW)

(6)

(7)

As it is difﬁcult to determine a suitable weight λ for the for-
mer objective function, we select the latter as our objective to
optimization. In fact, as we will see, the latter is just a spe-
cial case of the former, where the weight λ is automatically
determined.

Therefore, we formulate the problem as a constrained op-

timization problem:

W

∗ = arg max

WT W=I

tr(WT ˜SbW)
tr(WT ˜SwW)

(8)

Fortunately, the globally optimal solution of this problem can
be efﬁciently calculated. In the next section, we will describe
the details for solving this constrained optimization problem.

3 The Constrained Optimization Problem

We address the above optimization problem in a more general
form which is described as follows:

The constrained optimization problem: Given the real sym-
and the positive semi-deﬁnite ma-
, rank(B) = r ≤ d. Find a matrix
that maximize the following objective function

metric matrix A ∈ Rd×d
trix B ∈ Rd×d
W ∈ Rd×m
with the constraint of WT W = I:

W

∗ = arg max

WT W=I

tr(WT AW)
tr(WT BW)

(9)

At ﬁrst, we propose Lemma 1, which shows that when
WT W = I and m > d − r, the value of tr(WT BW) will
not be equal to zero.

Lemma1. Suppose W ∈ Rd×m

is
a positive semi-deﬁnite matrix, and rank(B) = r ≤ d, m >
d − r, then it holds that tr(WT BW) > 0.

, WT W = I, B ∈ Rd×d

proof. According to the result of Rayleigh quotient [Golub
i=1 βi,
and van Loan, 1996], min
where β1, β2, . . . , βm are the ﬁrst m smallest eigenvalues
of B. As B is positive semi-deﬁnite, rank(B) = r, and
m > d − r, then
i=1 βi > 0. Therefore, with the constraint
of WT W = I, tr(WT BW) ≥ min tr(WT BW) > 0.

tr(WT BW) =

(cid:5)m

WT W=I

(cid:5)m

Thus we discuss this optimization problem in two cases.
Case 1: m > d − r,
Lemma 1 ensures that the optimal value is ﬁnite in this case.
Suppose the optimal value is λ∗, Guo [2003] has derived that

tr(WT (A − λ∗B)W) = 0.

sb = tr(W

T ˜SbW)

(4)

max

WT W=I

IJCAI-07

994

Note that tr(WT BW) > 0, so we can easy to see,
tr(WT (A − λB)W) < 0 ⇒ λ > λ∗, and
max
tr(WT (A − λB)W) > 0 ⇒ λ < λ∗.

max

WT W=I

WT W=I

WT W=I

On the other hand, max

tr(WT (A − λB)W) = γ,
where γ is the sum of the ﬁrst m largest eigenvalues of A −
λB. Given a value λ, if γ = 0,then λ is just the optimal value,
otherwise γ > 0 implies λ is smaller than the optimal value
and vice versa. Thus the global optimal value of the problem
can be obtained by an iterative algorithm. Subsequently, in
order to give a suitable value λ, we need to determine the
possible bound of the optimal value. Theorem 1 is proposed
to solve this problem.

Theorem 1. Given the real symmetric matrix A ∈ Rd×d
and the positive semi-deﬁnite matrix B ∈ Rd×d
, rank(B) =
r ≤ d. If W1 ∈ Rd×m1 , W2 ∈ Rd×m2 and m1 > m2 >
T
2 AW2)
d − r, then max
.
2 BW2)

T
1 AW1)
1 BW1)

≤ max

tr(W
tr(WT

tr(W
tr(WT

WT

2 W2=I

WT

1 W1=I

The proof of Theorem 1 is based on the following lemma:
≤ · · · ≤ ak
Lemma 2. If ∀i, ai ≥ 0, bi > 0 and
bk

≤ a2
b2

a1
b1

,

then

a1+a2+···+ak
b1+b2+···+bk

≤ ak
bk .

Proof. Let
qbi. Therefore

ak
bk = q. So ∀i, ai ≥ 0, bi > 0, we have ai ≤
a1+a2+···+ak
b1+b2+···+bk

≤ ak
bk

Now we give the proof of Theorem 1 in the following.
Proof of theorem 1. Suppose W1

T

∗ = [w1, w2, ..., wm1 ]
m1 = h,

1 AW1)
1 BW1) . Let Cm2

tr(W
tr(WT

and W∗

1 = arg max

WT

1 W1=I

without loss of generality, we suppose

T

tr(W
tr(WT

p(1)AWp(1))
BWp(1))

p(1)

≤

T

tr(W
tr(WT

p(2)AWp(2))
BWp(2))

p(2)

≤ · · · ≤ tr(W
tr(WT

T

p(h)AWp(h))
BWp(h))

p(h)

Let Cm2−1

where Wp(i) ∈ Rd×m2

is the i-th combination of
w1, w2, ..., wm1 with m2 elements(note that m1 > m2), so
the number of combinations is h.

m1−1 = l, note that each of wj(1 ≤ j ≤ m1) occurs
l times in {Wp(1), Wp(2), ..., Wp(h)}. According to Lemma
2, we have
max
1 W1=I
p(1)AWp(1))+tr(W
BWp(1))+tr(WT
T
p(h)AWp(h))
BWp(h))

p(2)AWp(2))+···+tr(W
BWp(2))+···+tr(WT
2 AW2)
2 BW2)

tr(W
tr(WT
≤ tr(W
tr(WT

∗)
l·tr(W1
l·tr(W1∗ T BW1 ∗)

p(h)AWp(h))
BWp(h))

T
1 AW1)
1 BW1)

≤ max

tr(W
tr(WT

tr(W
tr(WT

WT
T

AW1

=

=

p(h)

p(1)

p(2)

∗ T

T

T

T

p(h)

WT

2 W2=I

According to Theorem 1 we know, with the reduced
dimension m increases,
the optimal value is decreased
monotonously. When m = d, the optimal value is equal to
tr(A)
tr(B) . So max

tr(W
AW)
tr(WT BW)

≥ tr(A)
tr(B) .

T

WT W=I

min

WT W=I

On the other hand, max

tr(WT AW) =
i=1 αi, and
i=1 βi, where α1, α2, . . . , αm
WT W=I
are the ﬁrst m largest eigenvalues of A, and β1, β2, . . . , βm
are the ﬁrst m smallest eigenvalues of B.
Therefore,

tr(WT BW) =

(cid:5)m

(cid:5)m

max

WT W=I

T

AW)

tr(W

tr(WT BW) ≤ α1+α2+···+αm
β1+β2+···+βm .

As a result, the bound of the optimal value is given by

tr(A)
tr(B) ≤ max

WT W=I

T

tr(W
AW)
tr(WT BW)

≤ α1+α2+···+αm
β1+β2+···+βm

Now, we obtain an iterative algorithm for obtaining the op-
timal solution, which is described in Table 1. From the al-
gorithm we can see, only a few iterative steps are needed to
obtain a precise solution. Note that the algorithm need not
calculate the inverse of B, and thus the singularity problem
does not exist in it naturally.

Case 2: m ≤ d − r,
In this case, when W lies in the null space of ma-
trix B, then tr(WT BW) = 0, the value of the objec-
tive function becomes inﬁnite.
Therefore, we can rea-
sonably replace the optimization problem with V∗ =
arg max
, and
Z = [z1, z2, ..., zd−r] are the eigenvectors corresponding to
d − r zero eigenvalues of B.

tr(VT (ZT AZ)V), where V ∈ R

We know that V∗

[μ1, μ2, ..., μm], where
μ1, μ2, ..., μm are the ﬁrst m largest eigenvectors of
ZT AZ. So, in this case, the ﬁnal solution is W∗ = Z · V∗

(d−r)×m

VT V=I

=

Input:

The real symmetric matrix A ∈ Rd×d

and the positive
, rank(B) = r ≤ d. The

semi-deﬁnite matrix B ∈ Rd×d
error constant .
Output:

Projection matrix W∗, where W∗ ∈ Rd×m

W∗ = I.

W∗T
In the case of : m > d − r.

and

1.λ1 ← tr(A)

tr(B) , λ2 ← α1+α2+···+αm

β1+β2+···+βm , λ ← λ1+λ2
,
where α1, α2, ..., αm are the ﬁrst m largest eigenvalues
of A, β1, β2, ..., βm are the ﬁrst m smallest eigenvalues
of B.

2

2.While λ2 − λ1 > , do

a) Calculate γ, where γ is the sum of the ﬁrst m

largest eigenvalues of A − λB.

b) If γ > 0, then λ1 ← λ, else λ2 ← λ.
c) λ ← λ1+λ2
End while.

2

.

W∗ = [ν 1, ν 2, ..., νm], where ν 1, ν 2, ..., νm are the

ﬁrst m largest eigenvectors of A − λB.
In the case of : m ≤ d − r.

W∗ = Z · [μ1, μ2, ..., μm], where μ1, μ2, ..., μm are
the ﬁrst m largest eigenvectors of ZT AZ, and Z =
[z1, z2, ..., zd−r] are the eigenvectors corresponding to
d − r zero eigenvalues of B.

Table 1: The algorithm for the optimization problem

4 Neighborhood MinMax Projections

The method of Neighborhood MinMax Projections(NMMP)
is described in Table 2 . In order to speed up, PCA can be
used as a preprocessing step before performing NMMP.

Denote the covariance matrix of data by St, and denote the
null space of St by φ, the orthogonal complement of φ by φ⊥.

IJCAI-07

995

0. Preprocessing: eliminate the null space of the co-
variance matrix of data, and obtain new data X =
[x1, x2, ..., xn] ∈ Rd×n
1. Input:

, where rank(X) = d

X = [x1, x2, ..., xn] ∈ R

d×n

, kw(i), kb(i), m

2. calculate ˜Sw and ˜Sb according to Eq.(3) and Eq.(5)
3. calculate W using the algorithm described in Table 1

4. Output:

y = WT x, where W ∈ Rd×m

and WT W = I.

1

0.5

0

−0.5

−1

1.5

1

0.5

0

−0.5

−1

−1.5

−1.5

−1

−0.5

0

0.5

1

1.5

−2

−2

−1.5

−1

−0.5

0

0.5

1

1.5

2

(a) Original data

(b) PCA

1.5

1

0.5

0

−0.5

−1

−1.5

1

0.5

0

−0.5

−1

Table 2: Algorithm of NMMP

−2

−1.5

−1

−0.5

0

0.5

1

1.5

2

−1.5

−1

−0.5

0

0.5

1

1.5

(c) LDA

(d) NMMP

It is well known that the null space of St can be eliminated
without lose of any information. In fact, it can be easy to
prove that the null space of St comprises the null space of
˜Sw and the null space of ˜Sb deﬁned in Section 2. Suppose
w ∈ φ⊥ and ξ ∈ φ, then

(w + ξ)T ˜Sb(w + ξ)
(w + ξ)T ˜Sw(w + ξ)

=

wT ˜Sbw
wT ˜Sww

(10)

Eq.(10) demonstrates that eliminating the null space of the
covariance matrix of data will not affect the result of the pro-
posed method. Thus we use PCA to eliminate the null space
of the covariance matrix of data.

5 Discussion
Our method is closely connected with LDA. Both of them are
supervised dimensionality reduction methods, and the goals
are also similar. They both try to maximize the scatter be-
tween different classes, and minimize the scatter within the
same class. The matrix ˜Sb deﬁned in Eq.(5) and ˜Sw deﬁned
in Eq.(3) are parallel to the between-class scatter matrix Sb
and within-class scatter matrix Sw in LDA respectively. In
fact, when the number of neighbors reaches the number of the
total available neighbors(kw(i) = ni − 1, and kb(i) = n − ni,
where ni is the data number of class i, and n is the number
of total data), we have ˜Sb + ˜Sw = n2
St, which is similar to
Sb + Sw = St in LDA.

However, in comparison with LDA, we do not impose the
faraway pairwise points within the same class to be close to
each other, which makes us focus more on the improvement
of the discriminability of local structure. This property is
especially useful when the distribution of class data is more
complex than Gaussian. We give a toy example to illustrate
it(Figure 2).

The toy data set consists of three classes(shown by dif-
ferent shapes). In the ﬁrst two dimensions, the classes are
distributed in concentric circles, while the other eight dimen-
sions are all Gaussian noise with large variance. Figure 2
shows the two-dimensional subspace learned by PCA, LDA
and NMMP, respectively. It illustrates that NMMP can ﬁnd
a low-dimensional transformation preserving manifold struc-
ture with more discriminability.

Moreover, compared with LDA, our method is able to ex-
tract more discriminative features and the singularity problem
existing in LDA will not occur naturally.

Figure 2: (a) is the ﬁrst two dimensions of the original ten-
dimensional data set; (b),(c),(d) are the two-dimensional sub-
space found by PCA, LDA and NMMP, respectively. It illus-
trates that NMMP can ﬁnd a low-dimensional transformation
preserving manifold structure with more discriminability.

T

Distance metric learning is an important problem for
the distance based classiﬁcation method. Learning a Ma-
halanobis distance metric is to learn a positive semideﬁ-
nite matrix M, and using the Mahalanobis distance met-
ric (xi − xj)T M(xi − xj) to replace the Euclidean
distance metric (xi − xj)T (xi − xj), where M ∈
Rd×d, xi, xj ∈ Rd
. Note that M is positive semideﬁnite,
with the eigen-decomposition, M = VV
, where V =
[σ1v1, σ2v2, ..., σdvd], σ and v are eigenvalues and eigen-
vectors of M. Therefore, the Mahalanobis distance metric
can be formulated as (VT xi − VT xj)T (VT xi − VT xj). In
this form, we can see that Learning a Mahalanobis distance
metric is to learn a weighted orthogonal linear transformation.
NMMP learns a linear transformation W with the constraint
of WT W = I. So it can be viewed as a special case of learn-
ing a Mahalanobis distance metric, where the weight value
σi is either 0 or 1. Note that directly learning the matrix M
is a very difﬁcult problem and it is usually formulated as a
semideﬁnite programming (SDP) problem, where the com-
putation burden is extremely heavy. However, if we learn the
transformation V instead of learning the matrix M, the prob-
lem will become much easier to solve.

6 Experimental Results
We evaluated the proposed NMMP algorithm on several data
sets, and compared it with LDA and LMNN method. The data
sets we used belong to different ﬁelds, a brief description of
these data sets is list on Table 3.

We use PCA as the preprocessing step to eliminate the null
space of data covariance matrix St. For LDA, due to the sin-
gularity problem existing in it, we further reduce the dimen-
sion of data such that the within-class scatter matrix Sw is
nonsingular.

In each experiment, we randomly select several samples
per class for training and the remaining samples for testing.
the average results and standard deviations are reported over
50 random splits. The classiﬁcation is based on k-nearest
neighbor classiﬁer(k = 3 in these experiments).

IJCAI-07

996

class
training number
testing number
input dimensionality
dimensionality after PCA

Iris Bal
3
3
60
60
90
565
4
4

4
4

Faces Objects USPS News

40
200
200

10304

199

20
120
1320
256
119

4
80

3794
256
79

4

120
3850
8014
119

data set method
Iris
baseline

Bal

Faces

Objects

USPS

News

LDA

LMNN
NMMP
baseline

LDA

LMNN
NMMP
baseline

LDA

LMNN
NMMP
baseline

LDA

LMNN
NMMP
baseline

LDA

LMNN
NMMP
baseline

LDA

LMNN
NMMP

Table 3: A brief description of the data sets.

Projection number Accuracy(%)

Std. Dev.(%) Training time(per run)

4
2
4
3
4
2
4
2

199
39
199
60
119
19
119
60
79
3
79
60
119

3

119
60

95.4
96.6
96.2
96.5
61.6
74.9
70.1
72.9
86.9
92.2
95.9
96.6
76.8
78.2
84.1
86.5
93.2
84.2
86.2
94.5
30.9
46.9
62.1
58.5

1.8
1.6
1.5
1.6
2.8
3.2
3.7
4.2
2.1
1.8
1.6
1.6
1.8
2.0
1.8
1.6
1.1
2.6
2.2
0.9
2.8
5.7
7.3
5.0

–
0s

4.91s
0.02s

–
0s

3.37s
0.02s

–

0.15s

399.03s

2.84s

–

0.04s

221.29s

0.66s

–

0.01s
70.83s
0.12s

–

0.05s
73.38s
0.40s

Table 4: Experimental results in each data set.

It is worth noting that the parameters in our method are not
sensitive. In fact, in each experiment, we simply set kb(i) to
10, and set kw(i) to ni/2 + 2 for each class i, where ni is the
training number of class i.

The experimental results are reported in Table 4. We use
the recognition result directly performed after the preprocess-
ing by PCA as the baseline.

In the following we describe the details of each experiment.
The UCI data sets
In this experiment, we perform on two small data sets, Iris
and Balance, taken from the UCI Machine Learning Reposi-
tory1. As the class distributions of this two data sets are not
very complex, LDA works well, and our method also demon-
strates the competitive performance.

Face recognition
The AT&T face database (formerly the ORL database) in-
cludes 40 distinct individuals and each individual has 10 dif-
ferent images. Some images were taken at different times,

1Available at http://www.ics.uci.edu/ mlearn/MLRepository.html

and have variations [Samaria and Harter, 1994] including ex-
pression and facial details. Each image in the database is of
size 112 × 92 and with 256 gray-levels.

In this experiment, no other preprocessings are performed
except the PCA preprocessing step. The result of our method
is much better than those of LDA and the baseline. LMNN
have a good performance too, but the computation burden is
extremely heavy.

We also perform the experiments on many other face
databases, and obtain the similar results, say, our method
demonstrates the much better performances uniformly.

Object recognition
The COIL-20 database [Nene et al., 1996] consists of im-
ages of 20 objects viewed from varying angles at the interval
of ﬁve degrees, resulting in 72 images per object.

In this experiment, each image is down-sampled to the size

of 16 × 16 for saving the computation time.

Similar to the face recognition experiments, the results of
our method and LMNN are much better than those of LDA

IJCAI-07

997

and the baseline. Note that both face images and object im-
ages distribute on an underlying manifold, the experiments
verify that NMMP can preserve manifold structure with more
discriminability than LDA.

Digit recognition
In this experiment, we focus on the digit recognition task
using the USPS handwritten 16× 16 digits data set2. The dig-
its 1,2,3,and 4 are used in this experiment as the four classes.
There are 1269, 929, 824 and 852 examples for each class,
with a total of 3874.

On this data set, the baseline already works well, and our
method still makes a little improvement. LDA fails in this
case, which demonstrates that the available projection num-
ber of LDA may be insufﬁcient when the data distributions
are more complex than Gaussian.

Text categorization
In this experiment, we investigated the task of text cat-
egorization using the 20-newsgroups data set3. The topic
rec which contains autos, motorcycles, baseball, and hockey
was chosen from the version 20-news-18828. The articles
were preprocessed with the same procedure as in [Zhou et
al., 2004]. This results in 3970 document vectors in a 8014-
dimensional space. Finally the documents were normalized
into TFIDF representation.

Our method and LMNN both bring signiﬁcant improve-
ments comparing with the baseline on this data set. In com-
parison, the performance of LDA is limited in that the avail-
able projection number of it is only 3, which is insufﬁcient
for this complex task.

7 Conclusion

In this paper, we propose a new method, Neighborhood Min-
Max Projections (NMMP), for supervised dimensionality re-
duction. NMMP focuses only on the pairwise points where
the two points are neighbors of each other. After the dimen-
sionality reduction, NMMP minimizes the distance of the
considered pairwise points within the same class, and max-
imizes the distance of those between different classes.
In
comparison with LDA, NMMP focuses more on the improve-
ment of the discriminability of local structure. This property
is especially useful when the distribution of class data is more
complex than Gaussian. Toy example and real world experi-
ments are presented to validate it. Moreover, other disadvan-
tages of LDA i.e., the singularity problem of Sw and the lim-
itation of the available number of dimension are also avoided
in our method.

As a linear dimensionality reduction method, NMMP can
be viewed as a special case of learning a Mahalanobis dis-
tance metric. Usually, the computation burden of learning a
Mahalanobis distance metric is extremely heavy. Our method
formulates the problem as a constrained optimization prob-
lem, and the global optimum can be effectively and efﬁciently
obtained. Experiments demonstrate that our method has a
competitive performance compared with the recent proposed
Mahalanobis distance metric learning method, LMNN, but
the computation cost is much lower.

2Available at http://www.kernel-machines.org/data
3Available at http://people.csail.mit.edu/jrennie/20Newsgroups/

References
[Belhumeur et al., 1997] Peter N. Belhumeur, Joao P. Hes-
panha, and David J. Kriegman. Eigenfaces vs. ﬁsher-
faces: Recognition using class speciﬁc linear projection.
IEEE Transactions on Pattern Analysis and Machine In-
telligence, 19(7):711–720, July 1997.

[Chen et al., 2000] L. Chen, H. Liao, M. Ko, J. Lin, and
G. Yu. A new lda based face recognition system which
can solve the small sample size problem. Pattern Recog-
nition, 33(10):1713–1726, October 2000.

[Duda. et al., 2000] Richard O. Duda., Peter E. Hart., and
Wiley-

Pattern Classiﬁcation.

David G. Stork.
Interscience, Hoboken, NJ, 2000.

[Fukunaga, 1990] Keinosuke Fukunaga. Introduction to Sta-
tistical Pattern Recognition,Second Edition. Academic
Press, Boston, MA, 1990.

[Goldberger et al., 2005] Jacob Goldberger, Sam Roweis,
Geoffrey Hinton, and Ruslan Salakhutdinov. Neighbour-
hood components analysis. In Advances in Neural Infor-
mation Processing Systems 17, pages 513–520. MIT Press,
Cambridge, MA, 2005.

[Golub and van Loan, 1996] Gene H. Golub and Charles F.
van Loan. Matrix Computations, 3rd Edition. The Johns
Hopkins University Press, Baltimore, MD, USA, 1996.

[Guo et al., 2003] Yue-Fei Guo, Shi-Jin Li, Jing-Yu Yang,
Ting-Ting Shu, and Li-De Wu. A generalized foley-
sammon transform based on generalized ﬁsher discrimi-
nant criterion and its application to face recognition. Pat-
tern Recognition Letter, 24(1-3):147–158, January 2003.

[Jolliffe, 2002] I. T. Jolliffe. Principal Component Analy-

sis,2nd Edition. Springer-Verlag, New York, 2002.

[Nene et al., 1996] S. A. Nene, S. K. Nayar, and H. Murase.
Columbia object image library (COIL-20), Technical Re-
port CUCS-005-96. Columbia University, 1996.

[Samaria and Harter, 1994] F. S. Samaria and A. C. Harter.
Parameterisation of a stochastic model for human face
identiﬁcation. In Proceedings of 2nd IEEE Workshop on
Applications of Computer Vision, pages 138–142, 1994.

[Weinberger et al., 2006] Kilian Weinberger, John Blitzer,
and Lawrence Saul. Distance metric learning for large
margin nearest neighbor classiﬁcation.
In Advances in
Neural Information Processing Systems 18, pages 1475–
1482. MIT Press, Cambridge, MA, 2006.

[Xing et al., 2003] E. Xing, A. Ng, M. Jordan, and S. Rus-
sell. Distance metric learning, with application to cluster-
ing with side-information. In Advances in Neural Infor-
mation Processing Systems, 2003.

[Yu and Yang, 2001] H. Yu and J. Yang. A direct lda algo-
rithm for high-dimensional data - with application to face
recognition. Pattern Recognition, 34:2067–2070, 2001.

[Zhou et al., 2004] Dengyong Zhou, Olivier Bousquet,
Thomas Navin Lal,
and Bernhard
Sch¨olkopf. Learning with local and global consistency. In
Advances in Neural Information Processing Systems 16.
MIT Press, Cambridge, MA, 2004.

Jason Weston,

IJCAI-07

998


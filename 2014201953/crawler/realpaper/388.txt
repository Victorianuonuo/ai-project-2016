Session No.  9 Pattern Recognition II Statistical Approaches 

391 

The  probability  distribution  of  the  events  is 
not  modifi ed  by  any  observation  knowledge , which 
then  gives  no  information  on  X. 

Information  theory  provides  a  measure  of  this 
information  obtained  by  the  observation  knowledge 

Among  many  possible  presentations  let  us  re­
call  briefly  the  "Questionnaire  theory"  approach, 
cf.  Pi card  (13) ,  Terrenoi re  (15)  ; 

Th.e  average  minimum  number  of  binary  ques­
tions  to  be  asked  to  obtain  the  prohahility  one 
answer  is  given  by 

APPLICATION OP QUESTIONNAIRE THEORY 

TO PATTERN RECOGNITION 

J.  C.  Simon 

and 

C.  Roche 

Institut  de  Programmation  -  9,  quai  Saint  Bernard 

PARTS 5e 

ABSTRACT 

Analogies  between  Pattern Recogn i tion  and  St a-

ti stacal  Communication  Theory  are  pointed  out. 
With  the  relative  entropy  between  the  "ideal a t t r i­
bute"  and  "heuristic  attribute"  sets  as  an  index 
of  performance,  optimum  selection  of  possible  heu-
r i s t ic  operators  resuJ ts  in  a  recognition  deci­
sion  in  probabilitty  ,  with  t he  smallest  possi ble 
average  number  of  tests  during  a  "sequential  ana­
lysis"'. 

On  the  other  hand,  a  computer  gencrati on  of 

quasi-optimal  operators  may  be  done  based  on  nume­
rical  attributes  using  the  same  entropy  index  of 
performance. 

Experimental  results  are  presented  : 
Optimal  operator  choice  is  demonstrated  for 
recognition  of  handwri tten  capital  letter  words 
and  for  vocoder  speech  recogniti on. 

Computer  buildi ng  of  quasi-optimal  operators 
Is  shown  for  digitized  images.  They  are  limited 
to  f i r st  level  linear  numerical  operators. 

PRKLTMrNARIES 

Information  or Communi cation  theory  deals with 

the  following  problem  : 

A  number  of  n  mutually  exclusive  "events"  or 
"messages"  xi  are  "realized"  or  "sent".  As  a  con­
sequence  an  "observation"  yj  may  be  made  among  m 
possible  observati ons.  Let  X  and  Y  be  the  corres­
ponding  sets,  p(xi)  be  the  a  priori  probability 
of  xi  and  p(yj  |  xi)  the  conditional  probability 
of  observing  yi 
last  two  setsof  quantities  usually  are  easy  to 
obtain  by  experiment.  From  them,  p(xi  |  yj)  is 
computed  through  the  Bayes  formula  : 

if  the  message  xi  is  sent.  These 

392 

Session No.  9  Pattern Recognition II Statistioal Approaohes 

From  [6]  ,  it  is  clear  that  one  may  compute  ei 

ther  H  [ X  |  Y]  or  1  [ X  ; Y] .  This  last  quantity 
usually  evaluates  the  information  capacity  of  a 
communication  channel.  If  hypothesis  H2  is  r e a l i­
zed  I  [X  ;  Y]  =  0,  if  hypothesis  H1  i s ,I  [ X  ;  Y] = 
H  [ Xj .  Indeed  1  [ X  ;  Y]  is  an  evaluation  of  the 
average  information  brought  by  an  observation. 

§1  INTRODUCTION 

1.1  DEFINITIONS 

Most  of  the  time,  Pattern  Recognition  may  be 

described  by  using  the  following  elements  : 
a.  A  set  lx}  of  "primitive  patterns"  X.  These 

b 

c 

patterns  X  may  be  themselves  sets  of  "elemen­
tary  patterns".  The  usual  representation  of 
an  elementary  pattern  is  a  name  and  fixed  or 
variable  qualifiers t  usually  numerical  values. 
For  instance,  a  retina  made  of  cells  delivers 
an  X,  which  is  the  set  of  elementary  measures 
given  by  the  cells.  One  of  these  measures  is 
representated  by  a  name,  the  variable  numeri­
cal  amplitude  value  and  the  fixed  qualifiers 
of  cell  coordinates  in  the  retina. 
A  number  of  "operators"  £j  acting  on  X.  Usual­
ly  these  operators  are  implemented  by  computer 
programs.  Their  final  result  may  be  called  an 
attribute,  which  in  concrete  form  is  some  com­
puter  register  state.  This  attribute  may  be 
interpreted  as  a  numerical  value  or  a  word  on 
the  primitive  pattern  names  alphabet.  We  shall 
call  the  corresponding  operator  "numerical"  or 
"linguistic"  operators. 
[Sometimes  an  "ideal"  operator  £id  delivers  an 
attribute  such  that,  with  it  alone,  a  recogni­
tion  decision  can  be  made  with  quasi-certain­
ty. Most  of  the  time,  different  attributes  arc-
used  to  make  a  decision.  For  this  purpose  ano­
ther  operator  acting  on  the  attribute  is  used, 
the  result  of  which  is  the  expected  classifi­
cation.  This  special  operator  is  called  by  va­
rious  names  :  "classifier",  "detector",  "rate-
gorizer",  "discriminant",  ''decision  machine" 
etc... 
recognition  of  a  specific  pattern,  represented 
by  a  name  and  eventually  some  qualifiers.  The 
obtained  set  uf  recognized  patterns  may  be 
used  as  a  new  "primitive  pattern",  which  in 
turn  may  be  analysed  by  some  other  operators 
giving  birth  to  new  recognized  patterns. 

Its  final  result  or  attribute  is  the 

I . j.  RELATIONS  BETWEEN  PATTERN  RECOGNITION  AND 

COMMUNICATION THEORY 
From  the  above  definitions,  it  is  clear  that 
an  "operator"  may  be  considered  as  a  communica­
tion  channel  between  the  primitive  pattern  set 

Let  us  admit  that  an  "ideal  operator"  is  avai­

lable.  A  pattern  X  belonging  to  class Pi 
(1  <  i  <  n)  w i ll  always  give  the  same 
analysed  by  &id.  Most  of  the  time,  it  will  be  a 
human  observer.  Then  comparisons  have  to  be  esta­

(i)  when 

blished  between  that  ideal  operator  (or  channel) 
and  thcjpractical  operators  (or  channels)  imple­
mented  into  the  computer.  Let  w(i)  €Ω  be  the 
ideal  operator  attributes,  qj(k)  €  Aj  be  the  prac 
t i c al  operator  attributes. 

Some  operators  being  implemented,  we  w i ll 

I[Ω 

;  Aj]  as  an  evaluation  function  for 

show  in  paragraph  I I,  that  their  sequential  use, 
to  approach  the  ideal  operator  result,  may  be  op­
timized  by  the  comparisons  between  these  opera­
tors  and  the  ideal  operator.  Lewis  (9)  has  propo­
sed 
"feature  selection  and  ordering",  cf.  also  Fu  (8) 
chap,  2  §1.  Benzecri  (3),  Bongard  (5)  chap.  7. 
However  the  analogy  with  Communication  Theory  has 
not  been  clearly  set  up.  As  a  consequence  the  au­
thors  derive  directly  the  entropy  concept  from 
linear 
In  fact, 
the  main  interest  of  our  paragraph II  applications 
is  to  show  that  this  concept  works  for  any  type 
of  operators,  whether  it  is  numerical  or  linguis­
t i c,  as  long  as  the  requested  statistical  informa 
tion  is  available. 

factor  analysis,  cf.  Tou  (15). 

In  fact,  it  would  be  quite  profitable  to  im­

plement  an  automatic  generation  of  operators  by 
computers  instead  of  using  man-made  programs. 

In  paragraph  111,  we  show  how  operators  may 

be  automatically  generated  and  evaluated  with 
"the  same  information  performance  index.  For  the 
envisioned  example  the  best  operators  found  are 
linear  digital  f i l t e rs  of  a  type  familiar  to  ima­
ge  processing  specialists.  However  the  program 
application(II.  5)is  not  restricted  to  this  type 
of  linear  operator. 

The  automatic  generation  of  numerical  opera­
tors  (character!zers)  has  been  initiated  by  Pr. 
Uhr  cf.  Uhr  and  Vossler  (17),  Uhr  and  Jordan  (18) 
In  a  sense  the  learning  studies  in  the  f i e ld  of 
Perceptrons,  Factor  analysis,  Karhunen-Loeve  ex­
pansion  may  be  considered  as  automatic  generation 
of  linear  numerical  operators. 

T.  G.  Evans  (9)  has  made  a  review  of  similar 

efforts  in  the  field  of  automatic  generation  of 
pattern  grammars,  called  by  us  linguistic  opera­
tors.  L i t t le  has  been  achieved  up  to  now  and  our 
statistical  approach  seems  promising  even  in  this 
domain,  to  which  it  may  be  applied  as  well. 

To  our  knowledge,  the  use  of  the  statistical 

information  methods  has  not  been  proposed  for  the 
generation  of  any  type  of  pattern  recognition  ope 
rators. 
Remark 

w(  i  )€  Ω  being  an  attribute  of  the  ideal  ope­
rator  &id  a  pattern  X  analysis  may  deliver w ( i) . 
This  attribute  classify  X  in  the  class  Pj_. 

An  operator  &j  w i ll  deliver  an  attribute 

aj(k)  €  A:  for  the  same  pattern  X. 

by  presenting  a  succession  of  pattern  X,  an 
array  of  the  frequency  of  the  simultaneous  occu­
rence  of  w(i)  and  aj(k)  may  be  obtained.  We  w i ll 
assume  that  this  array  converges  towards  the  pro­
bability  density  matrix. 

Session No.  9  Pattern Recognition II  Statistical Approaches 

393 

H  I A  |  A  ]  thus  exists,  cf.  Fano  '(7).  Our  process 
of  choosing  the  highest  decreasing  available  r.tep 
insures  that  this  iower  bound  is  approached  as 
fast  as  possible.  A  new  probability  distribution 
w i ll  result  from  the  process  after  the  use  of  q 
attributes. 

We  are  sure  that  at  each  step  H  [  Ω]  Aq  ]  is 

a  non  increasing  quantity  ;  a  lower  bound  of 

394 

Session No.  9 Pattern Recognition II Statistical Approaches 

large, 
I  [Ω  ;  A1] 
I  [Ak  ;  A1]  small. 

The  conditions  can  be  generalized  to  estimate 

operators  interest,  having  had  the 

the  (q  +  1) 
informations  provided  by  the  q  others. 
N.  B.  The  condition  I  [  Ak  ;  A1]  small  is  essential 
for  a  practical  application  of  Bayes  formula, 
which  permits  computing  p [w( i) 
ly,  Ak  and  A1  should  be  independent  to  apply 
Bayes'  formula. 
II.3  CAPITAL  HANDWRITTEN  WORDS  RECOGNITION 

|  aq]  .  Rigourous-

An  optical  system  delivers  for  a  written  word 

of  five  letters  a  quantized  pattern  X,  made  of 
five  subpatterns  Xj,  each  representing  a  handwrit­
ten  capital  letter.  Questionnaire  methods  as  des­
cribed  above  are  applied  to  both  stages  :  word 
and  letters. 
1. Word  recognition 

£5  number  of  features  linked  by  the  middle. 
&6  POS  1  +  3  x  POS  2  of  second  line  ;  this  line 

is  the  f i r st  obtained  through  the  preproces­
sing  performed  in  a  standard  way. 

£7  angle  of  second  line  (column  five). 
&8  angle  of  third  line. 
&9  apgle  of  fourth  line. 

Experience  shows  that  only &1 ,  &2  and  &8 are 

called  consistently  by  the  program.  Alone  they 
are  sufficient  to  reach  a  decision  on  a  letter 
with  high  probability.  The  other  operators  thus 
are  rejected. 

On  the  average  four  questions  were  sufficient 
to  select  a  word  among  f i f ty  with  an  80  %  probabi 
l i t y.  For  instance  ZEBRE  was  tested  that  way  : 

The five operators of this stage are  :  {vhich 

letter is the jth | j = 1, Z\. 5). 

The program asks sequentially for the most in­

formative letter detection, until the word proba­
bility detection reaches 80 %. Fifty words of five 
letters are to be read ; they were selected at 
random. On the average only two letters out of f i­
ve had to be determined to obtain an BO % probabi­
lity. 
2. Letter recognition 

When the j ^ '1 letter has to be "read", prepro­

cessing of X;  is made. From the primitive pattern 
Xj  of 0 and  i, "characteristic features" are ex­
tracted by a program. These features are straight 
segments. A new primitive pattern Y is obtained. 
Let us represent Y. 

The first line contains the names of object 

or relations  considered in the columns.  Other l i­
nes are the analysis results or attributes  for 
relations of features. Let us take the example of 
the letter A. 

The  second  line  contains  features  1  and  1: 

(column  1  and  2')  ;  0  in  the  third  and  fourth  co­
lumns  means  that  they  are  linked  by  their  tails  ; 
0  in  the  f i f th  column  codes  the  angle  in  octal  re­
presentation.  This  representation  is  quite  similar 
to  Shaw's  P.  b.  L.  (lh) 

Nine  operators  are  applied  to  a  pattern  Y. 

&1  number  of  primitive  features. 
fij  number  of 
&3  number  of  primitive  features  found  only  in 
i.e.  linked  to  one  other  feature 

relations. 

one  line, 
only. 

fill  number  of  graph  cycles  of  a  letter. 

Four  questions  were  asked  instead  of  the  for­

ty  five  possible  questions. 
I I .4  SPEECH  RECOGNITION 
Description  of  the  experiment 

The  goal  of  this  program  is  to  detect  the 
highest  probability  "phonemes"  in  experimental 
patterns  delivered  by  a  "vocoder".  Fig.  1  dis­
plays  the  vocoder  output  ;  a  "line"  is  obtained 
every  ?"i  milliseconds,  it  gives  the  digitized  va­
lues  of  the  selective  f i l t e rs  and  the  pitch  value 
a.  Seventeen  heuristic  operators  &1, . ..  &17  were 

implemented,  such  as  : 
-  existence  of  a  pitch  different  from  zero  or 

not. 

-  number  of  relative  maxima  (formants). 
-  relative  difference  between  f i r st  maximum 

and minimum. 

-  difference  between  biggest  maximum  and  smal 

lest  minimum. 

-  variation  of  average  intensity  between  two 

successive  lines. 

The  operator  choice  was  based  on  the  need 
for  being  invariant  to  translations  of  the 
frequency  and  of  the  intensity  axes  (logarith 
mic  scales). 

b.  "Training  set".  A  human  operator  had  to  deci­

de  what  is  the  phoneme  to  be  recognized 
(setΩ  )  versus  the  pattern  obtained  with  the 
vocoder.  Fig.  1  shows  some  of  the  VTSU  pro­
gram  results  used  by  the  human  operator. 

Session No.  9 Pattern Recognition II Statistical Approaches 

395 

396 

Session No.  9 Pattern Recognition II Statistical Approaches 

Session No.  9 Pattern Recognition II Statistical Approaches 

397 

398 

Session No.  9  Pattern Recognition II Statistical Approaches 

II.5  GENERAL  RECOGNITION  PROGRAMMATIQN 

The  programs  PRORA,  CORREL,  SYNTHE  are  general 

and  may  be  utilized  as  an  aid  for  "building  a  p a t­
tern  recognition  system  : 
(i)  Take  a  "training  set"  and  a  "test  set"  of data 
Both  of  them  are  presented  to  the  "teacher" 
or  Ideal  operator  &id  ;  the  results  are  intro­
duced  in  the  computer. 

( i i)  Implement  some  "heuristic"  operators 

{&j  I  J  €  1 ,. ..  p). 

( i i i)  See  with  CORREL  if  they  are  correlated  by 
pair.  Modify  the  operators  u n t il 
Max  1  I &j  ;  &k]  <  Is,  either  by  grouping  or 
suppressing  some  of  them. 
(iv)  Use  SYNTHE  and  see  if  the  cho  sen  number  of 

operators  is  sufficient 
operators  to  the  set  and  go  to  ( i i) 
is  sufficient  go  to  ( i v ). 

;  if  not,  add  other 
;  if  it 

(v)  What are  the  operators  used by  SYNTHE? Use  PRO-
BA  to  see  if  a  probability  clustering  is  appa­
If  possible,  group  some  operators  accor­
rent. 
ding  to  this  clustering.  Go  to  ( i i i ). 
If  (v)  is  successful,  only  one  operator  re­

mains,  very  close  to  the  "ideal  operator".  Other­
wise  a  pattern  recognition  process  as  described  in 
§1.1.  has  to  be  started  anew  on  the  new  attribute 
sets,  giving  birth  to  a  new  recognition  level. 

By  reducing  the  dimensionality  we  must  try  not 

to  loose  much  information. 
I I I .2  EXPERIMENT 

The  pattern  X  is  the  result  of  an  image  d i g i­

tizer  of  four  levels  (0,  1,  2,  3).  The  objects 
are  simple  shapes  of  nearly  uniform  level,  cf. 
Fig.  5-a. 

Let  us  call  &T  a  parametered  operator,  such 

that  it  gives  a  7  x  7  square  subpattern.  The 
parameters  of &T, are  the  coordinates  xT,  yT  of 
the  square  center.  Inside  this  7 x7  square,  U9 
attributes  may  be  obtained  for  a  certain  xT,  yT. 
They  may  be  considered  to  be  the  results  of  49 
suboperators  &T(v),  with  1  < v  <  49. 

The  goal  is  to  decide  which  class  the  subpat­

tern  selected  by  &T  is  from  among  the  set 

Ω= {.., ,N}. The elements of are determined by 

Sid,  here  a  human  experimenter  working  with  a 
"training  set",  cf.  Fig.  4. 

§ll  GENERATION  OF  QUASI  OPTIMAL 

NUMERICAL OPERATORS 

lll.l 

PRINCIPLE 

In  paragraphe  I I,  the  operators  &j  are  suppo­
sed  to  be  implemented  by  the  human  experimenter. 
The  computer  production  of  new  operators  certain­
ly  would  be  of  interest.  For  this  purpose,  we 
t r i ed  to  introduce  a  program  that  builds  new  ope­
rators  by  combining  the  attributes  of  some  former 
operators. 

Let  us  consider  a  "retina"  made  of  elementary 
cells.  The  result  registered  by  one  of  these  cells 
may  be  considered  as  the  attribute  of  a  "hidden" 
operator.  Let  a-,  a ; . ..  be  the  variable  names  of 
these  cell  attributes  ;  each  of  these  is  an  ele­
ment  of  the  sets  Aj ,  A; .. .  Again  the  ideal  opera­
tor  &id  w i ll  result  in  classifying  the  X  presented 
to  the  retina.  Here  this  £id  w i ll  be  implemented 
by  the  experimenter  himself,  who  w i ll  decide  to 
which  class  X  belongs. 

In  paragraph  I I,  the  process  of  using  the 

string  aj1  ,  aj2  . . .  ajq  could  be  considered  the  re­
sult  of  the  action  ofqa  "union  operator",  the  at­
tribute  of  which  is  an  element  of  Aq  (cf.  I I. 1  of 
this  paper).  We  found  that  the  information  value 
of  a  string  was  H  [ Ω |  Aq]  . 

Let  us  consider  an  operator  &op  working  on  p 
attributes  and  giving  a  new  attribute  aop  €  Ao p. 
The  value  of  an  operator  £op  is  to  reduce  the 
dimensionality  of  the  attribute  space. 
It  follows 
from  the  same  source  as  the  idea  of  defining  "re­
gions"  by  decision  surfaces.  But  we  can  show,  cf. 
Ash  (1)  p.  85,  that  the 

performance  index 

Table  I 

The  quantities  have  to  be  compared  to 

H(Ω)  =  143.  The  center  cell  maximum  mutual  infor­
mation  value  shows  that  the  knowledge  of  the  cor­
responding  attribute  alone  is  far  from  enabling 
us  to  make  a  correct  decision. 

Session No.  9  Pattern Recognition II Statistical Approaches 

399 

400 

Session No.  9 Pattern Recognition II Statistical Approaches 

Recognition 

401 

IRE  Trans.  Infor.  Theory  8_,  pp.  171-178 
(1962). 

(11)  Masson  C.  G.  :  "Hierarchical  Clustering  of 
Data  with  a  Mutual  Entropy  Criterion, 
its  Structural  Feature  Extraction  Abili­
ty".  IEEE  Conference  Record  of  the  sym­
posium  on  feature  extraction  and  selec­
tion  in  Pattern  Recognition  p.  155  ; 
Argonne  National  Laboratory,  Argonne 
I l l i n o i s,  U.  S.  A.  Oct.  5-7-(1970). 

(12)  Monod  J. 

:  "Le  hasard  et  la  necessite".  Edi­

tions  du  Seuil.  Paris  (1970). 

(13)  Picard  :  "Theorie  des  Questionnaires".  Gau-

thier-Villars.  Paris  (1965). 

(14)  Shaw  A.  C.  :  "The  formal  Description  and  Par­

sing  of  Pictures".  SLAG  report  n°  84, 
Stanford  Linear  Accelerator  Center, 
Stanford  Univ.,  Calif.  (March  1968). 
(15)  Terrenoire,  Faure  and  Chesnais  :  "Aide  au 

diagnostic  en  Toxicologic".  Congres 
d'Informatique.  Toulouse  (March  1970). 
(16)  Tou  J.  T.  and  Heydron  R.  P.  :  "Some  approa­
ches  to  optimum  feature  extraction"  in 
"Computer  and  Information  Sciences". 
Academic  Press.  N.  Y.  1967. 

(17)  Uhr  L.  and  Vossler  Ch.  : "A  pattern  recogni­
tion  program  that  generates  and  evalua­
tes  its  own  operators"  en  "Computer  and 
Thought"  by  Feigelbaum  and  Feldman, 
Mc.  Graw  H i l l,  1963. 

(18)  Uhr  L.  and  Jordan  S.  :  "The  Learning  of  Para 

meters  for  Generating  Compound  Charac­
t e r i z es  for  Pattern  Recognition".  Pro­
ceedings  of  the  IJCAI  1969.  May  7-9. 
Washington  D.  C. 

Examination  of  Table  VI  leads  to  choosing  the 
thresholds  between  3  and  7  and  23  and  28.  A  new 
operator  classifier  &c  of  attribute  ac  is  imple­
mented  such  that  Ac  =  { 
The  couple  &c  o  &  of  sequentially  applied  ope­
rators  is  an  approximation  of  &id.  The  discrepan­
cy  rate  with  &id  is  close  to  11  %.  Application  of 
this  machine  generated  operator  is  shown  on  Fig. 
5-b.  The  results  are  satisfactory. 
I l l .3  CONCLUDING  REMARK 

,  .  ,  N}. 

Modern  biochemists  believe  that  an  evolutiona­
ry  process  has  built  up  the  nucleotide  strings  by 
random  variations  followed  by  performance  tests. 
Life  did  not  allow  the  uninteresting  or  unfit  bio-
molecules  to  survive,  cf.  Monod  J.  (12). 

Our  results  have  a  similarity  to  this  point 

of  view.  One  wonders  if  brain  organisation  may  not 
have  gone  through  a  similar  evolution  :  random 
formation  of  new  operators,  selection  of  the  best 
by  their  performances.  This  point  of  view  is  also 
tempting  in  practical  Pattern  Recognition,  though 
the  process  of  building  up  at  random  the  highest-
order  operators  seems  somewhat  improbable. 
Acknowledgement  :  The  authors  wish  to  thank  a  num­
ber  of  graduate  students,  who  during  69-71  have 
participated  to  this  study  :  MM.  Davencens,  Huet, 
Heude,  Raabe,  Sabah,  Poussin,  Sechet. 

REFERENCES 

(1) 

(2) 

(3) 

(4) 

(5) 

Ash  R.  B.  :"information  Theory".  Interscience 

Publishers.  John  Wiley  &  sons  (1967). 
Becker  P.  W.  :  "Recognition  of  Patterns". 

Polyteknisk  Forlag,  Copenhaguen  (1968). 

Benzecri  J.  P. 

:  "Theorie  de  I'Information 

Benzecri  J.  P. 

et  Classification".  ISUP.  Paris  (1969). 
:  "Statistical  Analysis  as  a 
tool  to  make  Patterns  emerge  from  data" 
pp.  35-74  in  Methodologies  of  Pattern 
Recognition  by  Watanabe  M.  S.  -  Academic 
Press.  (1969). 

Bongard  M.  :  "Pattern  Recognition".  Spartan 

books  (1970). 

(6)  Bush  R.  R.  and  Estes  W.  K.  :  "Studies  in  Ma­

thematical  Learning  Theory".  Stanford 
Univ.  Press.  -  Calif.  (1968). 

(7)  Fano  R.  M.  :  "Transmission  of  Information". 

MIT  press.  (1963). 

(8)  Fu  K.  S.  ;  "Sequential  Methods  in  Pattern  Re 
cognition  and  Machine  Learning".  Acade­
mic  Press.  (1968). 

(9)  Evans  T.  G.  :  "Grammatical  Inference  Techni­
ques  in  Pattern  Analysis".  Third  Inter­
national  Symposium  on  Computer  and  I n­
formation  Sciences".  Miami  Beach,  Fla. 
Dec.  18-20(1969). 

(10)  Lewis  P.  M.  :  "The  Characteristic  Selection 

Problem  in  Recognition  Systems". 


A Universal Measure of Intelligence for Artiﬁcial Agents(cid:3)

Shane Legg and Marcus Hutter

IDSIA, Galleria 2, Manno-Lugano 6928,

Switzerland

fshane,marcusg@idsia.ch

1 The concept of intelligence
A fundamental difﬁculty in artiﬁcial intelligence is that no-
body really knows what intelligence is, especially for systems
with senses, environments, motivations and cognitive capac-
ities which are very different to our own. If we look to def-
initions of human intelligence given by experts, we see that
although there is no consensus, most views cluster around a
few common perspectives and share many key features.

In all cases, intelligence is a property of an entity, which we
will call the agent, that interacts with an external problem or
situation, which we will call the environment. An agent’s in-
telligence is typically related to its ability to succeed in envi-
ronments, which implies that there is some kind of objective,
which we will call the goal. The emphasis on learning, adap-
tation and ﬂexibility common to many deﬁnitions implies that
the environment is not fully known to the agent. Thus intelli-
gence is the ability to deal with a wide range of possibilities,
not just a few speciﬁc situations. Putting these things together
gives us our informal deﬁnition of intelligence:

Intelligence measures an agent’s general ability to
achieve goals in a wide range of environments.

We are conﬁdent that this deﬁnition captures the essence of
many common perspectives on intelligence. It also describes
what we want to achieve in machines: A very general capac-
ity to adapt and perform well in a wide range of situations. In
this paper we will try to formalise this view of intelligence.

2 A formal framework
We refer to the signals sent from the agent to the environment
as actions, and the signals sent back as perceptions. Our deﬁ-
nition requires there to be a goal for the agent to try to achieve
which implies that the agent knows what it is. If the goal was
known in advance it could be built into the agent, however
this would limit each agent to just one goal. The alternative
is to inform the agent of what the goal is. Unfortunately, the
possession of a high level of language is too strong an as-
sumption to make. Thus we deﬁne a communication channel
with very simple semantics: A signal that indicates how good
the agent’s current situation is, called the reward. The agent
then simply tries to maximise the amount of reward it receives
by learning about the structure of the environment and what
it needs to accomplish in order to receive large rewards.

(cid:3)This work was supported by SNF grant 2100-67712.02.

This is the extremely ﬂexible reinforcement

learning
framework commonly used in AI, and is equivalent to the
controller-plant framework in control theory. In our formu-
lation we will include the reward signal as part of the percep-
tion generated by the environment. Thus, as the agent’s goal
is implicitly deﬁned by the environment, to test an agent in
any given way it is sufﬁcient to fully deﬁne its environment.
Formally, the agent sends information to the environment
by sending symbols from some ﬁnite set called the action
space, for example, A := fup; down; left; rightg. The en-
vironment responds with symbols from a ﬁnite set called the
perception space, denoted P. The reward space, denoted by
R, is a ﬁnite subset of [0; 1] \ Q. Every perception consists
of two separate parts; a reward and a non-reward part called
an observation. For symbols being sent we will use the lower
case variable names o, r and a for observations, rewards and
actions respectively. We index these in the order in which
they occur, thus a3 is the agent’s third action. The agent and
the environment take turns at sending symbols, starting with
the environment. This produces an increasing history of ob-
servations, rewards and actions, o1r1a1o2r2a2o3r3a3o4 : : :.
The agent is a function, denoted by (cid:25), which takes the cur-
rent history as input and chooses the next action as output.
We represent this as a probability measure over actions condi-
tioned on the current history, for example, (cid:25)(a3jo1r1a1o2r2).
The workings of the agent is unspeciﬁed, though in artiﬁcial
intelligence the agent will be a machine and thus (cid:25) will be
a computable function. The environment, denoted (cid:22), is sim-
ilarly deﬁned: 8k 2 N the probability of okrk, given the
current history is (cid:22)(okrkjo1r1a1o2r2a2 : : : ok(cid:0)1rk(cid:0)1ak(cid:0)1).
The agent must try to maximise the total reward it receives,
however what this means depends on how we value rewards
at different points in the future. The standard way of express-
ing this is to weight the future reward at time i by a factor (cid:13)i.
Thus the future value is V (cid:25)(cid:22) := E(cid:0) P1
i=1 (cid:13)iri(cid:1), where ri is
the reward in cycle i of a given history, and the expected value
is taken over all possible interaction histories of (cid:25) and (cid:22). The
choice of (cid:13)i is a subtle issue that controls how greedy or far
sighted the agent should be. Here we use the near-harmonic
(cid:13)i := 1=i2 as this produces an agent with increasing farsight-
edness of the order of its current age [Hutter, 2004].

As we desire an extremely general deﬁnition of intelligence
for arbitrary systems, our space of environments should be
as large as possible. An obvious choice is the space of all
probability measures, however this causes serious problems

as we cannot even describe some of these measures in a ﬁ-
nite way. The solution is to require the measures to be com-
putable. This allows for an inﬁnite space of possible environ-
ments with no bound on their complexity. It also permits en-
vironments which are non-deterministic as it is only their dis-
tributions which need to be computable. This space, denoted
E, appears to be the largest useful space of environments.

3 A formal measure of intelligence
We want to compute the general performance of an agent
in unknown environments. As there are an inﬁnite number
of environments, we cannot simply take a uniform distribu-
tion over them. If we consider the agent’s perspective on the
problem, this is the same as asking: Given several different
hypotheses which are consistent with the data, which hypoth-
esis should be considered the most likely? This is a standard
problem in inductive inference for which the usual solution
is to invoke Occam’s razor: Given multiple hypotheses which
are consistent with the data, the simplest should be preferred.
As this is generally considered the most intelligent thing to
do, we should test agents in such a way that they are, at least
on average, rewarded for correctly applying Occam’s razor.
This means that our a priori distribution over environments
should be weighted towards simpler environments.

As each environment is described by a computable mea-
sure, one way of measuring the complexity of an environment
is by taking its Kolmogorov complexity. If U is a preﬁx-free
universal Turing machine then the Kolmogorov complexity of
an environment (cid:22) is the length of the shortest program on U
that computes (cid:22), formally K((cid:22)) := minpfl(p) : U(p) = (cid:22)g.
Unfortunately, K is not computable and is provably difﬁ-
cult to approximate. For the purposes of Occam’s razor, it
also seems philosophically unnatural to consider short pro-
grams which require an enormous amount of time to compute
to be “simple”. We can address both of these problems by
using a notion of complexity that takes execution time into
account, such as Kt complexity [Levin, 1973]. Formally,
Kt((cid:22)) := minpfl(p) + log t(p) : U(p) = (cid:22)g where t(p) is
the number of steps required to compute (cid:22) on U. This gives
us a computable distribution 2(cid:0)Kt((cid:22)) over our space of pos-
sible environments which is consistent with the notion that
very simple algorithms should be short and fast to compute.
Another alternative is the Speed Prior [Schmidhuber, 2002].
We can now deﬁne the universal intelligence of an agent
(cid:25) to simply be its expected performance when faced with an
unknown environment sampled from this distribution,

(cid:7)((cid:25)) := X

2(cid:0)Kt((cid:22))V (cid:25)(cid:22):

(cid:22)2E

4 Properties of universal intelligence
It is clear by construction that universal intelligence measures
the general ability of an agent to perform well in a very wide
range of environments, as required by our informal deﬁnition
of intelligence given earlier. The deﬁnition places no restric-
tions on the internal workings of the agent; it only requires
that the agent is capable of generating output and receiving
input which includes a reward signal. Universal intelligence
also reﬂects Occam’s razor in a natural way that respects both

the minimal description and computation time of an environ-
ment.
Indeed it is similar to intelligence tests for humans
which usually deﬁne the correct answer to a question to be
the simplest consistent with the given information. Although
our space of possible environments E is inﬁnite, as Kt is
computable, (cid:7)((cid:25)) can also be computed.

By considering V (cid:25)(cid:22) for a number of basic environments,
such as small MDPs, and agents with simple but very gen-
eral optimisation strategies, it is clear that (cid:7) correctly orders
the relative intelligence of these agents in a natural way. If we
consider a highly specialised agent, for example IBM’s Deep-
Blue chess super computer, then we can see that this agent
will be ineffective outside of one very speciﬁc and complex
environment, and thus would have a very low universal intel-
ligence value. This is consistent with our view of intelligence
as being a highly adaptable and general ability.

A very high value of (cid:7) would imply that an agent was
able to perform well in many environments. Such a machine
would obviously be of large practical signiﬁcance. If we re-
place near-harmonic discounting with a ﬁnite length horizon
and ignore computation time, it is possible to deﬁne an or-
der relation between agents over interaction histories, known
as the Intelligence Order Relation (IOR) [Hutter, 2004]. The
maximal agent with respect to this order relation is AIXI, and
with minor adjustments, it would also be maximal with re-
spect to (cid:7). AIXI has been shown to have many optimality
properties, including the ability to be self-optimising in envi-
ronments in which this is at all possible [Hutter, 2004]. These
results demonstrate the power of agents which rate highly
with respect to the IOR and the related (cid:7) deﬁned here.

Clearly (cid:7) spans simple adaptive agents right up to super
intelligent agents like AIXI, unlike the pass-fail Turing test
which is useful only for agents with near human intelligence.
Moreover, the Turing test is highly anthropomorphic, with
many suggesting that it is a test of humanness rather than in-
telligence. We have avoided this problem of human bias, as
well as the need for human judges, by basing our deﬁnition
on the fundamentals of information and computation theory.
The only related work to ours is the C-Test [Hern´andez-
Orallo, 2000]. While (cid:7) is an interactive test, the C-Test is a
static sequence prediction test which always ensures that each
question has an unambiguous answer. We believe that these
are unrealistic and unnecessary assumptions. The C-Test was
able to compute a number of usable test problems which were
shown to correlate with real IQ test scores for humans.

References
[Hern´andez-Orallo, 2000] J. Hern´andez-Orallo. Beyond the
Turing test. Journal of Logic, Language and Information,
9(4):447–466, 2000.

[Hutter, 2004] M. Hutter. Universal Artiﬁcial Intelligence:
Sequential Decisions based on Algorithmic Probability.
Springer, Berlin, 2004.

[Levin, 1973] L. A. Levin. Universal sequential search prob-

lems. Problems of Information Trans, 9:265–266, 1973.

[Schmidhuber, 2002] J. Schmidhuber. The Speed Prior: A
new simplicity measure yielding near-optimal computable
predictions. In Proc. COLT 2002, Lecture Notes in Artiﬁ-
cial Intelligence, pages 216–228, July 2002. Springer.


Bayesian Information Extraction Network 

Leonid Peshkin and Avi Pfeffer 

Harvard University 

Cambridge, MA 02138, USA 

{pesha,avi}@eecs.harvard.edu 

Abstract 

Dynamic  Bayesian  networks (DBNs) offer an ele(cid:173)
gant  way  to  integrate  various  aspects  of language 
in one model.  Many existing algorithms developed 
for learning and inference  in  DBNs  are applicable 
to probabilistic language modeling. To demonstrate 
the potential of DBNs for natural language process(cid:173)
ing, we employ a DBN in an information extraction 
task.  We show how to  assemble wealth of emerg(cid:173)
ing linguistic instruments for shallow parsing, syn(cid:173)
tactic and semantic tagging, morphological decom(cid:173)
position,  named entity recognition etc.  in order to 
incrementally build a robust information extraction 
system.  Our method  outperforms previously pub(cid:173)
lished results on an established benchmark domain. 

Information Extraction 

1 
Information  extraction  (IE)  is  the  task  of  filling  in  template 
information  from previously  unseen  text  which belongs to a 
pre-defined domain.  The resulting database is suited for for(cid:173)
mal  queries and  filtering.  IE  systems generally work by de(cid:173)
tecting  patterns  in  the  text  that  help  identify  significant  in(cid:173)
formation.  Researchers have shown [Freitag and McCallum, 
1999;  Ray  and  Craven,  2001]  that  a probabilistic  approach 
allows  the  construction  of robust  and  well-performing  sys(cid:173)
tems.  However,  the  existing probabilistic  systems  are  gen(cid:173)
erally  based  on  Hidden  Markov  Models  (HMMs).  Due  to 
this  relatively  impoverished  representation,  they  are  unable 
to  take  advantage  of the  wide  array  of linguistic  information 
used by many non-probabilistic  IE systems.  In addition, ex(cid:173)
isting HMM-based systems model each target category sepa(cid:173)
rately, failing to capture relational information,  such as typ(cid:173)
ical  target order,  or the  fact that each  element  only  belongs 
to a single category.  This paper shows how to incorporate a 
wide array of knowledge into a probabilistic IE system, based 
on  dynamic  Bayesian  networks  (DBN)—a  rich  probabilistic 
representation that generalizes HMMs. 

Let us  illustrate  IE  by describing  seminar announcements 
which  got  established  as  one  of  the  most  popular  bench(cid:173)
mark domains in the  field  [Califf and Mooney,  1999; Freitag 
and McCallum, 1999; Soderland, 1999; Roth and Yih, 2001; 
Ciravegna,  2001].  People  receive  dozens  of  seminar  an(cid:173)
nouncements weekly and need to manually extract informa(cid:173)

tion and paste it into personal  organizers.  The goal  of an  IE 
system  is  to  automatically  identify  target  fields  such  as  lo(cid:173)
cation and topic of a seminar, date and starting time, ending 
time  and  speaker.  Announcements  come  in  many  formats, 
but usually follow some pattern.  We often find a header with 
john@host . d o m a i n; 
a gist in the form " P o s t e d B y: 
Who:  D r. 
1  a m ;" and so forth. 
Also in the body of the message, the speaker usually precedes 
both location and starting time, which  in turn precedes end(cid:173)
ing  time  as  in:  MD r. 
H a ll  at  one  a m . ''  The task is complicated since some 
fields may be missing or may contain multiple values. 

S t e a ls  p r e s e n ts 

S t e a l s;  When: 

in  Dean 

This  kind  of data  falls  into  the  so-called  semi-structured 
text  category. 
Instances  obey  certain  structure  and  usually 
contain  information  for  most  of the  expected  fields  in  some 
order. There are two other categories:  free text and structured 
text.  In structured text, the positions of the information  fields 
are  fixed  and  values  are  limited  to pre-defined  set.  Conse(cid:173)
quently, the IE systems focus on specifying the delimiters and 
order associated with each  field.  At the opposite end lies the 
task of extracting information  from free text which, although 
unstructured, is assumed to be grammatical.  Here IE systems 
rely more on syntactic, semantic and discourse knowledge in 
order  to  assemble  relevant  information  potentially  scattered 
all over a large document. 

IE  algorithms  face  different  challenges  depending  on  the 
extraction targets and the kind of the text they are embedded 
in.  In some cases, the target is uniquely identifiable (single-
slot), while in others, the targets are linked together in multi-
slot association frames.  For example, a conference schedule 
has several slots for related speaker, topic and time of the pre(cid:173)
sentation, while a seminar announcement usually refers to a 
unique event.  Sometimes it is necessary to identify each word 
in a target slot, while some benefit may be reaped from par(cid:173)
tial identification of the target, such as labeling the beginning 
or end of the slot separately.  Many applications involve pro(cid:173)
cessing of domain-specific jargon like lnternetese—a style of 
writing prevalent  in news  groups,  e-mail  messages,  bulletin 
boards and online chat rooms.  Such documents do not follow 
a  good  grammar,  spelling  or  literary  style.  Often  these  are 
more  like  a stream-of-consciousness  ranting  in which  ascii-
art and pseudo-graphic sketches are used and emphasis is pro(cid:173)
vided by all-capitals, or using multiple exclamation signs.  As 
we  exemplify  below,  syntactic  analysers  easily  fail  on  such 

INFORMATION  EXTRACTION 

421 

corpora. 

Other  examples  of  Hi  application  domains  include  job 
advertisements  [Califf  and  Mooney,  1999]  (RAPIER),  ex(cid:173)
ecutive  succession  [Soderland,  1999]  (WHISK),  restaurant 
guides  [Muslea  et  al  .,  2001]  (STALKER),  biological  pub(cid:173)
lications  [Ray  and  Craven,  2001]  etc. 
Initial  interest  in 
the  subject  was  stimulated  by  ARPA\s  Message  Under(cid:173)
standing  Conferences  (MUC)  which  put  forth  challenges 
e.g.  parsing  newswire  articles  related  to  terrorism  (sec  e.g. 
Mikheev [1998]).  Below we briefly review various IE systems 
and approaches  which mostly originated from  MUC compe(cid:173)
titions. 

Successful  IE  involves  identifying  abstract patterns  in the 
way information is presented  in text.  Consequently, all pre(cid:173)
vious  work  necessarily  relies  on  some  set  of  textual  fea(cid:173)
tures.  The  overwhelming  majority  of  existing  algorithms 
operate  by  building  and  pruning  sets  of induction  rules  de(cid:173)
fined on these features (SRV, RAPIER, WHISK, LP2). There 
are  many  features  that  are  potentially  helpful  for  extracting 
specific  fields,  e.g.  there are tokens and delimiters that sig(cid:173)
nal  the  beginning  and  end  of particular  types  of  informa(cid:173)
tion.  Consider an example  in table  1  which shows  how the 
phrase  ' ' D o c t or  S t e a ls  p r e s e n ts 
in  Dean  H a ll 
at  one  a m ."  is  represented  through  feature  values.  For 
example, the lemma "am" designates the end of a time field, 
while  the  semantic  feature  "Title"  signals  the  speaker,  and 
the  syntactic  category  NNP  (proper noun)  often  corresponds 
to speaker or location.  Since many researchers use the semi(cid:173)
nar announcements domain as a testbed, we have chosen this 
domain in order to have a good basis of comparison. 

One  of the  systems  we  compare  to  (specifically  designed 
for single-slot  problems)  is  SRV  [Freitag,  1998].  It  is  built 
on  three  classifiers  of text  fragments.  The  first  classifier  is 
a  simple  look-up  table  containing  all  correct  slot-fillers  en(cid:173)
countered  in the training set.  The second one computes the 
estimated probability of finding the fragment tokens in a cor(cid:173)
rect slot-filler.  The last one uses constraints obtained by rule 
induction over predicates like token identity, word length and 
capitalization, and simple semantic features. 

RAPIER  [Califf  and  Mooney,  1999]  is  fully  based  on 
bottom-up rule induction on the target fragment and a few to(cid:173)
kens from its neighborhood.  The rules are templates specify(cid:173)
ing a list of surrounding items to be matched and potentially, 
a maximal  number of tokens  for each  slot.  Rule generation 
begins  with  the  most  specific  rules  matching  a  slot.  Then 
rules for identical slots are generalized via pair-wise merging, 

until no improvement can be made.  Rules in RAPIER are for(cid:173)
mulated as lexical and semantic constraints and may include 
PoS tags.  WHISK  [Soderland,  1999] uses constraints similar 
to RAPIER, but its rules are formulated as regular expressions 
with wild cards for intervening tokens.  Thus, WHISK encodes 
a relative, rather than absolute position of tokens with respect 
to the target.  This enables modeling long distance dependen(cid:173)
cies in the text.  WHISK performs well on both single-slot and 
multi-slot extraction tasks. 

Ciravcgna  [2001]  presents  yet  another  rule  induction 
method (LP)2.  He considers  several candidate  features such 
as lemma, lexical and semantic categories and capitalization 
to form a set of rules for inserting tags into text.  Unlike other 
approaches,  (LP)2  generates  separate  rules  targeting the  be(cid:173)
ginning and ending of each  slot.  This allows for more flexi(cid:173)
bility in subjecting partially correct extractions to several re(cid:173)
finement stages,  also  relying  on  rule  induction  to  introduce 
corrections.  Emphasizing the relational aspect of the domain, 
Roth and Yoh  [2001]  developed a knowledge representation 
language that enables efficient feature generation.  They used 
the features in a multi-class classifier SNOW-IE to obtain the 
desired  set of tags.  The  resulting method  (SNOW-IE)  works 
in two  stages:  the  first  filters  out the  irrelevant parts  of text, 
while the second identifies the relevant slots. 

Freitag  &  McCallum  [1999]  use  hidden  Markov  models 
(HMM).  A separate HMM is used for each target slot.  No pre(cid:173)
processing  or  features  is  used  except  for the  token  identity. 
For each hidden state, there is a probability distribution over 
tokens encountered as slot-fillers in the training data. Weakly 
analogous to templates,  hidden state transitions encode reg(cid:173)
ularities  in  the  slot  context. 
In  particular  prefix  and  suffix 
states are  used  in addition to target and background  slots to 
capture  words  frequently  found  in  the  neighborhood  of tar(cid:173)
gets.  Ray&Craven  [2001]  make  one  step  further by  setting 
HMM  hidden  states  in  a  product  space  of syntactic  chunks 
and target tags to model the text structure.  The success of the 
HMM-based  approaches  demonstrate  the  viability  of proba(cid:173)
bilistic methods  for this domain.  However,  they do not take 
advantage of the  linguistic  information  used by the other ap(cid:173)
proaches.  Furthermore, they are limited by using a separate 
HMM  for  each  target  slot,  rather  than  extracting  data  in  an 
integrated way. 

The  main  contribution  of this  paper  is  in  demonstrating 
how to integrate various aspects of language in a single prob(cid:173)
abilistic  model,  to  incrementally  build  a  robust  information 
extraction  system based on  a  Bayesian  network.  This  sys-

422 

INFORMATION  EXTRACTION 

tern overcomes the following dilemma.  It is tempting to use 
a lot of linguistic  features in order to account for multiple as(cid:173)
pects of text structure.  However, deterministic rule induction 
approaches  seem  vulnerable  to  the  performance  of  feature 
extractors  in  pre-processing  steps.  This presents  a problem 
since syntactic instruments that have been trained on highly-
polished grammatical corpora,  are particularly  unreliable on 
weakly grammatical semi-structured text.  Furthermore incor(cid:173)
porating  many  features  complicates  the  model  which  often 
has to be learned from sparse data, which harms performance 
of classifier-based  systems. 

2  Features 
Our approach  is  statistical,  which generally  speaking means 
that  learning  corresponds  to  inferring  frequencies  of events. 
The statistics we collect originates in various sources.  Some 
statistics reflect regularities of the  language  itself, while oth(cid:173)
ers  correspond  to  the peculiarities  of the  domain.  With  this 
in mind we design features which reflect both aspects.  There 
is  no  limitation  on  the  possible  set  of features.  Local  fea(cid:173)
tures  like part-of-speech,  number of characters  in  the  token, 
capitalization  and  membership  in  syntactic  phrase  are  quite 
customary in the  in.  In addition one could obtain such char(cid:173)
acteristics of the word as imagibility, frequency of use, famil(cid:173)
iarity, or even predicates on numerical  values.  Since there is 
no need for features to be local, one might find useful includ(cid:173)
ing  frequency  of a  word  in the  training  corpus or number of 
occurrences in the document.  Notice that the same set of fea(cid:173)
tures would work for many domains.  This includes semantic 
features along with orthographic and syntactic features. 

Before  we  move  on  to  presenting  our  system  for  proba(cid:173)
bilistic  reasoning,  let us discuss  in  some  detail  notation and 
methods we used  in preliminary data processing and  feature 
extraction.  To  use the data efficiently,  we  need to factor the 
text  into  "orthogonal"  features.  Rather  than  working  with 
thousands of listems (generic  words1)  in  the  vocabulary,  and 
combining their features,  we compress the vocabulary by an 
order  of magnitude  by  lemmatisation  or  stemming.  Ortho(cid:173)
graphic and syntactic information is kept in  feature variables 
with just a few values each. 

Tokenization 
Tokenization is the first step of textual data processing.  A to(cid:173)
ken is a minimal part of text which is treated as a unit in sub(cid:173)
sequent steps.  In our case tokenization mostly involves sepa(cid:173)
rating punctuation characters from words. This is particularly 
non-trivial  for  separating  a  period  [Manning  and  Schutze, 
2001] since it requires identifying sentence boundaries. Con(cid:173)
sider  a  sentence:  S p e a k e r:  D r. 
S t e a l s,  C h i ef 
Exec. 

of  r i c h . c o m,  w o r th  $10.5  m i l. 

Lemmatisation 
We have developed a simple lemmatiser which combines out(cid:173)
come of some standard lematisers and stemmers into a look(cid:173)
up  table.  Combined  with  lemmatisation  is  a  step  of spell 

'A word is a sequence of alphabetical characters, which has some 
meaning assigned to it. This would cover words found in general and 
special vocabulary as well abbreviations, proper names and such. 

checking to  catch  misspelled  words.  This  is  done  by  inter(cid:173)
facing with the  UNIX  ispell utility. 
Gazetteer 
Our original  corpus  contains  about  11,000  different  listems. 
This  does  not  take  into  account  tokens  consisting  of punc(cid:173)
tuation  characters,  numbers  and  such.  About  10%  are 
proper  nouns.  The  question  of building  a  vocabulary  auto(cid:173)
matically  was  previously  addressed  in  IE  literature(see  e.g. 
Riloff [1996]).  We use the intersection of two sets.  The first 
set consists of words  encountered  as part of target  fields  and 
in their neighborhood.  The second  set consists of words fre(cid:173)
quently seen in the corpus.  Aside from vocabulary there are 
two reserved values for Out-of-Vocabulary (OoV) words and 
Not-a-Word (NaW). For example see blank slots in the lemma 
row  of Table  1.  The  first  category  encodes  rare  and  unfa(cid:173)
miliar words, which are still identified as words according to 
their part of speech.  The second category is for mixed alpha-
numerical tokens, punctuation and symbolic tokens. 
Syntactic  Categories 
We  used  LTChunk  software  from  U.of  Edinburgh  NLP 
group  [Mikheev et al,  1998].  It produces  47  PoS  tags  from 
UPenn  TreeBank  set  [Marcus  et  ai,  1994].  We  have  clus(cid:173)
tered these into 7 categories:  cardinal numbers (CD), nouns 
(NN), proper nouns (NNP), verbs (VB), punctuation (.), prepo(cid:173)
sition/conjunction  (IN) and other (SYM).  The choice of clus(cid:173)
ters  seriously  influences  the  performance,  while  keeping  all 
47 tags will lead to large CPTs and sparse data. 
Syntactic  Chunking 
Following  Ray&Craven  [2001],  we  obtain  syntactic  seg(cid:173)
ments  (aka  syntactic  chunks)  by  running the Sundance sys(cid:173)
tem  [Riloff,  1996]  and  flattening  the  output  into  four  cate(cid:173)
gories corresponding to noun phrase (NP), verb phrase (VP), 
prepositional  phrase (PP) and  other ( N / A ).  Table  1  shows a 
sample  outcome.  Note  that  both  the  part-of-speech  tagger 
and the syntactic chunker easily get confused by non-standard 
capitalization of a word "Presents" as shown by incorrect la(cid:173)
bels in parenthesis. "Steals" is incorrectly identified as a verb, 
whose subject is "Doctor" and object is "Presents".  Remark(cid:173)
ably, other state-of-the-art syntactic analysis tools [Charniak, 
1999; Ratnaparkhi,  1999] also failed on this problem. 

Capitalization  and  Length 
Simple  features  like  capitalization  and  length  of word  are 
used by many researchers (e.g.  SRV [Freitag and McCallum, 
1999]) Case representation process is straightforward except 
for the choice of number of categories. We found useful intro(cid:173)
ducing an extra category for words which contain both lower 
and upper case  letters (not counting the initial  capital  letter) 
which tend to be abbreviations. 
Semantic  Features 
There are several semantic features which play important role 
in a variety of application domains.  In particular,  it is useful 
to be able to recognize what could be a person's name, geo(cid:173)
graphic  location, various parts of address, etc.  For example, 
we are using a list of secondary location  identifiers provided 
by US postal service, which identifies as such words like hall, 
wing, floor and auditorium.  We also use a list of 100000 most 

INFORMATION  EXTRACTION 

423 

popular names from US census bureau; the list is augmented 
by  rank  which  helps  to  decide  in  favor of first or  last  name 
for cases  like " A l e x a n d e r ".  In general  this task  could  be 
helped by using a hypernym feature of WordNet project [Fell-
baum,  1998].  The next section presents probabilistic model 
which makes use of the aforementioned feature variables. 

Last  target 

3  BIEN 
We convert the  IH  problem  into a classification problem  by 
assuming that each token in the document belongs to a target 
class corresponding to ether one of the target tags or the back(cid:173)
ground (compare  to  Freitag  [1999]).  Furthermore,  it  seems 
important not to ignore the information about interdependen-
cies of target fields and document segments.  To combine ad(cid:173)
vantages  of stochastic  models  with  feature-based reasoning, 
we use a Bayesian network. 

A dynamic Bayesian network (DBN) is ideal for represent(cid:173)
ing probabilistic information about these features.  Just like a 
Bayesian network, it encodes interdependence among various 
features.  In addition, it incorporates the clement of time, like 
an HMM, so that time-dependent patterns ;,uch as common or(cid:173)
ders of fields can be represented. All this is done in a compact 
representation that can be learned from data. We refer to a re(cid:173)
cent dissertation  [Murphy,  2002]  for a good overview  of all 
aspects of Dynamic Bayesian Networks. 

Each document  is considered to be a single  stream of to(cid:173)
kens.  In our DBN, called the Bayesian Information Extraction 
Network (BIEN), the same structure is repeated for every in(cid:173)
dex.  Figure  1  presents the structure of BIEN.  This structure 
contains state variables and feature variables.  The most im(cid:173)
portant state variable, for our purposes, is "Tag" which corre(cid:173)
sponds to information we are trying to extract.  This variable 
classifies each token according to its target information field, 
or has  the  value  "background"  if the  token  does  not  belong 
to  any  field.  "Last Target"  is  another hidden  variable  which 
reflects the order in which target information is found in the 
document.  This variable is our way of implementing a mem(cid:173)
ory in a "memory-less" Markov model.  Its value is determin-
istically  defined  by  the  last  non-background  value  of "Tag" 
variable.  Another hidden variable, "Document Segment",  is 
introduced to account for differences in patterns between the 
header and  the  main  body  of the  document.  The  former  is 
close to the structured text format, while the latter to the free 
text. 
"Document  Segment"  influences  "Tag"  and  together 
these two influence the set of observable variables which rep(cid:173)
resent  features  of the  text  discussed  in  section  2.  Standard 
inference algorithms for DBNs are similar to those for HMMS. 
In a  DBN,  some  of the  variables  will  typically be  observed, 
while others will be hidden.  The typical inference task is to 
determine the probability distribution over the states of a hid(cid:173)
den variable over time, given time series data of the observed 
variables.  This  is  usually  accomplished  using the  forward-
backward  algorithm.  Alternatively,  we might  want to  know 
the most likely sequence of hidden variables.  This is accom(cid:173)
plished using the Viterbi algorithm.  Learning the parameters 
of a DBN from data is accomplished using the EM algorithm 
(see  e.g.  Murphy  [2002]).  Note  that  in  principle,  parts  of 
the system could be trained separately on independent corpus 

Figure  1:  A schematic representation of BIEN. 

to  improve performance.  For example,  one  could  learn  in(cid:173)
dependently  the  conditional  vocabulary  of email/newsgroup 
headers,  or learn  a probability of part-of-speech  conditioned 
on a word, to avoid dependence on external PoS taggers. Also 
prior knowledge about the domain and the language could be 
set in the system this  way.  The  fact that etime almost never 
precedes stime as well as the fact that speaker is never a verb 
could be encoded in a conditional probability table (CPT). In 
large  DBNs,  exact  inference  algorithms  are  intractable,  and 
so  a  variety  of approximate  methods  have  been  developed. 
However,  the  number of hidden  state  variables  in  our model 
is  small  enough to  allow  exact algorithms to work.  Indeed, 
all  hidden  nodes  in  our  model  are  discrete  variables  which 
assume just a few values.  "DocumentSegment"  is  binary  in 
{Header,  Body)  range;  "LastTargct"  has  as many  values  as 
"Tag"—four per number of target fields plus one for the back(cid:173)
ground. 

4  Results 
Several  researchers have reported results on  the CMU  sem(cid:173)
inar announcements corpus,  which  we have chosen in  order 
to have a good basis of comparison.  The CMU seminar an(cid:173)
nouncements  corpus  consists  of 485  documents.  Each  an(cid:173)
nouncement contains some tags  for target slots.  On average 
starting time appears twice per document, while location and 
speaker  1.5  times,  with  up  to  9  speaker  slots  and  4  loca(cid:173)
tion slots per document.  Sometimes multiple instances of the 
same slot differ, e.g.  speaker D r.  S t e a ls  also appears as 
Joe  S t e a l s2.  Ending time, speaker and location are miss(cid:173)
ing from  48%,  16% and  5%  of documents correspondingly. 
In order to demonstrate our method, we have developed a web 
site  which  works  with  arbitrary  seminar  announcement  and 
reveals some semantic tagging.  We also make available a list 
of errors in the original corpus, along with our new derivative 
seminar announcement corpus3. 

2Obtaining 100% performance on the original corpus is impos(cid:173)
sible since some tags are misplaced and in general the corpus is not 
marked uniformly—sometimes secondary occurrences are ignored. 
3The  corpus  and  demo  for  this  paper  are  available  from 
http://www.eecs.harvard.edu/~pesha/papers.html 

424 

INFORMATION  EXTRACTION 

Table 2:  Fl  performance measure for various IE systems. 

is  calculated 

in 

and 

by  precision 

The  performance 

the  usual  way, 
recall 
combined  into  F  measure  geo(cid:173)
metrical  average 
We  report  results  using 
the  same  ten-fold  cross  validation  test  as  other  publi(cid:173)
cations  concerning  this  data  set  [Roth  and  Yih,  2001; 
Ciravegna,  2001].  The  data  is  split  randomly  into  training 
and  testing  set.  The  reported  results  are  averaged over  five 
runs.  Table 2 presents a comparison with numerous previous 
attempts at the CMU  seminar corpus.  The figures are taken 
from Roth and Yih [2001 J.  BIEN performs comparably to the 
best  system  in  each  category,  while  notably  outperforming 
other  systems  in  finding  location.  This  is  partly  due  to  the 
"LastTarget"  variable.  "LastTargct"  variable turns out to be 
generally useful.  Here is the learned conditional probability 
table  (CPT)  for  P(Targct\LastTaryet)9  where  the  element 
(/, J)  corresponds to the probability to get target tag .7 after 
target  tag  /  was  seen.  We  learn  that  initial  tag  is stime  or 
speaker with 2:1  likelihood ratio; etime is naturally the most 
likely  follower to stime and in turn forecasts location. 

Other variables turn out to be  useless, e.g.  the number of 
characters does not add anything to the performance, and nei(cid:173)
ther  does  the  initially  introduced  "SeenTag"  variable  which 
kept  track  of all  tags  seen  up  to  the  current  position.  Ta(cid:173)
ble  3  presents  performance  of BIEN  with  various  individual 
features turned  off.  Note that figures  for complete  BIEN  are 

Table 3:  Fl  performance comparison across implementations 
of BIEN  with disabled features. 

Figure 2:  A learning curve for precision and recall with grow(cid:173)
ing training sample size. 

slightly better than in Table 2 since we pushed the fraction of 
the training data to the maximum.  Capitalization helps iden(cid:173)
tify  location  and  speaker,  while  losing  it  does  not  damage 
performance drastically.  Although information is reflected in 
syntactic and semantic features, most names in documents do 
not identify a speaker. One would hope to capture all relevant 
information  by  syntactic  and  semantic  categories,  however 
BIEN does not fare well without observing "Lemma".  Losing 
the semantic feature seriously undermines performance in lo(cid:173)
cation and speaker categories- -  ability to recognize names is 
rather valuable for many domains. 

Reported  figures  are  based  on  80%-20%  split  of the  cor(cid:173)
pus.  Increasing  the  size  of training  corpus  did  not  dramati(cid:173)
cally improve the performance in terms of F measure, as fur(cid:173)
ther illustrated by Figure 2, which presents a learning curve— 
precision and recall averaged over all fields, as a function of 
training data fraction.  Trained on a small sample, BIEN acts 
very  conservatively  rarely  picking  fields,  therefore  scoring 
high precision and poor recall.  Having seen hundreds of tar(cid:173)
get field instances and tens of thousands of negative samples, 
BIEN  learns  to  generalize,  which  leads  to  generous  tagging 
i.e.  lower precision and higher recall. 

So  far  we  provide  results  obtained  on  the  original  CMU 
seminar  announcements  data,  which  is  not  very  challeng(cid:173)
ing.  Most documents contain the header section with all  the 
target  fields  easily  identifiable  right  after  the  corresponding 
key  word.  We  have  created  a  derivative  dataset  in  which 
documents  are  stripped  of headers  and  two  extra  fields  are 
sought:  date and topic.  Indeed this corpus turned out to be 
more difficult, with our current set of features we obtain only 
64% performance on speaker and 68% performance on topic. 
Date does not present a challenge except for cases of regular 
weekly events or relative dates like "tomorrow".  Admittedly, 
the bootstrapping test performance is not a guarantee of sys(cid:173)
tems performance on novel data since preliminary processing, 
i.e.  tokenization and gazetteering, as well as choice for PoS 
tag set, lead to a strong bias towards the training corpus. 

INFORMATION  EXTRACTION 

425 

5  Discussion 
We  have  described  how  to  integrate  various  aspects  of lan(cid:173)
guage  into  a  single  probabilistic  model,  and  to  incremen(cid:173)
tally build a robust IE system based on a Bayesian network. 
Currently,  we are working on  learning the structure of BIEN 
automatically. 
It  seems  to  subject  itself nicely  to  structural 
KM [Friedman,  1998; Murphy, 2002].  The first step is auto(cid:173)
matic selection of relevant features.  Another direction of cur(cid:173)
rent work is using approximate inference.  We have tried LBP 
(Loopy-belief Propagation)  [Murphy  et  aL,  1999;  Murphy, 
2002],  but for the  current structure  of BIEN  it seems to  give 
no gain.  More challenging applications which require larger, 
stronger connected  networks,  will  benefit  from  approximate 
inference  algorithms.  It  will  enable  quick  on-line  inference 
on the  network  learned off-line with exact methods,  as well 
as learning for cases where exact inference is infeasible.  One 
such  network  will  result  from  integrating  a  PoS  tagger and 
other  feature  extractors  into  BIEN.  This  is  a natural  exten(cid:173)
sion of BIEN since various text processing routines are mutu(cid:173)
ally dependent.  Consider for example PoS tagging, sentence 
boundary detection and named entities recognition.  Another 
complex  BIEN  structure  will  result  if we  try to  better  reflect 
complex  relational  information  [Califif and  Mooney,  1999; 
Roth and Yih, 2001; 2002] e.g.  to process cases like seminar 
cancellations and rescheduling; and handle multi-slot extrac(cid:173)
tion,  e.g.  multiple  seminar announcements  and  conference 
schedules. 

Acknowledgments 
Kevin Murphy provided BNT, Kobi Gal helped to handle the 
corpus, anonymous reviewers gave helpful feedback. 

References 
[Califfand Mooney,  1999]  Mary  Elaine  Califf  and  Ray(cid:173)
mond  J.  Mooney.  Relational  learning  of pattern-match 
In  Proceedings  of the 
rules  for  information  extraction. 
Sixteenth  National  Conf.  on  Artificial  Intelligence,  pages 
328-334, 1999. 

[Charniak,  1999]  Eugene  Charniak.  A  maximum-entropy-
inspired parser.  Technical  Report CS-99-12, Brown Uni(cid:173)
versity, 1999. 

[Ciravegna, 2001]  F.  Ciravegna.  Adaptive  information  ex(cid:173)
traction from text by rule induction and generalisation.  In 
Proceedings  of the  Seventeenth  International Joint  Conf. 
on  Artificial Intelligence,  2001. 

[Fellbaum,  1998]  Christiane Fellbaum, editor.  WordNet: An 

Electronic Lexical Database.  MIT Press,  1998. 

[Freitag and McCallum,  1999]  Dayne  Freitag  and  Andrew 
Information  extraction  with  HMMs  and 
In  Proceedings  of the AAAI-99  Workshop  on 

McCallum. 
shrinkage. 
Machine  Learning  for  Information  Extraction,  1999. 

[Freitag,  1998]  Dayne  Freitag.  Machine  Learning for  In(cid:173)
formation  Extraction  in  Informal  Domains.  PhD  thesis, 
Carnegie Mellon University,  1998. 

[Friedman,  1998]  Nir Friedman.  The bayesian structural EM 
algorithm. In Proceedings of the Fourteenth Conf on Un(cid:173)
certainty  in  Artificial  Intelligence,  1998. 

[Manning and Schutze, 2001]  Christopher  D.  Manning  and 
Hinrich  Schutze.  Foundations  of Statistical  Natural  Lan(cid:173)
guage Processing. MIT Press, 2001. 

[Marcus et al,  1994]  M.  Marcus,  Beatrice  Santorini,  and 
M.A.  Marcinkiewicz.  Building  a  large  annotated  corpus 
of English:  The  Penn  treebank.  Computational  Linguis(cid:173)
tics,  19(2):313-330,  1994. 

[McCallum et aL, 2000]  Andrew McCallum, Dayne Freitag, 
and Fernando Pereira.  Maximum entropy Markov models 
for information extraction and segmentation.  In Proceed(cid:173)
ings  of the  Seventeenth  International  Conf  on  Machine 
Learning, pages 591-598, Stanford, CA, 2000. 

[Mikheev^/a/.,  1998]  Andrei  Mikheev,  Claire  Grover,  and 
Marc  Moens.  Description  of the  LTG  system  used  for 
MUC-7.  In Seventh Message Understanding Conference 
(MUC-7),  1998. 

[Murphy et  al,  1999]  Kevin  Murphy,  Yair  Weiss,  and 
Michael  Jordan.  Loopy-belief propagation  for  approxi(cid:173)
In  Proceedings  of the Fifteenth  Conf  on 
mate  inference. 
Uncertainty  in  Artificial  Intelligence,  1999. 

[Murphy, 2002]  Kevin  Murphy.  Dynamic  Bayesian  Net(cid:173)
works:  Representation,  Inference and Learning.  PhD the(cid:173)
sis, UC Berkeley, 2002. 

[Musteaet aL, 2001]  Ion  Muslea,  Steven  Minton,  and 
Craig  A.  Knoblock.  Hierarchical  wrapper  induction  for 
semistructured  information  sources.  Autonomous Agents 
and Multi-Agent Systems, 4( 1 /2):93-l 14, 2001. 

[Ratnaparkhi,  1999]  Adwait Ratnaparkhi.  Learning to parse 
natural language with maximum entropy models.  Machine 
Learning,  34(\-3):\5l-175,  1999. 

[Ray and Craven, 2001]  Souyma  Ray  and  Mark  Craven. 
Representing sentence structure in hidden Markov models 
for information  extraction.  In Proceedings of the Seven(cid:173)
teenth  International  Joint  Conf  on  Artificial  Intelligence, 
2001. 

[Riloff,  1996]  Ellen  Riloff.  An empirical study of automated 
dictionary construction for information extraction in three 
domains.  Artificial Intelligence,  85(1-2): 101-134,  1996. 
[Roth and Yih, 2001]  Dan Roth and Wen-tau Yih. Relational 
learning via propositional algorithms:  An information ex(cid:173)
traction case study. In Proceedings of the Seventeenth In(cid:173)
ternational Joint  Conf  on  Artificial  Intelligence,  2001. 

[Roth and Yih, 2002]  Dan  Roth  and  Wen-tau  Yih.  Proba(cid:173)
bilistic reasoning for entity & relation recognition.  In The 
20th  International  Conference  on  Computational Linguis(cid:173)
tics, 2002. 

[Sodedand,  1999]  Stephen Soderland.  Learning information 
extraction rules for semi-structured and free text. Machine 
Learning,  34(\-3):233-272,  1999. 

426 

INFORMATION  EXTRACTION 


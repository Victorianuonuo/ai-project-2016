Optimal Nonmyopic Value of Information in Graphical Models –

Efﬁcient Algorithms and Theoretical Limits

Andreas Krause

Carnegie Mellon University

Carlos Guestrin

Carnegie Mellon University

Abstract

Many real-world decision making tasks require us to
choose among several expensive observations. In a sen-
sor network, for example, it is important to select the sub-
set of sensors that is expected to provide the strongest re-
duction in uncertainty. It has been general practice to use
heuristic-guided procedures for selecting observations. In
this paper, we present the ﬁrst efﬁcient optimal algorithms
for selecting observations for a class of graphical models
containing Hidden Markov Models (HMMs). We provide
results for both selecting the optimal subset of observa-
tions, and for obtaining an optimal conditional observation
plan. For both problems, we present algorithms for the ﬁl-
tering case, where only observations made in the past are
taken into account, and the smoothing case, where all ob-
servations are utilized. Furthermore we prove a surpris-
ing result: In most graphical models tasks, if one designs
an efﬁcient algorithm for chain graphs, such as HMMs,
this procedure can be generalized to polytrees. We prove
that the value of information problem is NPPP-hard even
for discrete polytrees. It also follows from our results that
even computing conditional entropies, which are widely
used to measure value of information, is a #P-complete
problem on polytrees. Finally, we demonstrate the effec-
tiveness of our approach on several real-world datasets.

Introduction

1
In probabilistic reasoning, where one can choose among sev-
eral possible but expensive observations, it is often a central
issue to decide which variables to observe in order to most ef-
fectively increase the expected utility. In a medical expert sys-
tem [14], for example, multiple tests are available, and each
test has a different cost. In such systems, it is thus important to
decide which tests to perform in order to become most certain
about the patient’s condition, at a minimum cost. Occasionally,
the cost of testing can even exceed the value of information for
any possible outcome.

The following running example motivates our research and
is empirically evaluated in Section 7. Consider a temperature
monitoring task, where wireless temperature sensors are dis-
tributed across a building. The task is to become most certain
about the temperature distribution, whilst minimizing energy
expenditure, a critically constrained resource [4].

Many researchers have suggested the use of myopic
(greedy) approaches to select observations [13; 15; 5; 1]. Un-
fortunately, this heuristic does not provide any performance

guarantees.
In this paper, we present efﬁcient algorithms,
which guarantee optimal nonmyopic value of information
in chain graphical models such as Hidden Markov Models
(HMMs). We address two settings: subset selection, where
the optimal subset of observations is obtained in an open-loop
fashion, and conditional plans, a closed-loop plan where the
observation strategy depends on the actual value of the ob-
served variables (c.f. Fig. 1(a)). To our knowledge, these are
the ﬁrst optimal and efﬁcient algorithms for these tasks for this
class of graphical models. For both settings, we address the
ﬁltering and the smoothing versions: Filtering is important in
online decision making, where our decisions can only utilize
observations made in the past. Smoothing arises for example
in structured classiﬁcation tasks, where there is no temporal
dimension in the data, and hence all observations can be taken
into account. We evaluate our algorithms empirically on three
real-world datasets, and also show that they are well-suited for
interactive classiﬁcation of sequential data.

Most problems in graphical models, such as probabilistic in-
ference and the most probable explanation, that can be solved
efﬁciently for chain-structured graphs, can also be solved efﬁ-
ciently for polytrees. We prove that the problem of maximizing
value of information is NPPP-hard even for discrete polytree
graphical models, giving a complexity theoretic classiﬁcation
of a core artiﬁcial intelligence problem. NPPP-hard prob-
lems are believed to be signiﬁcantly harder than NP-complete
or even #P-complete problems commonly arising in the con-
text of graphical models. As a special case, we also prove that
computing conditional entropies is #P-complete even in the
case of discrete polytrees. This is a surprising result about a
measure of uncertainty that is frequently used in practice.

2 Optimization criteria
In order to maximize value of information, our objective func-
tions should depend on probability distributions over variables.
Let S = {X1, . . . , Xn} be a set of discrete random variables.
We consider a class of local reward functions R, which are de-
ﬁned on the marginal probability distributions of the variables.
This class has the computational advantage that local rewards
can be evaluated using probabilistic inference techniques. The
total reward will then be the sum of all local rewards.
Let O be a subset of S. Then P (Xj | O = o) denotes the
marginal distribution of variable Xj conditioned on observa-
tions o. For classiﬁcation purposes, it can be more appropriate
to consider the max-marginals P max(Xj = xj | O = o) =
maxx P (X = x, Xj = xj | O = o), that is, for Xj set to value

xj, the probability of the most probable assignment to all other
random variables conditioned on the observations o. The local
reward Rj is a functional on the probability distribution P or
P max over Xj. We write

Rj(Xj | O) ,X

P (O = o)Rj(P (Xj | O = o))

o

as an abbreviation to indicate expected local rewards, where
the expectation is taken over all assignments o to the observa-
P
tions O. Important measures for value of information include:
• Entropy. If we set Rj(P (Xj | O)) = −H(Xj | O) =
xj ,o P (xj, o) log P (xj | o), the objective in the opti-
mization problem becomes to minimize the sum of resid-
ual entropies. We choose this reward function in our run-
ning example to measure the uncertainty about the tem-
perature distribution.
• Maximum expected utility. The concept of local reward
functions also includes the concept of utility nodes in in-
ﬂuence diagrams. If Uj : Aj × dom Xj → R is a utility
function mapping an action a ∈ Aj and an outcome x ∈
dom Xj to a reward, then the maximum expected utility
principle states that actions should be selected as to max-
x P (x | o)Uj(a, x). The more
certain we are about Xj, the more economically we can
choose our action. Hence we can deﬁne our local reward
o P (o) maxa EUj(a | o).
• Margin. We can also consider the margin of conﬁ-
|
o P (o)[P max(x∗
dence: Rj(P max(Xj
o)−P max(¯xj | o)], where x∗ = argmaxxj P max(xj | o)
and ¯x = argmaxxj6=x∗ P max(xj | o), which describes
the margin between the most likely outcome and the clos-
est runner up. This reward function is very useful for
structured classiﬁcation purposes, as shown in Sec. 7.

imize EUj(a | o) = P
function R(P (Xj | O)) =P

| O)) = P

j

These examples demonstrate the generality of our notion of lo-
cal reward. One can generalize the algorithms even more, e.g.,
to measure the total entropy or the margin between the most
probable explanation and its runner up. Details are omitted
here due to space limitations.

We also want to capture the constraint that observations are
expensive. This can mean that each observation Xj has an
associated positive penalty Cj that effectively decreases the
reward.
In our example, we might be interested in trading
off accuracy with sensing energy expenditure. Alternatively,
it is also possible to deﬁne a budget B for selecting observa-
tions, where each one is associated with an integer cost βj.
Here, we want to select observations whose sum cost is within
the budget, but these costs do not decrease the reward.
In
our running example, the sensors could be powered by solar
power, and regain a certain amount of energy per day, which
allows a certain amount of sensing. Our formulation of the
optimization problems allows both for penalties and budgets.
Xj∈O Cj and

To simplify notation we also write C(O) = P
β(O) =P

Xj∈O βj to extend C and β to sets.

3 Decomposing Rewards
In the following Sections 4 and 5, we present efﬁcient algo-
rithms for two problems of optimizing value of information in
the class of chain graphical models.

Figure 1: Example, and decomposing reward idea

(a) Example conditional plan.
(b) Decomposition of the reward.
The set of random variables S = {X1, . . . , Xn} forms a
chain graphical model (a chain), if Xi is conditionally inde-
pendent of Xk given Xj whenever i < j < k. We can assume
that the joint distribution is speciﬁed by the prior P (X1) and
conditional probability distributions P (Xi+1 | Xi). The time
series model for the temperature measured by one sensor in
our example can be formulated as a chain graphical model.

Chain graphical models originating from time series have
additional, speciﬁc properties: In a system for online decision
making, only observations from the past and present time steps
can be taken into account, not observations which will be made
in the future. This is in general referred to as the ﬁltering prob-
lem. In this setting, the notation P (Xi | O) will refer to the
distribution of Xi conditional on observations in O prior to and
including time i. For structured classiﬁcation problems as dis-
cussed in Section 7, in general observations made anywhere in
the chain must be taken into account. This situation is usually
referred to as the smoothing problem. We will provide algo-
rithms both for ﬁltering and smoothing.

We will now describe the key insight, which allows for ef-
ﬁcient optimization in chains. Consider a set of observations
O ⊆ S. If the j variable is observed, i.e., Xj ∈ O, then the
local reward is simply R(Xj | O) = R(Xj | Xj). Now
consider Xj /∈ O, and let Oj be the subset of O contain-
ing the closest ancestor (and for the smoothing problem also
the closest descendant) of Xj in O. The conditional inde-
pendence property of the graphical model implies that, given
Oj, Xj is independent of the rest of the observed variables,
| O) = P (Xj
| Oj). Thus, it follows that
i.e., P (Xj
R(Xj | O) = R(Xj | Oj).

These observations imply that the expected reward of some
set of observations decomposes along the chain. For simplicity
of notation, we add two independent dummy variables X0 and
Xn+1, where R0 = C0 = β0 = Rn+1 = Cn+1 = βn+1 = 0.
P
Let O = {Xi0, . . . , Xim+1} where il < il+1, i0 = 0 and
im+1 = n + 1. Using this notation, the total reward R(O) =
j Rj(Xj | O) for the smoothing case is given by:
mX

Riv(Xiv | Xiv) − Civ +

Rj(Xj | Xiv , Xiv+1)

iv+1−1X

 .

v=0

j=iv+1

In ﬁltering settings, we simply replace Rj(Xj | Xiv , Xiv+1)
by Rj(Xj | Xiv). Figure 1(b) illustrates this decomposition.
Consider now a Hidden Markov Model unrolled for n time
steps,
i.e., S can be partitioned into the hidden variables
{X1, . . . , Xn} and the emission variables {Y1, . . . , Yn}.
In
HMMs, the Yi are observed and the variables Xi form a chain.
In many applications, some of which are discussed in Sec-
tion 7, we can observe some of the hidden variables, e.g., by
asking an expert, in addition to observing the emission vari-
ables. In these cases, the problem of selecting expert labels
also belongs to the class of chain graphical models addressed
by this paper.

Tmorn=high?Tnoon=high?Teve=high?yesnonX

4 Subset Selection
In the subset selection problem, we want to ﬁnd a most in-
formative subset of the variables to observe in advance, i.e.,
before any observations are made. In our running example, we
would, before deploying the sensors, identify k time points that
are expected to provide the most informative sensor readings
according to our model.

First, deﬁne the objective function L on subsets of S by

L(O) =

Rj(Xj | O) − C(O).

(4.1)

The subset selection problem is to ﬁnd the optimal subset

j=1

O∗ = argmax
O⊆S,β(O)≤B

L(O)

maximizing the sum of expected local rewards minus the
penalties, subject to the constraint that the total cost must not
exceed the budget B.

We solve this optimization problem using a dynamic pro-
gramming algorithm, where the chain is broken into sub-chains
using the insight from Sec. 3. Consider a sub-chain from vari-
able Xa to Xb. We deﬁne La:b(k) to represent the expected
total reward for the sub-chain Xa, . . . , Xb, where Xa (and Xb
in the smoothing case) is observed, and with a budget level of
k. More formally:

La:b(k) =

O⊂{Xa+1...Xb−1}

max
β(O)≤k

b−1

Xj=a+1

for the ﬁltering version, and

Rj(Xj | O ∪ {Xa}) − C(O),

La:b(k) =

O⊂{Xa+1...Xb−1}

max
β(O)≤k

b−1

Xj=a+1

Rj(Xj | O∪{Xa, Xb})−C(O),

the smoothing version.

Note that L0:n+1(B) =
for
maxO:β(O)≤B L(O), as in Eq. (4.1), i.e., by computing the
values for La:b(k), we compute the maximum expected total
reward for the entire chain.

We can compute La:b(k) using dynamic programming. The

base case is simply:

b−1X
b−1X

j=a+1

j=a+1

La:b(0) =

Rj(Xj | Xa),

for ﬁltering, and

La:b(0) =

Rj(Xj | Xa, Xb),

for smoothing. The recursion for La:b(k) has two cases: we
can choose not to spend any more of the budget, reaching the
base case, or we can break the chain into two sub-chains, se-
lecting the optimal observation Xj, where a < j < b:

La:b(k) = max{La:b(0),

max

j:a<j<b,βj≤k

{−Cj+

+ Rj(Xj | Xj) + La:j(0) + Lj:b(k − βj)}}.

Input: Budget B, rewards Rj, costs βj and penalties Cj
Output: Optimal selection O of observation times
begin
for 0 ≤ a < b ≤ n + 1 do compute La:b(0);
for k = 1 to B do

for 0 ≤ a < b ≤ n + 1 do
sel(−1) := La:b(0);
for j = a + 1 to b − 1 do sel(j) :=
−Cj + Rj(Xj | Xj) + La:j(0) + Lj:b(k − βj);
La:b(k) = maxa<j<b sel(j);
Λa:b(k) = argmaxa<j<b sel(j);

end

end
a := 0; b := n + 1; k := B; O := ∅;
repeat

j := Λa:b(k);
if j ≥ 0 then O := O ∪ {Xj}; k := k − βj;

until j = −1;

end

Algorithm 1: Optimal subset selection.

since the subset problem is open-loop and the order of the ob-
servations is irrelevant, we only need to consider split points
where the ﬁrst sub-chain receives zero budget.

A pseudo code implementation is given in Alg. 1. If we do
not consider different costs β, we would simply choose βj = 1
for all variables and compute La:b(N). Alg. 1 uses the quanti-
ties Λa:b to recover the optimal subset by tracing the maximal
values occurring in the dynamic programming equations. Us-
ing an induction proof, we obtain:
Theorem 1. The dynamic programming algorithm described
above computes the optimal subset with budget B in ( 1
6 n3 +
O(n2))B evaluations of expected local rewards.

If the variables Xi are continuous, our algorithm is still ap-
plicable when the integrations and inferences necessary for
computing the expected rewards can be performed efﬁciently.

5 Conditional Plan
In the conditional plan problem, we want to compute an opti-
mal querying policy: We sequentially observe a variable, pay
the penalty, and depending on the observed values, select the
next query as long as our budget sufﬁces. The objective is
to ﬁnd the plan with the highest expected reward, where, for
each possible sequence of observations, the budget B is not
exceeded. For ﬁltering, we can only select observations in the
future, whereas in the smoothing case, the next observation can
be anywhere in the chain. In our running example, the ﬁlter-
ing algorithm would be most appropriate: The sensors would
sequentially follow the conditional plan, deciding on the most
informative times to sense based on the previous observations.
Fig. 1(a) shows an example of such a conditional plan.

The formal deﬁnition of the objective function J is given

recursively. The base case considers the exhausted budget:

J(O = o; 0) = X

Xj∈S

Rj(Xj | O = o) − C(O).

At ﬁrst, it may seem that this recursion should consider the op-
timal split of the budget between the two sub-chains. However,

The recursion, J(O = o; k), represents the maximum expected
reward of the conditional plan for the chain where O = o has

been observed and the budget is limited to k:
J(O = o; k) = max{J(O = o; 0), max
{
Xj /∈O

P (Xj = y | O = o) · J(O = o, Xj = y; k − βj)}}.

X

y

The optimal plan has reward J(∅; B).

We propose a dynamic programming algorithm for obtain-
ing the optimal conditional plan that is similar to the subset
algorithm presented in Sec. 4. Again, we utilize the decompo-
sition of rewards described in Section 3. The difference here
is that the observation selection and budget allocation now de-
pend on the actual values of the observations.

We again consider sub-chains Xa, . . . , Xb. The base case

deals with the zero budget setting:

Ja:b(xa; 0) =

Rj(Xj | Xa = xa),

b−1

Xj=a+1

for ﬁltering, and

Ja:b(xa, xb; 0) =

b−1

Xj=a+1

Rj(Xj | Xa = xa, Xb = xb),

The

recursion

smoothing.

deﬁnes Ja:b(xa; k)
for
(Ja:b(xa, xb; k) for smoothing),
the expected reward for
the problem restricted to the sub-chain Xa, . . . , Xb condi-
tioned on the values of Xa (and Xb for smoothing), and with
budget limited by k. To compute this quantity, we again iterate
through possible split points j, such that a < j < b. Here
we observe a notable difference between the ﬁltering and
the smoothing case. For smoothing, we now must consider
all possible splits of the budget between the two resulting
sub-chains, since an observation at time j might require us to
make an additional, earlier observation:
Ja:b(xa, xb; k) = max{Ja:b(xa, xb; 0), max

{−Cj+

a<j<b

P (Xj = xj | Xa = xa, Xb = xb){Rj(Xj | Xj)+

Looking back in time is not possible in the ﬁltering case, hence
the recursion simpliﬁes to

Ja:b(xa; k) = max{Ja:b(xa; 0), max

{−Cj+
P (Xj = xj | Xa = xa){Rj(Xj | Xj)+

a<j<b:βj≤k

Xxj
Ja:j(xa; 0) + Jj:b(xj; k − βj)}}}.

The optimal reward is obtained by J0:n+1(∅; B) = J(∅; B).
Alg. 5 presents a pseudo code implementation for the smooth-
ing version – the ﬁltering case is a straight-forward modiﬁca-
tion. The plan itself is compactly encoded in the quantities πa:b
which determines the next variable to query and σa:b, which
determines the allocation of the budget. Considering the ex-
ponential number of possible sequences of observations, it is
remarkable that the optimal plan can even be represented us-
ing only polynomial space. Alg. 5 indicates how the computed
plan can be executed. The procedure is recursive, requiring the
parameters a := 0, xa := 1, b := n + 1, xb := 1 and k := B
for the initial call. Again, by induction, we obtain:

Xxj

max

0≤l≤k−βj

[Ja:j(xa, xj; l) + Jj:b(xj, xb; k − l − βj)]}}}.

j := πa:b(xa, xb; k);
if j ≥ 0 then

Theorem 2. The algorithm for smoothing presented above
6 n3+O(n2))
computes an optimal conditional plan in d3·B2·( 1
evaluations of local rewards, where d is the maximum domain
size of the random variables X1, . . . , Xn. In the ﬁltering case,
or if no budget is used, the optimal plan can be computed us-
6 n4 + O(n3)) evaluations
ing d3 · B · ( 1
respectively.

6 n3 + O(n2)) or d3 · ( 1

The faster computation for the no budget case is obtained by
observing that we do not require the third maximum computa-
tion, which distributes the budget into the sub-chains.

Input: Budget B, rewards Rj, costs βj and penalties Cj
Output: Optimal conditional plan (πa:b, σa:b)
begin

for 0 ≤ a < b≤ n + 1, xa ∈ dom Xa, xb ∈ dom Xb do
compute Ja:b(xa, xb; 0);
for k = 1 to B do

for 0≤ a < b≤ n+1, xa∈dom Xa, xb∈dom Xb do

sel(−1) := Ja:b(0);
for a < j < b do
sel(j) := −Cj + Rj(Xj | Xj);
for a < j < b, xj ∈ dom Xj do

for 0 ≤ l ≤ k − βj do ;
bd(l) :=
Ja:j(xa, xj; l) + Jj:b(xj, xb; k − l − βj);
sel(j) := sel(j) + P (xj | xa, xb) · maxl bd(j);
σ(j, xj) = argmaxl bd(j);

end
Ja:b(k) = maxa<j<b sel(j);
πa:b(xa, xb; k) = argmaxa<j<b sel(j);
for xj ∈ dom Xπa:b(k) do
σa:b(xa, xb, xj; k) = σ(πa:b(k), xj);

end

end

end

Algorithm 2: Computation of optimal conditional plan.

Input: Budget k, observations Xa = xa, Xb = xb, σ, π
begin

Observe Xj = xj;
l := σa:b(xa, xb, xj; k);
Recurse with k := l, a := a, b := j;
Recurse with k := k − l − βj, a := j, b := b;

end

end
Algorithm 3: Observation selection using conditional plan.

6 Theoretical Limits
Many problems that can be solved efﬁciently for discrete chain
graphical models can also be efﬁciently solved for discrete
polytrees. Examples include probabilistic inference and the
most probable explanation (MPE). Surprisingly, we prove that
for the optimization problems discussed in this paper, this gen-
eralization is not possible, unless P = NP. All proofs in this
section are stated in the Appendix.

In order to solve the optimization problems, we will most
likely have to evaluate the objective function, i.e., the expected
local rewards. Our ﬁrst result states that this problem is in-
tractable even for discrete polytrees.

(a) Temperature data: Improvement over
the uniform spacing heuristic.

(b) CpG island data set: Effect of increas-
ing the number of observations on margin
and classiﬁcation accuracy.

(c) Part-of-Speech tagging data set: Ef-
fect of increasing the number of observa-
tions on margin and F1 score.

Figure 2: Experimental results.

Theorem 3. The computation of expected local rewards for
discrete polytrees is #P-complete.1

This negative result can be specialized to the conditional en-
tropy, one of the most frequently used reward function to char-
acterize the residual uncertainty in value of information prob-
lems.
Corollary 4. The computation of conditional entropy for dis-
crete polytrees is #P-complete.

Since evaluating local rewards is #P-complete, it can be
suspected that the subset selection problem is at least #P-
hard. We show that it is even NPPP-complete2, a complexity
class containing problems that are believed to be signiﬁcantly
harder than NP or #P complete problems. This result pro-
vides a complexity theoretic classiﬁcation of value of informa-
tion, a core AI problem.
Theorem 5. Subset Selection is NPPP-complete even for dis-
crete polytrees.

For our running example, this implies that the generalized
problem of optimally selecting k sensors from a network of
correlated sensors is most likely computationally intractable
without resorting to heuristics. A corollary extends the hard-
ness of subset selection to the hardness of conditional plans.
Corollary 6. Computing conditional plans is NPPP-hard
even for discrete polytrees.

7 Experiments
In this section, we evaluate the proposed methods for several
real world data sets. A special focus is on the comparison of
the optimal methods with the greedy heuristic and other heuris-
tic methods for selecting observations, and on how the algo-
rithms can be used for interactive structured classiﬁcation.

7.1 Temperature time series
The ﬁrst data set consists of temperature time series collected
from a sensor network deployed at Intel Research Berkeley [4]
as described in our running example. Data was continuously
collected for 19 days, linear interpolation was used in case of
missing samples. The temperature was measured once every

1#P contains problems such as counting the number of satisfying

assignments to a Boolean formula.

2NPPP is natural for AI planning problems [9]. A complete
problem is EM AJSAT , where one has to ﬁnd an assignment to the
ﬁrst k variables of a 3CNF formula, such that the formula is satisﬁed
under the majority of assignments to the remaining variables.

60 minutes, and it was discretized into 10 bins of 2 degrees
Kelvin. To avoid overﬁtting, we used pseudo counts α = 0.5
when learning the model. Using parameter sharing, we learned
four sets of transition probabilities: from 12 am - 7am, 7 am -
12 pm, 12 pm - 7 pm and 7 pm - 12 am. Combining the data
from three adjacent sensors, we got 53 sample time series.

The goal of this task was to select k out of 24 time points
during the day, during which sensor readings are most infor-
mative. The experiment was designed to compare the perfor-
mance of the optimal algorithms, the greedy heuristic, and a
uniform spacing heuristic, which distributed the k observations
uniformly over the day. Fig. 2(a) shows the relative improve-
ment of the optimal algorithms and the greedy heuristic over
the uniform spacing heuristic. The performance is measured
in decrease of expected entropy, with zero observations as the
baseline. It can be seen that if k is less than about the half of
all possible observations, the optimal algorithms decreased the
expected uncertainty by several percent over both heuristics.
The improvement gained by the optimal plan over the subset
selection algorithms appears to become more drastic if a large
number of observations (over half of all possible observations)
is allowed. Furthermore, for a large number of observations,
the optimal subset and the subset selected by the greedy heuris-
tic were almost identical.

7.2 CpG-Island detection
We then studied the bioinformatics problem of ﬁnding CpG
islands in DNA sequences. CpG islands are regions in the
genome with a high concentration of the cytosine-guanine se-
quence. These areas are believed to be mainly located around
the promoters of genes, which are frequently expressed in
the cell.
In our experiment, we considered the gene loci
HS381K22, AF047825 and AL133174, for which the Gen-
Bank annotation listed three, two and one CpG islands each.
We ran our algorithm on a 50 base window at the beginning
and end of each island, using the transition and emission prob-
abilities from [6] for our Hidden Markov Model, and we used
the sum of margins as reward function.

The goal of this experiment was to locate the beginning and
ending of the CpG islands more precisely by asking experts,
whether or not certain bases belong to the CpG region or not.
Fig. 2(b) shows the mean classiﬁcation accuracy and mean
margin scores for an increasing number of observations. The
results indicate that, although the expected margin scores are
similar for the optimal algorithm and the greedy heuristic, the
mean classiﬁcation performance of the optimal algorithm was
still better than the performance of the greedy heuristic.

148121620240246810Number of observationsPercent improvementOptimal conditional planOptimal subsetGreedy heuristic1234560.60.70.80.91Number of observationsMean margin of optimal subsetMean margin of greedy heuristicMean accuracy ofoptimal subsetMean accuracy ofgreedy heuristic010203040500.860.880.90.920.940.960.981Number of observationsMean marginMean F1 score7.3 Part-of-Speech Tagging
In our third experiment, we investigated the structured classi-
ﬁcation task of part-of-speech (POS) tagging [3]. Problem in-
stances are sequences of words (sentences), where each word
is part of an entity (e.g., “United States of America”), and each
entity belongs to one of ﬁve categories: Location, Miscella-
neous, Organization, Person or Other. Imagine an application,
where automatic information extraction is guided by an expert:
Our algorithms compute an optimal conditional plan for asking
the expert, trying to optimize classiﬁcation performance while
requiring as little expert interaction as possible.

We used a conditional random ﬁeld for the structured clas-
siﬁcation task, where each node corresponds to a word, and
the joint distribution is described by node potentials and edge
potentials. The sum of margins was used as reward function.
Measure of classiﬁcation performance was the F1 score, the
geometric mean of precision and recall. The goal of this ex-
periment was to analyze how the addition of expert labels in-
creases the classiﬁcation performance, and how the indirect,
decomposing reward function used in our algorithms corre-
sponds to real world classiﬁcation performance.

Figure 2(c) shows the increase of the mean expected mar-
gin and F1 score for an increasing number of observations,
summarized over ten 50 word sequences. It can be seen that
the classiﬁcation performance can be effectively enhanced by
optimally incorporating expert labels. Requesting only three
out of 50 labels increased the mean F1 score from by more
than ﬁve percent. The following example illustrates this ef-
fect: In one scenario both words of an entity, the sportsman
‘P. Simmons’, were classiﬁed incorrectly – ‘P.’ as Other and
‘Simmons’ as Miscellaneous. The ﬁrst request of the optimal
conditional plan was to label ‘Simmons’. Upon labeling this
word correctly, the word ‘P.’ was automatically labeled cor-
rectly also, resulting in an F1 score of 100 percent.

8 Related Work
Decision Trees [12] popularized the value of information as a
criterion for creating conditional plans. Unfortunately, there
are no guarantees on the performance of this greedy method.
Bayer-Zubek [1] proposed a heuristic method based on the
Markov Decision Process framework. Several researchers [15;
5] suggested myopic, i.e., greedy approaches for selectively
gathering evidence in graphical models. Heckerman et al. [7]
propose a method to compute the maximum expected utility
for speciﬁc sets of observations. While their work considers
more general graphical models than this paper, they provide
only large sample guarantees for the evaluation of a given se-
quence of observations, and use a heuristic without guarantees
to select such sequences The subset selection problem as an in-
stance of feature selection is a central issue in machine learn-
ing, with a vast amount of literature (see [10] for a survey).
The problem of choosing observations also has a strong con-
nection to the ﬁeld of active learning [2] in which the learning
system designs experiments based on its observations.

9 Conclusions
We have described novel efﬁcient algorithms for optimal sub-
set selection and conditional plan computation in chain graph-
ical models, including HMMs. Empirical evaluation indi-
cates that these algorithms can improve upon commonly used

heuristics for decreasing expected uncertainty. Our algorithms
can also effectively enhance performance in interactive struc-
tured classiﬁcation tasks.

Unfortunately,

the optimization problems become in-
tractable for even a slight generalization of chains. We pre-
sented surprising theoretical limits, which indicate that com-
monly used local reward functions, such as conditional en-
tropies, cannot be efﬁciently computed even in discrete poly-
tree graphical models. We also identiﬁed optimization of value
of information as a new class of problems that are intractable
(NPPP-complete) for polytrees.

Our hardness results, along with other recent results for
polytree graphical models, the NP-completeness of maximum
a posteriori assignment [11] and NP-hardness of inference in
conditional linear Gaussian models [8], suggest the possibil-
ity of developing a generalized complexity characterization of
problems that are hard in polytree graphical models.

In light of these theoretical limits for computing optimal
solutions, it is a natural question to ask whether approxima-
tion algorithms with non-trivial performance guarantees can be
found. We are currently focusing our research in this direction.
Acknowledgements. We would like to thank Ben Taskar
for providing the part-of-speech tagging model, and Reuters
for making their news archive available. We would also like
to thank Brigham Anderson and Andrew Moore for helpful
comments and discussions.
Appendix
Proof of Theorem 3. Membership in #P is straightforward.
To show hardness, we use a construction similar to the one
presented in [11] for the maximum a posteriori problem.
Let φ be an instance of #3SAT , where we have to count
the number of assignments to X1, . . . , Xn satisfying φ. Let
C = {C1, . . . , Cm} be the set of clauses. Now create a
Bayesian network with nodes Ui for each Xi, each with uni-
form Bernoulli prior. Add variables Y0, which uniformly varies
over {1, . . . , m} and Y1, . . . , Yn with CPTs deﬁned the follow-
ing way:

Yi | [Yi−1 = j, Ui = ui] ∼( 0,

j,

if j = 0, or Ui = ui
satisﬁes clause Cj;
otherwise.

In this model, Yn = 0 iff U1, . . . , Un encode a satisfying
assignment of φ. Let all nodes have zero reward, except for
Yn, which is assigned the following reward:

R(Yn | O = o) = 2n,

0,

if P (Yn = 0 | O = o) = 1;
otherwise.

Since the prior probability of any assignment is 2−n, the ex-
pected reward R(Yn | U1, . . . , Un) is exactly the number of
satisfying assignments to φ.

Proof of Corollary 4. We start from the same construction as
in the proof of Theorem 3, and add an additional random vari-
able Z after Yn on the chain. Z | Yn is 0 if Yn = 0, and
takes uniformly random values in {0, 1} if Yn 6= 0. Then
H(Z | U = u) is 0 if u is a satisfying assignment, and 1
otherwise. Hence H(Z | O) = 1 − K2−n, where K is the
number of satisfying assignments to φ.
Proof of Theorem 5. Membership follows from Theorem 3.
Let φ be an instance of EM AJSAT , where we have to ﬁnd an

instantiation of X1, . . . , Xn such that φ(X1, . . . , X2n) is true
for the majority of assignments to Xn+1, . . . , X2n. Let C =
{C1, . . . , Cm} be the set of 3CNF clauses. Create the Bayesian
network shown in Fig. 3, with nodes Ui, each having a uniform
Bernoulli prior. Add bivariate variables Yi = (seli, pari), 0 ≤
i ≤ 2n, where seli takes values in {0, . . . , m} and pari is
a parity bit. The CPTs for Yi are deﬁned as: sel0 uniformly
varies over {1, . . . , m}, par0 = 0, and for Y1, . . . , Y2n:
seli | [seli−1 = j, Ui = ui] ∼ 0,
pari | [pari−1 = bi−1, Ui] ∼ bi−1 ⊕ Ui,
where ⊕ denotes the parity (XOR) operator.
We now add variables Z T

if j = 0, or ui satisﬁes Cj;
otherwise;

for 1 ≤ i ≤ n and let

j,

i and Z F
i

(cid:26) I({0, 1}),
(cid:26) I({0, 1}),

0,

0,

if ui = 1;
otherwise;

if ui = 0;
otherwise.

where I denotes the uniform distribution. Similarly, let

| [Ui = ui] ∼

Z T
i

| [Ui = ui] ∼

Z F
i

i = 1 guarantees us that Ui = 1, whereas Z T

We ﬁrst assign penalties ∞ to all nodes except Z T

Intuitively, Z T
0 leaves us uncertain about Ui. The case of Z F
i

i =
is symmetric.
We use the subset selection algorithm to choose the Zis that
encode the solution to EM AJSAT . If Z T
is chosen, it will
i
indicate that Xi should set to true, similarly Z F
indicates a
i
false assignment to Xi. The parity function is going to be used
to ensure that exactly one of {Z T
i } is observed for each i.
for
1 ≤ i ≤ n, and Uj for n + 1 ≤ j ≤ 2n, which are assigned
zero penalty. Let all nodes have zero reward, except for Y2n,
which is assigned the following reward:
if P (sel2n = 0 | O = o) = 1 and
[P (par2n = 1 | O = o) = 1 or
P (par2n = 0 | O = o) = 1];
otherwise.

 4n,

R(Y2n | O = o) =

i , Z F

i , Z F
i

0,

Note that sel2n = 0 with probability 1 iff U1, . . . , U2n encode
a satisfying assignment of φ, as in the proof of Theorem 3.
Furthermore, we get positive reward only if we are both cer-
tain that sel2n = 0, i.e., the chosen observation set must con-
tain a proof that φ is satisﬁed, and we are certain about par2n.
The parity certainty will only occur if we are certain about the
assignment U1, . . . , U2n. It is only possible to infer the value
of each Ui with certainty by observing one of Ui, Z T
i or Z F
i .
Since, for i = 1, . . . , n, the cost of observing Ui is ∞, to re-
ceive any reward we must observe at least one of Z T
i or Z F
i .
Assume that we compute the optimal subset ˆO for budget 2n,
then we can only receive positive reward by observing exactly
one of Z T
i as an assignment
to the ﬁrst n variables of EM AJSAT . Let ˆR = R(Y2n | ˆO).
We claim that φ ∈ EM AJSAT if and only if ˆR > 0.5. First
let φ ∈ EM AJSAT , with assignment x1, . . . , xn to the ﬁrst
n variables. Now add Un+1, . . . , U2n to O and add Z T
to O
i
iff xi = 1 and Z F
to O iff xi = 0. This selection guarantees
i
ˆR > 0.5.
U1, . . . , U2n consistent if for any 1 ≤ i ≤ n, if Z T

Now assume ˆR > 0.5. We call an assignment

We interpret the selection of Z T

to
i ∈ ˆO,

i and Z F

i or Z F
i .

Figure 3: Graphical model used in proof of Theorem 6.

i ∈ ˆO then Ui = 0. For any consis-
then Ui = 1 and if Z F
tent assignment, the chance that the observations Zi prove the
consistency is 2−n. Hence ˆR > 0.5 implies that the major-
ity of all provably consistent assignments satisfy φ and hence
φ ∈ EM AJSAT . This proves that subset selection is NPPP
complete.

Proof of Corollary 6. The construction in the proof of Theo-
rem 5 also proves that computing conditional plans is NPPP-
hard, since, in this instance, any plan with positive reward must
observe all Un+1, . . . , U2n and one each of the Z1, . . . , Zn, to
satisfy the parity condition. In this case, the order of selec-
tion is irrelevant, and, hence, the conditional plan effectively
performs subset selection.

References
[1] V. Bayer-Zubek. Learning diagnostic policies from examples by

systematic search. In UAI, 2004.

[2] D. A. Cohn, Z. Gharamani, and M. I. Jordan. Active learning

with statistical models. J AI Research, 4:129–145, 1996.

[3] CoNLL. Conference on computational natural language learn-

ing shared task. http://cnts.uia.ac.be/conll2003/ner/, 2003.

[4] A. Deshpande, C. Guestrin, S. Madden, J. Hellerstein, and
W. Hong. Model-driven data acquisition in sensor networks.
In VLDB, 2004.

[5] S. Dittmer and F. Jensen. Myopic value of information in inﬂu-

ence diagrams. In UAI, pages 142–149, San Francisco, 1997.

[6] R. Durbin, S. R. Eddy, A. Krogh, and G. Mitchison. Biolog-
ical Sequence Analysis : Probabilistic Models of Proteins and
Nucleic Acids. Cambridge University Press, 1999.

[7] D. Heckerman, E. Horvitz, and B. Middleton. An approximate
nonmyopic computation for value of information. IEEE Trans.
Pattern Analysis and Machine Intelligence, 15:292–298, 1993.
[8] U. Lerner and R. Parr. Inference in hybrid networks: Theoretical

limits and practical algorithms. In UAI, 2001.

[9] M. Littman, J. Goldsmith, and M. Mundhenk. The computa-
tional complexity of probabilistic planning. Journal of Artiﬁcial
Intelligence Research, 9:1–36, 1998.

[10] L. Molina, L. Belanche, and A. Nebot. Feature selection algo-
rithms: A survey and experimental evaluation. In ICDM, 2002.
[11] J. D. Park and A. Darwiche. Complexity results and approx-
imation strategies for map explanations. Journal of Aritiﬁcial
Intelligence Research, 21:101–133, February 2004.

[12] J. R. Quinlan. Induction of decision trees. Machine Learning,

1:81–106, 1986.

[13] T. Scheffer, C. Decomain, and S. Wrobel. Active learning of
partially hidden markov models for information extraction. In
ECML/PKDD Workshop on Instance Selection, 2001.

[14] P. D. Turney. Cost-sensitive classiﬁcation: Empirical evaluation
of a hybrid genetic decision tree induction algorithm. Journal
of Artiﬁcial Intelligence Research, 2:369–409, 1995.

[15] L. van der Gaag and M. Wessels. Selective evidence gathering

for diagnostic belief networks. AISB Quart., 86:23–34, 1993.


SMDP Homomorphisms: An Algebraic Approach to Abstraction in Semi-Markov 

Decision Processes 

Balaraman Ravindran and Andrew G. Barto 

Department of Computer Science 

University of Massachusetts 

Amherst, MA, U. S. A. 

{ravi | barto} @cs.umass.edu 

Abstract 

To  operate  effectively  in  complex  environments 
learning agents require the ability to selectively ig(cid:173)
nore irrelevant details and form useful abstractions. 
In this article we consider the question of what con(cid:173)
stitutes a useful abstraction in a stochastic sequen(cid:173)
tial decision problem modeled as a semi-Markov 
Decision Process (SMDPs).  We introduce the no(cid:173)
tion of SMDP homomorphism and argue that it pro(cid:173)
vides a useful tool for a rigorous study of abstrac(cid:173)
tion for SMDPs.  We present an SMDP minimiza(cid:173)
tion framework and an abstraction framework for 
factored MDPs based on SMDP homomorphisms. 
We also model different classes of abstractions that 
arise in hierarchical systems. Although we use the 
options framework for purposes of illustration, the 
ideas are more generally applicable. We also show 
that the conditions for abstraction we employ are a 
generalization of earlier work by Dietterich as ap(cid:173)
plied to the options framework. 

Introduction 

1 
The ability to form abstractions is one of the features that al(cid:173)
lows humans to operate effectively in complex environments. 
We systematically ignore information that we do not need for 
performing the immediate task at hand.  While driving, for 
example, we may ignore details regarding clothing and the 
state of our hair. Researchers in artificial intelligence (AI), in 
particular machine learning (ML), have long recognized that 
applying computational approaches to operating in complex 
and real-world domains requires that we incorporate the abil(cid:173)
ity to handle and form useful abstractions.  In this article we 
consider the question of what constitutes a useful abstraction. 
Since this is a difficult problem when stated in general terms, 
much  of the  work  in  this  field  is  specialized  to  particular 
classes of problems or specific modeling paradigms.  In this 
work we will focus on Markov decision processes (MDPs), a 
formalism widely employed in modeling and solving stochas(cid:173)
tic sequential decision problems and semi-Markov decision 
processes  (SMDPs),  an  extension  to  MDPs  recently  em(cid:173)
ployed in modeling hierarchical systems [Sutton et ai, 1999; 
Dietterich, 2000; Parr and Russell, 1997]. 

Our objective in this article is to introduce the concept of 
an SMDP homomorphism and argue that it provides a unified 
view of key issues essential  for a rigorous treatment of ab(cid:173)
straction for stochastic dynamic decision processes. The con(cid:173)
cept of a homomorphism between dynamic systems, some(cid:173)
times called a "dynamorphism"  [Arbib and Manes,  1975], 
has played an important role in theories of abstract automata 
[Hartmanis  and  Stearns,  1966],  theories  of modeling  and 
simulation [Zeigler,  1972], and it is frequently used by re(cid:173)
searchers studying model checking approaches to system val(cid:173)
idation [Emerson and Sistla,  1996].  Although those study(cid:173)
ing approximation  and abstraction  methods  for MDPs  and 
SMDPs have  employed formalisms that  implicitly embody 
the idea of a homomorphism, they have not made explicit use 
of the appropriate homomorphism concept. We provide what 
we claim is the appropriate concept and give examples of how 
it can be widely useful as the basis of the study of abstraction 
in a dynamic setting. 

Informally,  the kind of homomorphism we consider is a 
mapping from one dynamic system to another that eliminates 
state distinctions while preserving the system's dynamics. We 
present a definition of homomorphisms that is appropriate for 
SMDPs. In earlier work [Ravindran and Barto, 2002] we de(cid:173)
veloped an MDP abstraction framework based on our notion 
of an MDP homomorphism.  This framework extended the 
MDP minimization framework proposed by  [Dean and Gi-
van,  1997] and enabled the accommodation of redundancies 
arising from symmetric equivalence of the kind illustrated in 
Figure 1. While we can derive reduced models with a smaller 
state set by applying minimization ideas, we do not neces(cid:173)
sarily simplify the description of the problem in terms of the 
number of parameters required to describe it.  But MDPs of(cid:173)
ten have additional structure associated with them that we 
can exploit to develop compact representations.  By inject(cid:173)
ing structure in the definition of SMDP homomorphism we 
can model abstraction schemes for structured MDPs.  In this 
paper we present one simple example of such an abstraction 
scheme and mention other factored abstraction methods that 
can be modeled by suitable structured homomorphisms. 

In the second part of the paper we extend  the notion of 
SMDP homomorphism to hierarchical systems.  In particu(cid:173)
lar, we apply homomorphisms in the options framework in(cid:173)
troduced by  [Sutton et aly  1999] and show that this facili(cid:173)
tates  employing  different  abstractions  at different  levels  of 

PROBABILISTIC  PLANNING 

1011 

in state s. The value of a state-action pair 

gives the probability of executing action 
under policy 
is the expected value of the sum of discounted future re(cid:173)

wards starting from state s, taking action a, and following 
thereafter. When the SMDP has well defined terminal states, 
we often do not discount future rewards.  In such cases an 
SMDP is equivalent to an MDP and we will ignore the tran(cid:173)
sition times. The action-value function, 
corresponding 
is the mapping from state-action pairs to their 
to a policy 
values. The solution of an MDP is an optimal policy, 
that 
uniformly dominates all other possible policies for that MDP. 

Let B be a partition  of a  set X.  For any 

denotes  the block  of B  to which 

belongs.  Any function 
from a set X to a set Y induces a partition (or equivalence 

if and only if, 

is a mapping that preserves 

relation) on X, with 
3  SMDP Homomorphisms 
to a dynamic 
A homomorphism from a dynamic system 
system 
's dynamics, while 
in general eliminating some of the details of the full system 
that 
with respect to the as(cid:173)
is nevertheless a valid model  of 
pect's  of 
state that it preserves.  The specific definition 
of homomorphism that we claim is most useful for MDPs and 
SMDPs is as follows: 

as a simplified model of 

One can think of 

Figure 1:  (a) A symmetric gridworld problem. The goal state is 
G and there are four deterministic actions. This gridworld is sym(cid:173)
metric about the NE-SW diagonal. For example, states A and B are 
equivalent since for each action in A, there is an equivalent action in 
B. Taking action E, say, in state A is equivalent to taking action N 
in state B, in the sense that they go to equivalent states that are each 
one step closer to the goal, (b) An equivalent reduced model of the 
gridworld in (a). The states A and B in the original problem corre(cid:173)
spond  to  the  single  state  in the reduced problem. A solution 
to this reduced gridworld can be used to derive a solution to the full 
problem. 

a hierarchy.  We also argue that homomorphisms allow us 
to  model  aspects  of symbolic  AI  techniques  in  represent(cid:173)
ing higher level  task structure.  Finally,  we  show  that the 
SMDP homomorphism conditions generalize the "safe" state 
abstraction conditions for hierarchical systems introduced by 
[Dietterich, 2000]. 

After  introducing  some  notation  (Section  2),  we  define 
SMDP  homomorphisms  and  discuss  modeling  symmetries 
(Section 3).  Then we present a brief overview of our model 
minimization framework (Section 4),  discuss abstraction in 
factored MDPs  (Section  5)  and abstraction  in hierarchical 
systems (Section 6).  We conclude with some discussion of 
directions for future research (Section 7). 

A discrete time semi-Markov decision process (SMDP) is a 
generalization of an MDP in which actions can take variable 
amounts of time to complete.  As with an MDP, an SMDP 
is a tuple 
are the sets of 
states, actions and admissible state-action pairs; 

where S, A and 

is the transition probability function with 
being the probability of transition from state 

s to state s' under action 

in N time steps, and 

is the expected discounted reward function, with 
being the expected reward for performing action 

a in state s and completing it in N time steps.1 

A  stochastic  policy,  is a mapping from 

terval [0,1] with 

to the real in(cid:173)
For any 

1 We are adopting the formalism of [Dietterich, 2000]. 

to denote 

to states  of 

. Each surjection 

the homomorphic image of M under 

We call 
use the shorthand 
maps states  of 

and we 
The surjection 
and since it is gener(cid:173)
ally many-to-one, it generally induces nontrivial equivalence 
classes of states s of M: 
recodes the 
actions admissible in state s  of 
to actions admissible in 
state f(s)  of 
This state-dependent recoding of actions is 
a key innovation of our definition, which we discuss in more 
detail below. Condition (1) says that the transition probabili(cid:173)
are expressible as sums of the 
ties in the simpler SMDP 
transition probabilities of the states of 
that / maps to that 
same state in 
This is the stochastic version of the stan(cid:173)
dard condition for homomorphisms of deterministic systems 
that requires that the homomorphism commutes with the sys(cid:173)
tem dynamics [Hartmanis and Stearns, 1966]. Condition (2) 
says that state-action pairs that have the same image under 
h have the same expected reward. An MDP homomorphism 
is similar to an SMDP homomorphism except that the condi(cid:173)
tions (1) and (2) apply only to the states and actions and not 
to the transition times. 

The state-dependent action mapping allows us to model 
symmetric equivalence in MDPs and SMDPs.  For example, 

1012 

PROBABILISTIC  PLANNING 

state set is necessarily simpler and hence might not lead to 
a simpler problem representation. Many classes of problems 
that are modeled as MDPs have some inherent structure. We 
define structured forms of homomorphisms that can exploit 
this structure in deriving simpler problem representations. 

Factored MDPs are a popular way to model  structure in 

The  set  of all  automorphisms  of an  SMDP  M,  denoted 
by AutM, forms a group under composition of homomor-
phisms.  This group is the symmetry group of M.  In the 
gridworld example of Figure 1, the symmetry group consists 
of the identity map on states and actions, a reflection of the 
states about the NE-SW diagonal and a swapping of actions 
N and E and of actions S and W. Any subgroup of the sym(cid:173)
metry group of an SMDP induces an equivalence relation on 
$, which can also be induced by a suitably defined homo-
morphism [Ravindran and Barto, 2001].  Therefore we can 
model symmetric equivalence as a special case of homomor-
phic equivalence. 
4  Minimization 
The notion of homomorphic equivalence immediately gives 
us an SMDP minimization  framework.  In  [Ravindran and 
Barto,  2001]  we  extended  the  minimization  framework  of 
Dean and Givan LDean and Givan, 1997; Givan et a/., 2003] 
to include state-dependent action recoding and showed that 
if two state-action pairs have the same image under a homo-
morphism, then they have the same optimal value.  We also 
showed that when  M'  is a homomorphic image of an MDP 
My a policy in Ml can induce a policy in M that is closely 
related. Specifically a policy that is optimal in M' can induce 
an optimal policy in M. Thus we can solve the original MDP 
by solving a homomorphic image.  It is easy to extend these 
results to SMDP models. 

Thus the goal of minimization is to derive a minimal image 
of the SMDP, i.e., a homomorphic image with the least num(cid:173)
ber of admissible state-action pairs. We also adapted an exist(cid:173)
ing minimization algorithm to find minimal images employ(cid:173)
ing state-action equivalence.  Employing state-dependent ac(cid:173)
tion recoding allows us to achieve greater reduction in model 
size than possible with Dean and Givan's framework. For ex(cid:173)
ample, the gridworld in Figure 1(a) is minimal if we do not 
consider state-dependent action mappings. 
5  Abstraction in Structured MDPs 
SMDP homomorphisms can also be used to model various 
SMDP abstraction frameworks.  The definition of homomor-
phism in Section 3 assumed a monolithic representation of 
the state set. While we can derive an equivalent MDP model 
with a smaller 
it does not follow that the description of the 

denotes the parents of node s[ in the 2-TBN 

where Pre 
corresponding to action a and each of the Prob 
is given by a conditional probability table (CPT) associated 
with node 
The reward function may be similarly repre(cid:173)
sented. 

Structuring the state space representation allows us to con(cid:173)
sider morphisms that are structured, i. e. surjections from one 
structured set to another.  An example of a structured mor-
phism is a simple projection onto a subset of features.  Here 
the state set of the homomorphic image is described by a sub(cid:173)
set of the features, while the rest are ignored.  We introduce 
some notation, after [Zeigler,  1972], to make the following 

The first condition implies that the subset F should be cho(cid:173)
sen such that the features chosen are sufficient to describe the 
block transition dynamics of the system.  In other words, the 

PROBABILISTIC  PLANNING 

1013 

subgraph of the 2-TBN described by the projection should in(cid:173)
clude all the incoming arcs incident on the chosen nodes. The 
second condition requires that all the parents and the incom(cid:173)
ing arcs to the reward node are also included in the subgraph. 
To find such homomorphic projections, we just need to work 
back from the reward node including arcs and nodes, until 
we reach the desired subgraph. Such an algorithm will run in 
time polynomial in the number of features.  It is evident that 
the space of such simple projections  is much smaller than 
that of general maps and in general may not contain a ho-
momorphism reducing a given MDP. Without suitable con(cid:173)
straints, often derived from prior knowledge of the structure 
of the problem,  searching for general structured homomor-
phisms results in a combinatorial explosion.  Abstraction al(cid:173)
gorithms developed by Boutilier and colleagues can be mod(cid:173)
eled as converging to constrained forms of structured mor-
phisms assuming various representations of the CPTs—when 
the space of morphisms  is defined by boolean formulae of 
the features [Boutilier and Dearden, 1994], when it is defined 
by decision trees on the features [Boutilier et  al,  1995] and 
when it is defined by first-order logic formulae [Boutilier et 
a/., 2001]. 

6  Abstraction in Hierarchical Systems 
In the previous  section  we  showed that  SMDP homomor-
phisms  can  model  various  abstraction  schemes  in  "flat" 
MDPs and SMDPs. SMDP homomorphisms are a convenient 
and powerful formalism for modeling abstraction schemes in 
hierarchical systems as well.  Before we explore various ab(cid:173)
straction approaches we first introduce a hierarchical archi(cid:173)
tecture that supports abstraction. 

6.1  Hierarchical Markov Options 
Recently several hierarchical reinforcement learning frame(cid:173)
works have been proposed [Parr and Russell, 1997; Sutton et 
al,  1999; Dietterich, 2000] all of which use the SMDP for(cid:173)
malism. In this article the hierarchical framework we adopt is 
the options framework [Sutton et ai, 1999], though the ideas 
developed here are more generally applicable.  Options are 
actions that take multiple time steps to complete.  They are 
usually described by: the policy to follow while the option is 
executing, the set of states in which the option can begin exe(cid:173)
cution, and the termination function fi which gives the prob(cid:173)
ability with which the option can terminate in a given state. 
The resulting systems are naturally modeled as SMDPs with 
the transition time distributions induced by the option poli(cid:173)
cies.  We present an extension to the options framework that 
readily facilitates modeling abstraction at multiple levels of 
the hierarchy using SMDP homomorphisms. 

We consider the class of options known as Markov options, 
whose policies satisfy the Markov property and that terminate 
on achieving a certain sub-goal. In such instances it is possi(cid:173)
ble to implicitly define the option policy as the solution to an 
option MDP, or an option SMDP if the option has access to 
other options.  Accordingly we define a hierarchical Markov 
sub-goal option: 
Definition:  A hierarchical Markov sub-goal option of an 
SMDP 

is the initiation set of the option, 

where 
is the termination function and M o is the option SMDP. 
The state set of Mo  is a subset of 5 and constitutes the do(cid:173)
main of the option.  The action set of Mo  is a subset of A 
and may contain other options as well as "primitive" actions 
in  A.  The reward  function  of Mo  is  chosen  to reflect the 
sub-goal  of the  option.  The  transition probabilities  of Mo 
are induced by P and the policies of lower level options. We 
assume that the lower-level options are following fixed poli(cid:173)
cies which are optimal in the corresponding option SMDPs. 
is obtained by solving Mo> treating it as 
The option policy 
an episodic task with the possible initial states of the episodes 
given by  and the termination of each episode determined by 
the option's termination function 

For example, in the gridworld task shown in Figure 2(a), 
an option to pick up the object and exit room 1 can be defined 
as the solution to the problem shown in 2(b), with a suitably 
defined reward function. The domain and the initiation set of 
the option are all states in the room and the option terminates 
on exiting the room with or without the object. 

6.2  Option  Specific  Abstraction 
The homomorphism conditions (1) and (2) are very strict and 
frequently we end up with trivial homomorphic images when 
deriving abstractions based on a non-hierarchical SMDP. But 
it is often possible to derive non-trivial reductions if we re(cid:173)
strict attention to certain sub-problems, i.e., certain sub-goal 
options.  In such cases we can apply the ideas discussed in 
Sections 4 and 5 to an option SMDP directly to derive abstrac(cid:173)
tions that are specific to that option. The problem of learning 
the option policy is transformed to the usually simpler prob(cid:173)
lem of learning an optimal policy for the homomorphic im(cid:173)
age. 

6.3  Relativized  Options 
Consider the problem of navigating in the gridworld environ(cid:173)
ment shown in Figure 2(a).  The goal is to reach the central 
corridor after collecting all the objects  in the environment. 
No non-trivial homomorphic image exists of the entire prob(cid:173)
lem. But there are many similar components in the problem, 
namely,  the five sub-tasks of getting the object and exiting 
roomt.  We can model these similar components by a "par(cid:173)
tial" homomorphic image—where the homomorphism con(cid:173)
ditions are applicable only to states in the rooms.  One such 
partial image is shown in Figure 2(b). Employing such an ab(cid:173)
straction lets us compactly represent a related family of op(cid:173)
tions, in this case the tasks of collecting objects and exiting 
each of the five rooms, using a single option MDP. We refer 
to this compact option as a relativized option. Such abstrac(cid:173)
tions are an extension of the notion of relativized operators 
introduced by [Iba, 1989].  Formally we define a relativized 
option as follows: 

1014 

PROBABILISTIC  PLANNING 

Figure 2: (a) A simple rooms domain with similar rooms and usual 
stochastic gridworld dynamics. The task is to collect all 5 objects 
(black diamonds) in the environment and reach the central corridor. 
The shaded squares are obstacles, (b) The option MDP correspond(cid:173)
ing to a get-object-and-leave-room option. 

where So is the 
Here the state set of 
domain of the option, and the admissible state-action set is 
//(ty). Going back to our example in Figure 2(a), we can now 
define a single get-object-and-leave-room relativized option 
using the option MDP of Figure 2(b).  The policy learned in 
this option MDP can then be suitably lifted to M to provide 
different policy fragments in the different rooms.  Figure 3 
demonstrates the speed-up in performance when using a sin(cid:173)
gle relativized option as opposed to five regular options.  In 
this experiment the option policies and the higher level pol(cid:173)
icy were learned simultaneously.2 

6.4  Modeling Higher Level Structure 
SMDP homomorphisms have the power to model a broader 
class of abstractions, those not supported by the base level 
dynamics of the system but which can be induced at some 
intermediate levels of the hierarchy.  For example, consider 
a multi-link assembly line robot arm that needs to move ob(cid:173)
jects from one assembly line to another. The state of the sys(cid:173)
tem is described by the joint angles and velocities, object-
shape, object-orientation, and Boolean variables indicating 
whether the object is  firmly-grasped  and whether the object 
is at-target-location.  The primitive actions are various joint 
torques.  Depending on the configuration of the arm and the 
shape and orientation of the object, this task requires different 
sequences of actions, or policies, even though conceptually 
the higher level task has the same "structure"—grasp object, 
move object, and place object. 

We can model this higher level task structure using a rel(cid:173)
ativized option.  First,  we define suitable grasp-object op(cid:173)
tions, such as grasp-cylinder, grasp-block etc. , and simi(cid:173)
lar options for moving and placing objects.  Then we form 
a partial  homomorphic  image  of the resulting  SMDP—the 
state set of the image is described by the two Boolean fea(cid:173)
tures and the action set consists of grasp, move and place 
actions.  The partial homomorphism, which applies only to 
the admissible  state-option  pairs  of the  SMDP,  consists  of 

2In [Ravindran and Barto, 2002] we have reported more detailed 

experiments in this setting. 

Figure 3: Comparison of performance of learning agents employ(cid:173)
ing regular and relativized options on the task shown in Figure 2. 

object-shape and similarly for the move-object and place-
object options.  A relativized option with this partial image 
and partial homomorphism as the option MDP and option 
homomorphism, respectively, captures the desired conceptual 
structure. Executing the optimal policy for this option results 
in action sequences of the form: grasp, move, place, — De(cid:173)
pending on the object-shape these abstract actions get bound 
to different lower level options.  While techniques  in sym(cid:173)
bolic Al have long been able to model such higher level task 
structure, the reinforcement learning community lacked an 
obvious mechanism to do the same.  Our work gives us new 
tools to model conceptual task structure at various temporal 
and spatial scales. 

6.5  Relation to MaxQ Safe State-abstraction 
[Dictterich,  2000]  introduced  safe  state-abstraction  condi(cid:173)
tions  for  the  MaxQ  architecture,  a  hierarchical  learning 
framework related to the options framework.  These condi(cid:173)
tions ensure that the resulting abstractions do not result in any 
loss of performance. He assumes that the sub-problems at dif(cid:173)
ferent levels of the hierarchy are specified by factored MDPs. 
The MaxQ architecture employs a form of value function de(cid:173)
composition and some of the safe abstraction conditions ap(cid:173)
ply only to this form of decomposition. The following condi(cid:173)
tion is more universal and applies to the hierarchical Markov 
options framework as well: 

Condition (i) states that the transition probability can be 
expressed as a product of two probabilities, one of which de(cid:173)
scribes the evolution of a subset of the features and depends 
only on that  subset.  Condition (ii) states that  if two states 
project to the same abstract state, then they have the same 
immediate reward.  From equations 1-4, it is evident that the 
above conditions are equivalent to the SMDP homomorphism 
conditions if we restrict our attention to simple projection ho(cid:173)
momorphisms and do not consider action remapping.  Thus 
the SMDP homomorphism conditions introduced here gener(cid:173)
alize Dietterich's safe state-abstraction condition as applica(cid:173)
ble to the hierarchical Markov option framework. 

PROBABILISTIC  PLANNING 

1015 

7  Discussion 
The equivalence classes induced by SMDP homomorphisms 
satisfy  the  stochastic  version  of the  substitution  property 
[Hartmanis and Stearns, 1966]. This property is also closely 
related to lumpability in Markov chains [Kemeny and Snell, 
1960] and bisimulation homogeneity [Givan et al., 2003] in 
MDPs.  We chose the SMDP homomorphism as our basic 
formalism because we believe that it is a simpler notion and 
provides a more intuitive explanation of various abstraction 
schemes. 

The homomorphism conditions (1) and (2) are very strict 
conditions that are often not met exactly in practice. One ap(cid:173)
proach is to relax the homomorphism conditions somewhat 
and allow small variations in the block transition probabil(cid:173)
ities and rewards.  We have explored this issue [Ravindran 
and Barto, 2002], basing our approximate homomorphisms 
on the concept of Bounded-parameter MDPs developed by 
[Givan et al., 2000]. We are currently working on extending 
approximate homomorphisms to hierarchical systems so as to 
accommodate variations in transition-time distributions. 

Although  SMDP  homomorphisms  are  powerful  tools  in 
modeling  abstraction,  finding  a  minimal  image  of a given 
SMDP is an NP-hard problem.  While taking advantage of 
structure allows us to develop efficient algorithms in special 
cases, much work needs to be done to develop efficient gen(cid:173)
eral purpose algorithms. Currently we are investigating meth(cid:173)
ods that allow us to determine homomorphisms given a set of 
candidate transformations in a hierarchical setting. 

In this article we presented a novel  definition of SMDP 
homomorphism that employs state-dependent recoding of ac(cid:173)
tions. This allows us to extend existing minimization and ab(cid:173)
straction methods to a richer class of problems. We developed 
notions of equivalence specialized to factored representations 
and hierarchical architectures.  We have shown that SMDP 
homomorphism can serve as the basis for modeling a variety 
of abstraction paradigms. 
Acknowledgments 
We wish to thank Dan Bernstein and Mike Rosenstein for 
many hours of useful  discussion and  Bob Givan and Matt 
Greig for clarifying certain ideas from their work. This mate(cid:173)
rial is based upon work supported by the National Science 
Foundation  under Grant No.  ECS-0218125  to Andrew G. 
Barto and Sridhar Mahadevan.  Any opinions,  findings  and 
conclusions or recommendations expressed in this material 
are  those  of the  authors  and  do not necessarily  reflect  the 
views of the National Science Foundation. 
References 
[Arbib and Manes, 1975] M. A. Arbib and E. G. Manes. Arrows, 

[Boutilier et al, 2001]  Craig  Boutilier,  Ray  Reiter,  and  Robert 
Price.  Symbolic dynamic programming for first-order mdps.  In 
Proceedings of the Seventeenth International Joint Conference 
on Artificial Intelligence, pages 541-547, 2001. 

[Dean and Givan, 1997] T. Dean and R. Givan. Model minimiza(cid:173)
tion in Markov decision processes. In Proceedings ofAAAl-97, 
pages 106 111. AAAI, 1997. 

[Dean and Kanazawa, 1989] Thomas Dean and K. Kanazawa.  A 
model for reasoning about persistence and causation. Computer 
Intelligence, 5(3): 142 150,1989. 

[Diettcrich, 2000]  T.  G.  Diettcrich.  Hierarchical  reinforcement 
learning with the MAXQ value function decomposition.  Arti(cid:173)
ficial Intelligence Research, 13:227-303, 2000. 

[Emerson and Sistla, 1996] E. A. Emerson and A. P. Sistla. Sym(cid:173)

metry and model checking. Formal Methods in System Design, 
9(1/2):105-131, 1996. 

[Givan et al, 2000] R. Givan, S. Leach, and T. Dean. Bounded-
parameter Markov decision processes.  Artificial Intelligence, 
122:71-109,2000. 

[Givan et al, 2003] R. Givan, T. Dean, and M. Greig. Equivalence 
notions and model minimization in Markov decision processes. 
To appear in Artificial Intelligence, 2003. 

[Hartmanis and Steams, 1966] J. Hartmanis and R. E. Steams. Al(cid:173)

gebraic Structure Theory of Sequential Machines. Prcnticc-Hall, 
Englewood Cliffs, NJ, 1966. 

[Iba, 1989]  Glenn A. Iba. A heuristic approach to the discovery of 

macro-operators. Machine Learning, 3:285 317, 1989. 

[Kemeny and Snell, 1960] J. G. Kemeny and J. L. Snell. Finite 

Markov Chains. Van Nostrand, Princeton, NJ, 1960. 

[Parr and Russell, 1997] Ronald Parr and Stuart Russell. Reinforce(cid:173)
ment learning with hierarchies of machines. In Proceedings of 
Advances in Neural Information Processing Systems 10, pages 
1043 1049. MIT Press, 1997. 

[Ravindran and Barto, 2001] B. Ravindran and A. G. Barto. Sym(cid:173)
metries and model minimization of Markov decision processes. 
Technical Report 01-43, University of Massachusetts, Amherst, 
2001. 

[Ravindran and Barto, 2002] Balaraman Ravindran and Andrew G. 

Barto. Model minimization in hierarchical reinforcement learn(cid:173)
ing. In Sven Koenig and Robert C. Holte, editors, Proceedings 
of the Fifth Symposium on Abstraction, Reformulation and Ap(cid:173)
proximation (SARA 2002), Lecture Notes in Artificial Intelligence 
2371, pages 196-211, New York, NY, August 2002. Springer-
Verlag. 

[Sutton et al., 1999] Richard S. Sutton, Doina Precup, and Satin-
der Singh.  Between MDPs and Semi-MDPs: A framework for 
temporal abstraction in reinforcement learning. Artificial Intelli(cid:173)
gence, 112:181-211, 1999. 

[Zeigler, 1972] Bernard P. Zeigler.  On the formulation of prob(cid:173)
lems in simulation and modelling in the framework of mathe(cid:173)
matical system theory. In Proceedings of the Sixth International 
Congress on Cybernetics, pages 363-385. Association Interna(cid:173)
tionale de Sybernetique, 1972. 

[Zinkevich and Balch, 2001] M. Zinkcvich and T. Balch. Symme(cid:173)
try in Markov decision processes and its implications for single 
agent and multi agent learning. In Proceedings of the 18th Inter(cid:173)
national Conference on Machine Learning, pages 632-640, San 
Francisco, CA, 2001. Morgan Kaufmann. 

Structures and Functors. Academic Press, New York, NY, 1975. 

[Boutilicr and Deardcn, 1994] C. Boutilier and R. Dearden. Using 
abstractions for decision theoretic planning with time constraints. 
In Proceedings of the AAA 1-94, pages 1016-1022. AAAI, 1994. 

[Boutilier et al, 1995]  C.  Boutilier,  R.  Dearden,  and M.  Gold-
szmidt. Exploiting structure in policy construction. In Proceed(cid:173)
ings of International Joint Conference on Artificial Intelligence 
R pages 1104-1111, 1995. 

1016 

PROBABILISTIC  PLANNING 


Parametric Distance Metric Learning with Label Information* 

Zhihua Zhang, James T. Kwok and Dit-Yan Yeung 
Hong Kong University of Science and Technology 

Clear Water Bay, Kowloon, Hong Kong 
{zhzhang, jamesk, dyyeung}@cs.ust.hk 

Abstract 

Distance-based methods in pattern recognition and 
machine  learning  have  to  rely  on  a  similarity  or 
dissimilarity  measure  between  patterns  in  the  in(cid:173)
put  space.  For many  applications,  Euclidean  dis(cid:173)
tance  in  the  input  space  is  not a good  choice and 
hence  more  complicated  distance  metrics  have  to 
be  used. 
In  this  paper,  we  propose  a  parametric 
method for metric learning based on class label in(cid:173)
formation.  We  first  define a dissimilarity measure 
that can be proved to be metric.  It has the favorable 
property that between-class dissimilarity  is always 
larger than within-class dissimilarity.  We then per(cid:173)
form parametric learning to find a regression map(cid:173)
ping from the input space to a feature space,  such 
that  the  dissimilarity  between  patterns  in  the  in(cid:173)
put  space  is  approximated  by  the  Euclidean  dis(cid:173)
tance  between  points  in  the  feature  space.  Para(cid:173)
metric learning is performed using the iterative ma-
jorization algorithm.  Experimental results on real-
world benchmark data sets show that this approach 
is promising. 

Introduction 

1 
The notion of similarity  or dissimilarity plays a fundamental 
role in pattern recognition and machine learning.  A promis(cid:173)
ing direction to pursue  is to learn good (dis)similarity mea(cid:173)
sures  from  data.  Recently,  learning  distance  metrics from 
data has aroused a great deal of interest from machine learn(cid:173)
ing  researchers.  One  typically  wants  to  embed  patterns  in 
a  (possibly non-metric)  input  space  into  a  feature  space,  in 
which the  Euclidean distance  between points  accurately  re(cid:173)
flects the  dissimilarity  between  the  corresponding  patterns. 
Therefore  the  (linear  or  nonlinear)  mapping  from  the  input 
space to the feature space corresponds to feature extraction. 
Alternatively,  the  feature  space  may  be  a  low-dimensional 
space for data visualization. 

In  this  paper,  we  propose  a  parametric  distance  metric 
learning  method  in  the  supervised  setting.  The  main  ideas 
This  research  has  been  partially  supported  by  the  Re(cid:173)
search  Grants  Council  of  the  Hong  Kong  Special  Administra(cid:173)
tive Region under grants DAG01/02.EG28, HKUST2033/00E and 
HKUST6195/02E. 

1450 

of our method  are  summarized as  follows.  Using  class  la(cid:173)
bel  information,  we define  a  similarity  measure  (and hence 
also  the  corresponding  dissimilarity  measure)  between  pat(cid:173)
terns in the input space.  The dissimilarity measure implicitly 
induces  a  metric  space  for embedding the original  patterns. 
To  explicitly  represent the  mapping  from the  input space  to 
the feature space, we then approximate the mapping by a re(cid:173)
gression model to embed the original patterns in an Euclidean 
space.  The  regression  parameters  are  estimated  from  data 
with the  objective  that the  dissimilarity  between  patterns  in 
the input space is approximated by the Euclidean distance be(cid:173)
tween points in the feature space.  Once the regression model 
has been found, any new pattern can be mapped to its corre(cid:173)
sponding location in the feature space. Distance-based meth(cid:173)
ods, such as  k-means  clustering, nearest neighbor classifiers 
and support vector machines, can then be applied in the fea(cid:173)
ture space for clustering or classification applications. 

The rest of this paper is organized as follows.  A modified 
metric  incorporating  class  label  information  is  proposed  in 
Section 2.  Section 3 outlines our regression model for metric 
learning and the corresponding optimization method.  Exper(cid:173)
imental results are presented in Section 4, and the last section 
gives some concluding remarks. 

2  Modified Metric with Label Information 
Denote  the  input  space  by  Rq  and  the  set  of all  C  possible 
class  (target)  labels  by  T.  A  training  set 
has 
n  patterns 
where  U  =  r  if pattern  i 
belongs to class r.  Here, each pattern is assumed to belong to 
only one class.  In general,  a number of similarity measures 
can be defined on these patterns [Gower and Legendre, 1986]. 
In this paper, we utilize also the label information in defining 
the similarity  sij  between patterns  xi  and Xj: 

where 
parameter.  The corresponding dissimilarity Sij  is then: 

denotes the Euclidean norm and B > 0 is a width 

(1) 

(2) 

POSTER PAPERS 

As illustrated in Figure  1, this (dis)similarity measure en(cid:173)
joys some nice properties for pattern discrimination.  For ex(cid:173)
ample,  the  (dis)similarity  between  any  two  patterns  in  the 
same class  is  always larger (smaller)  than that between any 
two  patterns  belonging  to  different  classes.  Moreover,  the 
larger  the  Euclidean  distance  between  the  patterns  is,  the 
smaller  is  the  within-class  similarity  while  the  larger  is  the 
between-class similarity. 

Figure 1:  Similarity and dissimilarity in (1) and (2). 

In recent years,  finite metric spaces and their embeddings 
have  received  much  attention  [Indyk,  2001;  Linial  et  al, 
1995].  Among embedding  into  normed  spaces,  embedding 
into an Euclidean space is the most popular. 

Given  the  dissimilarity  matrix 

we are in(cid:173)
terested in the question of whether and how the dissimilarity 
can  be embedded.  In  other words,  for the origi(cid:173)
matrix 
nal points 
we  attempt to  find  a configuration 
of points 
such that 
the squared distances between these points  will  be equal  to 
the so-defined dissimilarities  

in some Euclidean space 

The following theorem confirms that 

Theorem 1  Define d ij ~ 
D — 
properties: 

is metric. In other words, 

with 

can be embedded. 
as in (2).  The matrix 
satisfies the following 

The  proof  of  this  theorem  can  be  found  in  IZhang  et  al, 
2003].  The  subsequent  task  is  then  to  find  the  embedding, 
i.e., points 
such that the  inter-point dis(cid:173)
tance is equal to  dij  In general, obtaining an exact solution 
for  these  xi's  is  difficult.  Nevertheless,  because  D  is  met(cid:173)
ric, an approximate solution can be easily obtained by using 
principal coordinate analysis or other multidimensional scal(cid:173)
ing (MDS) methods [Cox and Cox, 2000].  We will return to 
this problem in Section 3. 

Notice that the resultant Euclidean embedding will still in(cid:173)
corporate information from both the input space representa(cid:173)
tion  (Xi) of the patterns  and their corresponding class  labels 

(tl).  Moreover, the distance metric dtJ, like the associated 
enjoys those nice properties useful for pattern discrimination. 

3  Metric Learning with Regression Model 
As  mentioned  in  Section  2,  an  approximate  solution  for 
can be obtained by using MDS. However, this 
may be intractable for large data sets. Moreover, for new pat(cid:173)
terns  with  unknown  labels,  the  problem  then  is  on  how  to 
determine sl3  in the first place. Following [Koontz and Fuku-
naga,  1972; Cox and Ferry,  1993; Webb,  1995], we attempt 
to find a mapping  from  x,  in  the original  input space 
to 
One possibility is to 
first obtain a MDS configuration, and then construct a regres(cid:173)
sion  model  from  xt  to 
iCox and Ferry,  1993].  However, 
this mapping is not determined as part of the MDS procedure 
[Webb,  1995].  In the following, we will follow the approach 
of [Webb,  19951. 

in the embedded Euclidean space 

Denote the mapping from the original input space 

embedded Euclidean space 
that each  fi  is a linear combination of p basis functions: 

to the 
by f — ( f 1 , . . ., fi)'. Assume 

(3) 

where  W  = 

contains the free parameters, and the 
are  basis  functions  that  can  be  linear  or  nonlinear. 
The regression mapping (3) can be written in matrix form as 

Let X be the target con(cid:173)
where 
figuration,  with 
is defined in (2). 
Using the iterative majorization algorithm, we then minimize 
the squared error 

where 

(4) 

4  Experiments 
In  this  Section,  we perform  experiments  on  six  benchmark 
data sets (Table 1) from the UCI repository [Murphy and Aha, 
1994]. The distance metric is learned using a small subset of 
the labeled patterns, with /  = p = q, 
=  x and the width 
in (1) set to the average distance of the labeled patterns to 
the class  means.  The  remaining  patterns  are  then  used  for 
testing. 

Table 2 shows the classification results by the nearest mean 
and nearest neighbor classifiers, with both the Euclidean and 
learned metrics.  As can be seen, the learned  metric almost 
always outperforms the original metric. 

Next,  we  perform  clustering  experiments  using  the  A;-
means clustering algorithm, with the value of k set to the true 
number of clusters  in  each  data  set.  The  clustered  patterns 
are assigned  labels and the clustering accuracy  is measured 
by comparing these  labels  with the true labels  (as in classi(cid:173)
fication problems).  As  these cluster labels can  be permuted 

POSTER  PAPERS 

1451 

Table  1: The six UCI data sets used in the experiments. 
# patterns for 
# patterns for 
metric  learning 
metric  learning 

total # of patterns 
class 

data set 
data set 

Pima Indians 

diabetes (diabetes) 

soybean 

wine 

Wisconsin breast 
cancer (WBC) 

ionosphere 

iris 

1 
2 
1 
2 
3 
4 
1 
2 
3 
1 
2 
1 
2 
1 
2 
3 

size 
500 
268 
10 
10 
10 
17 
59 
71 
48 
212 
357 
126 
225 
50 
50 
50 

80 
50 
2 
2 
2 
4 
20 
20 
20 
50 
50 
50 
50 
10 
10 
10 

Table 3:  Clustering accuracies on the UCI data sets (Numbers 
in bold indicate the better results). 

data set 
diabetes 
soybean 

wine 
WBC 

ionosphere 

iris 

Euclidean metric 

learned metric 

459/638 
37/37 
85/118 
412/469 
168/251 
107/120 

480/638 
37/37 
117/118 
446/469 
221/251 
110/120 

References 
[Cox and Cox, 2000]  T.F.  Cox  and  M.A.A.  Cox.  Multidi(cid:173)
mensional Scaling. Chapman & Hall/CRC, second edition, 
2000. 

[Cox and Ferry,  1993]  T.F.  Cox and G.  Ferry.  Discriminant 
analysis using non-metric multidimensional  scaling.  Pat-
tern Recognition, 26(1): 145-153,  1993. 

[Gower and Legendre,  1986]  J.C.  Gower  and  P.  Legendre. 
Metric  and  Euclidean  properties  of dissimilarities  coeffi(cid:173)
cients.  Journal  of  Classification,  3:5-48,  1986. 

[Indyk, 2001]  P.  Indyk.  Algorithmic  applications  of  low-
In  Proceedings  of the 
distortion  geometric  embeddings. 
42nd Annual  IEEE  Symposium  on  Foundations  of Com(cid:173)
puter Science, pages 10-33, 2001. 

[Koontzand Fukunaga,  1972]  VV.L.G.  Koontz and K.  Fuku-
naga.  A nonlinear feature extraction algorithm using dis(cid:173)
tance  information. 
IEEE  Transactions  on  Computers, 
21(l):56-63,  1972. 

[Lini-dl  etal,  1995]  N.  Linial,  E.  London,  and  Y.  Rabi-
novich.  The geometry of graphs and some of its algorith(cid:173)
mic applications.  Combinatorica,  15(2):215-245,  1995. 

[Murphy and Aha,  1994]  P.M.  Murphy  and  D.W.  Aha. 
databases. 

UCI 
http://www.ics.uci.edu/~mlearn/MLRepository.html, 
1994. 

of  machine 

repository 

learning 

[Scholkopf, 2002]  B.  Scholkopf and  A.J.  Smola.  Learning 

with Kernels.  MIT Press, 2002. 

[Webb,  1995]  A.R.  Webb.  Multidimensional  scaling  by  it(cid:173)
erative majorization  using radial basis functions.  Pattern 
Recognition, 28(5):753-759,  1995. 

[Zhang et  al,  2003]  Z.  Zhang,  J.T.  Kwok  and  D.Y.  Yc-
ung.  Parametric  distance  metric  with  label  information. 
ftp://ftp.cs.ust.hk/pub/techreport/03/tr03-02.ps.gz,  2003. 

without changing the clustering solution, results reported here 
arc based on the labeling with the highest clustering accuracy. 
As can be seen from Table 3, the learned metric outperforms 
that with the original metric on all data sets. 

Table 2: Classification accuracies on the UCI data sets (Num(cid:173)
bers in bold indicate the better results). 

data set 
data set 

1 

nearest mean 

| 

nearest neighbor 

Euclidean 

1  metric 
diabetes  1 1  463/638 
36/37 
soybean 
86/118 
430/469 
159/251 
108/120 

wine 
WBC 

iris 

ionosphere 

Euclidean 

learned 
metric 
1  metric 
475/638 1  432/638 
35/37 
37/37 
77/118 
115/118 
451/469 
420/469 
212/251 
201/251 
110/120 
114/120 

learned 
metric 
425/638 
37/37 
117/118 
453/469 
225/251 
114/120 

5  Concluding Remarks 
In this paper, we proposed a new parametric method for dis(cid:173)
tance metric learning based on class  label  information.  Ex(cid:173)
periments on UCI data sets show promising results. 

The  current  work  can  be  extended  in  several  directions. 
First,  nonlinear basis  functions  can  be  used  to  improve  the 
approximation power of the regression mapping.  Second, al(cid:173)
though Theorem  1  states  that  the  dissimilarity  measure  in(cid:173)
duces a metric,  it is not clear whether the matrix is also Eu(cid:173)
clidean.  If this is the case, a new kernel can then be defined 
on  the joint  space  of the  input  space  and  class  label  space 
[Scholkopf,  2002].  Third,  in  addition  to  using  label  infor(cid:173)
mation, we will also incorporate manifold structure between 
neighboring patterns into our metric learning process. 

1452 

POSTER  PAPERS 


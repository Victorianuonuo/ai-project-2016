A Fast Normalized Maximum Likelihood Algorithm for Multinomial Data

Petri Kontkanen, Petri Myllym¨aki

Complex Systems Computation Group (CoSCo)

Helsinki Institute for Information Technology (HIIT)

University of Helsinki & Helsinki University of Technology

P.O. Box 9800, FIN-02015 HUT, Finland.
{Firstname}.{Lastname}@hiit.fi

Abstract

Stochastic complexity of a data set is deﬁned as the
shortest possible code length for the data obtainable
by using some ﬁxed set of models. This measure is
of great theoretical and practical importance as a
tool for tasks such as model selection or data clus-
tering.
In the case of multinomial data, comput-
ing the modern version of stochastic complexity,
deﬁned as the Normalized Maximum Likelihood
(NML) criterion, requires computing a sum with
an exponential number of terms. Furthermore, in
order to apply NML in practice, one often needs to
compute a whole table of these exponential sums.
In our previous work, we were able to compute this
table by a recursive algorithm. The purpose of this
paper is to signiﬁcantly improve the time complex-
ity of this algorithm. The techniques used here are
based on the discrete Fourier transform and the con-
volution theorem.

Introduction

1
The Minimum Description Length (MDL) principle devel-
oped by Rissanen [Rissanen, 1978; 1987; 1996] offers a well-
founded theoretical formalization of statistical modeling. The
main idea of this principle is to represent a set of models
(model class) by a single model imitating the behaviour of
any model in the class. Such representative models are called
universal. The universal model itself does not have to belong
to the model class as often is the case.

From a computer science viewpoint, the fundamental idea
of the MDL principle is compression of data. That is, given
some sample data, the task is to ﬁnd a description or code
of the data such that this description uses less symbols than
it takes to describe the data literally.
Intuitively speaking,
this approach can in principle be argued to produce the best
possible model of the problem domain, since in order to be
able to produce the most efﬁcient coding of data, one must
capture all the regularities present in the domain.

The MDL principle has gone through several evolutionary
steps during the last two decades. For example, the early re-
alization of the MDL principle, the two-part code MDL [Ris-
sanen, 1978], takes the same form as the Bayesian BIC cri-
terion [Schwarz, 1978], which has led some people to incor-

rectly believe that MDL and BIC are equivalent. The latest
instantiation of the MDL is not directly related to BIC, but
to the formalization described in [Rissanen, 1996]. Unlike
Bayesian and many other approaches, the modern MDL prin-
ciple does not assume that the chosen model class is correct.
It even says that there is no such thing as a true model or
model class, as acknowledged by many practitioners. The
model class is only used as a technical device for constructing
an efﬁcient code. For discussions on the theoretical motiva-
tions behind the modern deﬁnition of the MDL see, e.g., [Ris-
sanen, 1996; Merhav and Feder, 1998; Barron et al., 1998;
Gr¨unwald, 1998; Rissanen, 1999; Xie and Barron, 2000;
Rissanen, 2001].

The most important notion of the MDL principle is the
Stochastic Complexity (SC), which is deﬁned as the shortest
description length of a given data relative to a model class M.
The modern deﬁnition of SC is based on the Normalized
Maximum Likelihood (NML) code [Shtarkov, 1987]. Unfor-
tunately, with multinomial data this code involves a sum over
all the possible data matrices of certain length. Computing
this sum, usually called the regret, is obviously exponential.
Therefore, practical applications of the NML have been quite
rare,

In our previous work [Kontkanen et al., 2003; 2005], we
presented a polynomial time (quadratic) method to compute
the regret. In this paper we improve our previous results and
show how mathematical techniques such as discrete Fourier
transform and convolution can be used in regret computation.
The idea of applying these techniques for computing a sin-
gle regret term was ﬁrst suggested in [Koivisto, 2004], but as
discussed in [Kontkanen et al., 2005], in order to apply NML
to practical tasks such as clustering, a whole table of regret
terms is needed. We will present here an efﬁcient algorithm
for this speciﬁc task. For a more detailed discussion of this
work, see [Kontkanen and Myllym¨aki, 2005].

2 NML for Multinomial Data
The most important notion of the MDL is the Stochastic Com-
plexity (SC). Intuitively, stochastic complexity is deﬁned as
the shortest description length of a given data relative to a
model class. To formalize things, let us start with a deﬁnition
of a model class. Consider a set Θ ∈ Rd, where d is a pos-
itive integer. A class of parametric distributions indexed by
the elements of Θ is called a model class. That is, a model

class M is deﬁned as

M = {P (· | θ) : θ ∈ Θ}.

(1)

Consider now a discrete data set (or matrix) xN =
(x1, . . . , xN ) of N outcomes, where each outcome xj is
an element of the set X consisting of all the vectors of the
form (a1, . . . , am), where each variable (or attribute) ai takes
on values v ∈ {1, . . . , ni}. Given a model class M, the Nor-
malized Maximum Likelihood (NML) distribution [Shtarkov,
1987] is deﬁned as

PNM L(xN | M) =

P (xN | ˆθ(xN ), M)

RN

M

,

(2)

where ˆθ(xN ) denotes the maximum likelihood estimate of
data xN , and RN

M is given by

P (xN | ˆθ(xN ), M),

(3)

RN

M =Xx

N

and the sum goes over all the possible data matrices of size N.
The term RN
M is called the regret. The deﬁnition (2) is intu-
itively very appealing: every data matrix is modeled using
its own maximum likelihood (i.e., best ﬁt) model, and then a
penalty for the complexity of the model class M is added to
normalize the distribution.

The stochastic complexity of a data set xN with respect to a
model class M can now be deﬁned as the negative logarithm
of (2), i.e.,

SC(xn | M) = − log

P (xN | ˆθ(xN ), M)

RN

M

(4)

= − log P (xN | ˆθ(xN ), M) + log RN

M. (5)
As in [Kontkanen et al., 2005], in the sequel we focus on
a multi-dimensional model class suitable for cluster analysis.
The selected model class has also been successfully applied
to mixture modeling [Kontkanen et al., 1996], case-based
reasoning [Kontkanen et al., 1998], Naive Bayes classiﬁca-
tion [Gr¨unwald et al., 1998; Kontkanen et al., 2000b] and
data visualization [Kontkanen et al., 2000a].

Let us assume that we have m variables, (a1, . . . , am), and
we also assume the existence of a special variable c (which
can be chosen to be one of the variables in our data or it can
be latent). Furthermore, given the value of c, the variables
(a1, . . . , am) are assumed to be independent. The resulting
model class is denoted by MT . Suppose the special vari-
able c has K values and each ai has ni values. The NML
distribution for the model class MT is now

that an efﬁcient way to compute the regret term is via the
following recursive formula:

RN

MT ,K =

N

Xr=0

N !

r!(N − r)!³ r

N´rµ N − r

N ¶N −r

· Rr

MT ,k1 · RN −r

MT ,k2,

(7)

where k1 + k2 = K.

As discussed in [Kontkanen et al., 2005], in order to ap-
ply NML to the clustering problem, we need to compute a
whole table of regret terms. This table consists of the terms
MT ,k for n = 0, . . . , N and k = 1, . . . , K, where K is the
Rn
maximum number of clusters.

The procedure of computing the regret table starts by ﬁll-
ing the ﬁrst column, i.e., the case k = 1, which is trivial
(see [Kontkanen et al., 2005]). To compute the column k,
for k = 2, . . . , K, the recursive formula (7) can be used by
choosing k1 = k − 1, k2 = 1. The time complexity of ﬁlling

the whole table is O¡K · N 2¢. For more details, see [Kon-

tkanen et al., 2005; Kontkanen and Myllym¨aki, 2005].

In practice, the quadratic dependency on the size of data
limits the applicability of NML to small or moderate size data
sets. In the next section, we will present a novel, signiﬁcantly
more efﬁcient method for computing the regret table.

3 The Fast NML Algorithm
In this section we will derive a very efﬁcient algorithm for
the regret table computation. The new method is based on
the Fast Fourier Transform algorithm. As mentioned in the
previous section, the calculation of the ﬁrst column of the
regret table is trivial. Therefore, we only need to consider the
case of calculating the column k given the ﬁrst k −1 columns.
Let us deﬁne two sequences a and b by

an =

nn
n!

Rn

MT ,k−1, bn =

nn
n!

Rn

MT ,1,

(8)

for n = 0, . . . , N. Evaluating the convolution of a and b
gives

(a ∗ b)n =

=

n

Xh=0

nn
n!

hh
h!

n

Xh=0

Rh

MT ,k−1

(n − h)n−h

(n − h)!

Rn−h
MT ,1

(9)

n!

h!(n − h)!µ h

n¶hµ n − h

n ¶n−h

· Rh

MT ,k−1Rn−h

MT ,1

=

nn
n!

Rn

MT ,k,

(10)

(11)

PNM L(xN | MT ) =" K

Yk=1µ hk

N ¶hk m
Yi=1

K

Yk=1

ni

Yv=1µ fikv

hk ¶fikv#

·

1
RN

MT ,K

,

(6)

where hk is the number of times c has value k in xN , fikv is
the number of times ai has value v when c = k, and RN
MT ,K
is the regret term. In [Kontkanen et al., 2005] it was proven

where the last equality follows from the recursion for-
mula (7). This derivation shows that the column k can be
computed by ﬁrst evaluating the convolution (11), and then
multiplying each term by n!/nn.

The standard convolution theorem states that convolutions
can be evaluated via the (discrete) Fourier transform, which in
turn can be computed efﬁciently with the Fast Fourier Trans-
form algorithm (see [Kontkanen and Myllym¨aki, 2005] for
details). It follows that the time complexity of computing the

whole regret table drops to O (N log N · K). This is a ma-

jor improvement over O¡N 2 · K¢ obtained by the recursion

method of Section 2.

4 Conclusion And Future Work
The main result of this paper was a derivation of a novel algo-
rithm for the regret table computation. The theoretical time
complexity of this algorithm allows practical applications of
NML in domains with very large datasets. With the earlier
quadratic-time algorithms, this was not possible.

In the future, we plan to conduct an extensive set of em-
pirical tests to see how well the theoretical advantage of the
new algorithm transfers to practice. On the theoretical side,
our goal is to extend the regret table computation to more
complex cases like general graphical models. We will also
research supervised versions of the stochastic complexity, de-
signed for supervised prediction tasks such as classiﬁcation.

Acknowledgements
This work was supported in part by the Academy of Finland
under the projects Minos and Civi and by the National Tech-
nology Agency under the PMMA project. In addition, this
work was supported in part by the IST Programme of the Eu-
ropean Community, under the PASCAL Network of Excel-
lence, IST-2002-506778. This publication only reﬂects the
authors’ views.

References
[Barron et al., 1998] A. Barron, J. Rissanen, and B. Yu. The
minimum description principle in coding and modeling.
IEEE Transactions on Information Theory, 44(6):2743–
2760, October 1998.

[Gr¨unwald et al., 1998] P. Gr¨unwald, P. Kontkanen, P. Myl-
lym¨aki, T. Silander, and H. Tirri. Minimum encod-
ing approaches for predictive modeling.
In G. Cooper
and S. Moral, editors, Proceedings of the 14th Interna-
tional Conference on Uncertainty in Artiﬁcial Intelligence
(UAI’98), pages 183–192, Madison, WI, July 1998. Mor-
gan Kaufmann Publishers, San Francisco, CA.

[Gr¨unwald, 1998] P. Gr¨unwald. The Minimum Description
Length Principle and Reasoning under Uncertainty. PhD
thesis, CWI, ILLC Dissertation Series 1998-03, 1998.

[Koivisto, 2004] M. Koivisto. Sum-Product Algorithms for
the Analysis of Genetic Risks. PhD thesis, Report A-
2004-1, Department of Computer Science, University of
Helsinki, 2004.

[Kontkanen and Myllym¨aki, 2005] P. Kontkanen and P. Myl-
lym¨aki. Computing the regret table for multinomial data.
Technical Report 2005-1, Helsinki Institute for Informa-
tion Technology (HIIT), 2005.

[Kontkanen et al., 1996] P. Kontkanen, P. Myllym¨aki, and
H. Tirri. Constructing Bayesian ﬁnite mixture models
by the EM algorithm. Technical Report NC-TR-97-003,
ESPRIT Working Group on Neural and Computational
Learning (NeuroCOLT), 1996.

[Kontkanen et al., 1998] P. Kontkanen, P. Myllym¨aki, T. Si-
lander, and H. Tirri. On Bayesian case matching.
In
B. Smyth and P. Cunningham, editors, Advances in Case-
Based Reasoning, Proceedings of the 4th European Work-
shop (EWCBR-98), volume 1488 of Lecture Notes in Arti-
ﬁcial Intelligence, pages 13–24. Springer-Verlag, 1998.

[Kontkanen et al., 2000a] P. Kontkanen, J. Lahtinen, P. Myl-
lym¨aki, T. Silander, and H. Tirri. Supervised model-based
visualization of high-dimensional data.
Intelligent Data
Analysis, 4:213–227, 2000.

[Kontkanen et al., 2000b] P. Kontkanen, P. Myllym¨aki, T. Si-
lander, H. Tirri, and P. Gr¨unwald. On predictive distribu-
tions and Bayesian networks. Statistics and Computing,
10:39–54, 2000.

[Kontkanen et al., 2003] P. Kontkanen, W. Buntine, P. Myl-
lym¨aki, J. Rissanen, and H. Tirri. Efﬁcient computation of
stochastic complexity. In C. Bishop and B. Frey, editors,
Proceedings of the Ninth International Conference on Ar-
tiﬁcial Intelligence and Statistics, pages 233–238. Society
for Artiﬁcial Intelligence and Statistics, 2003.

[Kontkanen et al., 2005] P. Kontkanen,

P. Myllym¨aki,
W. Buntine, J. Rissanen, and H. Tirri. An MDL frame-
In P. Gr¨unwald, I.J. Myung,
work for data clustering.
and M. Pitt, editors, Advances in Minimum Description
Length: Theory and Applications. The MIT Press, 2005.

[Merhav and Feder, 1998] N. Merhav and M. Feder. Univer-
sal prediction. IEEE Transactions on Information Theory,
44(6):2124–2147, October 1998.

[Rissanen, 1978] J. Rissanen. Modeling by shortest data de-

scription. Automatica, 14:445–471, 1978.

[Rissanen, 1987] J. Rissanen. Stochastic complexity. Jour-
nal of the Royal Statistical Society, 49(3):223–239 and
252–265, 1987.

[Rissanen, 1996] J. Rissanen.

information and
stochastic complexity. IEEE Transactions on Information
Theory, 42(1):40–47, January 1996.

Fisher

[Rissanen, 1999] J. Rissanen. Hypothesis selection and test-
ing by the MDL principle. Computer Journal, 42(4):260–
269, 1999.

[Rissanen, 2001] J. Rissanen. Strong optimality of the nor-
malized ML models as universal codes and informa-
tion in data.
IEEE Transactions on Information Theory,
47(5):1712–1717, July 2001.

[Schwarz, 1978] G. Schwarz. Estimating the dimension of a

model. Annals of Statistics, 6:461–464, 1978.

[Shtarkov, 1987] Yu M. Shtarkov. Universal sequential cod-
ing of single messages. Problems of Information Trans-
mission, 23:3–17, 1987.

[Xie and Barron, 2000] Q. Xie and A.R. Barron. Asymp-
totic minimax regret for data compression, gambling, and
prediction.
IEEE Transactions on Information Theory,
46(2):431–445, March 2000.


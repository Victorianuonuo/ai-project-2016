Learning to Play Like the Great Pianists

Asmir Tobudic

Austrian Research Institute for
Arti(cid:2)cial Intelligence, Vienna

asmir.tobudic@ofai.at

Gerhard Widmer

Department of Computational Perception,

Johannes Kepler University, Linz
Austrian Research Institute for
Arti(cid:2)cial Intelligence, Vienna

gerhard.widmer@jku.at

Abstract

An application of relational instance-based learn-
ing to the complex task of expressive music per-
formance is presented. We investigate to what ex-
tent a machine can automatically build ‘expres-
sive pro(cid:2)les’ of famous pianists using only min-
imal performance information extracted from au-
dio CD recordings by pianists and the printed
score of the played music.
It turns out that the
machine-generated expressive performances on un-
seen pieces are substantially closer to the real per-
formances of the ‘trainer’ pianist than those of all
others. Two other interesting applications of the
work are discussed: recognizing pianists from their
style of playing, and automatic style replication.

Introduction

1
Relational
instance-based learning is a machine learning
paradigm that tries to transfer the successful nearest-neighbor
or instance-based learning (IBL) framework to richer (cid:2)rst-
order logic (FOL) representations [Emde and Wettschereck,
1996; Tobudic and Widmer, 2005]. As such it is a part of in-
ductive logic programming (ILP), the (cid:2)eld of research which
(cid:150) after the euphoria in the nineties (cid:150) suffered a certain (cid:3)at-
tening of interest in recent years, the main reason being the
dif(cid:2)culties of effectively constraining the extremely large hy-
pothesis spaces. Nevertheless, some ILP systems have re-
cently been shown to achieve performance levels competi-
tive to those of human experts in very complex domains (e.g.
[King et al., 2004]).

A successful application of relational IBL to a real-world
task from classical music has been presented in previous work
([Tobudic and Widmer, 2005]). A system that predicts ex-
pressive timing and dynamics patterns for new pieces by anal-
ogy to similar musical passages in a training set has been
shown to learn to play piano music ‘expressively’ with sub-
stantial musical quality.

Here we investigate an even more interesting question: can
a relational learner learn models that are ‘personal’ enough
to capture some aspects of the playing styles of truly great pi-
anists? A system is presented that builds performance models
of six famous pianists using only crude information related to
expressive timing and dynamics obtained from the pianist’s

CD recordings and the printed score of the music. Exper-
iments show that the system indeed captures some aspect of
the pianists’ playing style: the machine’s performances of un-
seen pieces are substantially closer to the real performances
of the ‘training’ pianist than those of all other pianists in our
data set. An interesting by-product of the pianists’ ‘expres-
sive models’ is demonstrated: the automatic identi(cid:2)cation of
pianists based on their style of playing. And (cid:2)nally, the ques-
tion of automatic style replication is brie(cid:3)y discussed.

The rest of the paper is laid out as follows. After a short
introduction to the notion of expressive music performance
(Section 2), Section 3 describes the data and its representa-
tion in FOL. We also discuss how the complex task of learn-
ing expressive curves can be decomposed into a well-de(cid:2)ned
instance-based learning task and shortly recapitulate the de-
tails of the relational instance-based learner DISTALL. Ex-
perimental results are presented in Section 4.

2 Expressive Music Performance
Expressive music performance is the art of shaping a musi-
cal piece by continuously varying important parameters like
tempo, loudness, etc. while playing [Widmer et al., 2003].
Human musicians do not play a piece of music mechanically,
with constant tempo or loudness, exactly as written in the
printed music score. Rather, skilled performers speed up at
some places, slow down at others, stress certain notes or pas-
sages etc. The most important parameter dimensions avail-
able to a performer are timing (tempo variations) and dynam-
ics (loudness variations). The way these parameters ‘should
be’ varied is not speci(cid:2)ed precisely in the printed score; at the
same time it is what makes a piece of music come alive, and
what distinguishes great artists from each other.

Tempo and loudness variations can be represented as
curves that quantify these parameters throughout a piece rel-
ative to some reference value (e.g.
the average loudness or
tempo of the same piece). Figure 1 shows a dynamics curve
of a small part of Mozart’s Piano Sonata K.280, 1st move-
ment, as played by (cid:2)ve famous pianists. Each point gives the
relative loudness (relative to the average loudness of the piece
by the same performer) at a given point in the piece. A purely
mechanical (unexpressive) rendition of the piece would cor-
respond to a (cid:3)at horizontal line at y = 1:0. Tempo variations
can be represented in an analogous way. In the next section
we discuss how predictive models of such curves can be auto-

D. Barenboim
G. Gould
M. J. Pires
A. Schiff
M. Uchida

i

s
c
m
a
n
y
d
 
.
l
e
r

1.6

1.4

1.2

1

0.8

0.6

0.4

32

34

36

38

40

bars

42

44

46

48

Figure 1: Dynamics curves of performances of (cid:2)ve famous
pianists for Mozart Sonata K.280, 1st mvt., mm. 31(cid:150)48. Each
point represents the relative loudness at the beat level (relative
to the average loudness of the piece by the same performer).

matically learned from information extracted from audio CD
recordings and the printed score of the music.

3 Data and Methodology
The data used in this work was obtained from commercial
recordings of famous concert pianists. We analyzed the per-
formances of 6 pianists across 15 different sections of piano
sonatas by W.A.Mozart. The pieces selected for analysis are
complex, different in character, and represent different tempi
and time signatures. Tables 1 and 2 summarize the pieces,
pianists and recordings selected for analysis.

For learning tempo and dynamics ‘pro(cid:2)les’ of the pianists
in our data set we extract time points from the audio record-
ings that correspond to beat 1 locations. From the (varying)
time intervals between these points, the beat-level tempo and
its changes can be computed. Beat-level dynamics is com-
puted from the audio signal as the overall loudness of the
signal at the beat times as a very crude representation of the
dynamics applied by the pianists. Extracting such informa-
tion from the CD recordings was an extremely laborious task,
even with the help of an intelligent interactive beat tracking
system [Dixon, 2001]. From these measurements, computing
pianists’ dynamics and tempo performance curves as shown
in Figure 1 (cid:150) which are the raw material for our experiments
(cid:150) is rather straightforward.

An examination of the dynamics curves in Figure 1 re-
veals certain trends common for all pianists (e.g. up-down,
crescendo-decrescendo tendencies). These trends re(cid:3)ect cer-
tain aspects of the underlying structure of the piece: A piece
of music commonly consists of phrases (cid:150) segments that are
heard and interpreted as coherent units. Phrases are organized
hierarchically: smaller phrases are grouped into higher-level
phrases, which are in turn grouped together, constituting a
musical context at a higher level of abstraction etc. In Figure
1, the hierarchical, three-level phrase structure of this passage
is indicated by three levels of brackets at the bottom. In this
work we aim at learning expressive patterns at different lev-
els of such a phrase structure, which roughly corresponds to
various levels of musical abstraction.

1The beat is the time points where listeners would tap their foot

along with the music.

Table 1: Mozart sonata sections selected for analysis. Sec-
tion ID should be read as <sonataName> : <movement> :
<section>. The total numbers of phrases are also shown.

Section ID Tempo descr.
kv279:1:1
kv279:1:2
kv280:1:1
kv280:1:2
kv280:2:1
kv280:2:2
kv280:3:1
kv280:3:2
kv282:1:1
kv282:1:2
kv283:1:1
kv283:1:2
kv283:3:1
kv283:3:2
kv332:2

fast 4/4
fast 4/4
fast 3/4
fast 3/4
slow 6/8
slow 6/8
fast 3/8
fast 3/8
slow 4/4
slow 4/4
fast 3/4
fast 3/4
fast 3/8
fast 3/8
slow 4/4

#phrases

460
753
467
689
129
209
324
448
199
254
455
519
408
683
549

Table 2: Pianists and recordings.

Daniel Barenboim EMI Classics CDZ 7 67295 2, 1984

Gramola 98701-705, 1990

Sony Classical SM4K 52627, 1967

Pianist name

ID
DB
RB
GG
MP Maria Jo(cid:152)ao Pires
Andr·as Schiff
AS
MU
Mitsuko Uchida

Roland Batik
Glenn Gould

Recording

DGG 431 761-2, 1991

ADD (Decca) 443 720-2, 1980
Philips Classics 464 856-2, 1987

3.1 Phrase Representation in FOL
Phrases and relations between them can be naturally repre-
sented in (cid:2)rst-order logic. In our collection of pieces, phrases
are organized at three hierarchical levels, based on a man-
ual phrase structure analysis. The musical content of each
phrase is encoded in the predicate phrCont/18.
It has the
form phrCont(Id,A1,A2,...), where Id is the phrase identi(cid:2)er
and A1,A2,... are attributes that describe very basic phrase
properties. The (cid:2)rst seven of these are numeric: the length
of a phrase, the relative position of the highest melodic point
(the ‘apex’), the melodic intervals between starting note and
apex, and between apex and ending note, metrical strengths
of starting note, apex, and ending note. The next three at-
tributes are discrete: the harmonic progression between start,
apex, and end, and two boolean attributes that state whether
the phrase ends with a ‘cumulative rhythm’, and whether it
ends with a cadential chord sequence. The remaining at-
tributes describe (cid:150) in addition to some simple information
about global tempo and the presence of trills (cid:150) global char-
acteristics of the phrases in statistical terms: mean and vari-
ance of the durations of the melody notes within the phrase
(as rough indicators of the general ‘speed’ of events and of
durational variability), and mean and variance of the sizes of
the melodic intervals between the melody notes (as measures
of the ‘jumpiness’ of the melodic line).

This propositional representation ignores an essential as-
its temporal nature. The temporal re-

pect of the music:

1.5

i

s
c
m
a
n
y
d

 
.
l

e
r

1

0.5

31

32

33

34

35

score position (bars)

36

37

38

39

Figure 2: Multilevel decomposition of dynamics curve of per-
formance of Mozart Sonata K.279:1:1, mm.31-38: original
dynamics curve plus the second-order polynomial shapes giv-
ing the best (cid:2)t at four levels of phrase structure.

lationships between successive phrases can be naturally ex-
pressed in FOL, as a relational predicate succeeds(Id2,Id1),
which simply states that the phrase Id2 succeeds the same-
level phrase Id1. Supplying the same information in a propo-
sitional representation would be very dif(cid:2)cult.

What is still needed in order to learn are the training exam-
ples, i.e. for each phrase in the training set, we need to know
how it was played by a musician. This information is given
in the predicate phrShape(Id,Coeffs), where Coeffs encode in-
formation about the way the phrase was played by a pianist.
This is computed from the tempo and dynamics curves, as
described in the following section.
3.2 Deriving the Training Instances: Multilevel

Decomposition of Performance Curves

Given a complex tempo or dynamics curve and the under-
lying phrase structure (see Figure 1), we need to calculate
the most likely contribution of each phrase to the overall ob-
served expression curve, i.e., we need to decompose the com-
plex curve into basic expressive phrase ‘shapes’. As approxi-
mation functions to represent these shapes we decided to use
the class of second-degree polynomials (i.e., functions of the
form y = ax2 + bx + c), because there is ample evidence
from research in musicology that high-level tempo and dy-
namics are well characterized by quadratic or parabolic func-
tions [Todd, 1992]. Decomposing a given expression curve
is an iterative process, where each step deals with a speci(cid:2)c
level of the phrase structure: for each phrase at a given level,
we compute the polynomial that best (cid:2)ts the part of the curve
that corresponds to this phrase, and ‘subtract’ the tempo or
dynamics deviations ‘explained’ by the approximation. The
curve that remains after this subtraction is then used in the
next level of the process. We start with the highest given level
of phrasing and move to the lowest. As tempo and dynamics
curves are lists of multiplicative factors (relative to a default
tempo), ‘subtracting’ the effects predicted by a (cid:2)tted curve
from an existing curve simply means dividing the y values on
the curve by the respective values of the approximation curve.
Figure 2 illustrates the result of the decomposition process
on the last part (mm.31(cid:150)38) of the Mozart Sonata K.279, 1st
movement, 1st section. The four-level phrase structure our
music analyst assigned to the piece is indicated by the four

levels of brackets at the bottom of the plot. The elementary
phrase shapes (at four levels of hierarchy) obtained after de-
composition are plotted in gray. We end up with a training
example for each phrase in the training set (cid:151) a predicate
phrShape(Id ; Coeﬁ ), where Coeﬁ = fa; b; cg are the coef-
(cid:2)cients of the polynomial (cid:2)tted to the part of the performance
curve associated with the phrase.

Input to the learning algorithm are the (relational) repre-
sentation of the musical scores plus the training examples
(i.e.
timing and dynamics polynomials), for each phrase in
the training set. Given a test piece the learner assigns the
shape of the most similar phrase from the training set to each
phrase in the test piece. In order to produce (cid:2)nal tempo and
dynamics curves, the shapes predicted for phrases at differ-
ent levels must be combined. This is simply the inverse of the
curve decomposition problem: Given a new piece, the system
starts with an initial ‘(cid:3)at’ expression curve (i.e., a list of 1.0
values) and then successively multiplies the current values by
the multi-level phrase predictions.
3.3 DISTALL, a Relational Instance-based

Learner

We approach phrase-shape prediction with a straightforward
nearest-neighbour (NN, IBL) method. Standard propositional
k-NN is not applicable to our data representation discussed in
section 3.1. Instead, we use DISTALL, an algorithm that gen-
eralizes propositional k-NN to examples described in (cid:2)rst-
order logic [Tobudic and Widmer, 2005].

DISTALL is a representative of the line of research (cid:2)rst
initiated in [Bisson, 1992], where a clustering algorithm to-
gether with its similarity measure was presented. This work
was later improved in [Emde and Wettschereck, 1996], in
the context of the relational instance-based learning algorithm
RIBL, which in turn can be regarded as DISTALL’s predeces-
sor. We skip technical details here, but the main idea behind
DISTALL’s similarity measure is that the similarity between
two objects depends not only on the similarities of their at-
tributes, but also on the similarities of the objects related to
them. The similarities of the related objects depend in turn
on their attributes and related objects. For our music learning
task it means that the ‘shaping’ of the current (test) phrase de-
pends not only on its attributes, but also on the preceding and
succeeding music (through the relation succeeds(Id1,Id2), see
section 3.1), which is (cid:150) from a musical point of view (cid:150) a rather
intuitive idea. For a more detailed description of DISTALL
see [Tobudic and Widmer, 2005].

Experimental results with DISTALL on MIDI-like (i.e.,
very detailed) performance data produced by a local pianist
are reported in [Tobudic and Widmer, 2005]. The new contri-
bution of the current paper is that we have laboriously mea-
sured audio recordings by truly famous artists and can show
(cid:151) for the (cid:2)rst time (cid:151) that DISTALL actually succeeds in
capturing something of personal artistic performance style.

4 Experiments
4.1 Learning Predictive Performance Models
For each pianist, we conducted a systematic leave-one-piece-
out cross-validation experiment: each of 15 pieces was once

Table 3: Results of piecewise cross-validation experiment.
The table cells list correlations between learned and real
curves, where rows indicate the ‘training pianist’, and
columns the pianist whose real performance curves are used
for comparison. The correlations are averaged over all pieces,
weighted by the relative length of the piece. Each cell is fur-
ther divided into two rows corresponding to dynamics and
tempo correlations, respectively. The highest correlations in
each row are printed in bold.

DB

RB

AS

compared with

learned from DB RB GG MP AS MU
.28
.31
.17
.27
.22
.28
.28
.34
.26
.32
.38
.50

.44
.44
.21
.28
.25
.25
.33
.31
.36
.32
.27
.34

.21
.27
.32
.42
.09
.18
.19
.23
.17
.29
.18
.30

.26
.26
.09
.20
.36
.32
.19
.27
.20
.28
.21
.32

.34
.32
.19
.22
.19
.23
.39
.38
.31
.25
.28
.36

.38
.31
.19
.30
.21
.29
.33
.28
.40
.41
.26
.37

MU

GG

MP

left aside as a test piece while the remaining 14 performances
(by the same pianist) were used for learning. DISTALL’s pa-
rameter for the number of nearest neighbors was set to 1, and
the parameter for the depth of starting clauses (see [Tobudic
and Widmer, 2005]) to 4 (meaning that the distance between
two phrases can be in(cid:3)uenced by at most 4 preceding and 4
succeeding phrases).

The expressive shapes for each phrase in a test piece are
predicted by DISTALL and then combined into a (cid:2)nal tempo
and dynamics curve, as described in section 3.2. The result-
ing curves are then compared to the real performance curves
of all pianists (for the same test piece). If the curve learned
from the performances of one pianist is more similar to the
real performance curve of the ‘teacher’ pianist than to those
of all other pianists, we could conclude that the learner suc-
ceeded in capturing something of the pianist’s speci(cid:2)c play-
ing style. The described procedure is repeated for all pieces
and all pianists in our data set.

Correlation is chosen as a measure of how well the pre-
dicted curve ‘follows’ the real one. The curves are (cid:2)rst nor-
malized so that their autocorrelations are identically 1, giv-
ing a correlation estimate between curves as a number in the
range [-1,1]. The results of the cross-validation experiment
averaged over all pieces (weighted by the relative length of
the pieces) are given in Table 3.

Interestingly, the system succeeded in learning curves that
are substantially closer to the ‘trainer’ than all others, for all
pianists. Some of the pianists are better ‘predictable’ than
others, e.g. Daniel Barenboim and Mitsuko Uchida, which
might indicate that they play Mozart in a more ‘consistent’
way. While at (cid:2)rst sight the correlations may not seem im-
pressive, one should keep in mind that artistic performance
is far from predictable, and the numbers in Table 3 are aver-
ages over all pieces (about half an hour of concert-level piano

Table 4: Detailed results (tempo dimension) of the cross-
validation experiment with Mitsuko Uchida was the ‘training’
pianist. For each piece, the correlations between predicted
curve and actual tempo curves from all pianists are given.
The average over all pieces is given in the last row (repro-
duced from the last row of Table 3)

Piece

kv279:1:1
kv279:1:2
kv280:1:1
kv280:1:2
kv280:2:1
kv280:2:2
kv280:3:1
kv280:3:2
kv282:1:1
kv282:1:2
kv283:1:1
kv283:1:2
kv283:3:1
kv283:3:2
kv332:2
Total

DB RB GG MP AS MU
.50
.36
.47
.31
.78
.40
.54
.42
.69
.53
.58
.44
.77
.72
.51
.51
.72
.38
.52
.23
.31
.21
.32
.17
.52
.30
.36
.19
.35
.32
.50
.34

.36
.28
.55
.37
.58
.42
.82
.49
.44
.42
.16
.23
.42
.21
.22
.36

.29
.34
.42
.47
.59
.50
.68
.43
.44
.46
.22
.22
.42
.32
.16
.37

.22
.35
.26
.42
.52
.39
.62
.44
.36
.33
.12
.21
.42
.29
.14
.32

.31
.35
.25
.37
.34
.13
.66
.52
.45
.36
.10
.18
.28
.15
.17
.30

music per artist). Moreover, the correlation estimates in Ta-
ble 3 are somewhat unfair, since we compare the performance
curve produced by composing the polynomials predicted by
the learner, with the curve corresponding to the pianists’ ac-
tual performances. However, what DISTALL learned from
was not the actual performance curves, but an approximation
curve which is implied by the three levels of quadratic func-
tions that were used as training examples. Correctly predict-
ing these is the best the learner could hope to achieve.

Table 4 shows a more detailed picture of the cross-
validation experiment, where the training pianist was Mitsuko
Uchida and the numbers refer to correlations in the tempo
domain.
In 13 out of 15 cases the learner produces tempo
curves which are closer to Uchida’s playing than to any other
pianist, with correlations of .7 and better (e.g., for kv280:3:1
and kv280:1:1). The results are even more interesting if we
recall that the learner is given a very crude, beat-level repre-
sentation of the tempo and dynamics applied by the pianist,
without any details about e.g. individual voices or timing de-
tails below the level of beat. On the other hand, the piecewise
results revealed that some of the pianists (e.g. Gould or Pires)
seem to be less ‘predictable’ with our approach than Uchida
(not reported here for lack of space).

Figure 3 shows an example of successful performance style
learning. We see a passage from a Mozart piano sonata as
‘played’ by the computer after learning from recordings of
other pieces by Daniel Barenboim (top) and Mitsuko Uchida
(bottom), respectively. Also shown are the performance
curves corresponding to these two pianists’ actual perfor-
mances of the test piece. In this case it is quite clearly visible
that the curves predicted by the computer on the test piece are
much more similar to the curves by the respective ‘teacher’
than to those by the other pianist.

D. Barenboim
M. Uchida
learned (Barenboim)

2

1.5

1

0.5

i

s
c
m
a
n
y
d
 
.
l
e
r

0
120

125

130

135

D. Barenboim
M. Uchida
learned (Barenboim)

1.4

1.2

1

0.8

o
p
m
e
t

0.6

120

125

130

135

D. Barenboim
M. Uchida
learned (Uchida)

2

1.5

1

0.5

i

s
c
m
a
n
y
d
 
.
l
e
r

0
120

125

130

135

D. Barenboim
M. Uchida
learned (Uchida)

1.4

1.2

1

0.8

o
p
m
e
t

0.6

120

125

130

135

140
bars

140
bars

140
bars

140
bars

145

150

155

160

145

150

155

160

145

150

155

160

145

150

155

160

Figure 3: Dynamics and tempo curves produced by DISTALL
on test piece (Sonata K.283, 3rd mvt., 2nd section, mm.120(cid:150)
160) after learning from Daniel Barenboim (top) and Mitsuko
Uchida (bottom), compared to the artists’ real curves as mea-
sured from the recordings.

Admittedly, this is a carefully selected example, one of the
clearest cases of style replication we could (cid:2)nd in our data.
The purpose of this example is more to give an indication of
the complexity of the curve prediction task and the difference
between different artists’ interpretations than to suggest that a
machine will always be able to achieve this level of prediction
performance.

Identi(cid:2)cation of Great Pianists

4.2
The primary goal of our work is learning predictive mod-
els of pianists’ expressive performances.2 But the models
can also be used in a straightforward way for recognizing pi-
anists. The problem of identifying famous pianists from in-
formation obtained from audio recordings of their playing has
been addressed in the recent literature [Saunders et al., 2004;
Stamatatos and Widmer, 2002; Widmer and Zanon, 2004]. In
[Widmer and Zanon, 2004], a number of low-level scalar fea-
tures related to expressive timing and dynamics are extracted
from the audio CD recordings, and various machine learning

2Note that learned tempo and dynamics curves as produced by
our system can be used to build truly machine generated expressive
performances: using the predicted tempo and dynamics curves (i.e.
relative tempo and dynamics for each beat in the piece), it is straight-
forward to calculate tempo and dynamics for each note in the piece
(e.g. by interpolation).

Table 5: Confusion matrix of the pianist classi(cid:2)cation exper-
iment. Rows correspond to the test performances of each pi-
anist (15 per row), columns to the classi(cid:2)cations made by the
system. The rightmost column gives the accuracy achieved
for all performances of the respective. The baseline accuracy
in this 6-class problem is 16.67%.
prediction

pianist DB RB GG MP AS MU Acc.[%]

2
0
0
12
0
0
-

2
0
0
0
10
0
-

0
1
3
2
2
14
-

73.3
80.0
66.7
80.0
66.7
93.3
76.7

DB
RB
GG
MP
AS
MU
Total

11
1
1
0
1
0
-

0
12
1
0
0
0
-

0
1
10
1
2
1
-

algorithms are applied to these. In [Saunders et al., 2004], the
sequential nature of music is addressed by representing per-
formances as strings and using string kernels in conjunction
with kernel partial least squares and support vector machines.
The string kernel approach is shown to achieve better perfor-
mance than the best results obtained in [Widmer and Zanon,
2004]. A clear result from both works is that identi(cid:2)cation of
pianists from their recordings is an extremely dif(cid:2)cult task.

The pianists studied in the present paper are identical
to those in [Widmer and Zanon, 2004] and [Saunders et
al., 2004]; unfortunately, the sets of recordings differ con-
siderably (because manual phrase structure analyses, which
are needed in our approach, were available only for certain
pieces), so a direct comparison of the results is impossible.
Still, to illustrate what can be achieved with a relational rep-
resentation and learning algorithm, we brie(cid:3)y describe a clas-
si(cid:2)cation experiment with DISTALL.

Each of the 15 pieces is set aside once. The 6 performances
of that piece (one by each pianist) are used as test instances.
A model of each pianist is built from his/her performances of
the remaining 14 pieces. The result is two predicted curves
per pianist for the test piece (for tempo and dynamics), which
we call model curves. The (cid:2)nal classi(cid:2)cation of a pianist,
represented by his/her tempo and dynamics curves tt and td
on the test piece, is then determined as

corr(tt; mpt) + corr(td; mpd)

2

c(tt; td) = argmaxp2P (

)
(1)
where P is set of all pianists and mpt and mpd are the pi-
anists’ model tempo and dynamics curves. In other words, the
performance is classi(cid:2)ed as belonging to the pianist whose
model curves exhibit the highest correlation (averaged over
tempo and dynamics) with the test curves. For each pianist,
DISTALL is tested on the 15 test pieces, which gives a to-
tal number of 90 test performances. The baseline accuracy
(cid:150) the success rate of pure guessing (cid:150) is 15, or 16.67%. The
confusion matrix of the experiment is given in Table 5.

Again, it turns out that the artists are identi(cid:2)able to varying
degrees, but the recognition accuracies are all clearly above
the baseline. In particular, note that the system correctly iden-

ti(cid:2)es performances by Uchida in all but one case. Obviously,
the learner succeeds in reproducing something of the artists’
styles in its model curves. While these (cid:2)gures seem to com-
pare very favourably to the accuracies reported in [Widmer
and Zanon, 2004] and [Saunders et al., 2004], they cannot
be compared directly, because different recordings were used
and, more importantly, the level of granularity of the train-
ing and test examples are different (movements in [Widmer
and Zanon, 2004; Saunders et al., 2004] vs. sections in this
paper), which probably makes our learning task easier.

4.3 Replicating Great Pianists?
Looking at Figure 3, one might be tempted to consider the
possibility of automatic style replication: wouldn’t it be inter-
esting to supply the computer with the score of a new piece
and have it perform it ‘in the style of’, say, Vladimir Horowitz
or Arthur Rubinstein? This question is invariably asked when
we present this kind of research to the public. Unfortunately
(?), the answer is: while it might be interesting, it is not cur-
rently feasible.

For one thing, despite the huge effort we invested in mea-
suring expressive timing and dynamics in recordings, the
amount of available training data is still vastly insuf(cid:2)cient
vis-a-vis the enormous complexity of the behaviour to be
learned. And secondly, the sort of crude beat-level varia-
tions in tempo and general loudness capture only a very small
part of what makes an expressive interpretation; essential de-
tails like articulation (e.g., staccato vs. legato), pedalling, the
shaping of individual voices, etc. are missing (and will be
very hard to measure from audio recordings at all). A com-
puter performance based only on applying these beat-level
tempo and loudness changes will not sound anything like a
human performance, as can be readily veri(cid:2)ed experimen-
tally. Thus we have to admit that the title we chose for this
paper is a bit pretentious: the computer cannot be expected
to play like the great pianists (cid:150) at least not given the current
methods and available training data. It can, however, extract
aspects of personal style from recordings by great pianists, as
has been shown in the our experiments.

5 Conclusion
An application of relational instance-based learning to a dif-
(cid:2)cult task from the domain of classical music was presented:
we showed how the problem of learning models of expres-
sive piano performance can be reduced to applying simple
expressive phrase-patterns by analogy to the most similar
phrases in the training set. In particular, it was shown that
a relational learner like DISTALL succeeds in learning per-
formance strategies that obviously capture aspects individual
artistic style, which was demonstrated in learning and artist
classi(cid:2)cation experiments.

The ultimate goal of this kind of research is not automatic
style replication or the creation of arti(cid:2)cial performers, but to
use computers to teach us more about the elusive artistic ac-
tivity of expressive music performance. While it is satisfying
to see that the computer is indeed capable of extracting infor-
mation from performance measurements that seems to cap-
ture aspects of individual style, this can only be a (cid:2)rst step.

In order to get real insight, we will need learning algorithms
that, unlike nearest-neighbor methods, produce interpretable
models. That is a natural next step in our future research.

Acknowledgments
This research is supported by the Austrian Fonds zur
F¤orderung der Wissenschaftlichen Forschung (FWF) (project
no. Y99-INF). The Austrian Research Institute for Arti(cid:2)cial
Intelligence acknowledges basic (cid:2)nancial support by the Aus-
trian Federal Ministry for Education, Science, and Culture,
and the Federal Ministry of Transport, Innovation, and Tech-
nology. Thanks to Werner Goebl for performing the harmonic
and phrase structure analysis of the Mozart sonatas.

References
[Bisson, 1992] Gilles Bisson. Learning in FOL with a simi-

larity measure. In Proceedings of the 10th AAAI, 1992.

[Dixon, 2001] Simon Dixon. Automatic extraction of tempo
and beat from expressive performances. Journal of New
Music Research, 30(1):39(cid:150)58, 2001.

[Emde and Wettschereck, 1996] Werner Emde and Dietrich
Wettschereck. Relational instance-based learning. In Pro-
ceedings of the 13th International Conference on Machine
Learning, 1996.

[King et al., 2004] R.D. King, K.E. Whelan, F.M. Jones,
P.G.K. Reiser, C.H. Bryant, S.H. Muggleton, D.B. Kell,
and S.G. Oliver. Functional genomic hypothesis gener-
ation and experimentation by a robot scientist. Nature,
427(6971):247(cid:150)252, 15 January 2004.

[Saunders et al., 2004] Craig Saunders, David R. Hardoon,
John Shawe-Taylor, and Gerhard Widmer. Using string
kernels to identify famous performers from their playing
style. In Proceedings of the 15th European Conference on
Machine Learning, 2004.

[Stamatatos and Widmer, 2002] Efstathios Stamatatos and
Gerhard Widmer. Music performer recognition using an
ensemble of simple classi(cid:2)ers. In Proceedings of the 15th
European Conference on Arti(cid:2)cial Intelligence, 2002.

[Tobudic and Widmer, 2005] Asmir Tobudic and Gerhard
Widmer. Relational IBL in classical music. Machine
Learning, to appear, 2005.

[Todd, 1992] McAngus N. Todd. The dynamics of dynam-
ics: A model of musical expression. Journal of the Acous-
tical Society of America, 91(6):3540(cid:150)50, 1992.

[Widmer and Zanon, 2004] Gerhard Widmer and Patrick
Zanon. Automatic recognition of famous artists by ma-
chine.
In Proceedings of the 17th European Conference
on Arti(cid:2)cial Intelligence, 2004.

[Widmer et al., 2003] Gerhard Widmer,

Simon Dixon,
Werner Goebl, Elias Pampalk, and Asmir Tobudic.
In
search of the Horowitz factor. AI Magazine, 24(3):111(cid:150)
130, 2003.


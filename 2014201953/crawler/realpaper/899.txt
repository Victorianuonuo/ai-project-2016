Metric Properties of Structured Data Visualizations through Generative

Probabilistic Modeling

Peter Ti ˇno

Nikolaos Gianniotis

University of Birmingham
School of Computer Science
Birmingham B15 2TT, UK

pxt,nxg@cs.bham.ac.uk

Abstract

Recently, generative probabilistic modeling princi-
ples were extended to visualization of structured
data types, such as sequences. The models are for-
mulated as constrained mixtures of sequence mod-
els - a generalization of density-based visualization
methods previously developed for static data sets.
In order to effectively explore visualization plots,
one needs to understand local directional magniﬁ-
cation factors, i.e. the extend to which small posi-
tional changes on visualization plot lead to changes
in local noise models explaining the structured
data. Magniﬁcation factors are useful for highlight-
ing boundaries between data clusters. In this paper
we present two techniques for estimating local met-
ric induced on the sequence space by the model for-
mulation. We ﬁrst verify our approach in two con-
trolled experiments involving artiﬁcially generated
sequences. We then illustrate our methodology on
sequences representing chorals by J.S. Bach.

1 Introduction
Topographic visualisation techniques have been an important
tool in multi-variate data analysis and data mining. Gener-
ative probabilistic approaches [Bishop et al., 1998b; 1998a;
Kab´an and Girolami, 2001; Nabney et al., 2005] demon-
strated advantages over non-probabilistic alternatives in terms
of a ﬂexible and technically sound framework that makes
various extensions possible in a principled manner.Recently,
Tino et al.[2004] introduced a general framework for visualiz-
ing sets of sequential data. Loosely speaking, a smooth two-
dimensional manifold in the space of appropriate noise mod-
els (e.g. Hidden Markov models (HMM) of a given topology)
is constructed so that HMMs constrained on the manifold ex-
plain the observed sequences well. Each sequence (data item)
is then projected on the manifold by visualizing the relevant
HMMs likely to be responsible for generating that sequence.
It is possible in non-linear projections that visualizations
of two data items get close in the visualization space, even
though they may be separated by a large distance in the data
space (and vice-versa). Therefore, to be of practical use, non-
linear projection methods must be equipped with representa-
tions in the visualization space of metric relationships among

data items in the original data space. Otherwise, one would
not be able to detect and understand the underlying cluster
structure of data items. To that end, in the case of static vec-
torial data of ﬁxed-dimensionality, Bishop et al. [1997] cal-
culated magniﬁcation factors of the Generative Topographic
Mapping (GTM) [Bishop et al., 1998b] using tools of differ-
ential geometry.

However, in the case of structured data types (such as se-
quences or trees) one has to be careful when dealing with the
notion of a metric in the data space. The generative prob-
abilistic visualization models studied in this paper naturally
induce a metric in the structured data space. Loosely speak-
ing, two data items (sequences) are considered to be close (or
similar) if both of them are well-explained by the same un-
derlying noise model (e.g. HMM) from the two-dimensional
manifold of noise models. Due to non-linear parametrization
of the manifold, projections of two quite different data items
may end up being mapped close to each other. We emphasise,
that in this framework, the distance between structured data
items is implicitly deﬁned by the local noise models that drive
topographic map formation. If the noise model changes, the
perception of what kind of data items are considered similar
changes as well. For example, if the noise models were 1st-
order Markov chains, sequences would be naturally grouped
with respect to their short-range subsequence structure. If,
on the other hand, the noise models were stochastic machines
specializing at latching special events occurring within the
data sequences, the sequences would naturally group with re-
spect to the number of times the special event occurred, ir-
respective of the length of temporal separation between the
events.

In this paper, we quantify the extend to which small po-
sitional changes on manifold of local noise models explain-
ing structured data (for visualization purposes the manifold
is identiﬁed with visualization space) lead to changes in the
distributions deﬁned by the noise models.
It is important
to quantify the changes in a parametrization-free manner -
we use approximations of Kullback-Leibler divergence. We
present two techniques for estimating local metric induced
on the structured data space by the model formulation. We
ﬁrst verify our approach in two controlled experiments in-
volving artiﬁcially generated sequences. We then illustrate
our methodology on sequences representing chorals by J.S.
Bach.

IJCAI-07

1083

(n)
t

2 A latent trait model for sequential data
Consider a set of sequences over the alphabet S of S sym-
bols, S = {1, 2, ..., S}. The n-th sequence is denoted by
s(n) = (s
)t=1:Tn , where n = 1 : N and Tn denotes
its length. The aim is to represent each sequence using a
two-dimensional latent (visualization) space V = [−1, 1]2
.
In order to exploit the visualization space fully, a maximum
entropy (uniform) prior distribution is imposed over the latent
space. With each latent point x, we associate a generative dis-
tribution over sequences p(s|x). One possibility (considered
in this paper) is to use HMM with K hidden states [Rabiner
and Juang, 1986]:

(cid:2)

Tn(cid:3)

Tn(cid:3)

p(s(n)|x) =

p(h1|x)

p(ht|ht−1, x)

(n)
t

p(s

|ht, x),

h

t=2

t=1

(1)
where h is the set of all Tn-tuples over the K hidden states.
For tractability reasons, we discretize the latent space into
a regular grid of C points1 x1, ..., xC . In order to have the
HMMs topologically organized, we (in the spirit of [Bishop
et al., 1998b; Kab´an and Girolami, 2001]) constrain the ﬂat
mixture of HMMs,

C(cid:2)

c=1

p(s) =

1
C

p(s|xc),

by requiring that the HMM parameters be generated through
a parameterised smooth nonlinear mapping from the latent
space V into the HMM parameter space:

p(h1 = k|x) = gk(A(π)φ(x)),
p(ht = k|ht−1 = l, x) = gk(A(T l)φ(x)),
p(st = s|ht = k, x) = gs(A(B k)φ(x)),

(2)

(3)

(4)

where indexes k, l run from 1 to K; index s ranges from 1 to
S, and

• the function g(.) is the softmax function2 and gk(.) de-

notes the k-th component returned by the softmax, i.e.

(cid:4)
(a1, a2, ..., aq)T

(cid:5)

=

gk

ak(cid:6)q

e
i=1 eai

, k = 1, 2, ..., q,

• φ(.) = (φ1(.), ..., φM (.))T

, φm(.) : R2 → R is an or-
dered set of M non-parametric nonlinear smooth basis
functions (typically RBFs),

• the matrices A(π) ∈ RK×M

, A(T l) ∈ RK×M
are free parameters of the model.

A(B k) ∈ RS×M

and

, n = 1 : N , were indepen-

dently generated, the model likelihood reads

Assuming the sequences s(n)
N(cid:3)

N(cid:3)

L =

p(s(n)

) =

C(cid:2)

1
C

n=1

n=1

c=1

p(s(n)|xc).

(5)

1these sample (grid) points are analogous to the nodes of a Self

Organising Map

2which is the canonical inverse link function of multinomial dis-

tributions

Maximum likelihood estimates of the free parameters can
be obtained via Expectation-Maximization (EM) algorithm
[Tino et al., 2004].

Given a sequence s, some regions of the latent visualiza-
tion space are better at explaining it than the others. A natural
representation (visualization) of s is the mean of the posterior
distribution over the latent space, given that sequence:

C(cid:2)

P roj(s) =

c=1

xc p(xc|s).

3 Quantifying metric properties of the

visualization space

In this section, we present two approaches to quantifying
metric properties of the visualization space V as sensitivi-
ties (measured by Kullback-Leibler divergence) of local noise
models (HMM) addressed by the points in V to small pertur-
bations in V.

3.1 Approximating Kullback-Leibler divergence
through observed Fisher information matrix

Consider the visualization (latent) space V = [−1, +1]2
and the two-dimensional manifold M of local noise mod-
els p(·|x) (e.g. HMM with 3 states, emissions over 7 sym-
bols) parametrized by the latent space through (1) and (2-4).
The manifold is embedded in manifold H of all noise models
of the same form (e.g. all HMMs with 3 states and emis-
sions over 7 symbols). Consider a latent point x ∈ V. If we
displace x by an inﬁnitesimally small perturbation dx, the
Kullback-Leibler divergence DKL(p(·|x)(cid:4)(p(·|x + dx)) be-
tween the corresponding noise models p(·|x), (p(·|x + dx) ∈
M can be determined via Fisher information matrix

F (x) = −Ep(·|x)[∇2

log p(.|x)]

that acts like a metric tensor on the Riemannian manifold M
[Kullback, 1959]:

DKL(p(·|x)(cid:4)(p(·|x + dx)) = dxT F (x) dx.

The situation is illustrated in ﬁgure 1.

Local noise models used in this paper (HMMs) are latent
variable models and there is no closed-form formula for cal-
culating the Fisher information matrix. However, Lystig and
Hughes[2002] presented a framework for efﬁcient calculation
of the observed Fisher information matrix of HMMs based on
forward calculations related to those used in maximum like-
lihood parameter estimation via EM algorithm. We adapt the
framework to a special kind of parametrization of HMM used
in the latent trait model of section 2. Each HMM has two pa-
rameters – latent space coordinates x = (x1, x2) (these can
be thought of as coordinates on the visualization screen).

Consider a set S(x) of N sequences independently gen-
erated by HMM p(·|x). All sequences have equal length T .
Given the n-th sequence, we start recursion from the begin-
ning of the sequence:

(n)
1 (k) = p(s

λ

(n)
1 |h1 = k, xc)p(h1 = k|x)

IJCAI-07

1084

+1

x2

V

1

x+dx

x

x 1

+1

p(.x+dx)

p(.x)

M

H

Figure 1: Two-dimensional manifold M of local noise models p(·|x) parametrized by the latent space V through (1) and (2-4).
The manifold is embedded in manifold H of all noise models of the same form. Latent coordinates x are displaced to x + dx.
Kullback-Leibler divergence DKL(p(·|x)(cid:4)(p(·|x + dx) between the corresponding noise models p(·|x), (p(·|x + dx) ∈ M
can be determined via Fisher information matrix F (x) that acts like a metric tensor on the Riemannian manifold M.

Recursive step:

and Ω

(n)
t

(xq, xr) =

j=1 ω

(n)
t

(cid:6)K

(n)
λ
t

(k) = p(s

(n)
t

, ht = k|s

(n)
1 , . . . , s

(n)
t−1, x)

K(cid:2)

=

(n)
t−1(i)p(s

[λ

(n)
t

|ht = k, x)

i=1
p(ht = k|ht−1 = i, x)](Λ

(cid:6)K
j=1 λt(j)(n)

.

(n)
t−1)

−1

,

where Λ

(n)
t =

Starting again at the beginning of the sequence, we recur-
sively evaluate 1st-order derivatives with respect to latent co-
ordinates x1, x2. Letting q ∈ {1, 2}, we have:

(n)
ψ
1

(k; xq) =

∂

∂xq

p(s

(n)
1 |h1 = k, x)p(h1 = k|x),

(n)
ψ
t

(k; xq) =

(cid:6)K

∂
∂xq p(s

(n)
(n)
1 , . . . s
t

p(s

(n)
1 , . . . , s

, ht = k|x)
(n)
t−1|x)

,

and Ψ

(n)
t

(xq) =

(n)
j=1 ψ
t

(j; xq).

(n)
t−1, λ

The calculations recursively employ ψ

(n)
t−1.
We also need 1st-order derivatives of initial state, state transi-
tion and state-conditional emission probabilities with respect
to latent coordinates. We present only the equation for state
transitions, other formulas can be obtained in the same man-
ner:

(n)
t−1 and Λ

(cid:7)
A(T l)

k

∂

∂xq

p(ht = k|ht−1 = l, x) = gk(A(T l)φ(x))
(cid:8)

∂φ(x)
∂xq

−

[gi(A(T l)φ(x))A(T l)

i

∂φ(x)
∂xq

]

,

K(cid:2)

i=1

where A(.)

i denotes the i-th row of the parameter matrix A(.)
.
2nd-order derivatives are obtained in a recursive manner as

well:

(n)
1

ω

(k; xq, xr) =

2

∂

∂xq∂xr

p(s

(n)
1 |h1 = k, x)p(h1 = k|x),

(n)
t

ω

(k; xq, xr) =

∂ 2

∂xq∂xr p(s

(n)
(n)
1 , · · · , s
t
(n)
(n)
t−1|x)
1 , · · · , s

p(s

, ht = k|x)

(j; xq, xr).
(n)
t−1, ω

The calculations recursively employ ψ
(n)
Λ
t−1. We also need both 1st- and 2nd-order derivatives of ini-
tial state, state transition and state-conditional emission prob-
abilities with respect to latent coordinates. Again, we present
only the equation for state transitions:

(n)
t−1, λ

(n)
t−1 and

2

∂

∂xq∂xr

(cid:7)
A(T l)

k

p(ht = k|ht−1 = l, x) =

∂

∂xr

gk(A(T l)φ(x))
(cid:8)

∂φ(x)
∂xq

−

[gi(A(T l)φ(x))A(T l)

i

∂φ(x)
∂xq

]

K(cid:2)

i=1

(cid:7)
A(T l)

k

2φ(x)
∂
∂xq∂xr

−

K(cid:2)

i=1

[

∂

∂xr

+gk(A(T l)φ(x))

gi(A(T l)φ(x))A(T l)

i

gi(A(T l)φ(x))A(T l)

i

∂φ(x)
∂xq
2φ(x)
∂
∂xq∂xr

+

(cid:8)

]

Denoting

Q(n)

q,r =

(n)
T (xq, xr)

Ω

(n)
T

Λ

− Ψ

(n)
T (xr)

(n)
T (xq)Ψ
(n)
T )2

(Λ

,

we calculate elements of the observed Fisher information ma-
trix

ˆF (x), given the set of sequences S(x), as

ˆF (x)q,r = − 1
N

Q(n)
q,r .

N(cid:2)

n=1

To illustrate metric properties of the visualization space,
ˆF (xc)
we calculate the observed Fisher information matrices
latent centers xc, c = 1, 2, ..., C, and detect
in all
ˆF (xc) directions of dominant lo-
through eigen-analysis of
cal change in Kullback-Leibler divergence between the HMM
parametrized by xc and its perturbed version parametrized by
xc + dx. In the visualization space, we signify magnitude of
ˆF (xc)) by lo-
the dominant change (dominant eigenvalue of
cal brightness of the background as well as mark the direction
of the dominant change by a piece of line.

IJCAI-07

1085

3.2 Direct recursive approximation of

Kullback-Leibler divergence

Another alternative approach to probe metric properties of
the visualization space is the estimate DKL(p(·|x)(cid:4)(p(·|x +
Δx)) directly. Do [2003] presented an efﬁcient algorithm for
approximating Kullback-Leibler (K-L) divergence between
two HMM of the same topology. The approximation is based
on backward calculations related to those used in maximum
likelihood parameter estimation via EM algorithm.

With each hidden state k ∈ {1, 2, ..., K} we associate an
auxiliary process βk(t). Given a sequence s = (st)t=1:T , the
likelihood p(s|x) can be efﬁciently calculated by

• starting at the end of the sequence:

βk(T ; x) = p(sT |k, x)

• Recursive step:

βk(t; x) = p(st|k, x)

K(cid:2)

i=1

p(ht+1 = i|ht = k, x)βi(t+1; x)

• Final step:

p(s|x) =

K(cid:2)

k=1

For u, v ∈ (0, 1], deﬁne

p(h1 = k|x)βk(1; x)

κ(u, v) = u log

u
v

.

The Kullback-Leibler (K-L) divergence between two HMMs
p(·|x) and p(·|x(cid:2)), can be approximated as follows:

• Recursion is initiated at end of the sequence:

Dk(T ; x, x(cid:2)) = κ(p(sT |k, x), p(sT |k, x(cid:2)))

• Recursive step:

Dk(t; x, x(cid:2)) = κ(p(st|k, x), p(st|k, x(cid:2))) +
DKL(p(ht+1|ht = k, x)||p(ht+1|ht = k, x(cid:2))) +
K(cid:2)

p(ht+1 = i|ht = k, x)Di(t + 1; x, x(cid:2))

i=1

• Final step: the empirical K-L divergence, given the se-

quence s, is bounded by

K(s, x, x(cid:2)) = DKL(p(h1|x)||p(h1|x(cid:2))) +

K(cid:2)

k=1

p(h1 = k|x)Dk(1; x, x(cid:2)).

Given the set of N sequences, S(x), generated inde-
pendently by the HMM addressed by x, an estimate of
DKL(p(·|x)(cid:4)(p(·|x + Δx)) is calculated as

ˆDKL(p(·|x)(cid:4)(p(·|x + Δx)) =

1
N

K(s(n)

, x, x + Δx).

N(cid:2)

n=1

4 Experiments

In this section we demonstrate techniques introduced in sec-
tions 3.1 and 3.2 to provide additional metric information
when visualizing sequential data.

In all experiments the latent space centres xc were posi-
tioned on a regular 10 × 10 square grid (C = 100) and there
were M = 16 basis functions φi. The basis functions were
spherical Gaussian functions of the same width σ = 1.0. The
basis functions were centred on a regular 4×4 square grid, re-
ﬂecting uniform distribution of the latent classes. We account
for a bias term by using an additional constant basis function
φ17(x) = 1.

Free parameters of the model were randomly initialized in
the interval [−1, 1]. Training consisted of repeating EM iter-
ations [Tino et al., 2004]. Typically, the likelihood levelled
up after 30-50 EM cycles.

When calculating metric properties of the visualization
space, each set S(xc) consisted of 100 generated sequences
of length 40.

4.1 Toy data

We ﬁrst generated a toy data set of 400 binary sequences
of length 40 from four HMMs (K = 2 hidden states) with
identical emission structure, i.e. the HMMs differed only in
transition probabilities. Each of the four HMMs generated
100 sequences. Local noise models p(·|x) were imposed as
HMMs with 2 hidden states. Visualization of the sequences
is presented in ﬁgure 2(a). Sequences are marked with four
different markers, corresponding to the four different HMMs
used to generate the data set. We stress that the model was
trained in a completely unsupervised way. The markers are
used for illustrative purposes only. Representations of in-
duced metric in the local noise model space based on Fisher
Information matrix (section 3.1) and direct K-L divergence
estimations (section 3.2) can be seen in ﬁgures 2(b) and 2(c),
respectively. Dark areas signify homogeneous regions of lo-
cal noise models and correspond to possible clusters in the
data space. Light areas signify abrupt changes in local noise
model distributions (as measured by K-L divergence) and cor-
respond to boundaries between data clusters. The visualiza-
tion plot reveals that the latent trait model essentially discov-
ered the organization of the data set and the user would be
able to detect the four clusters, even without help of the mark-
ing scheme in ﬁgure 2(a). Of course, the latent trait model
beneﬁted from the fact that the distributions used to gener-
ate data were from the same model class as the local noise
models. There were few atypical sequences generated by the
HMM marked with ’+’, and this is clearly reﬂected by their
projections in the lower left corner.

4.2 Melodic lines of chorals by J.S. Bach

Again, to illustrate metric properties of the visualization
space, we perturb latent centers xc in regular intervals around
small circle centered at xc and for each perturbation Δx cal-
culate ˆDKL(p(·|x)(cid:4)(p(·|x + Δx)). As before, in the visual-
ization space, we signify magnitude of the dominant change
by local brightness of the background as well as mark the di-
rection of the dominant change by a piece of line.

Next we subjected the latent trait model to a set of 100
chorales by J.S. Bach from UCI repository of Machine Learn-
ing Databases. We extracted the melodic lines – pitches are
represented in the space of one octave, i.e.
the observation
symbol space consists of 12 different pitch values. Local
noise models had K = 3 hidden states. Figure 3 shows choral
visualizations, while representations of induced metric in the

IJCAI-07

1086

(a)

(b)

(c)

Figure 2: (a) Visualization of 400 binary sequences of length 40 generated by four HMMs with 2 hidden states and with
identical emission structure. Sequences are marked with four different markers, corresponding to the four HMMs. Also shown
are representations of induced metric in the local noise model space based on Fisher Information matrix (b) and direct K-L
divergence estimations (c).

flats

Latent space visualization

sharps

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

infrequentflats/sharps

gfgpatterns

−1

−0.8

−0.6

−0.4

−0.2

0

0.2

0.4

0.6

0.8

1

tritons

caddc

repeatedfnotesingfgpatterns

gfffg

Figure 3: Visualization of 100 melodic lines of chorales by
J.S. Bach.

local noise model space based on Fisher Information matrix
and direct K-L divergence estimations can be seen in ﬁgures
4(a) and 4(b), respectively. The method discovered natural
topography of the key signatures, corroborated with similar-
ities of melodic motives. The melodies can contain sharps
and ﬂats other than those included in their key signature due
to both modulation and ornaments. The upper region con-
tains melodic lines that utilise keys with ﬂats. Central part
of the visualisation space is taken by sharps (left) and almost
no sharps/ﬂats (center). The lower region of the plot con-
tains chorals with tense patterns (e.g containing tritons) and
is quite clearly strongly separated from other chorals. Again,
the overall clustering of chorals is well-matched by the met-
ric representations of ﬁgures 4(a) and 4(b). Flats, sharps and
tense patterns are clearly separated, as are sharps and infre-
quent sharps/ﬂats. There are more interesting features to this
visualization (e.g. enharmony patterns) that cannot be ad-
dressed due to space limitations.

5 Discussion and conclusion

When faced with the problem of non-linear topographic data
visualization, one needs additional information putting rel-
ative distances of data projections in proportion to local
stretchings/contractions of the visualization manifold in the
data space. It is important to base representations of manifold
stitchings/contractions on the particular notion of distance or
similarity between data items that drives formation of the to-
pographic mapping. If one visualizes structured data, such
as sequences, this can be a highly non-trivial task, especially
for the family of neural-based topographic mappings of se-
quences [Koskela et al., 1998; Horio and Yamakawa, 2001;
Voegtlin, 2002; Strickert and Hammer, 2003]. However,
when the visualization model is formulated as a latent trait
model, such calculations follow naturally from the model
formulation. Two sequences are considered to be similar if
both of them are well-explained by the same underlying noise
model (in our particular case - HMM). Noise models are or-
ganized on a smooth two-dimensional manifold and we can
study changes of the local metric (based on Kullback-Leibler
divergence) due to non-linear parametrization. We have pre-
sented two approaches to metric analysis of the visualization
space and experimentally veriﬁed that additional information
provided by metric representations in the visualization space
can be useful for understanding of the overall organization of
data. We stress. that the data organization revealed is always
with respect to the notion of similarity between data items
imposed in the model construction.

The results can be easily generalized e.g.

to tree-
structured data using, for example, Hidden Markov Tree
models (HMTM) [Durand et al., 2004]). We stress that the
presented framework is general and can be applied to vi-
sualizations through latent trait models of a wide variety of
structured data, provided a suitable noise model is used. For
example, we are currently working on topographic organiza-
tions (and metric properties of such) of ﬂuxes form binary star

IJCAI-07

1087

(a)

(b)

Figure 4: Representations of induced metric in the local noise model space based on Fisher Information matrix (a) and direct
K-L divergence estimations (b) for visualizations of choral by J.S. Bach.

complexes, where the noise models are based on real physical
models of binary stars.

[Kullback, 1959] S. Kullback.

Information Theory and

Statistics. Wiley, New York, NY, 1959.

[Lystig and Hughes, 2002] Theodore C. Lystig and James P.
Hughes. Exact computation of the observed information
matrix for hidden Markov models. Journal of Computa-
tional and Graphical Statistics, 11(3):678–689, 2002.

[Nabney et al., 2005] I. Nabney, Y. Sun, P. Tiˇno, and
A. Kab´an. Semisupervised learning of hierarchical latent
trait models for data visualisation. IEEE Transactions on
Knowledge and Data Engineering, 17(3), 2005.

[Rabiner and Juang, 1986] R.L. Rabiner and B.H. Juang. An
introduction to hidden markov models. IEEE ASSP Mag-
azine, 3:4–16, 1986.

[Strickert and Hammer, 2003] M. Strickert and B. Hammer.
Neural gas for sequences. In Proceedings of the Workshop
on Self-Organizing Maps (WSOM’03), pages 53–57, 2003.

[Tino et al., 2004] P. Tino, A. Kaban, and Y. Sun. A gen-
erative probabilistic approach to visualising sets of sym-
bolic sequences.
In W. DuMouchel J. Ghosh. R. Ko-
havi, J. Gehrke, editor, Proceedings of the Tenth ACM
SIGKDD International Conference on Knowledge Discov-
ery and Data Mining (KDD’04), pages 701–706. ACM
Press, 2004.

[Voegtlin, 2002] T. Voegtlin.

Recursive self-organizing

maps. Neural Networks, 15(8–9):979–992, 2002.

References

[Bishop et al., 1997] C.M. Bishop, M. Svens´en, and C.K.I.
Williams. Magniﬁcation factors for the GTM algorithm.
In Proceedings IEE Fifth International Conference on Ar-
tiﬁcial Neural Networks, pages 64–69. IEE, London, 1997.

[Bishop et al., 1998a] C.M. Bishop, M. Svens´en, and C.K.I.
Williams. Developments of the Generative Topographic
Mapping. Neurocomputing, 21:203–224, 1998.

[Bishop et al., 1998b] C.M. Bishop, M. Svens´en, and C.K.I.
Williams. GTM: The generative topographic mapping.
Neural Computation, 10(1):215–235, 1998.

[Do, 2003] Minh N. Do. Fast approximation of kullback–
leibler distance for dependence trees and hidden markov
models,. Signal Processing Letters, IEEE, 10(4):115–118,
2003.

[Durand et al., 2004] J.-B Durand, P. Goncalves,

and
Y. Guedon. Computational methods for hidden markov
tree models-an application to wavelet
IEEE
Transactions on Signal Processing, 52(9):2552–2560,
2004.

trees.

[Horio and Yamakawa, 2001] K. Horio and T. Yamakawa.
Feedback self-organizing map and its application to
spatio-temporal pattern classiﬁcation. International Jour-
Intelligence and Applications,
nal of Computational
1(1):1–18, 2001.

[Kab´an and Girolami, 2001] A. Kab´an and M. Girolami. A
combined latent class and trait model for the analysis and
visualization of discrete data. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 23(8):859–872,
2001.

[Koskela et al., 1998] T. Koskela, M. Varsta znd J. Heikko-
nen, and K. Kaski. Recurrent SOM with local linear mod-
els in time series prediction. In 6th European Symposium
on Artiﬁcial Neural Networks, pages 167–172, 1998.

IJCAI-07

1088


Generalizing Plans to New Environments in Relational MDPs 

Carlos Guestrin  Daphne Koller  Chris Gearhart  Neal Kanodia 

Computer Science Department, Stanford University 
{guestrin, koller, cmg33, nkanodia}@cs.stanford.edu 

Abstract 

A longstanding goal in planning research is the ability to gen(cid:173)
eralize plans developed for some set of environments to a 
new but similar environment, with minimal or no replanning. 
Such generalization can both reduce planning time and al(cid:173)
low us to tackle larger domains than the ones tractable for 
direct planning.  In this paper,  we present an approach to 
the generalization problem based on a new framework of re(cid:173)
lational Markov Decision Processes (RMDPs). An RMDP 
can model a set of similar environments by representing ob(cid:173)
jects as instances of different classes.  In order to generalize 
plans to multiple environments,  we define an approximate 
value function specified in terms of classes of objects and, in 
a multiagent setting, by classes of agents.  This class-based 
approximate value function is optimized relative to a sam(cid:173)
pled subset of environments, and computed using an efficient 
linear programming method.  We prove that a polynomial 
number of sampled environments suffices to achieve perfor(cid:173)
mance close to the performance achievable when optimizing 
over the entire space. Our experimental results show that our 
method generalizes plans successfully to new,  significantly 
larger, environments, with minimal loss of performance rel(cid:173)
ative to environment-specific planning. We demonstrate our 
approach on a real strategic computer war game. 
Introduction 

1 
Most planning methods optimize the plan of an agent in a 
fixed environment. However, in many real-world settings, an 
agent will face multiple environments over its lifetime, and 
its experience with one environment should help it to perform 
well in another, even with minimal or no replanning. 

Consider, for example, an agent designed to play a strate(cid:173)
gic computer war game, such as the Freecraft game shown 
in Fig.  1  (an open source version of the popular Warcraft 
game).  In this game, the agent is faced with many scenar(cid:173)
ios. In each scenario, it must control a set of agents (or units) 
with different skills in order to defeat an opponent. Most sce(cid:173)
narios share the same basic elements: resources, such as gold 
and wood; units, such as peasants, who collect resources and 
build structures, and footmen, who fight with enemy units; 
and structures, such as barracks, which are used to train foot(cid:173)
men.  Each scenario is composed of these same basic build(cid:173)
ing blocks, but they differ in terms of the map layout, types 
of units available, amounts of resources, etc.  We would like 
the agent to learn from its experience with playing some sce(cid:173)
narios, enabling it to tackle new scenarios without significant 
amounts of replanning. In particular, we would like the agent 
to generalize from simple scenarios, allowing it to deal with 
other scenarios that are too complex for any effective planner. 
The idea of generalization has been a longstanding goal in 
Markov Decision Process (MDP) and reinforcement learning 

Figure 1:  Freecraft strategic domain with 9 peasants, a barrack, a 
castle, a forest, a gold mine, 3 footmen, and an enemy, executing the 
generalized policy computed by our algorithm. 
research 115; 161, and even earlier in traditional planning [5]. 
This problem is a challenging one, because it is often unclear 
how to translate the solution obtained for one domain to an(cid:173)
other.  MDP solutions assign values and/or actions to states. 
Two different MDPs (e.g., two Freecraft scenarios), are typ(cid:173)
ically quite different,  in that they  have  a different set (and 
even number) of states and actions. In cases such as this, the 
mapping of one solution to another is not well-defined. 

Our approach is based on the insight that many domains 
can be described in terms of objects and the relations between 
them. A particular domain will involve multiple objects from 
several classes.  Different tasks in the same domain will typ(cid:173)
ically involve different sets of objects, related to each other 
in different ways.  For example, in Freecraft, different tasks 
might involve different numbers of peasants, footmen, ene(cid:173)
mies, etc. We therefore define a notion of a relational MDP 
(RMDP), based on the probabilistic relational model (PRM) 
framework  110].  An  RMDP  for  a  particular domain  pro(cid:173)
vides a general schema for an entire suite of environments, 
or worlds, in that domain.  It specifies a set of classes, and 
how the dynamics and rewards of an object in a given class 
depend on the state of that object and of related objects. 

We use the class structure of the RMDP to define a value 
function that can be generalized from one domain to another. 
We begin with the assumption that the value function can 
be well-approximated as a sum of value subfunctions for the 
different objects in the domain.  Thus, the value of a global 
Freecraft state is approximated as a sum of terms correspond(cid:173)
ing to the state of individual peasants, footmen, gold, etc. We 
then assume that individual objects in the same class have 
a very similar value function.  Thus,  we define  the notion 
of a class-based value function, where each class is associ(cid:173)
ated with a class subfunction.  All objects in the same class 
have the value subfunction of their class, and the overall value 
function for a particular environment is the sum of value sub-
functions for the individual objects in the domain. 

A set of value subfunctions for the different classes imme-

PROBABILISTIC  PLANNING 

1003 

diately determines a value function for any new environment 
in the domain, and can be used for acting. Thus, we can com(cid:173)
pute a set of class subfunctions based on a subset of environ(cid:173)
ments, and apply them to another one without replanning. 

We provide an optimality criterion for evaluating a class-
based value function for a distribution over environments, and 
show how it can, in principle, be optimized using a linear pro(cid:173)
gram.  We can also "learn" a value function by optimizing 
it relative to a sample of environments encountered by the 
agent.  We prove that a polynomial number of sampled en(cid:173)
vironments suffice to construct a class-based value function 
which is close to the one obtainable for the entire distribution 
over environments.  Finally, we show how we can improve 
the quality of our approximation by automatically discover(cid:173)
ing subclasses of objects that have "similar" value functions. 
We present experiments for a computer systems admin(cid:173)
istration  task  and  two  Freecraft  tasks.  Our  results  show 
that we can successfully generalize class-based value func(cid:173)
tions.  Importantly, our approach also obtains effective poli(cid:173)
cies for problems significantly larger than our planning algo(cid:173)
rithm could handle otherwise. 
2  Relational Markov Decision Processes 
A relational MDP defines the system dynamics and rewards 
at the level of a template for a task domain. Given a particu(cid:173)
lar environment within that domain, it defines a specific MDP 
instantiated for that environment. As in the PRM framework 
of [10], the domain is defined via a schema, which speci(cid:173)
fies a set of object classes 
Each class 
C is also associated with a set of state variables 

which describe the state of an object in 
that class. Each state variable C.S has a domain of possible 
We define Sc  to be the set of possible 
values Doni 
states for an object in 
the possible assignments to the 
state variables of C. 
example, 

domain  might 
have  classes  such  as  Peasant,  Footman,  Gold; 
the  class  Peasant  may  have  a  state  variable 
Task  whose  domain 
= 
{Waiting, Mining, Harvesting, Building},  and  a  state 
variable  Health  whose  domain  has  three  values. 
In  this 
case,  SPeasant  would have 4 â€¢ 3  =  12 values, one for each 
combination of values for Task and Health. 

Dom[Peasant.Task:] 

our  Freecraft 

For 

is 

The  schema  also  specifies  a  set  of  links 

= 
for each class representing links between ob(cid:173)

Enemy.MyJFootmen 

jects in the domain. Each link C.L has a range 
For example,  Peasant objects might be linked to Barrack 
objects 
[Peasant.BuildTarget]  =  Barrack, and to the 
global Gold and Wood resource objects.  In a more com(cid:173)
plex  situation,  a link may  relate C to many instances of a 
class C, which we denote by 
for example, 
indicates that an instance 
of the enemy class may be related to many footman instances. 
A  particular  instance  of  the  schema  is  defined  via  a 
specifying the set of objects of each class; we use 
to denote the objects in class C, and 
The world 

note the total set of objects in 
ifies the links between objects,  which we take to be fixed 
throughout  time.  Thus,  for  each  link  C.L,  and  for  each 

to de(cid:173)
also spec(cid:173)

world 

specifies a set of objects 

de(cid:173)
noted o.L.  For example, in a world containing 2 peasants, 
we  would have 
if  Peasantl  is  building  a  barracks,  we  would  have  that 
Peasant l.BuildTarget = Barrackl. 

The  dynamics  and  rewards  of an  RMDP  are  also  de(cid:173)
fined  at  the  schema  level.  For  each  class,  the  schema 
specifies  an  action  C.A,  which  can  take  on  one  of sev(cid:173)
eral  values Doni 
Wait, Mine, Harvest, Build  Each class C is also associ(cid:173)
, which specifies the proba(cid:173)
ated with a transition model 
bility distribution over the next state of an object 
in class 
C, given the current state of 
the action taken on  , and the 
states and actions of all of the objects linked to o: 

For example,  D o

n

i-

(1) 
For  example,  the  status  of  a  barrack,  Barrack.Sta/z*/, 
depends  on  its  status  in  the  previous  time  step,  on 
the  task  performed  by  any  peasant  that  could  build  it 
(Barrack.BuiltBy.Task), on the amount of wood and gold, etc. 
The transition model is conditioned on the state of C.Lt, 
which is, in general, an entire set of objects (e.g., the set of 
peasants linked to a barrack).  Thus we must now provide 
a compact specification of the transition model that can de(cid:173)
pend on the state of an unbounded number of variables.  We 
can deal with this issue using the idea of aggregation [10]. 
In Freecraft, our model uses the count aggregator  where 
the probability that Barrack.Status transitions from Unbuilt to 
Built depends on  Barrack.BuiltBy.Task - Built], the num(cid:173)
ber of peasants in Barrack.BuilBy whose Task is Build. 

Finally, we also define rewards at the class level.  We as(cid:173)
sume for simplicity that rewards are associated only with the 
states of individual objects; adding more global dependencies 
is possible, but complicates planning significantly. We define 
a reward function 
which represents the con(cid:173)
tribution to the reward of any object in C.  For example, we 
may have a reward function associated with the Enemy class, 
which specifies a reward of 10 if the state of an enemy object 
is 
(Enemy.State  =  Dead)  =  10.  We assume 
that the reward for each object is bounded by 

Dead: 

Given a world, the RMDP uniquely defines a ground fac(cid:173)
tored MDP 
whose transition model is specified (as usual) 
as a dynamic Bayesian network (DBN) [3]. The random vari(cid:173)
ables in this factored MDP are the state variables of the in(cid:173)
dividual  objects  o.S,  for each 
and for each 
S 
Thus, the state s of the system at a given point in 
time is a vector defining the states of the individual objects in 
the world. For any subset of variables X in the model, we de(cid:173)
fine s[X] to be the part of the instantiation s that corresponds 
to the variables X.  The ground DBN  for the transition dy(cid:173)
namics specifies the dependence of the variables at time 
on the variables at time  The parents of a variable _.S' are 
the state variables of the objects 
that are linked to  . In our 
example with the two peasants, we might have the random 
variables  Peasantl.Task,  Peasant2.Task,  Barrackl.Stato, 
variable Barrackl.Status' 
etc. The parents of the time 
variables Barrackl. Status', Peasant l.Task, 
are the time 
Peasant2.Task, Goldl Amount and Woodl Amount. 

The transition model is the same for all instances in the 
same class, as in (1).  Thus, all of the o.Status variables for 

1004 

PROBABILISTIC  PLANNING 

Figure 2: Freecraft tactical domain: (a) Schema; (b) Resulting fac(cid:173)
tored MDP for a world with 2 footmen and 2 enemies. 
barrack objects o share the same conditional probability dis(cid:173)
tribution.  Note, however, that each specific barrack depends 
on the particular peasants linked to it. Thus, the actual parents 
in the DBN of the status variables for two different barrack 
objects can be different. 

The reward function is simply the sum of the reward func(cid:173)

tions for the individual objects: 

Thus,  for  reward  function  for  the  Enemy  class  described 
above, our overall reward function in a given state will be 
10 times the number of dead enemies in that state. 

It remains to specify the actions in the ground MDP The 
RMDP specifies a set of possible actions for every object in 
the world. In a setting where only a single action can be taken 
at any time step,  the agent must choose both an object to 
act on,  and which action to perform on that object.  Here, 
the  set of actions  in  the ground  MDP  is  simply the  union 
Dom[o.i4].  In a setting where multiple actions can be 
performed in parallel (say, in a multiagent setting), it might 
be possible to perform an action on every object in the domain 
at every step. Here, the set of actions in the ground MDP is a 
vector specifying an action for every object: 
Dom[o.i4]. 
Intermediate cases, allowing degrees of parallelism, are also 
possible. For simplicity of presentation, we focus on the mul(cid:173)
tiagent case, such as Freecraft, where, an action is an assign(cid:173)
ment to the action of every unit. 
Example 2.1 (Freecraft tactical domain)  Consider a sim(cid:173)
plified version of Freecraft,  whose schema is illustrated 
where  only  two  classes  of units  partici(cid:173)
in  Fig. 
pate  in  the game: 
Both 
the footman and the enemy classes have only one state 
variable  each,  Health,  with  domain Dom[Health] 
The footman class contains 
{Healthy, Wounded, 
one single-valued link: 
Enemy. 
Thus  the  transition  model for  a footman's  health  will 
depend on the health of its enemy: 

[Footman.MyJEnemy] 

if 

i.e., 

footman's enemy is 
not  dead,  than  the  probability  that  a footman  will  be(cid:173)
come  wounded,  or die,  is significantly higher.  A foot(cid:173)
man can choose to attack any enemy.  Thus each foot(cid:173)
man is associated with an action Footman.A which se(cid:173)
lects the enemy it is attacking.1  As  consequence, 

A model where an action can change the link structure in the 

In our setting, the state space is exponentially large, with 
one state for each joint assignment to the random variables 
o.S of every object (e.g., exponential in the number of units in 
the Freecraft scenario). In a multiagent problem, the number 
of actions is also exponential in the number of agents.  Thus 
this LP has both an exponential number of variables and an 
exponential number of constraints. Therefore the exact solu(cid:173)
tion to this linear program is infeasible. 

We  address  this  issue  using  the  assumption  that  the 
value  function  can  be  well-approximated  as  a  sum  of 
local  value  subfunctions  associated  with  the  individual 
objects  in  the  model. 
(This  approximation  is  a  special 
case  of  the  factored  linear  value  function  approach  used 
in  [6].)  Thus  we  associate  a  value  subfunction 
with 
every  object  in  w.  Most  simply,  this  local  value  function 
can  depend  only  on  the  state  of the  individual  object  Sa. 
In  our  example,  the  local  value  subfunction 
for 
enemy object Enemy 1 might associate a numeric value for 
each assignment to the variable Enemy J.Health. A richer 
approximation might associate a value function with pairs, 
or even small subsets, of closely related objects.  Thus, the 
world requires a small extension of our basic representation.  We 
omit details due to lack of space. 

PROBABILISTIC  PLANNING 

1005 

does not help us provide a value function for objects in other 
worlds, especially worlds with different sets of objects. 

To obtain generalization, we build on the intuition that dif(cid:173)
ferent objects in the same class behave similarly: they share 
the transition model and reward function. Although they dif(cid:173)
fer in their interactions with other objects, their local contri(cid:173)
bution to the value function is often similar.  For example, 
it may be reasonable to assume that different footmen have a 
similar long-term chance of killing enemies. Thus, we restrict 
our class of value functions by requiring that all of the objects 
in a given class share the same local value subfunction. 

Formally, we define a class-based local value subfunc(cid:173)
tion 
for each class.  We assume that the parameteriza(cid:173)
tion  of this  value  function  is  well-defined  for every object 
o in C.  This assumption holds trivially  if the scope of 
we  simply  have  a  parameter  for each  as(cid:173)
is  simply 
signment to Dom 
When the local value function can 
also depend on the states of neighboring objects,  we must 
define  the  parameterization  accordingly;  for  example,  we 
might  have  a  parameter  for  each  possible joint  state  of a 
linked  footman-enemy  pair.  Specifically  rather than defin(cid:173)
ing  separate  subfunctions 
we de(cid:173)
fine a  class-based  subfunction 
Now  the  contri(cid:173)
bution  of  Footmanl  to  the  global  value  function  will  be 
(F1.Health,El.Health).  Similarly  Footman!  will 

A class-based value function defines a specific value func(cid:173)
tion for each world w, as the sum of the class-based local 
value functions for the objects in u)\ 

contribute 

(F2.Health, E2.Health). 

As for any linear approximation to the value function, the 
LP approach can be adapted to use this value function rep(cid:173)
resentation [14]. Our LP variables are now the local compo(cid:173)
nents of the individual local value functions: 

(3) 
In our example, there will be one LP variable for each joint 
assignment of FI.Health and El.Health to represent the com(cid:173)
ponents of 
Similar LP variables will be included for 
the components of 

As before, we have a constraint for each global state s and 

each global action 

This transformation has the effect of reducing the number of 
free variables in the LP to n (the number of objects) times the 
number of parameters required to describe an object's local 
value function.  However, we still have a constraint for each 
global state and action, an exponentially large number. 

Guestrin, Koller and Parr [6] (GKP hereafter) show that, 
in certain cases,  this exponentially large LP can be solved 
efficiently and exactly.  In particular,  this compact solution 
applies  when  the MDP is  factored  (i.e.,  represented as  a 
DBN), and the approximate value function is decomposed 
as a weighted linear combination of local basis functions, as 
above. Under these assumptions, GKP present a decomposi(cid:173)
tion of the LP which grows exponentially only in the induced 
tree width of a graph determined by the complexity of the 
process dynamics and the locality of the basis function. 

This approach applies very easily here.  The structure of 
the DBN representing the process dynamics  is highly fac(cid:173)
tored, defined via local interactions between objects.  Simi(cid:173)
larly, the value functions are local, involving only single ob(cid:173)
jects or groups of closely related objects. Often, the induced 
width of the resulting graph in such problems is quite small, 
allowing the techniques of GKP to be applied efficiently. 
4  Generalizing Value Functions 
Although this approach provides us with a principled way 
of decomposing a high-dimensional value function in certain 
types of domains, it does not help us address the generaliza(cid:173)
tion problem: A local value function for objects in a world 

(5) 

This value function depends both on the set of objects in the 
world and (when local value functions can involve related ob(cid:173)
jects) on the links between them.  Importantly, although ob(cid:173)
jects in the same class contribute the same function into the 
summation of (5), the argument of the function for an object 
is the state of that specific object (and perhaps its neighbors). 
In any given state, the contributions of different objects of the 
same class can differ. Thus, every footman has the same local 
value subfunction parameters, but a dead footman will have a 
lower contribution than one which is alive. 
5  Finding Generalized MDP Solutions 
With a class-level value function, we can easily generalize 
from one or more worlds to another one.  To do so, we as(cid:173)
sume that a single set of local class-based value functions 
is a good approximation across a wide range of worlds  As(cid:173)
suming we have such a set of value functions, we can act in 
any new world  without replanning, as described in Step 3 
of Fig. 3. We simply define a world-specific value function as 
in (5), and use it to act. 

We must now optimize 

in a way that maximizes the 
value over an entire set of worlds. To formalize this intuition, 
we assume that there is a probability distribution 
over 
the worlds that the agent encounters.  We want to find a sin(cid:173)
gle set of class-based local value functions 
that is a 
good fit for this distribution over worlds. We view this task as 
one of optimizing for a single "meta-level" MDP 
where 

1006 

and the rest of the dynam(cid:173)
nature first chooses a world 
. Precisely, the state 
ics are then determined by the 
The transi(cid:173)
space of 
nature 
tion model is the obvious one: From the initial state 
and an initial state in 
chooses a world  according to 
w according to the initial starting distribution 
over the 
states in  The remaining evolution is then done according to 
dynamics. In our example, nature will choose the number 
of footmen and enemies, and define the links between them, 
which then yields a well-defined MDP,e.g., 
5.1  LP Formulation 
allows us to formalize the task of finding a 
The meta-MDP 
generalized solution to an entire class of MDPs. Specifically, 
we wish to optimize the class-level parameters for 
not for 
a single ground MDP 

but for the entire 

We can  address  this  problem  using  a similar LP solu(cid:173)
tion to the one we used for a single world in Sec. 3.  The 
variables are simply parameters of the local class-level value 
subfunctions 
For 
the constraints, recall that our object-based LP formulation 
in (4) had a constraint for each state s and each action vector 
In the generalized solution, the state space 
is the union of the state spaces of all possible worlds.  Our 
constraint set for 
will, therefore, be a union of constraint 
sets, one for each world 

each with its own actions: 

: 

(6) 
where the value function for a world, 
(s), is defined at 
the class level as in Eq. (5). In principle, we should have an 
additional constraint for the state s0. However, with a natural 
choice of state relevance weights a, this constraint is elimi(cid:173)
nated and the objective function becomes: 

Minimize: 

(7) 

if 
In some models,  the potential number 
of objects may be infinite, which could make the objective 
function unbounded.  To prevent this problem,  we assume 
that the 
goes  to  zero  sufficiently  fast,  as  the  num(cid:173)
ber of objects tends to infinity.  To understand this assump(cid:173)
tion, consider the following generative process for selecting 
worlds:  first,  the number of objects is chosen according to 
then,  the  classes  and  links  of each  object  are  cho(cid:173)
Using this decomposition, we 
The intuitive assump(cid:173)

sen according to 
have that 
tion described above can be formalized as: 

for 

some 

Thus, the distribution 

over number of objects can be chosen arbitrarily, as long as it 
is bounded by some exponentially decaying function. 
5.2  Sampling worlds 
The main problem with this formulation is that the size of 
the LP â€” the size of the objective and the number of con(cid:173)
straints â€” grows with the number of worlds, which, in most 
situations, grows exponentially with the number of possible 
objects, or may even be infinite.  A practical approach to ad(cid:173)
dress this problem is to sample some reasonable number of 
worlds from the distribution 
and then to solve the LP 

for these worlds only.  The resulting class-based value func(cid:173)
tion can then be used for worlds that were not sampled. 

We will start by sampling a set 

of  worlds according 
We can now define our LP in terms of the worlds 

to 
in D, rather than all possible worlds. For each world 
our LP will contain a set of constraints of the form presented 
in Eq. (4). Note that in all worlds these constraints share the 
variables 
which represent our class-based value function. 
The complete LP is given by: 

in 

Variables: 
Minimize: 

Subject to: 

is the marginalization of 

(8) 
to the vari(cid:173)
where 
ables in 
For each world, the constraints have the same 
form as the ones in Sec. 3.  Thus, once we have sampled 
worlds, we can apply the same LP decomposition techniques 
of GKP to each world to solve this LP efficiently. Our gener(cid:173)
alization algorithm is summarized in Step 2 of Fig. 3. 

The solution obtained by the LP with sampled worlds will, 
in general, not be equal to the one obtained if all worlds are 
considered simultaneously.  However, we can show that the 
quality of the two approximations is close, if a sufficient num(cid:173)
ber of worlds are sampled.  Specifically, with a polynomial 
number of sampled worlds, we can guarantee that, with high 
probability,  the quality of the value function approximation 
obtained when sampling worlds is close to the one obtained 
when considering all possible worlds. 
Theorem 5.1 Consider the following class-based value func(cid:173)
tions (each with k parameters): obtained from the LP over 
all possible worlds by minimizing Eq. (7) subject to the con(cid:173)
obtained from the LP with the sampled 
straints in 
worlds in (8); and 
the optimal value function of the meta-
For a number of sampled worlds m polynomial in 
MDP 
the error is bounded by: 

with probability at least 1 
where 
is the maximum per-object reward. 
The proof, which is omitted for lack of space (see online ver(cid:173)
sion of this paper), uses some of the techniques developed by 
de Farias and Van Roy [2] for analyzing constraint sampling 
in general MDPs.  However, there are two important differ(cid:173)
ences: First, our analysis includes the error introduced when 
sampling the objective, which in our case is a sum only over 
a subset of the worlds rather than over all of them as in the 
LP for the full meta-MDP. This issue was not previously ad(cid:173)
dressed. Second, the algorithm of de Farias and Van Roy re(cid:173)
lies on the assumption that constraints are sampled according 

PROBABILISTIC  PLANNING 

1007 

to some "ideal" distribution (the stationary distribution of the 
optimal  policy).  Unfortunately,  sampling from this distribu(cid:173)
tion is as difficult as computing a near-optimal policy.  In our 
analysis, after each world is sampled, our algorithm exploits 
the factored structure in the model to represent the constraints 
exactly, avoiding the dependency on the "ideal" distribution. 
6  Learning Classes of Objects 
The  definition  of a  class-based  value  function  assumes  that 
all  objects  in a class have the  same  local  value function.  In 
many cases, even objects in the same class might play differ(cid:173)
ent roles in the model, and therefore have a different impact 
on  the  overall  value.  For example,  if only  one  peasant  has 
the capability to build barracks, his status may have a greater 
impact.  Distinctions of this type are not usually known in ad(cid:173)
vance, but are learned by an agent as it gains experience with 
a domain and detects regularities. 

We propose a procedure that takes exactly this approach: 
of  worlds 

Assume that we have been presented with a set 

For each  world 

an approximate  value  function 

was computed as described  in  Sec.  3.  In addi(cid:173)
tion, each object is associated with a set of features. 
For 
example, the features may include local information, such as 
whether the object is a peasant linked to a barrack, or not, as 
well as global information, such as whether this world con(cid:173)
tains archers in addition to footmen. We can define our "train(cid:173)
ing data" 

We now have a well-defined learning problem:  given this 
training  data,  we  would  like  to  partition  the  objects  into 
classes, such that objects of the same class have similar value 
functions.  There  are  many  approaches  for  tackling  such  a 
task. We choose to use decision tree regression, so as to con(cid:173)
struct a tree that predicts the local value function parameters 
given the features.  Thus, each split in the tree corresponds to 
a feature in 
each branch down the tree defines a subset 
of local  value functions in  whose feature values are as de(cid:173)
fined by the path; the leaf at the end of the path is the average 
value function for this set.  As the regression tree learning al(cid:173)
gorithm tries to construct a tree which is predictive about the 
local value function, it will aim to construct a tree where the 
mean at each leaf is very close to the training data assigned to 
that leaf. Thus, the leaves tend to correspond to objects whose 
local value functions are similar.  We can thus take the leaves 
in  the  tree  to define  our subclasses,  where  each  subclass  is 
characterized  by  the  combination  of feature  values  specified 
by the path to the corresponding leaf.  This algorithm is sum(cid:173)
marized in Step 1 of Fig. 3. Note that the mean subfunction at 
a leaf is not used as the value subfunction for the correspond(cid:173)
ing class; rather, the parameters of the value subfunction are 
optimized using the class-based LP in Step 2 of the algorithm. 
7  Experimental  results 
We evaluated  our generalization  algorithm  on  two domains: 
computer network administration and Freecraft. 
7.1  Computer  network  administration 
For this  problem,  we implemented our algorithm in Matlab, 
using CPLEX as the LP solver.  Rather than using the full LP 
decomposition  of GKP  [6],  we  used  the  constraint  genera(cid:173)
tion extension proposed in 113], as the memory requirements 

1. Learning Subclasses: 

â€¢  Input: 

(c)  Define a subclass for each leaf, characterized by the fea(cid:173)

ture vector associated with its path. 
2.  Computing Class-Based Value Function: 

â€¢  Algorithm: 
(a)  Compute the parameters for 

the LP in (8) relative to the worlds in 

that optimize 

3.  Acting in a New World: 

â€¢  Input: 

â€¢  Algorithm: Repeat 
(a)  Obtain the current state s. 
(b)  Determine the appropriate class 

cording to its features. 

(c)  Define 
(d)  Use  the  coordination  graph  algorithm  of GKP  to com(cid:173)

according to (5). 

pute an action a that maximizes 

(e)  Take action 

in the world. 

Figure 3:  The overall generalization algorithm. 

were lower for this second approach.  We experimented with 
the multiagent computer network examples in [6], using vari(cid:173)
ous network topologies and "pair" basis functions that involve 
states of neighboring machines (see [6]).  In one of these prob(cid:173)
lems,  if we have n computers, then the underlying MDP has 
9n  states  and  2n  actions.  However,  the  LP  decomposition 
algorithm uses structure in the underlying factored  model  to 
solve such problems very efficiently [61. 

We  first  tested  the  extent  to  which  value  functions  are 
shared across objects. In Fig. 4(a), we plot the value each ob(cid:173)
ject  gave  to  the  assignment  Status  =  working,  for instances 
of the  'three  legs'  topology.  These  values  cluster  into  three 
classes.  We used 
to  learn  decision  trees  for  our 
class partition. In this case, the learning algorithm partitioned 
the  computers  into  three  subclasses  illustrated  in  Fig.  4(b): 
'server*,  'intermediate', and  'leaf.  In Fig. 4(a), we see that 
'server'  (third column) has the highest value, because a bro(cid:173)
ken server can cause a chain reaction affecting the whole net(cid:173)
work, while  'leaf  value (first column) is lowest, as it cannot 
affect any other computer. 

We then evaluated the generalization quality of our class-
based value function by comparing its performance to that of 
planning specifically for a new environment.  For each topol(cid:173)
ogy, we computed the class-based value function with 5 sam(cid:173)
pled  networks  of up  to  20  computers.  We  then  sampled  a 

1008 

PROBABILISTIC  PLANNING 

(a) 

(b) 

(c) 

(c) 

(d) 

Figure 4: Network administrator results: (a) Training data for learning classes; (b) Classes learned for 'three legs'; (c) Generalization quality 
(evaluated by 20 Monte Carlo runs of 100 steps); (d) Advantage of learning subclasses. Tactical Freecraft: (e) 3 footmen against 3 enemies. 
new network and computed for it a value function that used 
the same factorization, but with no class restrictions.  This 
value function has more parameters â€” different parameters 
for each object, rather than for entire classes, which arc opti(cid:173)
mized for this particular network. This process was repealed 
for 8 sets of networks.  The results, shown in Fig. 4(c), in(cid:173)
dicate that the value of the policy from the class-based value 
function is very close to the value of replanning, suggesting 
that we can generalize well to new problems.  We also com(cid:173)
puted a utopic upper bound on the expected value of the opti(cid:173)
mal policy by removing the (negative) effect of the neighbors 
on the status of the machines.  Although this bound is loose, 
our approximate policies still achieve a value close to it. 

for a world with 3 footmen and 3 enemies, shown in Fig. 4(e). 
The resulting policy (which is fairly complex) demonstrates 
successful  coordination  between  our  footmen:  initially  all 
three footmen focus on  one enemy.  When the enemy be(cid:173)
comes injured, one footman switches its target. Finally, when 
the enemy is very weak, only one footman continues to at(cid:173)
tack it, while the others tackle a different enemy.  Using this 
policy, our footmen defeat the enemies in Freecraft. 

Next, we wanted to determine if our procedure for learn(cid:173)
ing classes  yields  better approximations  than  the ones ob(cid:173)
tained from the default classes. Fig. 4(d) compares the max-
norm error between our class-based value function and the 
one obtained  by replanning.  The  graph  suggests  that,  by 
learning classes using our decision trees regression tree pro(cid:173)
cedure, we obtain a much better approximation of the value 
function we would have, had we replanned. 
7.2  Freecraft 
In order to evaluate our algorithm in the Freecraft game, we 
implemented the methods in C++ and used CPLEX as the LP 
solver. We created two tasks that evaluate two aspects of the 
game: long-term strategic decision making and local tactical 
battle maneuvers.  Our Freecraft interface, and scenarios for 
these and other more complex tasks are publicly available at: 
http://dags stanfoid.edu/Freecraft/. For each task we designed 
an RMDP model to represent the system,  by consulting a 
"domain expert". After planning, our policies were evaluated 
on  the  actual  game.  To  better  visualize  our  results,  we 
direct the reader to view videos of our policies at a website: 
http.//lobotics.stantord.edu/~guestrin/Re3earch/Generalization/. 
This website also contains details on our RMDP model. It is 
important to note that, our policies were constructed relative 
to  a  very  approximate  model  of the  game,  but  evaluated 
against the real game. 

In the tactical model, the goal is to take out an opposing 
enemy force  with  an  equivalent number of units.  At each 
time step, each footman decides which enemy to attack. The 
enemies are controlled using Freecraft's hand-built strategy. 
We modelled footmen and enemies as each having 5 "health 
points", which can decrease as units are attacked. We used a 
simple aggregator to represent the effect of multiple attack(cid:173)
ers. To encourage coordination, each footman is linked to a 
"buddy" in a ring structure. The local value functions include 
terms over triples of linked variables.  We solved this model 

The factors generated in our planning algorithm grow ex(cid:173)
ponentially in the number of units, so planning in larger mod(cid:173)
els is infeasible.  Fortunately, when executing a policy, we 
instantiate the current state at every time step, and action se(cid:173)
lection is significantly faster [61. Thus, even though we can(cid:173)
not execute Step 2 in Fig. 3 of our algorithm for larger sce(cid:173)
narios, we can generalize our class-based value function to a 
world with 4 footmen and enemies, without replanning using 
only Step 3 of our approach. The policy continues to demon(cid:173)
strate successful coordination between footmen, and we again 
beat Freecraft's policy.  However, as the number of units in(cid:173)
creases, the position of enemies becomes increasingly impor(cid:173)
tant. Currently, our model does not consider this feature, and 
in a world with 5 footmen and enemies, our policy loses to 
Freecraft in a close battle. 

In the strategic model, the goal is to kill a strong enemy. 
The player starts with a few peasants, who can collect gold or 
wood, or attempt to build a barrack, which requires both gold 
and wood.  All resources are consumed after each Build ac(cid:173)
tion. With a barrack and gold, the player can train a footman. 
The footmen can choose to attack the enemy. When attacked, 
the enemy loses "health points", but fights back and may kill 
the footmen. We solved a model with 2 peasants, 1 barrack, 2 
footmen, and an enemy. Every peasant was related to a "cen(cid:173)
tral" peasant and every footman had a "buddy".  The scope 
of our local value function included triples between related 
objects. The resulting policy is quite interesting: the peasants 
gather gold and wood to build a barrack, then gold to build a 
footman. Rather than attacking the enemy at once, this foot(cid:173)
man waits until a second footman is built. Then, they attack 
the enemy together.  The stronger enemy is able to kill both 
footmen, but it becomes quite weak. When the next footman 
is trained, rather than waiting for a second one, it attacks the 
now weak enemy, and is able to kill him. Again, planning in 
large scenarios is infeasible, but action selection can be per(cid:173)
formed efficiently.  Thus, we can use our generalized value 
function to tackle a world with 9 peasants and 3 footmen, 
without replanning.  The 9 peasants coordinate to gather re(cid:173)
sources.  Interestingly, rather than attacking with 2 footmen, 
the policy now waits for 3 to be trained before attacking. The 
3 footmen kill the enemy, and only one of them dies.  Thus, 

PROBABILISTIC PLANNING 

1009 

we have successfully generalized from a problem with about 
10G joint state-action pairs to one with over 1013 pairs. 
8  Discussion and Conclusions 
In this paper, we have tackled a longstanding goal in planning 
research, the ability to generalize plans to new environments. 
Such generalization has two complementary uses:  First we 
can tackle new environments with minimal or no replanning. 
Second it allows us to generalize plans from smaller tractable 
environments to significantly larger ones, which could not be 
solved directly with our planning algorithm. Our experimen(cid:173)
tal results support the fact that our class-based value function 
generalizes well to new plans and that the class and subclass 
structure discovered by our learning procedure improves the 
quality of the approximation.  Furthermore, we successfully 
demonstrated our methods on a real strategic computer game, 
which contains many characteristics present in real-world dy(cid:173)
namic resource allocation problems. 

Several other papers consider the generalization problem. 
Several approaches can represent value functions in general 
terms,  but usually require it to be hand-constructed for the 
particular task. Others [12; 8; 4 J have focused on reusing so(cid:173)
lutions from isomorphic regions of sta4e space.  By compari(cid:173)
son, our method exploits similarities between objects evolv(cid:173)
ing in parallel. It would be very interesting to combine these 
two types of decomposition.  The work of Boutilier et al.  [ll 
on symbolic value iteration computes first-order value func(cid:173)
tions, which generalize over objects.  However, it focuses on 
computing exact value functions, which are unlikely to gen(cid:173)
eralize to a different world. Furthermore, it relies on the use 
of theorem proving tools, which adds to the complexity of the 
approach. Methods in deterministic planning have focused on 
generalizing from compactly described policies learned from 
many domains to incrementally build a first-order policy [9; 
11].  Closest in spirit to our approach is the recent work of 
Yoon et al. [17], which extends these approaches to stochastic 
domains. We perform a similar procedure to discover classes 
by finding structure in the value function.  However, our ap(cid:173)
proach finds regularities in compactly represented value func(cid:173)
tions rather than policies. Thus, we can tackle tasks such as 
multiagent planning, where the action space is exponentially 
large and compact policies often do not exist. 

The key assumption in our method is interchangeability 
between objects of the same class. Our mechanism for learn(cid:173)
ing subclasses  allows us to deal  with cases where objects 
in the domain can vary, but our generalizations will not be 
successful in very heterogeneous environments, where most 
objects have very different influences on the overall dynam(cid:173)
ics or rewards.  Additionally,  the efficiency of our LP solu(cid:173)
tion algorithm depends on the connectivity of the underlying 
problem.  In a domain with strong and constant interactions 
between many objects (e.g., Robocup), or when the reward 
function depends arbitrarily on the state of many objects (e.g., 
Blocksworld), the solution algorithm will probably not be ef(cid:173)
ficient. In some cases, such as the Freecraft tactical domain, 
we can use generalization to scale up to larger problems.  In 
others, we could combine our LP decomposition technique 
with constraint sampling [2] to address this high connectiv(cid:173)
ity issue. In general, however, extending these techniques to 
highly connected problems is still an open problem. Finally, 

although we have successfully applied our class-value func(cid:173)
tions to new environments without replanning, there are do(cid:173)
mains where such direct application would not be sufficient 
to obtain a good solution.  In such domains, our generalized 
value functions can provide a good initial policy, which could 
be refined using a variety of local search methods. 

We have assumed that relations do not change over time. 
In many domains (e.g., Blocksworld or Robocup),  this as(cid:173)
sumption is false.  In recent work, Guestrin et al.  [71 show 
that context-specific independence can allow for dynamically 
changing coordination structures in multiagent environments. 
Similar ideas may allow us to tackle dynamically changing 
relational structures. 

In summary, we believe that the class-based value func(cid:173)
tions methods presented here will significantly further the ap(cid:173)
plicability of MDP models to large-scale real-world tasks. 
Acknowledgements  We are very grateful to Ron Parr for many 
useful discussions.  This work was supported by the DoD MURI 
program, administered by the Office of Naval Research under Grant 
N00014-00-1-0637, and by Air Force contract F30602-00-2-0598 
under DARPA's TASK program. 
References 
[I]  C. Boutilier, R. Reitcr, and B. Price. Symbolic dynamic pro(cid:173)

gramming for first-order MDPs. In IJCA1-0I, 2001. 

12]  D.P. de Farias and B. Van Roy.  On constraint sampling for 
the linear programming approach to approximate dynamic pro(cid:173)
gramming. Submitted to Math, of Operations Research, 2001. 

[3]  T. Dean and K. Kanazawa.  Probabilistic temporal reasoning. 

In AAA1-88, 1988. 

[4]  T. G. Dietterich. Hierarchical reinforcement learning with the 
MAXQ value function decomposition.  Journal of Artificial 
Intelligence Research, 13:227-303, 2000. 

[5] R E. Fikes, P. E. Hart, and N. J. Nilsson. Learning and execut(cid:173)
ing generalized robot plans. Artf. Intel, 3(4):251-288, 1972. 
[6]  C. E. Guestrin, D. Koller, and R. Parr.  Multiagent planning 

with factored MDPs. In NIPS-14, 2001. 

|7]  C. E. Guestrin, S. Venkataraman, and D. Roller.  Context 
specific multiagent coordination and planning with factored 
MDPs. In AAA/-02, 2002. 

18]  M.  Hauskrecht,  N.  Meuleau,  L.  Kaelbling,  T.  Dean,  and 
C. Boutilier.  Hierarchical solution of Markov decision pro(cid:173)
cesses using macro-actions. In UAI, 1998. 

[9]  R. Khardon. Learning action strategics for planning domains. 

Artificial Intelligence, 113:125-148, 1999. 

[10]  D. Koller and A. Pfeffer.  Probabilistic frame-based systems. 

In AAA A 1998. 

[II]  M. Martin and H. Gcffner.  Learning generalized policies in 

planning using concept languages. In KR, 2000. 

[12] R. Parr. Flexible decomposition algorithms for weakly coupled 

markov decision problems. In UAI-98, 1998. 

[13] D. Schuurmans and R. Patrascu.  Direct value-approximation 

for factored MDPs. In NIPS-14, 2001. 

[14] P. Schweitzer and A. Seidmann. Generalized polynomial ap(cid:173)
proximations in Markovian decision processes. / of Mathe(cid:173)
matical Analysis and Applications, 110:568-582, 1985. 

[15] R. Sutton and A. Barto. Reinforcement Learning: An Intro-

duction. MIT Press, Cambridge, MA, 1998. 

[16] S. Thrun and J. O'Sullivan. Discovering structure in multiple 

learning tasks: The TC algorithm. In 1CML-96, 1996. 

[17] S. W. Yoon, A. Fern, and B. Givan. Inductive policy selection 

for first-order MDPs. In UAI-02, 2002. 

1010 

PROBABILISTIC  PLANNING 


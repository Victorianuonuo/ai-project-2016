Distributed Clustering Based on Sampling Local Density Estimates 
Gianluca  Moro 
Matthias Klusch 

Stefano Lodi 

Deduction and Multiagent Systems  Department of Electronics, 

Department  of Electronics, 

Computer Science and Systems  Computer Science and Systems 

German Research Centre 
for Artificial Intelligence 
Stuhlsatzenhausweg 3 

University of Bologna 
Viale Risorgimento 2 

University  of Bologna 
Via Rasi e Spinelli, 176 
1-47023 Cesena FC, Italy 

66123  Saarbruecken, Germany 

1-40136 Bologna BO, Italy 
klusch@dfki.de slodi@deis.unibo.it gmoro@deis.unibo.it 

Abstract 

Huge  amounts  of data  are  stored  in  autonomous, 
geographically  distributed  sources.  The  discov(cid:173)
ery  of previously  unknown,  implicit  and  valuable 
knowledge  is  a  key  aspect  of the  exploitation  of 
such  sources. 
In  recent  years  several  approaches 
to  knowledge  discovery  and  data  mining,  and  in 
particular to  clustering,  have been  developed,  but 
only a few of them are designed for distributed data 
sources.  We propose a novel distributed clustering 
algorithm  based  on  non-parametric  kernel  density 
estimation,  which takes  into  account the  issues  of 
privacy and communication costs that arise in a dis(cid:173)
tributed environment. 

Introduction 

1 
Knowledge  discovery  is  a  process  aiming  at  the  extraction 
of previously  unknown  and  implicit  knowledge  out  of large 
databases, which may potentially be of added value for some 
given application [Fayyad et  al.,  1996]. 

Data  mining,  which  is  devoted  to  the  automated  extrac(cid:173)
tion of unknown patterns from given data, is a central element 
among the steps of the overall knowledge discovery process; 
the steps include preparation of the data to be analyzed as well 
as evaluation and visualization of the discovered knowledge. 
The large variety of data mining techniques which have been 
developed over the past decade include:  methods for pattern-
based similarity search, cluster analysis,  decision-tree based 
classification, generalization taking the data cube or attribute-
oriented induction approach, and mining of association rules 
[Chen et al. , 1996] 

The  increasing  demand  to  scale  up  to  massive  data  sets 
which  are  inherently  distributed  over  networks  with  limited 
bandwidth and  computational  resources  has  led to  methods 
for  parallel  and  distributed  knowledge  discovery  [Kargupta 
et al.,  2000],  The related pattern extraction problem in dis(cid:173)
tributed knowledge discovery is referred to as distributed data 
mining.  Distributed data mining is expected to perform par(cid:173)
tial  analysis  of data  at  individual  sites  and then to  send  the 
outcome as partial result to other sites where it is sometimes 
aggregated to the global result. 

One of the  most common approaches of business  applica(cid:173)
tions to perform distributed data mining is to centralize dis(cid:173)

tributed  data  into  a  data  warehouse  on  which  to  apply  the 
usual data mining techniques.  Data warehousing is a popular 
technology which integrates data from multiple data sources 
into a single repository in order to efficiently execute complex 
analysis queries [Moro and Sartori, 2001].  However, despite 
its commercial success, this approach may be impractical or 
even impossible for some business settings, for instance: 

•  when huge amounts of data are (frequently) produced at 
different sites and the cost for their centralization cannot 
scale in terms of communication, storage and computa(cid:173)
tion; 

•  whenever data owners cannot or do not want to release 
information,  for  instance  to  protect privacy  or  because 
disclosing such information may result in a competitive 
advantage or a considerable commercial added value. 

One of the most studied data mining techniques in central(cid:173)
ized  environments  is  data  clustering.  The  goal  of this  tech(cid:173)
nique is to decompose or partition a data set into groups such 
that both  intra-group  similarity  and  inter-group  dissimilarity 
are maximized.  Despite the success of data clustering in cen(cid:173)
tralized environments, only a few approaches to the problem 
in a distributed environment are available to date. 

In this  work  we  present  KDEC,  a  novel  approach  to  dis(cid:173)
tributed data clustering based on sampling density estimates. 
In KDEC each data source transmits an estimate of the prob(cid:173)
ability density  function  of its  local  data to a helper site,  and 
then  executes  a  density  based  clustering  algorithm  that  is 
driven by  the  overall  density  estimate,  which  is built by the 
helper from the samples of the local densities. 

The  paper  is  organized  as  follows. 

In  Section  2  we  de(cid:173)
scribe related work and highlight differences with respect to 
our approach.  Section 3 and 4 present the KDEC scheme to 
distributed data clustering.  Finally,  Section 5 concludes the 
paper and outlines ongoing and future research work. 

2  Related work 
In [Johnson and Kargupta,  1999] a tree clustering approach is 
taken to build a global  dendrogram  from individual  dendro(cid:173)
grams that are computed at local data sites subject to a given 
set of requirements.  In contrast to the approach presented in 
this paper, the distributed data sets are assumed to be hetero(cid:173)
geneous,  therefore  every  site has  access  only to  a  subset of 
the features of an object.  The proposed solution implements 

LEARNING 

485 

a  distributed  version  of  the  single-link  clustering  algorithm 
which  generates clusters that are substantially different from 
the ones generated by density-based  methods.  In particular, 
it  suffers  from  the  so-called  chaining  effect,  by  which  any 
of two well  separated and  internally homogeneous groups of 
objects connected only by a dense sequence of objects are re(cid:173)
garded as a single cluster.  [Kargupta et  al,  2001] proposes a 
technique for distributed principal  component analysis, Col(cid:173)
lective PCA. It is shown that the technique satisfies efficiency 
and data security requirements and can be integrated with ex(cid:173)
isting clustering methods in order to cluster distributed, high-
dimensional  heterogeneous  data.  Since  the  dimensionality 
of the  data  is  reduced  prior  to  clustering  by  applying  PCA, 
the approach is orthogonal to ours.  Another related research 
direction  deals  with  incremental  clustering  algorithms.  The 
BIRCH  iZhang  et a/.,  1996]  and  related  BUBBLE  method 
[Ganti  et  al,  1999],  compute  the  most  accurate  clustering, 
given the amount of memory available, while minimizing the 
number  of  I/O  operations. 
It  uses  a  dynamic  index  struc(cid:173)
ture of nodes that store synthetic, constant-time maintainable 
summaries of sets of data objects.  The method is sufficiently 
scalable  requiring  0(N\ogN)  time  and  linear  I/O.  However, 
since it uses the centroid to incrementally aggregate objects, 
the  method  exhibits  a  strong bias  towards  globular clusters. 
IncrementalDBSCAN LEster et a/.,  1998] is a dynamic clus(cid:173)
tering method supporting both insertions and deletions, which 
is shown to be equivalent to the well-known static DBSCAN 
algorithm. Since in turn DBSCAN can be shown to be equiv(cid:173)
alent to a method based on density estimation when the kernel 
function is the square pulse and the clusters are density-based, 
IncrementalDBSCAN  is less general than methods based on 
kernel  density  estimates.  Its  time complexity  is  0(N\ogN). 

3  Data Clustering 
3.1  The cluster analysis problem 
Cluster  analysis  is  a  a  descriptive  data  mining  task  which 
aims  at  decomposing  or  partitioning  a  usually  multivariate 
data  set  into  groups  such  that the data objects  in one group 
are  similar  to each  other and  are  different as  possible  from 
those in other groups.  Therefore, a clustering algorithm J?(-) 
is a mapping from any data set S of objects to a clustering of 
5, that is, a collection of pairwise disjoint subsets of 5.  Clus(cid:173)
tering  techniques  inherently  hinge  on  the  notion  of distance 
between data objects to be grouped, and all we need to know 
is the set of interobject distances but not the values of any of 
the data object variables.  Several techniques for data cluster(cid:173)
ing are available but must be matched by the developer to the 
objectives  of the  considered  clustering  task  [Grabmeier  and 
Rudolph, 2002].  In partition-based clustering,  for example, 
the task  is to partition  a given data set into multiple disjoint 
sets of data objects such that the objects within each  set are 
as homogeneous as possible.  Homogeneity here is captured 
by  an  appropriate  cluster  scoring  function.  Another  option 
is based on the  intuition that homogeneity  is expected to be 
high in densely populated regions of the given data set.  Con(cid:173)
sequently, searching for clusters may be reduced to searching 
for dense regions of the data space which  are more likely to 
be populated by data objects.  That leads us to the approach 

of density estimation based clustering. 

3.2  Density  estimation  based  clustering 
In  density  estimation  (DE)  based  clustering  the  search  for 
densely  populated  regions  is  accomplished  by  estimating  a 
so-called  probability  density  function  from  which  the  given 
data  set  is  assumed  to  have  arisen.  Many  techniques  for 
DE-based  clustering  are  available  from  the  vast  KDD  liter(cid:173)
ature [Ankerst et  al,  1999; Ester et  al,  1996; Schikuta, 1996; 
Hinneburg and Keim, 1998] and statistics [Silverman, 1986]. 
In  both  areas,  the  proposed  clustering  methods  require  the 
computation  of  a  non-parametric  estimation  of  the  density 
function  from  the  data.  One  important  family  of  non-
parametric estimates is known as kernel estimators. The idea 
is  to  estimate  a  density  function  by  defining  the  density  at 
any data object  as being proportional  to  a  weighted  sum  of 
all objects  in the data set,  where the  weights are defined by 
an  appropriately  chosen  kernel  function. 
In  the  following 
we introduce kernel-based density estimation [Parzen,  1962; 
Silverman,  1986]  and  our  approach  to  density  estimation 
based clustering. 

Let us  assume a set 

of  data 
points  or  objects.  Kernel  estimators  originate  from  the  in(cid:173)
tuition  that  the  higher  the  number  of neighbouring  data  ob(cid:173)
jects x;  of some  given  object 
,  the higher the density 
at this  object x.  However,  there  can  be  many  ways  of cap(cid:173)
turing  and  weighting  the  influence  of  data  objects.  When 
given the distance between one data object x and another Jc, 
as an argument,  the influence of jf; may be quantified by us(cid:173)
ing  a  so  called  kernel  function.  A  kernel function  K (x)  is 
a  real-valued,  non-negative  function  on  R  which  has finite 
integral over R.  When computing a kernel-based density es(cid:173)
timation  of the  data  set  5,  any  element jf,-  in  S  is  regarded 
as  to  exert  more  influence  on  some 
than  elements 
than  the  element.  Accordingly, 
which  are  farther  from 
kernel  functions  are  often  non-increasing  with 
.  Promi(cid:173)
nent examples of kernel functions are the square pulse func(cid:173)
tion 
and the Gaussian function 

A  kernel-based  density  estimate 

is 
defined,  modulo a normalization factor,  as the sum  over all 
data objects xi in S of the distances between 
scaled by 
a factor A, called window width,  and weighted by the kernel 
function  K: 

(1) 

The  influence  of data  objects  and  the  smoothness  of the 
estimate  is  controlled  by  both  the  window  width  h  and  the 
shape of kernel K\  h controls the smoothness of the estimate, 
whereas  K  determines  the  decay  of the  influence  of a  data 
object according to the distance.  Even if the number N of data 
objects is very large, in practice it is not necessary to compute 
N  distances  for  calculating  the  kernel  density  estimate  at  a 
given  object x.  In  fact,  the  value  of commonly  used  kernel 
functions is negligible for distances larger than a few h units; 

486 

LEARNING 

it may even be zero  if the kernel  has bounded  support,  as  it 
is the case,  for example, for the square pulse.  Using kernel-
based density estimation,  it is straightforward to decompose 
the clustering problem into three phases as follows. 

1.  Choose a window width h and a kernel function K. 
2.  Compute  the  kernel-based  density  estimate 

from the given data set. 

3.  Detect  regions of the  data space  where  the  value  of the 
estimate  is high and group all  data objects of space re(cid:173)
gions into corresponding clusters. 

In the  literature,  many different definitions of cluster have 
been proposed formalizing the clusters referred  to  in  step 3 
above.  A  density-based  cluster  [Ester  el  al,  1996]  collects 
all  data  objects  included  in  a  region  where  density  exceeds 
a  threshold.  Center-defined clusters  [Hinneburg  and  Kcim, 
1998]  are based on the  idea that every  local  maximum  of 
corresponds to a cluster including all data objects which can 
be connected to the maximum by a continuous, uphill path in 
the  graph  of (p.  Finally,  an arbitrary-shape cluster  LHinneb-
urg  and  Keim,  1998]  is the  union  of center-defined  clusters 
having their maxima connected by a continuous path whose 
density exceeds a threshold. 

to retrieve,  given 

Algorithm  1  (DE-cluster)  implements  the  computation  of 
center-defined clusters by a climbing procedure driven by the 
density  estimate.  The  main  procedure  is  DECluster,  taking 
as inputs an instance S of the class of data objects, the kernel 
function  K,  the  window  width  h,  and  returning  a  clustering 
represented  by  C,  which  stores  a  mapping  from  each x,  to 
the  unique  integer label  ofxif,'s  cluster.  It  is  assumed  that S 
is an instance of a class which provides the following meth(cid:173)
ods:  get(i)  to  access  object  x,  given  index  i,  NQ(k,x)  and 
Radius 
the  indexes and maxi(cid:173)
nearest  neighbours.  Uphill  computes 
mum distance  of. 
the steepest direction on the graph of the estimated density as 
the  versor of its gradient,  computed by  function DEGradient 
(cf.  [Hinneburg and Keim,  1998]).  Uphill then moves in that 
direction a fractionS . _ of the distance S.Radius 
of the 
k th nearest neighbour off, and finally returns the index of the 
nearest neighbour in S of the reached position.  Every  nested 
call  to FixedPoint  marks the  current object x,  as visited and 
calls  Uphill  to  get  the  index  of the  next  data  object  Xj. 
If 
such object has already been visited, the proximity of a local 
maximum has been reached and j is taken as new cluster la(cid:173)
bel.  Otherwise 
is inductively assumed to lie at the bottom 
end  of a path  leading to the proximity  of a  local  maximum, 
and to be already labeled accordingly.  (If 
is not marked as 
clustered, a recursive call ensures that the assumption holds.) 
The complexity of the DE-cluster algorithm  is that of call(cid:173)
ing N — S.count  times FixedPoint.  At the beginning of every 
iteration  in  DECluster,  the  sets  of clustered  and  visited  ob(cid:173)
jects  are  equal.  FixedPoint  is  never called with  a  clustered 
object  as  argument,  and  visits  unclustered  objects  at  most 
once.  Therefore,  even  if the  number of visited  data  objects 
in one  call  of FixedPoint  is  bounded  only by N,  the  number 
of visited  data  objects  in  all  calls  is  still  only  N.  For  each 
visited  object  a  single K-nearest neighbour query  suffices to 
compute the gradient and the next uphill object.  The methods 
can be efficiently implemented by 

equipping the class of S with a spatial access method like the 
KD-, or MVP-, or M-tree.  Therefore, the time complexity of 
DEClusler  is  0(Nq(N)),  where  q(N)  is  the  cost  of a  k  near(cid:173)
est neighbour query in any such access method.  Note that in 
many practical cases, q(N) is very close to log TV. 

4  Distributed Data Clustering 
The  body  of work  on  applications  of data  clustering  in  dis(cid:173)
tributed  environments,  the  problem  of so  called  distributed 
data  clustering  (DDC),  is  comparatively  small.  In  this  sec(cid:173)
tion we adopt the kernel  density estimation based clustering 
approach  presented  above  for the  distributed  case  assuming 
homogeneous data, which means that a data object cannot be 
split across two sites. 

The  D DC  Problem 

4.1 
We define the problem of homogeneous distributed data clus(cid:173)
tering for a clustering algorithm  A  as  follows.  Let 

be  a  data  set  of objects.  Let 

1 , . . ., M, be a finite set of sites.  Each site 
stores one data 
set Dj,  and  it  will  be  assumed  that 
.  The DDC 
problem  is  to  find  a  site  clustering  Cj  residing  in  the  data 
space of L  j, for 
(i). 
(ii).  Time  and  communications  costs  are  minimized  (effi(cid:173)

(correctness  requirement) 

such that 

ciency  requirement) 

(iii). At the end of the computation, the size of the subset of S 
which has been transferred out of the data space of any 
site 

is  minimized  (privacy  requirement). 

LEARNING 

487 

The traditional solution to the homogeneous distributed data 
clustering problem is to simply collect all the distributed data 
sets  Dj  into  one  centralized  repository  where  their  union  S 
is  computed,  and  the  clustering  C  of the  union  S  is  com(cid:173)
puted and transmitted to the sites.  Such approach, however, 
does not  satisfy our problem's requirements both in terms of 
privacy and efficiency.  We therefore propose a different ap(cid:173)
proach yielding a kernel density estimation based clustering 
scheme, called KDEC. 

The  K D EC  Scheme  for  D DC 

4.2 
The  key  idea  of the  KDEC  scheme  is  based  on  the  follow(cid:173)
ing observation:  Although the density estimate computed on 
each  local  data  set  gives  information  on  the  distribution  of 
the objects in the data set, it conceals the objects themselves. 
Moreover, the local density estimate can be coded to provide 
a more compact representation of the data set for the purpose 
of transmission.  In the sequel, we tacitly assume that all sites 
Lj  agree  on  using  a  global  kernel  function  K  and  a  global 
window  width  h.  We  will  therefore omit  K  and  h from  our 
notation, and write 
.  Density estimates 
in the form of Equation  (1) are additive,  i.e.  the global den(cid:173)
sity estimate 
can be decomposed info the sum of the 
site density estimates, one estimate for every data set Dj. 

(2) 

Thus,  the  local  density  estimates  can  be  transmitted  to  and 
summed up at a distinguished helper site yielding the global 
estimate which can be returned to all sites.  Each site Lj may 
then apply, in its local data space, the hill-climbing technique 
of Algorithm 1 (DE-cIuster) to assign clusters to the local data 
objects.  There is nevertheless a weakness in such a plan:  the 
definition of a density estimate explicitly refers to all the data 
objects J?;.  Hence,  knowing how  to manipulate the estimate 
entails  knowing  the data objects,  which  contradicts  the pri(cid:173)
vacy  requirement.  However,  only  an  intensional,  algebraic 
definition of the estimate includes knowledge of the data ob(cid:173)
jects.  Multidimensional  sampling  theory  provides  the  basis 
for  an  alternative  extensional  representation  of the  estimate 
which makes no explicit reference to the data objects. 

The theoretical  idea of sampling is to represent a function 
/ by  a sampling  series,  that  is,  a  summation  of suitable  ex(cid:173)
pansion  functions  weighted  by  the  values  of /  at  a  discrete 
subset of its domain [Higgins,  1996].  In the following, let the 
/th  coordinate of 

and let  Diag 
diagonal  matrix 

= 

be denoted by 
,  denote  th< 
having  diagonal  w,  i.e.,  defined  by 
Diag 
e
of sampling periods.  The sampled 

t' 
form 
is  the  s e q u e n c e d e f i n ed  by 

Further 

l

if 
a  vector 
of 

at intervals 

(3) 

where • is the inner product between vectors. Therefore, 
is  the  sequence  of  the  values  of 
at  all  the  real,  n-
dimensional  vectors  whose  i th  coordinates  are  spaced  by  a 
multiple of the i th sampling period Ti , 1 = 1 , . . . , / !. The sam(cid:173)
pled forms of the local density estimates are defined in a sim(cid:173)
ilar  way  by 
. . . , M.  It 

is immediate to see by (2) that addivity holds for the sampled 
forms: 

(4) 

Therefore,  after  receiving  the  sampled  forms 
of  the 
M  density  estimates,  the helper site  can compute  by  (4)  the 
sampled  form  of the  overall  estimate  and  transmit  it  to  the 
sites Lj.  Sites Lj can then cluster local data with respect to the 
overall  density  estimate,  using  the  gradient  of the  sampling 
series 

(5) 

as needed in the hill-climbing function. 

We  briefly  discuss  the  extent  to  which  the  series  (5)  can 
It  is  well  known  that,  under 
be  used  to  represent 
mild  conditions,  sampling  a  function 
is  an  invertible 
transformation  if,  for  every  coordinate  i  =  1 , . ..  n,  there  is 
such  that  the  Fourier transform  of g  differs 
a frequency 
from  zero  only  in  the  interval 
,  and the samples 
are computed  with a period not greater than 
(cf.  [Higgins,  1996]).  Under these assumptions, the value of 
the sampling series computed at x equals g(x).  Unfortunately, 
most  popular  kernel  functions  (hence  summations  of kernel 
functions)  do  not  satisfy  these  hypotheses  since  the  support 
of their Fourier transform  is  unbounded.  Consequently sam(cid:173)
pling density estimates yields an information loss.  However, 
it can be shown that the Fourier transform of a kernel density 
estimate is negligible everywhere except for 
not greater 
than 
Therefore,  the global  density estimate can  be re(cid:173)
constructed from its samples by (5) introducing only a small 
error  if 

It is worth noting that the infinite series (5) need not be ap(cid:173)
proximated,  if it has  finitely  many nonzero terms.  The latter 
case holds if the used kernel function has a bounded support, 
since the density estimate will also have bounded support.  If, 
however, the kernel function has unbounded support, like the 
Gaussian  kernel,  then  the  density  estimate  can  be  approxi(cid:173)
mated  by  regarding  its  value  to  be  zero  everywhere  except 
inside an appropriately chosen bounded region. 

According to this approach,  we propose the following al(cid:173)
gorithmic KDEC scheme for computing the kernel density es(cid:173)
timation based clusters for local data spaces at M distributed 
data sites Lj (see Algorithm 2).  Every local site runs the pro(cid:173)
cedure  SiteDECluster  whereas  the  helper  site  runs  Helper. 
SiteDECluster is  passed  a reference H  to the helper and the 
local data set D,  and returns a clustering  in a class  instance 
C.  Helper  is  passed  a  list  of references  L to  the  local  sites. 
The  procedure  SiteNegotiate  carries  out  a  negotiation  with 
the  other  local  sites  through  HelperNegotiate  at  the  helper 
site to determine the  sampling  periods  x,  the  bounding  cor(cid:173)
, the kernel K, and 
ners of the sampling rectangle 

488 

LEARNING 

vances a fraction 5 of the gradient in its direction, if the gradi(cid:173)
ent's norm exceeds a threshold e. If a 
of the 
object returmed by  Uphill  contains an already clustered data 
object, the current cluster label  Id is set from that object's la(cid:173)
bel.  Otherwise,  checking whether  Uphill  returned the  same 
space  object  signals  to  FixedPoint  that  the  proximity  of the 
local maximum has been reached.  The maximum is marked 
by adding the current space object x as a dummy object to the 
local data set;  this ensures that subsequent paths converging 
to  the  same  local  maximum  will  use  the  same  cluster  label 
as  the  current  path.  Method  D.Add{)  returns  the  identifier 
of the added object, which is used as current cluster label.  If 
neither case holds, the label Id is obtained by a recursive call. 
Finally,  all  objects  in  a  small  neighbourhood  of the  current 
object  are  labeled  by  Id.  Note  that  adding  dummy  objects 
has eflect only on the range queries, and does not modify the 
density estimate. 

4.3  Complexity  of the  K D EC  scheme 
In  terms  of the  complexity  in  computation  and  communica(cid:173)
tion one crucial point of the KDEC scheme is how many sam(cid:173)
ples have to be computed and transferred among the sites.  In 
most cases,  to obtain  good  density estimates,  h must not be 
less than a small multiple of the smallest object distance.  As 
Ti  ~  h/2,  the  number  of samples  should  rarely  exceed  the 
number  of objects,  if only  space  regions  where  the  density 
estimate  is  not  negligible  are  sampled.  Since  the  size  of a 
sample  is  usually  much  smaller  than  the  size  of a  data  ob(cid:173)
ject,  the  overall  communication  costs  of our DDC  approach 
will be in most cases significantly lower than in a centralized 
approach.  Of course, the precise number of samples depends 
on the bounding region that is being sampled by every site.  In 
Algorithm 2 the site Lj  determines autonomosly the rectangle 
that contains the computed samples. 

The  computational costs of the KDEC scheme in terms of 
used CPU  cycles and I/O do not exceed the one  in the cen(cid:173)
tralized approach where clustering is performed on data col(cid:173)
lected in a single repository. The computational complexity is 
linear in the number of samples.  The precise cost of compu(cid:173)
tation of any KDEC-based DDC algorithm as an instance of 
the proposed scheme largely depends also on the used kernel 
function and local clustering algorithm. The DE-cluster algo(cid:173)
rithm we  developed  for the  KDEC  scheme  in  Section 3.2  is 
of complexity  0(Nq(N)),  where  q(N)  is  the  cost  of a  nearest 
neighbour query (which in practical cases is close to logjV). 
Algorithm 2  implements  a slightly  different approach in the 
hill-climbing  function  than  Algorithm  1,  since  the  function 
does  not  use  data  objects  to  direct  the  uphill  path.  How(cid:173)
ever, preliminary results of experiments conducted on a pro(cid:173)
totype implementation show good scalability of the approach, 
in terms of number of executed range queries. 

5  Conclusion 
Due  to  the  explosion  in  the  number  of  autonomous  data 
sources  there  is  a  growing  need  for  effective  approaches  to 
distributed knowledge discovery and data mining.  In this pa(cid:173)
per we have presented KDEC, a novel scheme for distributed 
data clustering which computes the density estimation, to per-

the window width h.  The negotiation procedures contain ap(cid:173)
propriate handshaking primitives to ensure that all  sites par(cid:173)
ticipate  and  exit  negotiations  only  if an  agreement  has been 
reached.  Each  local  site  computes  the  sampled  form  of the 
estimate  of D,  by  calling  function  Sample,  and  sends  it  to 
the helper.  (Function DE  computes the density estimate and 
is  omitted  for brevity.)  The  helper receives the  sampled es(cid:173)
timates,  sums them by  sampling  indexes  into a global  sam(cid:173)
ple, and returns it to all sites.  Procedures Send and Receive 
implement  appropriate  blocking  and  handshaking  to  ensure 
the transmission takes place.  Each local site uses the global 
sample  in  functions  FixedPoint  and  Uphill  to  compute  the 
values  of the  gradient  of the  global  density  estimate.  Func(cid:173)
tion  SeriesGradient  can  be  easily  derived  from  (5).  Local 
sites  perform  a  DE-cluster  algorithm  to  compute  the  corre(cid:173)
sponding local data clusters.  The details of the hill-climbing 
strategy are however different from Algorithm  1  because the 
sites are allowed access to local data objects only.  Uphill ad(cid:173)

LEARNING 

489 

form the clustering, from sampled forms of local densities at 
each data source site. 

The approach exploits statistical density estimation and in(cid:173)
formation  theoretic  sampling  to  minimize  communications 
between  sites.  Moreover,  the  privacy  of data  is  preserved 
to  a  large  extent  by  never transmitting  data  values  but  ker(cid:173)
nel based density estimation samples outside the site of ori(cid:173)
gin.  The approach does not require CPU and I/O costs sig(cid:173)
nificantly higher than  a  similar centralized approach  and  its 
communication  costs  may  be  lower.  Ongoing  research  fo(cid:173)
cuses in particular on implementations of a multiagent system 
for KDEC-based distributed data clustering in a peer-to-peer 
network, and investigation on methods to mitigate the risk of 
security and privacy violations in distributed data mining en(cid:173)
vironments. 

References 
[Ankerstef a/.,  1999]  Mihael  Ankerst,  Markus  M.  Breunig, 
Hans-Peter Kriegel, and Jorg Sander.  OPTICS: Ordering 
points to identify the clustering structure.  In Proceedings 
of the  1999  ACM  SIGMOD  International  Conference  on 
Management of Data, pages 49-60, Philadephia, PA, June 
1999. 

[Chen et al,  1996]  Ming-Syan  Chen,  Jiawei  Han,  and 
Philip S. Yu.  Data mining:  an overview from a database 
perspective.  IEEE  Trans.  On  Knowledge And Data Engi(cid:173)
neering, 8:866-883,  1996. 

[Ester etal,  1996]  Martin  Ester,  Hans-Peter  Kriegel,  Jorg 
Sander,  and Xiaowei  Xu.  A  density-based algorithm  for 
discovering clusters  in large  spatial databases with noise. 
In  Proceedings  of the  2nd  International  Conference  on 
Knowledge  Discovery  and Data  Mining  (KDD-96),  pages 
226-231, Portland, OR,  1996. 

[Ester etal.,  1998]  Martin  Ester,  Hans-Peter  Kriegel,  Jorg 
Sander,  Michael Wimmer,  and Xiaowei Xu.  Incremental 
clustering for mining in a data warehousing environment. 
In  Proceedings  of the  24th  International  Conference  on 
Very Large Data Bases (VLDB '98), pages 323-333, New 
York City, NY, August  1998. 

[Fayyad eta/., 1996]  Usama  Fayyad,  Gregory  Piatetsky-
Shapiro,  Padrhaic  Smyth,  and  Ramasamy  Uthurusamy. 
Advances  in  Knowledge  Discovery  and  Data  Mining. 
AAAI Press/MIT Press,  1996. 

[Ganti etal.,  1999]  Venkatesh  Ganti,  Raghu  Ramakrishnan, 
Johannes Gehrke, Allison L. Powell, and James C. French. 
Clustering large datasets in arbitrary metric spaces. In Pro(cid:173)
ceedings of the 15th International Conference on Data En(cid:173)
gineering  (ICDE  1999),  pages  502-511,  Sydney,  Austri-
alia, March 1999. 

[Hinneburg and Keim,  1998]  Alexander  Hinneburg  and 
Daniel  A.  Keim.  An  efficient  approach  to  clustering  in 
In  Proceedings 
large  multimedia  databases  with  noise. 
of  the  Fourth  International  Conference  on  Knowledge 
Discovery  and  Data  Mining  (KDD-98),  pages  58-65, 
New York City, New York, USA,  1998. AAAI Press. 

[Johnson and Kargupta,  1999]  Erik Johnson and Hillol Kar-
gupta.  Collective, hierarchical clustering from distributed 
heterogeneous data.  In M. Zaki and C. Ho, editors, Large-
Scale Parallel KDD Systems,  Lecture  Notes  in  Computer 
Science, pages 221-244. Springer-Verlag, 1999. 

[Kargupta et al, 2000]  H.  Kargupta,  B.  Park,  D.  Hersh-
berger,  and  E.  Johnson.  Advances  in  Distributed  and 
Parallel Knowledge Discovery,  chapter  5,  Collective  Data 
Mining:  A New Perspective Toward Distributed Data Min(cid:173)
ing, pages 131-178.  AAAI/MIT Press, 2000. 

[Kargupta et al, 2001]  H.  Kargupta,  W.  Huang,  S.  Krish-
namoorthy,  and E.  Johnson.  Distributed clustering using 
collective  principal  component analysis.  Knowledge and 
Information Systems Journal, 3(4):422-448, 2001. 

[Moro and Sartori, 2001]  Gianluca  Moro  and  Claudio  Sar-
tori.  Incremental  maintenance  of multi-source  views.  In 
M.  E.  Orlowska and J.  Roddick,  editors,  Proceedings of 
the  12th Australasian  conference  on  Database  technolo(cid:173)
gies, ADC 2001, ACM  International Conference Proceed(cid:173)
ing,  pages  13-20,  Queensland,  Australia,  2001.  IEEE 
Computer Society Press. 

[Parzen,  1962]  E.  Parzen.  On  estimation  of a  probability 
density function and mode.  Ann.  Math.  Statist., 33:1065-
1076, 1962. 

[Schikuta,  1996]  Erich  Schikuta.  Grid-clustering:  An  effi(cid:173)
cient  hierarchical  clustering  method  for  very  large  data 
sets. In Proceedings of the 13th International Conference 
on Pattern Recognition, pages  101-105. IEEE,  1996. 

[Silverman,  1986]  B.  W.  Silverman.  Density Estimation for 
Statistics and Data Analysis.  Chapman and Hall, London, 
1986. 

[Zhang et al,  1996]  Tian Zhang,  Raghu Ramakrishnan,  and 
Miron Livny.  BIRCH: An efficient data clustering method 
In  Proceedings  of the  1996 
for  very  large  darabases. 
ACM SIGMOD  International  Conference  on  Management 
of Data, pages 103-114, Montreal, Canada, June 1996. 

[Grabmeier and Rudolph, 2002]  J. 

Grabmeier 

and 
A.  Rudolph. 
in 
data  mining.  Data  Mining  and  Knowledge  Discovery, 
6:303-360, 2002. 

Techniques  of  cluster  algorithms 

[Higgins,  1996]  J.  R.  Higgins.  Sampling  Theory in Fourier 

and Signal Analysis.  Clarendon Press, Oxford,  1996. 

490 

LEARNING 


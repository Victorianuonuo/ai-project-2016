Learning Coordination Classiﬁers

Yuhong Guo

Russell Greiner

Dale Schuurmans

Department of Computing Science

University of Alberta

fyuhong,greiner,daleg@cs.ualberta.ca

Abstract

We present a new approach to ensemble classiﬁca-
tion that requires learning only a single base clas-
siﬁer. The idea is to learn a classiﬁer that simulta-
neously predicts pairs of test labels—as opposed to
learning multiple predictors for single test labels—
then coordinating the assignment of individual la-
bels by propagating beliefs on a graph over the data.
We argue that the approach is statistically well mo-
tivated, even for independent identically distributed
(iid) data. In fact, we present experimental results
that show improvements in classiﬁcation accuracy
over single-example classiﬁers, across a range of
iid data sets and over a set of base classiﬁers. Like
boosting, the technique increases representational
capacity while controlling variance through a prin-
cipled form of classiﬁer combination.

1 Introduction
Supervised learning has been by far the most studied task in
machine learning research. The problem is to take a ﬁnite set
of observed training examples (x1; y1); :::; (xn; yn) and pro-
duce a classiﬁer f : X ! Y that achieves small misclassiﬁ-
cation error on subsequent test examples. Most research has
tended to adopt a standard “iid” assumption that the training
and test examples are independent and identically distributed.
In fact, this assumption is fundamental to much of the theo-
retical research on the topic [Anthony and Bartlett, 1999] and
also characterizes most standard learning methods—as exem-
pliﬁed by the fact that most machine learning methods clas-
sify each test pattern in isolation, independently of other test
patterns.

Recently, however, increasing attention has been paid to
problems where the training and test labels are not indepen-
dent, but instead strongly related. For example, in domains
such as part of speech tagging and webpage classiﬁcation,
each word-tag or webpage-label depends on the tag or label of
proximal words or webpages, in addition to just the features
of the immediate word or webpage. Various forms of “rela-
tional” learning models have been developed to handle these
kinds of problems over the last few years. A notable example
is work on probabilistic relational models (PRMs), where the
correlation between the class labels of different instances is

explicitly represented in a directed graphical model [Getoor et
al., 2001; 2002]. Other approaches for learning multivariate
classiﬁers include conditional random ﬁelds (CRFs) [Lafferty
et al., 2001], relational Markov networks (RMNs) [Taskar et
al., 2002], and maximum margin Markov networks (M3N)
[Taskar et al., 2003]. All of these methods have led to sub-
stantial progress on learning classiﬁers that make dependent
predictions of test labels that are explicitly related.

Although learning multivariate predictors is an exciting
problem, we nevertheless focus on the classical iid case in
this paper. However, we demonstrate what we believe is a
surprising and counterintuitive connection: that learning mul-
tivariate dependent predictions is a beneﬁcial idea even in
the iid setting. In particular, we develop a relational learn-
ing strategy that classiﬁes test patterns by connecting their la-
bels in a graphical model—hence correlating the subsequent
predictions—even when it is explicitly assumed that all train-
ing and test examples are iid.

Before explaining the rationale behind our approach and
explaining why dependent prediction still makes sense in
the iid setting, we note that standard relational learning ap-
proaches, such as PRMs, CRFs, RMNs and M3Ns, do not
naturally correlate predictions on iid data. That is, these tech-
niques only consider label dependencies that are explicitly as-
serted to hold in the true underlying model of the domain. In
the iid case, since no dependencies are asserted between test
labels, these standard relational approaches reduce to single-
label learning techniques, such as univariate logistic regres-
sion and support vector machines. However, what we are
proposing is different: we intentionally add dependencies be-
tween test labels, even when all labels are explicitly assumed
to be independent in the underlying data generation process.
Surprisingly, we demonstrate that correlating predictions can
still be advantageous.

After introducing our basic approach below, we then mo-
tivate and justify our technique in three separate ways. First,
we show that predicting correlated test labels is statistically
justiﬁed in the iid setting, even when the independence as-
sumptions are explicitly taken into account. In fact, we show
that it is incorrect to conclude that a learned predictor can suf-
ﬁciently treat test cases as independent simply because they
come from an iid source. Second, we show that our pro-
posed relational learning technique can be viewed as a nat-
ural generalization of similarity-based learning techniques.

Moreover, it can also be viewed as a simple form of ensem-
ble learning method that has some advantages over standard
approaches. Third, we show empirically that our proposed
method can achieve improvements in classiﬁcation accuracy
across a range of iid domains, for different base learning al-
gorithms.

2 Learning Coordinated Label Predictors
We begin by simply introducing our learning method, and
then attempt to motivate it more thoroughly below. Initially,
we will focus on using probabilistic classiﬁers (although we
brieﬂy consider an extension to nonprobabilistic classiﬁers in
Section 5 below).

In the iid setting, the standard approach to probabilistic
classiﬁcation is to learn a univariate model P (yjx; (cid:18)) that as-
serts a conditional probability distribution over a single clas-
siﬁcation variable y given an input pattern x, where (cid:18) rep-
resents the parameters of the model. Given such a repre-
sentation, there are two key steps to building a univariate
classiﬁer: The ﬁrst is to learn a speciﬁc predictive model
P (yjx; (cid:18)) given training data (x1; y1); :::; (xn; yn), based on
using a principle such as maximum (conditional) likelihood
or maximum a posteriori estimation. Then, given a set of test
patterns x
i independently by
computing the label ^yi that maximizes the estimated condi-
tional probability, ^yi = arg maxy P (yjx
i ; (cid:18)). Natural ex-
amples of this approach are learning naive Bayes classiﬁers
[Friedman et al., 1997], logistic regression classiﬁers [Hastie
et al., 2001], kernel logistic regression classiﬁers [Zhu and
Hastie, 2001], sigmoid network classiﬁers [Neal, 1996], and
Bayesian network classiﬁers [Greiner and Zhou, 2002].

m, one classiﬁes each x

1; :::; x

(cid:3)

(cid:3)

(cid:3)

(cid:3)

Our approach is different.

Instead of learning a univari-
ate classiﬁer that predicts only a single label, we instead
propose to learn a pairwise label classiﬁer P (yiyjjxixj; (cid:30))
that takes an arbitrary pair of input patterns, xi and xj, and
asserts a conditional joint distribution over the pair of la-
bels yi and yj. For example, if there are two classes, say
0 and 1, then a pairwise classiﬁer would assert the condi-
tional probability of the four possible pair labelings (yi; yj) 2
f(0; 0); (0; 1); (1; 0); (1; 1)g given the two input patterns xi
and xj. In general the pairwise predictor does not assume that
yi and yj are independent given xi and xj, even though they
are indeed assumed to be independent in the true model (we
present a justiﬁcation of this in Section 3 below). We refer
to a pairwise label classiﬁer of this form as a “coordination
classiﬁer” to highlight the fact that it attempts to model any
coordination that might appear between the labels yi and yj
given the input patterns xi and xj. Given the alternative rep-
resentation P (yiyjjxixj; (cid:30)) we next describe the two main
processes of, ﬁrst, training a coordination classiﬁer from data,
and then using it to label test patterns.

2.1 Training a Coordination Classiﬁer
A coordination classiﬁer doubles the number of input fea-
tures and squares the number of output classes from an orig-
inal univariate classiﬁer. Despite the increase in model com-
plexity, training a coordination classiﬁer remains concep-
tually straightforward. Assume a standard training sample

(x1; y1); :::; (xn; yn) is given. First we construct a set of
pairs from the set f(xixj; yiyj)g and then supply these as
a conventional training set for learning a predictive model
P (yiyjjxixj; (cid:30)) from data.
(In our experiments below we
ignore duplicate pairs but otherwise include both orderings
of each distinct pair to ensure that the learned model is sym-
metric.) Once the training data is constructed, the parame-
ters of the model, (cid:30), can then be estimated in the same way
as the univariate case, either by using a maximum (condi-
tional) likelihood or maximum a posteriori estimation prin-
ciple. For example, given a linear logistic representation for
P (yijxi; (cid:18)), we use an analogous linear logistic representa-
tion for P (yiyjjxixj; (cid:30)) but over the joint feature space xixj;
thus training the two models under the same estimation prin-
ciple, but using different (although related) data sets.

A coordination model learned in this way will generally
not make independent predictions of yi and yj, since the ex-
tended parameters (cid:30) are not constrained to enforce indepen-
dence.1 That is, we expect the model to learn to make de-
pendent, coordinated predictions of the two labels from the
corresponding input patterns. Interestingly, learning a coor-
dination classiﬁer has the advantage of potentially squaring
the number of available training examples, even though this
advantage is mitigated by subsampling and the increase in the
complexity of the model being learned.

(cid:3)

(cid:3)

(cid:3)

2.2 Classifying Test Data with Coordination
Given a coordination classiﬁer, we require a principle for
classifying individual test patterns x
(cid:3). In fact, the problem of
classifying test patterns becomes much more involved in this
case. The approach we take is to consider the set of training
m as
examples (x1; y1); :::; (xn; yn) and test patterns x
1; :::; x
a whole. That is, rather than classify each test pattern x
i in
isolation, we instead seek to classify test patterns in a depen-
dent manner. To perform classiﬁcation, we proceed in three
stages reminiscent of conditional random ﬁelds: First, we
construct a graph over the test and training labels. Once the
graph has been constructed, we then use the learned coordi-
nation classiﬁer P (yiyjjxixj; (cid:30)) to assign “potentials” to the
possible labelings of each edge (yi; yj). These potentials can
then be used to deﬁne a Markov random ﬁeld over test label
assignments, thereby establishing a joint probability distribu-
tion over the labelings. Finally, we compute a joint labeling
m of the test examples that maximizes (or at least ap-
y(cid:3)
1; :::; y(cid:3)
proximately maximizes) the joint label probability. We de-
scribe each of these three steps in more detail below.

Deﬁning the graph First, to construct the graph, we only
consider edges that connect a pair of test labels (y (cid:3)
j ), or a
test label and a training label (y (cid:3)
i ; yj). That is, after training
we do not make any further use of edges between training
labels.

i ; y(cid:3)

To classify test patterns, the simplest approach, concep-
tually, is to consider the complete graph that connects each

1Many readers are probably objecting at this point that, given the
iid assumption, there can be no new information to be gained from
the n
2 example pairs that was not already present in the original n
examples. However, Section 3 argues that this conclusion is gener-
ally incorrect in a machine learning context.

test label y(cid:3)
to all other test and training labels. However,
i
we have found that it is usually impractical to consider all
m((m (cid:0) 1)=2 + n) test pairs (ignoring duplicate pairs).
Therefore, we reduce the number of edges by adding a few
restrictions. The natural alternatives we consider below are:
(i) connecting each test label only to training labels (which,
as we will see, is analogous to standard similarity and kernel
based learning methods), (ii) connecting each test label only
to other test labels (which, surprisingly, gives the best results
in our experiments below), and (iii) connecting each test label
to both training and test labels. To further reduce the overall
number of edges, we then uniformly subsample edges, sub-
ject to these different restrictions.

Deﬁning the potentials Once a graph has been con-
structed, we then assign potentials to the conﬁgurations of
each edge. There are two cases depending on whether the
edge connects two test labels, or a test label and a training
label.

For an edge that connects only two test labels, (y (cid:3)

i ; y(cid:3)

j ),
j ; (cid:30))

x

(cid:3)
i

(cid:3)

we simply assign the potential  (y (cid:3)
given by the learned coordination classiﬁer.

i ; y(cid:3)

j ) = P (y(cid:3)

i y(cid:3)

j jx

For an edge that connects a test and a training label,
i ; yj), we assign a unit potential to the singleton node (y (cid:3)
i )
i given yj. That is,

(y(cid:3)
given by the conditional probability of y (cid:3)
we assign

 yj (y(cid:3)

i ) = P (y(cid:3)

i jyj; x

(cid:3)
i

xj; (cid:30)) =

(cid:3)
i

P (y(cid:3)
i yjjx
Py P (yyjjx

xj; (cid:30))

xj; (cid:30))

(cid:3)
i

i ) we
Once a potential has been assigned to the singleton (y (cid:3)
then remove the edge (y (cid:3)
i ; yj) from the graph. Thus, the re-
sulting graph only has edges between test labels, and possibly
a combination of singleton potentials on nodes (y (cid:3)
i ) and pair-
wise potentials on edges (y (cid:3)

Once all of the potentials have been assigned, we then de-
ﬁne a joint probability distribution over node labelings in the
same manner as a Markov random ﬁeld, by taking the product
form

i ; y(cid:3)

j ).

P (y(cid:3)

1; :::; y(cid:3)

m) =

1
Z

m

Y

i=1

2
4Y

j 0

 yj 0 (y(cid:3)
i )

3
5

2
4Y

j>i

 (y(cid:3)

i ; y(cid:3)
j )

3
5

and normalizing by an appropriate constant Z.

Computing the labeling Finally, given a joint probability
distribution deﬁned by the Markov random ﬁeld, our goal is
to compute the joint test pattern labeling that has maximum
probability. (Since we are only interested in computing the
maximum probability assignment, we can ignore the normal-
ization constant above.) Depending on which edge model we
use, there are different implications.

First, assuming model (i) (test labels only connect to train-
ing labels), there are no pairwise potentials and the Markov
random ﬁeld becomes completely factored.
In this case,
computing the maximum probability assignment is easy and
can be determined independently for each test pattern. Es-
sentially, removing test-test edges reduces our technique
to a classical method in which each test pattern is classi-
ﬁed independently. Here the learned coordination model

true

conditional

model

learned

conditional

model

test examples

x1

x2

y1

y2

Figure 1: In an iid setting, the true test labels y1 and y2 are
independent given the true conditional model. However, they
are not independent given a learned estimate of the model.

(cid:3)
i

i yjjx

xj; (cid:30)) plays the role of a generalized similarity
P (y(cid:3)
measure for classifying y (cid:3)
i in terms of (x1; y1); :::; (xn; yn).
The only difference is that the coordination model is learned
in an earlier training phase, rather than being ﬁxed before-
hand.

The remaining cases (ii) and (iii) are more difﬁcult because
they introduce edges between test labels, which causes the la-
bels to become dependent. Surprisingly, we have found that
exploiting test label dependence can actually improve clas-
siﬁcation accuracy, even when the test data is known to be
iid. (This is one of the main points of the paper.) For these
models, computing the maximum probability assignment is
hard, because the graph can contain many loops. To cope
with the problem of performing probabilistic inference in a
complex graphical model, we use loopy belief propagation to
efﬁciently compute an approximate solution [Murphy et al.,
1999]. Below we ﬁnd this gives adequate results.

3 Rationale and Discussion
Before presenting our experimental results, it is important to
explain the rationale behind our technique and suggest why
coordinated classiﬁcation even makes sense in an iid setting.
Given the assumption that the training and test data are in-
dependent, we are proposing to predict test labels by building
a graph, asserting joint potentials over pairs of labels (from
a learned coordination classiﬁer), and using belief propaga-
tion to make dependent predictions. Why does it make sense
to make dependent predictions of iid labels? It turns out that
this approach is justiﬁed even when taking the independence
assumptions into account. Figure 1 illustrates the basic argu-
ment. In a standard machine learning setting, it is indeed true
that, given the correct model for generating iid data, the label
y1 for an input pattern x1 is independent of the label y2 for an-
other pattern x2. However, note that this requires knowledge
of the correct model (or at least its correct structure), which is
rarely the case in classiﬁcation learning. Instead, given only
an estimate of the true model obtained from training data, y1
and y2 remain dependent, as Figure 1 clearly shows. That is,
in the context of supervised learning it is generally the case
that test labels are dependent given a learned model. In fact,
it is obvious that supervised learning algorithms correlate the
labels on the training data. Our observation is simply that the
same principle can also be applied to test data.

Although using a relational technique for an iid problem
might still appear awkward, it has a well known precedent
in machine learning research: transductive learning [Vapnik,
1998; Zhu et al., 2003]. In transduction, the learner knows the
set of test patterns beforehand, and exploits this knowledge
to make predictions that are ultimately dependent.
In fact,
this idea has been exploited in recent approaches to semi-
supervised learning using Markov random ﬁelds [Zhu et al.,
2003]. What we are proposing is a general framework for ex-
tending standard probabilistic learning algorithms to be trans-
ductive in a similar fashion.

(cid:3)

(cid:3)

(cid:3)

i ; x1); :::; k(x

i based on similarities k(x

Our method can be further motivated by noting that it is
a natural extension of standard ideas in supervised iid clas-
siﬁcation. As observed, learning a coordination classiﬁer
P (yiyjjxixj; (cid:30)) is a natural generalization of learning meth-
ods that use a similarity measure k(xi; xj) to classify test
examples x
i ; xn) to
the training patterns. In fact, this corresponds to our graph
choice (i) above, which only connects test labels to training
labels. Coordination classiﬁcation extends the standard sim-
ilarity based approach by ﬁrst learning how patterns predict
dependencies between the labels (using standard methods ap-
plied in a novel way), and then correlating test predictions
over a graph. Although there has indeed been recent work on
learning kernels for classiﬁcation [Lanckriet et al., 2004], as
well as transductive learning with kernels [Xu et al., 2004],
thus far these formulations have remained hard to extend and
apply in practice.

Another interesting view of coordination classiﬁcation is
that it is a novel form of ensemble method. That is, the label
i is computed by a combination of votes
for a test pattern x
from multiple predictors associated with different test (and
training) patterns x((cid:3))
. In fact, even remotely connected pat-
terns can inﬂuence a classiﬁcation via belief propagation.

j

(cid:3)

As an ensemble method, coordination classiﬁcation has
some useful features. First, it only requires the training of
a single base classiﬁer P (yiyjjxixj; (cid:30)), rather than multiple
base classiﬁers trained from perturbed data. Second, as with
boosting and bagging, coordination classiﬁcation increases
the representational capacity of an original (univariate) classi-
ﬁer. That is, given a classiﬁer representation for a single label
P (yijxi; (cid:18)), as mentioned previously, a coordination classi-
ﬁer P (yiyjjxixj; (cid:30)) doubles the number of input features and
squares the number of output classes. In addition, the pre-
diction of a test label can, in principle, depend on all training
and test patterns. Of course, simply increasing the representa-
tional capacity of a base classiﬁer increases the risk of overﬁt-
ting. However, the advantage of ensemble methods is that the
resulting classiﬁer, although more complex, is “smoothed” by
a principled form of model combination, which helps avoid
overﬁtting while exploiting added representational complex-
ity. That is, the process of model combination can be used
to reduce the variance of the learned predictor. In our case,
we base our model combination principle on inference in a
Markov random ﬁeld. We will see below that, in fact, coordi-
nation classiﬁcation is competitive as an ensemble technique.
The biggest drawback of coordination classiﬁcation is the
need to perform probabilistic inference (via loopy belief

propagation) in order to label test instances. However, we
have still found the method to be robust to approximations,
like running only a single iteration of loopy belief propaga-
tion, or even just taking local votes or products.

4 Experimental Results
We implemented the proposed coordination classiﬁcation
technique for a few different forms of probabilistic classiﬁers
and using various standard iid data sets. Our intent was to
determine whether the approach indeed had merit and was
also robust to alterations in classiﬁers and data sets. Our ex-
periments were conducted on standard two-class benchmark
data sets from the UCI repository. The data sets used were:
1. australian, 2. breast, 3. chess, 4. cleve, 5. corral, 6.
crx, 7. diabetes, 8. ﬂare, 9. german, 10. glass2, 11. heart,
12. hepatitis, 13. mofn-3-7, 14. pima, and 15. vote. All of
our experimental results were obtained by 5-fold cross vali-
dation, repeated 10 times for different randomizations of the
graph structures. The tables and plots report averages of these
results, with standard deviations included in the tables.

Table 1 and Figure 2 show the results of our ﬁrst exper-
iment. In this case, we implemented a standard logistic re-
gression model, using unaltered input features, to learn a base
coordination classiﬁer P (yiyjjxixj; (cid:30)). Classiﬁcation was
performed by running loopy belief propagation until the test
labels stabilized (usually after 2 to 8 iterations). In the ﬁrst
experiment we used a graph over test labels only, to deter-
mine whether introducing label dependency would have any
beneﬁcial effect (see “edge” results). Here we subsampled
test-test edges uniformly at random for an overall density of
18 edges per test example. Table 1 and Figure 2 show the re-
sulting misclassiﬁcation error obtained by coordination clas-
siﬁcation in comparison to learning a standard logistic regres-
sion model, P (yijxi; (cid:18)). Here we see a notable reduction in
overall misclassiﬁcation error (19%!16%), with a signiﬁ-
cant improvement on some data sets (Breast, -10%, Diabetes,
-6%, MofN, -11%, Pima, -6%), and a minor increase on two
data sets (Cleve, +1%, and Corral, +1%).

Table 1 also compares the error of coordination clas-
siﬁcation to boosting the base logistic regression model
P (yijxi; (cid:18)). Here we used 18 rounds of Adaboost [Freund
and Schapire, 1996; 1997], thereby combining approximately
the same number of votes per test pattern as coordination
classiﬁcation. This experiment shows that as an ensemble
method, coordination classiﬁcation performs competitively in
this case. An advantage of coordination classiﬁcation is that
it only needs to learn a single base classiﬁer, as opposed to the
multiple training episodes required by boosting. The need to
run loopy belief propagation on the output labels is a disad-
vantage however.

To investigate the robustness of the method, we repeated
the previous experiments using a different base classiﬁer. Ta-
ble 2 and Figure 3 show the results of an experiment using
naive Bayes instead of logistic regression as the base clas-
siﬁcation method. Here the results are not as strong as the
ﬁrst case we tried, although they are still credible. Note that
boosting obtains a few larger improvements, but also larger
losses. Classiﬁcation coordination appears to be fairly stable.

Table 1: A comparison of average misclassiﬁcation error (%)
on UCI data using logistic regression as a base model. (cid:1) =
average improvement over base.

Table 2: A comparison of average misclassiﬁcation error (%)
on UCI data using naive Bayes as a base model. (cid:1) = aver-
age improvement over base.

australian
breast
chess
cleve
corral
crx
diabetes
ﬂare
german
glass2
heart
hepatitis
mofn-3-7
pima
vote
average

base
15.1
14.5
7.6
15.3
8.8
16.5
31.2
17.4
25.8
29.4
16.3
17.5
28.6
30.9
6.7
18.8

boosted (cid:1) (cid:6) std
14.5
-0.6(cid:6) 0.4
14.9
0.4(cid:6) 0.3
8.3
0.7(cid:6) 0.2
14.9
-0.4(cid:6) 0.6
8.8
0(cid:6) 0.0
16.3
-0.2(cid:6) 0.4
31.3
0.1(cid:6) 0.1
17.5
0.1(cid:6) 0.5
25.9
0.1(cid:6) 0.1
27.5
-1.9(cid:6) 1.3
15.9
-0.4(cid:6) 0.4
17.5
0(cid:6) 0.0
28.6
0(cid:6) 0.0
30.5
-0.4(cid:6) 0.3
6.7
0(cid:6) 0.8
18.6
-0.2(cid:6) 0.4

base vs boosted

edge (cid:1) (cid:6) std
14.2
-0.9(cid:6) 0.9
4.2
-10.3(cid:6) 2.4
7.6
0(cid:6) 0.1
16.6
1.3(cid:6) 1.1
9.9
1.1(cid:6) 0.9
14.4
-2.1(cid:6) 0.8
24.8
-6.4(cid:6) 1.1
17.5
0.1(cid:6) 1.3
24.8
-1.0(cid:6) 1.0
28.8
-0.6(cid:6) 4.7
15.9
-0.4(cid:6) 1.9
14.1
-3.4(cid:6) 3.0
17.3
-11.3(cid:6) 0.8
24.9
-6.0(cid:6) 0.8
5.8
-0.9(cid:6) 0.8
16.1
-2.7(cid:6) 1.4

australian
breast
chess
cleve
corral
crx
diabetes
ﬂare
german
glass2
heart
hepatitis
mofn-3-7
pima
vote
average

base
13.8
2.6
20.1
16.3
13.6
14.6
22.6
16.8
25.5
15.6
15.9
13.8
14.2
21.8
9.9
15.8

boosted (cid:1) (cid:6) std
16.2
2.4(cid:6) 1.6
5.0
2.4(cid:6) 1.1
8.2
-11.9(cid:6) 3.1
17.0
0.7(cid:6) 0.4
13.6
0(cid:6) 5.5
16.6
2.0(cid:6) 0.9
22.6
0(cid:6) 0.0
16.7
-0.1(cid:6) 0.1
26.3
0.8(cid:6) 0.7
11.9
-3.7(cid:6) 1.2
17.0
1.1(cid:6) 1.3
15.0
1.2(cid:6) 6.1
0.0
-14.2(cid:6) 0.6
21.7
-0.1(cid:6) 0.1
5.5
-4.4(cid:6) 1.8
14.2
-1.6(cid:6) 1.6

base vs boosted

edge (cid:1) (cid:6) std
14.2
0.4(cid:6) 0.4
2.7
0.1(cid:6) 0.1
19.1
-1.0(cid:6) 0.6
16.4
0.1(cid:6) 0.2
14.2
0.6(cid:6) 0.7
14.4
-0.2(cid:6) 0.2
22.6
0(cid:6) 0.2
17.0
0.2(cid:6) 0.4
25.1
-0.4(cid:6) 0.6
13.9
-1.7(cid:6) 1.2
16.1
0.2(cid:6) 0.2
9.3
-4.5(cid:6) 1.2
1.0
-13.2(cid:6) 0.8
22.2
0.4(cid:6) 0.4
9.8
-0.1(cid:6) 0.2
14.5
-1.3(cid:6) 0.5

e
s
a
b

 
f

o

 
r
o
r
r
e
n
o

 

i
t

a
c
i
f
i
s
s
a
C

l

e
s
a
b

 
f

o
 
r
o
r
r
e
 
n
o
i
t
a
c
i
f
i
s
s
a
C

l

40

35

30

25

20

15

10

5

0

0

40

35

30

25

20

15

10

5

0

0

5

15

10
30
Classification error of boosted

20

25

35

40

base vs edge

5

10

15

20

25

Classification error of edge

30

35

40

e
s
a
b

 
f

o

 
r
o
r
r
e
n
o

 

i
t

a
c
i
f
i
s
s
a
C

l

e
s
a
b

 
f

o
 
r
o
r
r
e
 
n
o
i
t
a
c
i
f
i
s
s
a
C

l

40

35

30

25

20

15

10

5

0

0

40

35

30

25

20

15

10

5

0

0

5

15

10
30
Classification error of boosted

20

25

35

40

base vs edge

5

10

15

20

25

Classification error of edge

30

35

40

Figure 2: A comparison of average misclassiﬁcation error on
UCI data sets using logistic regression. Top plot: base model
versus boosted logistic regression. Bottom plot: base model
versus “edge”-based coordination classiﬁcation.

Figure 3: A comparison of average misclassiﬁcation error
on UCI data sets using naive Bayes. Top plot: base model
versus boosted naive Bayes. Bottom plot: base model versus
“edge”-based coordination classiﬁcation.

The above experiments were run using only edges between
test labels. To compare to standard approaches for iid data,
we repeated the experiments using only edges between test
and training labels, hence decoupling the test labels and elim-
inating the need for belief propagation (as discussed in Sec-
tion 2.2). In this case, test labels are predicted independently.
Table 3 and Figure 4 (top) still show, however, that this ap-
proach generally improves the base logistic regression classi-
ﬁer P (yijxi; (cid:18)) (see “node” results). We conclude that cor-
relating the test labels appears to be a beneﬁcial idea, even
in an iid setting. The marginal improvement of label depen-
dence, although positive, might be secondary to the beneﬁt of
learning a similarity measure.

All of the previous results were obtained by subsampling
edges at a density of 18 edges per test label. To test the sen-
sitivity of our results to the edge density, we repeated the
ﬁrst experiment (test-test edges) using different edge densi-
ties. Figure 5 shows that the performance of coordination
classiﬁcation does not appear to be sensitive to edge density.
Finally, we experimented with the combined edge ap-
proach (iii) which randomly mixed test-test edges and test-
train edges yielding the results of Table 3 and Figure 4 (bot-
tom). The results in this case do not appear to surpass the
performance of using test only edges (see “mix” results).

5 Extensions
All of our results so far have involved probabilistic classi-
ﬁers whose output is a conditional distribution over the label
y given the input pattern x. Of course, many of the most im-
portant classiﬁcation learning technologies, such as decision
trees and support vector machines do not naturally produce
probabilistic classiﬁcations over the class label (although they
can be extended in various ways to do this [Platt, 2000]). This
raises the obvious question of generalizing our technique to
consider nonprobabilistic classiﬁers.

It is always possible to extend a classiﬁcation learning
method to learn to predict label pairs (yi; yj) given paired
input patterns (xi; xj). The difﬁculty is combining several
paired predictions to render a sensible classiﬁcation for a sin-
gle test pattern. A convenient way to do this would be to
convert the predicted outputs to nonnegative potentials over
labelings. However, rather than do this, we tried the sim-
pler alternative of combining pair classiﬁcations by a simple
vote to classify a single test pattern x
i . This is a less pow-
erful combination method than loopy belief propagation, but
requires fewer extensions to existing methods to test the co-
ordination classiﬁer idea in these cases.

(cid:3)

To test this simple idea, we conducted an experiment on
the same UCI data using a neural network classiﬁer. Speciﬁ-
cally we used a feedforward neural network with one hidden
layer and logistic activation functions. The base neural net-
work used one output unit, whereas the pairwise neural net-
work used four output units (two units to indicate the class
of each of the two input vectors) and double the number of
input units. In each case the number of hidden units was set
to 20, subject to the constraint that (nin + nout) (cid:2) nhidden (cid:20)
train size=2. We trained the networks to minimize cross
entropy error using the quasi-Newton method from Netlab

Table 3: Alternative comparison of average error (%) on UCI
data using logistic regression as a base model. (cid:1) = average
improvement over base.

australian
breast
chess
cleve
corral
crx
diabetes
ﬂare
german
glass2
heart
hepatitis
mofn-3-7
pima
vote
average

base
15.1
14.5
7.6
15.3
8.8
16.5
31.2
17.4
25.8
29.4
16.3
17.5
28.6
30.9
6.7
18.8

node (cid:1) (cid:6) std mix (cid:1) (cid:6) std
14.1
-0.9(cid:6) 0.9
3.8
-10.7(cid:6) 2.5
8.1
0.3(cid:6) 0.2
16.6
1.5(cid:6) 1.4
11.2
1.5(cid:6) 0.8
14.9
-1.8(cid:6) 0.9
24.7
-6.3(cid:6) 1.3
17.6
0.4(cid:6) 1.1
24.7
-1.0(cid:6) 0.9
29.7
0(cid:6) 4.6
15.7
-0.4(cid:6) 1.8
15.0
-3.5(cid:6) 2.8
25.1
-5.0(cid:6) 0.4
25.1
-6.4(cid:6) 1.0
6.0
-0.8(cid:6) 0.7
16.8
-2.2(cid:6) 1.4

-1.0(cid:6) 0.9
-10.7(cid:6) 2.4
0.5(cid:6) 0.3
1.3(cid:6) 1.3
2.4(cid:6) 1.2
-1.6(cid:6) 0.8
-6.5(cid:6) 1.3
0.2(cid:6) 1.2
-1.1(cid:6) 0.9
0.3(cid:6) 5.0
-0.6(cid:6) 2.0
-2.5(cid:6) 3.4
-3.5(cid:6) 0.2
-5.8(cid:6) 0.8
-0.7(cid:6) 0.8
-2.0(cid:6) 1.5

14.2
3.8
7.9
16.8
10.3
14.7
24.9
17.8
24.8
29.4
15.9
14.0
23.6
24.5
5.9
16.6

base vs node

e
s
a
b

 
f

o

 
r
o
r
r
e
n
o

 

i
t

a
c
i
f
i
s
s
a
C

l

e
s
a
b

 
f

o
 
r
o
r
r
e
 
n
o
i
t
a
c
i
f
i
s
s
a
C

l

40

35

30

25

20

15

10

5

0

0

40

35

30

25

20

15

10

5

0

0

5

10

5

10

15

20

25

Classification error of node

base vs mix

15

20

25

Classification error of mix

30

35

40

30

35

40

Figure 4: Alternative comparison on UCI data using logistic
regression. Top plot: base model versus “node”-based co-
ordination classiﬁcation. Bottom plot: base model versus a
mix of “edge” and “node” based coordination classiﬁcation.

r
o
r
r
e

 

n
o

i
t

a
c
i
f
i
s
s
a
C

l

30

25

20

15

10

5

0

5

Edge density control

breast
diabetes
hepatitis
mofn−3−7
pima

10

15

Edge/node ratio

20

Figure 5: Average misclassiﬁcation error of “edge”-based
coordination classiﬁcation using logistic regression, compar-
ing different ratios of edges to the number of test patterns.

[Nabney, 2001] (www.ncrg.aston.ac.uk/netlab).

Once a pairwise neural network classiﬁer was learned, we
classiﬁed test examples according to the previous “edge”
model, again by building a random graph between test la-
bels (using an average of 18 edges per test label as before),
using the learned coordination neural network to make hard
predictions for each edge, and then combining the edge pre-
dictions using a simple vote to classify each test example.
Table 4 (“edge”) and Figure 6 show the results of this exper-
iment. Surprisingly, we still obtain a slight overall reduction
in misclassiﬁcation error over the base level neural network
classiﬁer, while again competing well against boosting.

Although encouraging, our results are not as positive in ev-
ery case. We also conducted a simple experiment with de-
cision trees [Quinlan, 1993] as the base coordination classi-
ﬁer, once again combining these predictions with a simple
vote to label speciﬁc test patterns. Unfortunately, we did not
observe an overall improvement over the base decision tree
classiﬁer in this case; see Figure 7. This result suggests that a
more powerful model combination idea might be required to
achieve robust improvements more generally.

6 Conclusion
We have proposed a novel classiﬁcation learning strategy that
coordinates the prediction of test labels on a graph over the
data. The coordination classiﬁcation idea can be used to ex-
tend any probabilistic classiﬁcation approach quite naturally,
and even seems to be applicable to nonprobabilistic tech-
niques as well (although more research needs to be done).

One insight behind the technique is that making correlated
predictions of test labels is justiﬁed, even advantageous, in
the standard iid setting. This fact has often been overlooked
in classiﬁcation learning, therefore we believe it to be a point
worth emphasizing. The ability to learn and predict coor-
dinated test labels, combining them with probabilistic infer-
ence, provides a ﬂexible new tool for improving classiﬁcation
accuracy on iid data.

Table 4: A comparison of average misclassiﬁcation error (%)
on UCI data using a neural network as a base model. (cid:1) =
average improvement over base.

australian
breast
chess
cleve
corral
crx
diabetes
ﬂare
german
glass2
heart
hepatitis
mofn-3-7
pima
vote
average

base
19.3
4.0
3.5
22.9
0
20.6
27.6
19.4
28.9
19.3
22.0
15.0
0
26.7
6.0
15.7

boosted (cid:1) (cid:6) std
16.5
-2.8(cid:6) 0.7
4.4
0.4(cid:6) 0.6
3.6
0.1(cid:6) 0.2
20.0
-2.9(cid:6) 1.4
0
0(cid:6) 0.0
18.5
-2.1(cid:6) 0.9
28.1
0.5(cid:6) 0.8
21.4
2.0(cid:6) 0.5
26.4
-2.5(cid:6) 0.6
16.3
-3.0(cid:6) 1.5
21.1
-0.9(cid:6) 2.1
13.8
-1.2(cid:6) 3.5
0
0(cid:6) 0.0
29.0
2.3(cid:6) 0.8
6.7
0.7(cid:6) 0.9
15.0
-0.7(cid:6) 1.0

base vs boosted

edge (cid:1) (cid:6) std
15.4
-3.9(cid:6) 0.4
4.1
0.1(cid:6) 0.4
6.7
3.2(cid:6) 0.9
18.8
-4.1(cid:6) 1.3
0
0(cid:6) 0.0
15.9
-4.7(cid:6) 1.9
25.0
-2.6(cid:6) 0.9
19.7
0.3(cid:6) 0.4
26.4
-2.5(cid:6) 1.0
19.8
0.5(cid:6) 1.2
18.8
-3.2(cid:6) 1.0
13.5
-1.5(cid:6) 2.0
0
0(cid:6) 0.0
25.0
-1.7(cid:6) 0.6
6.2
0.2(cid:6) 0.4
14.4
-1.3(cid:6) 0.8

e
s
a
b

 
f

o

 
r
o
r
r
e
n
o

 

i
t

a
c
i
f
i
s
s
a
C

l

e
s
a
b

 
f

o
 
r
o
r
r
e
 
n
o
i
t
a
c
i
f
i
s
s
a
C

l

40

35

30

25

20

15

10

5

0

0

40

35

30

25

20

15

10

5

0

0

5

15

10
30
Classification error of boosted

20

25

35

40

base vs edge

5

10

15

20

25

Classification error of edge

30

35

40

Figure 6: A comparison of average misclassiﬁcation error on
UCI data sets using a neural network. Top plot: base model
versus boosted neural network. Bottom plot: base model
versus “edge”-based coordination classiﬁcation.

base vs edge

[Greiner and Zhou, 2002] R. Greiner and W. Zhou. Struc-
tural extension to logistic regression: Discriminant param-
eter learning of belief net classiﬁers. In Proceedings of the
18th Annual National Conference on Artiﬁcial Intelligence
(AAAI), pages 167–173, 2002.

[Hastie et al., 2001] T. Hastie, R. Tibshirani, and J. Fried-
man. The Elements of Statistical Learning. Springer, 2001.
[Lafferty et al., 2001] J. Lafferty, A. McCallum, and F. Pere-
ria. Conditional random ﬁelds: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of
the 18th International Conference on Machine Learning
(ICML), pages 282–289, 2001.

e
s
a
b

 
f

o

 
r
o
r
r
e

 

n
o

i
t

a
c
i
f
i
s
s
a
C

l

40

35

30

25

20

15

10

5

0

0

5

10

15

20

Classification error of edge

25

30

35

40

Figure 7: A comparison of average misclassiﬁcation error on
UCI data sets using C4.5 as the base classiﬁer.

One idea for future work that we are considering is to ex-
tend the notion of a pairwise edge classiﬁer to a more general
clique classiﬁer. We are also investigating alternative prin-
ciples for deﬁning potentials on label pairs to see perhaps if
there are other combination ideas that work more effectively.
Finally, we are also investigating whether combining standard
“single node” classiﬁers with our generalized “edge” predic-
tors might lead to further accuracy improvements.

Acknowledgments
Research supported by the Alberta Ingenuity Centre for Ma-
chine Learning, NSERC, MITACS, and the Canada Research
Chairs program.

References
[Anthony and Bartlett, 1999] M. Anthony and P. Bartlett.
Theoretical Foundations.

Neural Network Learning:
Cambridge, 1999.

[Freund and Schapire, 1996] Y. Freund and R. Schapire. Ex-
periments with a new boosting algorithm. In Proceedings
of the 13th International Conference on Machine Learning
(ICML), pages 148–156, 1996.

[Freund and Schapire, 1997] Y. Freund and R. Schapire. A
decision-theoretic generalization of on-line learning and
an application to boosting. Journal of Computer and Sys-
tems Sciences, 55(1):119–139, 1997.

[Friedman et al., 1997] N. Friedman, D. Geiger,

and
M. Goldszmidt. Bayesian network classiﬁers. Machine
Learning, 29:121–163, 1997.

[Getoor et al., 2001] L. Getoor, E. Segal, B. Taskar, and
D. Koller. Probabilistic models of text and link structure
for hypertext classiﬁcation. In IJCAI01 Workshop on Text
Learning, 2001.

[Getoor et al., 2002] L. Getoor, N. Friedman, D. Koller, and
B. Taskar. Learning probabilistic models of link struc-
ture. In Journal of Machine Learning Research, volume 3,
pages 679–707, 2002.

[Lanckriet et al., 2004] G.

N. Cristianini,
P. Bartlett, L. El Ghaoui, and M. Jordan. Learning the
kernel matrix with semideﬁnite programming. Journal of
Machine Learning Research, 5:27–72, 2004.

Lanckriet,

[Murphy et al., 1999] K. Murphy, Y. Weiss, and M. Jordan.
Loopy belief propagation for approximate inference: an
empirical study.
In Proceedings of the 15th Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), pages 467–
475, 1999.

[Nabney, 2001] I. Nabney.

Pattern Recognition.
http://www.ncrg.aston.ac.uk/netlab.

NETLAB: Algorithms for
Springer, New York, 2001.

[Neal, 1996] R. Neal. Bayesian Learning for Neural Net-

works. Springer, 1996.

[Platt, 2000] J. Platt. Probabilities for SV machines.

In
A. Smola, P. Bartlett, B. Schoelkopf, and D. Schuurmans,
editors, Advances in Large Margin Classiﬁers, pages 61–
74. MIT Press, 2000.

[Quinlan, 1993] J. Quinlan. C4.5: Programs for Machine

Learning. Morgan Kaufmann, San Mateo, 1993.

[Taskar et al., 2002] B. Taskar, P. Abbeel, and D. Koller.
Discriminative probabilistic models for relational data. In
Proceedings of the 18th Conference on Uncertainty in Ar-
tiﬁcial Intelligence (UAI), pages 485–492, 2002.

[Taskar et al., 2003] B. Taskar, C. Guestrin, and D. Koller.
Max-margin Markov networks.
In Advances in Neural
Information Processing Systems 16 (NIPS), pages 25–32,
2003.

[Vapnik, 1998] V. Vapnik. Statistical Learning Theory. Wi-

ley, New York, 1998.

[Xu et al., 2004] L. Xu, B. Larson, J. Neufeld, and D. Schu-
urmans. Maximum margin clustering.
In Advances in
Neural Information Processing Systems 17 (NIPS), pages
1537–1544, 2004.

[Zhu and Hastie, 2001] J. Zhu and T. Hastie. Kernel logistic
regression and the import vector machine. In Advances in
Neural Information Processing Systems 14 (NIPS), pages
1081–1088, 2001.

[Zhu et al., 2003] X. Zhu, Z. Ghahramani, and J. Lafferty.
Semi-supervised learning using gaussian ﬁelds and har-
monic functions. In Proceedings of the 20th International
Conference on Machine Learning (ICML-03), pages 912–
919, 2003.


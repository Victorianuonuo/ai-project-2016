Peripheral-Foveal Vision for Real-time Object Recognition and Tracking in Video

Stephen Gould, Joakim Arfvidsson, Adrian Kaehler, Benjamin Sapp, Marius Messner,

Gary Bradski, Paul Baumstarck, Sukwon Chung and Andrew Y. Ng

Stanford University

Stanford, CA 94305 USA

Abstract

Human object recognition in a physical 3-d envi-
ronment is still far superior to that of any robotic
vision system. We believe that one reason (out
of many) for this—one that has not heretofore
been signiﬁcantly exploited in the artiﬁcial vision
literature—is that humans use a fovea to ﬁxate on,
or near an object, thus obtaining a very high resolu-
tion image of the object and rendering it easy to rec-
ognize. In this paper, we present a novel method for
identifying and tracking objects in multi-resolution
digital video of partially cluttered environments.
Our method is motivated by biological vision sys-
tems and uses a learned “attentive” interest map
on a low resolution data stream to direct a high
resolution “fovea.” Objects that are recognized in
the fovea can then be tracked using peripheral vi-
sion. Because object recognition is run only on
a small foveal image, our system achieves perfor-
mance in real-time object recognition and tracking
that is well beyond simpler systems.

Introduction

1
The human visual system far outperforms any robotic system
in terms of object recognition. There are many reasons for
this, and in this paper, we focus only on one hypothesis—
which has heretofore been little exploited in the computer vi-
sion literature—that the division of the retina into the foveal
and peripheral regions is of key importance to improving per-
formance of computer vision systems on continuous video
sequences. Brieﬂy, the density of photoreceptor rod and cone
cells varies across the retina. The small central fovea, with
a high density of color sensitive cone cells, is responsible
for detailed vision, while the surrounding periphery, contain-
ing a signiﬁcantly lower density of cones and a large num-
ber of monochromatic rod cells, is responsible for, among
other things, detecting motion. Estimates of the equivalent
number of “pixels” in the human eye vary, but based on
spatial acuity of 1/120 degrees [Edelman and Weiss, 1995;
Fahle and Poggio, 1981], appears to be on the order of 5×108
pixels. For a more in-depth treatment of human visual per-
ception, we refer the reader to one of the many excellent text-
books in the literature, for example [Wandell, 1995].

Because very little changes in a visual stream from one
frame to the next, one expects that it is possible to identify
objects or portions of a scene one at a time using the high
resolution fovea, while tracking previously identiﬁed objects
in the peripheral region. The result is that it is possible to
use computationally expensive classiﬁcation algorithms on
the relatively limited portion of the scene on which the fovea
is ﬁxated, while accumulating successful classiﬁcations over
a series of frames in real-time.

A great deal has happened in recent years in the area of
location and identiﬁcation of speciﬁc objects or object classes
in images. Much of this work, however, has concentrated on
single frames with the object of interest taking up a signiﬁcant
proportion of the ﬁeld-of-view. This allows for accurate and
robust object recognition, but implies that if we wish to be
able to ﬁnd very small objects, we must go to much higher
image resolutions.1 In the naive approach, there is signiﬁcant
computational cost of going to this higher resolution.

For continuous video sequences, the standard technique is
simply to treat each frame as a still image, run a classiﬁca-
tion algorithm over the frame, and then move onto the next
frame, typically using overlap to indicate that an object found
in frame n + 1 is the same object found in frame n, and us-
ing a Kalman ﬁlter to stabilize these measurements over time.
The primary difﬁculty that this simple method presents is that
a tremendous amount of effort is misdirected at the vast ma-
jority of the scene which does not contain the objects we are
attempting to locate. Even if the Kalman ﬁlter is used to pre-
dict the new location of an object of interest, there is no way
to detect the appearance of new objects which either enter the
scene, or which are approached by a camera in motion.

The solution we propose is to introduce a peripheral-foveal
model in which attention is directed to a small portion of the
visual ﬁeld. We propose using a low-resolution, wide-angle
video camera for peripheral vision and a pan-tilt-zoom (PTZ)
camera for foveal vision. The PTZ allows very high resolu-
tion on the small portion of the scene in which we are inter-
ested at any given time. We refer to the image of the scene
supplied by the low-resolution, wide-angle camera as the pe-
ripheral view and the image supplied by the pan-tilt-zoom

1Consider, for example, a typical coffee mug at a distance of ﬁve
meters appearing in a standard 320 × 240 resolution, 42◦
ﬁeld-of-
view camera. The 95mm tall coffee mug is reduced to a mere 8
pixels high, rendering it extremely difﬁcult to recognize.

IJCAI-07

2115

camera as the foveal view or simply fovea.

We use an attentive model to determine regions from the
peripheral view on which we wish to perform object classi-
ﬁcation. The attentive model is learned from labeled data,
and can be interpreted as a map indicating the probability
that any particular pixel is part of an unidentiﬁed object.
The fovea is then repeatedly directed by choosing a region
to examine based on the expected reduction in our uncer-
tainty about the location of objects in the scene. Identiﬁca-
tion of objects in the foveal view can be performed using
any of the state-of-the-art object classiﬁcation technologies
(for example see [Viola and Jones, 2004; Serre et al., 2004;
Brubaker et al., 2005]).

The PTZ camera used in real-world robotic applications
can be modeled for study (with less peak magniﬁcation) using
a high-deﬁnition video (HDV) camera, with the “foveal” re-
gion selected from the video stream synthetically by extract-
ing a small region of the HDV image. The advantage of this
second method is that the input data is exactly reproducible.
Thus we are able to evaluate different foveal camera motions
on the same recorded video stream. Figure 1 shows our robot
mounted with our two different camera setups.

outlines related work. In Section 3 we present the probabilis-
tic framework for our attention model which directs the foveal
gaze. Experimental results from a sample HDV video stream
as well as real dual-camera hardware are presented in Sec-
tion 4. The video streams are of a typical ofﬁce environment
and contain distractors, as well as objects of various types
for which we had previously trained classiﬁers. We show that
the performance of our attention driven system is signiﬁcantly
better than naive scanning approaches.

2 Related Work
An early computational architecture for explaining the visual
attention system was proposed by Koch and Ullman (1985) .
In their model, a scene is analyzed to produce multiple fea-
ture maps which are combined to form a saliency map. The
single saliency map is then used to bias attention to regions
of highest activity. Many other researchers, for example [Hu
et al., 2004; Privitera and Stark, 2000; Itti et al., 1998], have
suggested adjustments and improvements to Koch and Ull-
man’s model. A review of the various techniques is presented
in [Itti and Koch, 2001].

A number of systems, inspired by both physiological mod-
els and available technologies, have been proposed for ﬁnd-
ing objects in a scene based on the idea of visual attention, for
example [Tagare et al., 2001] propose a maximum-likelihood
decision strategy for ﬁnding a known object in an image.

More recently, active vision systems have been proposed
for robotic platforms. Instead of viewing a static image of
the world, these systems actively change the ﬁeld-of-view to
obtain more accurate results. A humanoid system that de-
tects and then follows a single known object using peripheral
and foveal vision is presented in [Ude et al., 2003]. A sim-
ilar hardware system to theirs is proposed in [Bj¨orkman and
Kragic, 2004] for the task of object recognition and pose esti-
mation. And in [Orabona et al., 2005] an attention driven sys-
tem is described that directs visual exploration for the most
salient object in a static scene.

An analysis of the relationship between corresponding
points in the peripheral and foveal views is presented in [Ude
et al., 2006], which also describes how to physically control
foveal gaze for rigidly connected binocular cameras.

Figure 1: STAIR platform (left) includes a low-resolution periph-
eral camera and high-resolution PTZ camera (top-right), and alter-
native setup with HDV camera replacing the PTZ (bottom-right). In
our experiments we mount the cameras on a standard tripod instead
of using the robotic platform.

The robot is part of the STAIR (STanford Artiﬁcial Intelli-
gence Robot) project, which has the long-term goal of build-
ing an intelligent home/ofﬁce assistant that can carry out tasks
such as cleaning a room after a dinner party, and ﬁnding and
fetching items from around the home or ofﬁce. The ability to
visually scan its environment and quickly identify objects is
of one of the key elements needed for a robot to accomplish
such tasks.

The rest of this paper is organized as follows. Section 2

3 Visual Attention Model
Our visual system comprises two separate video cameras.
A ﬁxed wide-angle camera provides a continuous low-
resolution video stream that constitutes the robot’s peripheral
view of a scene, and a controllable pan-tilt-zoom (PTZ) cam-
era provides the robot’s foveal view. The PTZ camera can
be commanded to focus on any region within the peripheral
view to obtain a detailed high-resolution image of that area
of the scene. As outlined above, we conduct some of our ex-
periments on recorded high-resolution video streams to allow
for repeatability in comparing different algorithms.

Figure 2 shows an example of a peripheral and foveal view
from a typical ofﬁce scene. The attention system selects a
foveal window from the peripheral view. The corresponding
region from the high-resolution video stream is then used for

IJCAI-07

2116

Figure 2: Illustration of the peripheral (middle) and foveal (right) views of a scene in our system with attentive map showing regions of high
interest (left). In our system it takes approximately 0.5 seconds for the PTZ camera to move to a new location and acquire an image.

(cid:2)
ok /∈T

(cid:2)
over all objects,
ok∈T

H =

Htracked(ξk(t)) +

Hunidentiﬁed(ξk(t))

(1)
where the ﬁrst summation is over objects being tracked, T ,
and the second summation is over objects in the scene that
have not yet been identiﬁed (and therefore are not being
tracked).

Since our system cannot know the total number of objects
(cid:3)
in the scene, we cannot directly compute the entropy over
ok /∈T H(ξk(t)). Instead, we learn the
unidentiﬁed objects,
probability P (ok | F) of ﬁnding a previously-unknown ob-
ject in a given foveal region F of the scene based on features
extracted from the peripheral view (see the description of our
interest model in section 3.2 below). If we detect a new ob-
ject, then one of the terms in the rightmost sum of Eq. (1) is
reduced; the expected reduction in our entropy upon taking
action A2 (and ﬁxating on a region F) is
Hunidentiﬁed(ξk(t))

(cid:4)
ΔHA2 = P (ok | F)

(cid:5)
− Htracked(ξk(t + 1))

classiﬁcation. The attentive interest map, generated from fea-
tures extracted from the peripheral view, is used to determine
where to direct the foveal gaze, as we will discuss below.

The primary goal of our system is to identify and track ob-
jects over time. In particular, we would like to minimize our
uncertainty in the location of all identiﬁable objects in the
scene in a computationally efﬁcient way. Therefore, the at-
tention system should select regions of the scene which are
most informative for understanding the robot’s visual envi-
ronment.

Our system is able to track previously identiﬁed objects
over consecutive frames using peripheral vision, but can only
classify new objects when they appear in the (high-resolution)
fovea, F. Our uncertainty in a tracked object’s position grows
with time. Directing the fovea over the expected position of
the object and re-classifying allows us to update our estimate
of its location. Alternatively, directing the fovea to a different
part of the scene allows us to ﬁnd new objects. Thus, since
the fovea cannot move instantaneously, the attention system
needs to periodically decide between the following actions:
A1. Conﬁrmation of a tracked object by ﬁxating the fovea
over the predicted location of the object to conﬁrm its
presence and update our estimate of its position;

A2. Search for unidentiﬁed objects by moving the fovea to
some new part of the scene and running the object clas-
siﬁers over that region.

it

Once the decision is made,

takes approximately
0.5 seconds—limited by the speed of the PTZ camera—for
the fovea to move to and acquire an image of the new region.2
During this time we track already identiﬁed objects using pe-
ripheral vision. After the fovea is in position we search for
new and tracked objects by scanning over all scales and shifts
within the foveal view as is standard practice for many state-
of-the-art object classiﬁers.

More formally, let ξk(t) denote the state of the k-th object,
ok, at time t. We assume independence of all objects in the
scene, so our uncertainty is simply the sum of entropy terms

2In our experiments with single high-resolution video stream, we
simulate the time required to move the fovea by delaying the image
selected from the HDV camera by the required number of frames.

.

(2)
Here the term Hunidentiﬁed(ξk(t)) is a constant that reﬂects
our uncertainty in the state of untracked objects,3 and
Htracked(ξk(t + 1)) is the entropy associated with the Kalman
ﬁlter that is attached to the object once it has been detected
(see Section 3.1).

As objects are tracked in peripheral vision, our uncertainty
in their location grows. We can reduce this uncertainty by
observing the object’s position through re-classiﬁcation of the
area around its expected location. We use a Kalman ﬁlter to
track the object, so we can easily compute the reduction in
entropy from taking action A1 for any object ok ∈ T ,

2 (ln|Σk(t)| − ln|Σk(t + 1)|)

(3)
where Σk(t) and Σk(t + 1) are the covariance matrices as-
sociated with the Kalman ﬁlter for the k-th object before and
after re-classifying the object, respectively.

ΔHA1 = 1

3Our experiments estimated this term assuming a large-variance
distribution over the peripheral view of possible object locations; in
practice, the algorithm’s performance appeared very insensitive to
this parameter.

IJCAI-07

2117

In this formalism, we see that by taking action A1 we re-
duce our uncertainty in a tracked object’s position, whereas
by taking action A2 we may reduce our uncertainty over
unidentiﬁed objects. We assume equal costs for each action
and therefore we choose the action which maximizes the ex-
pected reduction of the entropy H deﬁned in Eq. (1).

We now describe our tracking and interest models in detail.

3.1 Object Tracking
We track identiﬁed objects using a Kalman ﬁlter. Because
we assume independence of objects, we associate a separate
Kalman ﬁlter with each object. An accurate dynamic model
of objects is outside the current scope of our system, and we
use simplifying assumptions to track each object’s coordi-
nates in the 2-d image plane. The state of the k-th tracked
object is

˙xk

˙yk]T

ξk = [xk

yk

(4)
and represents the (x, y)-coordinates and (x, y)-velocity of
the object.

On each frame we perform a Kalman ﬁlter motion update
step using the current estimate of the object’s state (position
and velocity). The update step is

⎡
⎢⎣1 0 ΔT
0 1
0 0
0 0

0
0 ΔT
0
1
0
1

⎤
⎥⎦ ξk(t) + ηm

(5)

ξk(t + 1) =

where ηm ∼ N (0, Σm) is the motion model noise, and ΔT
is the duration of one timestep.

We compute optical ﬂow vectors in the peripheral view
generated by the Lucas and Kanade (1981) sparse optical
ﬂow algorithm. From these ﬂow vectors we measure the ve-
locity of each tracked object, zv(t), by averaging the optical
ﬂow within the object’s bounding box. We then perform a
Kalman ﬁlter observation update, assuming a velocity obser-
vation measurement model

zv(t) =

ξk(t) + ηv

(6)
where ηv ∼ N (0, Σv) is the velocity measurement model
noise.

When the object appears in the foveal view, i.e., after tak-
ing action A1, the classiﬁcation system returns an estimate,
zp(t), of the object’s position, which we use to perform a cor-
responding Kalman ﬁlter observation update to greatly reduce
our uncertainty in its location accumulated during tracking.
Our position measurement observation model is given by

(cid:12)
(cid:13)
0 0 1 0
0 0 0 1

(cid:12)
(cid:13)
1 0 0 0
0 1 0 0

zp(t) =

ξk(t) + ηp

(7)
where ηp ∼ N (0, Σp) is the position measurement model
noise.

We incorporate into the position measurement model the
special case of not being able to re-classify the object even
though the object is expected to appear in the foveal view.
For example, this may be due to misclassiﬁcation error, poor

estimation of the object’s state, or occlusion by another ob-
ject. In this case we assume that we have lost track of the
object.

Since objects are recognized in the foveal view, but are
tracked in the peripheral view, we need to transform coor-
dinates between the two views. Using the well-known stereo
calibration technique of [Zhang, 2000], we compute the ex-
trinsic parameters of the PTZ camera with respect to the
wide-angle camera. Given that our objects are far away rel-
ative to the baseline between the cameras, we found that
the disparity between corresponding pixels of the objects in
each view is small and can be corrected by local correlation.4
This approach provides adequate accuracy for controlling the
fovea and ﬁnding the corresponding location of objects be-
tween views.

Finally, the entropy of a tracked object’s state is required
for deciding between actions. Since the state, ξ(t), is a Gaus-
sian random vector, it has differential entropy

2 ln(2πe)4|Σk(t)|

Htracked(ξk(t)) = 1

Interest Model

(8)
4×4 is the covariance matrix associated with

where Σk(t) ∈ R
our estimate of the k-th object at time t.
3.2
To choose which foveal region to examine next (under action
A2), we need a way to estimate the probability of detecting
a previously unknown object in any region F in the scene.
To do so, we deﬁne an “interest model” that rapidly identi-
ﬁes pixels which have a high probability of containing objects
that we can classify. A useful consequence of this deﬁnition
is that our model automatically encodes the biological phe-
nomena of saliency and inhibition of return—the processes
by which regions of high visual stimuli are selected for fur-
ther investigation, and by which recently attended regions are
prevented from being attended again (see [Klein, 2000] for a
detailed review).

In our approach, for each pixel in our peripheral view, we
estimate the probability of it belonging to an unidentiﬁed ob-
ject. We then build up a map of regions in the scene where the
density of interest is high. From this interest map our atten-
tion system determines where to direct the fovea to achieve
maximum beneﬁt as described above.

More formally, we deﬁne a pixel to be interesting if it is
part of an unknown, yet classiﬁable object. Here classiﬁable,
means that the object belongs to one of the object classes
that our classiﬁcation system has been trained to recognize.
We model interestingness, y(t), of every pixel in the periph-
eral view, x(t), at time t using a dynamic Bayesian network
(DBN), a fragment of which is shown in Figure 3. For the
(i, j)-th pixel, yi,j(t) = 1 if that pixel is interesting at time
t, and 0 otherwise. Each pixel also has associated with it a
vector of observed features φi,j(x(t)) ∈ R

n.

4We resize the image from the PTZ camera to the size it would
appear in the peripheral view. We then compute the cross-correlation
of the resized PTZ image with the peripheral image, and take the
actual location of the fovea to be the location with maximum cross-
correlation in a small area around its expected location in the periph-
eral view.

IJCAI-07

2118

yi-1,j-1(t-1)
yi-1,j(t-1)
yi-1,j+1(t-1)

yi,j-1(t-1)
yi,j(t-1)
yi,j+1(t-1)

yi+1,j-1(t-1)
yi+1,j(t-1)
yi+1,j+1(t-1)

procedure we can hand label a single training video and auto-
matically generate data for learning the interest model given
any speciﬁc combination of classiﬁers and foveal movement
policies.

We adapt the parameters of our probabilistic models so
as to maximize the likelihood of the training data described
above. Our interest model is trained over a 450 frame video
sequence, i.e., 30 seconds at 15 frames per second. Figure 4
shows an example of a learned interest map, in which the
algorithm automatically selected the mugs as the most inter-
esting region.

yi,j(t)

i,j(x(t))

Figure 3: Fragment of dynamic Bayesian network for modeling at-
tentive interest.

The interest belief state over the entire frame at time t
is P (y(t) | y(0), x(0), . . . , x(t)). Exact inference in this
graphical model is intractable, so we apply approximate infer-
ence to estimate this probability. Space constraints preclude a
full discussion, but brieﬂy, we applied Assumed Density Fil-
tering/the Boyen-Koller (1998) algorithm, and approximate
this distribution using a factored representation over individ-
i,j P (yi,j(t)). In our experiments,

ual pixels, P (y(t)) ≈ (cid:14)

this inference algorithm appeared to perform well.

(cid:4)

(cid:5)
yi,j(t) | yN (i,j)(t − 1)

In our model, the interest for a pixel depends both on
the interest in the pixel’s (motion compensated) neighbor-
hood, N(i, j), at the previous time-step and features ex-
tracted from the current frame. The parameterizations for
and P (yi,j(t) | φi,j(x(t)))
both P
are given by logistic functions (with learned parameters), and
we use Bayes rule to compute P (φi,j(x(t)) | yi,j(t)) needed
in the DBN belief update. Using this model of the interest
map, we estimate the probability of ﬁnding an object in a
given foveal region, P (ok | F), by computing the mean of
the probability of every pixel in the foveal region being inter-
esting.5

The features φi,j used to predict interest in a pixel are ex-
tracted over a local image patch and include Harris corner
features, horizontal and vertical edge density, saturation and
color values, duration that the pixel has been part of a tracked
object, and weighted decay of the number of times the fovea
had previously ﬁxated on a region containing the pixel.
3.3 Learning the Interest Model
Recall that we deﬁne a pixel to be interesting if it is part of an
as yet unclassiﬁed object. In order to generate training data so
that we can learn the parameters of our interest model, we ﬁrst
hand label all classiﬁable objects in a low-resolution training
video sequence. Now as our system begins to recognize and
track objects, the pixels associated with those objects are no
longer interesting, by our deﬁnition. Thus, we generate our
training data by annotating as interesting only those pixels
marked interesting in our hand labeled training video but that
are not part of objects currently being tracked. Using this

5Although one can envisage signiﬁcantly better estimates, for ex-
ample using logistic regression to recalibrate these probabilities, the
algorithm’s performance appeared fairly insensitive to this choice.

Figure 4: Learned interest. The righthand panel shows the proba-
bility of interest for each pixel in the peripheral image (left).

4 Experimental Results
We evaluate the performance of the visual attention system
by measuring the percentage of times that a classiﬁable object
appearing in the scene is correctly identiﬁed. We also count
the number of false-positives being tracked per frame. We
compare our attention driven method for directing the fovea
to three naive approaches:
(i) ﬁxing the foveal gaze to the center of view,
(ii) linearly scanning over the scene from top-left to bottom-

right, and,

(iii) randomly moving the fovea around the scene.
Our classiﬁers are trained on image patches of roughly
100× 100 pixels depending on the object class being trained.
For each object class we use between 200 and 500 positive
training examples and 10, 000 negative examples. Our set
of negative examples contained examples of other objects,
as well as random images. We extract a subset of C1 fea-
tures (see [Serre et al., 2004]) from the images and learn a
boosted decision tree classiﬁer for each object. This seemed
to give good performance (comparable to state-of-the-art sys-
tems) for recognizing a variety of objects. The image patch
size was chosen for each object so as to achieve good classi-
ﬁcation accuracy while not being so large so as to prevent us
from classifying small objects in the scene.

We conduct

two experiments using recorded high-
deﬁnition video streams, the ﬁrst assuming perfect classiﬁ-
cation (i.e., no false-positives and no false-negatives, so that
every time the fovea ﬁxates on an object we correctly classify
the object), and the second using actual trained state-of-the-
art classiﬁers. In addition to the different fovea control algo-
rithms, we also compare our method against performing ob-
ject classiﬁcation on full frames for both low-resolution and

IJCAI-07

2119

high-resolution video streams. The throughput (in terms of
frames per second) of the system is inversely proportional to
the area scanned by the object classiﬁers. The low resolution
was chosen to exhibit the equivalent computational require-
ments of the foveal methods and is therefore directly com-
parable. The high resolution, however, being of much larger
size than the foveal region, requires additional computation
time to complete each frame classiﬁcation. Thus, to meet
real-time operating requirements, the classiﬁers were run ten
times less often.

Figure 5 shows example timelines for recognizing and
tracking a number of different objects using our method (bot-
tom), randomly moving the fovea (middle), and using a ﬁxed
fovea (top). Each object is shown on a different line. A dot-
ted line indicates that the object has appeared in the scene but
has not yet been recognized (or that our tracking has lost it).
A solid line indicates that the object is being tracked by our
system. The times when the fovea ﬁxates on each object are
marked by a diamond ((cid:5)). It is clear that our method recog-
nizes and starts tracking objects more quickly than the other
methods, and does not waste effort re-classifying objects in
regions already explored (most apparent when the fovea is
ﬁxed).

Figure 5: Example timelines for identiﬁcation and tracking of ob-
jects in a continuous video sequence using different methods for
controlling the fovea: ﬁxed (top), random (middle), and our inter-
est driven method (bottom).

In our experiments we use a ﬁxed size fovea covering ap-
proximately 10% of the peripheral ﬁeld-of-view.6 All recog-
nizable objects in our test videos were smaller than the size
of the foveal window. We record peripheral and foveal video
streams of an ofﬁce environment with classiﬁers trained on
commonly found objects: coffee mugs, disposable cups, sta-
plers, scissors, etc. We hand label every frame of the video
stream. The video sequence consists of a total of 700 frames
recorded at 15 frames per second—resulting in 100 foveal

6Although direct comparison is not possible, for interest we note

that the human fovea covers roughly 0.05% of the visual ﬁeld.

ﬁxations (since it takes approximately seven frames for the
fovea to move).

A comparison between our method and the other ap-
proaches is shown in Table 1. We also include the F1-score
when running state-of-the-art object classiﬁers in Table 2.

Fovea control

Full image (low-resolution)
Full image (high-resolution)
Fixed at center
Linear scanning
Random scanning
Our (attentive) method

Perfect

classiﬁcation

n/a
n/a

16.1%
61.9%
62.0%
85.1%

Actual

classiﬁers

0.0%a
62.2%
15.9%
43.1%
60.3%
82.9%

aOur object classiﬁers are trained on large images samples (of ap-
proximately 100×100 pixels). This result illustrates that objects in the
low-resolution view occupy too few pixels for reliable classiﬁcation.

Table 1: Percentage of objects in a frame that are correctly identiﬁed
using different fovea control algorithms.

Fovea control
Full image (low-res)
Full image (high-res)
Fixed at center
Linear scanning
Random scanning
Our method

Precision

Recall
0.0%
0.0%
62.2%
95.0%
15.9% 100.0%
97.2%
43.1%
98.9%
60.3%
82.9%
99.0%

F1-Score

0.0%
75.2%
27.4%
59.7%
74.9%
90.3%

Table 2: Recall, precision and F1-score for objects appearing the
scene. The low-resolution run results in an F1-score of zero since
objects appearing in the scene occupy too few pixels for reliable
classiﬁcation.

The results show that our attention driven method performs
signiﬁcantly better than the other foveal control strategies.
Our method even performs better than scanning the entire
high-resolution image. This is because our method runs much
faster since it only needs to classify objects in a small region.
When running over the entire high-resolution image, the sys-
tem is forced to skip frames so that it can keep up with real-
time. It is therefore less able to detect objects entering and
leaving the scene than our method. Furthermore, because we
only direct attention to interesting regions of the scene, our
method results in signiﬁcantly higher precision, and hence
better F1-score, than scanning the whole image.

Finally, we conduct experiments with a dual wide-angle
and PTZ camera system.
In order to compare results be-
tween the different foveal control strategies, we ﬁx the robot
and scene and run system for 100 frames (approximately 15
foveal ﬁxations) for each strategy. We then change the scene
by moving both the objects and the robot pose, and repeat the
experiment, averaging our results across the different scenes.
The results are shown in Table 3. Again our attention driven
method performs better than the other approaches.
Videos demonstrating our results are provided at
http://ai.stanford.edu/∼sgould/vision/

IJCAI-07

2120

Fovea control
Fixed at centera
Linear scanning
Random scanning
Our method

Precision
Recall
9.49%
97.4%
13.6% 100.0%
84.1%
27.7%
62.2%
83.9%

F1-Score
17.3%
24.0%
41.6%
71.5%

aIn some of the trials a single object happened to appear in the
center of the ﬁeld of view and hence was detected by the stationary
foveal gaze.

Table 3: Recall, precision and F1-score of recognizable objects for
hardware executing different foveal control strategies over a number
of stationary scenes.

5 Discussion
In this paper, we have presented a novel method for im-
proving performance of visual perception on robotic plat-
forms and in digital video. Our method, motivated from
biological vision systems, minimizes the uncertainty in the
location of objects in the environment using a with con-
trollable pan-tilt-zoom camera and ﬁxed wide-angle camera.
The pan-tilt-zoom camera captures high-resolution images
for improved classiﬁcation. Our attention system controls the
gaze of the pan-tilt-zoom camera by extracting interesting re-
gions (learned as locations where we have a high probabil-
ity of ﬁnding recognizable objects) from a ﬁxed-gaze low-
resolution peripheral view of the same scene.

Our method also works on a single high-resolution digital
video stream, and is signiﬁcantly faster than naively scanning
the entire high-resolution image. By experimentally compar-
ing our method to other approaches for both high-deﬁnition
video and on real hardware, we showed that our method ﬁx-
ates on and identiﬁes objects in the scene faster than all of
the other methods tried. In the case of digital video, we even
perform better than the brute-force approach of scanning the
entire frame, because the increased computational cost of do-
ing so results in missed frames when run in real-time.
Acknowledgments
We give warm thanks to Morgan Quigley for help with the
STAIR hardware, and to Pieter Abbeel for helpful discus-
sions. This work was supported by DARPA under contract
number FA8750-05-2-0249.

References
[Bj¨orkman and Kragic, 2004] Marten Bj¨orkman

and Danica
Kragic. Combination of foveal amd peripheral vision for object
recognition and pose estimation.
In Proceedings of the 2004
IEEE International Conference on Robotics and Automation,
pages 5135–5140, 2004.

[Boyen and Koller, 1998] Xavier Boyen and Daphne Koller.
Tractable inference for complex stochastic processes.
In Pro-
ceedings of the 14th Conference on Uncertainty in Artiﬁcial
Intelligence—UAI 1998, pages 33–42. San Francisco: Morgan
Kaufmann, 1998.

[Brubaker et al., 2005] S. Charles Brubaker, Jianxin Wu, Jie Sun,
Matthew D. Mullin, and James M. Rehg. On the design of cas-
cades of boosted ensembles for face detection.
In Tech report
GIT-GVU-05-28, 2005.

[Edelman and Weiss, 1995] Shimon Edelman and Yair Weiss. Vi-

sion, hyperacuity, 1995.

[Fahle and Poggio, 1981] M. Fahle and T. Poggio. Visual hyperacu-
ity: Spatiotemporal interpolation in human vision. In Proceed-
ings of the Royal Society of London. The Royal Society, 1981.

[Hu et al., 2004] Yiqun Hu, Xing Xie, Wei-Ying Ma, Liang-Tien
Chia, and Deepu Rajan. Salient region detection using weighted
feature maps based on the human visual attention model. In Fifth
IEEE Pacifc-Rim Conference on Multimedia, Tokyo Waterfront
City, Japan, November 2004.

[Itti and Koch, 2001] L. Itti and C. Koch. Computational modeling
of visual attention. Nature Reviews Neuroscience, 2(3):194–203,
Mar 2001.

[Itti et al., 1998] Laurent Itti, Christof Koch, and Ernst Niebur. A
model of saliency-based visual attention for rapid scene analy-
sis. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 20(11):1254–1259, 1998.

[Klein, 2000] Raymond M. Klein. Inhibition of return. Trends in

Cognitive Sciences, 4(4), April 2000.

[Koch and Ullman, 1985] C. Koch and S. Ullman. Shifts in selec-
tive visual attention: Towards the underlying neural circuitry. Hu-
man Neurobiology, 4:219–227, 1985.

[Lucas and Kanade, 1981] Bruce D. Lucas and Takeo Kanade. An
iterative image registration technique with an application to
stereo vision (ijcai). In Proceedings of the 7th International Joint
Conference on Artiﬁcial Intelligence (IJCAI ’81), pages 674–679,
April 1981.

[Orabona et al., 2005] Francesco Orabona, Giorgio Metta, and
Giulio Sandini. Object-based visual attention: a model for a be-
having robot. IEEE Conference on Computer Vision and Pattern
Recognition, 3:89, 2005.

[Privitera and Stark, 2000] Claudio M. Privitera and Lawrence W.
Stark. Algorithms for deﬁning visual regions-of-interest: Com-
parison with eye ﬁxations. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 22(9):970–982, 2000.

[Serre et al., 2004] Thomas Serre, Lior Wolf, and Tomaso Poggio.
A new biologically motivated framework for robust object recog-
nition. In Al Memo 2004-026, November 2004.

[Tagare et al., 2001] Hermant D. Tagare, Kentaro Toyama, and
Jonathan G. Wang. A maximum-likelihood strategy for direct-
ing attention during visual search. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 23(5):490–500, 2001.

[Ude et al., 2003] A. Ude, C. G. Atkeson, and G. Cheng. Combin-
ing peripheral and foveal humanoid vision to detect, pursue, rec-
ognize and act. In Proc. IEEE/RSJ Int. Conf. Intelligent Robots
and Systems, pages 2173–2178, October 2003.

[Ude et al., 2006] A. Ude, C. Gaskett, and G. Cheng. Foveated vi-
In Proceedings 2006
sion systems with two cameras per eye.
IEEE International Conference on Robotics and Automation,
2006. ICRA 2006., pages 3457–3462, May 2006.

[Viola and Jones, 2004] Paul Viola and Michael J. Jones. Robust
real-time face detection. International Journal of Computer Vi-
sion, 57(2):137–154, 2004.

[Wandell, 1995] Brian A. Wandell. Foundations of Vision. Sinauer

Associates, Inc., 1995.

[Zhang, 2000] Zhengyou Zhang. A ﬂexible new technique for cam-
era calibration. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 22(11):1330–1334, 2000.

IJCAI-07

2121


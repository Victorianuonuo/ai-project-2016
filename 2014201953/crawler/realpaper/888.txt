Kernel Conjugate Gradient for Fast Kernel Machines

Nathan D. Ratliff, and J. Andrew Bagnell

Robotics Institute, Carnegie Mellon University,

Pittsburgh, PA. 15213 USA
{ndr,dbagnell}@ri.cmu.edu

Abstract

We propose a novel variant of the conjugate gradi-
ent algorithm, Kernel Conjugate Gradient (KCG),
designed to speed up learning for kernel machines
with differentiable loss functions. This approach
leads to a better conditioned optimization problem
during learning. We establish an upper bound on
the number of iterations for KCG that indicates it
should require less than the square root of the num-
ber of iterations that standard conjugate gradient re-
quires. In practice, for various differentiable ker-
nel learning problems, we ﬁnd KCG consistently,
and signiﬁcantly, outperforms existing techniques.
The algorithm is simple to implement, requires no
more computation per iteration than standard ap-
proaches, and is well motivated by Reproducing
Kernel Hilbert Space (RKHS) theory. We further
show that data-structure techniques recently used
to speed up kernel machine approaches are well
matched to the algorithm by reducing the dominant
costs of training: function evaluation and RKHS in-
ner product computation.

Introduction

1
Kernel methods, in their various incarnations (e.g. Gaussian
Processes (GPs), Support Vector Machines (SVMs), Kernel
Logistic Regression (KLR)) have recently become a preferred
approach to non-parametric machine learning and statistics.
They enjoy this status because of their conceptual clarity,
strong empirical performance, and theoretical foundations.
The primary drawback to kernel methods is their computa-
tional complexity. GPs require the inversion of an n × n (co-
variance/kernel) matrix, implying a running time of O(n3),
where n is the size of the training set. SVMs require sim-
ilar computation to solve the convex program, although in-
tense research has gone into fast, specialized approximations
[Sch¨olkopf & Smola, 2002].

State-of-the-art approaches to kernel

learning revolve
largely around two techniques:
iterative optimization algo-
rithms, and learning by representing the solution with only a
subset of the original data points. Our algorithm applies most
directly to the former line of research, although we address
the latter in the conclusions.

We propose a novel variant of the conjugate gradient algo-
rithm, Kernel Conjugate Gradient (KCG), designed to speed
up learning for kernel machines with differentiable loss func-
tions (e.g. Gaussian Process mean inference, Kernel Logistic
Regression). This algorithm is motivated by the understand-
ing that all gradient-based methods rely, at least implicitly,
on a particular metric or inner product [Sch¨olkopf & Smola,
2002]. It is natural in kernel learning problems for the algo-
rithm to inherit the metric on functions that the kernel pro-
vides. In Section 2.4 we show that such an approach can be
interpreted as a Riemannian metric method similar to Amari’s
Natural Gradient [Amari & Nagaoka, 2000], although the
kernel metric is much less expensive to compute.

In Section 5, we establish an upper bound on the number
of iterations for KCG that indicates it should require fewer
than the square root of the number of iterations that stan-
dard conjugate gradient requires. The algorithm is simple to
implement, requires no more computation per iteration than
standard approaches, and is well motivated by Reproducing
Kernel Hilbert Space (RKHS) theory. In practice, for various
differentiable kernel learning problems, we ﬁnd KCG consis-
tently, and signiﬁcantly, outperforms existing techniques.

Recent research has demonstrated the beneﬁts of space-
partitioning data-structures to speed up certain kernel ma-
chines [Shen et al., 2006; Gray & Moore, 2001]. We show
that these techniques work well with KCG as they reduce the
dominant computational burdens in training: RKHS function
evaluations and inner product computations.

2 Preliminaries
Below we brieﬂy review the theory of kernel machines in
terms of Reproducing Kernel Hilbert Spaces. We then de-
scribe the regularized risk functionals we are interested in
optimizing during learning, and ﬁnish by reviewing the func-
tional gradient, a generalization of the standard gradient to
inner product spaces of functions.
2.1 Reproducing Kernel Hilbert Spaces
An RKHS of functions Hk is a complete inner product space,
known as a Hilbert space, that arises from the completion of
k = {k(x, .) | x ∈ X}, where
a set of basis functions BX
k : X ×X → R is a symmetric, positive-deﬁnite kernel func-
tion, and X is a continuous domain sometimes known as an
index set. A common kernel, and one used exclusively in our

IJCAI-07

1017

Numerous other important examples occur in the litera-
ture [Sch¨olkopf & Smola, 2002]. We focus on these, two
of the best known kernel algorithms other than the non-
differentiable Support Vector Machine, as they are particu-
larly important tools in machine learning for classiﬁcation
and regression.

2.3 The Functional Gradient
For large-scale problems and problems for which direct so-
lution is inapplicable, the most popular approach to optimiz-
ing these functionals is through the use of iterative, gradient-
based techniques. Gradient-based algorithms, such as steep-
est descent, are traditionally deﬁned with respect to the gradi-
ent arising from the Euclidean inner product between param-
eter vectors. There are many ways in which a given regular-
ized risk functional can be parameterized, though, and each
gives rise to a different parameter gradient. It is intuitively
natural to follow the gradient deﬁned uniquely by the RKHS
inner product. We review some basic concepts behind the
functional gradient below.

The functional gradient may be deﬁned implicitly as the
linear term of the change in a function due to a small pertur-
bation  in its input [Mason et al., 1999]

F [f + g] = F [f] + (cid:4)∇F [f], g(cid:5) + O(

2).

Following this deﬁnition using the RKHS inner product
(cid:4)., .(cid:5)Hk allows us to deﬁne the kernel gradient of a regular-
ized risk functional [Sch¨olkopf & Smola, 2002]. We use three
basic formulas that can be easily derived from this deﬁnition:
(cid:2)
1. Gradient of the evaluation functional. Deﬁne Fx :
i αik(xi, x). Then

Hk → R as Fx[f] = f(x) =
∇kFx[f] = k(x, .).
2. Gradient of the square RKHS norm. Deﬁne F(cid:4).,.(cid:5)
:
Hk → R as F(cid:4).,.(cid:5)[f] = (cid:7)f(cid:7)2Hk = (cid:4)f, f(cid:5)Hk. Then
∇kF(cid:4).,.(cid:5)[f] = 2f.
3. Chain rule. Let g : R → R be a differentiable func-
tion, and let F : Hk → R be an arbitrary differentiable
functional. Then ∇kg(F [f]) = g
A straight forward application of the above formulas brings
us to the following result. The kernel gradient of a regularized
risk functional (Equation 2) is

(cid:2)(F [f])∇kF [f].

∇kR[f] =

=

l(xi, yi, z)|

f (xi)∇kFxi[f] + λf

l(xi, yi, z)|

f (xi)k(xi, .)+

(5)

i=1

∂
∂z

n(cid:3)
n(cid:3)
n(cid:3)
αik(xi, .)
(cid:6)
n(cid:3)

∂
∂z

i=1

i=1

λ

(cid:7)

k(xi, .)

−(cid:2)x−x(cid:3)(cid:2)2

experiments, is the exponential Radial Basis Function (RBF)
(cid:2)
(cid:2)) = e
. The RKHS inner product between two
k(x, x
j βjk(xj, .) is de-
functions f =
ﬁned as

(cid:2)
i αik(xi, .) and g =

(cid:3)

2σ2

(cid:4)f, g(cid:5)Hk =

αiβjk(xi, xj).

(1)

i,j

Central to the idea of an RKHS is the reproducing prop-
erty which follows directly from the above deﬁnition.
It
states that the basis functions k(x, .) ∈ BX
k are represen-
ters of evaluation. Formally, for all f ∈ Hk and x ∈ X ,
(cid:4)f, k(x, .)(cid:5)Hk = f(x). When the basis functions are normal-
ized, this means the evaluation of f at x is the scalar projec-
tion of f onto k(x, .). Note that there exists a simple mapping
k between the domain X and the RKHS basis BX
φ : X → BX
k
deﬁned by the kernel as φ(x) = k(x, .). It follows that for any
x, x

)(cid:5)Hk = (cid:4)k(x, .), k(x
(cid:2)

, .)(cid:5)Hk = k(x, x
(cid:2)

).

(cid:2) ∈ X
(cid:4)φ(x), φ(x
(cid:2)

A complete overview of these concepts can be found in
[Aronszajn, 1950].

A fundamental result ﬁrst proven in [Kimeldorf & Wahba,
1971] and then generalized in [Sch¨olkopf et al., 2001] is
the Representer Theorem, which makes possible the direct
minimization of a particular class of functionals.
It states
that given a subset D = {(xi, yi)}n
i=1 ⊂ X × R (i.e.
the dataset), the minimizer of a functional with the form
F [f] = c((x1, y1, f(x1)), . . . , (xn, yn, f(xn))) + g((cid:7)f(cid:7)2Hk),
where c : X × R2 → R is arbitrary and g : [0,∞] → R
(cid:2)
is strictly monotonically increasing, must have the form ˜f =
xi∈D αik(xi, .).

2.2 Regularized Risk Functionals
An important class of functionals, common in machine learn-
ing, for which the Representer Theorem holds is the regular-
ized risk functional [Sch¨olkopf & Smola, 2002]:
(cid:4)f, f(cid:5)Hk

R[f] =

n(cid:3)

l(xi, yi, f(xi)) + λ
2

(2)

i=1

These functionals combine a data-dependent risk term, with
a “prior” (or regularization) term that controls the complexity
of the solution by penalizing the norm of the function f in the
RKHS. Our work focuses on the case where l is differentiable
in its third argument; many important kernel machines have
this property. For instance, we can write Kernel Logistic Re-
gression [Zhu & Hastie, 2001] in this form. Let y ∈ {−1, 1}.
Then

(cid:4)

(cid:5)

Rklr [f] =

log

1 + eyif (xi)

(cid:4)f, f(cid:5)Hk

+ λ
2

(3)

Similarly, we may write the popular Gaussian Process Re-
gression (for the mean function) and the Regularized Least
Squares Classiﬁcation algorithm in this form 1:

n(cid:3)

i=1

n(cid:3)

i=1

Rrls[f] =

1
2

(yi − f(xi))2 + λ
2

(cid:4)f, f(cid:5)Hk

(4)

=

l(xi, yi, z)|

∂
∂z

f (xi) + λαi

i=1

1These two differently motivated algorithms optimize the same

functional.

where we again use αi to denote the parameters of the expan-
sion of f in terms of the kernels k(xi, .).

IJCAI-07

1018

Equation 5 allows us to easily ﬁnd the kernel gradient of
the functionals we described above and use in the remainder
of this paper:

Kernel Logistic Regression (KLR):
∇kRklr [f] =

n(cid:3)

(cid:6)

(cid:7)

yi
−yif (xi)

1 + e

k(xi, .)

(6)

Regularized Least Squares (RLS), Gaussian Process:
(f(xi) − yi + λαi)k(xi, .)

∇kRrls[f] =

i=1

λαi +
n(cid:3)

i=1

2.4 The Kernel Gradient and Riemannian Metrics
Above we described the kernel gradient as a function; that
is as a linear combination of basis functions. The ker-
nel gradient as in Equation 5 demonstrates a property sim-
(cid:2)
ilar to the Representer Theorem: namely that ∇kF [f] =
i=1 γik(xi, .) for appropriate γi ∈ R. In other words, the
kernel gradient of F is represented in the ﬁnite-dimensional
subspace SD = span{BD
} of Hk. A gradient descent type
method through SD then amounts to modifying the coefﬁ-
cients αi of the current function f by γi. That is, we can
understand the kernel gradient as modifying parameters,

n

k

˜f ← f − λ∇kF [f] ⇔ ˜αi ← αi − λγi,

γ

just as a standard gradient descent algorithm would. The dif-
ference is that the coefﬁcients γi for the kernel gradient are
not the same as those of the parameter gradient. One can ver-
ify that they differ by ∇αF [f] = Kγ where K is the kernel
matrix and γ is the vector of coefﬁcients.
We can derive this relation in another way. Starting from
the parameter gradient, we deﬁne a Riemannian metric [Has-
sani, 1998] on the space of parameters. This deﬁnes our no-
tion of size in the space of parameters. We then consider an
alternate deﬁnition of the gradient, as the direction of steepest
ascent for a “small” change in coefﬁcients α:

∇F [α] = max

F [α + γ] s.t. (cid:7)γ(cid:7) < .

K

It can be shown that taking (cid:7)γ(cid:7)2
α as γT γ gives the vanilla pa-
(cid:2)
rameter gradient ∇αF , while deﬁning the norm with respect
to the RKHS inner product (cid:7)γ(cid:7)2Hk =
i,j γiγjk(xi, xj) =
γT Kγ gives the functional gradient coefﬁcients ∇kF =
−1∇αF . [Hassani, 1998]
This interpretation of the kernel gradient makes connec-
tions with other metric methods more clear. For instance,
Amari [Amari & Nagaoka, 2000] considers the use of a met-
ric derived from information geometry that leads to the “nat-
ural gradient”. Such algorithms are applicable here as well
since we can compute the metric for the probabilistic models
given by Gaussian Processes or KLR. Unfortunately, comput-
ing the natural gradient in these cases is very expensive: for
instance, in Gaussian Processes it is as expensive as inverting
the kernel matrix, the very computational difﬁculty we are
striving to avoid. By contrast, computing the kernel gradi-
ent is very cheap: cheaper in fact then the standard parameter
gradient.2

2Furthermore, in practice we often ﬁnd deriving the kernel gra-
dient using the functional gradient rules easier than deriving the pa-
rameter gradient.

Algorithm 1 Kernel Conjugate Gradient
1: procedure KCG(F : Hk → R, f0 ∈ Hk,  > 0)
2:
3:
4:
5:
6:

(cid:2)
i ← 0
g0 ← ∇kF [f0] =
n
j=1 γ
h0 ← −g0
while (cid:4)gi, gi(cid:5)Hk >  do

j k(xj, .)

(0)

λhi]

fi+1 ← fi + λihi where λi = arg minλ F [fi +
gi+1 ← ∇kF [fi+1] =
hi+1 ← −gi+1 + ηihi where ηi =
(cid:4)gi+1−gi,gi+1(cid:5)Hk
i ← i + 1

(cid:2)
(i+1)
n
j=1 γ
j
(i+1)−γ
γ(i)T

(i))T
Kγ
Kγ(i)

(cid:4)gi,gi(cid:5)Hk

k(xj, .)

= (γ

(i+1)

(7)

7:
8:

9:
10:
11:
12: end procedure

end while
return fi

3 The Kernel Conjugate Gradient Algorithm
Both in theory and in practice it is understood that conjugate
gradient (CG) methods outperform standard steepest descent
procedures [Ashby et al., 1990]. These techniques have been
used profusely throughout machine learning, in particular,
for regularized risk minimization and kernel matrix inversion
[Gibbs, 1997; Sch¨olkopf & Smola, 2002].

In this section, we present an algorithm we term Kernel
Conjugate Gradient (KCG) that takes advantage of conju-
gate direction search while utilizing the RKHS inner prod-
uct (cid:4)f, g(cid:5)Hk = αT Kβ. Algorithm 1 gives the general (non-
linear) KCG algorithm in Polak-Ribi`ere form [Ashby et al.,
1990]. In essence, Algorithm 1 comes directly from conju-
gate gradient by replacing all gradients by their functional
equivalents and replacing Euclidean inner products with an
RKHS inner product.

Note that the computational complexity per iteration of
KCG is essentially identical to that of the more conventional
Parameter Conjugate Gradient (PCG) algorithm. Intuitively,
while the kernel inner product takes time O(n2) compared to
the O(n) vanilla inner product used by PCG, KCG is corre-
(cid:2)
spondingly more efﬁcient in the gradient computation since
∇αF [f] = Kγ, where ∇kF [f] =
i γik(xi, .). It is pos-
sible in the case of RLS to step through an iteration of each
algorithm and show that the number of operations is equiva-
lent.

inner product αT β → (cid:2)

We emphasize that despite its somewhat involved deriva-
tion, the implementation of this algorithm is just a simple ex-
tension of PCG. The differences amount to only a change of
i,j αiβjk(xi, xj) = αT Kβ, and a
different, though in some ways simpler, gradient computa-
tion. We also point out that the line optimization (step 6) can
(cid:2)
be solved in closed-form in the case of quadratic risk func-
(cid:2)
tionals (e.g. RLS). For starting point f =
i αik(xi, .) and
i γik(xi, .) we have
search direction h =
F [f + λh] = − αT Kγ
γT Aγ

arg min
λ

where A is the Hessian of the quadratic functional when pa-
rameterized by α. Note that this formula differs from that

IJCAI-07

1019

derived under the parameter gradient (−αT γ/γT Aγ) only
in the numerator’s inner product, as is a common theme
throughout this algorithm. The theoretical and experimen-
tal results given below suggest that there is little reason why
one should prefer PCG to KCG in most (differentiable) kernel
algorithms.

4 Experimental Results - Kernel Conjugate

Gradient

We bench-marked KCG against PCG for both classiﬁcation
and regression tasks. In all cases, KCG signiﬁcantly out per-
formed PCG.

Our ﬁrst test was performed using KLR on the USPS
dataset (with a training/test size of 7291/2007) for the com-
mon one-vs-all task of recognizing the digit 4. We used a
length scale hyperparameter σ = 5 as was used in [Rifkin
et al., 2003] for RLS classiﬁcation, and a regularization con-
stant λ = 0. Figure 1 summarizes the results in log scale.

Second, we used RLS for both regression and classiﬁcation
using the Abalone and Soil datasets in addition to the USPS
dataset. The Abalone dataset 3 consisted of 3133 training
examples and 1044 test examples in 8 attributes. Our exper-
imental setup was equivalent to that in [Smola & Sch¨olkopf,
2000]. The Soil dataset contained three-dimensional exam-
ples of soil pH levels in areas of Honduras partitioned into a
training set of size 1709 and a test set of size 383. The latter
dataset and corresponding hyperparameters (λ = 0.1514 and
σ = 0.296283) were provided by [Gonzalez, 2005]. Again,
the results are summarized in Figure 1.

For RLS, there exists a quadratic

p(α) =

1

2 αT (K + λI)α − yT α

that can provide a lower bound to the regularized risk [Gibbs,
1997; Sch¨olkopf & Smola, 2002]. As theory suggests (see
below) the upper bound under KCG converges comparably
with the lower bound, while the upper bound under PCG lags
considerably behind. This implies faster convergence under a
gap termination criterion [Sch¨olkopf & Smola, 2002].

The right-most plots of Figure 1 contrast (in iterations,
equivalent to multiples of wall-clock time) the speeds of PCG
and KCG for both RLS and KLR. We plot of the number of
iterations of each to reach the same level of performance in
terms of loss on the data-set. Plots are terminated when con-
vergence is achieved as measured with the gap termination
criterion[Sch¨olkopf & Smola, 2002], and hyper-parameters
were chosen on a hold-out set. These ﬁgures conﬁrm what
analysis in the next section suggests: it takes more than the
square of the amount of time to achieve the same level of
performance using PCG as it does with KCG. Finally, in Ta-
ble 2 we directly compare the number of iterations needed
to achieve convergence on a number of data sets, all taken
again from the UCI data repository, for both RLS and KLR.
Averaged over the datasets, KCG is 54 times faster than the
standard conjugate gradient approach.

3See UCI

repository:

∼mlearn/MLRepository.html

http://www.ics.uci.edu/

5 KCG Analysis
We derived the Kernel Conjugate Gradient algorithm from a
normative point of view arguing that (cid:4)f, g(cid:5)Hk deﬁned the nat-
ural notion of inner product in the RKHS and hence for the
optimization procedure as well. The strong empirical perfor-
mance of KCG noted in the previous section, while in some
sense not surprising given we are using the “correct” inner
product, deserves analysis. We examine here the linear case
(as in RLS) where the analysis is more transparent, although
presumably similar results hold near the optima of non-linear
risk functionals.

We note a classic bound on the error reduction of CG (see

[Luenberger, 2003]),

(cid:7)ei(cid:7)A ≤ 2

(cid:6)√
√

κ − 1
κ + 1

(cid:7)
i (cid:7)e0(cid:7)A,

κ) for PCG.

where i is the iteration number, A is the Hessian of the
quadratic form with condition number κ, (cid:7)x(cid:7)A = xT Ax is a
norm on x, and ei = xi − x
∗. Loosely speaking, this gives a
√
running time complexity of O(
We start by analyzing the effective condition number of
KCG. As with essentially all variants of CG, the algorithm’s
dynamics can be described in terms of a preconditioning to
the spectrum of the Hessian [Ashby et al., 1990]. It can be
veriﬁed by inspection of the algorithm, that KCG is equiva-
lent to an implicitly preconditioned conjugate gradient algo-
rithm with preconditioner K[Ashby et al., 1990]. The follow-
ing theorem relates the running time of PCG to KCG in light
of the bound given above.

Theorem. Let κP CG be the condition number of Rrls
(Equation 4), and let κK be the condition number of the ker-
nel matrix K = [k(xi, xj)]i,j. Then the condition number
κKCG resulting from preconditioning the RLS risk functional
by K has the relation κP CG = κKκKCG.
Proof. Let σ1 ≥ σ2 ≥ . . . ≥ σn be the eigenvalues of
K. The condition number of K is then κK = σ1/σn. The
Hessian of Rrls is A = K T K + λK and has eigenvalues
σ2
i + λσi = σi(σi + λ), given in terms of the eigenvalues of
K. This implies

(cid:6)

(cid:7)

= κK

σ1 + λ
σn + λ

.

σ1 + λ
σn + λ

κP CG = σ1
σn
−1A = K

Since K is symmetric, positive-deﬁnite, the preconditioned
−1(K T K + λK) = K + λI,
Hessian becomes K
with corresponding eigenvalues σi + λ. Thus, κP CG =
κKκKCG.
2
The condition number κK of K is typically very large.
In particular, as the regularization constant decreases, the
asymptotic bound on the convergence of PCG approaches
the square of the bound on KCG. Alternatively, as the reg-
ularization constant increases, κKCG approaches 1 implying
an O(1) convergence bound for KCG, while the convergence
bound of PCG remains bounded below by O(κK). We would
thus expect a number of iterations for KCG that is dramati-
cally less than that of PCG.

It is informative to note that the decrease in computational
complexity from PCG to KCG (O(κ1/2) to O(κ1/4)) is at
least the amount we see from steepest descent(O(κ) to PCG
O(κ1/2)) [Luenberger, 2003].

IJCAI-07

1020

Figure 1: Upper left shows relative performances (in log scale) of KCG and PCG on the USPS data set optimizing KLR. The
remaining three left-most plots show relative convergence under RLS; green and red lines depict PCG performance on the upper
and lower bound gap-convergence quadratic forms, and the light blue line gives the (signiﬁcantly tighter) performance of KCG
on the upper bound. The third column shows the beneﬁt of using KD-trees for a single run (bottom) and by training set size
(top) using KCG. The right-most two plots show the equivalent number of PCG iterations required to achieve the performance
per iteration of KCG on the COVTYPE data set from the UCI data repository. Top right and bottom right show the performances
of RLS and KLR, respectively. An approximately quadratic relationship can be seen in both cases as the theory suggests.

, .) ∈ BD

6 Tree-Augmented Algorithms
For many stationary kernels, it is often the case that the ma-
jority of the basis functions in BD
k are nearly orthogonal. This
often stems from a relationship between the degree of orthog-
(cid:2)
k and the
onality of two basis functions k(x, .), k(x
(cid:2). This is often the case
Euclidean distance between x and x
when using the exponential RBF kernel given above, in which
the orthogonality between two basis functions increases ex-
ponentially with the square Euclidean distance between the
two points (cid:7)x − x

(cid:2)(cid:7)2.
(cid:2)
Previous work has shown how the evaluation of RKHS
i αik(xi, x) can be made fast when this
functions f(x) =
holds using N-body type algorithms [Gray & Moore, 2001].
Intuitively, the idea is to store the training data in a space-
partitioning tree, such as a KD-tree [Moore, 1990] as was
used in our experiments below, and recursively descend the
tree during evaluation, pruning negligible contributions.

The maximum and minimum impact of each set of pruned
points can be easily calculated resulting in upper and lower
bounds on the evaluation error. We demonstrate how such
data-structures and algorithms can be used to reduce the per
iteration O(n2) computational cost of both KCG and PCG
during learning as well as evaluation.

The inner loop computational bottleneck of KCG is
inner
in evaluating functions and calculating the kernel
(cid:2)
(cid:2)
If we rewrite the RKHS inner product be-
product.
(cid:2)
(cid:2)
i βik(xi, .) as
tween f =
g =
i αik(xi, .),
(cid:4)f, g(cid:5)Hk =
i αig(xi), then re-
i,j αiβjk(xi, xj) =
ducing the computational complexity of RKHS function
evaluations will simultaneously encompass both of these
bottlenecks.
the iteration complexity of PCG

Similarly,

is dominated by the computation of the parameter gradi-
ent. We can rewrite the parameter gradient as ∇αF [f] =
[∇kF [f](x1),∇kF [f](x2), . . . ,∇kF [f](xn)]T (see 2.4), re-
ducing the complexity to that of ﬁnding ∇kF [f] ∈ Hk and
evaluating it n times. As was the case with the KCG al-
gorithm without trees, the tradeoff still balances out so that
the per iteration complexity is essentially equivalent between
KCG and PCG using tree-augmented function evaluation.

Noting [f(x1), . . . , f(xn)]T = Kα suggests that the
closed form quadratic line minimization for both the upper
and lower bounds of RLS under either KCG or PCG can eas-
ily be augmented as well by expanding the Hessians Au =
K T K + λK in the case of the upper bound and Al = K + λI
in the case of the lower bound. This was used in the tree-
augmented RLS experiments described below.

7 Experimental Results - Tree-Augmented

Algorithms

For these experiments, in addition to using the Soil dataset
described in section 4, we performed large scale RLS regres-
sions using a tree-augmented KCG on variable sized sub-
sets of a PugetSound elevation map4 using hyperparameters
λ = 0.1/n and σ2 = kN/(nπ), where n is the size of the
training set, and N is the size of the entire height map. In this
case, we chose N = 500 × 500 = 250, 000, and k = 15.
The largest resulting datasets were on the order of 100,000
points. It should be noted that the n¨aive implementation in
this case did not cache kernel evaluations in a kernel matrix
as such matrices for datasets above O(15, 000) points proved

4http://www.cc.gatech.edu/projects/large models/

IJCAI-07

1021

RLS

examples
PCG iters
KCG iters
Speedup

KLR

examples
PCG iters
KCG iters
Speedup

cmc
1000
1749
17

102.9
cmc
1000
1490
29
51.4

covtype
6000
318
12
26.5

covtype
4000
232
9

25.8

glass
150
30
5
6.0
glass
150
35
9
3.9

ionosphere

300
45
6
7.5

ionosphere

300
177
11
16.1

iris
120
72
11
6.5
iris
120
74
11
6.7

page-block

5000
120
10
12.0

page-block

-
-
-
-

pima
568
1825
17

107.4
pima
568
682
11
62.0

spam wine
128
4000
2106
24
5
40
52.6
4.8
spam wine
128
2000
11045
52
6
8.7

424.8

26

Figure 2: Times (in iterations) to achieve convergence of RLS (top) and KLR (bottom) on a subset of UCI data-sets. KCG
decreases the number of iterations on average by a factor of 54.

intractable for the machine on which the experiments were
performed.

Figure 1 shows that tree-augmentation signiﬁcantly outper-
forms the na¨ıve algorithm. Extrapolating from the plot on the
right, trees make possible accurate kernel learning on very
large datasets without requiring explicit subset selection tech-
niques.

8 Conclusions and Future Work
We have demonstrated that the novel gradient method, Ker-
nel Conjugate Gradient, can dramatically improve learning
speed for differentiable kernel machines. Furthermore, we
have shown how this can be understood as a very efﬁcient
preconditioning that naturally derives from the inner product
on functions deﬁned by the kernel. In practice, for various
differentiable kernel learning problems, we ﬁnd KCG con-
sistently, and signiﬁcantly, outperforms existing techniques.
We emphasize that the algorithm is simple to implement and
requires no more computation per iteration than standard ap-
proaches.

Further, we demonstrated that space-partitioning data-
structures, also developed by other authors [Shen et al.,
2006], for optimizing Gaussian Processes extend naturally to
other kernel methods. We ﬁnd that this approach meshes well
with the KCG algorithm, by signiﬁcantly speeding up the in-
ner loop computations of functions and inner products.

While conjugate gradient is a powerful optimization al-
gorithm, there are other approaches, like Limited-Memory
Quasi-Newton methods [Luenberger, 2003] that may also be
derived in similar ways in terms of the kernel inner product.
These algorithms have proved to be practical when using the
Euclidean inner product; we expect they would also gain the
beneﬁts of preconditioning that KCG enjoys.

Finally, very large scale kernel applications, it seems, will
invariably need to rely on sparse representation techniques
that do not use kernels at all of the data points. Nearly all
of these methods require the efﬁcient solution of large ker-
nel problems using an iterative approximation. It is natural
to explore how Kernel Conjugate Gradient can speed up the
expensive inner loop of these approximation procedures.

Acknowledgements
The authors gratefully acknowledge the partial support of this re-
search by the DARPA Learning for Locomotion contract.

References
Amari, S., & Nagaoka, H. (2000). Methods of information geometry.

Oxford University Press.

Aronszajn, N. (1950). Theory of reproducing kernels. Transactions

of the American Mathematical Society.

Ashby, S. F., Manteuffel, T. A., & Saylor, P. E. (1990). A taxon-
omy for conjugate gradient methods. SIAM Journal on Numerical
Analysis (pp. 1542–1568).

Gibbs, M. N. (1997). Bayesian gaussian processes for regression
and classiﬁcation. Doctoral dissertation, University of Cam-
bridge.

Gonzalez, J. P. (2005). Carnegie Mellon University, PhD. candidate.
Gray, A., & Moore, A. (2001). N-body problems in statistical learn-
ing. Advances in Neural Information Processing Systems. MIT
Press.

Hassani, S. (1998). Mathematical physics. Springer.
Kimeldorf, G. S., & Wahba, G. (1971). Some results on Tcheby-

chefﬁan spline functions. J. Math. Anal. Applic. (pp. 82–95).

Luenberger, D. G. (2003). Linear and nonlinear programming, sec-

ond edition. Springer.

Mason, L., J.Baxter, Bartlett, P., & Frean, M. (1999). Functional

gradient techniques for combining hypotheses. MIT Press.

Moore, A. (1990). Efﬁcient memory-based learning for robot con-

trol. Doctoral dissertation, University of Cambridge.

Rifkin, Yeo, & Poggio (2003). Regularized least squares classiﬁca-
tion. Advances in Learning Theory: Methods, Models and Appli-
cations. IOS Press.

Sch¨olkopf, B., Herbrich, R., & Smola, A. J. (2001). A general-
ized representer theorem. Lecture Notes in Computer Science
(pp. 416–426). Springer-Verlag.

Sch¨olkopf, B., & Smola, A. J. (2002). Learning with kernels: Sup-
port vector machines, regularization, optimization, and beyond.
MIT Press.

Shen, Y., Ng, A., & Seeger, M. (2006). Fast gaussian process re-
gression using kd-trees. In Y. Weiss, B. Sch¨olkopf and J. Platt
(Eds.), Advances in neural information processing systems 18,
1227–1234. Cambridge, MA: MIT Press.

Smola, A. J., & Sch¨olkopf, B. (2000). Sparse greedy matrix ap-
proximation for machine learning. International Conference on
Machine Learning.

Zhu, J., & Hastie, T. (2001). Kernel logistic regression and the im-
port vector machine. Advances in Neural Information Processing
Systems.

IJCAI-07

1022


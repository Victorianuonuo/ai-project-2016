Topological Value Iteration Algorithm for Markov Decision Processes

Peng Dai and Judy Goldsmith

Computer Science Dept.
University of Kentucky

773 Anderson Tower

Lexington, KY 40506-0046

Abstract

Value Iteration is an inefﬁcient algorithm for
Markov decision processes (MDPs) because it puts
the majority of its effort into backing up the en-
tire state space, which turns out to be unnecessary
in many cases.
In order to overcome this prob-
lem, many approaches have been proposed. Among
them, LAO*, LRTDP and HDP are state-of-the-
art ones. All of these use reachability analysis
and heuristics to avoid some unnecessary backups.
However, none of these approaches fully exploit the
graphical features of the MDPs or use these fea-
tures to yield the best backup sequence of the state
space. We introduce an algorithm named Topolog-
ical Value Iteration (TVI) that can circumvent the
problem of unnecessary backups by detecting the
structure of MDPs and backing up states based on
topological sequences. We prove that the backup
sequence TVI applies is optimal. Our experimen-
tal results show that TVI outperforms VI, LAO*,
LRTDP and HDP on our benchmark MDPs.

1 Introduction
State-space search is a very common problem in AI planning
and is similar to graph search. Given a set of states, a set of
actions, a start state and a set of goal states, the problem is
to ﬁnd a policy (a mapping from states to actions) that starts
from the start state and ﬁnally arrives at some goal state. De-
cision theoretic planning [Boutilier, Dean, & Hanks, 1999] is
an attractive extension of the classical AI planning paradigm,
because it allows one to model problems in which actions
have uncertain and cyclic effects. Uncertainty is embodied
in that one event can leads to different outcomes, and the oc-
currence of these outcomes are unpredictable, although they
are guided by some form of predeﬁned statistics. The sys-
tems are cyclic because an event might leave the state of the
system unchanged or return to a visited state.

Markov decision process (MDP) is a model for represent-
ing decision theoretic planning problems. Value iteration
and policy iteration [Howard, 1960] are two fundamental dy-
namic programming algorithms for solving MDPs. How-
ever, these two algorithms are sometimes inefﬁcient. They
spend too much time backing up states, often redundantly.

Recently several types of algorithms have been proposed
to efﬁciently solve MDPs. The ﬁrst type uses reachabil-
ity information and heuristic functions to omit some unnec-
essary backups, such as RTDP [Barto, Bradke, & Singh,
1995], LAO* [Hansen & Zilberstein, 2001], LRTDP [Bonet
& Geffner, 2003b] and HDP [Bonet & Geffner, 2003a]. The
second uses some approximation methods to simplify the
problems, such as [Guestrin et al., 2003; Poupart et al., 2002;
Patrascu et al., 2002]. The third aggregates groups of states of
an MDP by features, represents them as factored MDPs and
solves the factored MDPs. Often the factored MDPs are ex-
ponentially simpler, but the strategies to solve them are tricky.
SPUDD [Hoey et al., 1999], sLAO* [Feng & Hansen, 2002],
sRTDP [Feng, Hansen, & Zilberstein, 2003] are examples.
One can use prioritization to decrease the number of inefﬁ-
cient backups. Focused dynamic programming [Ferguson &
Stentz, 2004] and prioritized policy iteration [McMahan &
Gordon, 2005] are two recent examples.

We propose an improvement of the value iteration algo-
It combines the
rithm named Topological Value Iteration.
ﬁrst and last technique. This algorithm makes use of graph-
ical features of MDPs. It does backups in the best order and
only when necessary. In addition to its soundness and op-
timality, our algorithm is ﬂexible, because it is independent
of any assumptions on the start state and can ﬁnd the opti-
mal value functions for the entire state space. It can easily
be tuned to perform reachability analysis to avoid backups
of irrelevant states. Topological value iteration is itself not a
heuristic algorithm, but it can efﬁciently make use of extant
heuristic functions to initialize value functions.

2 Background
In this section, we go over the basics of Markov decision pro-
cesses and some of the extant solvers.

2.1 MDPs and Dynamic Programming Solvers
An MDP is a four-tuple (S, A, T, R). S is the set of states
that describe how a system is at a given time. We consider
the system developing over a sequence of discrete time slots,
or stages. In each time slot, only one event is allowed to take
effect. At any stage t, each state s has an associated set of
applicable actions At
s. The effect of applying any action is
to make the system change from the current state to the next
state at stage t+1. The transition function for each action, Ta:

IJCAI07

1860

(cid:2)H

S × S → [0, 1], speciﬁes the probability of changing to state
s(cid:2)
after applying a in state s. R : S → R is the instant reward
(in our formulation, we use C C, the instant cost, instead of
R). A value function V , V : S → R, gives the maximum
value of the total expected reward from being in a state s.
The horizon of a MDP is the total number of stages the sys-
tem evolves. In problems where the horizon is a ﬁnite number
i=0 C(si) in
H, our aim is to minimize the value V (s) =
H steps. For inﬁnite-horizon problems, the reward is accu-
mulated over an inﬁnitely long path. To deﬁne the values of
an inﬁnite-horizon problem, we introduce a discount factor
γ ∈ [0, 1] for each accumulated reward. In this case, our goal
is to minimize V (s) =

i=0 γ iC(si).

Given an MDP, we deﬁne a policy π : S → A to be a
mapping from states to actions. An optimal policy tells how
we choose actions at different states in order to maximize the
expected reward. Bellman [Bellman, 1957] showed that the
expected value of a policy π can be computed using the set
0 (s) is
of value functions V π. For ﬁnite-horizon MDPs, V π
deﬁned to be C(s), and we deﬁne V π
t+1 according to V π
t :

(cid:2)∞

(cid:3)

t+1(s) = C(s) +
V π

{Tπ(s)(s(cid:2)|s)V π

t (s(cid:2))}.

(1)

s(cid:2)∈S

For inﬁnite-horizon MDPs, the (optimal) value function is de-
ﬁned as:
V (s) = mina∈A(s)[C(s) + γ

Ta(s(cid:2)|s)V (s(cid:2))], γ ∈ [0, 1].

(cid:3)

s(cid:2)∈S

(2)
The above two equations are named Bellman equations.
Based on Bellman equations, we can use dynamic program-
ming techniques to compute the exact value of value func-
tions. An optimal policy is easily extracted by choosing an
action for each state that contributes its value function.

Value iteration is a dynamic programming algorithm that
solves MDPs. Its basic idea is to iteratively update the value
functions of every state until they converge. In each iteration,
the value function is updated according to Equation 2. We
call one such update a Bellman backup. The Bellman resid-
ual of a state s is deﬁned to be the difference between the
value functions of s in two consecutive iterations. The Bell-
man error is deﬁned to be the maximum Bellman residual of
the state space. When this Bellman error is less than some
threshold value, we conclude that the value functions have
converged sufﬁciently. Policy iteration [Howard, 1960] is an-
other approach to solve inﬁnite-horizon MDPs, consisting of
two interleaved steps: policy evaluation and policy improve-
ment. The algorithm stops when in some policy improvement
phase, no changes are made. Both algorithms suffer from ef-
ﬁciency problems. Although each iteration of each algorithm
is bound polynomially in the number of states, the number of
iterations is not [Puterman, 1994].

The main drawback of the two algorithm is that, in each it-
eration, the value functions of every state are updated, which
is highly unnecessary. Firstly, some states are backed up be-
fore their successor states, and often this type of backup is
fruitless. We will show an example in Section 3. Secondly,
different states converge with different rates. When only a
few states are not converged, we may only need to back up a
subset of the state space in the next iteration.

2.2 Other solvers

[Barto, Bradke, & Singh, 1995] proposed an
Barto et al.
online MDP solver named real time dynamic programming.
This algorithm assumes that initially the algorithm knows
nothing about the system except the information on the start
state and the goal states.
It simulates the evolution of the
system by a number of trials. Each trial starts from the start
state and ends at a goal state. In each step of the trial, one
greedy action is selected based on the current knowledge and
the state is changed stochastically. During the trial, all the
visited states are backed up once. The algorithm succeeds
when a certain number of trials are ﬁnished.

LAO* [Hansen & Zilberstein, 2001] is another solver that
uses heuristic functions. Its basic idea is to expand an ex-
plicit graph G(cid:2)
iteratively based on some type of best-ﬁrst
strategy. Heuristic functions are used to guide which state is
expanded next. Every time a new state is expanded, all its an-
cestor states are backed up iteratively, using value iteration.
LAO* is a heuristic algorithm which uses the mean ﬁrst pas-
sage heuristic. LAO* converges faster than RTDP since it
expands states instead of actions.

The advantage of RTDP is that it can ﬁnd a good sub-
optimal policy pretty fast, but the convergence for RTDP is
slow. Bonet and Geffner extended RTDP to labeled RTDP
(LRTDP) [Bonet & Geffner, 2003b], and the convergence of
LRTDP is much faster. In their approach, they mark a state
s as solved if the Bellman residuals of s and all the states
that are reachable through the optimal policy from s are small
enough. Once a state is solved, we regard its value function as
converged, so it is treated as a “tip state” in the graph. LRTDP
converges when the start state is solved.

HDP is another state-of-the-art algorithm, by Bonet and
Geffner [Bonet & Geffner, 2003a]. HDP not only uses a
similar labeling technique to LRTDP, but also discovers the
connected components in the solution graph of a MDP. HDP
labels a component as solved when all the states in that com-
ponent have been labeled. HDP expands and updates states
in a depth-ﬁrst fashion rooted at the start states. All the
states belonging to the solved components are regarded as tip
states. Their experiments show that HDP dominated LAO*
and LRTDP on most of the racetrack MDP benchmarks when
the heuristic function hmin [Bonet & Geffner, 2003b] is used.
The above algorithms all make use of start state informa-
tion by constraining the number of backups. The states that
are unreachable from the start state are never backed up. They
also make use of heuristic functions to guide the search to the
promising branches.

3 A limitation of current solvers

None the algorithms listed above make use of inherent fea-
tures of MDPs. They do not study the sequence of state back-
ups according to an MDP’s graphical structure, which is the
intrinsic property of an MDP and potentially decides the com-
plexity of solving it [Littman, Dean, & Kaelbling, 1995]. For
example, Figure 1 shows a simpliﬁed version of an MDP. For
simplicity, we omit explicit action nodes, transition probabil-
ities, and reward functions. The goal state is marked in the
ﬁgure. A directed edge between two states means the second

IJCAI07

1861

state is a potential successor state when applying some action
in the ﬁrst state.

s1

s2

s3

s4

Goal

Figure 1: A simpliﬁed MDP

Observing the MDP in Figure 1, we know the best se-
quence to back up states is s4, s3, s2, s1, and if we apply
this sequence, all the states except s1 and s2 only require
one backup. However, not enough efforts of the algorithms
mentioned above have been put to detect this optimal backup
sequence. At the moment when they start on this MDP, all
of them look at solving it as a common graph search problem
with 5 vertices and apply essentially the same strategies as
solving an MDP whose graphical structure is equivalent to a
5-clique, although this MDP is much simpler to solve than a
5-clique MDP. So the basic strategies of those solvers do not
have an “intelligent” subroutine to distinguish various MDPs
and to use different strategies to solve them. With this intu-
ition, we want to design an algorithm that is able to discover
the intrinsic complexity of various MDPs by studying their
graphical structure and to use different backup strategies for
MDPs with different graphical properties.

4 Topological Value Iteration
Our ﬁrst observation is that states and their value functions
are causally related. If in an MDP M , one state s(cid:2)
is a succes-
sor state of s after applying action a, then V (s) is dependent
on V (s(cid:2)). For this reason, we want to back up s(cid:2)
ahead of s.
The causal relation is transitive. However, MDPs are cyclic
and causal relations are very common among states. How
do we ﬁnd an optimal backup sequence for states? Our idea
is the following: We group states that are mutually causally
related together and make them a metastate, and let these
metastates form a new MDP M (cid:2)
is no longer
cyclic. In this case, we can back up states in M (cid:2)
in their re-
verse topological order. In other words, we can back up these
big states in only one virtual iteration. How do we back up
the big states that are originally sets of states? We can apply
any strategy, such as value iteration, policy iteration, linear
programming, and so on. How do we ﬁnd those mutually
causally related states?

. Then M (cid:2)

To answer the above question, let us look at the graphical
structure of an MDP ﬁrst. An MDP M can be regarded as
a directed graph G(V, E). The set V has state nodes, where
each node represents a state in the system, and action nodes,
where each action in the MDP is mapped to a vertex in G.
The edges, E, in G represent transitions, so they indicate the
causal relations in M . If there is an edge e from state node
s to node a, this means a is a candidate action for state s.
Conversely, an edge e pointing from a to s(cid:2)
means, applying
action a, the system has a positive probability of changing to
state s(cid:2)
in G, we know
that state s is causally dependent on s(cid:2)
. So if we simplify

. If we can ﬁnd a path s → a → s(cid:2)

into directed edges from s to s(cid:2)

G by removing all the action nodes, and changing paths like
s → a → s(cid:2)
, we get a causal
relation graph Gcr of the original MDP M . A path from state
s1 to s2 in Gcr means s1 is causally dependent on s2. So the
problem of ﬁnding mutually causally related groups of states
is reduced to the problem of ﬁnding the strongly connected
components in Gcr.

We use Kosaraju’s [Cormen et al., 2001] algorithm of de-
tecting the topological order of strongly connected compo-
nents in a directed graph. Note that Bonet and Geffner [Bonet
& Geffner, 2003a] used Tarjan’s algorithm in detection of
strongly connected components in a directed graph in their
solver, but they do not use the topological order of these
components to systematically back up each component of an
MDP. Kosaraju’s algorithm is simple to implement and its
time complexity is only linear in the number of states, so
when the state space is large, the overhead in ordering the
state backup sequence is acceptable. Our experimental re-
sults also demonstrate that the overhead is well compensated
by the computational gain.

The pseudocode of TVI is shown in Figure 2. We ﬁrst
use Kosaraju’s algorithm to ﬁnd the set of strongly connected
components C in graph Gcr, and their topological order. Note
that each c ∈ C maps to a set of states in M . We then use
value iteration to solve each c. Since there are no cycles in
those components, we only need to solve them once. Notice
that, when the entire state space is causally related, TVI is
equivalent to VI.

Theorem 1 Topological Value Iteration is guaranteed to
converge to the optimal value function.

Proof We ﬁrst prove TVI is guaranteed to terminate in ﬁnite
time. Since each MDP contains a ﬁnite number of states, it
contains a ﬁnite number of connected components. In solving
each of these components, TVI uses value iteration. Because
value iteration is guaranteed to converge in ﬁnite time, TVI,
which is actually a ﬁnite number of value iterations, termi-
nates in ﬁnite time. We then prove TVI is guaranteed to con-
verge to the optimal value function. According to the update
sequence of TVI, at any point of the algorithm, the value func-
tions of the states (of one component) that are being backed
up only depend on the value functions of the components that
have been backed up, but not on those of the components that
have not been backed up. For this reason, TVI lets the value
functions of the state space converge sequentially. When a
component is converged, the value functions of the states can
be safely used as tip states, since they can never be inﬂuenced
by components backed up later.

A straightforward corollary to the above theorem is:

Corollary 2 Topological Value Iteration only updates the
value functions of a component when it is necessary. And
the update sequence of the update is optimal.

4.1 Optimization

In our implementation, we added two optimizations to our al-
gorithm. One is reachability analysis. TVI does not assume
any initial state information. However, given that informa-
tion, TVI is able to detect the unreachable components and

IJCAI07

1862

(cid:2)

Ta(s(cid:2)|s)V (s(cid:2))]

s(cid:2)∈S

cr of Gcr

Topological Value Iteration

for each state s ∈ S
V (s) = mina∈A(s)[C(s) + γ
if (Bellman error is less than δ)
return

TVI(MDP M , δ)
1 . scc(M )
2 . for i ← 1 to cpntnum;
3 . S ← the set of states s where s.id = cpntnum
4 . vi(S, δ)
vi(S: a set of states, δ)
5 . while (true)
6 .
7 .
8 .
9 .
scc(MDP M ) (Kosaraju’s algorithm)
10. construct Gcr from M by removing action nodes
11. construct the reverse graph G(cid:2)
12. size ← number of states in Gcr
13. for s ← 1 to size
14. s.id ← −1
15. // postR and postI are two arrays of length size
16. cnt ← 1, cpntnum ← 1
17. for s ← 1 to size
18.
19.
20. for s ← 1 to size
21. postR[s] ← postI[s]
22. cnt ← 1, cpntnum ← 1
23. for s ← 1 to size
24. s.id ← −1
25. for s ← 1 to size
26.
27.
28.
29. return (cpntnum, Gcr)
dfs(Graph G, s)
30. s.id ← cpntnum
31. for each successor s(cid:2)
32.
33.
34. postI[cnt] ← s
35. cnt ← cnt + 1

if (s.id = −1)
df s(Gcr, postR[s])
cpntnum ← cpntnum + 1

if (s(cid:2).id = −1)
df s(G, s(cid:2))

if (s.id = −1)
df s(G(cid:2), s)

5 Experiment

We tested the topological value iteration and compared its
running time against value iteration (VI), LAO*, LRTDP and
HDP. All the algorithms are coded in C and properly opti-
mized, and run on the same Intel Pentium 4 1.50GHz proces-
sor with 1G main memory and a cache size of 256kB. The
operating system is Linux version 2.6.15 and the compiler is
gcc version 3.3.4.

5.1 Domains

We use two MDP domains for our experiments. The ﬁrst
domain is a model simulating PhD qualifying exams. We
consider the following scenario from a ﬁctional department:
To be qualiﬁed for a PhD in Computer Science, one has to
pass exams in each CS area. Every two months, the depart-
ment offers exams in each area. Each student takes each
exam as often as he wants until he passes it. Each time,
he can take at most two exams. We consider two types of
grading criteria. For the ﬁrst criterion, we only have pass
and fail (and of course, untaken) for each exam. Students
who have not taken and who have failed certain exam be-
fore have the same chance of passing that exam. The sec-
ond criterion is a little trickier. We assign pass, conditional
pass, and fail to each exam, and the probabilities of passing
certain exams vary, depending on the student’s past grade on
that exam. A state in this domain is a value assignment of the
grades of all the exams. For example, if there are ﬁve exams,
fail,pass,pass,condpass,untaken is one state. We refer to the
ﬁrst criterion MDPs as QEs(e) and second as QEt(e), where
e refers to the number of exams.

For the second domain, we use artiﬁcially-generated “lay-
ered” MDPs. For each MDP, we deﬁne the number of states,
and partition them evenly into a number nl of layers. We
number these layers by numerical values. We allow states in
higher numbered layers to be the successor states of states in
lower numbered layers, but not vice versa, so each state has
only a limited set of allowable successor states succ(s). The
other parameters of these MDPs are: the maximum number
of actions each state can have is ma, the maximum number of
successor states of each action, ms. Given a state s, we let the
pseudorandom number generator of C pick the number of ac-
tions from [1, ma], and for each action, we let that action have
a number of successor states in [1, ms]. The states are chosen
uniformly from succ(s) together with normalized transition
probabilities. The advantage of generating MDPs this way is
that these layered MDPs contain at least nl connected com-
ponents.

There are actual applications that lead to multi-layered
MDPs. A simple example is the game Bejeweled: each level
is at least one layer. Or consider a chess variant without
pawns, played against a stochastic opponent. Each set of
pieces that could appear on the board together leads to (at
least one strongly-connected component. There are other,
more serious examples, but we know of no multi-layered
standard MDP benchmarks. Examples such as race-track
MDPs tend to have a single scc, rendering TVI no better than
VI. (Since checking the topological structure of an MDP takes
negligible time compared to running any of the solvers, it is

of s

Figure 2: Pseudocode of Topological Value Iteration

ignore them in the dynamic programming step. Reachability
is computed by a depth ﬁrst search. The overhead of this anal-
ysis is linear, and it helps us avoid considering the unreach-
able components, so the gains can well compensate for the
trouble introduced. It is extremely useful when only a small
portion of the state space is reachable. Since the reachabil-
ity analysis is straightforward, we do not provide any pseu-
docode for it. The other optimization is the use of heuristic
functions. Heuristic values can serve as a good starting point
for value functions in TVI. In our program, we use the hmin
heuristic from [Bonet & Geffner, 2003b]. Reachability anal-
ysis and the use of heuristics help strengthen the competitive-
ness of TVI. hmin replaces the expected future reward part of
the Bellman equation by the minimum of such value. It is an
admissible heuristic.

hmin(s) = mina[C(s) + γ · mins(cid:2):Ta(s(cid:2)|s)>0V (s(cid:2))].

(3)

IJCAI07

1863

domain
|S|
|a|
#of scc(cid:2)s
v∗(s0)
hmin
VI(h = 0)
LAO*(h = 0)
LRTDP(h = 0)
HDP(h = 0)
TVI(h = 0)
VI(hmin)
LAO*(hmin)
LRTDP(hmin)
HDP(hmin)
TVI(hmin)

QEs(7)
2187
28
2187
11.129919
4.0
1.08
0.73
0.44
5.44
0.42
1.05
0.53
0.28
4.42
0.16

QEs(8)
6561
36
6561
12.640260
4.0
4.28
4.83
1.91
75.13
1.36
4.38
3.75
1.22
59.71
0.56

QEs(9)
19683
45
19683
14.098950
5.0
15.82
26.72
7.73
1095.11
4.50
15.76
19.16
4.90
768.59
1.86

QEs(10)
59049
55
59049
15.596161
5.0
61.42
189.15
32.65
1648.11
15.89
61.06
126.84
20.15
1583.77
6.49

QEt(5)
1024
15
243
7.626064
3.0
0.31
0.27
0.28
0.75
0.20
0.31
0.25
0.28
0.95
0.19

QEt(6)
4096
21
729
9.094187
4.0
1.89
2.18
2.05
29.37
1.04
1.87
1.94
1.95
30.14
0.98

QEt(7)
16384
28
2187
10.565908
4.0
10.44
16.57
16.68
1654.00
5.49
10.41
14.96
16.22
1842.62
5.29

QEt(8)
65536
36
6561
12.036075
5.0
59.76
181.44
126.75
2130.87
35.10
59.73
123.26
124.69
2915.05
30.79

Table 1: Problem statistics and convergence time in CPU seconds for different algorithms with different heuristics for the qual.
exams examples ( = 10−6

)

easy to decide whether to use TVI.) Thus, we use our artiﬁ-
cially generated MDPs for now.

5.2 Results
We consider several variants of our ﬁrst domain, and the re-
sults are shown in Table 1. The statistics have shown that:

• TVI outperforms the rest of the algorithms in all the in-
stances. Generally, this fast convergence is due to both
the appropriate update sequence of the state space and
avoidance of unnecessary updates.

• The hmin helps TVI more than it helps VI, LAO* and

LRTDP, especially in the QEs domains.

• TVI outperforms HDP, because our way of dealing with
components is different. HDP updates states of all the
unsolved components together in a depth-ﬁrst fashion
until they all converge. We pick the optimal sequence of
backing up components, and only back up one of them
at a time. Our algorithm does not spend time checking
whether all the components are solved, and we only up-
date a component when it is necessary.

We notice that HDP shows pretty slow convergence in the QE
domain. That is not due to our implementation. HDP is not
suitable for solving problems with large numbers of actions.
Readers interested in the performance of HDP on MDPs with
smaller action sets can refer to [Bonet & Geffner, 2003a].

The statistics of the performance on artiﬁcially generated
layered MDPs are shown in Table 2 and 3. We do not include
the HDP statistics here, since HDP is too slow in these cases.
We also ignore the results on applying the hmin heuristic,
since they display the same scale as not using the heuristic.
For each element of the table, we take the average of running
20 instances of MDPs with the same conﬁguration. Note that
varying |S|, nl, ma, and ms yields many MDP conﬁgura-
tions. We present a few whose results are representative.

For the ﬁrst group of data, we ﬁx the state space to have
size 20,000 and change the number of layers. Statistics in Ta-
ble 2 show our TVI dominates others. We note that, as the

layer number increases, the MDPs become more complex,
since the states in large numbered layers have relatively small
succ(s) against ms, therefore cycles in those layers are more
common, so it takes greater effort to solve large numbered
layers than small numbered ones. Not surprisingly, from Ta-
ble 2 we see that when the number of layers increases, the
running time of each algorithm also increases. However, the
increase rate of TVI is the smallest (the rate of greatest against
smallest running time of TVI is 2 versus 4 of VI, 3.5 of LAO*,
and 2.3 of LRTDP). This is due to the fact that TVI applies the
best update sequence. As the layer number becomes large, al-
though the update of the large numbered layers requires more
effort, the time TVI spends on the small numbered ones re-
mains stable. But other algorithms do not have this property.
For the second experiment, we ﬁx the number of layers and
vary the state space size. Again, TVI is better than other al-
gorithms, as seen in Table 3. When the state space is 80,000,
TVI can solve the problems in around 12 seconds. This shows
that TVI can solve large problems in a reasonable amount of
time. Note that the statistics we include here represent the
common cases, but were not chosen in favor of TVI. Our best
result shows, TVI runs in around 9 seconds for MDPs with
|S|=20,000, nl=200, ma=20, ms=40, while VI needs more
than 100 seconds, LAO* takes 61 seconds and LRTDP re-
quires 64 seconds.

6 Conclusion

We have introduced and analyzed an MDP solver, topolog-
ical value iteration, that studies the dependence relation of
the value functions of the state space and use the dependence
relation to decide the sequence to back up states. The algo-
rithm is based on the idea that different MDPs have different
graphical structures, and the graphical structure of an MDP
intrinsically determines the complexity of solving that MDP.
We notice that no current solvers detect this information and
use it to guide state backups. Thus, they solve MDPs of the
same problem sizes but with different graphical structure with

IJCAI07

1864

nl
VI(h = 0)
LAO*(h = 0)
LRTDP(h = 0)
TVI(h = 0)

20
19.482
6.088
9.588
2.563

40
17.787
6.584
9.651
2.686

60
12.310
5.702
8.483
2.587

80
24.538
7.297
8.352
2.624

100
31.684
9.250
11.165
2.805

200
40.782
12.801
11.108
3.186

300
46.554
12.463
13.125
3.061

400
52.248
12.259
13.443
4.065

500
64.706
16.255
14.160
4.264

600
77.658
21.991
22.057
5.131

Table 2: Problem statistics and convergence time in CPU seconds for different algorithms on solving layered MDPs with
different number of layers (|s|=20000, ma=10, ms=20,  = 10−6

)

|S|
VI(h = 0)
LAO*(h = 0)
LRTDP(h = 0)
TVI(h = 0)

10000
9.322
3.056
3.903
1.212

20000
27.683
6.201
8.814
2.589

30000
38.529
10.561
10.725
4.055

40000
46.178
13.663
20.708
5.571

50000
64.915
21.409
27.156
7.133

60000
72.087
23.461
53.773
8.626

70000
95.479
29.248
49.954
10.307

80000
117.359
38.746
50.730
11.969

Table 3: Problem statistics and convergence time in CPU seconds for different algorithms on solving layered MDPs with
different state spaces (nl=20, ma=10, ms=20,  = 10−6

)

almost the same strategies. In this sense, they are not “intel-
ligent”. Topological value iteration is proposed to solve this
problem. It is guaranteed to ﬁnd the optimal solution of a
Markov decision process sequentially. Topological value it-
eration is a ﬂexible algorithm, which can use the initial state
information and apply reachability analysis. However, even
without this information, TVI runs soundly and completely.

We coded up an academic example and our experimen-
tal results show that TVI outperforms not only VI, but also
LAO*, LRTDP and HDP, state of the art algorithms. The rea-
son is because our algorithm is able to detect the optimal se-
quence of updating each component and components are only
updated when necessary. Without standard, layered bench-
marks, we test our algorithm on artiﬁcially generated layered
MDPs. Our results on these MDPs have shown that TVI is
extremely useful in MDPs with many connected components.
The complexity increase of TVI is not as great as other algo-
rithms as the number of layers increase, which shows that
TVI is very suitable for solving MDPs with layered struc-
tures.

References

[Barto, Bradke, & Singh, 1995] Barto, A.; Bradke, S.; and Singh,
S. 1995. Learning to act using real-time dynamic programming.
JAI 72:81–138.

[Bellman, 1957] Bellman, R.

1957. Dynamic Programming.

Princeton, NJ: Princeton University Press.

[Bonet & Geffner, 2003a] Bonet, B., and Geffner, H. 2003a. Faster
heuristic search algorithms for planning with uncertainty and full
feedback.
In Gottlob, G., ed., Proc. of IJCAI-03, 1233–1238.
Acapulco, Mexico: Morgan Kaufmann.

[Bonet & Geffner, 2003b] Bonet, B., and Geffner, H. 2003b. La-
beled RTDP: Improving the convergence of real-time dynamic
programming. In Giunchiglia, E.; Muscettola, N.; and Nau, D.,
eds., Proc. 13th ICAPS, 12–21. Trento, Italy: AAAI Press.

[Cormen et al., 2001] T. H. Cormen, C. E. Leiserson, R. L. Rivest,
and C. Stein. Introduction to Algorithms, Second Edition. The
MIT Press, 2001.

[Feng & Hansen, 2002] Feng, Z., and Hansen, E. A. 2002. Sym-
bolic heuristic search for factored Markov decision processes. In
Proc. of AAAI-05.

[Feng, Hansen, & Zilberstein, 2003] Feng, Z.; Hansen, E. A.; and
Zilberstein, S. 2003. Symbolic generalization for on-line plan-
ning. In Proc. of UAI-03, 209–216.

[Ferguson & Stentz, 2004] Ferguson, D., and Stentz, A. 2004. Fo-
cussed dynamic programming: Extensive comparative results.
Technical Report CMU-RI-TR-04-13, Carnegie Mellon Univer-
sity, Pittsburgh, PA.

[Guestrin et al., 2003] Guestrin, C.; Koller, D.; Parr, R.; and
Venkataraman, S. 2003. Efﬁcient solution algorithms for fac-
tored MDPs. JAIR 19:399–468.

[Hansen & Zilberstein, 2001] Hansen, E., and Zilberstein, S. 2001.
LAO*: A heuristic search algorithm that ﬁnds solutions with
loops. JAI 129:35–62.

[Hoey et al., 1999] Hoey, J.; St-Aubin, R.; Hu, A.; and Boutilier, C.
1999. SPUDD: Stochastic planning using decision diagrams. In
Proc. of UAI-99, 279–288.

[Howard, 1960] Howard, R. 1960. Dynamic Programming and

Markov Processes. Cambridge, Massachusetts: MIT Press.

[Littman, Dean, & Kaelbling, 1995] Littman, M. L.; Dean, T.; and
Kaelbling, L. P. 1995. On the complexity of solving Markov
decision problems. In Proc. of UAI-95, 394–402.

[McMahan & Gordon, 2005] McMahan, H. B., and Gordon, G. J.
2005. Fast exact planning in Markov decision processes. In Proc.
of ICAPS-05.

[Patrascu et al., 2002] Patrascu, R.; Poupart, P.; Schuurmans, D.;
Boutilier, C.; and Guestrin, C. 2002. Greedy linear value-
approximation for factored Markov decision processes. In Proc.
of AAAI-02, 285–291.

[Poupart et al., 2002] Poupart, P.; Boutilier, C.; Patrascu, R.; and
Schuurmans, D. 2002. Piecewise linear value function approxi-
mation for factored MDPs. In Proc. of AAAI-02, 292–299.

[Boutilier, Dean, & Hanks, 1999] Boutilier, C.; Dean, T.; and
Hanks, S. 1999. Decision-theoretic planning: Structural assump-
tions and computational leverage. JAIR 11:1–94.

[Puterman, 1994] Puterman, M. 1994. Markov Decision Processes:
John Wiley, New

Discrete Stochastic Dynamic Programming.
York.

IJCAI07

1865


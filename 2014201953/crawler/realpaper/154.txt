Theory of Alignment Generators

and

Applications to Statistical Machine Translation

Raghavendra Udupa U. Hemanta K. Maji

IBM India Research Laboratory, New Delhi

{uraghave, hemantkm}@in.ibm.com

Abstract

Viterbi Alignment and Decoding are two fundamen-
tal search problems in Statistical Machine Trans-
lation. Both the problems are known to be NP-
hard and therefore, it is unlikely that there exists
an optimal polynomial time algorithm for either of
these search problems.
In this paper we charac-
terize exponentially large subspaces in the solution
space of Viterbi Alignment and Decoding. Each
of these subspaces admits polynomial time opti-
mal search algorithms. We propose a local search
heuristic using a neighbourhood relation on these
subspaces. Experimental results show that our al-
gorithms produce better solutions taking substan-
tially less time than the previously known algo-
rithms for these problems.

Introduction

1
Statistical Machine Translation (SMT) is a data driven ap-
proach to Machine Translation [Brown et al., 1993], [Al-
Onaizan et al., 1999], [Berger et al., 1996]. Two of the fun-
damental problems in SMT are [Brown et al., 1993]:

a∗ = argmax
(e∗, a∗) = argmax

a

e,a

P r (f , a|e)
P r (f , a|e) P r (e)

(ViterbiAlignment)

(Decoding)

Viterbi Alignment has a lot of applications in Natural Lan-
guage Processing [Wang and Waibel, 1998], [Marcu and
Wong, 2002]. While there exist simple polynomial time al-
gorithms for computing the Viterbi Alignment for IBM Mod-
els 1-2, only heuristics are known for Models 3-5. Recently,
[Udupa and Maji, 2005a] showed that the computation of
Viterbi Alignment is NP-Hard for IBM Models 3-5.
Decoding algorithm is an essential component of all SMT
systems [Wang and Waibel, 1997], [Tillman and Ney, 2000],
[Och et al., 2001], [Germann et al., 2003], [Udupa et al.,
2004]. The problem is known to be NP-Hard for IBM Mod-
els 1-5 when the language model is a bigram model [Knight,
1999].
Unless P = NP, it is unlikely that there exist polynomial
time algorithms for either Viterbi Alignment or Decoding.
Therefore, there is a lot of interest in ﬁnding fast heuristics
to ﬁnd acceptable solutions for the problems.

Previous approaches to the Viterbi Alignment problem have
focussed on deﬁning a graph over all possible alignments
where the connectivity is based on local modiﬁcations. The
best known algorithm for Decoding (both in terms of speed
and translation quality) employs a greedy search algorithm.
However, these approaches can look at only polynomial num-
ber of alignments in polynomial time.
In this paper, we characterize exponentially large subspaces
of the solution space of Viterbi Alignment and Decoding. We
propose polynomial time optimal dynamic programming al-
gorithms to solve Viterbi Alignment and Decoding when the
search space is restricted to a particular subspace. There are
exponentially many such subspaces. So, we perform a local
search on these subspaces under a suitable neighbourhood re-
lation. Though, there is no polynomial bound on the number
of iterations of the local search, experiments show that our
algorithms procduce better solutions in signiﬁcantly less time
than the current algorithms.
We introduce the notion of an alignment generator in Sec-
tion 2. There are m! distinct alignment generators (where m
is the length of the source language sentence). Each align-
ment generator (gi, where 1 ≤ i ≤ m!) is associated with an
exponentially large subspace of alignments (Ai). The solu-
tion space of Viterbi Alignment and Decoding can be written
as the union of these subspaces. We next consider the graph
induced on the set of Ais by a neighbourhood relation and
perform local search on this graph. The explicit mathematical
deﬁnition of Ai and the neighbourhood relation are presented
in Section 3. We present polynomial time optimal algorithms
for Viterbi Alignment and Decoding over a particular Ai (Sec-
tion 4 and Section 5 respectively). Experiments show that our
search algorithms produce better solutions taking subtantially
less time than current algorithms (Section 6).

2 Preliminaries
IBM Models 1-5 assume that there is a hidden alignment be-
tween the source and target sentences.
Deﬁnition 1 (Alignment) An alignment
a(m, l) : {1, . . . , m} → {0, 1, . . . , l}.
{1, . . . , m} is the set of source positions and {0, 1, . . . , l} is
the set of target positions. The target position 0 is known
as the null position. We denote a(m, l)(j) by a(m, l)
. The
fertility of the target position i in alignment a(m, l) is φi =

is a function

j

Pm

(cid:16)

(cid:17)

, i

.

jk

j

j

j1

. . .

jm0

(cid:17)

(cid:17)

(cid:16)

(cid:17)

a(m, l)
j

j=1 δ

j, a(m, l)

jm0 , a(m, l)

where the a(m, l)

such that j > v or a(m, l)

If φi = 0, then the target position i
is an infertile position, otherwise it is a fertile position. Let
{k, . . . , k + γ − 1} ⊆ {1, . . . , l} be the longest range of con-
secutive infertile positions, where γ ≥ 0. We say that align-
ment a(m, l) has an infertility width of γ.
Representation of Alignments: Since all source positions
that are not connected to any of the target positions 1, . . . , l
are assumed to be connected to the null position (0), we can
represent an alignment, without loss of generality, as a set
of tuples {(j, aj)|aj 6= 0}. Let m0 = m − φ0. Thus, a
sequence representation of an alignment has m0 tuples. By
sorting the tuples in ascending order using the second ﬁeld
as the key, a(m, l) can now be represented as a sequence
j1, a(m, l)
s are non-
decreasing with increasing k. Note that there are (l + 1)m

(cid:16)
possible alignments for a given m, l > 0. Let a(m, l)(cid:12)(cid:12)(v,u)
(cid:16)

be the alignment {1, . . . , v} → {0, . . . , u} where all the tu-
ples
> u have been
removed.
Deﬁnition 2 (Generator) A generator is a bijection g(m) :
{1, . . . , m} → {1, . . . , m}.
Representation of Generators: A generator g(m) can be
represented as a sequence of tuples (j1, 1) . . . (jm, m). Note
that there are m! different possible generators for a given
m > 0. The identity function g(m)(j) = j is denoted by
g(m)
0
Deﬁnition 3 (g(m) −→ a(m, l)) An alignment a(m, l) (with
tuple sequence
) is said to
be generated by a generator g(m) (with tuple sequence
(k1, 1) . . . (km, m))
is a subsequence of
k1 . . . km.
Note that the above deﬁnition does not take into account the
infertility width of the alignment. Hence, we reﬁne the above
deﬁnition.
γ−→ a(m, l)) An alignment a(m, l) is said
Deﬁnition 4 (g(m)
to be γ-generated by g(m), if g(m) −→ a(m, l) and the infer-
tility width of a(m, l) is at most γ.
Deﬁnition 5 (Ag(m)
γ, l )

jm0 , a(m, l)

j1, a(m, l)

j1 . . . jm0

(cid:16)

(cid:17)

(cid:16)

(cid:17)

j1

if

jm0

. . .

.

n

Ag(m)
γ, l =

a(m, l) | g(m)

γ−→ a(m, l)o

.

γ, l

γ,? = ∪l Ag(m)

Thus, for every generator g(m) and integer γ ≥ 0 there is
an associated family of alignments Ag(m)
for a given l ∈ N.
Further, Ag(m)
Swap Operation:
and
(j1, 1) . . . (jk, k) . . . (jk0, k0) . . . (jm, m) be its tuple se-
quence. The result of a SWAP operation on the kth and
k0th tuples in the sequence is another
tuple sequence
(j1, 1) . . . (jk0 , k) . . . (jk, k0) . . . (jm, m).
The new tuple
sequence deﬁnes a generator g0(m). The relationship between
any two generators is given by:

γ, l
Let g(m) be

a generator

Lemma 1 If g(m) and g0(m) are two generators then g0(m)
can be obtained from g(m) by a series of swap operations.

Proof: Follows from the properties of permutations.

We can apply a sequence of Swap operators to modify
any given generator g(m) to g(m)
. The source position
rv (1 ≤ v ≤ m) in g(m) corresponds to the source position v
in g(m)
. For brevity, we denote a sequence vi vi+1 . . . vj by
vj
i. The source sentence is represented by f m
1 and the target
sentence is represented by el
1.

0

0

3 Framework

l, l

We now describe the common framework for solving Viterbi
Alignment and Decoding. The solution space for Viterbi
Alignment can be written as ∪g(m) Ag(m)
, and the solution
space of Decoding can be approximated by ∪g(m) Ag(m)
for
a large enough γ. The key idea is to ﬁnd the optimal solution
in an exponentially large subspace A corresponding to a gen-
erator (A = Ag(m)
for Viterbi Alignment and
Decoding respectively). Given an optimal search algorithm
for A, we use Lemma 1 to augment the search space. Our
framework is as follows

and A = Ag(m)

γ,?

γ,?

l, l

1. Choose any generator g(m) as the initial generator.
2. Find the best solution for the problem in A.
3. Pick a better generator g0(m) via a swap operation on

g(m).

4. Repeat steps 2 and 3 until the solution cannot be im-

proved further.

3.1 Algorithms

algorithm for ﬁnding a good approximate

Viterbi Alignment
so-
The
lution
subroutine
Viterbi For Generator to ﬁnd the best solution in
the exponentially large subspace deﬁned by Ag(m)
. The
implementation of this subroutine is discussed in Section 4.

to Viterbi Alignment

employs

γ, l

a

.

0

Algorithm 1 Viterbi Alignment
1: g(m) ← g(m)
2: while (true) do
3:
4:

a∗(m, l) = Viterbi For Generator(cid:0)g(m), f m
if(cid:0)a0(m, l) = NIL(cid:1) then

Find an alignment a0(m, l) with higher score by using
swap operations on a∗(m, l).

1 , el
1

(cid:1).

break.

5:
6:
7:
8:
9: end while
10: Output a∗(m, l)

end if
g(m) ← A generator of a0(m, l).

algorithm for Decoding employs

Decoding
Our
subroutine
a
Decode For Generator to ﬁnd the best solution in the
exponentially large subspace deﬁned by Ag(m)
γ,? . The im-
plementation of this subroutine is discussed in Section 5.

.

0

Algorithm 2 Decoding
1: g(m) ← g(m)
2: while (true) do
3:
4:

1 , a∗(m, l)(cid:11) = Decode For Generator(cid:0)g(m), f m
(cid:10)e∗l
if(cid:0)a0(m, l) = NIL(cid:1) then

Find an alignment a0(m, l) with higher score by using
swap operations on a∗(m, l) and e∗l
1 .

1

(cid:1).

break.

5:
6:
7:
8:
9: end while
10: Output e∗l
1 .

end if
g(m) ← A generator of a0(m, l).

3.2 Theory of Generators
In this section, we prove some important results on genera-
tors.

Structure Transformation Operations
We deﬁne a set of structure transformation operations on
generators. The application of each of these operations on
a generator modiﬁes the structure of a generator and pro-
duces an alignment. Without loss of generality 1, we assume
that the generator is the identity generator g(m)
and deﬁne
the structural transformation operations on it. The tuple se-
quence for g(m)
is (1, 1) . . . (m, m). Given an alignment
0
(j−1)
a(j−1,i) ∈ Ag
with φi > 0, we explain the modiﬁcations
0
γ,i
introduced by each of these operators to the jth tuple.
1. SHRINK: Extend a(j−1,i) ∈ Ag

to a(j,i) ∈ Ag

0

(j−1)
0
γ,i

(j)
0
γ,i

such that a(j,i)

j = 0.

2. MERGE: Extend a(j−1,i) ∈ Ag

(j−1)
0
γ,i

to a(j,i) ∈ Ag

(j)
0
γ,i

such that a(j,i)

j = i.

3. (γ, k)-GROW (0 ≤ k ≤ γ): Extend a(j−1,i) ∈ Ag

to a(j,i+k+1) ∈ Ag
1.

(j)
0

γ,i+k+1 such that a(j,i+k+1)

j

(j−1)
0
γ,i

= i+k+

By removing the infertile positions at the end, we obtain the
following result:
Lemma 2 a(m, l) ∈ Ag

tion iff there exists s such that 0 ≤ s ≤ γ, a(m, l)(cid:12)(cid:12)(m, l−s) ∈

γ, l and has at least one fertile posi-

(m)
0

(m)
0

Ag
γ, l−s and φl−s > 0.

1by permuting the given generator g(m) to the identity generator

g(m)
0

Theorem 1 If a(m,l−s) ∈ Ag(m)
it can be obtained from g(m)
and (γ, k)-GROW operations.

0

γ, l−s such that φl−s > 0 then
by a series of SHRINK, MERGE

0

0

0

Proof: WLOG we assume that the generator is g(m)

a SHRINK operation on the tuple (v, v) if a(m, l−s)

. We
provide a construction that has m phases. In the vth step, we
in the following manner:

construct a(m, l−s)(cid:12)(cid:12)(v,u) from g(v)
• We construct a(m, l−s)(cid:12)(cid:12)(v−1,u) from g(v−1)
• We construct a(m, l−s)(cid:12)(cid:12)(v−1,u) from g(v−1)
• If φu = 1: Let a(m, l−s)(cid:12)(cid:12)(v,u) deﬁne φt = 0 for u −
Construct a(m, l−s)(cid:12)(cid:12)v−1,u−k−1 from g(v−1)

k ≤ t ≤ u − 1 and φu−k−1 > 0, where 0 ≤ k ≤ γ.
and apply
(γ, k)-GROW to the (v, v) tuple. (Note: If (u− k− 1) =
0, then consider the alignment where {1, . . . , v − 1} are
all aligned to the null position)

a MERGE operation on the tuple (v, v) if φu > 1.

and employ
= 0.

and employ

0

0

v

γ, l

Lemma 3

As these are the only possibilities in the vth phase, at the end
of the mth phase we get a(m,l−s).
The dynamic programs for Viterbi Alignment and Decoding
are based on ideas in Lemma 2 and Theorem 1.

(cid:12)(cid:12)(cid:12) is given by the coefﬁcient of xl in
(cid:12)(cid:12)(cid:12)Ag(m)
(cid:18) αγ − 2
(cid:19)(cid:20)
(cid:21)
, where αγ = 2 +Pγ
(cid:12)(cid:12)(cid:12)Ag(m)

(cid:16) (γ+1)(γ+3)m+1

Proof: Refer [Udupa and Maji, 2005b]

γ − 1
αγ − 1

(cid:18) αm

(αγ − 2)

k=0 xk+1.

Theorem 2

(cid:19)

(cid:17)

+ 1

(1)

γ+2

γ,?

x

Proof: Substitute x = 1 in Equation 1

Decode For Generator ﬁnds the optimal solution over all
possible alignments in Ag(m)

in polynomial time.

(cid:12)(cid:12)(cid:12) = (γ + 1)
(cid:12)(cid:12)(cid:12)Ag(m)

γ, l

γ,?

(cid:12)(cid:12)(cid:12) = Ω (2m).

Theorem 3 For a ﬁxed l,

Proof: Refer [Udupa and Maji, 2005b]

γ, l

n

in polynomial time.

Viterbi For Generator ﬁnds the optimal solution over all
possible alignments in Ag(m)
Lemma 4 Let S =
g(m)
1
tors. Deﬁne span (S) = ∪N
γ=l, l. Let p(.) be a poly-
nomial. If l ≥ 2 and N = O (p(m)), then there exists an
alignment a(m, l) such that a(m, l) 6∈ span (S).

, . . . , g(m)
i=1 Ag

be a set of genera-

N
(m)
i

o

Proof: Refer [Udupa and Maji, 2005b]

This Lemma shows that by considering any polynomially
large set of generators we can not obtain the optimal solu-
tions to either Viterbi Alignment or Decoding.

If u − k − 1 = 0, then

(cid:18)m − ϕ0

(cid:19)

s3,k =

ϕ0

m−2ϕ0p1

p0

ϕ0

v−1Y

p=1

t (fp|e0)

u−1Y

p=u−k

algorithm for

4 Viterbi Alignment
We now develop the
Viterbi For Generator for
IBM Model 3.
that this subroutine solves the following problem:
1 , a(m, l)|el
f m

a∗(m, l) = argmax
a(m, l)∈Ag(m)

(cid:16)

the

P r

1

(cid:17)

γ, l

subroutine
Recall

.

0

Our algorithm has a time complexity polynomial in m and l.
Without loss of generality, we assume that g(m) = g(m)

Note that a∗(m, l)(cid:12)(cid:12)(m, l−s) (where 0 ≤ s ≤ γ and φl−s > 0)

can be obtained from g0 using the structure transformation
operations described in Section 3.2. Therefore, we build the
Viterbi alignment from left to right. We scan the tuple se-
quence from left to right in m phases and in any phase we
determine the best structure transformation operation for that
phase.
We consider all possible partial alignments between f v
1 and
1, such that φu = ϕ > 0 and φ0 = ϕ0.
eu
Let
B (u, v, ϕ0, ϕ) be the best partial alignment between f v
1 and
1 and A (u, v, ϕ0, ϕ) be its score2. Here, ϕ0 is the number
eu
of French words aligned to e0 in the partial alignment.
The key idea here is to compute A (u, v, ϕ0, ϕ) and
B (u, v, ϕ0, ϕ) recursively using Dynamic Programming. In
the vth phase (corresponding to the tuple (v, v)), we consider
each of the structure transformation operation.

1. SHRINK: If ϕ0 > 0, the SHRINK operation extends
B (u, v − 1, ϕ0 − 1, φ) and the score of the resulting
partial alignment is

s1 = cA (u, v − 1, ϕ0 − 1, ϕ)

where c = t (fv|NULL) p1

p2
0

(m−2ϕ0+1)(m−2ϕ0+2)

(m−ϕ0+1)ϕ0

.

2. MERGE:

If ϕ > 0, the MERGE operation extends
B (u, v − 1, ϕ0, ϕ − 1) and the score of the resulting
partial alignment is

s2 = cA (u, v − 1, ϕ0, ϕ − 1)
n(ϕ−1|eu) t (fv|eu) d (rv|u, l, m).

where c = n(ϕ|eu)φ
3. (γ, k)-GROW: Let

ϕ0 = argmax

k

B (u − k − 1, v − 1, ϕ0, k) .

(γ, k)-GROW extends B (u − k − 1, v − 1, ϕ0, ϕ0) if
ϕ = 1. If u − k − 1 > 0, the score of the resulting
partial alignment is

s3,k = cA (u − k − 1, v − 1, ϕ0, ϕ0)

where
c = n (1|eu) t (fv|eu) d (rv|u, l, m)

u−1Y

p=u−k

n (0|ep) .

2A naive implementation of the table B(∗,∗,∗,∗) takes O (m)
size for each element. It can instead be implemented as a table of
decisions and a pointer to the alignment that was extended to obtain
it. This makes the size of each entry O (1).

.

× n (1|eu) t (fv|eu) d (rv|u, l, m)

n (0|ep) .

We choose that operation which gives the best score. As a re-
sult, B (u, v, ϕ0, ϕ) now represents the best partial alignment
so far and A (u, v, ϕ0, ϕ) represents its score. We have,
1 , a(v, u)|eu
f v

(cid:16)

(cid:17)

P r

1

A (u, v, ϕ0, ϕ) = argmax
a(v, u)∈Ag(v)
γ, u
φu=ϕ, φ0=ϕ0

We determine A (u, v, ϕ0, ϕ) and B (u, v, ϕ0, ϕ), for all 0 ≤
u ≤ l, 0 ≤ v ≤ m, 0 < ϕ ≤ ϕmax and 0 ≤ ϕ0 ≤ m
2 .
To complete the scores of the alignments, we multiply each

A (u, m, ϕ0, ϕ) byQl

p=u+1 n (0|ep) for l − γ ≤ u ≤ l.

Let

(ˆu, ˆϕ0, ˆϕ) =

A (u, m, ϕ0, ϕ) .

argmax

l−γ ≤ u ≤ l
0 ≤ ϕ0 ≤ m
2
0 < ϕ ≤ ϕmax

The Viterbi Alignment is, therefore, B (ˆu, m, ˆϕ0, ˆϕ).
4.1 Time Complexity
The algorithm computes B (u, v, ϕ0, ϕ) and A (u, v, ϕ0, ϕ)

and therefore, there are O(cid:0)lm2ϕmax

(cid:1) entries that need to be

computed. Note that each of these entries is computed incre-
mentally by a structure transformation operation.

• SHRINK operation requires O (1) time.
• MERGE operation requires O (1) time.
• (γ, k)-GROW (0 ≤ k ≤ γ) operation requires O (ϕmax)
time for ﬁnding the best alignment and another O (1)
time to update the score3.
Each iteration takes O (γϕmax) time. Computation of the

(cid:1) time. The ﬁnal step
tables A and B takes O(cid:0)lm2γϕ2
O (mγϕmax) time4. Thus, the algorithm takes O(cid:0)lm2γϕmax
(cid:1)
plexity of our algorithm is O(cid:0)lm2(cid:1).
(cid:1) entries in each of the two tables.
There are O(cid:0)lm2ϕmax
of Viterbi For Generator is O(cid:0)lm2(cid:1).

Assuming γ and ϕmax to be constants, the space complexity

time. In practice, γ and ϕmax are constants. The time com-

of ﬁnding the Viterbi alignment from the table entries takes

4.2 Space Complexity

max

IBM Models 4 and 5 Extending this procedure for IBM
Models 4 and 5 is easy and we leave it to the reader to ﬁg-
ure out the modiﬁcations.

3Note: The product Qu−1

p=u−k n (0|ep) can be calculated incre-
mentally for each k, so that at each iteration only one multiplication
p=1 t (fp|NULL) can be
needs to be done. Similarly, the score Qv−1
calculated incrementally over the loop on v.
u+1 n (0|ep) can be calculated incrementally over
the loop of u. So, it takes O (1) time in each iteration.

4The factor Ql

in

spirit

similar

5 Decoding
The algorithm for the subroutine Decode For Generator
is
for
Viterbi For Generator. We provide the details for
IBM Model 4 as it is the most popular choice for Decoding.
We assume that the language model is a trigram model.
VE is the target language vocabulary and IE ⊂ VE is the
set of infertile words. Our algorithm employs Dynamic
Programming and computes the following:

algorithm

the

to

(cid:16)

1 , a∗(m,L)(cid:17)

e∗L

=

argmax
a(m,L)∈Ag(m)

γ,? , eL
1

P r

1 , a(m,L)|eL
f m

1

(cid:16)

(cid:17)

1 (e00, e0, ϕ0, ϕ, ρ) be the set of all partial hypotheses
Let Hv
1 and have e00e0 as their last two
which are translations of f v
words (ρ is the center of cept associated with e0 and ϕ > 0
is the fertility of e0). Observe that the scores of all par-
1 (e00, e0, ϕ0, ϕ, ρ) are incremented by the
tial hypothesis in Hv
same amount thereafter if the partial hypotheses are extended
with the same sequence of operations. Therefore, for every
1 (v, e00, e0, ϕ0, ϕ, ρ) it sufﬁces to work only on the best
Hv
partial hypothesis in it.
The initial partial hypothesis is h0 (., ., 0, 0, 0) with a score of
m. Initially, the hypothesis set H has only one hypothe-
p0
sis, h0 (., ., 0, 0, 0). We scan the tuple sequence of g(m)
left
to right in m phases and build the translation left to right.
In the vth phase, we extend each partial hypothesis in H
by employing the structure transformation operations. Let
hv−1
(e00, e0, ϕ0, ϕ, ρ) be the partial hypothesis which is be-
ing extended.

0

1

• SHRINK: We

a new partial hypothesis
1 (e00, e0, ϕ0 + 1, ϕ, ρ) whose score is the score of
hv
hv−1

create

1

(e00, e0, ϕ0, ϕ, ρ) times
t (fv|NULL) p1
p0

2

(m − 2ϕ0 − 1) (m − 2ϕ0)

(m − ϕ0) (ϕ0 + 1)

.

the

put

hypothesis

• MERGE: We

We
new
1 (e00, e0, ϕ0 + 1, ϕ, ρ).
Hv
create
1 (e00, e0, ϕ0, ϕ + 1, ρ) whose score is
hv
of hv−1

(e00, e0, ϕ0, ϕ, ρ) times

1

a new partial hypothesis
the score

in

to

n (ϕ + 1|e0)
n (ϕ|e0) t (fv|e0) dnew

dold

.

Here, dold is the distortion probability score of the last
tableau before expansion, dnew is the distortion prob-
ability of the expanded hypothesis after rv is inserted
into the tableau and ρ0 is the new center of the cept
associated with e0. We put the new hypothesis into
1 (e00, e0, ϕ0, ϕ + 1, ρ0).
Hv

• (γ, k)-GROW: We choose k words(cid:0)e(1), . . . , e(k)(cid:1) from
IE and one word (cid:0)e(k+1)(cid:1) from VE. We create a
(cid:0)e0, e(1), ϕ0, 1, rv
(cid:1) if k = 0,
(cid:0)e(k), e(k+1), ϕ0, 1, rv
(cid:1) otherwise. The score of this
kY

new partial hypothesis hv
1
hv
1
hypothesis is the score of hv−1
n (1|ek+1) t (fv|ek+1)d (rv − ρ|A (e0) ,B (fv))

(e00, e0, ϕ0, ϕ, ρ) times

1

n (0|ep)

p=1

(cid:0)ek+1, ek, ϕ0, 1, rv

put

the

(cid:1).

new

hypothesis

in

to

We
Hv
1

distinct partial hypotheses.

the beginning of vth phase,

At the end of the phase, we retain only the best hypothesis
1 (e00, e0, ϕ0, ϕ, ρ). After m phases, we append at
for each Hv
most γ infertile words to all hypotheses in H. The hypothesis
with the best score in H is the output of the algorithm.
5.1 Time Complexity
At

there are at most
takes
O (1) + O (ϕmax) + O (|IE|γ |VE| ϕmax) time to extend
the algorithm takes totally
a hypothesis.
time. Now, since |VE|, |IE|, ϕmax
and γ are assumed to be constant, the time complexity of the

(cid:17)
O(cid:16)|VE|2 v2ϕmax
O(cid:16)|VE|3 |IE|γ m3ϕ2
algorithm is O(cid:0)m3(cid:1).
In each phase, we need O(cid:16)|VE|2 m2ϕmax
O(cid:0)m2(cid:1).

space. Assuming
the space complexity is

|VE| and ϕmax to be constant,

5.2 Space Complexity

Therefore,

(cid:17)

(cid:17)

max

It

Extending this procedure for IBM Models 3 and 5 is easy.

6 Experiments and Results
6.1 Decoding
We trained the models using the GIZA++ tool [Och, 2001] for
the translation direction French to English. For the current set
of experiments, we set γ = 1 and ϕmax = 10. To compare
our results with those of a state of the art decoder, we used the
Greedy decoder [Marcu and Och, 2004]. Our test data con-
sisted of a corpus of French sentences with sentence length in
the range 06− 10, 11− 15, . . . , 56− 60. Each sentence class
had 100 French sentences. In Table 1, we compare the mean
logscores (negative logarithm of the probability) of each of
the length classes. Lower logscore indicates better probabil-
ity score. We observe that the scores for our algorithm are
(20%) better than the scores for Greedy algorithm.
In the
same table, we report the average time required for decoding
for each sentence class. Our algorithm is much faster than the
Greedy algorithm for most length classes. This demonstrates
the power of our algorithm to search an exponentially large
subspace in polynomial time.
We compare the NIST and BLEU scores of our solutions with
the Greedy solutions. Our NIST scores are 14% better while
our BLEU scores are 16% better than those of the Greedy
solutions.

6.2 Viterbi Alignment
We compare our results with those of local search algorithm
of GIZA++ [Och, 2001]. We set γ = 4 and ϕmax = 10.
The initial generator for our algorithm is obtained from the
solution of the local search algorithm.
In Table 3, we report the mean logscores of the alignments
(for each length class) found by our algorithm and the local
search algorithm. Our scores are about 5% better than those
of the local search algorithm.

Time (sec.)
Our

Score

Our

025.31
036.35
048.49
061.90
073.62
087.40
098.84
112.06
122.88
135.31
146.21

Our

4.6922
5.0521
5.6887
5.4845
5.6388
5.2857
5.8144
6.0273
6.2733
6.5108
6.4538

Greedy
028.09
041.36
054.52
072.29
086.04
101.98
116.62
133.10
146.28
160.75
175.53

Greedy
4.2705
4.7236
5.1301
4.8847
5.0480
4.6997
5.1785
5.1311
5.2489
5.4288
5.4089

Table 1: Mean Time and Mean logscores

BLEU

NIST

Class
06-10
11-15
16-20
21-25
26-30
31-35
36-40
41-45
46-50
51-55
56-60

Class
06-10
11-15
16-20
21-25
26-30
31-35
36-40
41-45
46-50
51-55
56-60

000.18
000.89
001.86
003.74
005.41
007.70
011.23
013.29
016.83
020.22
024.21

Our

0.3285
0.2942
0.3335
0.2816
0.3261
0.2666
0.3347
0.3303
0.3620
0.3686
0.3627

Greedy
000.08
000.40
001.47
003.82
008.21
016.62
029.09
048.19
074.03
112.82
168.64

Greedy
0.2696
0.2701
0.2867
0.2328
0.3014
0.2330
0.2988
0.2812
0.3092
0.3032
0.3030

Table 2: BLEU and NIST Scores

7 Conclusions
It would be interesting to see if the theory of alignment gener-
ators can be applied successfully for other problems in NLP.

References
[Al-Onaizan et al., 1999] Y. Al-Onaizan, J. Curin, M. Jahr,
K. Knight, J. Lafferty, D. Melamed, F. Och, D. Purdy,
N. Smith, and D. Yarowsky. Statistical machine transla-
tion: Final reprot. JHU Workshop, 1999.

[Berger et al., 1996] A. Berger, P. Brown, S. Della Pietra,
V. Della Pietra, A. Kehler, and R. Mercer. Language trans-
lation apparatus and method using context-based transla-
tion models. United States Patent 5,510,981, 1996.

[Brown et al., 1993] P. Brown, S. Della Pietra, V. Della
Pietra, and R. Mercer. The mathematics of machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263–311, 1993.

[Germann et al., 2003] U. Germann, M. Jahr, D. Marcu, and
K. Yamada. Fast decoding and optimal decoding for ma-
chine translation. Artiﬁcial Intelligence, 2003.

[Knight, 1999] Kevin Knight. Decoding complexity in word-
replacement translation models. Computational Linguis-
tics, 25(4), 1999.

Class
06-10
11-15
16-20
21-25
26-30
31-35
36-40
41-45
46-50
51-55
56-60

Our

017.51
026.77
037.63
046.96
057.31
068.46
079.83
090.04
102.24
112.65
122.15

Greedy
017.76
027.49
038.98
048.61
059.74
071.46
083.59
094.24
107.74
119.40
129.46

Table 3: Viterbi Alignment Logscores

[Marcu and Och, 2004] D. Marcu and F. Och. Greedy de-
coder for statistical machine translation. http://www.
isi.edu/licensed-sw/rewrite-decoder,
2004.

[Marcu and Wong, 2002] D. Marcu and W. Wong. A phrase-
based, joint probability model for statistical machine trans-
lation.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP),
pages 133–139. Philadelphia, 2002.

[Och et al., 2001] F. Och, N. Uefﬁng, and H. Ney. An ef-
ﬁcient a* search algorithm for statistical machine trans-
lation.
In Proceedings of the ACL 2001 Workshop on
Data-Driven Methods in Machine Translation, pages 55–
62. Toulouse, France, 2001.

[Och, 2001] F. Och.

Giza++.

http://www-i6.

informatik.rwth-aachen.de/Colleagues/
och/software/GIZA++.html, 2001.

[Tillman and Ney, 2000] C. Tillman and H. Ney. Word re-
ordering and dp-based search in statistical machine trans-
lation. In Proceedings of the 18th COLING, pages 850–
856. Saarbrucken, Germany, 2000.

[Udupa and Maji, 2005a] R. Udupa and H. Maji. On the
computational complexity of statistical machine transla-
tion. 2005.

[Udupa and Maji, 2005b] R. Udupa and H. Maji. Theory of
alignment generators (extended version). http://www.
geocities.com/hemanta maji/algo.pdf,
2005.

[Udupa et al., 2004] R. Udupa, T. Faruquie, and H. Maji. An
algorithmic framework for the decoding problem in sta-
tistical machine translation. In Proceedings of COLING.
Geneva, Switzerland, 2004.

[Wang and Waibel, 1997] Y. Wang and A. Waibel. Decod-
ing algorithm in statistical machine translation.
In Pro-
ceedings of the 35th ACL, pages 366–372. Madrid, Spain,
1997.

[Wang and Waibel, 1998] Y. Wang and A. Waibel. Modeling
with structures in statistical machine translation. In Pro-
ceedings of the 36th ACL. Montreal, Canada, 1998.


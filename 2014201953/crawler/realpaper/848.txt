Kernel Carpentry for Online Regression using Randomly Varying Coefﬁcient

Model

∗†
Narayanan U Edakunni
∗ School of Informatics, University of Edinburgh, Edinburgh EH9 3JZ, UK

Sethu Vijayakumar

Stefan Schaal

∗

†

† Department of Computer Science, University Southern California, Los Angeles, CA 90089, USA

n.u.edakunni@sms.ed.ac.uk, sschaal@usc.edu, sethu.vijayakumar@ed.ac.uk

Abstract

We present a Bayesian formulation of locally
weighted learning (LWL) using the novel concept
of a randomly varying coefﬁcient model. Based
on this, we propose a mechanism for multivariate
non-linear regression using spatially localised lin-
ear models that learns completely independent of
each other, uses only local information and adapts
the local model complexity in a data driven fashion.
We derive online updates for the model parameters
based on variational Bayesian EM. The evaluation
of the proposed algorithm against other state-of-
the-art methods reveal the excellent, robust gener-
alization performance beside surprisingly efﬁcient
time and space complexity properties. This paper,
for the ﬁrst time, brings together the computational
efﬁciency and the adaptability of ‘non-competitive’
locally weighted learning schemes and the mod-
elling guarantees of the Bayesian formulation.

1 Introduction
Locally weighted projection regression (LWPR) [Vijayaku-
mar et al., 2005] is a prime example of recent developments
in the area of localised learning schemes that have resulted in
powerful non-linear regression algorithms capable of operat-
ing in real-time, high dimensional, online learning scenarios.
They have been proven to work on many real world appli-
cations including, for e.g., supervised learning of sensorimo-
tor dynamics in multiple degree of freedom anthropomorphic
robotic systems [Vijayakumar et al., 2002].

All locally weighted schemes (including LWPR) have to
determine a region of validity of the local models, i.e., an
adaptive local distance metric, in a data driven fashion. This
is usually achieved by minimising some sort of cross vali-
dation cost on the ﬁt using gradient descent methods. How-
ever, the initialization of the local complexity parameter or
distance metric, the forgetting factor and the learning rates
involved in the gradient method necessitate careful hand tun-
ing of multiple open parameters in existing methods. This
may not be trivially achieved in many real world problems
with limited prior domain knowledge. Also, there exists no
proper probabilistic formulation of the local weighted learn-
ing framework – a necessary development in order to exploit

the model selection guarantees that Bayesian methods pro-
vide while retaining the ﬂexibility provided by nonparametric
localised learning.

One of the most attractive characteristics of LWPR-like lo-
calised learning schemes is its independent learning rules for
each individual local model, which combines or blends the
outputs only at the stage of prediction. In addition to avoid-
ing negative interference [Schaal and Atkeson, 1998], this
property also allows asynchronous learning of local models
leading to improved efﬁciency. We preserve this property in
our model by building a generative probabilistic model for
each individual local model and derive corresponding learn-
ing rules. We show that our novel formulation performs ro-
bustly in estimating local model complexity, competes with
the state-of-the-art methods in generalization capability, can
be extended to learn in truly incremental fashion, i.e., without
storing data and is surprisingly efﬁcient in both computational
complexity and space.

2 Randomly Varying Coefﬁcient model

Modelling spatially localized linear models using a proba-
bilistic framework involves deriving a formulation that allows
to model the ﬁt, in our case a linear ﬁt, and the bandwidth at
a particular location in the input space. Each of these local
models can then be combined to provide a prediction for a
novel data. Additionally, in order for the local models to be
independent, each of them should be capable of modelling
the entire data by learning the correct bandwidth that parti-
tions the data into two parts – one which corresponds to the
linear region of interest and the other which does not.
In
this paper, we accomplish this by formulating a probabilis-
tic model called Randomly Varying Coefﬁcient(RVC) model
which builds upon the idea of a random coefﬁcient model
[Longford, 1993].

For a locally linear region centered around xc a generative

model for the data points can be written as:

yi = βT

i xi + 

(1)

i − xc)T , 1]T

where xi ≡ [(x(cid:2)
represents the center sub-
tracted, bias augmented input vector, βi ≡ [β
]T
represents the corresponding regression coefﬁcient and  ∼
N (0, σ2) is the Gaussian mean zero noise with a standard de-
viation σ. The data is assumed to have been generated in an

(d+1)
. . . β
i

(1)
i

IJCAI-07

762

Region of locality

b

c

a

b

Figure 1: Variation of prior with the location of the input

Figure 2: The ‘local’ generative model

IID fashion. Crucially, we allow the regression coefﬁcient to
be a random variable with a prior distribution given by:

βi ∼ N ( ˆβ, C i)

(2)
where we have assumed that each βi is generated from a
ˆβ with the conﬁdence being repre-
Gaussian centered around
sented by the covariance Ci. The covariance itself is deﬁned
to be proportional to the distance of x(cid:2)
i from the center. This
has the effect that for points that lie close to the center, the dis-
ˆβ resulting in a linear region
tribution of βi is peaked around
around the center. This has been illustrated schematically in
Fig. 1 where point c is the center of the local model: for a
point a that lies close to c we assign a prior that is fairly tight
around the mean whereas for a point b that lies away from c
the prior is much broader. One can consider various distance
functions to index the variation of the covariance matrix C.
Here, we restrict ourselves to a diagonal version, each diago-
nal element varying quadratically with x as:

Ci(j, j) = ((x(cid:2)

i − xc)T (x(cid:2)

i − xc) + 1)/h2

j = xT

i xi/h2

j

(3)

higher conﬁdence over larger regions of the data. Therefore,
we use a Gamma regularizer prior over the bandwidth param-
eters such that it favors relatively small values of hj leading
to more localised models:

j ∼ Gamma(aj, bj )
h2

(4)

We shall
further assign noninformative Normal prior
ˆβ and a noninformative inverse
N (μ, S) for the parameter
Gamma prior with hyperparameters c and d for σ. We as-
sume a uniform prior for the regularizer hyperparameters aj
and bj. Fig. 2 summarizes the resultant probabilistic model
for a single local model. In this model, one can marginalize
out the hidden variables βi to obtain

Z

P (yi| ˆβ, σ, h1 . . . hd+1) =

P (yi|β T

i xi, σ2)P (βi| ˆβ, C i)dβi

⇒ yi ∼ N ( ˆβ

T

xi, xT

i C ixi + σ2)

(5)

It is interesting to note that the form of likelihood in Eq. (5)
corresponds to a heteroscedastic regression and will be used
in later sections for prediction. In the next section we deal
with computing the parameter updates and the resultant en-
semble posteriors in an efﬁcient manner.

3 Learning

ˆβ,
Our objective is to learn the posterior over the parameters
hj, σ and to obtain point estimates for the hyperparameters –
aj, bj. The joint posterior is given by:
P (h, ˆβ, σ|y, a, b, c, d, μ, S) = P (y, ˆβ, h, σ, a, b, c, d, μ, S)

P (y, a, b, c, d, μ, S)

d+1]T

1 . . . h2

(6)
where we have used h to denote the vector [h2
and
, a ≡ [a1 . . . ad+1]T
y denotes the training data [y1 . . . yN ]T
and b ≡ [b1 . . . bd+1]T
. However, the posterior over the
parameters is rendered intractable due to the difﬁculty in
evaluating the denominator of Eq. (6). This necessitates
the use of variational Bayesian EM to evaluate the posterior
P (h, ˆβ, σ|y, a, b, c, d, μ, S) and learn the regulariser hyper-
parameters a and b.

3.1 Variational approximation
To learn the parameters of the model we can maximize the
marginal log likelihood with respect to the parameters treat-
ing βi as the hidden variables. The marginal log likelihood is
given by:

L = ln P (y|a, b, c, d, μ, S)

= ln

P (y, β1 . . . βN , h, ˆβ, σ|a, b, μ, S, c, d)dβ1 . . . dβN

Z
Z "Y
Y

dhd ˆβdσ

i

= ln

P (yi|βi, σ)P (βi| ˆβ, h1, . . . hd+1)

#

P (h2

j |aj , bj)P ( ˆβ|μ, S)P (σ2|c, d)

j

dh1 . . . dhd+1d ˆβdσ

dβ1 . . . dβN

(7)

where hj is the bandwidth parameter of the kernel deﬁning
the extent of the locality along the j-th dimension. This
choice of the kernel parametrization allows us to use a con-
jugate Gamma prior over hj. The higher values of hj im-
ply lesser variation amongst the coefﬁcients βi and hence,
larger regions of linearity. Although the bandwidth modu-
lates the bias-variance tradeoff, an unconstrained likelihood
maximization will, in general favor large hj since it implies a

IJCAI-07

763

Using Jensen’s inequality, the objective function that lower
bounds L is given by:

Z h

F =

Q(β1 . . . βN , h, ˆβ, σ2)

˜S = (

(cid:4)Ci(cid:5)−1 + S−1)−1,

˜μ = ˜S(

(cid:4)Ci(cid:5)−1 ν i + S−1μ)

i

X
X
X

i

h

˜aj = aj + N/2
˜
bj = bj +

i

(16)

(17)

(18)

(19)

i

/(2xT

i xi)

(ν i,j − ˜μi,j)2 + Gi,jj + ˜Sjj

Here, ν i,j and ˜μi,j denote the j-th element of the respective
˜Sjj denotes the j-th diagonal element.
vectors and Gi,jj and
h

(20)

i

X

(yi − ν T

i xi)2 + xT

i Gixi

/2

(21)

˜c = c + N/2
˜
d = d +

i

We also need to learn the point estimates for the regulariser
hyperparameters aj and bj. Maximum likelihood value for
the hyperparameters aj and bj can be found by maximiz-
ing the bound Fapprox given by Eq. (9) with respect to these
hyperparameters keeping the posterior distributions Q ﬁxed.
Considering only the terms involving the hyperparameters:

Z

E =

Q(h2

j |˜aj ,

˜
bj ) ln P (h2

j |aj , bj)dh2

j

Maximising E with respect to the hyperparameters is equiv-
alent to minimising the KL divergence between the distribu-
tions Q and P . Since the posterior Q and prior P share the
same parametric form, KL divergence is minimised when the
parameters of these distributions match. This leads to the sim-
ple update rule for the hyperparameters given by:

aj = ˜aj,

bj = ˜
bj

(22)

The hyperparameters μ, S, c and d are initialised such that the
corresponding priors are non-informative. An initialisation of
μ = 0, S = 10−3×I, c = 10−3
ensures such a
condition. On the other hand the regulariser hyperparameters
a and b are initialised such that it encourages small h. A
value of a = 1 and a sufﬁciently large value for b ensures
such a bias. These are the settings used by RVC for all the
evaluations carried out in Sec. 4.

and d = 10−3

3.2 Prediction using the committee of local models

We have dealt so far with building a coherent probabilis-
tic model for each local expert and have derived inference
procedures to estimate the parameters of individual model.
Given the ensemble of trained local experts, in order to pre-
dict the response yq for a new query point xq, we take the
normalised product of the predictive distribution of each lo-
cal expert. This is close in spirit to the paradigm of Product
of Experts [Hinton, 1999] and the Bayesian Committee Ma-
chines [Tresp, 2000]. The predictive distribution of each local
expert is given by:

Z

P (yq|y) =

P (yq| ˆβ, σ, h)Q( ˆβ|y)Q(σ2|y)Q(h|y)dhd ˆβdσ2

(23)
where P (yq| ˆβ, σ, h) has the form given by Eq. (5). We
ˆβ from Eq. (23), but cannot do
can further integrate out
the same for σ2
and h. Hence, we approximate Q(σ2|y)
and Q(h|y) by a delta function at the mode which implies

ln P (y, β1 . . . βN , h, ˆβ, σ2|a, b, μ, S, c, d)

Q(β1 . . . βN , h, ˆβ, σ2)

dhd ˆβdσ2

#

dβ1 . . . dβN

(8)

since

value

optimal

for Q(β1 . . . βN , h, ˆβ, σ)

the posterior over

that
The
is given by the joint poste-
makes the bound tight
rior P (β1 . . . βN , h, ˆβ, σ|y) but
this posterior
is intractable, we make an approximation by assum-
ing that
the variables is indepen-
dent and can be expressed as Q(β1 . . . βN , h, ˆβ, σ) =
(cid:2)
This form of
approximation is often called an ensemble variational ap-
proximation, details of which can found in [Beal, 2003].
Substituting the factorised approximation in Eq. (8) we get:
Fapprox =

j |y)Q( ˆβ|y)Q(σ2|y).

i Q(βi|y)

(cid:2)

i

j Q(h2
h
X
(cid:4)ln P (yi|βi, σ)(cid:5)Qβi
D
X
˙
X

ln P (βi| ˆβ, h1 . . . hd+1)

ln P (σ2|c, d)

j |aj, bj)

ln P (h2

ﬁ

˙

¸

¸

Qhj

−

X

σ2

Q

j

i

˙

¸

ln Qhj

−

Qhj

j

+

+

+

−

σ2

,Q

E

D

+

D

ﬂ
ln Q ˆβ

#

E

Qβi

,Qh1

...Qhd+1,Q ˆβ

ln P ( ˆβ|μ, S)

E

ln Qβ

i

Qβi

(9)

Q ˆβ

− (cid:4)ln Qσ2 (cid:5)Q

σ2

Q ˆβ

where (cid:4).(cid:5)Q denotes the expectation with respect to the dis-
tribution Q. The optimal values of the posterior probabili-
ties can be computed iteratively by maximizing the functional
Fapprox with respect to each individual posterior distribution
keeping the other distributions ﬁxed akin to an EM procedure.
Such a procedure can be shown to improve our factorised ap-
proximation of the actual posterior in each iteration. Skipping
the derivation, such a procedure yields the following posterior
distributions:

Q(βi|y) ∼ N (ν i, Gi)
Q( ˆβ|y) ∼ N ( ˜μ,
˜
Q(h2
bj )
˜
Q(σ2|y) ∼ Inv-Gamma(˜c,
d)

j |y) ∼ Gamma(˜aj,

˜S)

˙

¸

where

Gi = (xixT

i /

= (cid:4)C i(cid:5) −

σ2
(cid:4)C i(cid:5) xixT
(cid:4)σ2(cid:5) + xT

+ (cid:4)C i(cid:5)−1)−1
i (cid:4)C i(cid:5)
i (cid:4)C i(cid:5) xi

(10)

(11)

(12)

(13)

(14)

where the second part has been derived by making use of
the Sherman-Morrison Woodbury theorem. Here (cid:4)Ci(cid:5) =
diag(xT
is the expectation with re-
spect to Qσ2 . Furthermore, using results from Eq. (14),

(cid:4)
Q(h2
j

(cid:3)
h2

) and

i xi/

σ2

(cid:3)

(cid:4)

)

j

ν i = Gi(yixi/

+ (cid:4)C i(cid:5)−1 ˜μi)

“

”

yi − xT

i ˜μi

+ ˜μi

(15)

˙

¸

σ2
(cid:4)C i(cid:5) xi

=

((cid:4)σ2(cid:5) + xT

i (cid:4)C i(cid:5) xi)

IJCAI-07

764

mode and Q(h|y) ≈ δh2

Q(σ2|y) ≈ δσ2
tive distribution for the k-th local model is:
T (˜Sk + C khmode

yq,k ∼ N ( ˜μT xq,k, xq,k

mode . The ﬁnal predic-

)xq,k + σ2

mode)

where xq,k refers to the query point with the k-th center sub-
tracted and augmented with bias. Blending the prediction of
different experts by taking their product and normalising it
results in a Normal distribution given by:

P

P

k αk ˜μT
k αk

k xq,k

, ζ 2 =

6:

.

1P

k αk

yq ∼ N (μ, ζ 2) where μ =

Here, μ is a sum of the means of each individual expert
weighted by the conﬁdence expressed by each expert in its
own prediction αk, ζ2
is the variance and αk is the precision
of each expert:
αk = 1/(xT

q,k(˜Sk + C k)xq,k + σ2

k), C k = diag{xT

q,kxq,k/h2

j,k}

3.3 Online updates
The iterative learning rules to estimate the posteriors over pa-
rameters given the appropriate prior and the data, represented
by Eqs. (16)-(21), can be rewritten in the form of online up-
dates by exploiting the Bayesian formalism. In a batch mode
of posterior evaluation, we have

NY

posteriorN =

(likelihoodi) × prior0

i

The same can be expressed as a set of online updates:

posteriori = likelihoodi × priori;

priori+1 = posteriori

Therefore we can transform the batch updates that we had
derived earlier into online updates given by :

h

˜ci = ci + 1/2
˜
di = di +

i

˜Si = ((cid:4)Ci(cid:5)−1 + S−1
)−1
˜μi = ˜Si((cid:4)Ci(cid:5)−1 ν i + S−1
˜ai,j = ai,j + 1/2
˜
bi,j = bi,j +

h

i μi)

(ν i,j − ˜μi,j)2 + Gi,jj + ˜Si,jj

i

/(2xT

i xi)

(24)

(25)

(26)

(27)

(28)

i

(yi − ν T

i xi)2 + xT

i Gixi

(29)
We repeat the above updates for a single data point {xi,yi}
˜Θ represents the posterior
till the posteriors converge – here,
of Θ. For the (i + 1)-th point, we then use posterior of i-th
step as the prior as illustrated in Algorithm 1.

/2

Addition/deletion of local models
The complexity of the learner is adapted by the addition and
deletion of local models. When the predictive likelihood for a
new data point is sufﬁciently low then one can conclude that
the complexity of the learner needs to be increased by adding
a new local model. This leads to the simple heuristic for the
addition of a local model wherein a local model is added at
a data point when the predictive probability for the particular
training data is less than a ﬁxed threshold. The data point
serves as the center for the added local model.

When two local models have sufﬁcient overlap in the re-
gion they model, then one of them is redundant and can be
pruned. The overlap between two local models can be deter-
mined by the difference in the conﬁdence expressed in their
prediction for a common test point. The addition and deletion
heuristics that have been used here is similar to the ones used
in [Schaal and Atkeson, 1998].

Algorithm 1 Training a local model
1: Initialise hyperparameters: Θ0 ≡ {μ0, S0, c0, d0, a0, b0}.
2: for i = 1 to N do
3:
4:
5:

Input xi, yi
repeat

˜Θi using Θi and

Estimate posterior hyperparameters
Eq. (14), (15) and Eqs. (24) - (29).
Estimate values of the hyperparameters a and b of
the regulariser prior using Eq. (22).

until convergence of posteriors

7:
8: Θi+1 = ˜Θi
9: end for

3.4 Complexity analysis
The time complexity of the algorithm is dominated by the
computation of Gi in Eq. (14). The equations that use Gi are
Eq. (27) and Eq. (29) and these can be rewritten to avoid ex-
plicit computation of Gi. Eq. (27) requires only the diagonal
elements of Gi which can be computed in O(d) since

Gi(j, j) = Ci(j, j) − (Ci(j, j)xi(j))2/(σ2 + γi)

using Eq. (14)

where γi = xT
i Cixi which can also be computed in O(d) due
to the fact that Ci is diagonal. On the other hand, Eq. (29) re-
quires the evaluation of xT
i Gixi which in turn can be written
down as:

i Gixi = σ2γi
xT
σ2 + γi

and can also be computed in O(d). Furthermore, the ma-
trix inverses in Eq. (24) and Eq. (25) can also be computed
in O(d) due to the fact that Si and Ci are diagonal matri-
ces. Therefore the overall time complexity per online update
is O(dM ) where d is the number of dimensions and M the
number of local models. The algorithm doesn’t require any
data points to be stored and hence, has a O(M ) space com-
plexity for the sufﬁcient statistics stored in the local models.
The independence of the local models also means that the ef-
fective time complexity can be brought down to O(d) using
M parallel processors. The time complexity for prediction is
O(dM ) including the evaluation of mean and the conﬁdence
bounds. We can see from this analysis that the algorithm is
very efﬁcient with respect to time and space (in fact it matches
LWPR’s efﬁciency) and hence, is a strong candidate for situ-
ations which require real time and online learning.

4 Evaluation

In this section, we demonstrate the salient aspects of the RVC
model by looking at some empirical test results, compare the
accuracy and robustness against state of the art methods and
evaluate its performance on some benchmark datasets.

Fig. 3(a) shows the local linear ﬁts (at selected test points)
learned by RVC from noisy training data on a function with
varying spatial complexity. Such functions are extremely
hard to learn since models with high bias tends to oversmooth
the nonlinear regions while more complex models tend to
ﬁt the noise. One can see that the linear ﬁt roughly corre-
sponds to the tangential line at the center of each local model
as expected. A more signiﬁcant result is the adaptation of

IJCAI-07

765

Data points
True fn.
Linear fits

2

1.5

1

0.5

y

h

t

i

d
w
d
n
a
B

0

3

2

1
0

2

4

x

x

(a)

Data points
True fn.
Learnt fn.
Confidence bounds

RVC

2

1.5

1

0.5

0

y

Data points
True fn.
Learnt fn.
Confidence bounds

GP

2

1.5

1

0.5

0

y

6

8

10

−0.5
0

2

4

x

(b)

6

8

10

−0.5
0

2

4

6

8

10

x

(c)

Figure 3: (a)Local ﬁts and bandwidth adaptation. Fit and conﬁdence bounds learned by (b) RVC model and by (c) GP model.

the local bandwidth. The bottom section of Fig. 3(a) plots
the converged locality measure computed as product of the
bandwidth parameters along each input dimension - illustrat-
ing the ability to adapt the local complexity parameter in a
data driven manner. For this illustration, the local centers are
placed in a dense, uniform grid in input space. Using the same
target function, we compare the ﬁts and conﬁdence bounds
learned by RVC and Gaussian Processes (GP) [Williams,
1998] in Fig. 3(b) and (c). It is important to note that we
have deliberately avoided using training data in [5.5,6.5] and
the conﬁdence bounds of RVC nicely reﬂect this.

y

100

50

0

−50

−100

−150
0

Motorcycle data: RVC

Data points
Learnt fn.
Confidence bounds

40

50

60

10

20

30
x

Figure 4: Fit and conﬁdence bounds for the motorcycle
dataset learned by the RVC model (local models were cen-
tered at 20 uniformly distributed points along the input)

Our next experiment aims to illustrate the ability of RVC
to model heteroscedastic data (i.e., data with varying noise
levels). Fig. 4 illustrates the ﬁt and the conﬁdence inter-
val learnt on the motorcycle impact data discussed in [Ras-
mussen and Gharamani, 2000]. Notice that the conﬁdence
interval correctly adapts to the varying amount of noise in the
data as compared to the conﬁdence interval learnt by a GP
with squared exponential kernel shown in Fig. 5. This abil-
ity to model non-stationary functions is another advantage of
RVC’s localised learning. In the evaluations presented so far,

we have used RVC in the batch mode using the updates that
we derived in Sec. 3(c.f. Eqs. 14 - 21). The subsequent eval-
uations in this section make use of the online updates derived
in Sec. 3.3.

y

100

50

0

−50

−100

−150
0

Motorcycle data: GP

Data points
Learnt fn.
Confidence bounds

40

50

60

10

20

30
x

Figure 5: Fit and conﬁdence bounds for the motorcycle
dataset learned by the Gaussian Processes model

To compare the online learning characteristics, we trained
the three candidate algorithms on 500 data points from the
sinc function corrupted with output noise:  ∼ N (0, 0.052).
After each training data was presented to the learner, the er-
ror in learning was measured using a set of 1000 uniformly
distributed test points. The RVC model was allowed only a
single EM iteration for each data point to ensure a fair com-
parison with LWPR. The resulting error dynamics is shown in
Fig. 6(a). In this comparison, GP exhibits a sharply decreas-
ing error curve which is not surprising considering that it is
essentially a batch method and stores away all of the train-
ing data for prediction. When we compare RVC with LWPR,
we ﬁnd that RVC converges faster while using roughly sim-
ilar number of local models. This can be attributed to the
Bayesian learning rules of RVC that estimates the posterior
over parameters rather than point estimates. Since the poste-
rior is a product of likelihood and prior, in the event of sparse
data (as in the initial stages of online learning), the prior en-
sures that the posterior distributions assigned to the parame-

IJCAI-07

766

GP
RVC
LWPR

RVC # model
LPWR # model

15

10

5

1

0.8

0.6

0.4

0.2

)
e
s
M
n
(
 
r
o
r
r

E

 
t
s
e
T

0
0

100

400

0
500

200
300
# data points

(a)

l

s
e
d
o
m
#

 

x 10−3

3

2

1

0

)
e
s
M
n
(
 
r
o
r
r

E

 
t
s
e
T

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

GP
LWPR
RVC

sinc

boston

ozone

(b)

Figure 6: (a) Comparison of online learning dynamics for sinc function (b) Comparison of generalization error

ters and in turn the predictions of the learner are reasonable.
Also the optimization of the regularizer hyperparameters for
every data point implies a faster adaptation and hence, a faster
convergence.

Finally, we compare the generalization performance of the
algorithms on artiﬁcial as well as real world datasets. The
sinc function, air dataset described in [Bruntz et al., 1974]
and the Boston housing dataset from the UCI repository were
used as benchmark datasets. The air(ozone) dataset which is a
three dimensional dataset with 111 data points was split into
83 training and 28 test points. The 13 dimensional Boston
dataset was split into 404 training and 102 test points. The on-
line learners namely RVC and LWPR were trained in epochs
of repeated presentation of the training data, till convergence
– LWPR required careful tuning of the distance metric initial-
ization and learning rates to achieve the performance reported
here as opposed to the uninformative priors used for RVC.
The performance of GP, RVC and LWPR shown in Fig. 6(b)
are statistics accumulated over 10 different train-test splits.
Asymptotically, all three methods perform very well on the
sinc data set, achieving nM SE of less than 0.0025. For the
ozone dataset which is highly nonlinear, RVC and LWPR per-
forms better than GP. For the Boston dataset, we ﬁnd that the
performance of RVC is close to that of LWPR while slightly
inferior to the GP results – although this difference is statisti-
cally insigniﬁcant.

5 Discussion

The major contribution of the paper is the development of
a Bayesian formulation for independent spatially localised
learners for multivariate nonlinear regression. We have used
a novel formulation of data dependent priors in order to carve
out locally linear regions while avoiding competition amongst
local models. The ‘non-competitive’ behaviour of each lo-
cal model allows independent, efﬁcient learning while the
Bayesian regularizer hyperpriors guard against the danger of
overﬁtting or over-smoothing through automatic local band-
width adaptation. We evaluated the RVC model against the
state of the art in non-linear regression techniques on artiﬁcial
as well as real world data sets. RVC matched the generaliza-

tion performance of LWPR while avoiding cumbersome pa-
rameter tuning for initialization. It achieves competitive per-
formance compared to GP – essentially a batch method, while
being much more computationally efﬁcient (linear in number
of training data and input dimensionality as opposed to cubic
in training data for GP). The space and computational efﬁ-
ciency of RVC coupled with the ability to grow model com-
plexity in a data driven fashion makes it a strong candidate
for practical online and real time learning scenarios.

References
[Beal, 2003] M.J. Beal. Variational algorithms for approximate
Bayesian inference. PhD thesis, Gatsby Computational Neuro-
science Unit, University College London, 2003.

[Bruntz et al., 1974] S.M. Bruntz, W.S. Cleveland, B. Kleiner, and
J.L. Warner. The dependence of ambient ozone on solar radiation,
temperature and mixing height. In Symp. Atmospheric Diffusion
and Air pollution. MIT Press, 1974.

[Hinton, 1999] G.E. Hinton. Product of experts. In Ninth Interna-

tional Conference on Artiﬁcial Neural Networks, 1999.

[Longford, 1993] N.T. Longford.

Random coefﬁcient models.

Clarendon Press, 1993.

[Rasmussen and Gharamani, 2000] C.E. Rasmussen and Z. Ghara-
mani. Inﬁnite mixtures of Gaussian process experts. In Advances
in Neural Information Processing Systems 14. MIT Press, 2000.
[Schaal and Atkeson, 1998] S. Schaal and C.G. Atkeson. Construc-
tive incremental learning from only local information. Neural
Computation, 10:2047–2084, 1998.

[Tresp, 2000] V. Tresp. A Bayesian committee machine. Neural

Computation, 12(11):2719–2741, 2000.

[Vijayakumar et al., 2002] S. Vijayakumar, A. D’Souza, T. Shibata,
J. Conradt, and S. Schaal. Statistical learning for humanoid
robots. Autonomous Robot, 12(1):55–69, 2002.

[Vijayakumar et al., 2005] S. Vijayakumar, A. D’Souza,

and
S. Schaal. Incremental online learning in high dimensions. Neu-
ral Computation, 17(12), 2005.

[Williams, 1998] C.K.I. Williams. Prediction with Gaussian pro-
cesses: from linear regression to linear prediction and beyond. In
Michael I. Jordan, editor, Learning in Graphical Models, pages
599–621. Kluwer, 1998.

IJCAI-07

767


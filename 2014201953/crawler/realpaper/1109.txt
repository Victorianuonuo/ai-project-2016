Using Graph Algebra to Optimize Neighborhood for Isometric Mapping

Guihua Wen , Lijun Jiang and Nigel R Shadbolt

South China University of Technology, Guangzhou 510641, China

University of Southampton, Southampton SO17 1BJ, United Kingdom

crghwen@scut.edu.cn cssturat@sohu.com nrs@ecs.soton.ac.uk

Abstract

Most nonlinear dimensionality reduction ap-
proaches such as Isomap heavily depend on the
neighborhood structure of manifold. They deter-
mine the neighborhood graph using Euclidean dis-
tance so that they often fail to nicely deal with
sparsely sampled or noise contaminated data. This
paper applies the graph algebra to optimize the
neighborhood structure for Isomap. The improved
Isomap outperforms the classic Isomap in visual-
ization and time complexity, as it provides good
neighborhood structure that can speed up the subse-
quent dimensionality reducing process. It also has
stronger topological stability and less sensitive to
parameters. This indicates that the more compli-
cated or even time-consuming approaches can be
applied to construct the better neighborhood struc-
ture whilst the whole time complexity will not raise
.The conducted experiments on benchmark data
sets have validated the proposed approach.

1 Introduction
Classic dimensionality reduction approaches can not be reli-
ably applied to ﬁnd the meaningful low-dimensional struc-
tures hidden in the high-dimensional observations for ex-
ploratory data analysis such as classiﬁcation and visualiza-
tion, so that two new approaches have recently been devel-
oped. One is locally linear embedding (LLE) that is based on
local approximation of the geometry of the manifold[Roweis
and Saul, 2000]. LLE has many variants, such as Laplacian
eigenmaps [Belkin and Niyogi, 2003]and Hessian eigenmaps
[Donoho and Grimes, 2003], incremental LLE [Kouropteva
and Pietikainen, 2005], supervised LLE [de Ridder et al.,
2003; Kouropteva and Pietikainen, 2005], integrated LLE
with classic PCA or LDA [Abusham et al., 2005; Chang et
al., 2004], integrated LLE with SOM [Xiao et al., 2005] etc.
The other is Isomap that preserves the manifold geometry at
all scales and has better ability to deal with nonlinear sub-
spaces [Tenenbaum et al., 2000]. It also has many variants,
such as Landmark Isomap [Silva and Tenenbaum, 2003], su-
pervised Isomap [Geng et al., 2005], spatio-temporal exten-
sion of Isomap [Jenkins and Mataric, 2004], incremental ex-
tension of Isomap [Law and Jain, 2006], integrated Isomap

with fuzzy LDA [Weng et al., 2005] etc. The other related
studies are also performed such as the selection of the op-
timal parameter value for LLE and Isomap [Kouropteva et
al., 2002; Shao and Huang, 2005], integration of LLE and
Isomap [Saxena et al., 2004] etc. LLE and Isomap have their
own superiorities so that they have been being developed si-
multaneously for various context applications.

Despite Isomap performs well in many cases, it often fails
to nicely deal with noisy data or sparsely sampled data [Geng
et al., 2005]. This is because in these cases the local neighbor-
hood structure on which Isomap largely depends is critically
distorted. This makes Isomap vulnerable to short-circuit er-
rors that some neighors of the current point come from other
different folds so that these neighbors are not nearest ones
on manifold [Silva and Tenenbaum, 2003]. This can in turn
lead to drastically incorrect low-dimensional embedding. Ac-
cordingly some supervised Isomap are proposed [Vlachos et
al., 2002; Geng et al., 2005] which employ the class labels
of the input data to guide the manifold learning. They im-
prove the Isomap in classiﬁcation and visualization, but they
can not work when the class labels of data are not avail-
able. Due to that Isomap can not works on the discon-
nected neighborhood graph, some approaches to construct-
ing connected neighborhood graph are proposed such as the
k-connected or k-edge-connected spanning subgraph of the
complete Euclidean graph of all data points[Yang, 2004;
2005]. This paper deals with the neighborhood graph from
another perspective.
It considers the implicit correlation
among data points using the path algebra on the neighbor-
hood graph, which is further applied to improve Isomap.

2 Approach to optimize the neighborhood
In many data analysis and pattern recognition tasks, similar-
ity between two objects can be established by direct com-
parison and induced by mediating objects. Namely, two ob-
jects might be considered similar when they are connected
by a chain of intermediate objects where all dissimilarities
or distances between neighboring objects in the chain are
small [Fischer and Buhmann, 2003b]. This concept can be
generalized by assuming that object similarity behaves tran-
sitive in many applications. Based on this intuition, the path
algebra-based clustering approach is proposed that can ex-
tract elongated structures from the data in a robust way which
is particularly useful in perceptual organization [Fischer and

IJCAI-07

2398

Buhmann, 2003a; 2003b]. This paper applies it to build the
neighborhood graph for Isomap. The intuitive picture that
two objects should be considered as neighbors if they can be
connected by a mediating path of intermediate objects.

Let G = (V, V × V ) be the complete Euclidean graph of
all data points. It weights the edges using Euclidean distance
de . A path l from v0 to vn in G is deﬁned as a sequence
of vertex (v0, · · · , vi, · · · , vn) such that (vi, vi+1) ∈ V × V ,
0 ≤ i < n.

Deﬁnition Let P (v0, vn)={l|l be a path from v0 to vn in
G}, we deﬁne path algebra as a set P with two binary oper-
ations ∨ and · which have the following properties [B.Carre,
1979]:

1. For all x, y, z ∈ P , the ∨ is idempotent, commutative,
and associative: x ∨ x = x, x ∨ y = y ∨ x,(x ∨ y) ∨ z =
x ∨ (y ∨ z)

2. For all x, y, z ∈ P , the · is associative, and distributive
over ∨: (x · y) · z = x · (y · z), x · (y ∨ z) = (x · y) ∨
(x · z), (y ∨ z) · x = (y · x) ∨ (z · x)

3. The set P contains a zero element φ and a unit element e
such that: φ · x = φ, φ ∨ x = x = x ∨ φ, e · x = x = x · e

Obviously, there are many ways to deﬁne the path algebra
based on different intuitions. This paper focuses on a simple
but important way to deﬁne the path algebra as follows, where
for all x, y ∈ R, φ = 0, and e = ∞

1. x ∨ y def= min(x, y)

2. x · y def= max(x, y)

In this way a new distance dm can be deﬁned as follows [Fis-
cher and Buhmann, 2003b]:
dm(v0, vn) = min

{de(vi, vi+1)}}
(1)
The difference between dm and de can be illustrated in Fig-
ure 1. This difference can signiﬁcantly impact the construc-

l∈P (v0,vn){max(vi,vi+1)∈l

 
),( bade
 
 

d

b

c

a

),(
bad
m

=

min{max{

bdddcdcad
,(
e

,(

),

e

),

,(

e

)},

bad
e

)},(

 

Figure 1: The difference between dm and de

tion of neigborhood graph on data manifold. For exam-
ple, the categories of 5-nearest neighbors of x using de dis-
tance,circled as in Figure 2, is

N e(x) = {square, square, square, circle, circle}

Generally points belonging to the same class are often closer
to each other than those belonging to different classes. Ob-
viously the short circuiting problem happens here because
points marked by square are not the most similar points to x

 

x

 

Figure 2: The different neighborhoods using dm and de

in terms of categories. By contrast, if dm distance is utilized
to determine the neighbors for x, the result is

N m(x) = {circle, circle, circle, circle, circle}

This is a correct result that indicates all neighbors of x lie on
the manifold. It seems that dm can pulls points belonging to
the same class closer and propels those belonging to different
classes further away. It ensures to preserve the intrinsic struc-
ture of the data set and therefore can be applied to improve
the Isomap.

3 Proposed Isomap with Optimized

Neighborhood

Isomap can perform dimensionality reduction well when the
input data are well sampled and have little noise.
In the
presence of the noise or when the data are sparsely sam-
pled, short-circuit edges pose a threat to Isomap by fur-
ther deteriorating the local neighborhood structure of the
data. Subsequently Isomap generates drastically incorrect
low-dimensional embedding. To relieve this negative effect
of the noise, dm can be applied to determine the true neigh-
borhoods as opposed to using Euclidean distance de. This
can get better estimation of geodesic distances and in turn
give lower residual variance and robustness. It also reduces
the time complexity because the better neighborhood struc-
ture can speed up the subsequent optimization process.

To avoid computational expense to calculate the dm be-
tween any two points, we ﬁnd an approximate way to build
the neighborhood graph using dm that can be computed ef-
ﬁciently. It ﬁrst applies de to build the neighborhood graph,
and then utilize the idea of dm to optimize this neighborhood
graph. The algorithm is given as follows.

Algorithm 1: OptimizeNeighborhood(X, k, m, d)
Input: X = {xi} be the high dimensional data set, k be
the neighborhood size, m be the scope for optimization of
neighborhood, and d be the dimension of the projected space.
Output: The optimized neighborhood set N = {N (xi)}

for all points.

1. Calculate the neighborhood N (xi) for any point xi us-

ing de, where N (xi) is sorted ascendingly.

For j= 1 to k

2. Compute n, which is the number of points in X.
3. For i=1 to n
4.
5.
6.
7.

Select j-th point from N (xi), denoted as xij
For p= 1 to m

Select p-th point from N (xij ), denoted as xijp

IJCAI-07

2399

8.

If de(xij , xijp) < de(xi, xik) and xijp /∈ N (xi)
and parent(xijp) ∈ N (xi)
Delete xik from N (xi)
Insert xijp to N (xi) ascendingly
Let j=1
Break

9.
10.
11.
12.
13.
14.
15.
16. EndFor
17. Return N = {N (xi)} that is the optimized neighbor-

EndIf
EndFor

EndFor

hoods for X

In algorithm 1, step 1 calculates the neighborhood graph
using Euclidean distance, as the same as most dimensionality
reduction approaches such as Isomap do. Its time complexity
< O(n3). From step 3 to step 16, the neighborhood of the
given point xi determined in step 1 is optimized using dm. It
includes two cycles. Although step 11 reset j, k and m are
neighborhood sizes so that they can be regarded as small con-
stants. It means that optimization of neighborhood of a point
can be ﬁnished in O(1). Therefore the neighborhood graph
can be efﬁciently optimized with additional time complexity
O(n). In Step 8, condition parent(xijp) ∈ N (xi) means that
xijp must be reachable from xi in the neighborhood graph
determined in step 1.

 

 

4

4

x

x2

9

(1)

2

x1

5

6

12

x11

x12

x13

(3)

2

x1

5

x

4

6

x11

12

x3 

x2

 

 

(2)

2

x

4

6

x1

5

4

x11

12

x11

x12

x13

(4)

2

x

4

5

x1

5

4

x11

12

x2

x12

x11

x12

x13

x11

x12

x13

 

Figure 3: Illustration of optimizing neighborhood of point x

In order to illustrate the idea behind the algorithm 1, we
give an example to optimize the neighborhood of point x,
shown as Figure 3, where k=3 and m=2. Figure 3(1) shows
the neighborhood graph of x determined using Euclidean dis-
It can be observed
tance, where only parts are described.
that x has neighbors N (x) = {x1, x2, x3}.
In algorithm
1, step 5 chooses its ﬁrst neighbor x1 and then step 7 se-
lects the neighbor x11 for x1. In step 8, x11 will be com-
pared with the largest neighbor of x1, namely x3. Because
the condition is satisﬁed, All steps between step 9 and step
12 will be executed. This results in that x11 will replace
x3. Consequently the neighorhood of x is optimized as
N (x) = {x1, x11, x2}. It is shown as Figure 3(2). Because

the neighborhood of point x has changed, the optimization
restarts from the smallest neighbor x1 of the point x and en-
ter the next cycle. Due to x11 has been in neighborhood of
x, x12 will be explored, shown as Figure 3(3). Because the
condition in step 8 is satisﬁed, all steps between step 9 and
step 12 will be executed. This results in that x12 will re-
place x2.Consequently the neighorhood of x is optimized as
N (x) = {x1, x11, x12},shown as Figure 3(4). Due to m=2,
x13 will never be exploited. So far all neighbors of x1 have
been tested. Next cycle will explore the second neighbor of x
in step 5, namely x11. It goes through all steps as exploring
the ﬁrst neighbor x1 of x, which is not explained more here.
Now OptimizeNeighborhood can be applied to improve the
basic Isomap. For hereafter comparison, we denote this im-
proved Isomap as m2-Isomap, which is described as algo-
rithm 2.

Algorithm 2: m2-Isomap(X, k, m, d)
Input: X = {xi} be the high dimensional data set, k be
the neighborhood size, m be the scope for optimization of
neighborhood, and d be the dimension of the projected space.

Output: The dimensionally reduced dataset Y = {yi}.
1. Utilize OptimizeNeighborhood(X, k, m, d) to calculate
the optimized neighborhoods for each point in input data
set X, which will be applied to construct the neighbor
graph in next step.

2. Construct the weighted neighbor graph Ge = (V, E)
for the dataset X, by connecting each point to all its k-
nearest neighbors, where (vi, vi+1) ∈ E,if xj is a mem-
ber of k neighbors of xi determined in step 1. This edge
has weight de(xi, xj ).

3. Employ the Ge to approximately estimate the geodesic
distance dg(xi, xj ) between any pair of points as the
shortest path through graph that connects them.

4. Construct d-dimensional embedding from Ge using

MDS

Compared with the basic Isomap, m2-Isomap adds step 1
and then modiﬁes the step 2. It should be noted that the edges
of neighborhood graph are weighted by de instead of dm. The
dm is only applied to optimize the neighborhood rather than
applied to estimate the geodesic distance. The step 3 and step
4 remains the same as the basic Isomap. Therefore the time
complexity increases from step 1 where the additional com-
plexity is O(n). However the optimized neighborhood will
speed up the step 4 so as to decrease the running time of the
whole algorithm. This will be proved empiricially in later
experiment.

 

 

 

4 Experimental Results
Several experiments are conducted to compare Isomap with
m2-Isomap in visualization and time complexity.
Isomap
uses the published matlab code [Tenenbaum et al., 2000]. The
m2-Isomap is also implemented in MATLAB 7.0. In exper-
iments, there are two parameters involved. Isomap has a pa-
rameter k to determine the size of local neighborhood whereas
m2-Isomap introduces an additional parameter m that con-
trols the scope of the neighborhood to be modiﬁed, where
k=5-15 and m =1-10 will be explored.

IJCAI-07

2400

A. Swiss Roll Data set
Swiss Roll dataset is widely applied to compare many
non-linear dimensionality reduction approaches [Roweis and
Saul, 2000; Tenenbaum et al., 2000; Balasubramanian and
Schwartz, 2002; Geng et al., 2005]. We take many ran-
dom samples from the Swiss Roll surface shown as Figure 4,
and do visualization experiments on them to compare Isomap
with m2-Isomap in the following cases: (1)well sampled data
without noise (2) well sampled data with noise (3)sparsely
sampled data.

Figure 4: Swiss roll surface sample

The quality of visualization can be evaluated subjectively,
while it is often quantitatively evaluated by the residual vari-
ance [Roweis and Saul, 2000; Tenenbaum et al., 2000; Bal-
asubramanian and Schwartz, 2002; Geng et al., 2005]. The
lower the residual variance is, the better high-dimensional
data are represented in the embedded space.

Experiment 1 On well sampled noiseless data sets

”12/6/0.00059146” indicates that the parameter k takes 12,m
takes 6, and the residual variance is 0.0.00059146. It can be
observed that on well sampled data sets without noise, m2-
Isomap outperforms Isomap. It gets better mapping results
and lower residual variances on any data set.

Experiment 2 On well sampled but noise contaminated

data sets

To compare the topologically stability of Isomap and m2-
Isomap on data sets with noise, we do experiments on the
ﬁrst data set with 1000 points used in experiment 1, but here
it is contaminated by adding random Gaussian noise. The
mean of the noise is 0 and the variance is 0.64. It can be seen
from Figure 6 that Isomap gets the best result with residual
variance 0.001286 when k=6, while m2-Isomap gets the best
result with the lower residual variance 0.0010773 with pa-
rameters k=12 and m=6. Obviously m2-Isomap outperforms
Isomap in terms of visualization and residual variance. It can
be also observed from Figure 6 that as k increases, the results
of Isomap change drastically, but those of m2-Isomap do not
change much. This indicate that m2-Isomap is also less sen-
sitive to k than Isomap.

Figure 5: The mapped results of two approaches on ﬁve well
sampled noiseless data sets

To make comparison on well sampled noiseless data sets,
ﬁve sample data sets with 1000 points are sampled randomly
from Swiss Roll surface. Firstly on the ﬁrst sampled data
set, two approaches are tested to choose the optimal param-
eters in the given scope, and then these parameters are ap-
plied to run the remaining four data sets. The mapped results
from left to right corresponding to ﬁve data sets are shown
as Figure 5, where the parameters and residual variance for
each data set are added as caption. For example on the ﬁrst
data set,in Figure 5(a)Isomap, the caption ”6/0.0015475” in-
dicates that the parameter k takes 6 whereas the residual vari-
ance is 0.0015475.
In Figure 5(b)m2-Isomap, the caption

Figure 6: The mapped results of two approaches on a well
sampled but noise contaminated data set

Experiment3 On sparsely sampled data sets
Generally, in sparsely sampled data sets, the Euclidean
distance between points in neighborhood becomes larger as
compared to the distance between different folds of the man-
ifold. This easily makes two approaches faces the problem of
short-circuiting. This experiment makes comparison between
two approaches about robust to samples with different sam-
pling density. Five data sets with 300,400,500,600 and 700
points respectively are randomly sampled from Swiss Roll
surface. Two approaches are tuned to select the best visu-
alization results on each data set. It can be observed from
Figure 7 that m2-Isomap outperforms Isomap on each datset
in terms of visualization performance and residual variance.

B. Face image data set
This data set consists of 698 images (each contains 64 × 64
pixels) of a human face rendered with different poses and
lighting directions [Tenenbaum et al., 2000]. To compare two
approaches on sparsity data set, we choose 350 (half of the
data set)images from this data set to form a new data set. The

IJCAI-07

2401

0.5

0.4

0.3

0.2

0.1

e
c
n
a
i
r
a
v
 
l

i

a
u
d
s
e
R

Isomap
m2−Isomap

0
1

2

3

4

5
6
Dimension

7

8

9

10

Figure 10: The residual variance of two approaches along the
dimension on face images varying in pose and illumination
(N=350).

residual variances and the correct intrinsic dimensionality of
data set, while Isomap gives the higher estimate to intrinsic
dimensionality than that of the data set. The intrinsic dimen-
sionality of the data can be estimated by looking for the ”el-
bow” at which this curve ceases to decrease signiﬁcantly with
added dimensions.

C. Time complexity comparison
We generate 10 data sets with the same size randomly sam-
pled from Swiss Roll surface. Two approaches run on these
ten data sets respectively and then average running time for
two approaches are calculated. And then the experiments are
performed on different sizes: 500 900 1300 1700 2100 2500
2900 3300. It can be observed from Table 1 that m2-Isomap
also exceeds the Isomap in time complexity. The larger the
data set size is, the superiorities of m2-Isomap is more obvi-
ous. This is because good neighborhood structure of data set
is beneﬁcial to speed up the subsequent optimization process.

5 Conclusion

This paper applies the graph algebra to build the neighbor-
hood graph for Isomap. The improved Isomap outperforms
the classic Isomap in visualization and time complexity. It
also has stronger topological stability and less sensitive to pa-
rameters. This indicates that the more complicated or even
time-consuming approaches can be applied to construct the
better neighborhood structure whilst the whole time complex-
ity will not raise. Because the transitivity of similarity is
not absolutely correct for any data set, in the future, we will
apply the fuzzy theory and probability theory to deﬁne the
graph algebra and then to build the neighborhood graph. And
besides, the proposed approach should be combined with k-
edge-connected spanning subgraph so as to guarantee to build
the connected neighborhood graph for any data set.

Acknowledgments

This work is supported by Natural Science Foundation
of China(60003019),Hubei Science and Technology Project
(2005AA101C17),and UK Engineering and Physical Sci-
ences Research Council under grant number GR/N15764/01.

Figure 7: The mapped results of two approaches on ﬁve
sparsely sampled data sets

mapped results are shown in Figure 8 and Figure 9.

In ﬁg-

100

80

60

40

20

0

−20

−20

100

80

60

40

20

0

−20

−20

0

20

40

60

80

100

120

140

Figure 8: Isomap (k=5)

0

20

40

60

80

100

120

140

160

Figure 9: m2-Isomap (k=5 m=1)

ures, the x-axis represents the left-right poses of the faces,
and the y-axis represents the up-down poses of the faces. The
corresponding points of the successive images from left to
right in the middle are marked by circles and linked by lines.
The nine critical face samples are marked by plus at the left-
bottom corner of each image indicates the point representing
the image. It can be observed from these ﬁgures that Isomap
can hardly reveal the different face poses. The middle left-
right line is heavily curved, and the arrangement of the nine
face samples is tangle some. By contrast, m2-Isomap puts the
middle left-right line better. The nine face samples are also
mapped to the approximately right positions corresponding
to the face poses. These indicate that m2-Isomap performs
better than Isomap.

This also can be supported from their residual variances. It
can be observed from Figure 10 that m2-Isomap gets lower

IJCAI-07

2402

Table 1: Comparison of time complexity (seconds)

Datasize → 500
9.7
9.5

Isomap
m2-Isomap

900
56.2
55.8

1300
166.9
166.4

1700
375.5
373.4

2100
707.9
708.2

2500
1205.0
1199.4

2900
1913.2
1907.8

3300
2833.0
2819.3

References

[Abusham et al., 2005] E.E. Abusham, D. Ngo, and A. Teoh.
Fusion of locally linear embedding and principal compo-
nent analysis for face recognition (ﬂlepca). Lecture Notes
in Computer Science, 3687:326–333, 2005.

[Balasubramanian and Schwartz, 2002] M.

nian and E.L. Schwartz.
topological stability. Science, 295(4):7, 2002.

Balasubrama-
The isomap algorithm and

[B.Carre, 1979] B.Carre. Graphs and Networks. Oxford

University Press, 1979.

[Belkin and Niyogi, 2003] M. Belkin and P. Niyogi. Lapla-
cian eigenmaps for dimensionality reduction and data rep-
resentation. Neural Computing, 15(6):1373–1396, 2003.

[Chang et al., 2004] J.P. Chang, H.X. Shen, and Z.H. Zhou.
Uniﬁed locally linear embedding and linear discriminant
analysis algorithm (ullelda) for face recognition. Lecture
Notes in Computer Science, 3338:296–304, 2004.

[de Ridder et al., 2003] D. de Ridder, O. Kouropteva, and
O. Okun. Supervised locally linear embedding. Lecture
Notes in Artiﬁcial Intelligence, 2714:333–341, 2003.

[Donoho and Grimes, 2003] D.L. Donoho and C. Grimes.
Hessian eigenmaps: Locally linear embedding, techniques
for high-dimensional data. Proc.Natl.Acad. Sci. U. S. A,
100(10):5591–5596, 2003.

[Fischer and Buhmann, 2003a] B. Fischer and J.M. Buh-
mann. Bagging for path-based clustering.
IEEE Trans.
Pattern Analysis and Machine Intelligence, 25(1):1411–
1415, 2003.

[Fischer and Buhmann, 2003b] B. Fischer and J.M. Buh-
Path-based clustering for grouping of smooth
mann.
curves and texture segmentation.
IEEE Trans. Pattern
Analysis and Machine Intelligence, 25(4):513–518, 2003.

[Geng et al., 2005] X. Geng, D.C. Zhan, and Z.H. Zhou. Su-
pervised nonlinear dimensionality reduction for visualiza-
tion and classiﬁcation.
IEEE Transactions on systems,
man, and cybernetics-part B: cybernetics, 35(6):1098–
1107, 2005.

[Jenkins and Mataric, 2004] O. Jenkins and M. Mataric. A
spatio-temporal extension to isomap nonlinear dimension
reduction.
In Proceedings of the Twenty-First Interna-
tional Conference on Machine Learning, pages 441–448,
Alberta, Canada, 2004.

[Kouropteva and Pietikainen, 2005] O. Kouropteva

and
Incremental locally linear embedding.

M. Pietikainen.
Pattern Recognition, 38:1764–1767, 2005.

[Kouropteva et al., 2002] O. Kouropteva, O. Okun, and
M. Pietikainen. Selection of the optimal parameter value

for the locally linear embedding algorithm. In Proceed-
ings of 1th International Conference on Fuzzy Systems and
Knowledge Discovery, pages 359–363, Singapore, 2002.

[Law and Jain, 2006] M.H.C. Law and A.K. Jain. Incremen-
tal nonlinear dimensionality reduction by manifold learn-
ing.
IEEE Trans. Pattern Analysis and Machine Intelli-
gence, 28:377–391, 2006.

[Roweis and Saul, 2000] S.T. Roweis and L.K. Saul. Nonlin-
ear dimensionality reduction by locally linear embedding.
Science, 290(5500):2323–2326, 2000.

[Saxena et al., 2004] A. Saxena, A. Gupta, and A. Muker-
jee. Non-linear dimensionality reduction by locally linear
isomaps. In Proceedings of 11th International Conference
on Neural Information Processing, pages 1038–1043, Cal-
cutta, India, 2004.

[Shao and Huang, 2005] C. Shao and H.K. Huang. Selec-
tion of the optimal parameter value for the isomap algo-
rithm. Lecture Notes in Artiﬁcial Intelligence, 3789:396–
404, 2005.

[Silva and Tenenbaum, 2003] V.D. Silva and J. B. Tenen-
baum. Global versus local methods in nonlinear dimen-
sionality reduction. Neural Information Processing Sys-
tems, 15:705–712, 2003.

[Tenenbaum et al., 2000] J.B. Tenenbaum, V. de Silva, and
J.C. Langford. A global geometric framework for nonlin-
ear dimensionality reduction. Science, 290(5500):2319–
2323, 2000.

[Vlachos et al., 2002] M. Vlachos, C. Domeniconi, and
D. Gunopulos et al. Non-linear dimensionality reduction
techniques for classiﬁcation and visualization. In Proceed-
ings of Knowledge Discovery and Data Mining, Edmon-
ton, Canada, 2002.

[Weng et al., 2005] S. Weng, C. Zhang, and Z. Lin. Explor-
ing the structure of supervised data by discriminant iso-
metric mapping. Pattern Recognition, 38:599– 601, 2005.

[Xiao et al., 2005] J. Xiao, Z.T. Zhou, and D.W. Hu. Self-
organized locally linear embedding for nonlinear dimen-
sionality reduction. Lecture Notes in Computer Science,
3610:101–109, 2005.

[Yang, 2004] L. Yang.

k-edge connected neighborhood
graph for geodesic distanc estimation and nonlinear data
projection.
In Proc. 17th Intl Conf. PatternRecognition,
pages 196–199, 2004.

[Yang, 2005] L. Yang. Building k edge-disjoint spanning
trees of minimum total length for isometric data embed-
ding. IEEE Transactions of Pattern Analysis and Intelli-
gence, 27:1680–1683, 2005.

IJCAI-07

2403


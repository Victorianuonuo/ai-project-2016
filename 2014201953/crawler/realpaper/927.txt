Learning Policies for Embodied Virtual Agents Through Demonstration

Jonathan Dinerstein

DreamWorks Animation

jondinerstein@yahoo.com

Parris K. Egbert Dan Ventura

CS Department, Brigham Young University

{egbert,ventura}@cs.byu.edu

Abstract

Although many powerful AI and machine learning
techniques exist, it remains difﬁcult to quickly cre-
ate AI for embodied virtual agents that produces
visually lifelike behavior. This is important for ap-
plications (e.g., games, simulators, interactive dis-
plays) where an agent must behave in a manner that
appears human-like. We present a novel technique
for learning reactive policies that mimic demon-
strated human behavior. The user demonstrates the
desired behavior by dictating the agent’s actions
during an interactive animation. Later, when the
agent is to behave autonomously, the recorded data
is generalized to form a continuous state-to-action
mapping. Combined with an appropriate animation
algorithm (e.g., motion capture), the learned poli-
cies realize stylized and natural-looking agent be-
havior. We empirically demonstrate the efﬁcacy of
our technique for quickly producing policies which
result in lifelike virtual agent behavior.

1 Introduction

An Embodied Virtual Agent, or EVA, is a software agent with
a body represented through computer graphics [Badler et al.,
1999; Dinerstein et al., 2006]. If given sufﬁcient intelligence,
these characters can animate themselves by choosing actions
to perform (where each action is a motion). Applications of
these agents include computer games, computer animation,
virtual reality, training simulators, and so forth.

Despite the success of EVAs in certain domains, some im-
portant arguments have been brought against current tech-
niques. In particular, programming the agent AI is difﬁcult
and time-consuming, and it is challenging to achieve natural-
looking behavior. We address these issues by learning a pol-
icy for controlling the agent’s behavior that is:

1. Scalable — The policy learning must scale to problems

with continuous and high-dimensional state spaces.

2. Aesthetic — The virtual agent behavior must appear

natural and visually pleasing.

3. Simple — The policy learning must be directable by a

non-technical user.

Reinforcement learning (RL) is a powerful scheme and an
obvious choice for policy learning. However, it does not meet
our requirements — RL does not always scale well to contin-
uous state spaces of high dimensionality (which are common
for EVAs), and it is challenging to encode aesthetic goals into
a reward structure. We take a different approach.

We present a technique for automatic learning of a policy
by mimicking demonstrated human behavior. This technique
not only provides a natural and simple method for the con-
struction of a policy, but it also allows an animator (or other
non-technical person) to be intimately involved in the con-
struction of the policy. We learn a reactive policy rather than
more advanced behavior because this learning is simple and
nearly automatic, thereby allowing for AI creation by non-
technical users. This approach is empirically shown to be ef-
fective at quickly learning useful policies whose behavior ap-
pears natural in continuous state (and action) spaces of high
dimensionality.

2 Related Work
Designing and developing embodied virtual agents is funda-
mentally a multi-disciplinary problem [Gratch et al., 2002;
Cole et al., 2003]. Indeed the study of EVAs overlaps with
several scientiﬁc ﬁelds, including artiﬁcial intelligence, com-
puter graphics, computer-human interfaces, etc.

The computer graphics community has been interested in
EVAs since Reynold’s seminal work on ﬂocking behavior
[Reynolds, 1987]. One of the most well-known examples in
ﬁlm is the Lord of the Rings trilogy [Duncan, 2002], depict-
ing epic-scale battle scenes of humanoids. Such large-scale
animation would be implausible if the characters were ani-
mated manually. Several notable techniques for constructing
EVAs have been developed (e.g., [Monzani et al., 2001]) but
the explicit programming of agent AI remains a difﬁcult task.
Game-oriented EVA research includes the development
of action selection and learning architectures [Laird, 2001].
Computational models of emotion have also been proposed
[Gratch and Marsella, 2005]. Recent work has examined
rapid behavior adaption such that an EVA can better inter-
act with a given human user [Dinerstein and Egbert, 2005;
Dinerstein et al., 2005]. In other work, a scheme has been de-
veloped whereby a non-embodied agent in a text-based com-
puter game learns through interaction with multiple users [Is-
bell et al., 2001].

IJCAI-07

1257

As discussed in the introduction, designing and program-
ming EVA AI is often challenging. Several authors have ad-
dressed this through reinforcement learning (a survey is given
in [Dinerstein et al., 2006]). RL is a natural choice since it has
long been used for agent policy learning. However, RL does
not meet our requirements in this paper. For example, it is
quite challenging to achieve natural-looking behavior since
these aesthetic goals must be integrated into the ﬁtness func-
tion. Also, many EVAs live in continuous virtual worlds and
thus may require continuous state and/or action spaces, such
as ﬂocking EVAs [Reynolds, 1987]. Traditional RL is in-
tractible for large or continuous state/action spaces, though
recent research has begun to address this problem [Ng and
Jordan, 2000; Wingate and Seppi, 2005]. Nevertheless, the
learning is still computationally expensive and limited. Our
technique learns stylized EVA behaviors quickly as empiri-
cally shown later.

The agents and robotics communities have long recog-
nized the need for simpliﬁed programming of agent AI.
There has been some interesting work performed in pro-
gramming by demonstration, such as [Pomerleau, 1996;
van Lent and Laird, 2001; Mataric, 2000; Kasper et al., 2001;
Angros et al., 2002; Nicolescu, 2003], where an agent is in-
structed through demonstrations by a user. Our technique ﬁts
in this category but is speciﬁcally designed for EVAs trained
by non-technical users.
In contrast to these existing tech-
niques, our approach does not require signiﬁcant effort from
a programmer and domain expert before demonstration can
be performed. The existing technique that is most related to
our work is “Learning to Fly” [Sammut et al., 1992]. How-
ever, our technique is unique in many aspects, in particular
because it performs conﬂict elimination, can operate in con-
tinuous action spaces, and can be operated by non-technical
users. There has been some previous work in conﬂict elim-
ination through clustering [Friedrich et al., 1996], but that
approach quantizes the training examples and culls many po-
tentially useful cases.

Contribution: We present a novel scheme for program-
ming EVA behavior through demonstration. While many of
the components of our scheme are pre-existing, our overall
system and application are unique. Moreover, some notable
and critical components of our scheme are novel. Speciﬁc
contributions made in this paper include:

• The application of agent programming-by-demonstra-

tion concepts to policy learning for humanoid EVAs.

• An intuitive interface for non-technical users to quickly

create stylized and natural-looking EVA behavior.

• A novel conﬂict elimination method (previous tech-
niques in agents/robotics have usually ignored or
blended conﬂicts).

3 EVA Programming by Demonstration
Learning an EVA policy through demonstration involves the
following steps (see Figure 1):

1. Train:

(a) Observe and record state-action pairs.

Figure 1: The workﬂow of our technique. (a) The simula-
tor, which drives the animation, provides a current state sss.
This is visualized for the demonstrator. The demonstrator re-
sponds with an action aaa, which is recorded and used to up-
date the simulation. (b) The state-action examples are pro-
cessed to eliminate conﬂicts and then generalized into a con-
tinuous policy function. This policy is used online to control
the agent.

(b) Eliminate conﬂicts.
(c) Generalize state-action pairs into a policy μ.

2. Autonomous behavior:

(a) Use μ to compute decisions for the agent, once per

ﬁxed time step Δt.

This is a process of learning to approximate intelligent de-
cision making from human example. We formulate decision
making as a policy:

μ : sss → aaa

(1)
where sss ∈ Rn is a compact representation of the current state
of the character and its world, and aaa ∈ Rm is the action chosen
to perform. Each component of sss is a salient feature deﬁning
some important aspect of the current state. If the character
has a ﬁnite repertoire of possible actions, then aaa is quantized.
A state-action pair is denoted (cid:4)sss, aaa(cid:5). The demonstrator’s
behavior is sampled at a ﬁxed time step Δt, which is equal
to the rate at which the character will choose actions when it
is autonomous. Thus the observed behavior is a set of dis-
crete cases, B = {(cid:4)sss, aaa(cid:5)1, . . . , (cid:4)sss, aaa(cid:5)q}. Each pair represents
one case of the target behavior. There is no ordering of the
pairs in the set. We construct μ by generalizing these cases.
Speciﬁcally, to ensure that the character’s behavior is smooth
and aesthetically pleasing, we construct μ by interpolating
the actions associated with these cases. Because sss ∈ Rn and
aaa ∈ Rm, we can theoretically use any continuous real-vector-
valued interpolation scheme. In our implementation, we ei-
ther use ε-regression SVM or continuous k-nearest neighbor,
as detailed later. If the action set is symbolic, we use voting
k-nearest neighbor.

For generalization of cases to succeed, it is critical that the
state and action spaces be organized by the programmer such
that similar states usually map to similar actions.
In other
words, (cid:6)sssi − sss j(cid:6) < α ⇒ (cid:6)aaai − aaa j(cid:6) < β, where α and β are
small scalar thresholds and (cid:6) · (cid:6) is the Euclidean metric. Cer-
tainly, this constraint need not always hold, but the smoother
the mapping, the more accurate the learned policy μ will be.

3.1 Training

In training mode, both the demonstrator and programmer
need to be involved. First, the programmer integrates our
technique into an existing (but thus far “brainless”) character.

IJCAI-07

1258

This integration involves designing the state and action spaces
such that μ will be learnable. Once integration is complete,
the demonstrator is free to create policies for the character
at will. In fact, the creation of multiple policies for one agent
may be interesting to achieve different stylized behaviors, etc.
Demonstration proceeds as follows. The character and its
virtual world are visualized in real-time, and the demonstra-
tor has interactive control over the actions of the character.
Note that the continuous, real-time presentation of state in-
formation to the demonstrator is critical to making the char-
acter training process as natural as possible, as this is analo-
gous to how humans naturally perceive the real world. As the
simulation-visualization of the character and its world pro-
ceeds in real-time, the demonstrator supplies the character
with the actions it is to perform. This information is saved
in the form of state-action pairs. Once enough state-action
examples have been collected, all conﬂicting examples are
automatically eliminated, as described below. We now have a
discrete but representative sampling of the entire policy func-
tion μ. Moreover, because the demonstrator has had control
of the character, she has forced it into regions of the state
space of interest — therefore the density of the sampling cor-
responds to the importance of each region of the state space.
Elimination of conﬂicting examples is important because
human behavior is not always deterministic, and therefore
some examples will likely conﬂict. This is an important issue,
because the machine learning schemes we utilize will “aver-
age” conﬂicting examples. This can result in unrealistic or
unintelligent-looking behavior. Note that conﬂict resolution
might also be possible by augmenting the state with context
(e.g. the last n actions performed). However, this increase in
the state-space dimensionality will only make the learning μ
more challenging and require additional training examples.
Thus we have designed our technique to directly eliminate
conﬂicts.

Conﬂicting examples are formally deﬁned as:
if (cid:6)sssi − sss j(cid:6) < ν and (cid:6)aaai − aaa j(cid:6) > υ, then conflict,

(2)
where ν and υ are scalar thresholds. To eliminate conﬂicts,
we cannot arbitrarily delete cases involved in a conﬂict —
this can lead to high frequencies in the policy. Our goal is to
remove those examples that represent high frequencies.

Pseudo-code for our conﬂict elimination technique is given
in Figure 2. To complement this pseudo-code, we now de-
scribe our technique. In brief, each state-action pair is tested
in turn to determine whether it is an outlier. First, the l neigh-
bors (according to state) of the current example are found
and their median action aaavm is computed, using the follow-
ing vector-median method [Koschan and Abidi, 2001]:

(cid:2)

(cid:3)

aaavm = arg

aaai

min
,aaa2

∈{aaa1

, ...,aaal

l

∑
j=1

}

(cid:6)aaai − aaa j(cid:6)

(3)

In other words, aaavm is equal to the action of the neighbor that
is the closest to all other actions of the neighbors according
(cid:8)(cid:6) > η, then the
to the Euclidean metric. Finally, if (cid:6)aaavm – aaa
case is an outlier and is marked for deletion. Marked cases are
retained for testing the other cases, and then are all deleted as
a batch at the conclusion of the conﬂict elimination algorithm.

For each recorded pair (cid:4)sss, aaa(cid:5)(cid:8)

...

Find l closest neighbors of (cid:4)sss, aaa(cid:5)(cid:8)
Compute median action aaavm of l neighbors using Eq. 3
if ((cid:6)aaavm – aaa

(cid:8)(cid:6) > η)

Mark (cid:4)sss, aaa(cid:5)(cid:8)

Delete all marked pairs

Figure 2: Conﬂict elimination algorithm.

In our experiments, we have found that it works well to use
l = 5. We use a threshold η of about 10% of the possible
range of component values in aaa.

3.2 Autonomous Behavior

Once an adequate set of state-action pairs has been collected,
we must construct the continuous policy, μ : sss → aaa. We
do this through one of two popular machine learning tech-
niques: ε-regression SVM [Haykin, 1999] or k-nearest neigh-
bor [Mitchell, 1997] (or through a classiﬁcation scheme if the
actions are symbolic). We have chosen these two techniques
because they are powerful and well-established, yet have con-
trasting strengths and weaknesses in our application.

The primary strength of continuous k-nearest neighbor (k-
nn) is that there are strong guarantees about its accuracy, as
it merely interpolates local cases (e.g., it can robustly handle
non-smooth mappings, and the outputs will be within the con-
vex hull of the k local cases). Therefore we use k-nn when-
ever the demonstrated policy is rough. However, k-nn does
have a large memory footprint (∼1 MB per policy), and more
state-action cases are required than with SVM due to weaker
generalization.

The support vector machine (SVM) is an interesting alter-
native to k-nn. It is a compact and global technique. As a
result, it performs powerful generalization, but can struggle
with highly non-smooth mappings. We have found SVM to
be useful when μ is somewhat smooth, especially when it is
C0 or C1 continuous. We use the popular ε-regression scheme
with a Gaussian kernel to perform function approximation.

4 Experimental Results

These experiments were designed to cover, in a general fash-
ion, most of the major distinguishing aspects of popular uses
of embodied virtual agents. Between our favorable experi-
mental results, and the results from the agents/robotics litera-
ture on programming-by-demonstration (see the related work
section), we have some empirical evidence that our scheme is
a viable tool for creating some popular types of EVA policies.
The results we achieved in our experiments are summa-
rized in Table 1. Comparisons against reinforcement learn-
ing, A*, and PEGASUS are summarized in Table 2 and dis-
cussed in more detail later (while we do not take an RL ap-
proach in our technique, we believe it is valuable to compare
against RL since it such a popular approach). The results of
an informal user case study are reported in Table 3. Videos of
the animation results are available from:

http://rivit.cs.byu.edu/a3dg/

publications.php

IJCAI-07

1259

Pre- Number of state-action pairs
Time to demonstrate all pairs
Time to eliminate conﬂicts

k-nn Compute target function values

Storage requirements
Total decision-making time to cross ﬁeld
Total animation time to cross ﬁeld

SVM Time to train ε-regression SVM

Compute new target function values
Storage requirements
Total decision-making time to cross ﬁeld
Total animation time to cross ﬁeld

6,000
8 min.
16 sec.
13 μsec.
∼1 MB
7.04 msec.
36.1 sec.

2.5 min.
2 μsec.
∼10 KB
1.13 msec.
37.7 sec.

Table 1: Summary of average usage and performance results
in our spaceship pilot case study (with a 1.7 GHz processor,
512 MB RAM, k ∈ [2, 7]). Similar results were achieved in
other experiments.

4.1 Spaceship Pilot

In our ﬁrst experiment, the virtual agent is a spaceship pilot
(see Figure 3). The pilot’s task is to maneuver the spaceship
through random asteroid ﬁelds, ﬂying from one end of the
ﬁeld to the other as quickly as possible with no collisions.
The animation runs at 15 frames per second, with an action
computed for each frame. The virtual pilot controls the ori-
entation of the spaceship.

μ is formulated as follows. The inputs are the space-
ship’s current orientation (θ, φ), and the separation vector
between the spaceship and the two nearest asteroids (ps −
pa1 and ps − pa2). Thus the complete state vector is sss =
(cid:4)θ, φ, Δx1, Δy1, Δz1, Δx2, Δy2, Δz2(cid:5). There are two outputs,
which determined the change in the spaceship’s orientation:
aaa = (cid:4)Δθ, Δφ(cid:5).

We achieved good results in this experiment, as shown in
Table 1 and in the accompanying video. The SVM and k-nn
policies worked well for all random asteroid ﬁelds we tried,
and the animation was natural-looking, intelligent, and aes-
thetically pleasing (veriﬁed in an informal user case study).

To gain a point of reference and to illuminate the interest-
ing computational properties of our technique, we analyzed
learning the policy through reinforcement learning (Table 2).
This analysis helps validate why a non-RL approach is im-
portant in fulﬁlling our goals. It is straightforward to formu-
late our case study as an MDP. The state and action spaces
have already been deﬁned. We have a complete model of the
deterministic environment (needed for animation), the initial
state is a position on one side of the asteroid ﬁeld, and the
reward structure is positive for safely crossing the ﬁeld and
negative for crashing. However, even using state-of-the-art
table-based techniques for solving large MDP’s [Wingate and
Seppi, 2005], it is currently implausible to perform RL with a
continuous state space of dimensionality greater than 6 (this
size even requires a supercomputer). Our MDP state space
dimensionality is 9. Even gross discretization is intractable:
e.g., for 10 divisions per axis, 109 states. The best result
reported in [Wingate and Seppi, 2005] is a 4-dimensional
state space discretized into approximately 107.85 states. We
also experimented with learning a policy through function-

Figure 3: Spaceship pilot test bed. The pilot’s goal is to cross
the asteroid ﬁeld (forward motion) as quickly as possible with
no collisions.

approximation-based RL using a multi-layer neural net, but
this failed (the learned policies were very poor). This type of
learning is typically considered most stable when employing
a linear function approximator, but this can limit the learning
[Tesauro, 1995; Sutton and Barto, 1998]. Thus, for our ap-
plication, demonstration-based learning is appealing since it
is effective in high-dimensional, continuous state and action
spaces.

We also tested a non-standard form of RL designed specif-
ically for continuous state spaces: PEGASUS [Ng and Jor-
dan, 2000; Ng et al., 2004]. In PEGASUS, a neural network
learns a policy through hill climbing. Speciﬁcally, a small
set of ﬁnite-horizon scenarios is used to evaluate a policy
and determine the ﬁtness landscape. Unlike traditional RL,
PEGASUS succeeded in learning an effective policy in our
case study (see Table 1). Interestingly, the performance of the
policies learned through our scheme and PEGASUS are sim-
ilar. However, PEGASUS learning was far slower than our
technique (hours versus minutes). Moreover, we found it ex-
tremely difﬁcult and technical to tune the PEGASUS reward
structure for aesthetics. Thus, while PEGASUS is a powerful
and useful technique, it does not fulﬁll the requirement given
in the introduction of providing non-technical users with an
intuitive and robust interface for programming EVA AI. As a
result, RL does not appear to be a good ﬁt for our application.
We also analyzed explicit action selection through A* (Ta-
ble 2). A* has empirically proven successful, but requires
signiﬁcantly more CPU than our policies to make decisions,
sometimes produces behavior that does not appear natural,
and requires the programmer to develop an admissible heuris-
tic.

4.2 Crowd of Human Characters

In our next experiment we created policies to control groups
of human characters (see Figure 4). The characters milled
about, then a boulder fell from the sky and they ran away.
The policies controlled the decision making of the charac-
ters (i.e., “turn left,” “walk forward,” etc), while the nuts-and-
bolts animation was carried out by a traditional skeletal mo-
tion system. Thus the policy only speciﬁed a small set of
discrete actions (the real-valued policy output was quantized
to be discrete). We created several crowd animations. In each
animation, all characters used the same policy, showing the
variety of behavior that can be achieved. Note that each pol-
icy we constructed only required a few minutes to capture all
necessary state-action examples, and only a few thousand ex-
amples were required for use online.

To train agents to interact in a crowd we performed sev-

IJCAI-07

1260

RL

Storage requirements
Time to learn policy

PEG.

Storage requirements
Time to learn policy
Time to select actions
Total decision-making time to cross ﬁeld
Total animation time to cross ﬁeld

A* Average storage requirement

Max storage requirement
Time to select actions
Total decision-making time to cross ﬁeld
Total animation time to cross ﬁeld

∼8 G
∼1 week
∼5 KB
∼2.3 hrs.
2 msec.
6.95 sec.
36.6 sec.
∼8 M
∼370 M
0.2 sec.
104.1 sec.
34.7 sec.

Table 2: Summary of average results using reinforcement
learning, PEGASUS, and A* in the spaceship pilot case
study. Compare to Table 1. The purpose of this comparison
to help validate why we have selected demonstration-based
learning for our problem domain. Reinforcement learning
takes a prohibitive amount of time with static or adaptive
table-based learning, and function-approximation-based RL
did not learn effective policies in our experiments. PEGA-
SUS successfully learns a useful policy in our case study, but
requires signiﬁcantly more learning time than our approach,
and it is difﬁcult to tune the learning for aesthetics. A* takes
far more CPU time to make decisions than the policy learned
through our technique, and requires an admissible heuristic.
Thus, from a computational standpoint, our technique is an
interesting alternative to these traditional schemes for agent
action selection.

Figure 4: Virtual human crowd test bed.

eral brief demonstration iterations, randomly placing some
static characters in a scene. The human trainer then guided
the learning agent through the scene. Thus the agent learned
how to behave in response to other agents around it in arbi-
trary positions.

We created two policies. The ﬁrst was used before the
boulder impact, and the second afterwards. In the ﬁrst pol-
icy (pre-boulder), the environment was discretized into a 2D
grid. sss was formulated as the current orientation of the char-
acter, and the separation between it and the next three clos-
est characters. Decision making was computed at a rate Δt
of about twice per second. After the boulder hit the ground,
we switched to another policy (with continuous state/action
spaces) which simply directed the characters to run straight
away from the boulder. This modularity made the policy
learning easier.

5 Discussion

Our technique has a number of strong points. As has been
shown empirically, policies can be learned rapidly. The aes-

Test bed User

Instruct. time Dem. time

Space ship Author

Artist
Game player

Crowd Author

Designer
Student

N.A.
3 min.
1 min.

N.A.
4 min.
3 min.

7 min.
9.5 min.
8.5 min.

12 min.
13 min.
13.5 min.

Table 3: Summary of an informal user case study. Instruct.
time is how long a new user needed to be tutored on the use
of our system. Our technique is intuitive (little instruction re-
quired) and user-directed policies can be quickly constructed.
The users in this study prefered the aesthetics of their demon-
strated policies over traditional policies.

thetic value of these policies was veriﬁed through informal
user case studies (Table 3). Also, its use is natural and
straightforward, it is applicable to non-technical users, and
there is empirical evidence that it is a viable tool for current
uses of EVAs (e.g., computer games and animation). In ad-
dition, our approach has a nearly ﬁxed online execution time,
which is a useful feature for interactive agents.

Improvements to our approach that are the subject of ongo-
ing research include the following. First, note that a demon-
strator’s choice of actions may not make the character traverse
all regions of the state space, leaving gaps in μ. This problem
can be automatically solved, during training, by periodically
forcing the character into these unvisited regions of the state
space. Another important issue is perceptual aliasing, which
can result in unanticipated character behavior. However, per-
ceptual aliasing can be minimized through effective design of
the compact state space.

We have opted for learning standard policies (no task
structure or rules, e.g. [Nicolescu, 2003]) because it allows
for the most work to be done through programming-by-
demonstration versus explicit programming. However, the
scalability of policies is limited. Our technique could be
augmented with a task learning method from the literature
and thereby gain additional scalability and planning behavior,
but at the cost of greater programmer involvement. A good
example is our crowd case study. Pre/post conditions (e.g.
[van Lent and Laird, 2001]) for the discrete contexts could be
learned, determining when to switch between policies.

Because our current technique learns deterministic poli-
cies, there are some interesting behaviors that it cannot learn.
For example, complex goal-directed behaviors that must be
broken down into sub-tasks. Moreover, our technique is lim-
ited to policies that are smooth state-action mappings.

Conﬂict elimination is important because demonstrator be-
havior may be non-deterministic, and conﬂicts may arise due
to varying delays in demonstrator response time. These con-
ﬂicts can result in unintended behavior or temporal aliasing
(dithering) in the behavior. Our conﬂict elimination tech-
nique safely removes high frequencies in the state-action
pairs, helping make μ a smooth and representative mapping.
Also, unlike elimination through clustering [Friedrich et al.,
1996], our method does not quantize or eliminate examples.
As an alternative to eliminating conﬂicts, one may consider

IJCAI-07

1261

allowing disjunctive action rules or incorporating probabilis-
tic actions, perhaps weighted by how often each conﬂicting
action is seen during the demonstration period.

Policies can be incrementally constructed or adapted by
adding new state-action pairs. If desired, older examples can
be deleted, or conﬂict elimination can be run to cull less ap-
plicable examples.

References
[Angros et al., 2002] R Angros, L Johnson, J Rickel, and
A Scholer. Learning domain knowledge for teaching pro-
cedural skills. In Proceedings of AAMAS’02, 2002.

[Badler et al., 1999] N Badler, M Palmer, and R Bindi-
ganavale. Animation control for real-time virtual humans.
Communications of the ACM, 42(8):65–73, 1999.

[Cole et al., 2003] R Cole, S van Vuuren, B Pellom, K Ha-
cioglu, J Ma, and J Movellan. Perceptive animated in-
terfaces: ﬁrst step toward a new paradigm for human-
computer interaction. Proceedings of the IEEE, 91(9),
2003.

[Dinerstein and Egbert, 2005] J Dinerstein and P K Egbert.
Fast multi-level adaptation for interactive autonomous
characters. ACM Transactions on Graphics, 24(2):262–
288, 2005.

[Dinerstein et al., 2005] J Dinerstein, D Ventura, and P K
Egbert. Fast and robust incremental action prediction for
interactive agents. Computational Intelligence, 21(1):90–
110, 2005.

[Dinerstein et al., 2006] J Dinerstein, P K Egbert, and David
Cline. Enhancing computer graphics through machine
learning: A survey. The Visual Computer, 22(12), 2006.

[Duncan, 2002] J Duncan. Ring masters. Cinefex, 89:64–

131, 2002.

[Friedrich et al., 1996] H. Friedrich, S. Munch, R. Dillmann,
S. Bocionek, and M. Sassin. Robot programming by
demonstration (RPD): supporting the induction by human
interaction. Machine Learning, 23(2-3):163–189, 1996.

[Gratch and Marsella, 2005] J Gratch and S Marsella. Eval-
uating a computational model of emotion. Autonomous
Agents and Multi-Agent Systems, 11(1):23–43, 2005.

[Gratch et al., 2002] J Gratch, J Rickel, E Andre, J Cassell,
E Petajan, and N Badler. Creating interactive virtual hu-
mans: some assembly required. IEEE Intelligent Systems,
pages 54–61, 2002.

[Haykin, 1999] S Haykin. Neural Networks: A Comprehen-
sive Foundation. Prentice Hall, Upper Saddle River, NJ.,
2nd edition, 1999.

[Isbell et al., 2001] C Isbell, C Shelton, M Kearns, S Singh,
and P Stone. A social reinforcement learning agent.
In Proceedings of Fifth International Conference on Au-
tonomous Agents, pages 377–384, 2001.

[Kasper et al., 2001] M Kasper, G Fricke, K Steuernagel,
and E von Puttkamer. A behavior-based mobile robot ar-
chitecture for learning from demonstration. Robotics and
Autonomous Systems, 34:153–164, 2001.

[Koschan and Abidi, 2001] A Koschan and M Abidi. A com-
parison of median ﬁlter techniques for noise removal in
color images. In 7th Workshop on Color Image Process-
ing, pages 69–79, 2001.

[Laird, 2001] J Laird.

It knows what you’re going to do:
Adding anticipation to the quakebot.
In Proceedings of
the Fifth International Conference on Autonomous Agents,
pages 385–392, 2001.

[Mataric, 2000] M Mataric. Getting humanoids to move and
imitate. IEEE Intelligent Systems, pages 18–24, July 2000.

[Mitchell, 1997] T Mitchell.

WCB/McGraw-Hill, 1997.

Machine

Learning.

[Monzani et al., 2001] J Monzani, A Caicedo, and D Thal-
Integrating behavioural animation techniques.

mann.
Computer Graphics Forum, 20(3), 2001.

[Ng and Jordan, 2000] A Y Ng and M Jordan. PEGASUS:
A policy search method for large MDPs and POMDPs. In
Proceedings of the Sixteenth Conference on Uncertainty in
Artiﬁcial Intelligence, 2000.

[Ng et al., 2004] A Y Ng, A Coates, M Diel, V Ganapathi,
J Schulte, B Tse, E Berger, and E Liang.
Inverted au-
tonomous helicopter ﬂight via reinforcement learning. In
International Symposium on Experimental Robotics, 2004.

[Nicolescu, 2003] M Nicolescu. A Framework for Learn-
ing From Demonstration, Generalization and Practice in
Human-Robot Domains. PhD thesis, University of South-
ern California, 2003.

[Pomerleau, 1996] D Pomerleau. Neural network vision for
robot driving.
In S Nayar and T Poggio, editors, Early
visual learning, pages 161–181. Oxford University Press,
New York, NY, 1996.

[Reynolds, 1987] C Reynolds. Flocks, herds, and schools:
A distributed behavioral model.
In Proceedings of SIG-
GRAPH 1987, pages 25–34. ACM Press / ACM SIG-
GRAPH, 1987.

[Sammut et al., 1992] C Sammut, S Hurst, D Kedzier, and
In Proceedings of Machine

D Michie. Learning to ﬂy.
Learning, pages 385–393, 1992.

[Sutton and Barto, 1998] R Sutton and A Barto. Reinforce-
ment Learning: An Introduction. MIT Press, Cambridge,
Massachusetts, 1998.

[Tesauro, 1995] G Tesauro. Temporal difference learning in
TD-Gammon. Communications of the ACM, 38(3):58–68,
1995.

[van Lent and Laird, 2001] M van Lent and J Laird. Learn-
ing procedural knowledge through observation.
In Pro-
ceedings of International Conference On Knowledge Cap-
ture, pages 179–186. ACM Press, 2001.

[Wingate and Seppi, 2005] D Wingate and K D Seppi. Pri-
oritization methods for accelerating MDP solvers. Journal
of Machine Learning Research, 6:851–881, 2005.

IJCAI-07

1262


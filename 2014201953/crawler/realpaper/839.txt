Locality Sensitive Discriminant Analysis

∗

Deng Cai

Xiaofei He

Kun Zhou

Department of Computer Science

Yahoo! Research Labs

Microsoft Research Asia

University of Illinois at Urbana Champaign

hex@yahoo-inc.com

kunzhou@microsoft.com

dengcai2@cs.uiuc.edu

Jiawei Han

Hujun Bao

Department of Computer Science

College of Computer Science

University of Illinois at Urbana Champaign

Zhejiang University

hanj@cs.uiuc.edu

bao@cad.zju.edu.cn

Abstract

Linear Discriminant Analysis (LDA) is a popular
data-analytic tool for studying the class relation-
ship between data points. A major disadvantage of
LDA is that it fails to discover the local geometri-
cal structure of the data manifold. In this paper, we
introduce a novel linear algorithm for discriminant
analysis, called Locality Sensitive Discriminant
Analysis (LSDA). When there is no sufﬁcient train-
ing samples, local structure is generally more im-
portant than global structure for discriminant analy-
sis. By discovering the local manifold structure,
LSDA ﬁnds a projection which maximizes the mar-
gin between data points from different classes at
each local area. Speciﬁcally, the data points are
mapped into a subspace in which the nearby points
with the same label are close to each other while the
nearby points with different labels are far apart. Ex-
periments carried out on several standard face data-
bases show a clear improvement over the results of
LDA-based recognition.

1 Introduction

Practical algorithms in supervised machine learning degrade
in performance (prediction accuracy) when faced with many
features that are not necessary for predicting the desired out-
put. An important question in the ﬁelds of machine learning,
knowledge discovery, computer vision and pattern recogni-
tion is how to extract a small number of good features. A
common way to attempt to resolve this problem is to use di-
mensionality reduction techniques. Two of the most popular

∗ The work was supported in part by the U.S. National Science
Foundation NSF IIS-03-08215/IIS-05-13678, Specialized Research
Fund for the Doctoral Program of Higher Education of China (No.
20030335083) and National Natural Science Foundation of China
(No. 60633070). Any opinions, ﬁndings, and conclusions or recom-
mendations expressed in this paper are those of the authors and do
not necessarily reﬂect the views of the funding agencies.

techniques for this purpose are Principal Component Analy-
sis (PCA) and Linear Discriminant Analysis (LDA) [Duda et
al., 2000].

PCA is an unsupervised method. It aims to project the data
along the direction of maximal variance. LDA is supervised.
It searches for the project axes on which the data points of
different classes are far from each other while requiring data
points of the same class to be close to each other. Both of
them are spectral methods, i.e., methods based on eigenvalue
decomposition of either the covariance matrix for PCA or
the scatter matrices (within-class scatter matrix and between-
class scatter matrix) for LDA. Intrinsically, these methods
try to estimate the global statistics, i.e. mean and covari-
ance. They may fail when there is no sufﬁcient number of
samples. Moreover, both PCA and LDA effectively see only
the Euclidean structure. They fail to discover the underlying
structure, if the data lives on or close to a submanifold of the
ambient space.

Recently there has been a lot of interest in geometri-
cally motivated approaches to data analysis in high dimen-
sional spaces. Examples include ISOAMP [Tenenbaum et
al., 2000], Laplacian Eigenmap [Belkin and Niyogi, 2001],
Locally Linear Embedding [Roweis and Saul, 2000]. These
methods have been shown to be effective in discovering the
geometrical structure of the underlying manifold. However,
they are unsupervised in nature and fail to discover the dis-
criminant structure in the data. In the meantime, manifold
based semi-supervised learning has attracted considerable at-
tention [Zhou et al., 2003], [Belkin et al., 2004]. These meth-
ods make use of both labeled and unlabeled samples. The la-
beled samples are used to discover the discriminant structure,
while the unlabeled samples are used to discover the geomet-
rical structure. When there is a large amount of unlabeled
samples available, these methods may outperform traditional
supervised learning algorithms such as Support Vector Ma-
chines and regression [Belkin et al., 2004]. However, in some
applications such as face recognition, the unlabeled samples
may not be available, thus these semi-supervised learning
methods can not be applied.

In this paper, we introduce a novel supervised dimension-

IJCAI-07

708

ality reduction algorithm, called Locality Sensitive Discrim-
inant Analysis, that exploits the geometry of the data mani-
fold. We ﬁrst construct a nearest neighbor graph to model the
local geometrical structure of the underlying manifold. This
graph is then split into within-class graph and between-class
graph by using the class labels. In this way, the geometri-
cal and discriminant structure of the data manifold can be
accurately characterized by these two graphs. Using the no-
tion of graph Laplacian [Chung, 1997], we can ﬁnd a linear
transformation matrix which maps the data points to a sub-
space. This linear transformation optimally preserves the lo-
cal neighborhood information, as well as discriminant infor-
mation. Speciﬁcally, at each local neighborhood, the margin
between data points from different classes is maximized.

The paper is structured as follows: in Section 2, we provide
a brief review of Linear Discriminant Analysis. The Locality
Sensitive Discriminant Analysis (LSDA) algorithm is intro-
duced in Section 3. In Section 4, we describe how to perform
LSDA in Reproducing Kernel Hilbert Space (RKHS) which
gives rise to kernel LSDA. The experimental results are pre-
sented in Section 5. Finally, we provide some concluding
remarks in Section 6.

2 Related Works
The generic problem of linear dimensionality reduction is the
following. Given a set x1, x2, · · · , xm in Rn
, ﬁnd a transfor-
mation matrix A = (a1, · · · , ad) ∈ Rn×d
that maps these m
, · · · , ym in Rd (d (cid:4) n), such
points to a set of points y1
that yi “represents” xi, where yi = AT

, y2

xi.

Linear Discriminant Analysis (LDA) seeks directions that
are efﬁcient for discrimination. Suppose these data points be-
long to c classes and each point is associated with a label
l(xi) ∈ {1, 2, · · · , c}. The objective function of LDA is as
follows:

aopt = arg max

a

T Sba
a
aT Swa

Sb =

mi(μμμi − μμμ)(μμμi − μμμ)T

c(cid:2)
⎛
⎝ mi(cid:2)

i=1

c(cid:2)

(1)

(2)

(3)

⎞
⎠

Sw =

i
(x

j − μμμi)(x

i

j − μμμi)T

i=1

j=1

where μμμ is the total sample mean vector, mi is the number of
samples in the i-th class, μμμi
is the average vector of the i-th
i
j is the j-th sample in the i-th class. We call Sw
class, and x
the within-class scatter matrix and SB the between-class scat-
ter matrix. The basis functions of LDA are the eigenvectors
of the following generalized eigen-problem associated with
the largest eigenvalues:

Sba = λSwa

(4)

Clearly, LDA aims to preserve the global class relationship
between data points, while it fails to discover the intrinsic
local geometrical structure of the data manifold.
In many
real world applications such as face recognition, there may
not be sufﬁcient training samples. In this case, it may not be
able to accurately estimate the global structure and the local
structure becomes more important.

Besides LDA, there is recently a lot of interests in graph
based linear dimensionality reduction. The typical algo-
rithms includes Locality Preserving Projections (LPP, [He
and Niyogi, 2003]), Local Discriminant Embedding (LDE,
[Chen et al., 2005]), Marginal Fisher Analysis (MFA, [Yan et
al., 2005]), etc. LPP uses one graph to model the geometri-
cal structure in the data. LDE and MFA are essentially the
same. Both of them uses two graphs to model the discrim-
inant structure in the data. However, these two algorithms
implicitly consider that the within-class and between-class re-
lations are equally important. This reduces the ﬂexibility of
the algorithms.

3 Locality Sensitive Discriminant Analysis
In this section, we introduce our Locality Sensitive Discrimi-
nant Analysis algorithm which respects both discriminant and
geometrical structure in the data. We begin with a description
of the locality sensitive discriminant objective function.

3.1 The Locality Sensitive Discriminant Objective

Function for Dimensionality Reduction

As we described previously, naturally occurring data may be
generated by structured systems with possibly much fewer
degrees of freedom than the ambient dimension would sug-
gest. Thus we consider the case when the data lives on or
close to a submanifold of the ambient space. One hopes
then to estimate geometrical and discriminant properties of
the submanifold from random points lying on this unknown
submanifold. In this paper, we consider the particular ques-
tion of maximizing local margin between different classes.

Given m data points {x1, x2, · · · , xm} ⊂ Rn

sampled
from the underlying submanifold M, one can build a near-
est neighbor graph G to model the local geometrical struc-
ture of M. For each data point xi, we ﬁnd its k nearest
neighbors and put an edge between xi and its neighbors. Let
N (xi) = {x1
i } be the set of its k nearest neighbors.
Thus, the weight matrix of G can be deﬁned as follows:

i , · · · , xk
(cid:7)

Wij =

if xi ∈ N (xj) or xj ∈ N (xi)

1,
0, otherwise.

(5)

The nearest neighbor graph G with weight matrix W char-
acterizes the local geometry of the data manifold.
It has
been frequently used in manifold based learning techniques,
such as [Belkin and Niyogi, 2001], [Tenenbaum et al., 2000],
[Roweis and Saul, 2000], [He and Niyogi, 2003]. However,
this graph fails to discover the discriminant structure in the
data.

In order to discover both geometrical and discriminant
structure of the data manifold, we construct two graphs, i.e.
within-class graph Gw and between-class graph Gb. Let
l(xi) be the class label of xi. For each data point xi, the
set N (xi) can be naturally split into two subsets, Nb(xi) and
Nw(xj ). Nw(xi) contains the neighbors sharing the same
label with xi, while Nb(xi) contains the neighbors having
different labels. Speciﬁcally,
i |l(xj
i |l(xj

i ) = l(xi), 1 ≤ j ≤ k}
i ) (cid:7)= l(xi), 1 ≤ j ≤ k}

Nw(xi) = {xj
Nb(xi) = {xj

IJCAI-07

709

(a)

(b)

(c)

(d)

Figure 1: (a) The center point has ﬁve neighbors. The points with the same color and shape belong to the same class. (b)
The within-class graph connects nearby points with the same label. (c) The between-class graph connects nearby points with
different labels. (d) After Locality Sensitive Discriminant Analysis, the margin between different classes is maximized.

(cid:7)
(cid:7)

Clearly, Nb(xi) ∩ Nw(xi) = ∅ and Nb(xi) ∪ Nw(xi) =
N (xi). Let Ww and Wb be the weight matrices of Gw and
Gb, respectively. We deﬁne:

Wb,ij =

Ww,ij =

if xi ∈ Nb(xj) or xj ∈ Nb(xi)

1,
0, otherwise.

if xi ∈ Nw(xj ) or xj ∈ Nw(xi)

1,
0, otherwise.

(6)

(7)

It is clear to see W = Wb + Ww and the nearest neighbor
graph G can be thought of as a combination of within-class
graph Gw and between-class graph Gb.

Now consider the problem of mapping the within-class
graph and between-class graph to a line so that connected
points of Gw stay as close together as possible while con-
nected points of Gb stay as distant as possible. Let y =
(y1, y2, · · · , ym)T
be such a map. A reasonable criterion for
choosing a “good” map is to optimize the following two ob-
jective functions:

(cid:2)
(cid:2)

ij

min

(yi − yj)2Ww,ij

(yi − yj)2Wb,ij

max

ij

(8)

(9)

under appropriate constraints. The objective function (8)
on within-class graph incurs a heavy penalty if neighboring
points xi and xj are mapped far apart while they are actu-
ally in the same class. Likewise, the objective function (9)
on between-class graph incurs a heavy penalty if neighboring
points xi and xj are mapped close together while they actu-
ally belong to different classes. Therefore, minimizing (8) is
an attempt to ensure that if xi and xj are close and sharing the
same label then yi and yj are close as well. Also, maximiz-
ing (9) is an attempt to ensure that if xi and xj are close but
have different labels then yi and yj are far apart. The learning
procedure is illustrated in Figure 1.

3.2 Optimal Linear Embedding

In this subsection, we describe our Locality Sensitive Dis-
criminant Analysis algorithm which solves the objective
functions (8) and (9). Suppose a is a projection vector, that
is, yT = aT X, where X = (x1, · · · , xm) is a n × m matrix.

By simple algebra formulation, the objective function (8) can
be reduced to
1
2

(yi − yj)2Ww,ij

(cid:2)
(cid:2)
(cid:2)

1
2

ij

ij

(cid:8)

=

=

aT xi − aT xj

(cid:9)2Ww,ij
(cid:2)

aT xiDw,iixT

i a −

aT xiWw,ij xT

j a

i

ij

= aT XDwX T a − aT XWwX T a

(cid:10)
where Dw is a diagonal matrix; its entries are column (or row,
since Ww is symmetric) sum of Ww, Dw,ii =
j Ww,ij .
Similarly, the objective function (9) can be reduced to

(cid:2)
(cid:2)

ij

(cid:8)

ij

1
2

1
2

=

(yi − yj)2Wb,ij

(cid:9)2Wb,ij

aT xi − aT xj

= aT X(Db − Wb)X T a
= aT XLbX T a

(cid:10)

where Db is a diagonal matrix; its entries are column (or row,
since Wb is symmetric) sum of Wb, Db,ii =
j Wb,ij . Lb =
Db − Wb is the Laplacian matrix of Gb.

Note that, the matrix Dw provides a natural measure on the
data points. If Dw,ii is large, then it implies that the class con-
taining xi has a high density around xi. Therefore, the bigger
the value of Dw,ii is, the more “important” is xi. Therefore,
we impose a constraint as follows:

yT Dwy = 1 ⇒ aT XDwX T a = 1

Thus, the objective function (8) becomes the following:

or equivalently,

min

a

1 − aT XWwX T a

max

a

aT XWwX T a

(10)

(11)

And the objective function (9) can be rewritten as follows:

max

a

aT XLbX T a

(12)

IJCAI-07

710

Finally, the optimization problem reduces to ﬁnding:

(cid:8)

(cid:9)

αLb + (1 − α)Ww

X T a

arg max

a

aT X

T XDw X T

a

a = 1

Let Φ denote the data matrix in RKHS:

(13)

Φ = [φ(x1), φ(x2), · · · , φ(xm)]

(cid:8)

where α is a suitable constant and 0 ≤ α ≤ 1. The projec-
tion vector a that minimizes (13) is given by the maximum
eigenvalue solution to the generalized eigenvalue problem:

X

αLb + (1 − α)Ww

X T a = λXDwX T a

(14)
Let the column vector a1, a2, · · · , ad be the solutions of
equation (14), ordered according to their eigenvalues, λ1 >
· · · > λd. Thus, the embedding is as follows:

(cid:9)

xi → yi = AT xi

A = (a1, a2, · · · , ad)

(cid:9)

Note that,

where yi is a d-dimensional vector, and A is a n × d matrix.
if the number of samples (m) is less than
(cid:8)
the number of features (n), then rank(X) ≤ m. Conse-
quently, rank(XDwX T ) ≤ m and rank
X(αLb + (1 −
α)Ww)X T
and X(αLb +
≤ m. The fact that XDwX T
(1 − α)Ww)X T
are n × n matrices implies that both of them
are singular. In this case, one may ﬁrst apply Principal Com-
ponent Analysis to remove the components corresponding to
zero eigenvalues.

4 Kernel LSDA
LSDA is a linear algorithm. It may fail to discover the in-
trinsic geometry when the data manifold is highly nonlinear.
In this section, we discussion how to perform LSDA in Re-
producing Kernel Hilbert Space (RKHS), which gives rise to
kernel LSDA.

Suppose X = {x1, x2, · · · , xm} ∈ X is the training sam-
ple set. We consider the problem in a feature space F induced
by some nonlinear mapping

φ : X → F

For a proper chosen φ, an inner product (cid:12), (cid:13) can be deﬁned
on F which makes for a so-called reproducing kernel Hilbert
space (RKHS). More speciﬁcally,

(cid:12)φ(x), φ(y)(cid:13) = K(x, y)

holds where K(., .) is a positive semi-deﬁnite kernel func-
tion. Several popular kernel functions are: Gaussian ker-
nel K(x, y) = exp(−(cid:14)x − y(cid:14)2/σ2); polynomial kernel
; Sigmoid kernel K(x, y) =
K(x, y) = (1 + (cid:12)x, y(cid:13))d
tanh((cid:12)x, y(cid:13) + α).

Given a set of vectors {vi ∈ F |i = 1, 2, · · · , d} which are
orthonormal ((cid:12)vi, vj(cid:13) = δi,j ), the projection of φ(xi) ∈ F
to these v1, · · · , vd leads to a mapping from X to Euclidean
space Rd

through

(cid:8)

yi =

(cid:12)v1, φ(xi)(cid:13), (cid:12)v2, φ(xi)(cid:13), · · · , (cid:12)vd, φ(xi)(cid:13)

(cid:9)T

Now, the eigenvector problem in RKHS can be written as fol-
lows:

(cid:9)
ΦT v = λΦDwΦT v

(15)

Φ

αLb + (1 − α)Ww

(cid:8)

Because the eigenvector of (15) are linear combinations
of φ(x1), φ(x2), · · · , φ(xm), there exist coefﬁcients αi, i =
1, 2, · · · , m such that

m(cid:2)

v =

i=1

αiφ(xi) = Φααα

(cid:8)
(cid:8)

where ααα = (α1, α2, · · · , αm)T ∈ Rm

.

Following some algebraic formulations, we get:

(cid:9)
(cid:9)
ΦT v = λΦDwΦT v
ΦT Φααα = λΦDwΦT Φααα

αLb + (1 − α)Ww
αLb + (1 − α)Ww

Φ
⇒ Φ
⇒ ΦT Φ

(cid:8)

αLb + (1 − α)Ww

(cid:9)
ΦT Φααα

(cid:8)
= λΦT ΦDwΦT Φααα

(cid:9)
Kααα = λKDwKααα

⇒ K

αLb + (1 − α)Ww

(16)
where K is the kernel matrix, Kij = K(xi, xj). Let the
column vectors ααα1, ααα2, · · · , αααm
be the solutions of equation
(16). For a test point x, we compute projections onto the
eigenvectors vk

according to

m(cid:2)

m(cid:2)

(vk · φ(x)) =

αk

i (φ(x) · φ(xi)) =

αk

i K(x, xi)

i=1

i=1

i is the ith

element of the vector αααk

where αk
. For the original
training points, the map can be obtained by y = Kααα, where
the ith
element of y is the one-dimensional representation of
xi.

5 Experimental Results
In this Section, we investigate the use of LSDA on face recog-
nition. We compare our proposed algorithm with Eigenface
(PCA, [Turk and Pentland, 1991]), Fisherface (LDA, [Bel-
humeur et al., 1997]) and Marginal Fisher Analysis (MFA,
[Yan et al., 2005]). We begin with a brief discussion about
data preparation.

5.1 Data Preparation
Two face databases were tested. The ﬁrst one is the Yale data-
base1, and the second one is the ORL database2. In all the
experiments, preprocessing to locate the faces was applied.
Original images were normalized (in scale and orientation)
such that the two eyes were aligned at the same position.
Then, the facial areas were cropped into the ﬁnal image for
matching. The size of each cropped image in all the experi-
ments is 32 × 32 pixels, with 256 gray levels per pixel. Thus,

We look for such {vi ∈ F |i = 1, 2, · · · , d} that helps
{yi|i = 1, · · · , m} preserve local geometrical and discrim-
inant structure of the data manifold. A typical scenario is
X = Rn, F = Rθ

with d << n < θ.

1http://cvc.yale.edu/projects/yalefaces/

yalefaces.html

2http://www.cl.cam.ac.uk/Research/DTG/

attarchive/facesataglance.html

IJCAI-07

711

Figure 2: Sample face images from the Yale database. For each subject, there are 11 face images under different lighting
conditions with facial expression.

Table 1: Recognition accuracy of different algorithms on the Yale database

Method
Baseline

Eigenfaces
Fisherfaces

MFA
LSDA

2 Train

3 Train

4 Train

5 Train

43.4%(1024)

49.4%(1024)

52.6%(1024)

56.2%(1024)

43.4%(29)
47.2%(10)
47.7%(10)
56.5%(14)

49.4%(44)
64.9%(14)
65.7%(14)
68.5%(14)

52.6%(58)
72.9%(14)
74.1%(14)
74.4%(14)

56.2%(74)
78.8%(14)
78.9%(14)
79.0%(14)

each image can be represented by a 1024-dimensional vec-
tor in image space. No further preprocessing is done. Dif-
ferent pattern classiﬁers have been applied for face recogni-
tion, including nearest neighbor [Turk and Pentland, 1991],
Bayesian [Moghaddam, 2002], and Support Vector Machines
[Phillips, 1998], etc. In this paper, we apply nearest neighbor
classiﬁer for its simplicity. In our experiments, the number
of nearest neighbors (k) is taken to be 5. The parameter α is
estimated by leave one out cross validation.

In short, the recognition process has three steps. First, we
calculate the face subspace from the training set of face im-
ages; then the new face image to be identiﬁed is projected
into d-dimensional subspace; ﬁnally, the new face image is
identiﬁed by nearest neighbor classiﬁer.

5.2 Face Recognition on Yale Database

The Yale face database is constructed at the Yale Center for
Computational Vision and Control. It contains 165 grayscale
images of 15 individuals. The images demonstrate variations
in lighting condition (left-light, center-light, right-light), fa-
cial expression (normal, happy, sad, sleepy, surprised, and
wink), and with/without glasses. Figure 2 shows some sam-
ple images of one individual.

For each individual, l(= 2, 3, 4, 5) images were randomly
selected as training samples, and the rest were used for test-
ing. The training set was used to learn a face subspace us-
ing the LSDA, Eigenface, and Fisherface methods. Recog-
nition was then performed in the subspaces. We repeated
this process 20 times and calculate the average recognition
rate. In general, the recognition rates varies with the dimen-
sion of the face subspace. The best performance obtained by
these algorithms as well as the corresponding dimensionality
of the optimal subspace are shown in Table 1. For the baseline
method, we simply performed face recognition in the original
1024-dimensional image space. Note that, the upper bound
of the dimensionality of Fisherface is c − 1 where c is the
number of individuals [Duda et al., 2000].

As can be seen, our algorithm outperformed all other three
methods. The Eigenface method performs the worst in all
cases. It does not obtain any improvement over the baseline
method. It would be interesting to note that, when there are
only two training samples for each individual, the best perfor-
mance of Fisherface is no longer obtained in a c − 1(= 14)
dimensional subspace, but a 10-dimensional subspace. LSDA

reaches the best performance almost always at c − 1 dimen-
sions. This property shows that LSDA does not suffer from
the problem of dimensionality estimation which is a crucial
problem for most of the subspace learning based face recog-
nition methods.

5.3 Face Recognition on ORL Database
The ORL (Olivetti Research Laboratory) face database is
used in this test.
It consists of a total of 400 face images,
of a total of 40 people (10 samples per person). The images
were captured at different times and have different variations
including expressions (open or closed eyes, smiling or non-
smiling) and facial details (glasses or no glasses). The images
were taken with a tolerance for some tilting and rotation of the
face up to 20 degrees. 10 sample images of one individual are
displayed in Figure 3. For each individual, l(= 2, 3, 4, 5) im-
ages are randomly selected for training and the rest are used
for testing.

The experimental design is the same as before. For each
given l, we average the results over 20 random splits. The best
result obtained in the optimal subspace and the corresponding
dimensionality for each method are shown in Table 2.

As can be seen, our LSDA algorithm performed the best
for all the cases. The Fisherface method performed com-
paratively to LSDA as the size of the training set increases.
Moreover, the optimal dimensionality obtained by LSDA and
Fisherface is much lower than that obtained by Eigenface.

5.4 Discussion
Several experiments on two standard face databases have
been systematically performed. These experiments have re-
vealed a number of interesting points:

1. All the three algorithms (LSDA, MFA, and Fisherface)
performed better in the optimal face subspace than in the
original image space. This indicates that dimensionality
reduction can discover the intrinsic structure of the face
manifold and hence improve the recognition rate.

2. In all the experiments, our LSDA algorithm consistently
outperformed the Eigenface, Fisherface and MFA meth-
ods. Especially when the size of the training set is small,
LSDA signiﬁcantly outperformed Fisherface. This is
probably due to the fact that Fisherface fails to accu-
rately estimate the within-class scatter matrix from only
a small number of training samples.

IJCAI-07

712

Figure 3: Sample face images from the ORL database. For each subject, there are 10 face images with different facial expression
and details.

Table 2: Recognition accuracy of different algorithms on the ORL database

Method
Baseline

Eigenfaces
Fisherfaces

MFA
LSDA

2 Train

3 Train

4 Train

5 Train

66.8%(1024)

66.8%(79)
71.3%(28)
71.6%(37)
76.7%(39)

77.0%(1024)
77.0%(119)
83.4%(39)
84.1%(39)
85.0%(39)

81.7%(1024)
81.7%(159)
89.6%(39)
89.7%(39)
90.5%(39)

86.6%(1024)
86.6%(198)
93.2%(39)
93.1%(39)
93.6%(39)

3. Eigenface fails to gain improvement over the baseline.
This is probably because that Eigneface does not encode
the discriminating information.

4. In all the experiments, the optimal dimensionality ob-
tained by LSDA is always c-1, where c is the number of
classes. In practice, when the computational complex-
ity is a major concern, one can simply project the face
images into a c-1 dimensional subspace.

6 Conclusion
We have introduced a novel linear dimensionality reduction
algorithm called Locality Sensitive Discriminant Analysis
(LSDA). For the class of spectrally based dimensionality re-
duction techniques, it optimizes a fundamentally different
criterion compared to classical dimensionality reduction ap-
proaches based on Fisher’s criterion (LDA) or Principal Com-
ponent Analysis. The most prominent property of LSDA is
the complete preservation of both discriminant and local geo-
metrical structure in the data. For LDA, on the other hand, it
can only preserve the global discriminant structure, while the
local geometrical structure is ignored. We have applied our
algorithm to face recognition. Experiments on Yale and ORL
databases have been conducted to demonstrate the effective-
ness of our algorithm.

References
[Belhumeur et al., 1997] Peter N. Belhumeur, J. P. Hepanha,
and David J. Kriegman. Eigenfaces vs. ﬁsherfaces: recog-
nition using class speciﬁc linear projection. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
19(7):711–720, 1997.

[Belkin and Niyogi, 2001] M. Belkin and P. Niyogi. Lapla-
cian eigenmaps and spectral techniques for embedding and
clustering. In Advances in Neural Information Processing
Systems 14, pages 585–591. MIT Press, Cambridge, MA,
2001.

[Belkin et al., 2004] M. Belkin, P. Niyogi, and V. Sindhwani.
On maniold regularization.
Technical report tr-2004-
05, Computer Science Department, The University of
Chicago, 2004.

[Chen et al., 2005] Hwann-Tzong Chen, Huang-Wei Chang,
and Tyng-Luh Liu. Local discriminant embedding and its

variants. In Proc. 2005 Internal Conference on Computer
Vision and Pattern Recognition, 2005.

[Chung, 1997] Fan R. K. Chung. Spectral Graph Theory,
volume 92 of Regional Conference Series in Mathematics.
AMS, 1997.

[Duda et al., 2000] R. O. Duda, P. E. Hart, and D. G. Stork.
Pattern Classiﬁcation. Wiley-Interscience, Hoboken, NJ,
2nd edition, 2000.

[He and Niyogi, 2003] Xiaofei He and Partha Niyogi. Lo-
cality preserving projections. In Advances in Neural In-
formation Processing Systems 16. MIT Press, Cambridge,
MA, 2003.

[Moghaddam, 2002] B. Moghaddam. Principal manifolds
and probabilistic subspaces for visual recognition. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 24(6), 2002.

[Phillips, 1998] P. J. Phillips. Support vector machines ap-
plied to face recognition. Advances in Neural Information
Processing Systems, 11:803–809, 1998.

[Roweis and Saul, 2000] S. Roweis and L. Saul. Nonlinear
dimensionality reduction by locally linear embedding. Sci-
ence, 290(5500):2323–2326, 2000.

[Tenenbaum et al., 2000] J. Tenenbaum, V. de Silva, and
J. Langford. A global geometric framework for nonlinear
dimensionality reduction. Science, 290(5500):2319–2323,
2000.

[Turk and Pentland, 1991] M. Turk and A. Pentland. Eigen-
faces for recognition. Journal of Cognitive Neuroscience,
3(1):71–86, 1991.

[Yan et al., 2005] Shuicheng Yan, Dong Xu, Benyu Zhang,
and Hong-Jiang Zhang. Graph embedding: A general
framework for dimensionality reduction. In Proc. 2005 In-
ternal Conference on Computer Vision and Pattern Recog-
nition, 2005.

[Zhou et al., 2003] D. Zhou, O. Bousquet, T.N. Lal, J. We-
ston, and B. Sch¨olkopf. Learning with local and global
consistency. In Advances in Neural Information Process-
ing Systems 16, 2003.

IJCAI-07

713


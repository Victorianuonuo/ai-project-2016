Effective Control Knowledge Transfer Through Learning Skill and

Representation Hierarchies

∗

Mehran Asadi

Manfred Huber

Department of Computer Science

Department of Computer Science and Engineering

West Chester University of Pennsylvania

The University of Texas at Arlington

West Chester, PA 19383

masadi@wcupa.edu

Arlington, TX 76019
huber@cse.uta.edu

Abstract

Learning capabilities of computer systems still lag
far behind biological systems. One of the rea-
sons can be seen in the inefﬁcient re-use of control
knowledge acquired over the lifetime of the arti-
ﬁcial learning system. To address this deﬁciency,
this paper presents a learning architecture which
transfers control knowledge in the form of behav-
ioral skills and corresponding representation con-
cepts from one task to subsequent learning tasks.
The presented system uses this knowledge to con-
struct a more compact state space representation
for learning while assuring bounded optimality of
the learned task policy by utilizing a represen-
tation hierarchy. Experimental results show that
the presented method can signiﬁcantly outperform
learning on a ﬂat state space representation and
the MAXQ method for hierarchical reinforcement
learning.

1 Introduction

Learning capabilities in biological systems far exceed the
ones of artiﬁcial agents, partially because of the efﬁciency
with which they can transfer and re-use control knowledge
acquired over the course of their lives.

To address this, knowledge transfer across learning tasks
has recently received increasing attention [Ando and Zhang,
2004; Taylor and Stone, 2005; Marthi et al., 2005; Marx et
al., 2005]. The type of knowledge considered for transfer
includes re-usable behavioral macros, important state fea-
tures,
information about expected reward conditions, and
background knowledge. Knowledge transfer is aimed at im-
proving learning performance by either reducing the learning
problem’s complexity or by guiding the learning process.

Recent work in Hierarchical Reinforcement Learning
(HRL) has led to approaches for learning with temporally

∗This work was supported in part by DARPA FA8750-05-2-0283
and NSF ITR-0121297. The U. S. Government may reproduce and
distribute reprints for Governmental purposes notwithstanding any
copyrights. The authors’ views and conclusions should not be inter-
preted as representing ofﬁcial policies or endorsements, expressed
or implied, of DARPA, NSF, or the Government.

extended actions using the framework of Semi-Markov De-
cision Processes (SMDPs) [Sutton et al., 1999], for learning
subgoals and hierarchical action spaces, and for learning ab-
stract representations [Givan et al., 1997]. However, most of
these techniques only address one of the aspects of transfer
and do frequently not directly address the construction of ac-
tion and representation hierarchies in life-long learning.

The work presented here focuses on the construction and
transfer of control knowledge in the form of behavioral skill
hierarchies and associated representational hierarchies in the
context of a reinforcement learning agent.
In particular,
it facilitates the acquisition of increasingly complex behav-
ioral skills and the construction of appropriate, increasingly
abstract and compact state representations which acceler-
ate learning performance while ensuring bounded optimality.
Moreover, it forms a state hierarchy that encodes the func-
tional properties of the skill hierarchy, providing a compact
basis for learning that ensures bounded optimality.

2 Reinforcement Learning
In the RL framework, a learning agent interacts with an en-
vironment over a series of time steps t = 0, 1, 2, 3, ... . At
each time t, the agent observes the state of the environment,
st , and chooses an action, at , which causes the environment
to transition to state st+1 and to emit a reward, rt+1. In a
Markovian system, the next state and reward depend only on
the preceding state and action, but they may depend on these
in a stochastic manner. The objective of the agent is to learn
to maximize the expected value of reward received over time.
It does this by learning a (possibly stochastic) mapping from
states to actions called a policy. More precisely, the objective
is to choose each action at so as to maximize the expected
i=0 γirt+i} , where γ ∈ [0, 1] is a discount-
return, E{
rate parameter. Other return formulations are also possible.
A common solution strategy is to approximate the optimal
action-value function, or Q-function, which maps each state
and action to the maximum expected return starting from the
given state and action and thereafter always taking the best
actions.
To permit the construction of a hierarchical learning system,
we model our learning problem as a Semi-Markov Decision
Problem (SMDP) and use the options framework [Sutton et
al., 1999] to deﬁne subgoals. An option is a temporally ex-
tended action which, when selected by the agent, executes

(cid:2)∞

IJCAI-07

2054

until a termination condition is satisﬁed. While an option is
executing, actions are chosen according to the option’s own
policy. An option is like a traditional macro except that in-
stead of generating a ﬁxed sequence of actions, it follows a
closed-loop policy so that it can react to the environment.
By augmenting the agent’s set of primitive actions with a
set of options, the agent’s performance can be enhanced.
More speciﬁcally, an option is a triple oi = (Ii, πi, βi),
where Ii is the option’s input set, i.e., the set of states in
which the option can be initiated; πi is the option’s pol-
icy deﬁned over all states in which the option can execute;
and βi is the termination condition, i.e., the option termi-
nates with probability βi(s) for each state s. Each option
that we use in this paper bases its policy on its own inter-
nal value function, which can be modiﬁed over time in re-
sponse to the environment. The value of a state s under
an SMDP policy πo
is deﬁned as [Boutilier et al., 1999;
Sutton et al., 1999]:

(cid:3)

(cid:5)

(cid:4)

V π(s) = E

R(s, oi) +

F (s(cid:3)|s, oi)V π(s(cid:3))

where

F (s(cid:3)|s, oi) =

∞(cid:4)

k=1

s(cid:2)

P (st = s(cid:3)|st = s, oi)γk

(1)

and
R(s(cid:3), oi) = E[rt+1 + γrt+2 + γ
i , s, t)] (2)
where rt denotes the reward at time t and (πo, s, t) denotes
the event of an action under policy πo
being initiated at time
t and in state s [Sutton et al., 1999].

rt+3 + . . . |(πo

2

3 Hierarchical Knowledge Transfer
In the approach presented here, skills are learned within
the framework of Semi-Markov Decision Processes (SMDP)
where new task policies can take advantage of previously
learned skills, leading from an initial set of basic actions to
the formation of a skill hierarchy. At the same time, abstract
representation concepts are derived which capture each skill’s
goal objective as well as the conditions under which use of
the skill would predict achievement of the objective. Figure 1
shows the approach.

The state representations are formed here within the frame-
work of Bounded Parameter Markov Decision Processes (BP-
MDPs) [Givan et al., 1997] and include a decision-level
model and a more complex evaluation-level model. Learning
of the new task is then performed on the decision-level model
using Q-learning, while a second value function is maintained
on the evaluation-level model. When an inconsistency is dis-
covered between the two value functions, a reﬁnement aug-
ments the decision-level model by including the concepts of
the action that led to the inconsistency.

Once a policy for the new task is learned, subgoals are
extracted from the system model and corresponding subgoal
skills are learned off-line. Then goal and probabilistic af-
fordance concepts are learned for the new subgoal skills and
both, the new skills and concepts are included into the skill
and representation hierarchies in the agent’s memory, mak-
ing them available for subsequent learning tasks.

Reward

QValue
Inconsistencies

Learning/Planning

State

Q(S,A)

Hierarchical
BPSMDPModel

Model

DecisionLevel

ActionEligibility

DecisionLevel
Actions

StateSpace
Construction

BehaviorMemory

ConceptMemory

ModelRefinement

Construction

NewStateConcepts

NewActions

Subgoal/Subtask
Identification
PolicyGeneralization

EvaluationLevel

State

Decision

Figure 1: System Overview of the Approach for Hierarchical
Behavior and State Concept Transfer.

3.1 Learning Transferable Skills

To learn skills for transfer, the approach presented here tries
to identify subgoals. Subgoals of interest here are states that
have properties that could be useful for subsequent learning
tasks. Because the new tasks’ requirements, and thus their
reward functions, are generally unknown, the subgoal crite-
rion used here does not focus on reward but rather on local
properties of the state space in the context of the current task
domain.
In particular, the criterion used attempts to iden-
tify states which locally form a signiﬁcantly stronger “attrac-
tor” for state space trajectories as measured by the relative
increase in visitation likelihood.

To ﬁnd such states, the subgoal discovery method ﬁrst gen-
erates N random sample trajectories from the learned policy
and for each state, s, on these trajectories and determines the
expected visitation likelihood, C ∗
H (s) is the
sum over all states in sample trajectories hi ∈ H weighed by
their accumulated likelihood to pass through s. The change
of visitation likelihoods along a sample trajectory, hi, is then
determined as ΔH (st) = C ∗
H (st−1), where st is
the tth
state along the path. The ratio of this change along the
path is then computed as

H (s), where, C ∗

H (st) − C ∗

ΔH (st)

max(1, ΔH (st+1))

for every state in which ΔH (st) > 0. Finally, a state st is
considered a potential subgoal if its average change ratio is
signiﬁcantly greater than expected from the distribution of
the ratios for all states 1. For all subgoals found, correspond-
ing policies are learned off-line as SMDP options, oi, and
added to the skill hierarchy.

Deﬁnition 1 A state s(cid:3)
s, if under a learned policy the action in state s(cid:3)
s i.e., P (s|s(cid:3), a) > 0.

is a direct predecessor of state
can lead to

Deﬁnition 2 The count metric for state s under a learned
policy, π, is the sum over all possible state space trajectories
weighed by their accumulated likelihood to pass through
state s.

1The threshold is computed automatically using a t-test based

criterion and a signiﬁcance threshold of 2.5%.

IJCAI-07

2055

Let C ∗

π(s) be the count for state s, then:

(cid:4)

1
π(s) =

C

and

C t

π(s) =

(cid:4)

s(cid:2)(cid:5)=s

P (s|s(cid:3), π(s(cid:3)))

s(cid:2)(cid:5)=s

P (s|s(cid:3), π(s(cid:3)))C t−1

π

(s(cid:3))

n(cid:4)

i=1

C i

π(s)

C ∗

π(s) =

(3)

(4)

(5)

π (s) = C n+1

where n is such that C n
(s) or n = |S|. The
condition s(cid:3)
(cid:4)= s prevents the counting of self loops and
P (s|s(cid:3), π(s(cid:3))) is the probability of reaching state s from state
s(cid:3)
π(st) along a
path, ρ, under policy π is:

by executing action π(s(cid:3)). The slope of C ∗

π

Δπ(st) = C ∗

π(st) − C ∗

π(st−1)

(6)

the gradient

where st is the tth
state along the path. In order to identify
ratio Δπ(st)/ max(1, Δπ(st+1))
subgoals,
is computed for states where Δπ(st) > 0. A state st is
considered a potential subgoal candidate if the gradient ratio
is greater than a speciﬁed threshold μ > 1. Appropriate
values for this user-deﬁned threshold depend largely on the
characteristics of the state space and result in a number of
subgoal candidates that is inversely related to the value of
μ. This approach is an extension of the criterion in [Goel
and Huber, 2003] with max(1, Δπ(st+1)) addressing the
effects of potentially obtaining negative gradients due to
nondeterministic transitions.
In order to reduce the computational complexity of the
above method in large state spaces, the gradient ratio is here
computed using Monte Carlo sampling.

Deﬁnition 3 Let H = {h1, ..., hN } be N sample tra-
jectories induced by policy π, then the sampled count metric,
C ∗
H (s), for each state s that is on the path of at least one
path hi can be calculated as the average of the accumulated
likelihoods of each path, hi, 1 ≤ i ≤ N , rescaled by the
total number of possible paths in the environment.
We can show that for trajectories hi and sample size N such
that

N ≥

H (st)

maxt C ∗
2
N

2(1 + N )log(

)

(7)

2

1 − p

the following statement is true with probability p:

|C ∗

H (st) − C ∗

π(st)| ≤ N

ΔH (st)

If
Δπ(st)

max(1,ΔH (st+1)) > μ +

Theorem 1 Let H = {h1, ..., hN } be N sample trajecto-
ries induced by policy π with N selected according to Equa-
max(1,ΔH (st+1)) , then
tion 7.
max(1,Δπ(st+1)) > μ with probability ≥ p.
Theorem 1 implies that for a sufﬁciently large sample size the
exhaustive and the sampling method predict the same sub-
goals with high probability.

2N (μ+1)

(cid:4)

(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)

(cid:4)

(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6) ≤ δ

3.2 Learning Functional Descriptions of State
The power of complex actions to improve learning per-
formance has two main sources; (i) their use reduces the
number of decision points necessary to learn a policy, and
(ii) they usually permit learning to occur on a more com-
pact state representation. To harness the latter, it is nec-
essary to automatically derive abstract state representations
that capture the functional characteristics of the actions. To
do so,
the presented approach builds a hierarchical state
representation within the basic framework of BPMDPs ex-
tended to SMDPs, forming a hierarchical Bounded Parame-
ter SDMP (BPSMDP). Model construction occurs in a multi-
stage, action-dependent fashion, allowing the model to adapt
rapidly to action set changes.

The BPSMDP state space is a partition of the original state
space where the following inequalities hold for all blocks
(BPSMDP states) Bi and actions oj [Asadi and Huber, 2005]:

F (s(cid:3)(cid:3)|s, oi) −

F (s(cid:3)(cid:3)|s(cid:3), oi)

(8)

s(cid:2)(cid:2)∈Bj

s(cid:2)(cid:2)∈Bj

|R(s, oi) − R(s(cid:3), oi)| ≤ 

(9)
where R(s, o) is the expected reward for executing option o in
state s, and F (s(cid:3)|s, o) is the discounted transition probability
for option o initiated in state s to terminate in state s(cid:3)
. These
properties of the BPSMDP model ensure that the value of
the policy learned on this model is within a ﬁxed bound the
optimal policy value on the initial model, where the bound is
a function of  and δ [Givan et al., 1997].

To make the construction of the BPSMDP more efﬁcient,
the state model is constructed in multiple steps. First func-
tional concepts for each option, o, are learned as termina-
tion concepts Ct,o, indicating the option’s goal condition, and
probabilistic prediction concepts (“affordances”), Cp,o,x, in-
dicating the context under which the option will terminate
successfully with probability x ± . These conditions guar-
antee that any state space utilizing these concepts in its state
factorization fulﬁlls the conditions of Equation 8 for any sin-
gle action.

To construct an appropriate BPMDP for a speciﬁc action
set Ot = {oi}, an initial model is constructed by concatenat-
ing all concepts associated with the options in Ot. Additional
conditions are then derived to achieve the condition of Equa-
tion 8 and, once reward information is available, the reward
condition of Equation 9. This construction facilitates efﬁcient
adaptation to changing action repertoires.

To further utilize the power of abstract actions, a hierarchy
of BPSMDP models is constructed here where the decision-
level model utilizes the set of options considered necessary
while the evaluation-level uses all actions not considered re-
dundant.
In the current system, a simple heuristic is used
where the decision-level set consists only of the learned sub-
goal options while the evaluation-level set includes all ac-
tions.

Let P = {B1, . . . , Bn} be a partition for state space
S derived by the action-dependent partitioning method,
using subgoals {s1, . . . , sk} and options to these subgoals
{o1, . . . , ok}.
If the goal state G belongs to the set of

IJCAI-07

2056

subgoals {s1, . . . , sk},
then G is achievable by options
{o1, . . . , ok} and the task is learnable according to Theorem
2.

Theorem 2 For any policy π for which the goal G can
be represented as a conjunction of terminal sets (subgoals)
of the available actions in the original MDP M , there is
a policy πP in the reduced MDP, MP , that achieves G
as long as for each state st in M for which there exists a
path to G , there exists a path such that F (G|st, πP (st)) > δ.

If G /∈ {s1, . . . , sk} then the task may not be solvable
using only the options that terminate at subgoals. The
proposed approach solves this problem by maintaining a
separate value function for the original state space while
learning a new task on the partition space derived from only
the subgoal options. During learning, the agent has access to
the original actions as well as all options, but makes decisions
only based on the abstract partition space information.
While the agent tries to solve the task on the abstract partition
space, it computes the difference in Q-values between the
best actions in the current state in the abstract state space
and in the original state space.
If the difference is larger
than a constant value (given by Theorem 2), then there is a
signiﬁcant difference between different states underlying the
particular block that was not captured by the subgoal options.
Theorem 3 [Kim and Dean, 2003] shows that if blocks are
stable with respect to all actions the difference between the
Q-values in the partition space and in the original state space
is bounded by a constant value.

Theorem 3 Given an MDP M = (S, A, T, R) and a
partition P of the state space MP , the optimal value function
of M given as V ∗
and the optimal value function of MP
given as V ∗

P satisfy the bound on the distance

(cid:7)

(cid:8)

(cid:7) V ∗ − V ∗

P (cid:7)∞≤ 2

1 + γ

p

1 − γ

where p = minVp

(cid:7) V ∗ − V ∗

P (cid:7)∞ and

LV (s) = max

a

[R(s, a) + γ

P (s(cid:3)|s, a)V (s)]

(cid:4)

s(cid:2)∈S

bounds if the policy only utilizes the actions in the decision-
level action set. Since, however, the action set has to be se-
lected without knowledge of the new task, it is generally not
possible to guarantee that it contains all required actions.

To address this, the approach maintains a second value
function on top of the evaluation-level system model. While
decisions are made strictly based on the decision-level states,
the evaluation-level value function is used to discover value
inconsistencies, indicating that a signiﬁcant aspect of the state
space is not represented in the evaluation-level state model.
The determination of inconsistencies here relies on the fact
that the optimal value function in a BPMDP, V ∗
P , is within a
ﬁxed bound of the optimal value function, V ∗
, on the under-
lying MDP [Givan et al., 1997].

Inconsistencies are discovered when the evaluation-level
value for a state signiﬁcantly exceeds the value of the cor-
responding state at the decision level. In this case, the action
producing the higher value is included for the corresponding
block at the decision level and the block is reﬁned with this
action to fulﬁll Equations 8 and 9 as illustrated in Figure 2.

B
2

B
1

B
3

Figure 2: Decision-level model with 3 initial blocks
(B1, B2, B3) where block B3 has been further reﬁned.

4 Experiments
To evaluate the approach,
it has been implemented on
the Urban Combat Testbed (UCT,), a computer game
(http://gameairesearch.uta.edu). For the experiments pre-
sented here, the agent is given the abilities to move through
the environment shown in Figure 3 and to retrieve and deposit
objects.

When the difference between the Q-values for states in block
Bi are greater than 2(1 + γ
1−γ p), then the primitive action
that achieves the highest Q-value on the original state in the
MDP will be added to the action space of those states that are
in block Bi and block Bi is reﬁned until it is stable for the
new action set. Once no such signiﬁcant difference exists, the
goal will be achievable in the resulting state space according
to Theorem 2.

3.3 Learning on a Hierarchical State Space

To learn new tasks, Q-learning is used here at the decision-
level of the BPSMDP hierarchy. Because the compact
decision-level state model encodes only the aspects of the en-
vironment relevant to a subset of the actions, it only ensures
the learning of a policy within the pre-determined optimality

Figure 3: Urban Combat Testbed (UCT) domain.

The state is here characterized by the agent’s pose as well
as by a set of local object precepts, resulting in an effective
state space with 20, 000 states.

The agent is ﬁrst presented with a reward function to learn
to move to a speciﬁc location. Once this task is learned, sub-
goals are extracted by generating random sample trajectories
as shown in Figure 4.

As the number of samples increases, the system identiﬁes
an increasing number of subgoals until, after 2, 000 samples,

IJCAI-07

2057

s

l
l
i

k
S

l

 
/
 
s
a
o
g
b
u
S
d
e
r
e
v
o
c
s
D

 

i

30

25

20

15

10

5

0

0

500

90

80

70

60

50

40

30

20

10

l

e
u
a
V
−
Q

Exhaustive Computing
Monte Carlo Sampling

No Transfer
Skill Transfer
Skill and Representation Transfer

1000

1500

2000

Number of Samples

2500

3000

0

0

500

1000

1500

Number of Iterations

2000

2500

3000

Figure 4: Number of subgoals discovered using sampling.

all 29 subgoals that could be found using exhaustive calcula-
tion have been captured.

Once subgoals are extracted, subgoal options, oi, are
learned and termination concepts, Ct,oi and probabilistic out-
come predictors, Cp,oi,x are generated. These subgoal op-
tions and the termination and prediction concepts are then
transferred to the next learning tasks.

The system then builds a hierarchical BPSMDP system
model where the decision-level only utilizes the learned sub-
goal actions while the evaluation-level model is built for all
available actions. On this model, a second task is learned
where the agent is rewarded for retrieving a ﬂag (from a
different location than the previous goal) and return it to
the home base. During learning, the system augments its
decision-level state representation to allow learning of a pol-
icy that is within a bound of optimal as shown in Figure 6.

85

s
e

80

t

t

 

a
S
P
D
M
S
P
B

 
f

o
 
r
e
b
m
u
N

75

70

65

60

55

50

45

40

0

500

Task−Independent Blocks
Task−Specific Refinement

1000

1500

Number of Iterations

2000

2500

3000

Figure 5: Size of the decision-level state representation.

Figure 5 shows that the system starts with an initial state
representation containing 43 states. During learning, as value
function inconsistencies are found, new actions and state
splits are introduced, eventually increasing the decision-level
state space to 81 states. On this state space, a bounded op-
timal policy is learned as indicated in Figure 6. This graph
compares the learning performance of the system against a
learner that only transfers the discovered subgoal options and
a learner without any transfer mechanism. These graphs show
a transfer ratio2 of ≈ 2.5 when only subgoal options are trans-

2The transfer ratio is the ratio of the area over the learning curve

between the no-transfer and the transfer learner.

Figure 6: Learning performance with and without skill and
representation/concept transfer.

ferred, illustrating the utility of the presented subgoal crite-
rion.
Including the representation transfer and hierarchical
BPSMDP learning approach results in signiﬁcant further im-
provement with a transfer ratio of ≈ 5.

5 Comparison with MAXQ
Dietterich [Dietterich, 2000] developed an approach to hi-
erarchical RL called the MAXQ Value Function Decompo-
sition, which is also called the MAXQ method. Like options
and HAMs, this approach relies on the theory of SMDPs. Un-
like options and HAMs, however, the MAXQ approach does
not rely directly on reducing the entire problem to a single
SMDP. Instead, a hierarchy of SMDPs is created whose so-
lution can be learned simultaneously. The MAXQ approach
starts with a decomposition of a core MDP M into a set of
subtasks {M0, . . . , Mn}. The subtasks form a hierarchy with
M0 being the root subtask, which means that solving M0
solves M . Actions taken in solving M0 consist of either exe-
cuting primitive actions or policies that solve other subtasks,
which can in turn invoke primitive actions or policies of other
subtasks, etc.
Each subtask, Mi, consists of three components. First, it has
a subtask policy, pi, that can select other subtasks from the
set of Mi ’s children. Here, as with options, primitive actions
are special cases of subtasks. We also assume the subtask
policies are deterministic. Second, each subtask has a ter-
mination predicate that partitions the state set, s, of the core
MDP into si, the set of active states in which Mi’s policy
can execute, and ti, the set of termination states which, when
entered, causes the policy to terminate. Third, each subtask
mi has a pseudo-reward function that assigns reward values
to the states in ti. The pseudo-reward function is only used
during learning.
Figure 7 shows the comparison between the MAXQ decom-
position and the learning of an SMDP with the sampling-base
subgoal discovery but without action-dependent partitioning.
This experiment illustrates that MAXQ will outperform an
SMDP with options to the subgoals that are discovered by
sampling-based subgoal discovery. The reason for this is that
while subgoals are hand designed in the MAXQ decompo-
sition, the sampling-based method is fully autonomous and
does not rely on human decision. As a result, subgoal dis-

IJCAI-07

2058

10

5

0

-5

-10

-15

d
r
a
w
e
r
 
e
g
a
r
e
v
A

-20

0

500000

1e+06

1.5e+06

Flat MDP
MAXQ Decomposition
Subgoal Discovery

2e+06
2.5e+06
Number of Actions

3e+06

3.5e+06

4e+06

4.5e+06

Figure 7: Comparison of policies derived with the MAXQ
method and with a SMDP with sampling-based subgoal dis-
covery.

covery generates additional subgoal policies that are not re-
quired for the task at hand and might not ﬁnd the optimal op-
tion set. Figure 8 illustrates the comparison between learning
time in MAXQ and the BPSMDP constructed by the action-
dependent partitioning method. This experiment shows that
action-dependent partitioning can signiﬁcantly outperform
the MAXQ decomposition since it constructs state and tem-
poral abstractions resulting in a more abstract state space. In
this form, it can transfer the information contained in previ-
ously learned policies for solving subsequent tasks.

10

5

0

-5

-10

-15

d
r
a
w
e
r
 
e
g
a
r
e
v
A

-20

0

500000

1e+06

1.5e+06

Flat MDP
MAXQ Decomposition
Action-Dependent Partitions

2e+06
2.5e+06
Number of Actions

3e+06

3.5e+06

4e+06

4.5e+06

Figure 8: Comparison of policies derived with the MAXQ
method and action-dependent partitioning with autonomous
subgoal discovery.

6 Conclusion
Most artiﬁcial learning agents suffer from the inefﬁcient re-
use of acquired control knowledge in artiﬁcial. To address
this deﬁciency, the learning approach presented here provides
a mechanism which extracts and transfers control knowledge
in the form of potentially useful skill and corresponding rep-
resentation concepts to improve the learning performance on
subsequent tasks. The transferred knowledge is used to con-
struct a compact state space hierarchy that captures the impor-
tant aspects of the environment in the context of the agent’s

capabilities and thus results in signiﬁcant improvements in
learning performance.

References
[Ando and Zhang, 2004] Rie K. Ando and Tong Zhang. A
framework for learning predictive structures from multiple
tasks and unlabeled data. Technical Report RC23462, IBM
T.J. Watson Research Center, 2004.

[Asadi and Huber, 2005] M. Asadi and M. Huber. Accelerat-
ing Action Dependent Hierarchical Reinforcement Learn-
ing Through Autonomous Subgoal Discovery. In Proceed-
ings of the ICML 2005 Workshop on Rich Representations
for Reinforcement Learning, 2005.

[Boutilier et al., 1999] C. Boutilier, T. Dean, and S. Hanks.
Decision-Theoretic Planning: Structural Assumptions and
Computational Leverage. Journal of Artiﬁcial Intelligence
Research, 11:1–94, 1999.

[Dietterich, 2000] T. G. Dietterich. Hierarchical reinforce-
ment learning with the maxq value function decomposi-
tion. Artiﬁcial Intelligence Research, 13:227–303, 2000.

[Givan et al., 1997] R. Givan, T. Dean,

, and S. Leach.
Model Reduction Techniques for Computing Approxi-
mately Optimal Solutions for Markov Decision Processes.
In Proceedings of the 13th Annual Conference on Uncer-
tainty in Artiﬁcial Intelligence (UAI-97), pages 124–131,
San Francisco, CA, 1997. Morgan Kaufmann Publishers.

[Goel and Huber, 2003] S. Goel and M. Huber. Subgoal
Discovery for Hierarchical Reinforcement Learning Using
Learned Policies. In Proceedings of the 16th International
FLAIRS Conference, pages 346–350. AAAI, 2003.

[Kim and Dean, 2003] K. Kim and T. Dean. Solving Fac-
tored MDPs using Non-Homogeneous Partitions. Artiﬁ-
cial Intelligence, 147:225–251, 2003.

[Marthi et al., 2005] Bhaskara Marthi, Stuart Russell, David
Latham, and Carlos Guestrin. Concurrent hierarchical re-
inforcement learning.
In International Joint Conference
on Artiﬁcial Intelligence, Edinburgh, Scotland, 2005.

[Marx et al., 2005] Zvika Marx, Michael T. Rosenstein, and
Leslie Pack Kaelbling. Transfer leraning with an ensemble
of background tasks. In NIPS 2005 Workshop on Transfer
Learning, Whistler, Canada, 2005.

[Sutton et al., 1999] R.S. Sutton, D. Precup, and S. Singh.
Between MDPs and Semi-MDPs: Learning, Planning, and
Representing Knowledge at Multiple Temporal Scales. Ar-
tiﬁcial Intelligence, 112:181–211, 1999.

[Taylor and Stone, 2005] Matthew E. Taylor and Peter
Stone. Behavior transfer for value-function-based rein-
forcement learning. In Frank Dignum, Virginia Dignum,
Sven Koenig, Sarit Kraus, Munindar P. Singh, and Michael
Wooldridge, editors, The Fourth International Joint Con-
ference on Autonomous Agents and Multiagent Systems,
pages 53–59, New York, NY, July 2005. ACM Press.

IJCAI-07

2059


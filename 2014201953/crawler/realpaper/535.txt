A Bayesian Approach to Imitation in Reinforcement Learning 

Bob Price 

University of British Columbia 

Vancouver, B.C., Canada V6T 1Z4 

price@cs.ubc.ca 

Craig Boutilier 

University of Toronto 

Toronto, ON, Canada M5S 3H5 

cebly@cs. toronto. edu 

Abstract 

In multiagent environments, forms of social learn(cid:173)
ing such as teaching and imitation have been shown 
to  aid  the  transfer  of knowledge  from  experts  to 
learners  in  reinforcement  learning  (RL).  We  re(cid:173)
cast the problem of imitation in a Bayesian frame-
work.  Our  Bayesian  imitation  model  allows  a 
learner to smoothly pool prior knowledge, data ob(cid:173)
tained  through  interaction  with  the  environment, 
and information inferred from observations of ex(cid:173)
pert  agent behaviors.  Our  model  integrates  well 
with recent Bayesian  exploration techniques,  and 
can be readily generalized to new settings. 

1  Introduction 
Reinforcement  learning  is  a  flexible,  yet  computationally 
challenging paradigm.  Recent results demonstrating that un(cid:173)
der certain assumptions the sample complexity of reinforce(cid:173)
ment learning is polynomial in the number of problem states 
[Kearns and Singh, 1998] are tempered by the sober fact that 
the number of states is generally exponential  in the number 
of the attributes defining a learning problem.  With recent in-
terest in building interacting autonomous agents,  reinforce-
ment learning is increasingly applied to multiagent tasks,  a 
development which only adds to the complexity of learning 
[Littman, 1994; Hu and Wellman, 1998]. In this paper, we ex(cid:173)
amine multi-agent reinforcement learning under the assump(cid:173)
tion that other agents in the environment are not merely ar(cid:173)
bitrary actors, but actors "like me".  That is, the other agents 
may have similar action capabilities and similar objectives. 
This assumption radically changes the optimal learning strat(cid:173)
egy.  Information about other agents "like me" can give the 
learning agent additional information about its own capabili(cid:173)
ties and how these capabilities relate to its own objectives. A 
number of techniques have been developed to exploit this, in(cid:173)
cluding imitation [Demiris and Hayes, 1999; Mataric, 2002], 
learning by watching [Kuniyoshi et al., 1994], teaching or 
programming by demonstration [Atkeson and Schaal, 1997] 
behavioral cloning [Sammut et al., 1992], and inverse rein(cid:173)
forcement learning [Ng and Russell, 2000]. 

Learning by observation of other agents has intuitive ap(cid:173)
peal; however, explicit communication about action capabil(cid:173)
ities between agents requires considerable infrastructure:  a 

communication channel,  a  sufficiently expressive represen(cid:173)
tation language, a transformation between possibly different 
agent bodies, and an incentive to communicate.  In dynamic, 
competitive domains, such as web-based trading, it is unreal(cid:173)
istic to expect all agents to be designed with compatible rep(cid:173)
resentations and altruistic intentions. Observation-based tech(cid:173)
niques, in which the learning agent observes only the outward 
behaviors of another agent,  can reduce the need  for explicit 
communication. Implicit communication through passive ob(cid:173)
servations has been implemented as implicit imitation [Price 
and Boutilier, 1999; 2001]. In this model, the effects of other 
agents' action choices on the state of the environment can be 
observed, but the internal state of other agents and their ac(cid:173)
tion control signals are not observable.  Independent explo(cid:173)
ration on the part of the observer is used to adapt knowledge 
implicit in observations of other agents to the learning agent's 
own needs. Unlike classic imitation models, the learner is not 
required to explicitly duplicate the behavior of other agents. 
In  this  paper,  we  recast  implicit  imitation  in  a  Bayesian 
framework.  This new formulation offers several advantages 
over existing  models.  First  it  provides  a  more  principled, 
and more elegant approach to the  smooth pooling of infor(cid:173)
mation from the agent's prior beliefs, its own experience and 
the observations of other agents (e.g., it eliminates the need 
for certain ad hoc tuning parameters in current imitation mod(cid:173)
els).  Second,  it  integrates well  with  state-of-the-art explo(cid:173)
ration techniques, such as Bayesian exploration. Finally, the 
Bayesian imitation model can be extended readily to partially-
observable domains, though the derivation and implementa(cid:173)
tion are considerably more complex and are not reported here. 

2  Background 
We assume a reinforcement learning (RL) agent is learning to 
control a Markov decision processes (MDP) 
with finite state and action sets  S,Ao,  reward function 
R,  and  dynamics  D.  The  dynamics  D  refers  to  a 
S 
and 
set of transition distributions 
are subscripted to distinguish them  from those 
rewards 
of other agents (see below).  We assume throughout that the 
but not the  dynamics D  of the  MDP  (thus 
agent knows 
we adopt the "automatic programming" perspective), and has 
the objective of maximizing discounted reward over an infi(cid:173)
nite horizon. Any of a number of RL techniques can be used to 
We focus here on model-
learn an optimal policy 

The actions 

712 

MULTIAGENT SYSTEMS 

based RL methods, in  which the observer maintains an esti(cid:173)
mated MDP 
based on the set of experiences 
obtained so far.  At each stage (or at suitable inter(cid:173)
vals) this MDP can be solved exactly, or approximately using 
techniques such as prioritized sweeping [Moore and Atkeson, 
1993].  Since R0 is known, we focus on learning dynamics. 
Bayesian methods in model-based RL allow agents to  in(cid:173)
corporate priors and explore optimally.  In  general,  we em(cid:173)
ploy a prior density P over possible dynamics D, and update it 
with each data point (s, a, t).  Letting  H 0)  = 
denote  the  (current) state history of the  observer,  and  A0  = 
be  the  action history,  we  use  the poste(cid:173)
rior 
to  update the  action  Q-values,  which  are 
used  in  turn  to  select  actions.  The  formulation  of Dearden 
et al  1999 renders this  update tractable by  assuming a con(cid:173)
venient  prior:  P  is  the  product  of local  independent  densi(cid:173)
ties for each transition distribution 
and each den(cid:173)
To model 
sity 
for each  possible 
.  Update  of a  Dirichlet  is  straightforward: 
successor state 
is 
given prior 
the number of observed transitions from s  to  t  under a),  the 
posterior is given by parameters 
Thus the poste(cid:173)
rior in Eq. 1 can be factored into posteriors over local families: 

is a Dirichlet with parameters 

we require one  parameter 

and data vector 

(where 

_ 

(1) 
where 
is  the  subset  of history  composed  of transitions 
from state s due to action a, and the updates themselves are 
simple Dirichlet parameter updates. 

The Bayesian approach has several advantages over other 
approaches to model-based RL. First, it allows the natural in(cid:173)
corporation of priors  over transition  and  reward parameters. 
Second, approximations to optimal Bayesian exploration can 
take advantage of this approach, and the specific structural as(cid:173)
sumptions on the prior discussed above [Dearden et al., 1999]. 

3  Bayesian Imitation 
In  multiagent  settings,  observations  of other  agents  can  be 
used  in  addition  to  prior beliefs  and  personal  experience to 
improve an agent's model  of its  environment.  These  obser(cid:173)
vations can have enormous impact when they provide  infor(cid:173)
mation to an agent about parts of the state space it has not yet 
visited.  The information can be used to bias exploration to(cid:173)
wards the most promising regions of state space and thereby 
reduce exploration costs and speed convergence dramatically. 
The  flexibility  of the Bayesian formulation leads to an ele(cid:173)
gant and principled mechanism for incorporating these obser(cid:173)
vations into the agent's model updates.  Following Price and 
Boutilier 1999, we assume two agents, a knowledgeable men(cid:173)
tor 77i and a naive observer o, acting simultaneously, but in(cid:173)
dependently, in a fixed environment.1  Like the observer, the 
mentor too is controlling an  MDP  
with the 
same underlying state space and dynamics (that is, for any ac(cid:173)
tion 
the dynamics are identical). The assump(cid:173)
tion that the two agents have the same state space is not criti(cid:173)
cal:  more important is that there is some analogical mapping 

We assume that the agents are performing non-interacting tasks. 

Figure 1:  Dependencies among model and evidence sources 

between the two  [Nehaniv and Dautenhahn,  1998].  Wc  as(cid:173)
sume full observability of the mentor's state space; but we do 
not assume the observer can identify the actions taken by the 
mentorâ€”it simply observes state transitions. 

and 

;  and  for each action 

which  induces  a  Markov  chain 

tor's  dynamics: 
icy 
Pr 
there  exists  an  action 

We  make two additional assumptions regarding the men(cid:173)
the  mentor  implements  a  stationary  pol(cid:173)
= 
taken  by  the  mentor, 
such  that  the  distributions 
are the same.  This latter assump(cid:173)
tion is the homogeneous action assumption and implies that 
the  observer can  duplicate  the  mentor's policy.2  As  a  con(cid:173)
sequence we  can treat the dynamics D  as the same  for both 
agents.  Note that  we  do  not assume  that  the  learner knows 
a priori  which  of its  actions  duplicates the  mentor's  (for any 
given state s), nor that the observer wants to duplicate this pol(cid:173)
icy (as the agents may have different objectives). 

Since  the  learner  can  observe  the  mentor's  transitions 
(though not its actions directly),  it can form estimates of the 
mentor's Markov chain, along with estimates of its own MDP 
(transition probabilities and reward  function).  In  [Price and 
Boutilier,  1999], this estimate is used to augment the normal 
Bellman backup, treating the observed distribution Pr(s,.) as 
a model of an action available to the observer. Imitators using 
augmented backups based  on their observations of a mentor 
often learn much more quickly, especially if the mentor's re(cid:173)
ward function or parts of its policy overlap with that of the ob(cid:173)
server.  Techniques like interval estimation [Kaelbling,  1993] 
can be used to suppress augmented backups where their value 
has low "confidence." 

In the Bayesian approach, the observer incorporates obser(cid:173)
vations of the mentor directly into an augmented model of its 
environment.  Let Hm  denote the history of mentor state tran(cid:173)
sitions observed by the learner.  As above,  H0  and A0 repre-
sents the observer's state and action history respectively.  Fig-
ure 1 illustrates the sources of information available to the im-
itator with  which  to  constrain  its  beliefs  about Z),  and their 
probabilistic dependence.  While the observer knows its own 
action history,  A0  it has no direct knowledge of the actions 
taken by the mentor:  at best it may have (often weak) prior 
knowledge about the mentor's  policy 
The learner's be(cid:173)
liefs over D can then be updated w.r.t. the joint observations: 

2The homogeneous action assumption can be relaxed [Price and 
Boutilier,  2001].  Essentially,  the  observer  hypothesizes  that viola-
tions can be "repaired" using a local search for a short sequence of 
actions  that roughly  duplicates  a short subsequence  of the  mentor's 
actions. If a repair cannot be found, the observer discards the mentor 
influence (at this point in state space). 

MULTIAGENT  SYSTEMS 

713 

We assume that the prior P(D)  has the factored Dirichlet 
form described above.  Without mentor influence,  a learner 
can maintain its posterior in the same factored form, updating 
each component of the model P 
independently. Unfor(cid:173)
tunately, complications arise due to the unobservability of the 
mentor's actions.  We show, however, that the model update 
in Eq. 2 can still be factored into convenient terms. 

We derive a factored update model  for  P 

describ(cid:173)
ing the dynamics at state s under action a by considering two 
cases. In case one, the mentor's unknown action 
could be 
different than the action a. In this case, the model factor 
would be independent of the mentor's history, and we can em(cid:173)
ploy the standard Bayesian update Eq. 1 without regard for the 
mentor. In case two, the mentor action 
is in fact the same 
as the observer's action a.  Then the mentor observations are 
relevant to the update of P  

Let 

be  the  prior parameter vector for  P 

, and 
denote  the  counts  of observer transitions  from  state  s 
via  action  a,  and 
the  counts  of the  mentor  transitions 
from state s.  The posterior augmented model factor density 
P 
is then a Dirichlet with parame(cid:173)
ters 
that is, we simply update with the sum 
of the observer and mentor counts: 

Since the observer does not know the mentor's action we 

compute the expectation w.r.t. these two cases: 

This allows a factored update of the usual conjugate form, but 
where the mentor counts 
are distributed across all actions, 
weighted by the posterior probability that the mentor's policy 
chooses that action at state  

With a mechanism to calculate the posterior over the men(cid:173)
tor's policy, Eq. 3 provides a complete factored update rule for 
incorporating evidence from observed mentors by a Bayesian 
model-based RL agent.  To tackle this last problemâ€”that of 
updating our beliefs about the mentor's policyâ€”we have: 

If we assume that the prior over the mentor's policy is fac(cid:173)
tored in the same way as the prior over modelsâ€”that is, we 

3This assumes that at least one of the observer's actions is equiv(cid:173)
alent to the mentor's, but our model can be generalized to the het(cid:173)
erogeneous case. An additional term is required to represent "none 
of the above". 

over  Am  for each sâ€” 
have independent distributions 
this update can be factored as well, with history elements at 
state s being the only ones relevant to computing the posterior 
over 
,  We still have the difficulty of evaluating the in(cid:173)
tegral over models. Following Dearden et al.  1999, we tackle 
this  by  sampling models  to  estimate  this  quantity.  Specif(cid:173)
from  the  factored Dirichlet 
ically,  we  sample models 
P 
with 
,  and  observed  counts  , the likelihood 
parameter vector 
of 

over P.4  Given a specific sample 

is: 

We  can combine the  expression for expected model fac(cid:173)
tor probability in Eq. 3 with our expression for mentor policy 
likelihood in Eq. 5 to obtain a tractable algorithm for updating 
the observer's beliefs about the dynamics model D based on 
its own experience, and observations of the mentor.5. 

A  Bayesian  imitator  thus  proceeds  as  follows.  At  each 
stage, it observes its own state transition and that of the men(cid:173)
tor, using each to update its density over models as just de(cid:173)
scribed. Efficient methods are used to update the agent's value 
function.  Using this updated value function, it selects a suit(cid:173)
able action, executes it, and repeats the cycle. 

Like any RL agent, an imitator requires a suitable explo(cid:173)
ration mechanism. In the Bayesian exploration model [Dear-
den et al,  1999], the uncertainty about the effects of actions 
is captured by a Dirichlet, and is used to estimate a distribu-
tion over possible Q-values for each  state-action pair.6  No-
tions such as value of information can then be used to approx(cid:173)
imate the optimal exploration policy. This method is compu(cid:173)
tationally demanding, but total reward including reward cap(cid:173)
tured during training is usually much better than that provided 
by heuristic techniques. Bayesian exploration also eliminates 
the parameter tuning required by methods like  -greedy, and 
adapts locally and instantly to evidence. These facts makes it 
a good candidate to combine with imitation. 

4  Experiments 
In  this  section  we  attempt  to  empirically  characterize  the 
applicability  and  expected  benefits  of  Bayesian  imitation 
through several experiments.  Using domains from the liter(cid:173)
ature  and two unique domains,  we  compare Bayesian  imi(cid:173)
tation to non-Bayesian imitation [Price and Boutilier,  1999], 
and to several standard model-based RL (non-imitating) tech-
niques, including Bayesian exploration, prioritized sweeping 
and  complete  Bellman  backups.  We  also  investigate  how 
Bayesian exploration combines with imitation. 

First, we describe the agents used in our experiments. The 
Oracle  employs  a  fixed  policy  optimized for each  domain, 

4Sampling is efficient as only one local model needs to be resam-

pled at any time step. 

5Scaling techniques such as those used in HMM's may be re(cid:173)

quired to prevent underflow in the term 

in Eq. 5. 

6The Q-value distribution changes very little with each update 

and can be repaired efficiently using prioritized sweeping. In fact, the 
Bayesian learner is cheaper to run than a full Bellman backup over 
all states. 

714 

MULTIAGENT SYSTEMS 

providing  both  a  baseline  and  a  source  of expert  behavior 
for the  observers.  The  EGBS  agent  combines  greedy  ex(cid:173)
ploration  (EG)  with  a  full  Bellman  backup  (i.e.,  sweep)  at 
each time  step.  It provides an example  of a generic model-
based approach to learning. The EGPS agent is a model-based 
RL  agent,  using  e-greedy  (EG)  exploration  with  prioritized 
sweeping (PS).  EGPS  use  fewer backups,  but applies  them 
where they are predicted to do the most good. EGPS does not 
have a fixed backup policy, so it can propagate value multiple 
steps across the state space in situations where EGBS would 
not.  The BE agent employs Bayesian exploration (BE) with 
prioritized sweeping for backups.  BEBI  combines Bayesian 
exploration  (BE)  with  Bayesian  imitation  (Bi).  EGBI  com-
bines c-greedy exploration (EG) with Bayesian imitation (Bl). 
The EGNB1 agent combines e-greedy exploration with non-
Bayesian imitation. 

In  each  experiment,  agents  begin  at  the  start  state.  The 
agents do not interact within the state space.  When an agent 
achieves the goal, it is reset to the beginning. The other agents 
continue unaffected.  Each agent has a fixed number of steps 
(which may be spread over varying numbers of runs) in each 
experiment. In each domain, agents are given locally uniform 
priors (i.e., every action has an equal probability of resulting 
in any of the local neighbouring states;  e.g., in  a grid world 
there are 8 neighbours).  Imitators observe the expert oracle 
agent concurrently with their own exploration. Results are re(cid:173)
ported as the total reward collected in the last 200 steps. This 
sliding window integrates the rewards obtained by the agent 
making  it  easier to  compare performance  of various  agents. 
During the first 200 steps,  the  integration window starts off 
empty causing the oracle's plot to jump from zero to optimal in 
the first 200 steps. The Bayesian agents use 5 sampled MDPs 
for estimating Q-value distributions and  10 samples for esti(cid:173)
mating the mentor policy from the Dirichlet distribution.  Ex(cid:173)
ploration rates for e-greedy agents were tuned for each exper(cid:173)
imental domain. 

Our first test of the agents was on the "Loop" and "Chain" 
examples (designed to  show the benefits of Bayesian explo(cid:173)
ration),  taken  from  [Dearden  et al,  1999].  In  these  experi-
ments, the imitation agents performed more or less identically 
to the optimal oracle agent and no separation could be seen 
amongst the imitators. 

Using the more challenging "FlagWorld" domain [Dearden 
et  al.,  1999],  we see meaningful differences in performance 
amongst the  agents.  In  FlagWorld,  shown  in  Figure  2,  the 
agent starts at state S and searches for the goal state G l.  The 
agent  may  pick  up  any  of three  flags  by  visiting  states  F l, 
F2 and F3.  Upon reaching the goal state, the agent receives 
1  point for each flag collected.  Each action (N,E,S,W) suc(cid:173)
ceeds  with  probability  0.9  if the  corresponding  direction  is 
clear, and with probability 0.1  moves the agent perpendicu(cid:173)
lar to the desired direction.  Figure 3  shows the reward col(cid:173)
lected in over the preceding 200 steps for each agent.  The Or(cid:173)
acle demonstrates optimal performance. The Bayesian imita(cid:173)
tor using Bayesian exploration (BEBI) achieves the quickest 
convergence to the optimal solution.  The e-greedy Bayesian 
imitator  (EGBI)  is  next,  but  is  not  able  to  exploit  informa(cid:173)
tion locally as well as BEBI. The non-Bayesian imitator (EG-
NBI) does better than the unassisted agents early on but fails 

Figure 2:  Flagworld Domain 

Figure 3:  Flag world results (50 runs) 

Figure 4:  Flag World Moved Goal (50 runs) 

MULTIAGENT SYSTEMS 

715 

Figure 5:  Tutoring domain results (50 runs) 

Figure 6:  No-south domain 

to  find  the  optimal  policy  in  this  domain.  A  slower  ex-
ploration rate  decay would  allow the agent to  find  the  opti(cid:173)
mal  policy,  but would  also  hurt  its  early performance.  The 
non-imitating Bayesian explorer fares poorly compared to the 
Bayesian imitators, but outperforms the remaining agents, as 
it exploits prior knowledge about the connectivity of the do(cid:173)
main.  The other agents show poor performance (though with 
high  enough exploration rates they  would converge eventu(cid:173)
ally). We conclude that Bayesian imitation makes the best use 
of the  information available to  the agents, particularly  when 
combined with Bayesian exploration. 

We altered the Flag World domain so that the mentor and the 
learners had different objectives.  The goal of the expert Ora(cid:173)
cle remained at location G1, while the learners had goal loca(cid:173)
tion G2 (Figure 2).  Figure 4 shows that transfer due to imita(cid:173)
tion is qualitatively similar to the case with identical rewards. 
We see that imitation transfer is robust to modest differences 
in mentor and imitator objectives.  This is readily explained by 
the fact that the mentor's policy provides model information 
over most states in the domain, which can be employed by the 
observer to achieve its own goals. 

The tutoring domain requires agents to schedule the presen(cid:173)
tation  of simple patterns  to  human  learners  in  order to  min(cid:173)
imize  training  time.  To  simplify  our experiments,  we  have 
the  agents teach  a  simulated  student.  The  student's  perfor-
mance  is modeled by  independent, discretely approximated, 
exponential forgetting curves  for each concept.  The agent's 
action will be its choice of concept to present.  The agent re(cid:173)
ceives a reward when the  student's forgetting rate has been 
reduced below a  predefined threshold  for all  concepts.  Pre-
senting a concept lowers its forgetting rate, leaving it unpre-
sented increases its forgetting rate.  Our model is too simple to 
serve as a realistic cognitive model of a student, but provides 
a qualitatively different problem to tackle.  We note that the 
action space grows linearly with the number of concepts, and 
the state space exponentially. 

The results presented in Figure 5 are based on the presen(cid:173)
tation of 5 concepts to a student.  (EGBS has been left out as 
it is time-consuming and generally fares poorly.) We see that 
all of the imitators learn quickly, but with the Bayesian imita(cid:173)
tors BEBI and EGBI outperforming EGNBI (which converges 
to  a  suboptimal  policy).7  The  generic  Bayesian  agent (BE) 
also chooses a suboptimal solution (which often occurs in BE 
agents  if its priors prevent adequate exploration).  Thus,  we 
see that imitation mitigates one of the drawbacks of Bayesian 
exploration:  mentor observations  can  be  used  to  overcome 
misleading priors.  We  see  also that  Bayesian  imitation  can 
also be applied to practical problems with factored state and 
action spaces and non-geometric structure. 

Figure 7:  No South results (50 runs) 

The next domain provides further insight into the combina(cid:173)
tion of Bayesian imitation and Bayesian exploration.  In this 
grid world (Figure 6), agents can move south only in the first 
column.  In this domain,  the  optimal  Oracle agent proceeds 
due south to the bottom corner and then east across to the goal. 
The Bayesian explorer (BE) chooses a path based on its prior 
beliefs that the space is completely connected. The agent can 

Increasing exploration allows EGNBI to find the optimal policy, 

but further depresses short term performance. 

716 

MULTIAGENT SYSTEMS 

easily  be  guided  down  one  of the  long  "tubes"  in  this  sce(cid:173)
nario,  only to have to retrace  it steps.  The results  in  this do(cid:173)
main,  shown  in  Figure  7,  clearly  differentiate  the  early  per(cid:173)
formance  of the  imitation  agents  (BEBI,  EGB1  and  EGNB1) 
from the Bayesian explorer (BE) and other independent learn(cid:173)
ers.  The  initial  value  function constructed  from the  learner's 
prior beliefs  about  the  connectivity  in  the  grid  world  lead  it 
to over-value many of the states that lead to a dead end.  This 
results in a costly misdirection of exploration and poor perfor(cid:173)
mance.  We  see  that the  ability  of the  Bayesian  imitator  BEBI 
to  adapt  to  the  local  quality  of information  allows  it  to  ex-
ploit the additional  information provided by the mentor more 
quickly  than  agents  using  generic  exploration  strategies  like 
e-greedy.  Again, mentor information is used to great effect to 
overcome misleading priors. 

5  Conclusions 

Bayesian  imitation,  like  the  non-Bayesian  implementation  of 
implicit  imitation,  accelerates  reinforcement  learning  in  the 
presence of other agents  with  relevant knowledge  without  re-
quiring either explicit communication with or the cooperation 
of these other agents.  The Bayesian  formulation is built on an 
elegant  pooling  mechanism  which  optimally  combines  prior 
knowledge, model observations from the imitator's own expe(cid:173)
rience and model observations derived from other agents.  The 
combination of Bayesian  imitation with  Bayesian exploration 
eliminates  parameter tuning  and  yields  an  agent  that  rapidly 
exploits  mentor  observations  to  reduce  exploration  and  in(cid:173)
crease  exploitation. 
In  addition,  imitation  often  overcomes 
one  of the  drawbacks  of Bayesian  exploration,  the  possibil(cid:173)
ity  of converging  to  a  suboptimal  policy  due  to  misleading 
priors.  Bayesian  imitation can easily be extended to multiple 
mentors,  and  though  we  did  not present  the  derivation here, 
it  can  also  be  extended  to  partially  observable environments 
with known state spaces.  Though the Bayesian formulation is 
difficult to implement directly, we have shown that reasonable 
approximations exist that result  in  tractable algorithms. 

There  are  several  very  promising  areas  of future  research 
that can benefit from the current formulation of Bayesian  imi(cid:173)
tation.  One obvious need is to extend the model to the hetero(cid:173)
geneous action setting by incorporating the notions of feasibil(cid:173)
ity testing and repair described in  [Price  and Boutilier, 2001 ]. 
We  are  particularly excited by  the prospects of its  generaliza-
tion to richer environmental and interaction models.  We have 
also  derived  one  possible  mechanism  for using the  Bayesian 
approach  in  domains  with  continuous attributes.  We  hope to 
extend  this  work  to  include  methods  for  discovering  corre-
spondences  between  the  state  and  action  spaces  of various 
agents.  We  also  plan to  introduce  game-theoretic  considera(cid:173)
tions into imitation so that agents can learn solutions to inter(cid:173)
acting tasks  from  experts and  reason  about both  the  reward-
oriented  aspects  of their  action  choices  as  well  as  the  infor(cid:173)
mation it reveals to others. 

Acknowledgements 
This research was supported by the Natural  Sciences and En(cid:173)
gineering Research Council and IRIS. 

References 
[Atkcson and Schaal,  1997]  C.  G.  Atkeson  and S.  Schaal.  Robot 
learning from demonstration.  In Proc. Fourteenth Intl. Conf. on 
Machine Learning, pp. 12-20, Nashville, TN, 1997. 

[Dcarden et aL,  1999]  R.  Dearden,  N.  Friedman,  and  D.  Andre. 
Model-based Bayesian exploration.  In Proc. Fifteenth Conf. on 
Uncertainty  in  Artificial Intelligence,  pp. 150-159,  Stockholm, 
1999. 

[Demiris and Hayes,  1999]  J.  Demins and G.  Hayes.  Active and 
passive routes to imitation. In Proc. AISB '99 Symposium on Imi(cid:173)
tation in Animals and Artifacts, pp.81 -87, Edinburgh, 1999. 

[Hu and Wellman,  1998]  J. Hu and M. P. Wellman. Multiagent rein-
forcement learning:  theoretical framework and an algorithm.  In 
Proc. Fijthteenth Intl. Conf. on Machine Learning, pp.242  250, 
Madison, Wisconsin, 1998. 

[Kaelbling, 1993]  L. Pack Kaelbling.  Learning in Embedded Sys(cid:173)

tems. MIT Press, Cambridge,MA, 1993. 

[Kearns and Singh,  1998]  M.  Kearns  and  S.  Singh.  Finite  sam(cid:173)
ple  convergence  rates  for  Q-learning  and  indirect  algorithms. 
In Eleventh Conf. on Neural Information Processing Systems, 
pp.996  1002, Denver, Colorado,  1998. 

[Kuniyoshi et  al,  1994]  Y.  Kuniyoshi,  M.  Inaba,  and  H.  Inoue. 
Learning by watching:  Extracting reusable task knowledge from 
visual observation of human performance. IEEE Transactions on 
Robotics and Automation, 10(6):799  822, 1994. 

[Littman,  1994]  M. L. Littman.  Markov games as a framework for 
multi-agent reinforcement learning. In Proc. Eleventh Intl. Conf. 
on Machine Learning, pp. 157-163, New Brunswick, NJ, 1994. 

[ Mataric, 2002]  M.J. Mataric. Visuo-motor primitives as a basis for 
learning by imitation:  Linking perception to action and biology 
to robotics.  In Imitation in Animals and Artifacts, pp.392  422, 
Cambridge, MA, 2002. MIT Press. 

[Moore and Atkcson,  1993]  A. W. Moore and C. G. Atkeson.  Pri(cid:173)
oritized sweeping: Reinforcement learning with less data and less 
realtime. Machine Learning,  13(1 ):103-30,  1993. 

[Nehaniv and Dautenhahn, 1998]  C.  Nehaniv  and  K.  Dautcnhahn. 
Mapping between  dissimilar bodies:  Affordanccs  and the alge-
braic foundations of imitation. In Proc. Seventh European Work-
shop on Learning Robots, pp.64-72, Edinburgh, 1998. 

[Ng and Russell, 2000]  A. Y. Ng and S. Russell.  Algorithms for in-
verse reinforcement learning. In Proc. Seventeenth Intl. Conf on 
Machine Learning,  pp.663-670.  Morgan Kaufmann,  San Fran(cid:173)
cisco, CA, 2000. 

[Price and Boutilier,  1999]  B. Price and C. Boutilier. Implicit imita(cid:173)
tion in multiagent reinforcement learning. In Proc. Sixteenth Intl. 
Conf on Machine Learning, pp.325-334, Bled, SI, 1999. 

[Price and Boutilier, 2001]  B.  Price  and  C.  Boutilier. 

Imitation 
and reinforcement learning in agents with heterogeneous actions. 
In  Proc.  Fourteenth  Canadian  Conf  on Artificial Intelligence, 
pp. 111-120, Ottawa, 2001. 

[Sammut et al., 1992]  C.  Sammut,  S.  Hurst,  D.  Kedzier,  and  D. 
Michie.  Learning to  fly.  In Proc. Ninth Intl.  Conf. on Machine 
Learning, pp.385  393, Aberdeen, UK,  1992. 

MULTIAGENT SYSTEMS 

717 


Argumentation Based Contract Monitoring in Uncertain Domains

Nir Oren, Timothy J. Norman, Alun Preece

Department of Computing Science, University of Aberdeen, Aberdeen, Scotland

noren,tnorman,apreece@csd.abdn.ac.uk

Abstract

Few existing argumentation frameworks are de-
signed to deal with probabilistic knowledge, and
none are designed to represent possibilistic knowl-
edge, making them unsuitable for many real world
domains. In this paper we present a subjective logic
based framework for argumentation which over-
comes this limitation. Reasoning about the state
of a literal in this framework can be done in poly-
nomial time. A dialogue game making use of the
framework and a utility based heuristic for play-
ing the dialogue game are also presented. We then
show how these components can be applied to con-
tract monitoring. The dialogues that emerge bear
some similarity to the dialogues that occur when
humans argue about contracts, and our approach is
highly suited to complex, partially observable do-
mains with fallible sensors where determining en-
vironment state cannot be done for free.

1 Introduction

Most research in argumentation theory has focused on inter-
actions between arguments [Prakken and Vreeswijk, 2002].
Researchers have proposed various underlying logics capable
of representing different domain attributes. For example, a
large body of work exists on representing and using defaults
for legal argumentation [Prakken and Sartor, 1998]. A num-
ber of researchers have also looked at how argument takes
place in domains with uncertainty. Most of the work in this
area is probability theory based, and includes Bayesian based
argumentation frameworks [Vreeswijk, 2004] and Pollock’s
OSCAR system [Pollock, 1995].

Due to its basis in plausible reasoning, we believe that un-
certainty theory is more applicable than probabilistic reason-
ing to many domains of argumentation.
In this paper, we
present a subjective logic [Jøsang, 2002] based approach to
argumentation. Using subjective logic, we are able to natu-
rally present arguments in which uncertainty exists, and can
encapsulate concepts such as accrual of arguments [Prakken,
2005], which many existing frameworks handle poorly.

After describing the argumentation framework, we present
a dialogue game which allows agents to make use of the

framework. We then present a utility based heuristic that al-
lows agents to participate in the dialogue game.

One possible application of such a framework is contract
monitoring and enforcement. That is, given a contract and
an environment, determining current contract state as well as
what sanctions should come into effect. Not only is our ap-
proach able to operate in partially observable domains with
fallible sensors, but we also take into account the fact that
probing a sensor might be associated with a cost. Other
systems for contract monitoring often require fully observ-
able domains with “honest” sensors (e.g. [Xu and Jeusfeld,
2003]). Even when able to handle faulty sensors, some ap-
proaches assume that all domain states can be probed, and
that all such probing is free [Daskalopulu et al., 2002].

In the next section, we provide an overview of subjective
logic. After that, we present our framework, and provide an
illustrative example. We conclude the paper by examining re-
lated research, and examine possible avenues of future work.

2 Subjective Logic

Subjective logic1 is an approach for combining and as-
sessing subjective evidence from multiple sources based
An opinion ω about an
on Dempster-Schafer theory.
atomic proposition φ is represented as a triple ω(φ) =
(cid:2)b(φ), d(φ), u(φ)(cid:3) where b(φ) represents the level of belief
that φ holds; d(φ) stands for the level of disbelief (i.e.
the
probability that φ does not hold), and u(φ) represents the
level of uncertainty of the opinion. b(φ) + d(φ) + u(φ) = 1,
and b(φ), d(φ), u(φ) ∈ [0..1].

A number of operations on opinions have been deﬁned, al-
lowing one to create compound opinions from atomic ones.
These are listed in Figure 1.

Subjective logic allows us to use common logical opera-
tors such as conjunction, disjunction and negation. The last
of these is shown in the ﬁgure. The remaining operators in the
ﬁgure are unique to subjective logic. The ⊗ operator is called
the recommendation operator, where ω(φ) ⊗ ω(ψ) yields an
opinion about ω(ψ) given an opinion ω(φ) about the source
informing the computing agent of ω(ψ). The consensus oper-
ator ⊕, yields a combination of two opinions about the same

1We present a simpliﬁed form of subjective logic here, similar to

the one presented in [Daskalopulu et al., 2002].

IJCAI-07

1434

ω(φ) ⊗ ω(ψ) = (cid:2)b(φ)b(ψ), b(φ)d(ψ), d(φ) + u(φ) + b(φ)u(ψ)(cid:3) where φ is an opinion about ψ

ωA(φ) ⊕ ωB(φ) = (cid:2) bA(φ)uB (φ)+uA(φ)bB (φ)

, dA(φ)uB (φ)+uA(φ)dB(φ)

, uA(φ)uB (φ)

(cid:3), k = uA(φ) + uB(φ) − uA(φ)uB(φ)

¬ω(φ) = (cid:2)d(φ), b(φ), u(φ)(cid:3)

k

k

k

Figure 1: The Subjective Logic negation, recommendation and consensus operators

thing. Note that the consensus operator is undeﬁned when no
uncertainty in opinions exists.

3 Formalism

In this section, we present three strands of work that tie up to
form our framework. At the lowest level lies the argumenta-
tion framework. It is here that we deﬁne arguments, and how
they interact with each other. Afterwards, we present a dia-
logue game, as well as the agents which play the game and
the heuristics they use for deciding which arguments to ad-
vance and which sensors to probe. Finally, we describe how
to transform contracts into a form usable by our framework.

3.1 The Argumentation Framework

Many frameworks for argumentation ignore probabilistic and
possibilistic concepts, focusing instead purely on the interac-
tions (such as rebutting and undercutting attacks) between ar-
guments. While important in their own right, applying frame-
works like these to domains in which uncertain evidence ex-
ists is difﬁcult. We thus propose a new argument framework
which, while borrowing ideas from existing work, suggests a
different way of thinking about arguments.

We deﬁne our language of discourse Σ as the set of literals
and their negation, while Σ+
is the set containing only un-
negated literals. A literal l has an opinion ω(l) = (cid:2)bl, dl, ul(cid:3)
associated with it.

An argument A is a tuple (P, c) where P ∈ 2Σ

and c ∈
Σ. We assume that if a literal p appears in P , then ¬p does
not appear, and vice-versa. P represents the premises of an
argument, while c is the argument’s conclusion. A fact a can
be represented by an argument ({}, a) (which we will also
write as (, a)). Let Args(Σ) represent the set of all possible
arguments in the language Σ.

Given a set of arguments and facts, as well as opinions stat-
ing how likely those facts are to be true, we would like to
determine what conclusions we may reasonably be allowed
to draw. This is done by repeatedly computing the conclu-
sions of arguments from known (opinions about) facts, until
nothing more can be deduced. At this stage, we can draw con-
clusions based on what literals are justiﬁed within the system.
A literal may be assigned a value in one of three ways.
First, it could be the conclusion of an argument. Second, it
might be based on a fact. Finally, it could be based on a
combination of values from both arguments and facts.

Given an argument A = (P, c), we assign an opin-
ion to it using a slightly modiﬁed form of statistical syllo-
gism. Let ωpi be the opinion for premise pi ∈ P 2. Then
(cid:3) if c is not negated, and
ω(A) = (cid:2)min(bpi

), 0, 1 − bpi

2Note that if pi is a negated literal, then the subjective logic nega-
tion operator must be applied when computing its contribution to the
argument’s opinion.

), 1 − bpi

(cid:2)0, min(bpi
(cid:3) otherwise. This deﬁnition captures the
intuition that an argument is as strong as its weakest element.
Facts are a special type of argument, as their conclusions
may not be negated literals. An opinion ω(F ) may be associ-
ated with a fact F = (, l) where l ∈ Σ+

.

(cid:2)

(cid:2)

Given a set of arguments about a literal l and its negation,
as well as a set of supporting facts, we may compute a ﬁnal
opinion regarding the value of the literal by utilising the con-
sensus operator ⊕. Slightly more formally, assuming we are
given a set of arguments and facts S as well as their opinions
ω(s), ∀s ∈ S, we may compute an opinion ω(l) for a literal l
¬ω(t) for all s = (Ps, cs), t = (Pt, ct) ∈ S
as
such that cs = l and ct = ¬l. A special case arises when
no uncertainty exists. We handle this case in an intuitively
justiﬁable manner as follows: given opinions (cid:2)b1, d1, 0(cid:3) and
(cid:2)b2, d2, 0(cid:3), we compute bc = b1 + b2 − d1 − d2 and create the
combined opinion (cid:2)bc, 0, 1−bc(cid:3) if bc > 0, and (cid:2)0, −bc, 1+bc(cid:3)
otherwise.

s ω(s)

t

We have shown how to compute the value of a literal given
all the applicable arguments and facts, as well as opinions
on their values. To complete the description of the argu-
mentation framework, we must show how we can determine
whether an argument is applicable, as well as give a procedure
to decide the values of all literals in an instance of a dialogue.
Models of varying complexity have been proposed regard-
ing whether an argument can be advanced [Toulmin, 1958].
In this paper, we use a simple model stating that if the
premises of an argument hold, it can be used. For a premise
to hold, it should exceed some threshold strength. Both com-
puting a premise’s strength, as well as setting the threshold
are domain speciﬁc tasks. For example, the strength of a
literal could be based on its level of uncertainty, strength
of belief, or some other combination of its attributes (see
[Jøsang, 2002] for more details). Terms from human legal
reasoning such as “beyond reasonable doubt” and “on the
balance of probabilities” show us the subjective nature of
the threshold. Nevertheless, we assume that some function
admissible(ω) → [true, false] exists and allows us to com-
pute whether an opinion exceeds the required threshold.

Before describing the algorithm to determine the state of
literals formally, we provide an informal overview. We can
represent the set of arguments under consideration as a di-
rected graph containing two types of nodes; those represent-
ing arguments, and those representing literals. An edge con-
nects literals to arguments in which they appear as premises,
while edges also exist from arguments to those literals which
appear in their conclusion. As shown in Figure 3, such a
graph has two types of edges, namely edges linking negative
literals to positive ones (or vice-versa), and all other edges.
One weakening assumption we make (discussed further in
Section 5) is that the resulting graph is acyclic.

Apart from the arguments, we assume that a set of facts has

IJCAI-07

1435

been put forward. Our algorithm operates by repeatedly for-
ward chaining applicable arguments from the set of facts. In
other words, we compute what literals we can derive from the
facts, and propagate those forward into the arguments, com-
puting literals from the derived arguments, and repeating this
procedure until nothing new can be computed. Two things
complicate this algorithm: accrual of arguments/evidence,
and the threshold of acceptability of literals. We cannot com-
pute an opinion regarding the value of a literal until all the
arguments and facts regarding the literal have been taken into
account. Similarly, we cannot determine the status of an argu-
ment until the value of all its premises have been computed.
As mentioned previously, if a literal’s strength falls below a
certain level of acceptability, the literal cannot be applied.

To deal with these issues, a node’s opinion is only com-
puted once all inﬂuencing opinions have been calculated. If
a literal falls below the threshold of acceptability, we erase
all the arguments in which it appears as a premise. A literal
exceeding this threshold is referred to as admissible.

More formally, given a set of arguments A, and deﬁning the
non-negated set of all literals appearing in A as L, we create
a directed graph of arguments GA = (A, L; PE , NE ) with
nodes A and L. For convenience, we write E = P E ∪ N E.
An edge e appears in PE , going from a ∈ A to l ∈ L if
a is of the form (P, l). An edge would also appear in PE ,
going from l ∈ L to a ∈ A if a = (P, c) and l ∈ P . An
edge will appear in NE , going from a ∈ A to l ∈ L if a is of
the form (P, ¬l). Alternatively, an edge will be a member of
NE , going from l ∈ L to a ∈ A if a = (P, l) and ¬l ∈ P .
In other words, an edge is in N E if it links the negated and
non-negated form of a literal, and is in P E otherwise. Differ-
entiating between these types of edges allows us to compute
an opinion correctly while only storing non-negated literals
in our graph.

Given a set of opinions on facts ω(F ) where F ⊆ A with
the restriction that ∀f = (P, c) ∈ F, P = {}, a graph rep-
resenting a set of arguments generated as described above,
and an admissibility function mapping opinions to truth and
falsehood, we can perform reasoning according to the algo-
rithm shown in Figure 2.

3.2 Agents and the Dialogue Game

The argumentation framework described above allows us to
statically analyse groups of arguments; letting us compute an
opinion about speciﬁc literals. The process of arguing is dy-
namic, with agents introducing arguments at various points in
the conversation. In this section, we specify what our agents
should look like, what environment they argue within, and
the protocol they use to argue with each other. This protocol
is referred to as a dialogue game. The aim of our dialogue
game is “fairness”, that is, allowing an agent to utilise any
arguments at its disposal to win. While many dialogue games
provide restrictions on agent utterances so as to maintain the
focus of a dispute, we leave this task to the heuristic layer of
our framework.
An agent α,

is a tuple (N ame, KB, Ωα, G, C), con-
sisting of its name, N ame, a private knowledge base
KB ⊆ 2Σ
containing arguments, a private opinion base
Ωα = {ωα(l1), . . . , ωα(ln)} storing opinions about literals

Given a graph G = (A, L; PE , NE ),

a set of opinions Ω over a = (, c) ⊆ A where c ∈ Σ
an admissibility function admissible

sourceNodes(t ) = {s|(s, t) ∈ E}
targetNodes(s) = {t|(s, t) ∈ E}

repeat
for each ω(n) ∈ Ω such that n /∈ V
V = V ∪ n
for each t ∈ targetNodes(n)
if sourceNodes(t ) ⊆ V
compute ω(t)
if t ∈ L and ¬admissible(ω(t))
for each r ∈ targetNodes(t )
for each s ∈ targetNodes(r )
E = E \ {(r, s)}
for each s ∈ sourceNodes(r )
E = E \ {(s, r)}

A = A \ targetNodes(t )
Ω = Ω ∪ {ω(t)}

until (cid:11) ∃ω(n) ∈ Ω such that n /∈ V
for each l ∈ L such that l /∈ V
Ω = Ω ∪ {ω(l) = (cid:2)0, 0, 1(cid:3)}

Figure 2: The algorithm for propagating opinions through the
argumentation network. Note that E = P E ∪ N E

l1, . . . , ln ∈ Σ, a function mapping the state of proven and
unproven literals to a utility value G : Σ, Σ+ → (cid:13),and a
record of how much probing actions performed to date have
cost C ∈ (cid:13). Thus, for example, G({¬a, b}, {c, d}) would
return theutility for the state where ¬a and b are proven, and
nothing is known about the status of c and d.

Agents operate within an environment. This environment
is a tuple of the form (Agents, CS, S, admissible, P C)
where Agents is a set of agents, and contains a record of
all the utterances made by an agent (CS ⊆ 2Args(Σ)
), as well
as a set of sensors S. The function admissible operates over
opinions, while P C maps probing actions to costs.

A sensor is a structure (Ωs, Ωp). The set Ωs contains an
opinion for a set of literals L ⊂ Σ+
, and represents the en-
vironment’s opinion about the reliability of the sensor. The
opinion for a literal l ∈ L, is ωs(l) ∈ Ωs. The set of probed
literals Ωp, stores the sensor’s opinions regarding the value of
the literal l, ωp(l) ∈ Ωp. We compute a sensor’s opinion for l
as ωs(l) ⊗ ωp(l).

Agents take turns to put forward a line of argument and
probe sensors to obtain more information about the environ-
ment. Such an action is referred to as an agent’s utterance.
In each turn, the contents of an agent’s utterance is added to
the global knowledge base; any sensors probed are marked
as such (a sensor may not be probed more than once for the
value of a speciﬁc literal), and costs are updated. We are now
in a position to formally deﬁne the dialogue game.

The utterance function utterance : Environment ×
N ame → 2Args(Σ) × P robes takes in an environment and

IJCAI-07

1436

an agent (via its name), and returns the utterance made by the
agent. The ﬁrst part of the utterance lists the arguments ad-
vanced by the agent, while the second lists the probes the
agent would like to undertake. P robes ∈ 2S×Σ+
is the
power set of all possible probes. The domain speciﬁc func-
tion P C : 2P robes → (cid:13) allows us to compute the cost of
performing a probe.Probing costs are deﬁned in this way to
allow for discounts on simultaneous probes, which in turn al-
lows for richer reasoning by agents.

a new environment

that
takes
and returns

We deﬁne a function turn : Environment × N ame →
Environment
in an environment and an
con-
agent
taining the effects of an agent’s utterances.
Given
an environment Env and an agent α, we deﬁne
turn(Env, N ame) =
the turn function as follows:
(N ewAgents, CS ∪ Ar, N ewSensors, P C)
where
Ar, N ewAgents
computed
from the
If
utterance(Env, N ame) = (Ar, P r) then N ewAgents =
Agents \ α ∪ (N ame, KB, Ωα, G, C + P C(P r)) and,
∀(s, l) ∈ P r, where l is a literal sensor s is able to probe,
N ewSensors = Sensors \ s ∪ (Ωs, Ωp ∪ ωp(l)).

and N ewSensors
utterance

are
function.

results

label,

the

of

It should be noted that the utterance depends on agent
strategy; we will deﬁne one possible utterance function in
the next section. Before doing so, we must deﬁne the dialogue
game itself.

are

our

that

agents

assume

We may

named
Agent0, Agent1, . . . , Agentsn−1 where n is the number of
agents participating in the dialogue. We can deﬁne the dia-
logue game in terms of the turn function by setting turn0 =
turn((Agents, CS0, S, admissible, P C), Agent0),
and
then having turni+1 = turn(turni, Agenti mod n
). The
game ends when turni . . . turni−n+1 = turni−n.

CS0 and Ω0 contains any initial arguments and opinions,
and are usually empty. Note that an agent may make a null
utterance {, } during its move to (eventually) bring the game
to an end. In fact, given a ﬁnite number of arguments and
sensors, our dialogue is guaranteed to terminate as eventu-
ally, no utterance will be possible that will change the public
knowledge base CS.

We can compute an agent’s utility by combining its utility
gain (for achieving its goals) with the current costs. At any
stage of the dialogue, given the environment’s CS, and the
set of all opinions probed by the sensors {Ωp|s = (Ωs, Ωp) ∈
S), and the environment’s admissibility function, we can run
the reasoning algorithm to compute a set of proven literals as
proven = {l|l is a literal in CS and admissible(l)}. Liter-
als which fall below the proven threshold, or for which noth-
ing is known are unproven: unproven = { literals in CS \
proven}.
Then the net utility gain for an agent α is
U (α, proven) = G(proven, unproven) − C.

At the end of the dialogue, we assume that agents agree

that literals in the proven set hold in the world.

3.3 The Heuristic
Agents are able to use the underlying argumentation frame-
work and dialogue game to argue. In this section, we will de-
scribe a heuristic which agents may use to argue. Informally,
this heuristic attempts to make utterances which maximise

agent utility using one step lookahead. If multiple arguments
still remain, one is selected at random.

When making an utterance, an agent has to decide what ar-
guments to advance, as well as what probing actions it should
undertake. Given an environment Env and an agent α, let
the set of possible arguments be PA = 2KB. We deﬁne the
set of all possible probes as PP = {(s, l)|s = (Ωs, Ωp) ∈
S such that ωs(l) ∈ Ωs and ωp(l) /∈ Ωp}. The cost for a
probe ⊆ PP is P C(probe).

Our agents use multiple sources of information to estimate
the results of a probe. First, they have opinions regarding lit-
erals, stored in Ωα. Second, they can compute the current
most likely value of the probe by examining sensors that al-
ready contain ωp(l). Lastly, they can infer the most likely
value of l by computing the effects of a probing action.

t∈P

p

(cid:2)

(ωs(l) ⊗ ωt

A naive agent will believe that an unprobed sensor s will
return an opinion ωα(l). A non-naive agent will alter its be-
liefs based on existing probed values. One possible combina-
tion function will have the agent believe that the sensor will
(l)) ⊕ ωα(l) for a literal
return an opinion
l given an agent’s personal belief ωα(l), a set of sensors P
(l) for any t ∈ P , and the unprobed sensor’s
containing ωt
p
ωs(l). Informally, this means that a na¨ıve agent will believe
that a probed sensor will share the same opinion of a literal
as the agent has, while a non-na¨ıve agent will weigh its ex-
pected opinion due to probes that have already been executed.
Different combination functions are possible due to different
weights being assigned to different opinions.

For each possible utterance in PA × 2PP , the agent calcu-
lates what the environment will look like, as well as the net
utility gain (using the functions P C and U ). It then selects
the utterance that will maximise its utility in the predicted fu-
ture environment.

3.4 Contracts
Having fully deﬁned the environment in which argument can
take place, we now examine one possible application domain,
namely contract monitoring. Since contract representations
are highly domain dependent, most of this section provides
only guidelines as to how to transform contracts into a form
suitable for use within the framework. To simplify the dis-
cussion, only two party contracts are examined.

To make use of the framework, a number of things must
be deﬁned. The (domain dependent) sensor set S and prob-
ing cost function P C must be initialised, and the agent’s
goal function G must also be speciﬁed. The agent’s private
knowledge base KB and the environment’s starting knowl-
edge base CS0 must also be populated, as well as the agent’s
prior opinions.

While nothing can be said about the generation of the sen-
sor set except that it should reﬂect the sensors within the en-
vironment, probing costs can often be obtained from the con-
tract. Contract based sanctions and rewards can also be used
to generate G. For example, if in a certain contract state an
agent α must pay β a certain amount of money, the literal
representing that state might have an associated negative util-
ity for α, and positive utility for β. One point that should be
noted when assigning utility to various states is the uneven-
ness of burden of proof [Prakken, 2001]. Assume that β gains

IJCAI-07

1437

(cid:2)0.6, 0.2, 0.2(cid:3), ωs(tr) = (cid:2)0.8, 0.0, 0.2(cid:3), ωs(p) = (cid:2)1, 0, 0(cid:3),
and let the cost of probing each of these sensors be 20, except
for the sensor probing f 1, which has a utility cost of 50.

Finally, let the utility for agent α showing g holds be
200, with agent β having utility -100 for this state. As-
sume α believes that literals c, h, f, ¬tr, ¬p hold with opin-
ion strength (cid:2)0.9, 0, 0.1(cid:3). Also assume that the contract
is initially stored in KB, and the admissibility function is
admissible((cid:2)b, d, u(cid:3)) = b > 0.5 (d > 0.5) for a literal (or its
negation) to be admissible.

Agent α might begin with the following utterance
({(, tr), (, p), ({¬p, ¬tr}, ¬ts), ({¬ts}, g)}, {p, tr}). This
will cost it 40 utility. Given its reasoning algorithm, it will
assume that once probed, ω(tr) = (cid:2)0, 0.72, 0.18(cid:3), ω(ts) =
ω(g) = (cid:2)0, 0.72, 0.18(cid:3). However, the sensor probing p re-
turns (cid:2)0.8, 0.1, 0.1(cid:3), making ¬p fall below the threshold of
acceptability, thus invalidating this chain of argument. Agent
β passes.

Agent α tries again, probing f 1 and advancing the ar-
gument ({f }, g), which returns (cid:2)0.7, 0.1, 0.2(cid:3) resulting in
an opinion (cid:2)0.56, 0.08, 0.36(cid:3) after the ⊗ operator is ap-
This means that g is now valid, with opinion
plied.
(cid:2)0.64, 0.08, 0.28(cid:3).

Agent β counters, probing h and claiming ({h}, ¬g). The
probe returns (cid:2)0.8, 0, 1, 0.1(cid:3), which means the argument is
of sufﬁcient strength to make g inadmissible (as ω(g) ≈
(cid:2)0.47, 0.34, 0.19(cid:3)).

Agent α undercuts β’s argument by probing c and advanc-
ing ({c}, ¬h). Given the sensor’s opinion (cid:2)0.6, 0.2, 0.2(cid:3), this
reinstates g by disallowing argument ({h}, ¬g).

Assume that β believes that f ’s true value is (cid:2)0, 0.8, 0.2(cid:3).
It would then probe f 2 with the utterance (, {f 2}). Assume
this returns an opinion (cid:2)0.1, 0.8, 0.1(cid:3). g would then remain
admissible (ω(g) = (cid:2)0.51, 0.28, 0.21(cid:3)).

Both agents now pass. Since g is admissible α has a net

utility gain of 90, while β’s utility loss is 140.

5 Discussion

As seen in the preceding example, our framework is able to
represent standard argumentation concepts such as rebutting
attacks, defeat and reinstatement, as well as higher level con-
cepts such as accrual of arguments, which many frameworks
have difﬁculty dealing with. When compared to standard ar-
gumentation systems, the main shortcoming of our approach
lies in its inability to handle looping arguments. However,
many other frameworks suffer from a similar (though weaker)
problem [Prakken and Vreeswijk, 2002]. The use of possi-
bilistic rather than probabilistic reasoning makes our frame-
work suitable for situations where uncertainty exists regard-
ing the probabilities to be assigned to an event. Our sample
domain is one area where such reasoning capabilities are use-
ful.

Most argument frameworks can be described in terms of
Dung’s [1995] semantics. These semantics are based on the
notion of defeat. Since defeat in our framework is dynami-
cally calculated, applying these semantics to our framework
is difﬁcult.
Investigating the semantics of our framework
more closely, and looking at how they compare to those of

Figure 3: An argument graph. Dashed lines represent negated
edges, while solid lines are normal edges.

utility for proving a literal l, while α gains if the ﬁrst agent
does not (e.g. an agent has to pay a penalty to another in a cer-
tain state). In such a situation, the second agent loses utility
if l is true. Utility is not lost if l is either false or unproven.

We assume that a trivial mapping exists between contract
clauses and arguments (for example, a clause stating “if a and
b then c” can be represented as ({a, b}, c}). Then one must
decide whether to place the translated clauses into the agent’s
private knowledge base KB, or CS0. Putting them into KB
will mean that agents will advance those clauses relevant to
the dispute during the course of the argument. Having the ar-
guments in CS0 represents a more realistic scenario wherein
the contract is public knowledge. KB can also contain do-
main speciﬁc arguments the agents might want to use in the
course of the dialogue. Generating the agent’s opinions about
the state of literals is also outside the scope of our work.

4 Example

The example we present here is based on a simpliﬁed contract
from the mobile multi–media domain. A supplier has agreed
to provide a consumer with a set of multi–media services, and
the consumer believes that the package it has received has not
lived up to the the provider’s contractual obligations. Assume
that we have a contract of the form

frameRate < 25 → giveSanction
horrorMovie → ¬giveSanction
cinderella → ¬horrorMovie
¬textSent → giveSanction
¬phoneFull , ¬textReceived → ¬textSent

From this, we can generate the argument graph shown in Fig-
ure 3, with the letter in the node being the ﬁrst letter of the
literal in the contract.

Assume further that we have sensors for whether the
movie’s name is “cinderella” (c), whether it is a horror
movie (h), two sensors to detect whether the f rameRate
is less than 25 (f 1, f 2), whether a text was received (tr),
and whether the phone’s memory was full (p). Let the
opinion sensors for these ratings be ωs(c) = (cid:2)1, 0, 0(cid:3),
ωs(h) = (cid:2)0.7, 0.2, 0.1(cid:3), ωs(f 1) = (cid:2)0.8, 0.1, 0.1(cid:3), ωs(f 2) =

IJCAI-07

1438

standard argumentation frameworks is one interesting direc-
tion in which this research can be taken.

Some minor modiﬁcations of our framework might be nec-
essary so that it can be used in other domains. For example,
when arguing in the legal domain, the loser is often obliged
to pay the winner’s costs. By changing the way costs are
computed in the turn function, our dialogue game is able to
function in such an environment.

Our heuristic has high computational complexity. The un-
optimised version presented here runs in exponential time
(O(2n)) for n arguments in the agent’s knowledge base) and
while optimisations are possible, exponential running time is
still possible in the worst case. However, it is easy to gener-
ate solutions of increasing length (and thus probably increas-
ing utility cost). By relaxing the requirement that the heuris-
tic ﬁnd the best possible move (given one move lookahead),
computation can be stopped at any time. It is trivial to show
that the algorithm presented in Figure 2 runs in O(n) time
where n is the number of graph edges. Extending our heuris-
tic to look ahead further is possible, but requires some model
of the opponent’s knowledge base and utility costs.

As a contract monitoring mechanism, our framework is
intended to operate “between” actions. Whenever a state
change occurs, the agent should reason about whether a con-
tract enforcement action should begin. If one does occur, it
should ﬁnish before another state change happens. By inte-
grating our framework with an expressive contract monitor-
ing language, this limitation can be overcome.

Another useful avenue for future work involves examin-
ing how default arguments can best be incorporated in the
framework. The presence of uncertainty allows us to repre-
sent many default rules with the simple addition of a conﬂict-
ing literal edge. However, we have not examined how generic
default rules can best be represented.

Our framework cannot deal with loops in arguments, and
our main short term focus involves addressing this weakness.
In the longer term, we hope to add opinions and weightings to
the arguments themselves (for example stating that one argu-
ment is more persuasive than another). By also having a more
complicated model for when an argument may be used (such
as Toulmin’s [1958] model of argument), this path of research
should allow us to represent argument schemes [Walton and
Krabbe, 1995] in our framework.

Finally, there are clear parallels between our work and the
work done on trust and reputation frameworks [Teacy et al.,
2006]. We intend to investigate whether any of the work done
on opinions in that ﬁeld will be of beneﬁt to our model.

6 Conclusions

In this paper we have put forward a new subjective logic
based framework for analysing combinations of arguments
and opinions. We then showed how this framework could be
used to have agents discuss the state of literals in an envi-
ronment where probing for environment state has an associ-
ated utility cost. After providing strategies for agents to argue
in such an environment, we demonstrated its applicability as
a technique for performing contract monitoring. While our
framework has some limitations, it promises to be a spring-

board for further research in this area of argumentation, and
has applications as a powerful mechanism for contract moni-
toring in complex domains.

7 Acknowledgements
This work is supported by the DTI/EPSRC E-Science
Core Program and BT, via the CONOISE-G project
(http://www.conoise.org).

References
[Daskalopulu et al., 2002] A. Daskalopulu, T. Dimitrakos,
and T. Maibaum. Evidence-based electronic contract per-
formance monitoring. Group Decision and Negotiation,
11(6):469–485, 2002.

[Dung, 1995] Phan Minh Dung. On the acceptability of ar-
guments and its fundamental role in nonmonotonic rea-
soning, logic programming and n-person games. Artiﬁcial
Intelligence, 77(2):321–357, 1995.

[Jøsang, 2002] A. Jøsang. Subjective evidential reasoning.
In Proc. of the 9th Int. Conf. on Information Processing
and Management of Uncertainty in Knowledge-Based Sys-
tems, pages 1671–1678, July 2002.

[Pollock, 1995] J. L. Pollock. Cognitive Carpentry. Brad-

ford/MIT Press, 1995.

[Prakken and Sartor, 1998] Henry Prakken and Giovanni
Sartor. Modelling reasoning with precedents in a for-
mal dialogue game. Artiﬁcial Intelligence and Law, 6(2-
4):231–287, 1998.

[Prakken and Vreeswijk, 2002] H.

and
G. Vreeswijk.
Logics for defeasible argumentation.
In Handbook of Philosophical Logic, 2nd Edition,
volume 4, pages 218–319. Kluwer, 2002.

Prakken

[Prakken, 2001] H. Prakken. Modelling defeasibility in law:
Fundamenta Informaticae, 48(2-

Logic or procedure?
3):253–271, 2001.

[Prakken, 2005] Henry Prakken. A study of accrual of argu-
ments, with applications to evidential reasoning. In Proc.
of the 10th Int. Conf. on Artiﬁcial Intelligence and Law,
pages 85–94, 2005.

[Teacy et al., 2006] W. T. L. Teacy, J. Patel, N. R. Jennings,
and M. Luck. Travos: Trust and reputation in the context
of inaccurate information sources. Autonomous Agents
and Multi-Agent Systems, 12(2):183–198, 2006.

[Toulmin, 1958] Stephen E. Toulmin. The Uses of Argument.

Cambridge University Press, 1958.

[Vreeswijk, 2004] G. A. W. Vreeswijk. Argumentation in
Bayesian belief networks. In Proc. of ArgMAS 2004, num-
ber 3366 in LNAI, pages 111–129. Springer, 2004.

[Walton and Krabbe, 1995] D. N. Walton and E. C. W.
Krabbe. Commitment in Dialogue. State University of
New York Press, 1995.

[Xu and Jeusfeld, 2003] L. Xu and M. A. Jeusfeld. Pro-
active monitoring of electronic contracts, volume 2681 of
LNCS, pages 584–600. Springer, 2003.

IJCAI-07

1439


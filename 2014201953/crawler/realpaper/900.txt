Ensembles of Partially Trained SVMs with Multiplicative Updates

Ivor W. Tsang

James T. Kwok

Department of Computer Science and Engineering

Hong Kong University of Science and Technology, Hong Kong

{ivor,jamesk}@cse.ust.hk

Abstract

The training of support vector machines (SVM) in-
volves a quadratic programming problem, which is
often optimized by a complicated numerical solver.
In this paper, we propose a much simpler approach
based on multiplicative updates. This idea was ﬁrst
explored in [Cristianini et al., 1999], but its con-
vergence is sensitive to a learning rate that has to
be ﬁxed manually. Moreover, the update rule only
works for the hard-margin SVM, which is known
to have poor performance on noisy data. In this pa-
per, we show that the multiplicative update of SVM
can be formulated as a Bregman projection prob-
lem, and the learning rate can then be adapted au-
tomatically. Moreover, because of the connection
between boosting and Bregman distance, we show
that this multiplicative update for SVM can be re-
garded as boosting the (weighted) Parzen window
classiﬁers. Motivated by the success of boosting,
we then consider the use of an adaptive ensemble
of the partially trained SVMs. Extensive experi-
ments show that the proposed multiplicative update
rule with an adaptive learning rate leads to faster
and more stable convergence. Moreover, the pro-
posed ensemble has efﬁcient training and compa-
rable or even better accuracy than the best-tuned
soft-margin SVM.

1 Introduction

Kernel methods, such as the support vector machines
(SVMs), have been highly successful in many machine learn-
ing problems. Standard SVM training involves a quadratic
programming (QP) problem, which is often solved by a com-
plicated numerical solver. Moreover, while a general-purpose
QP solver can be used, they are inefﬁcient on this particu-
lar type of QPs. Consequently, a lot of specialized optimiza-
tion techniques have been developed. The most popular ap-
proach is by using decomposition methods such as sequential
minimization optimization (SMO) [Platt, 1999], which in-
volves sophisticated working set selection strategies, advance
caching schemes for the kernel matrix and its gradients, and
also heuristics for stepsize prediction.

A much simpler approach is to use multiplicative updates,
which was ﬁrst explored in [Cristianini et al., 1999]. How-
ever, its convergence is sensitive to a learning rate that has to
be ﬁxed manually. When the learning rate is too large, over-
shooting and oscillations occur; while when it is too small,
convergence is slow.

A second problem with the multiplicative update rule is
that it can only work with the hard-margin SVM. However,
on noisy data, the hard-margin SVM has poor performance
and the soft-margin SVM, which can trade off complexity
with training error, is usually preferred. However, the choice
of this tradeoff parameter, which can be from zero to inﬁn-
ity, is sometimes critical. Various procedures have been pro-
posed for its determination, such as by using cross-validation
or some generalization error bounds [Evgeniou et al., 2004].
However, they are typically very computationally expensive.
Another possibility that avoids parameter tuning com-
pletely is by using the ensemble approach. [Kim et al., 2003]
combines multiple SVMs with different parameters, and ob-
tains improved performance. However, as a lot of SVMs still
have to be trained, this approach is also very expensive.

In this paper, we ﬁrst address the learning rate problem by
formulating the multiplicative update of SVM as a Bregman
projection problem [Collins and Schapire, 2002]. It can then
be shown that this learning rate can be adapted automatically
based on the data. Moreover, because of the connection be-
tween boosting and Bregman distance [Collins and Schapire,
2002; Kivinen and Warmuth, 1999], we show that this mul-
tiplicative update can be regarded as a boosting algorithm in
which the (weighted) Parzen window classiﬁers are the base
hypotheses, and each base hypothesis added to the ensemble
is a partially trained hard-margin SVM. Now, in the boosting
literature, it is well-known that the whole ensemble performs
better than a single base hypothesis. Hence, to address the
possibly poor performance of the hard-margin SVM, we con-
sider using the whole ensemble of partially trained SVMs for
prediction. Note that this comes at little extra cost in the mul-
tiplicative updating procedure. Experimentally, this ensemble
is observed to have comparable or even better accuracy than
the best-tuned soft-margin SVM.

The rest of this paper is organized as follows. The learn-
ing rate problem in multiplicative update is addressed in Sec-
tion 2. The connection between this update process and
boosting, together with the proposed ensemble of partially

IJCAI-07

1089

trained hard-margin SVMs, is then discussed in Section 3.
Experimental results are presented in Section 4, and the last
section gives some concluding remarks.

2 Multiplicative Updating Rule of SVM

In this paper, we focus on a particular variant of the hard-
margin SVM, called ρ-SVM in the sequel, which will be
introduced in Section 2.1. We then develop in Sections 2.2
and 2.3 an iterative multiplicative updating procedure based
on the Bregman projection [Collins and Schapire, 2002;
Kivinen and Warmuth, 1999]. The convergence of this it-
erative procedure is shown in Section 2.4.

2.1 Hard-Margin ρ-SVM
Given a training set {xi, yi}m
and yi ∈ R,
the ρ-SVM ﬁnds the plane f (x) = w(cid:2)ϕ(x) in the kernel-
induced feature space (with feature map ϕ) that separates the
two classes with maximum margin ρ/(cid:3)w(cid:3):

i=1, where xi ∈ R

d

2ρ − (cid:3)w(cid:3)2

: yiw

(cid:2)ϕ(xi) ≥ ρ, i = 1, . . . , m.

max
w,ρ

The corresponding dual is:

(cid:2) ˜Kα : α ≥ 0, α

(cid:2)

α

1 = 1,

min

α

(1)

(2)

where 0 and 1 are vectors of zero and one respectively, α =
[α1, . . . , αm](cid:2) is the vector of Lagrange multipliers, and ˜K =
[yiyjk(xi, xj )]. It can be easily shown that
mX

mX

w =

αiyiϕ(xi), and f (x) =

αiyik(xi, x).

(3)

i=1

i=1

Because of the zero duality gap, the objectives in (1) and (2)
are equal at optimality1, i.e.,

ρ∗ = α

∗(cid:2) ˜Kα

∗.

(4)

Denote

u(α, ¯ρ) =

1
2

(cid:2) ˜Kα − ¯ρ(α

α

(cid:2)

1 − 1),

where ¯ρ is the Lagrangian multiplier for the dual constraint
(cid:2)1 = 1. Moreover, the Karush-Kuhn-Tucker (KKT) condi-

α
tions lead to:

0 ≤ α

∗⊥ ∂u(α, ¯ρ∗)

∂α

≥ 0,

(5)

(cid:2)(cid:2)(cid:2)(cid:2)

α∗

where ⊥ denotes that the two vectors are orthogonal, and

∂u(α, ¯ρ)

∂α

= ˜Kα − ¯ρ1 = [y1f (x1) − ¯ρ, . . . , ymf (xm) − ¯ρ](cid:2), (6)

on using (3). Moreover, the optimal ¯ρ (i.e., the dual variable
of the dual) is indeed equal to the optimal primal variable ρ:

¯ρ∗ = ρ∗,

as ¯ρ∗ = ¯ρ∗

∗(cid:2)

α

1 = α

∗(cid:2) ˜Kα

∗ = ρ∗ on using (4), (5) and (6).

2.2 Bregman Projection

m | α ≥ 0, α

As α ∈ Pm = {α ∈ R

In this section, we assume that we have access to the optimal
value of ρ∗. We will return to the issue of determining ρ∗ in
(cid:2)1 = 1}, the m-
Section 2.3.
dimensional probability simplex, a natural choice of the Breg-
man distance is the (unnormalized) relative entropy. At the t-
th iteration, we minimize the Bregman distance between the
1 , . . . , α(t)
new α and the current estimate αt = [α(t)
m ](cid:2), while
(cid:2)( ˜Kαt − ρ∗1) = 0,
requiring α to satisfy α
which is similar to the constraint in (5). We then have the
following entropy projection problem:
”

(cid:2) ∂u(αt,ρ∗)

= α

“

∂α

mX

αi ln

− αi + α(t)

i

: ρ∗ = α

(cid:2) ˜Kαt.

(7)

min
α∈Pm

i=1

αi
α(t)

i

α

(cid:5)

(cid:4)

is needed) and on setting the derivative of the Lagrangian

Introducing Lagrange multipliers for the constraints ρ∗ =
(cid:2) ˜Kαt and α
(cid:2)1 = 1 (it will be shown that the remaining
constraint α ≥ 0 is inactive and so no Lagrange multiplier
(cid:3)m
(cid:2)1−1)
ρ∗ − α
i=1 αi ln αi
α(t)
i exp(−ηtyift(xi) +
w.r.t. α to zero, we have αi = α(t)
λ) on using (3).
(cid:2)1 =
(cid:3)m
1, we then obtain λ = − ln Zt(ηt), where Zt(ηt) =
i=1 α(t)

Substituting this back into α

−αi +α(t)

i −ηt

−λ(α

(cid:2) ˜Kαt

i exp(−ηtyift(xi)), and thus
α(t+1)

i exp(−ηtyift(xi))/Zt(ηt).

= α(t)

(8)

i

i

Such an update is commonly known as multiplicative up-
date [Cristianini et al., 1999] or exponentiated gradient (EG)
[Kivinen and Warmuth, 1997], with ηt playing the role of
the learning rate. Notice that by initializing2 α1 > 0, then
αt > 0 in all subsequent iterations. Hence, as mentioned
earlier, α ≥ 0 in (7) is an inactive constraint. Finally, it can
be shown that ηt can be obtained from the dual of (7), as:

− ln(Zt(ηt) exp(ρ∗ηt)).

max

ηt

(9)

As mentioned in Section 1, a similar multiplicative updat-
ing algorithm for the standard hard-margin SVM is also ex-
plored in [Cristianini et al., 1999]. However, the learning rate
η there is not adaptive and the performance is sensitive to the
manual choice of η. On the other hand, for our ρ-SVM vari-
ant, the value of ηt can be automatically determined from (9).
The improved convergence properties of our adaptive scheme
will be experimentally demonstrated in Section 4.1.

2.3 Estimating the Value of ρ∗
We now return to the question of estimating ρ∗. Recall that
we initialize α as α1 = 1
1. From (2) and (4), ρ∗ is equal to
the optimal dual objective (which is to be minimized), and so
∗ should yield a smaller objective than that by
the optimal α
α1 (which is equal to ¯ρ1 = 1
m2 1(cid:2) ˜K1). Moreover, from (4),
we have ρ∗ ≥ 0 as K (cid:7) 0. Hence, ρ∗ ∈ [0, ¯ρ1].
of ρt’s such that ¯ρ1 ≥ ρt ≥ ρ∗

In the following, we consider using a decreasing sequence
. Consequently, the constraint

m

1Here, variables at optimality are denoted by the superscript ∗.

2In this paper, we initialize as α1 = 1

m

1.

IJCAI-07

1090

in (7) has to be replaced by ρt ≥ ρ∗ = α

(cid:2) ˜Kαt, and the

entropy projection problem becomes
”

“

mX

min
α∈Pm

i=1

αi
α(t)

i

αi ln

− αi + α(t)

i

:

ρt ≥ α

(cid:2) ˜Kαt.

(10)

Let ¯ρt ≡ α
Proposition 1. For any 1 > t > 0, if ¯ρt satisﬁes

˜Kαt. We ﬁrst consider the feasibility of (10).

(cid:2)
t

1 − t
1 + t

¯ρt

≥ ρ∗ > 0,

(11)

then there exists an α ∈ Pm such that
Proof. Since ˜K (cid:7) 0, we have v(cid:2) ˜Kv ≥ 0 for any vector v.
Put v = αt − α

≥ α

(cid:2) ˜Kαt.

1+t

¯ρt

∗, then
˜Kαt ≥ 2α

(cid:2)
α
t

∗(cid:2) ˜Kαt − α

(12)

From (4), the condition

1 − t
1 + t

¯ρt

≥ ρ∗ ⇒ α

(cid:2)
t

˜Kαt

Summing (12) and (13), we have

¯ρt

1 + t

≥ α

∗(cid:2) ˜Kαt,

∗.

∗(cid:2) ˜Kα
1 − t
1 + t

≥ α

∗(cid:2) ˜Kα

∗. (13)

(14)

and thus α = α

∗ satisﬁes the condition.

Hence, when ¯ρt

1−t
1+t

≥ ρ∗, a feasible solution of (10) ex-

ists. We set αt+1 as the optimal α of (10). Notice that (10)
differs from (7) only in that the equality constraint in (7) now
becomes an inequality constraint. Hence, optimization of
(10) is almost exactly the same as that in Section 2.2, except

that optimization of ηt now has an extra constraint η ≥ 0.

Subsequently, we set ¯ρt+1 = α

(cid:2)
t+1

˜Kαt+1 and ρt+1 ac-

cording to

ρt =

¯ρt

1 + t

.

From the KKT condition of (10), ηt(α
This, together with3 ηt > 0, leads to

(cid:2)
α
t+1

˜Kαt = ρt.

Using (3), then

cos ωt,t+1 =

√

(cid:2)
α
t+1
¯ρt

√
˜Kαt
¯ρt+1

=

1

1 + t

(15)

˜Kαt − ρt) = 0.

(cid:2)
t+1

(cid:6)

¯ρt
¯ρt+1

,

(16)

(17)

where ωt,t+1 is the angle between wt and wt+1. Recall that
this entropy projection procedure is used to solve the dual in
(2). When (2) starts to converge, the angle ωt,t+1 tends to
zero, and cos ωt,t+1 → 1. As t > 0, if ¯ρt+1 > ¯ρt, from
(17), the optimization progress becomes deteriorated. This
implies that the current t is too large, and we have overshot
and this solution is discarded. This is also the case when (10)
is infeasible. Instead, we can relax the constraint in (10) by

setting t ← t/2 to obtain a larger ρt. This is repeated until
¯ρt+1 ≤ ¯ρt. Note that we always have ρ∗ ≤ ··· ≤ ¯ρt+1 ≤ ¯ρt
as ρ∗ = α
∗ is optimal. Optimization of (10)
then resumes with the new αt+1 and ρt+1. The whole process
iterates until t is smaller than some termination threshold .
The complete algorithm4 is shown in Algorithm 1.

∗ and α

∗(cid:2) ˜Kα

3When ηt = 0, no process can be made as we achieve the global

optimal. Hence, we terminate the algorithm and αt = α

∗.

4In the experiments, we use  = 0.005 and initialize 1 = 0.1.

Algorithm 1 Training the ρ-SVM.
1: Input: S = {(xi, yi)}i=1,...,m, tolerance 1 and .
2: Initialize t = 1: α(1)

i = 1/m for all i = 1, . . . , m, and

¯ρ1 = α
(cid:2)
1

˜Kα1.

3: Use (3) to compute ft(xi).
4: Set ρt = ¯ρt/(1 + t).
5: Set ηt = arg minη≥0 Zt(η) exp(ρtη). If ηt < 0, then
t = t/2, and goto Step 8 to test whether t becomes too
small; else if ηt = 0, then set T = t − 1 and terminate.
6: Update the Lagrangian multipliers αi’s using (8), as
i exp (−ηtyift(xi)) /Zt(ηt), and compute
˜Kαt+1.
7: If ¯ρt ≥ ¯ρt+1, then t+1 = t and t ← t + 1 and goto
8: If t ≥ , then goto Step 4; else set T = t − 1 and
terminate.
9: Output: f

Step 3; else t = t/2 and check whether t < .

α(t+1)
¯ρt+1 = α

SVM(x) = fT +1(x).

= α(t)

(cid:2)
t+1

i

2.4 Convergence
In this section, we will design an auxiliary function, which is
then used to lower bound the amount that the loss decreases at
each iteration [Collins and Schapire, 2002]. Denote the rela-
tive entropy (which is our Bregman distance here) by Δ(·,·).
∗ solution of (2) is in the intersec-
From (14), the optimal α
tion of all the hyperplanes deﬁned by the linear constraints

(cid:2) ˜Kαt for all t’s. Using (14), (15) and (16), we have

ρt ≥ α

˜Kαt ≥ α

∗(cid:2) ˜Kαt.

(cid:2)
α
t+1

(18)

Now, the decrease in loss at two consecutive iterations is:

(cid:5)
∗, αt+1)
− ln α(t)

i

(cid:2)
t1 = α
(as α

(cid:2)
t+11 = 1)

α∗
i

i

∗, αt) − Δ(α
(cid:4)
ln α(t+1)
m(cid:7)
m(cid:7)

i=1

i

α(t+1)
(cid:4)

i=1

Δ(α

m(cid:7)

=

i=1

= −ηt

≥ −ηt
m(cid:7)

=

i=1

i yift(xi) − ln Zt(ηt)
α∗

(using (8))

yift(xi) − ln Zt(ηt)

(using (18))

(cid:5)

α(t+1)

i

ln α(t+1)

i

− ln α(t)

i

,

(using (8))

or, for all t,

∗, αt) − Δ(α

∗, αt+1) ≥ Δ(αt+1, αt) ≡ A(αt), (19)
Δ(α
where A(·) is the auxiliary function. As A(·) never increases
and is bounded below by zero (as Δ(·,·) ≥ 0), the sequence
of A(αt)’s converges to zero. Since α ∈ Pm is compact, by
the continuity of A and the uniqueness of α
∗, the limit of this
∗.
sequence of minimizers converges to the global optimum α
t =
. In
the special case where ft(xi) ∈ [−1, 1], (cid:3) ˜Kαt(cid:3)∞ ≤ 1. Us-
ing the H¨older’s inequality, we have (cid:3)αt+1 − αt(cid:3)1 ≥ t ¯ρt

Moreover, using (15) and (16), we have α
¯ρt

t = ¯ρt, then |(αt − αt+1)(cid:2) ˜Kαt| =

t+1(cid:2) ˜Kα
t ¯ρt
1+t

t(cid:2) ˜Kα

. As α

1+t

.

1+t

IJCAI-07

1091

We then apply the Pinsker’s inequality [Fedotov et al., 2001]
and obtain

Δ(αt+1, αt) >

t ¯ρ2
2

t

2(1 + t)2

.

Let ν = min

t ¯ρt
1+t

, and summing up (19) at each step, then

Δ(α

∗, α1) − Δ(α
⇒ ln m ≥ Δ(α

∗, αT +1) ≥ T(cid:7)

t=1

∗, α1) > T ν2/2,

Δ(αt+1, αt),

and so the number of iterations required is at most T =
ν2 (cid:14) for a ﬁxed . This gives a faster ﬁnite convergence
(cid:13) 2 ln m
result than the linear asymptotic convergence of the SMO al-
gorithm [Lin, 2001].

Assuming that (11) is satisﬁed for all t and following the
convergence proof here5, the solution of the ρ-SVM’s dual,
αt → α
˜KαT +1 →
ρ∗. Thus, the ﬁnal solution is the desired ρ-SVM classiﬁer.

∗ in at most T iterations such that α

(cid:2)
T +1

3 Ensemble of Partially Trained SVMs
There is a close resemblance between the update rules of the
ρ-SVM with variants of the AdaBoost algorithm. In partic-
ular, we will show in Section 3.1 that the algorithm in Sec-
tion 2.2 (with ρ∗ assumed to be known) is similar to the
AdaBoost algorithm (Algorithm 2); while that in Section 2.3
(with ρ∗ unknown) is similar to the AdaBoost∗
ν algorithm
[R¨atsch and Warmuth, 2005].
Inspired by this connection,
in Section 3.2, we consider using the output of the boosting
ensemble for improved performance. The time complexity
will be considered in Section 3.3.

Algorithm 2 AdaBoost [R¨atsch and Warmuth, 2005]

1: Input: S = {(xi, yi)}i=1,...,m; Number of iteration T .
2: Initialize: d(1)
3: for t = 1, . . . , T do
4:

Train a weak classiﬁer w.r.t. the distribution di for each
pattern xi to obtain a hypothesis ht ∈ [−1, 1].

i = 1/m for all i = 1, . . . , m.

5:

Set

Nt(c) exp(c),

(20)

ct = arg min
c≥0

(cid:3)m
i=1 d(t)

i exp (−cyiht(xi)).

where Nt(c) =
Update distributions di:

6:

d(t+1)
i

i exp (−ctyiht(xi)) /Nt(ct). (21)

= d(t)
(cid:3)T

t=1

7: end for
8: Output: f (x) =

ctP

T

r=1 cr

ht(x).

3.1 ρ-SVM Training vs Boosting
By comparing the update rules of α(t)
with those of d(t)

and ηt in Section 2.2
and ct in AdaBoost, we can see that they

i

i

5If ¯ρt

1−t
1+t

< ρ∗, then ρ∗ ≤ α

(cid:2)
t

˜Kαt < ρ∗(1 + 2t
1−t

) which

satisﬁes the loose KKT condition for the ρ-SVM.

i

= α

i playing the role of d(t)

are identical, with α(t)
(compare (8)
and (21)), and ηt the role of ct (compare (9) and (20)). This is
a consequence of the fact that boosting can also be regarded
as entropy projection [Kivinen and Warmuth, 1999]. From
this boosting perspective, we are effectively using base hy-
pothesis of the form (3), with α ∈ Pm (not necessarily equal
∗). This base hypothesis can be regarded as a variant of
to α
the Parzen window classiﬁer, with the patterns weighted by
α. Note that the edge6 of the base hypothesis at the t-th itera-
(cid:2) ˜Kαt. Thus,
tion is
the constraint in (7) is the same as requiring that the edge of
the base hypothesis w.r.t. the αi distribution to be ρ∗.

(cid:5)
i yik(xi, xj )

(cid:4)(cid:3)m

j=1 α(t)

(cid:3)m

i=1 αiyi

The same correspondence also holds between the ρ-SVM
algorithm (Algorithm 1) and the AdaBoost∗
ν algorithm7,
with additionally that ρt in ρ-SVM is analogous to t in
ν. Moreover,  in the ρ-SVM is similar to ν in
∗
AdaBoost
AdaBoost∗
ν in controlling the approximation quality of the
maximum margin. Note, however, that ν is quite difﬁcult to
set in practice. On the other hand, our t can be set adaptively.
There have been some other well-known connections be-
tween SVM and boosting (e.g., [R¨atsch, 2001]). However,
typically these are interested in relating the combined hypoth-
esis with the SVM, and the fact that boosting uses the L1
norm while SVM uses the L2 norm. Here, on the other hand,
our focus is on showing that by using the weighted Parzen
window classiﬁer as the base hypothesis, then the last hy-
pothesis is indeed a SVM.

3.2 Using the Whole Ensemble for Prediction
As mentioned in Section 1, the hard-margin SVM (which is
the same as the last hypothesis) may have poor performance
on noisy data. Inspired by its connection with boosting above,
we will use the boosting ensemble’s output for prediction:

T(cid:7)

t=1

ηt(cid:3)T

r=1 ηr

ft(x),

which is a convex combination of all the partially trained
SVMs. Note that this takes little extra cost. As evidenced
in the boosting literature, the use of such an ensemble can
lead to better performance, and this will also be experimen-
tally demonstrated in Section 4.

3.3 Computational Issue
In (8), each step of the EG update takes O(m2) time, which
can be excessive on large data sets. This can be alleviated by
performing low-rank approximations on the kernel matrix K
[Fine and Scheinberg, 2001], which takes O(mr2) time (r is
the rank of K). Then, the EG update and computation of ¯ρt
both take O(mr), instead of O(m2), time.

4 Experiments
In this section, experiments are performed on ﬁve small
(breast cancer, diabetis, german, titanic and waveform)8 and

6The edge of the hypothesis ht is γt =
7In the AdaBoost∗

ν algorithm,  in Algorithm 2 is replaced by

i=1 d(t)

i yiht(xi).

t = minr=1,...,t γr − ν, where γt is the edge of ht.

Pm

8http://ida.ﬁrst.fraunhofer.de/projects/bench/benchmarks.htm

IJCAI-07

1092

seven medium-to-large real-world data sets (astro-particles,
adult1, adult2, adult3, adult, web1 and web2)9 (Table 1).

Table 1: A summary of the data sets used.

dim # training patterns

# test patterns

data set
cancer
diabetis
german
titanic

9
8
20
3
waveform 21
4

astro
adult1
adult2
adult3
adult
web1
web2

123
123
123
123
300
300

200
468
700
150
200
3,089
1,605
2,265
3,185
32,561
2,477
3,470

77
300
300
2,051
4,600
4,000
30,956
30,296
29,376
16,281
47,272
46,279

4.1 Adaptive Learning Rate

We ﬁrst demonstrate the advantages of the adaptive learning
rate scheme (in Section 2.2). Because of the lack of space, we
only report results on breast cancer and waveform. Similar
behavior is observed on the other data sets. For comparison,
we also train the ρ-SVM using the multiplicative updating
rule (with ﬁxed η) in [Cristianini et al., 1999].

As can be seen from Figure 1, the performance of [Cris-
tianini et al., 1999] is sensitive to the ﬁxed choice of η. When
η is small (e.g., η = 0.01 or smaller), the objective value de-
creases gradually. However, when η is slightly larger (say,
at 0.02), the estimate will ﬁnally over-shoot (as indicated by
the vertical lines) and convergence can no longer be obtained.
On the other hand, our proposed adaptive scheme always con-
verges much faster.

100

10-1

10-2

10-3

e
v
i
t
c
e
b
o

j

10-4

100

ρ-SVM(adaptive)
SVM(η=0.005)
SVM(η=0.01)
SVM(η=0.02)
SVM(η=0.04)

100

e
v
i
t
c
e
b
o

j

10-1

10-2

ρ-SVM(adaptive)
SVM(η=0.005)
SVM(η=0.01)
SVM(η=0.02)
SVM(η=0.04)

101

102

#iterations
(a) cancer

103

10-3

100

101

102

103

#iterations

(b) waveform

Figure 1: Comparing the EG update (with ﬁxed η) with our
adaptive scheme.

4.2 Accuracy and Speed

Next, we show that the proposed ensemble of partially trained
hard-margin SVMs (Section 3.2) has comparable perfor-
mance as the best-tuned soft-margin SVM (C-SVM). We use
the Gaussian kernel k(xi, xj) = exp(−(cid:3)xi − xj(cid:3)2/β), with

9astro-particles

from
http://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/, while
others are from http://research.microsoft.com/users/jplatt/smo.html.

downloaded

is

(cid:3)m

(cid:8)(cid:8)(cid:8)xi − 1

m

(cid:3)m

(cid:8)(cid:8)(cid:8)2

m

i=1

β = 1
Table 1 comes with 100 realizations, and the performance is
obtained by averaging over all these realizations. However,
for each large data set, only one realization is available.

. Each small data set in

j=1 xj

The ensemble is compared with the following:

1. SVM (BEST): The best-performing SVM among all the
C-SVMs obtained by the regularization path algorithm10
[Hastie et al., 2005];

2. SVM (LOO): The C-SVM which has the best leave-one-
out-error bound [Evgeniou et al., 2004] among those ob-
tained by the regularization path algorithm;

3. the Parzen window classiﬁer, which is the ﬁrst hypothe-

sis added to the ensemble;

4. the (hard-margin) ρ-SVM, which is the last hypothesis;
5. the proposed ensemble.

All these are implemented in Matlab11 and run on a 3.2GHz
Pentium–4 machine with 1GB RAM.

Table 2: Testing errors (in %) on the different data sets.

data set
cancer
diabetis
german
titanic

waveform

astro
adult1
adult2
adult3
web1
web2

SVM

(BEST)

23.1
24.8
22.6
22.7
11.2
5.8
16.3
16.0
16.1
2.3
2.1

SVM
proposed
(LOO) window SVM ensemble

Parzen

ρ-

26.1
28.9
24.9
25.5
18.8
9.3
17.7
17.4
16.2
2.8
2.2

27.4
33.4
29.8
23.3
22.0
7.0
24.1
24.0
24.1
3.0
3.0

33.5
34.5
29.3
45.2
11.2
22.1
19.4
22.8
19.7
14.4
3.4

28.2
24.8
23.7
22.5
10.0
3.4
15.9
15.8
15.9
2.8
2.1

As can be seen from Table 2, the ensemble performs much
better than the (hard-margin) ρ-SVM. Indeed, its accuracy is
comparable to or sometimes even better that of SVM(BEST).
Table 3 compares the time12 for obtaining the ensemble with
that of running the regularization path algorithm. As can be
seen, obtaining the ensemble is much faster, and takes a small
number of iterations. To illustrate the performance on large
data sets, we experiment on the full adult data set in Table 1.
The regularization path algorithm cannot be run on this large
data. So, we use instead a ν-SVM (using LIBSVM13) for
comparison, where ν (unlike the C parameter in the C-SVM)

10Alternatively, one can ﬁnd the best-tuned SVM by performing
a grid search on the soft-margin parameter. However, as each grid
point then requires a complete re-training of the SVM, the regular-
ization path algorithm is usually more efﬁcient [Hastie et al., 2005].
11The inner QP in the regularization path algorithm is solved by
using the optimization package Mosek (http://www.mosek.com).
12This includes the time for computing the kernel matrix, which
can be expensive on large data sets. Moreover, time for computing
the leave-one-out bound is not included in the timing of SVM(LOO).
from

implementation)

13Version

(C++

2.8.1

http://www.csie.ntu.edu.tw/ cjlin/libsvm/.

IJCAI-07

1093

Table 3: Training time (in seconds) and number of iterations.

regularization path

data set
cancer
diabetis
german
titanic

waveform

astro
adult1
adult2
adult3
web1
web2

time
0.4
3.1
22.2
0.08
2.7

180.3
243.3
632.7
957.3
2,853.7
6,978.7

proposed ensemble
time
#iterations
0.03
0.1
0.3
0.08
0.1
9.5
10.7
23.1
49.1
64.3
129.9

62.3
65.5
44.9
434.4
42.4
310
42
40
104
33
125

can only be in [0, 1]. In the experiments, we further restrict
ν to [0.05, 0.45] as the optimization becomes infeasible when
ν falls outside this range. Moreover, low-rank approximation
of the kernel matrix as mentioned in Section 3.3 is used.

As can be seen from Figure 2, training of the single best-
tuned ν-SVM takes 348.9s, while training the whole ensem-
ble takes 3,555.1s, which is thus around 10 times slower.
However, recall that ours is implemented in Matlab while
LIBSVM is in C++. Moreover, as shown in Figure 2(b),
SVM’s training time is highly dependent on the value of ν.
Hence, in practice, one has to perform SVM training together
with ν parameter selection, which can be much more expen-
sive than ensemble training.

21

20

19

18

17

16

)

%
 
n
i
(
 
e
t
a
r
 
r
o
r
r
e
 
t
s
e
t

15
0

ensemble
ν-SVM

104

)
s
 

n
i
(
 

e
m

i
t
 

103

ensemble
ν-SVM

i

g
n
n
a
r
t

i

0.1

0.2

ν

0.3

0.4

0.5

(a) Testing error.

102
0

0.1

0.2

ν

0.3

0.4

0.5

(b) Training time.

Figure 2: Training of the ensemble vs training of a single ν-
SVM on the adult data.

5 Conclusion

In this paper, we show that the hard-margin SVM can be
trained by a simple multiplicative updating rule. By cast-
ing its training as a Bregman projection problem, the optimal
learning rate can be adaptively obtained from the data, and
a ﬁnite convergence result can also be derived. Moreover,
inspired from a connection between boosting and Bregman
projection, we propose an ensemble classiﬁer consisting of
the partially trained hard-margin SVMs. Though its base hy-
pothesis are hard-margin SVMs, experimental results show
that this ensemble is resistant to overﬁtting and has compara-
ble performance with the best-tuned soft-margin SVM. More-
over, training the whole ensemble (which in Matlab) is only
10 times slower than that of training the best-tuned SVM (in

C++). Thus, we can obtain an accurate classiﬁer while avoid-
ing tedious tuning of the soft-margin parameter.

One problem with the ensemble is that its solution is no
longer sparse. In the future, we will investigate the use of
reducd set methods to alleviate this.

Acknowledgments
This research has been partially supported by the Research
Grants Council of the Hong Kong Special Administrative Re-
gion.

References
[Collins and Schapire, 2002] M. Collins and R.E. Schapire. Lo-
gistic regression, AdaBoost and Bregman distances. Machine
Learning, 48(1/2/3), 2002.

[Cristianini et al., 1999] N. Cristianini, C. Campbell, and J. Shawe-
Taylor. Multiplicative updatings for support vector machines.
In Proceeding of European Symposium on Artiﬁcial Neural Net-
works, Bruges, Belgium, 1999.

[Evgeniou et al., 2004] T. Evgeniou, M. Pontil, and A. Elisseeff.
Leave one out error, stability, and generalization of voting com-
binations of classiﬁers. Machine Learning, 55(1):71–97, April
2004.

[Fedotov et al., 2001] A.A. Fedotov, P. Harremoes, and F. Topsoe.
Vajda’s tight lower bound and reﬁnements of Pinsker’s inequality.
In Proceedings of the International Symposium on Information
Theory, San Mateo, CA, 2001.

[Fine and Scheinberg, 2001] S. Fine and K. Scheinberg. Efﬁcient
SVM training using low-rank kernel representations. Journal of
Machine Learning Research, 2:243–264, December 2001.

[Hastie et al., 2005] T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu.
The entire regularization path for the support vector machine. In
Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors, Ad-
vances in Neural Information Processing Systems 17, pages 561–
568. MIT Press, Cambridge, MA, 2005.

[Kim et al., 2003] H.-C. Kim, S. Pang, H.-M. Je, D. Kim, and
S. Bang. Constructing support vector machine ensemble. Pat-
tern Recognition, 36(12):2757–2767, 2003.

[Kivinen and Warmuth, 1997] J. Kivinen and M.K. Warmuth. Ad-
ditive versus exponentiated gradient updates for linear prediction.
Information and Computation, 132(1):1–64, 1997.

[Kivinen and Warmuth, 1999] J. Kivinen and M. K. Warmuth.
Boosting as entropy projection.
In Proceedings of the Twelfth
Annual Conference on Computational Learning Theory, pages
134 – 144, Santa Cruz, CA, USA, 1999.

[Lin, 2001] C.-J. Lin. Linear convergence of a decomposition
method for support vector machines. Technical report, National
Taiwan University, 2001.

[Platt, 1999] J.C. Platt. Fast training of support vector machines us-
ing sequential minimal optimization. In B. Sch¨olkopf, C. Burges,
and A. Smola, editors, Advances in Kernel Methods – Support
Vector Learning, pages 185–208. MIT Press, Cambridge, MA,
1999.

[R¨atsch and Warmuth, 2005] G R¨atsch and M Warmuth. Efﬁcient
margin maximization with boosting. Journal of Machine Learn-
ing Research, 6:2131–2152, 2005.

[R¨atsch, 2001] G. R¨atsch. Robust Boosting via Convex Optimiza-
tion: Theory and Applications. PhD thesis, University of Pots-
dam, October 2001.

IJCAI-07

1094


A General Model for Online Probabilistic Plan Recognition 

Hung H. Bui 

Department  of Computing 

Curtin  University of Technology 

PO Box U1987, Perth, WA 6001, Australia 
URL: http://www.cs.curtin.edu.aurbuihh 

Email: buihh@cs.curtin.edu.au 

Abstract 

We  present  a  new  general  framework  for  online 
probabilistic  plan  recognition  called  the  Abstract 
Hidden  Markov  Memory  Model  (AHMEM).  The 
new model is an extension of the existing Abstract 
Hidden  Markov Model to allow  the policy to have 
internal memory which can be updated in a Markov 
fashion.  We  show  that  the  A H M EM  can  repre­
sent a richer class of probabilistic plans, and at the 
same  time  derive  an  efficient  algorithm  for  plan 
recognition  in  the  A H M EM  based  on  the  Rao-
Blackwellised Particle Filter approximate inference 
method. 

Introduction 

1 
The ability to perform plan recognition can be very useful in 
a wide range of applications such as monitoring and surveil­
lance, decision supports,  and team work.  However the plan 
recognizing agent's task is usually complicated by the uncer­
tainty  in the plan  refinement process,  in the outcomes of ac­
tions,  and  in  the  agent's  observations  of the  plan.  Dealing 
with these issues in plan recognition is a challenging task, es­
pecially when  the  recognition has to be done online  so that 
the observer can react to the actor's plan in real-time. 

The uncertainty problem has been  addressed by the  sem­
inal  work  [Charniak  and  Goldman,  19931  which  phrases 
the  plan  recognition  problem  as  the  inference  problem  in 
a  Bayesian  network  representing  the  process  of  executing 
the  actor's  plan.  More  recent  work  has  considered  dy­
namic  models  for  performing  plan  recognition  online  [Py-
nadath  and  Wellman,  1995;  2000;  Goldmand  et a/.,  1999; 
Huber et ai,  1994; Albrecht et a/.,  1998].  While this offers a 
coherent way of modelling and dealing with  various sources 
of uncertainty in the plan execution model, the computational 
complexity and scalability of inference is the main issue, es­
pecially for dynamic models. 

Inference  in  dynamic  models  such  as  the  Dynamic 
Bayesian  Networks  (DBN)  [Nicholson  and  Brady,  1994]  is 
more  difficult  than  in  a  static  model. 
Inference  in  a  static 
network  utilizes  the  sparse  structure  of the  graphical  model 
to  make  it  tractable. 
In  the  dynamic  case,  the  DBN  belief 
state  that we need to maintain usually does not  preserve the 
conditional  independence  properties  of the  single  time-slice 

network,  making  exact  inference  intractable even  when  the 
DBN  has  a sparse  structure.  Thus,  online  plan  recognition 
algorithms  based  on  exact  inference  will  run  into  problems 
when the belief state becomes too large, and will be unable to 
scale up to larger or more detailed plan hierarchies. 

In  our previous work,  we  have proposed a framework for 
online  probabilistic  plan  recognition  based  on  the  Abstract 
Hidden  Markov  Models  (AHMM)  [Bui  et  a/.,  2002].  The 
A H MM  is a stochastic  model for representing the execution 
of  a  hierarchy  of contingent  plans  (termed  policies).  Scal­
ability  in  policy  recognition  in  the  A H MM  is  achieved  by 
using an  approximate  inference scheme  known  as the  Rao-
Blackwellised  Particle  Filter  (RBPF)  [Doucet  et  al  20001. 
It  has  been  shown  that  this  algorithm  scales  well  w.r.t.  the 
number of levels in the plan hierarchy. 

Despite 

its  computational  attractiveness, 

the  current 
AHMM  is  limited  in  its expressiveness,  in  particular,  its  in­
ability  to  represent  an  uninterrupted  sequence  of plans  and 
actions. This is due to the fact that each policy in the AHMM 
is  purely  reactive  on  the  current  state  and  has  no  memory. 
This  type  of memoryless  policies  cannot  represent  an  unin­
terrupted  sequence  of sub-plans  since  they  have  no  way  of 
remembering  the  sub-plan  in  the  sequence  that  is  currently 
being  executed.  In  other words,  the  decision  to  choose  the 
next sub-plan can only be dependent on the current state, and 
not on the sub-plans that have been chosen in the past.  Other 
models for plan  recognition such  that the Probabilistic  State 
Dependent Grammar (PSDG) [Pynadath and Wellman, 2000; 
Pynadath,  1999]  are  more  expressive  and  do  not  have  this 
limitation.  Unfortunately, the existing exact inference method 
for the PSDG in [Pynadath, 1999] has been found to be flawed 
and inadequate [Bui, 2002]. 

The main motivation in this paper is to extend the existing 
AHMM framework to allow for policies with memories to be 
considered.  We  propose an  extension  of the AHMM  called 
the  Abstract  Hidden  Markov  mEmory  Model  (AHMEM). 
The expressiveness of the new model encompasses that of the 
PSDG  [Pynadath  and Wellman,  2000],  thus  the  new  model 
removes the  current restriction  of the  AHMM.  More  impor­
tantly, we show that the RBPF approximate inference method 
used  for  the  A H MM  can  be  extended  to  the  more  general 
A H M EM  as well,  ensuring that the new generalized model 
remains computationally attractive. To the best of our knowl­
edge, we are the first to provide a scalable inference method 

USER  MODELING 

1309 

for this general  type of hierarchical probabilistic plan hierar(cid:173)
chy. 

The  paper  is  structured  as  follows.  Section  2  provides 
a  more  detailed  discussion  of  the  AHMM,  PSDG  and  re(cid:173)
lated  models  for  online  probabilistic  plan  recognition.  The 
A H M EM  is  introduced  in  section  3  and  the  algorithms  for 
plan recognition are presented in section 4.  Experimental re(cid:173)
sults with a prototype system are provided in section 5.  Fi(cid:173)
nally,  we conclude and discuss directions for future work in 
section 6. 

2  Related Models for Online Probabilistic 

Plan  Recognition 

In the AHMM [Bui et al, 2000; 2002], an agent's probabilis(cid:173)
tic plan is modeled by an abstract Markov policy (AMP). An 
AMP  is  an  extension  of a  policy  in  Markov  Decision  Pro(cid:173)
cesses (MDP) defined within a subset of the environment state 
space  so  that  it can  select  other more  refined  AMPs  and  so 
on  to  form  a  hierarchy of policies.  The  AMP is  thus  simi(cid:173)
lar to a contingent plan that prescribes which sub-plan should 
be invoked at each  applicable state of the world.  The noisy 
observation about the environment state can be modelled by 
making the state "hidden", similar to the hidden state in the 
Hidden Markov Models [Rabiner,  1989].  The stochastic pro(cid:173)
cess resulting from the execution of an AMP is termed the Ab(cid:173)
stract Hidden Markov Model.  Intuitively, the AHMM models 
how  an  AMP causes  the  adoption  of other policies  and  ac(cid:173)
tions at different levels of abstraction, which in turn generate 
a  sequence of states  and  observations.  In  the plan  recogni(cid:173)
tion  task,  an  observer is given  an  A H MM corresponding to 
the actor's plan hierarchy, and is asked to infer about the cur(cid:173)
rent  policy  being  executed  by  the  actor  at  all  levels  of the 
hierarchy,  taking  into  account  the  sequence  of observations 
currently  available.  This  problem  is  termed policy  recogni(cid:173)
tion  [Bui  et al,  2002]. 

in 

the  A H MM 

Scalability  of  policy  recognition 

is 
achieved  by  using  a  hybrid  inference  method,  a  variant 
of  the  Rao-Blackwellised  particle  filter  (RBPF)  [Doucet 
et  al,  2000].  When  applied  to  DBN  inference,  Rao-
Blackwellisation  [Casella  and  Robert,  1996]  splits  the  net(cid:173)
work into two sets of variables:  the set of variables that need 
to  be  sampled  (termed  the  Rao-Blackwellising  (RB)  vari(cid:173)
ables),  and the  set of remaining variables whose  belief state 
conditioned  on  the  RB  variables  need  to  be  maintained  via 
exact  inference  (termed  the  Rao-Blackwellised  (RB)  belief 
state).  The RBPF thus allows us to combine sampling-based 
approximate  inference  with  exact  inference  to  achieve  effi(cid:173)
ciency and improve accuracy. 

The Probabilistic State-Dependent Grammar (PSDG)  [Py-
nadath, 1999; Pynadath and Wellman, 2000] can be described 
as the Probabilistic Context Free Grammar (PCFG)  [Jelinek 
et al,  1992], augmented with a state space, and a state transi(cid:173)
tion probability table for each terminal symbol of the PCFG. 
In  addition,  the  probability of each production rule  is  made 
state dependent.  As  a result,  the terminal  symbol now acts 
like  primitive  actions  and  the  non-terminal  symbol  chooses 
its  expansion  depending  on  the  current  state.  The  A H MM 
is equivalent to a special class of PSDG where only produc(cid:173)

YX  and  A" 

tion  rules  of the  form  X 
are allowed. 
The  first  rule models  the adoption of a lower level  policy  Y 
by a higher level policy A', while the second rule models the 
termination  of the  policy  X.  The  PSDG  model  considered 
by  Pynadath  and Wellman  allows  for more  general  rules  of 
the form  X 
i.e.,  the recursion symbol  must 
be located at the end of the expansion.  Thus in a PSDG, a 
policy might be  expanded into a sequence of policies  at the 
lower level  which  will  be executed one after another before 
control is returned to the higher level policy. 

Although  more  expressive  than  the  AHMM,  the  existing 
computational method for inference with  the PSDG  remains 
inadequate.  Pynadath proposed an  exact method  for updat(cid:173)
ing the belief state of the PSDG in a "compact" closed form. 
The proposed algorithm seemingly gets around the exponen(cid:173)
tial  blow up in the size of the belief state.  Unfortunately, the 
derivation of the  algorithm  is  based  on a flawed assumption 
that the higher levels in the belief state are independent of the 
lower levels  given  the current level.  For more details  about 
the flaw in the inference algorithm for PSDG, interested read(cid:173)
ers are referred to [Bui, 20021. 

The  AHMM,  PSDG,  and  the  proposed A H M EM  are  re(cid:173)
lated  to  the  Hierarchical  Abstract  Machines  (HAM)  [Parr, 
1998]  used in abstract probabilistic planning.  In this model, 
the  policy  is  represented  by  a  stochastic  finite  automaton, 
which can call other automata at the lower level. Despite their 
representational  similarity,  the computational techniques for 
A H M EM  and related models are intended for plan recogni(cid:173)
tion whereas the HAM model is used for speeding up the pro(cid:173)
cess of finding an optimal policy for MDP. 

If we ignore the state dependency, the DBN structure of the 
A H M EM  and PSDG is similar to the structure of the Hierar(cid:173)
chical  Hidden  Markov  Model  (HHMM)  [Fine  et  al,  1998; 
Murphy and Pashkin, 2001].  However, while the HHMM is 
a  type  of Probabilistic  Context  Free  Grammar  (PCFG),  the 
A H M EM and PSDG are not due to the state dependency in 
the model. 

3  The Abstract Hidden Markov Memory 

Models 

This  section  introduces  the  Abstract  Hidden  Markov  Mem(cid:173)
ory Models  (AHMEM),  an  extension  of the  A H MM  where 
the  policy  can  have  internal  memory.  Our  main  aim  is  to 
construct a general model for plan recognition whose expres(cid:173)
siveness encompasses that of the current A H MM  and  PSDG 
models,  while  retaining  the  computational  attractiveness  of 
the A H MM framework. We first define the A H M EM in sub-
section 3.1.  The DBN structure of the new model is given in 
subsection 3.2. 

3.1  The Model 
Consider an MDP-like model with  S representing the states 
of the  environment,  and  A  representing  the  set  of primitive 
actions available to the agent.  Each action a  €  A is defined 
by its transition probability from the current state s to the next 
state  s':  oa(s,s').  The  set  of  abstract  policies  will  include 
every primitive actions.  Furthermore, the A H MM  [Bui  et al, 

1310 

USER  MODELING 

2000] defines higher level abstract policies on top of a set of 
policies as follows: 

distributions, i.e.  
l,and 

1. 

When  an  AMPE 

according to  the distribution 
be executed until  it terminates  at  some state 
state  .s',  the  policy 

is  executed  from  a  state  .s,  it first 
initialises  its  memory  value  m  according  to  the  distribution 
Then a policy at the lower level  will be selected 
This  policy  will 
At  the  new 
itself  will  terminate  with  probability 
If it  does  not  terminate,  the  memory  variable  will 
be given a new value m' according to the transition probabil­
ity 
is 
selected with probability 

Then a new policy at the lower level 

and so on. 

on top of 

Using the AMPEs, we can construct a hierarchy of abstract 
policies in the same way as in the AHMM. We start with a set 
=  A,  and for k  =  1 , . . ., K build a set 
of primitive  actions 
of new AMPEs 
Then, if a top-level policy 
is executed, it invokes a sequence of level-(K-l) policies, 
each of which invokes a sequence of leveI-(K-2) policies and 
so on.  A  level-1  policy  will  invoke a  sequence of primitive 
actions which leads to a sequence of states.  We can then in­
troduce the hidden states and model the noisy observation of 
the state by an observation model 
The 
dynamic process of executing a top-level AMPE is termed the 
Abstract Hidden Markov  mEmory Model  (AHMEM). 

Some  special  cases  of the  A H M EM  are  worth  mention­
ing here.  First, the AHMM itself is a special AHMEM. The 
memoryless  policies  of  the  AHMM  are  equivalent  to  AM­
PEs where the dependency on the memory variable is ignored 
(e.g.  when  M  is  a  singleton  set).  The  class  of PSDG  con­
sidered  in  [Pynadath,  1999]  can  also be easily  converted to 
an A H M E M. The terminal symbols in the PSDG are equiv­
alent to the  primitive actions.  Each  non-terminal  symbol  is 
equivalent to a memoryless policy. In addition, each sequence 
encountered  on  the  RHS  of a  production 
is equivalent to a 
policy  whose  memory  m  taking  on  the  values  1 , . .. ,n.  Y 
then simply selects the (memoryless) policy  

Note that in the A H M EM definition, we assume a balanced 
policy  hierarchy  for  the  ease  of presentation  (all  the  actions 
must appear at the same bottom level). However, we can also 
specify an unbalanced hierarchy by introducing some dummy 
policies which arc equivalent to primitive actions at the higher 
levels in the hierarchy. 

3.2  DBN Representation  of the A H M EM 

An AMP as defined above is purely reactive,  in the sense 
that it selects the next policy at the lower level based only on 
the  current  state  s.  This  restricts  the  set  of behaviours  that 
an  AMP can  represent.  For example,  it  will  not  be  able  to 
represent a plan consisting of a few sub-plans, one followed 
by another regardless of the state sequence.  To represent this 
kind  of plans,  the  agent  needs  to  have  some  form  of inter­
nal  memory to remember the current stage of execution.  Let 
M  be  a set of possible  internal  memory  states.  We  first  ex­
tend the definition of our policy to include a memory variable 
which takes on values in M and is updated after each stage of 
execution of the policy.1 
Definition  2  (Abstract  Markov  Policy  with  Memory 
(AMPE)).  Let  II  be a set of AMPEs, an AMPE  * over II is 
defined as a tuple 

where: 

is the set of applicable states. 
is the set of destination states. 

(0,1]  is  the  terminating  proba-

is  the  probability  that  the  policy 

would stop if the current state is d and the current mem­
ory value is 777. 

ability,  a  (.s,  m,)  is the probability that 
policy  at the state s and memory value  

is the policy selection prob­
selects the 

memory values. 
tial memory is m if  * commences at state s. 

[0,1]  is  the  initial  distribution  of 
is the probability that the ini­

is  the  memory transi­
tion probability. 
is the probability that the 
next memory value is  m'  given that the current memory 
value is m and the current state is .s. 

Subsequently, we  will  drop the  subscript 

if there  is  no 
confusion  about the  policy  in  the  context.  Note  that  all  the 
states in D \ S are called terminal states and thus  

Also, the policy selection, memory 
initial and transition probability have to be proper probability 

'One can argue that wc can always incorporate the memory vari­
able in the environment state, and hence we gain no extra represen­
tational power just by introducing the memory variables.  However, 
incorporating the memory variables in the state variable would blow 
up the size of the state space and thus defeat our purpose of keeping 
the model computationally feasible. 

USER MODELING 

1311 

at  the  start:  either through  the  starting  state  or the  starting 
time.  Thus,  the conditional  independence theorem for poli­
cies  in  the  A H MM  still  holds  in  this  more  general  setting. 
We state the theorem for the A H M EM below.  The proof for 
the AHMM [Bui et ai, 2002] can be directly extended to this 
general case using the context specific  independence proper­
ties described in the previous subsection. 

Let 

Note that  k n o w i n g is  equivalent to 
knowing precisely when each of the policies starts and ends. 
Therefore, given 
the  starting  time  and  state of every 
current policy are known.  The following corollary is thus a 
direct consequence of theorem  1. 
Corollary  1.  Let C t  represent  the  conditional joint distribu(cid:173)
tion 
Then  Ct  has  the following 
Bayesian  network factorization: 

Ct  thus  also  has  the  following  undirected  network  repre­

sentation.  We  first  form  the  set  of cliques:  

Note  that  the  set  of 
cliques CO-K form a chain of cliques in this order, therefore we 
term Ct  the policy-clique chain.  This  extends the  concept of 
the policy chain  in  the  memoryless  case  of the  AHMM  [Bui 
et al,  2000].  Ct  can  be  factored  into  the  product  of poten-

network factorization, the potentials are said to be in canoni(cid:173)
cal form.  Any potential representation of the clique chain can 
be canonicalized by first perform message passing (exact in­
ference) to compute the marginal at each clique. The canoni­
cal form can then be computed directly from these marginals. 
Later on  we  will  use the  undirected representation of Ct  for 
exact inference, and the canonical form (directed representa­
tion)  of Ct  for  obtaining  samples  from  the joint  distribution 
using simple forward sampling. 
4  Approximate Inference for A H M EM 
In  this  section,  we  look  at  the  online  inference  prob­
lem  in  the  A H M E M.  Assume  that  at  time  f,  we  have 
a  sequence  of  observations  about  the  environment  state 
We  need  to  compute  the  be­
lief  state  of  the  DBN  which  is  the  joint  distribution  of 
all  the  current  variables  given  this  observation  sequence: 
,  From this, we can answer 
various queries about the current status of the plan execution. 

= 

Policy Termination and Selection 
The policy termination and selection model for the A H M EM 
is essentially the same as in the AHMM, except for the depen­
dency on  the value of the  current memory variable 
We 
note that the same context specific independence [Boutilier et 
a/.,  1996] properties of the A H MM still hold.  For example, if 
becomes  independent of the 
remaining parents of this variable.  Now consider the variable 
is independent of the 

=  F  then 

=  F then 

=  F  and 

and 

remaining parents;  otherwise,   

and 

is independent of  

Memory  Update 
There are two parents of this node 
Consider the variable 
=  F, 
that act  like context variables: 
the policy at the lower level has not terminated and memory 
be­
is not updated at this time.  Thus, 
=  T 
comes independent of all the remaining parents.  If 

and 

and 

If 

Independence Properties in the A H M EM 

3.3 
Even  though AMPEs  are  more expressive than  memoryless 
policies,  they  remain  "autonomous",  in  the  sense  that  the 
higher layers  have  no  influence  over the  state  of an  AMPE 
during its execution. The only way the higher layers can influ­
ence the current state of an AMPE is through the conditions 

1312 

USER  MODELING 

For  example,  the  marginal  probability  of 
tells  us  about 
the current policy the actor is executing at some level k;  the 
probability P r
ecution of a policy 

ls us  about the current stage of ex­
the probability  Pr 

tells us if 

t e l

will end after the current time, etc. 
Since  there  is  no  compact  closed  form  representation 
for  the  above  belief  state,  exact  inference  in  the  structure 
of  the  A H M EM  is  intractable  when  K  is  large.  How­
ever,  theorem  1  suggests  that  we  can  apply  the  Rao-
Blackwellised Particle Filter (RBPF) to this problem in a sim­
ilar  way  as  in  the  A H MM  [Bui  et  al.,  2002],  i.e.  by  us­
ing  rt  as  the  Rao-Blackwellising  (RB)  variables.  The Rao-
Blackwellised  (RB)  belief state  is  then  similar  to  the  origi­
nal  belief state  of the  A H M E M,  except  that  now 
are 
known: 
Note  that 
the  RB  belief  state 
can  be  obtained  directly  from  the 
policy-clique  chain  Ct  by  adding  in  the  network  represent­
ing  the  conditional  distribution  of  ot  and 
,  — 
Pr 

Pr(oi|st)Ct(seeFig.2). 

=  Pr 

Two main  steps  in  the RBPF procedure are:  (1) updating 
the  RB  belief state  using exact  inference,  and  (2)  sampling 
the RB  variable rt  from the current RB belief state. 
4.1  Updating the RB Belief State 
Fig.  2  shows  the  modified  2-time-slice  DBN  when  the  RB 
variables are  known 
We  note  that  all  the 
nodes  from  above  level  /  remain  unchanged,  while  all  the 
links  across  time  slices  from  level  /  and  below  can  be  re­
moved.  This greatly simplifies the network structure,  allow­
ing the updating operations to be performed efficiently. 

Since the Bt  can be obtained directly from Ct, all we have 
from Ct.  The procedure for updating 
to do is to compute 
as usual has two stages:  (1) absorbing the new evidence, i.e. 
from Ct,  we  need to compute 
and  (2)  projecting to  the new  time  slice,  i.e.  from 
need to compute Ct+\. 

—  Pr   

we 

In principle, we apply a simplified version of the junction-
tree algorithm Uensen,  1996] on the undirected network rep­
resentation of Ct  to perform the update step.  This is  in  fact 
a generalization of the arc-reversal procedure which operates 
on directed network representation of the policy chain  in the 
AHMM  [Bui  et al,  2002].  The  algorithm  for  updating  the 
RB belief state is given in Fig. 3. The absorbing step involves 
simply  incorporating the evidence likelihood into the poten­
tials  of Ct  to  obtain  the  potentials  for  Ct+.  The  projecting 
step involves first adding time-slice t +  1 to Ct+ (see Fig. 2), 
then marginalizing the now redundant variables 
in the old time slice. The marginalization is done by perform­
ing message passing  between  the cliques of the  2-time-slice 
network shown in Fig. 2. 

Since the potentials of Ct  from level / + 2 to level  K'  stay 
unchanged, the complexity of the algorithm is 0(1) where / is 
the highest level of termination.  Furthermore, if one of these 
potentials is in canonical form,  it remains in canonical form 
after the updating procedure. 
4.2  RBPF  for  A H M EM 
The  full  RBPF  algorithm  for  policy  recognition  in  the 
A H M EM  is provided in Fig 4.  The general structure is the 

Figure 2:  Two time slices of the RB belief state 

same as  the RBPF procedure for A H MM  [Bui  et al,  2002]. 
At each time step t, the algorithm maintains a set of N  sam­
ples, each consists of a value for st~i  and / / _ i, and a para­
metric representation of  Ct  The differences are in the details 
of how to obtain new samples and update the RB belief state. 
To obtain each new sample  (st,lt), we first need to canoni-
calize the potentials of Ct.  However, assuming that Ct-\  is in 
canonical  form,  we only have to canonicalize the potentials 
of Ct  between  level  0 and level I  +  1  with complexity O(1). 
Thus  the  complexity  of the  sampling  step  is  O(Nl).  Since 
the complexity of the updating step  is also O(Nl), the overall 
complexity of the algorithm at each time  step  is  O(Nl).  Fur­
thermore, the distribution for I  usually decays exponentially, 
thus  the  average complexity  is  only  O(N). 

If at  some  time  t,  an  estimation  e.g.  Pr 

required,  we  need  to  compute  h  = 
for each  sam­
ple.  This involves performing message passing for the entire 
chain  Ct.  Thus  the  complexity  for this  time  step  is  0(NK). 
Other types of queries are also possible, as long as the proba­
bility required can be computed from the RB belief state. For 
example,  we  can  ask  the  question:  if the  actor  is  currently 
executing 
what  is  the  current  stage  of execution  of this 
policy?  To answer this query, we need to compute the con­
ditional  probability 
This can easily  be 
achieved by  replacing  the  ft  function  in  the  algorithm  with 

5  Experimental Results 
We have implemented the above algorithm in a surveillance 
domain to demonstrate its working in a practical application. 
The environment consists of a spatial area which has two sep­
arate rooms and a corridor monitored by a set of 5  cameras 
(Fig.  5).  The monitored area is divided into a grid of cells, 
and the cell coordinates constitute the overall state space 5. 
The coordinates returned by the cameras are modelled as the 
noisy observations of the true coordinates of the tracked per­
son, and over time, provide the sequence of observations o\ ;t. 

USER MODELING 

1313 

Figure 3: Updating Ct 

Figure 4:  RBPF for AHMEM 

At the top-level, each policy models a type of activity per­
formed  by  the  person.  For  example,  print  is  a  policy  that 
involves a sequence of going to the computer and the printer, 
possibly going to the paper store if the printer is out of paper, 
and exiting. Note that this policy cannot be represented in the 
AHMM framework. In the AHMEM, this can be represented 
by a policy whose memory transition model is given in Fig. 6. 
The policies at the lower level, such as going to a computer, 
model the person's trajectories toward a set of special land­
marks in the environment.  These policies are constructed as 
memoryless policies in the same way as in the AHMM [Bui 
etal,  2002]. 

The  result of querying  the top-level  policy  for  the  trajec­
tory in Fig. 5 is given in Fig. 7.  The system correctly identi­
fies the most probable policy as print.  The result of querying 
the memory variable of this policy is given in Fig. 8. The sys­
tem correctly identifies  the  sequence of lower-level  policies 
invoked  by  the print policy.  These results  are  obtained  by 
running the RBPF algorithm with  3000 samples.  The aver­
age processing time for each observation is approximately 0.9 
seconds on a  1.7GHz desktop machine.  For a more detailed 
description  of this  surveillance  system,  readers  are  referred 
to [Nguyen et al., 2003]. 

6  Conclusion 
In  conclusion,  we  have  presented  the  Abstract  Hidden 
Markov  Memory  Models  (AHMEM),  a  general  framework 
for representing  and  recognizing probabilistic  plans  online. 
The new framework extends the AHMM by allowing the poli­
cies to have internal memories which are updated in a Markov 
fashion.  This allows the AHMEM  to represent a richer set 
of hierarchical probabilistic plans, including those belonging 
to  the  class  of PSDG  previously  considered.  Furthermore, 

Figure 5: The environment and a person's trajectory 

Figure 6:  Memory transition of the policy "print" 

1314 

USER MODELING 

1/2002,  School  of  Computing  Science,  Curtin  University  of 
Technology,  Perth,  W A,  Australia,  2002. 
from 
http://www.cs.curtin.edu.aurbuihh/. 

Available 

[Casella  and  Robert,  1996]  George Casella and Christian P.  Robert. 
Rao-Blackwcllisation  of  sampling  schemes.  Hiometrika,  panes 
81-94,  1996. 

[Charniak  and Goldman,  1993]  E.  Charniak  and  R.  P.  Goldman. 
Artificial  Intelligence, 

A  Bayesian  model  of  plan  recognition. 
64:53-79,  1993. 

[Doucet  et  al,  2000]  Arnaud  Doucct,  Nando  dc  Freitas,  Kevin 
Murphy,  and  Stuart  Russell.  Rao-Blackwellised  particle  filter(cid:173)
In  Proceedings  of the  Six(cid:173)
ing  for  dynamic  Bayesian  networks. 
teenth  Annual  Conference  on  Uncertainty 
Intelli(cid:173)
gence,  2000. 

in  Artificial 

[ Fine et  al,  1998]  Shai  Fine,  Yoram  Singer,  and  Naftali  Tishby. 
The  hierarchical  Hidden  Markov  Model:  Analysis  and  applica(cid:173)
tions.  Machine  Learning,  32,  1998. 

Time (seconds) 

Figure 7:  Querying the top level policies 

iGoldmand  et  ai,  1999]  Robert  Goldmand,  Chistopher  Geib,  and 
In  Proceedings 
in  Artificial 

Chirs  Miller.  A  new  model  of  plan  recognition. 
of  the  Fifteenth  Annual  Conference  on  Uncertainty 
Intelligence, 

1999. 

Figure 8:  The progress of executing the policy "print' 

Time (seconds) 

we  have  shown  that  the  Rao-Blackwellised  Particle  Filter 
(RBPF)  approximate  inference method  used  for the  AHMM 
can  be  extended  to  the  A H M E M,  resulting  in  efficient and 
scalable  procedures for plan  recognition  in  this  general  set(cid:173)
ting.  Our work demonstrates the advantage of phrasing prob(cid:173)
abilistic plan recognition as inference in DBN so that suitable 
approximate methods can be employed to cope with the com(cid:173)
plexity issue.  A similar approach can be applied to more gen(cid:173)
eral  models  to  consider more  complex  plan  constructs  such 
as multi-agent plans, interleaving plans etc. 

References 
[Albrccht et  al,  1998]  David  W.  Albrecht,  Ingrid  Zukcrman,  and 
Ann  E.  Nicholson.  Bayesian  models  for  keyhole  plan  recogni(cid:173)
tion  in  an  adventure  game.  User  Modelling  and  User-adapted 
Interaction,  8 ( l - 2 ) : 5 - 4 7,  1998. 

[Boutilier et  al,  1996]  Craig  Boutilier,  Nir  Friedman,  Moiscs 
Goldszmidt,  and  Daphne  Koller.  Context-specific  independence 
In  Proceedings  of  the  Twelveth  Annual 
in  Bayesian  networks. 
Conference  on  Uncertainty 

in  Artificial  Intelligence, 

1996. 

iBui  et ai,  2000]  Hung  H.  B u i,  Svetha  Venkatcsh,  and  Geoff West. 
On  the  recognition  of abstract  Markov  policies.  In  Proceedings  of 
the  National  Conference  on  Artificial 
(AAAI-2000), 
2000. 

Intelligence 

[Bui  et  al,  2002]  Hung  H.  B u i,  Svetha Venkatesh,  and  Geoff West. 
Policy  recognition  in  the  Abstract  Hidden  Markov  Models.  Jour(cid:173)
nal  of Artificial  Intelligence  Research, 

17:451-499,  2002. 

[Bui,  2002]  Hung  H.  Bui. 

Efficient  approximate 

for  online  probabilistic  plan  recognition. 

inference 
Technical  Report 

[Huber  et  al,  1994]  Marcus  J.  Huber,  Edmund  H.  Durfee,  and 
Michael  P.  Wellman.  The  automated  mapping  of  plans  for  plan 
In  Proceedings  of  the  Tenth  Annual  Conference  on 
recognition. 
Uncertainly 

in  Artificial 

Intelligence, 

1994. 

[Jelinek  et  al.,  1992]  F.  Jelinek,  J.  D.  Lafferty,  and  R.  L.  Mcrcci. 
Basic  methods  of probabilistic  context  free  grammar.  In  P.  Lafaee 
and  R.  Dc  M o r i,  editors,  Recent  Advances  in  Speech  Recognition 
and  Understanding,  pages  345-360.  Springer-Vcrlag,  1992. 

[Jensen,  1996]  F.  Jensen.  An  Introduction 

to  Bayesian  Networks. 

Springer,  1996. 

[Murphy  and  Pashkin,  20011  Kevin  Murphy  and  Mark  Pashkin. 
Linear  time  inference  in  hierarchical  H M M s.  In  NIPS-0I,  2001. 
[Nguyen  et  al,  2003]  Nam  T.  Nguyen,  Hung  H.  Bui,  Svetha 
Venkatesh,  and  Geoff  West.  Recognising  and  monitoring  high-
In  Proceed(cid:173)
level  behaviours  in  complex  spatial  environments. 
ings  of  the 
and  Pattern  Recognition 

International  Conference  on  Computer  Vision 

(CVPR-03),  2003. 

IEEL 

[Nicholson  and  Brady,  1994]  A.  E.  Nicholson  and J.  M.  Brady.  Dy(cid:173)
IEEE  Trans(cid:173)
namic  belief  networks  for  discrete  monitoring. 
actions  on  Systems,  Man  and  Cybernetics,  24(11):  1593-1610, 
1994. 

[Parr,  1998]  Ronald  Parr. 

learning  for 
Markov  Decision  Processes.  PhD  thesis,  University  of  Califor(cid:173)
nia,  Berkeley,  1998. 

Hierarchical  control  and 

[Pynadath  and Wellman,  19951  David  V.  Pynadath  and  Michael  P. 
Wellman.  Accounting  for context  in  plan  recognition,  with  appli 
In  Proceedings  of the  Eleventh  An(cid:173)
cation  to  traffic  monitoring. 
nual  Conference  on  Uncertainty 
1995. 
[Pynadath  and Wellman,  2000]  David  V.  Pynadath  and  Michael  P. 
Wellman.  Probabilistic  state-dependent  grammars  for plan recog(cid:173)
In  Proceedings  of  the  Sixteenth  Annual  Conference  on 
nition. 
Uncertainty 

in  Artificial  Intelligence, 

in  Artificial 

Intelligence, 

2000. 

[Pynadath,  1999]  David  V.  Pynadath. 

Probablistic  grammars  for 
plan  recognition.  PhD  thesis,  Computer  Science  and  Engineer(cid:173)
ing,  University  of Michigan,  1999. 

[Rabiner,  1989]  Lawrence  R.  Rabiner. 

tutorial  on  Hidden 
Markov  Models  and  selected  applications  in  speech  recognition. 
Proceedings  of the  IEEE,  77(2):257-286,  1989. 

A 

USER MODELING 

1315 


Learning to Identify Unexpected Instances in the Test Set 

Xiao-Li Li

Bing Liu

Institute for Infocomm

Department of Computer

See-Kiong Ng
Institute for Infocomm

Research,

Science, University of Illinois at Chicago, 851

Research,

South Morgan Street,

Chicago, IL 60607-7053

liub@cs.uic.edu

21 Heng Mui Keng Terrace,

Singapore, 119613

skng@i2r.a-star.edu.sg

21 Heng Mui Keng Terrace,

Singapore, 119613

xlli@i2r.a-star.edu.sg

Abstract

Traditional classification involves building a classi-
fier using labeled training examples from a set of
predefined classes and then applying the classifier to
classify test instances into the same set of classes. In
practice, this paradigm can be problematic because
the test data may contain instances that do not be-
long to any of the previously defined classes. De-
tecting such unexpected instances in the test set is an
important issue in practice. The problem can be
formulated as learning from positive and unlabeled
examples (PU learning). However, current PU
learning algorithms require a large proportion of
negative instances in the unlabeled set to be effec-
tive. This paper proposes a novel technique to solve
this problem in the text classification domain. The
technique first generates a single artificial negative
document AN. The sets P and {AN} are then used to
build a naïve Bayesian classifier. Our experiment
results show that this method is significantly better
than existing techniques.

1 Introduction
Classification is a well-studied problem in machine learn-
ing. Traditionally, to build a classifier, a user first collects a
set of training examples that are labeled with predefined or
known classes. A classification algorithm is then applied to
the training data to build a classifier that is subsequently
employed to assign the predefined classes to instances in a
test set (for evaluation) or future instances (in practice).

This paradigm can be problematic in practice because
some of the test or future instances may not belong to any of
the predefined classes of the original training set. The test set
may contain additional unknown subclasses, or new sub-
classes may arise as the underlying domain evolves over time.
For example, in cancer classification, the training set consists
of data from currently known cancer subtypes. However,
since cancer is a complex and heterogeneous disease, and still
a perplexing one to-date, it is likely that the test data contain
cancer subtypes that are not yet medically classified (they are
therefore not covered in the training data). Even if the training
data do contain all the current cancer subtypes, new subtypes

may be formed at a later stage as the disease evolves due to
mutations or other cancer-causing agents. This phenomenon
is not uncommon even in the seemingly simpler application
domains. For example, in document classification, topics are
often heterogeneous and new topics evolve over time. A
document classifier built for classifying say, computer sci-
ence papers, would face the similar problems as the cancer
classifier described above. This is because computer science
is a heterogeneous and increasingly cross-disciplinary do-
main; it is also a rapidly evolving one with new topics being
created over time.

Thus, a classifier created based on the notion of a fixed set
of predefined classes is bound to be inadequate in the com-
plex and dynamic real-world in the long run, requiring the
user to manually go through the classification results to re-
move the unexpected instances. In practice, a competent clas-
sifier should learn to identify unexpected instances in the test
set so as to automatically set these unclassifiable instances
apart. In some applications, this can be important in itself. For
example, in the cancer example above, detection of the unex-
pected instances can alert the scientists that some new medi-
cal discovery (a new cancer subtype) may have occurred.

In recent years, researchers have studied the problem of
learning from positive and unlabeled examples (or PU
learning). Given a positive set P and an unlabelled set U, a
PU learning algorithm learns a classifier that can identify
hidden positive documents in the unlabeled set U. Our
problem of identifying unexpected instances in the test set
can be modeled as a PU learning problem by treating all the
training data as the positive set P and the test set as the un-
labeled set U. A classifier can then be learned using PU
learning algorithms to classify the test set to identify those
unexpected (or negative) instances before applying a tradi-
tional classifier to classify the remaining instances into the
original predefined classes.

However, as the current PU techniques operate by trying
to identify an adequate set of reliable negative data from the
unlabeled set U to learn from, they require a large propor-
tion of unexpected instances in the unlabeled set U to be
effective.
In practice, the number of unexpected instances
in the test data can be very small since they are most likely
to be arising from an emerging class. This means that the
classifiers built with existing PU learning techniques will

IJCAI-07

2802

perform poorly due to the small number of unexpected
(negative) instances in U.

In this paper, we propose a novel technique called LGN
(PU Learning by Generating Negative examples), and we
study the problem using text classification. LGN uses an en-
tropy-based method to generate a single artificial negative
document AN based on the information in P and U, in which
the features’ frequency distributions correspond to the de-
grees of “negativeness” in terms of their respective entropy
values. A more accurate classifier (we use the naïve Bayesian
method) can be built to identify unexpected instances with the
help of the artificial negative document AN. Experimental
results on the benchmark 20 Newsgroup data showed that
LGN outperforms existing methods dramatically.

2 Related Work
PU learning was investigated by several researchers in recent
years. A study of PAC learning from positive and unlabeled
examples under the statistical query model was given in
[Denis, 1998]. [Liu et al., 2002] reported sample complexity
results and showed how the problem may be solved.

Subsequently, a number of practical algorithms [Liu et al.,
2002; Yu et al., 2002; Li and Liu, 2003] were proposed. They
all conformed to the theoretical results in [Liu et al., 2002]
following a two-step strategy: (1) identifying a set of reliable
negative documents from the unlabeled set; and (2) building a
classifier using EM or SVM iteratively. Their specific differ-
ences in the two steps are as follows. S-EM proposed in [Liu
et al., 2002] is based on naïve Bayesian classification and the
EM algorithm [Dempster, 1977]. The main idea was to first
use a spying technique to identify some reliable negative
documents from the unlabeled set, and then to run EM to
build the final classifier. PEBL [Yu et al., 2002] uses a dif-
ferent method (1-DNF) to identify reliable negative examples
and then runs SVM iteratively to build a classifier.

More recently, [Li and Liu, 2003] reported a technique
called Roc-SVM. In this technique, reliable negative docu-
ments are extracted by using the information retrieval tech-
nique Rocchio [Rocchio, 1971], and SVM is used in the
second step.
In [Fung et al., 2005], a method called
PN-SVM is proposed to deal with the situation when the
positive set is small. All these existing methods require that
the unlabeled set have a large number of hidden negative
instances. In this paper, we deal with the opposite problem,
i.e. the number of hidden negative instances is very small.

Another line of related work is learning from only positive
data. In [Scholkopf, 1999], a one-class SVM is proposed. It
was also studied in [Manevitz and Yousef, 2002] and
[Crammer, 2004]. One-class SVM builds a classifier by
treating the training data as the positive set P. Those instances
in test set that are classified as negative by the classifier can
be regarded as unexpected instances. However, our experi-
ments show that its results are poorer than PU learning, which
indicates that unlabeled data helps classification.

3 The Proposed Algorithm
Given a training set {ci} (i = 1, 2, …, n) of multiple classes,
our target is to automatically identify those unexpected in-

stances in test set T that do not belong to any of the training
classes ci. In the next subsection (Section 3.1), we describe a
baseline algorithm that directly applies PU learning tech-
niques to identify unexpected instances. Then, in Section
3.2, we present our proposed LGN algorithm.

3.1 Baseline Algorithms: PU Learning
To recapitulate, our problem of identifying unexpected in-
stances in the test set can be formulated as a PU learning
problem as follows. The training instances of all classes are
first combined to form the positive set P. The test set T then
forms the unlabeled set U, which contains both positive in-
stances (i.e., those belonging to training classes ci) and nega-
tive/unexpected instances
in T (i.e., those not belonging to
any training class ci). Then, PU learning techniques can be
employed to build a classifier to classify the unlabeled set U
(test set T) to identify negative instances in U (the unexpected
instances). Figure 1 gives the detailed framework for gener-
ating baseline algorithms based on PU learning techniques.

P = training examples from all classes (treated as positive);

1. UE = ;
2.
3. U = T (test set, ignore the class labels in T if present);
4. Run an existing PU learning algorithm with P and U to build

a classifier Q;
For each instance di

∈ U (which is the same as T)

Use a classifier Q to classify di
If di is classified as negative then

5.
6.
7.
8.
9.
Figure 1. Directly applying existing PU learning techniques

UE = UE ∪ {di};

output UE

In the baseline algorithm, we use a set UE to store the
negative (unexpected) instances identified. Step 1 initializes
UE to the empty set, while Steps 2-3 initialize the positive set
P and unlabeled set U as described above. In Step 4, we run
an existing PU learning algorithm (various PU learning tech-
niques can be applied to build different classifiers) to con-
struct a classifier Q. We then employ the classifier Q to clas-
sify the test instances in U in Steps 5 to 8. Those instances
that are classified by Q as negative class are added to UE as
unexpected instances. After we have iterated through all the
test instances, Step 9 outputs the unexpected set UE.

3.2 The Proposed Technique: LGN
In traditional classification, the training and test instances are
drawn independently according to some fixed distribution D
over X × Y, where X denotes the set of possible documents in
our text classification application, and Y = {c1, c2, ..., cn} de-
notes the known classes. Theoretically, for each class ci, if its
training and test instances follow the same distribution, a
classifier learned from the training instances can be used to
classify the test instances into the n known classes.

In our problem, the training set Tr with instances from
classes c1, c2, ..., cn are still drawn from the distribution D.
However, the test set T consists of two subsets, T.P (called
positive instances in T) and T.N (called unexpected / negative
instances in T). The instances in T.P are independently drawn

IJCAI-07

2803

from D, but the instances in T.N are drawn from an unknown
and different distribution Du. Our objective is to identify all
the instances drawn from this unknown distribution Du, or in
other words to identify all the hidden instances in T.N.

Let us now formally reformulate this problem as a
two-class classification problem without labeled negative
training examples. We first rename the training set Tr as the
positive set P by changing every class label ci ∈ Y to “+”
(the positive class). We then rename the test set T as the
unlabeled set U, which comprises both hidden positive in-
stances and hidden unexpected instances. The unexpected
instances in U (or T) are now called negative instances with
the class label “−” (bear in mind that there are many hidden
positive instances in U). A learning algorithm will select a
function f from a class of functions F: X → {+, −} to be
used as a classifier that can identify the unexpected (nega-
tive) instances from U. The problem here is that there are no
labeled negative examples for learning. Thus, it becomes a
problem of learning from positive and unlabeled examples
(PU learning). As discussed in the previous section, this
problem has been studied by researchers in recent years, but
existing PU techniques performed poorly when the number
of negative (unexpected) instances in U is very small. To
address this, we will propose a technique to generate artifi-
cial negative documents based on the given data.

Let us analyze the problem from a probabilistic point of
view. In our text classification problem, documents are com-
monly represented by frequencies of words w1, w2, ..., w|v| that
appear in the document collection, where V is called the vo-
cabulary. Let w+ represent a positive word feature that char-
acterizes the instances in P and let w- represent a negative
feature that characterizes negative (unexpected) instances in
U. If U contains a large proportion of positive instances, then
the feature w+ will have similar distribution in both P and U.
However, for the negative feature w- , its probability distribu-
tions in the set P and U are very different. Our strategy is to
exploit this difference to generate an effective set of artificial
negative documents N so that it can be used together with the
positive set P for a classifier training to identify negative
(unexpected) documents in U accurately.

Given that we use the naïve Bayesian framework in this
work, before going further, we now introduce naïve Baye-
sian classifier for text classification.

NAÏVE BAYESIAN CLASSIFICATION
Naïve Bayesian (NB) classification has been shown to be an
effective technique for text classification [Lewis, 1994;
McCallum and Nigam, 1998]. Given a set of training docu-
ments D, each document is considered an ordered list of
words. We use wdi,k to denote the word in position k of
document di, where each word is from the vocabulary V =
{w1, w2, ..., w|V |}. The vocabulary is the set of all words we
consider for classification. We also have a set of predefined
classes, C = {c1, c2, ..., c|C|}. In order to perform classifica-
tion, we need to compute the posterior probability, Pr(cj|di),
where cj is a class and di is a document. Based on the Baye-
sian probability and the multinomial model, we have

∑ =

|

|
D

i

1

Ρ

(r

c

)

j

=

Ρ

(r

c

j

|

d

i

)

|

D

|

(1)

and with Laplacian smoothing,

Ρ

(r

cw

|

t

=

)

j

|

V

|

|

|

∑
+
1
∑ ∑
+

D
=
1

|
|
V
=
1

s

i

|

|

D
=
1

i

dwN

(

,

t

i

Ρ
(r)

d

)

i

(2)

j

|

c
Ρ
(r)

dwN

(

,

s

i

c

j

|

d

i

)

where N(wt,di) is the count of the number of times that the
word wt occurs in document di and Pr(cj|di)∈ {0,1} depend-

ing on the class label of the document.

Finally, assuming that the probabilities of the words are

independent given the class, we obtain the NB classifier:

Ρ

(r

c

j

|

d

i

)

=

Ρ
∑

|
|
C
=
1

r

(r
c
Ρ

∏

)

j

(r

c

r

)

i

|

k

|
d
=
1

Ρ
∏

|

i

|
d
=
1

k

(r
w
Ρ

|

c

)

j

kd

,

i

(r

w

kd

,

i

|

c

r

)

(3)

In the naive Bayesian classifier, the class with the highest

Pr(cj|di) is assigned as the class of the document.

GENERATING NEGATIVE DATA
In this subsection, we present our algorithm to generate the
negative data. Given that in a naïve Bayesian framework, the
conditional probabilities Pr(wt|-) (Equation (2)) are computed
based on the accumulative frequencies of all the documents in
the negative class, a single artificial negative instance AN
would work equally well for Bayesian learning. In other
words, we need to generate the negative document AN in such
a way to ensure Pr(w+|+) − Pr(w+|-) > 0 for a positive feature
w+ and Pr(w-|+) − Pr(w-|-) < 0 for a negative feature w-. We
use an entropy-based method to estimate if a feature wi in U
has significantly different conditional probabilities in P and in
U (i.e, (Pr(wi|+) and Pr(wi|-))). The entropy equation is:

entropy

(

w
i

)

−=

∑

−+∈
},{
c

Pr(

cw

|

i

*)

log(Pr(

cw

|

i

))

(4)

The entropy values show the relative discriminatory
power of the word features: the bigger a feature’s entropy is,
the more likely it has similar distributions in both P and U
(i.e. less discriminatory). This means that for a negative
feature w-, its entropy entropy(w-) is small as Pr(w-|-) (w-
mainly occurring in U) is significantly larger than Pr(w-|+),
while entropy(w+) is large as Pr(w+|+) and Pr(w+|-) are simi-
lar. The entropy (and its conditional probabilities) can
therefore indicate whether a feature belongs to the positive
or the negative class. We generate features for AN based on
the entropy information, weighted as follows:

−=
1)

wq
i

(

entropy

(

w
i

)

max

=
...,2,1
|
V

j

|

(

entropy

(

w

))

j

(5)

If q(wi)= 0, it means that wi uniformly occurs in both P
and U and we therefore do not generate wi in AN. If q(wi) = 1,
we can be almost certain that wi is a negative feature and we
generate it for AN, based on its distribution in U. In this way,
those features that are deemed more discriminatory will be
generated more frequently in AN. For those features with
q(wi) between the two extremes, their frequencies in AN are
generated proportionally.

We generate the artificial negative document AN as fol-
lows. Given the positive set P and the unlabeled set U, we
compute each word feature’s entropy value. The feature’s
frequency in the negative document AN is then randomly
generated following a Gaussian distribution according to
q(wi) = 1-entropy(wi)/max(entropy(wj), wj∈V). The detailed
algorithm is shown in Figure 2.

IJCAI-07

2804

1. AN =;
2. P = training documents from all classes (treated as positive);
3. U = T (test set, ignore the class labels in T if present);
4. For each feature wi
5.

Compute the frequency of wi in each document dk freq(wi,
dk), dk ∈ U;
Let mean

freq∑

is the set of

∈ U

dw

)

(

,

k

i

where Dwi

∈=μ

Dd

k

w
i

iw

|

D

w
i

|

,

documents in containing wi
Let variance

1

2

σ

w

=

∑

∈

Dd

k

iw

(

freq

(

dw

,

i

−

μ

w

i

2

)

;

)

k

−
)1|

D
w

i

i

(|
8. For each feature wi ∈ V
9.

Compute Pr(wi|+), Pr(wi |-) using Equation (2) assuming
that all the documents in U are negative;
Let

;

Pr(

cw

*)|

log(Pr(

cw

|

))

entropy

(

−=

∑

w
i

)

i

i

−+∈
},{
c

6.

7.

10.

14.
15.

that summarizes the unlabelled data set, but with the fea-
tures indicative of positive class dramatically reduced.

BUILDING THE FINAL NB CLASSIFIER
Finally, we describe how to build an NB classifier with the
positive set P and the generated single negative document
AN to identify unexpected document instances. The detailed
algorithm is shown in Figure 3.

Equations (1) and (2);

1. UE =;
2. Build a naïve Bayesian classifier Q with P and {AN} using
3. For each document di ∈ U
4.
5.
6.
7. output UE;
Figure 3. Building the final NB classifier

Using Q to classify di using Equation (3);
If (Pr(-|di) > Pr(+|di))

UE = UE {di};

UE stores the set of unexpected documents identified in U
(or test set T), initialized to empty set in Step 1. In Step 2, we
use Equations (1) and (2) to build a NB classifier by comput-
ing the prior probabilities Pr(+) and Pr(-), and the conditional
probabilities of Pr(wi|+) and Pr(wi|-). Clearly, Pr(wi|+) and
Pr(wi|-) can be computed based on the positive set P and the
single negative document AN respectively (AN can be regarded
as the average document of a set of virtual negative docu-
ments). However, the problem is how to compute the prior
probabilities of Pr(+) and Pr(-). It turns out that this is not a
major issue  we can simply assume that we have generated
a negative document set that has the same number of docu-
ments as the number of documents in the positive set P. We
will report experimental results that support this in the next
section. After building the NB classifier Q, we use it to clas-
sify each test document in U (Steps 3-6). The final output is
the UE set that stored all the identified unexpected documents
in U.

4 Empirical Evaluation
In this section, we evaluate our proposed technique LGN.
We compare it with both one-class SVM (OSVM, we use
LIBSVM http://www.csie.ntu.edu.tw/~cjlin/libsvm/)
and
existing PU learning methods: S-EM [Liu et al., 2002],
PEBL [Yu et al., 2002] and Roc-SVM [Li and Liu, 2003].
S-EM and Roc-SVM are publicly available1. We imple-
mented PEBL as it is not available from its authors.

4.1 Datasets
For evaluation, we used the benchmark 20 Newsgroup col-
lection, which consists of 11997 documents from 20 differ-
ent UseNet discussion groups. The 20 groups were also
categorized into 4 main categories, “computer”, “recrea-
tion”, “science”, and “talk”. We first perform the following
two sets of experiments:
2-classes: This set of experiments simulates the case in
which the training data has two classes, i.e. our positive set
P contains two classes. The two classes of data were chosen

1 http://www.cs.uic.edu/~liub/LPU/LPU-download.html

11. Let m = max(entropy(wj)), j =1, ..., |V|;
12. For each feature wi ∈ V
13.

entropy

w
i

)

(

;

wq
i

(

−=

1)

For j = 1 to |Dwi

m
| * q(wi)

Generate a frequency fnew(wi), using the Gaussian
distribution

−

−

x

)

(

2

μ
iw
σ
2
iw

2

1

πμ

2

w
i

e

AN = AN

16.
17. Output AN
Figure 2. Generating the negative document AN

{(wi, fnew(wi))}

In the algorithm, Step 1 initializes the negative docu-
ment AN (which consists of a set of feature-frequency pairs)
to the empty set while Steps 2 and Step 3 initialize the posi-
tive set P and the unlabeled set U. From Step 4 to Step 7, for
each feature wi that appeared in U, we compute its fre-
quency in each document, and then calculate the frequency
mean and variance in those documents Dwi that contain wi.
These information are used to generate AN later. From Step
8 to Step 10, we compute the entropy of wi using Pr(wi|+)
and Pr(wi|-) (which are computed using Equation (2) by
assuming that all the documents in U are negative). After
obtaining the maximal entropy value in Step 11, we gener-
ate the negative document AN in Steps 12 to 16. In particular,
Step 13 computes q(wi), which shows how “negative” a
feature wi is in terms of how different the wi’s distributions
in U and in P are
the bigger the difference, the higher the
frequency with which we generate the feature. Steps 14 to
16 is an inner loop and |Dwi| * q(wi) decides the number of
times we generate a frequency for word wi. Thus, if q(wi) is
small, it means that wi has occurred in both P and U with
similar probabilities, and we generate fewer wi. Otherwise,
wi is quite likely to be a negative feature and we generate it
with a distribution similar to the one in U. In each iteration,
Step 15 uses a Gaussian distribution with corresponding wi
and σwi to generate a frequency fnew(wi) for wi. Step 16
places the pair (wi, fnew(wi)) into the negative document AN.
Finally, Step 17 outputs our generated negative set AN. Note
that the frequency for each feature wi in AN may not of an
integer value as it is generated by a Gaussian distribution.
AN is essentially a randomly generated aggregated document

IJCAI-07

2805

from two main categories, “computer” and “science”, in
which the “computer” group has five subgroups, and the
“science” group has four subgroups. Every subgroup con-
sists of 1,000 documents.

Each data set for training and testing is then constructed
as follows: The positive documents for both training and
testing consist of documents from one subgroup (or class) in
“computer” and one subgroup (or class) in “science”. This
gives us 20 data sets. For each class (or subgroup), we parti-
tioned its documents into two standard subsets: 70% for
training and 30% for testing. That is, each positive set P for
training contains 1400 documents of two classes, and each
test set U contains 600 positive documents of the same two
classes. We then add negative (unexpected) documents to U,
which are randomly selected from the remaining 18 groups.
In order to create different experimental settings, we vary
the number of unexpected documents, which is controlled
by a parameter α, a percentage of |U|, i.e., the number of
unexpected documents added to U is α× |U|.
3-classes: This set of experiments simulates the case in
which the training data has three different classes, i.e. our
positive set P contains three classes of data. We used the
same 20 data sets formed above and added another class to
each for both P and U. The added third class was randomly
selected from the remaining 18 groups. For each data set,
the unexpected documents in U were then randomly se-
lected from the remaining 17 newsgroups. All other settings
were the same as for the 2-classes case.

4.2 Experimental Results
2-classes: We performed experiments using all possible c1
and c2 combinations (i.e., 20 data sets). For each technique,
namely, OSVM, S-EM, Roc-SVM, PEBL and LGN, we
performed 5 random runs to obtain the average results.
In
each run, the training and test document sets from c1 and c2
as well as the unexpected document instances from the other
18 classes were selected randomly. We varied αfrom 5% to
100%. Table 1 shows the classification results of various
techniques in terms of F-score (for negative class) when α=
5%. The first column of Table 1 lists the 20 different com-
binations of c1 and c2. Columns 2 to 5 show the results of
four techniques OSVM, S-EM, Roc-SVM and PEBL re-
spectively. Column 6 gives the corresponding results of our
technique LGN.

We observe from Table 1 that LGN produces the best re-
sults consistently for all data sets, achieving an F-score of
77.0% on average, which is 54.8%, 32.8%, 60.2% and
76.5% higher than the F-scores of existing four techniques
(OSVM, S-EM, Roc-SVM and PEBL) respectively in abso-
lute terms. We also see that LGN is highly consistent across
different data sets. In fact, we have checked the first step of
the three existing PU learning techniques and found that
most of the extracted negative documents were wrong. As a
result, in their respective second steps, SVM and EM were
unable to build accurate classifiers due to very noisy nega-
tive data. Since the S-EM algorithm has a parameter, we
tried different values, but the results were similar.

46.3
54.1
39.0
49.5
43.1
39.6
36.4
40.5
46.0
42.4
52.6
40.0
46.5
47.5
41.9
41.5
54.1
48.2
39.9
34.4
44.2

17.2
15.8
15.3
15.7
18.3
16.3
16.5
17.9
17.5
17.5
16.5
17.5
16.9
17.1
16.4
17.4
17.3
16.0
16.1
16.4
16.8

0.0
3.5
0.0
0.0
0.0
0.0
0.0
0.0
1.2
0.0
0.0
0.0
0.0
1.3
0.0
1.3
2.3
0.0
1.3
0.0
0.5

22.5
17.9
17.2
22.2
23.6
15.9
18.6
20.8
23.1
18.8
18.3
21.3
25.4
19.6
17.4
21.3
22.6
19.4
20.4
18.3
20.2

Table 1. Experimental results for α= 5%.
Data Set
OSVM S-EM Roc-SVM PEBL LGN
82.1
graphic-crypt
78.0
graphic-electro
64.9
graphic-med
71.7
graphic-space
82.8
os-crypt
80.2
os-electronics
75.2
os-med
78.2
os-space
84.8
mac.hardware-crypt
84.3
mac.hardware-electro
70.4
mac.hardware-med
77.9
mac.hardware-space
82.9
ibm.hardware-crypt
82.4
ibm.hardware-electro
74.5
ibm.hardware-med
75.5
ibm.hardware-space
82.0
windows-crypt
76.3
windows-electro
66.2
windows-med
69.8
windows-space
Average
77.0
Figure 4 shows the macro-average results of all αvalues
(from 5% to 100%) for all five techniques in the 2-classes
experiments. Our method LGN outperformed all others sig-
nificantly for α≤ 60%. When αwas increased to 80% and
100%, Roc-SVM achieved slightly better results than LGN.
We also observe that OSVM, S-EM and Roc-SVM outper-
formed PEBL since they were able to extract more reliable
negatives than the 1-DNF method used in PEBL. PEBL
needed a higher α(200%) to achieve similar good results.

e
r
o
c
s
-
F

100.0

80.0

60.0

40.0

20.0

0.0

LGN
S-EM
Roc-SVM
PEBL
OSVM

5%

10%

15%

20%

40%

60%

80%

100%

a % of unexpected documents

Figure. 4. The comparison results with different percentages
of unexpected documents in U in the 2-classes experiments.

3-classes: Figure 5 shows the 3-classes results where LGN
still performed much better than the methods when the pro-
portion of unexpected documents is small (α ≤ 60%) and
comparably with S-EM and Roc-SVM when the proportion
is larger. OSVM’s results are much worse than S-EM,
Roc-SVM and LGN when α is larger, showing that PU
learning is better than one-class SVM in the problem.
Again, PEBL required a much larger proportion of unex-
pected documents to produce comparable results.

IJCAI-07

2806

100.0

80.0

e
r
o
c
s
-
F

60.0

40.0

20.0

0.0

LGN
S-EM
Roc-SVM
PEBL
OSVM

5%

10%

15%

20%

40%

60%

80%

100%

a % of unexpected documents

Figure. 5. The comparison results with different percentages
of unexpected documents in U in the 3-classes experiments.

In summary, we conclude that LGN is significantly better
(with high F-scores) than the other techniques when α is
small (α≤ 60%), which indicates that it can be used to effec-
tively extract unexpected documents from the test set even in
the challenging scenarios in which their presence in U is
non-obvious. The other methods all failed badly when αis
small. LGN also performed comparably in the event when the
proportion of unexpected instances is large (α≥ 80%).

Finally, we also conducted 10-classes experiments in
which ten different classes from both the 20 Newsgroups and
Reuter collections (with same experimental setting for the
3-classes) were used. The behaviors of the algorithms for 10
classes were the same as for 2 classes and 3 classes. Using the
Reuter collection with 10 classes and αset to 5%, 10%, 15%,
20% and 40%, our algorithm LGN achieved 32.77%,
32.14%, 27.82%, 18.43%, 11.11% higher F-scores respec-
tively than the best results of the existing methods (OSVM,
S-EM, Roc-SVM and PEBL). Similarly, using 10 classes
from the 20 newsgroup collection, LGN achieved 10.56%,
4.80%, 5.46%, 6.20%, and 4.00% higher F-scores for α=5%,
10%, 15%, 20% and 40% of unexpected documents respec-
tively than the best of the four other existing methods.
Effect of priors: Recall that in Section 3 we have left the
prior probabilities as a parameter since we only generate a
single artificial negative document. To check the effect of
priors, we also varied the prior in our experiments by
changing the proportion of negative documents as a per-
centage of the number of positive documents in P. We tried
40%, 60%, 80% and 100%. The results were virtually the
same, with average differences only within ±1%. Thus, we
simply choose 100% as the default of our system, which
gives us Pr(+) = Pr(-) = 0.5. All the experimental results
reported here were obtained using this default setting.

5 Conclusion
In real-world classification applications, the test data may
differ from the training data because unexpected instances
that do not belong to any of the predefined classes may be
present (or emerge in the long run) and they cannot be identi-
fied by traditional classification techniques. We have shown
here that the problem can be addressed by formulating it as a
PU learning problem. However, directly applying existing PU

learning algorithms performed poorly as they require a large
proportion of unexpected instances to be present in the unla-
beled test data, which is often not the case in practice.

We then proposed a novel technique LGN to identify un-
expected documents by generating a single artificial negative
document to help train a classifier to better detect unexpected
instances. Our experimental results in document classification
demonstrate that LGN performed significantly better than
existing techniques when the proportion of unexpected in-
stances is low. The method is also robust irrespective of the
proportions of unexpected instances present in the test set.
Although our current experiments were performed in the text
classification application using an NB classifier, we believe
that the approach is also applicable to other domains. Using a
single artificial negative document, however, will not suitable
for other learning algorithms. In our future work, we plan to
generate a large set of artificial documents so that other
learning methods may also be applied.

References
[Crammer and Chechik, 2004] K. Crammer and G. Chechik.
local one-class optimization,

A needle in a haystack:
ICML, 2004.

[Dempster et al., 1977] A. Dempster, N. Laird and D. Rubin,
Maximum likelihood from incomplete data via the EM
algorithm, Journal of the Royal Statistical Society, 1977.

[Denis, 1998] F. Denis, PAC learning from positive statisti-

cal queries. ALT, 1998.

[Denis , 2002] F. Denis, R. Gilleron, and M. Tommasi. Text
classification from positive and unlabeled examples.
IPMU, 2002.

[Fung, 2005] G. Fung, J. Yu, H. Lu, and P Yu. Text Classi-
fication without Labeled Negative Documents. ICDE,
2005.

[Lewis and Gale, 1994] D. Lewis and W. Gale. A sequential

algorithm for training text classifiers. SIGIR, 1994.

[Li and Liu, 2003] X. Li, and B. Liu. Learning to classify

text using positive and unlabeled data. IJCAI, 2003.

[Liu et al., 2002] B. Liu, W. Lee, P. Yu, and X. Li. Partially
supervised classification of text documents. ICML, 2002.
[Manevitz and Yousef, 2001] L. Manevitz, and M. Yousef.
One class SVMs for document classification. Journal of
Machine Learning Research, 2, 139–154, 2001.

[McCallum and Nigam, 1998] A. McCallum, and K. Nigam,
A comparison of event models for naïve Bayes text clas-
sification. AAAI, 1998.

[Muggleton, 2001] S. Muggleton. Learning from the posi-

tive data. Machine Learning, 2001.

[Rocchio, 1971] J. Rocchio. Relevant feedback in informa-
tion retrieval. G. Salton. The smart retrieval system: ex-
periments in automatic document processing, 1971.

[Scholkopf et al., 1999] B. Scholkopf, J. Platt, J. Shawe, A.
Smola & R. Williamson. Estimating the support of a
high-dimensional
Report
MSR-TR-99-87, Microsoft Research, 1999.

distribution.

Technical

[Yu et al., 2002] H. Yu, J. Han, and K. Chang. PEBL: Posi-
tive example based learning for Web page classification
using SVM. KDD, 2002.

IJCAI-07

2807


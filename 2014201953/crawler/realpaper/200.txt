Word Sense Disambiguation with Distribution Estimation

Yee Seng Chan and Hwee Tou Ng
Department of Computer Science
National University of Singapore

3 Science Drive 2, Singapore 117543

fchanys, nghtg@comp.nus.edu.sg

Abstract

A word sense disambiguation (WSD) system
trained on one domain and applied to a different
domain will show a decrease in performance. One
major reason is the different sense distributions
between different domains. This paper presents
novel application of two distribution estimation al-
gorithms to provide estimates of the sense distribu-
tion of the new domain data set. Even though our
training examples are automatically gathered from
parallel corpora, the sense distributions estimated
are good enough to achieve a relative improvement
of 56% when incorporated into our WSD system.

1 Introduction
Many words have multiple meanings. The process of iden-
tifying the sense of a word in a particular context is known
as word sense disambiguation (WSD). WSD is an important
task of natural language processing (NLP), and is important
for applications such as machine translation and information
retrieval. Past research has shown that a good approach to
WSD is the use of supervised machine learning. With this
approach, a large text corpus in which each ambiguous word
has been annotated with the correct sense has to be collected
to serve as training data.

This corpus-based approach, however, raises the problem
of domain dependence faced by these supervised WSD sys-
tems. A system trained on data from one domain (for exam-
ple, sports), will show a decrease in performance when ap-
plied to a different domain (for example, economics). This
issue of domain difference affecting WSD performance had
been brought up by various researchers. Escudero et al.
[2000] showed that training a WSD system in one domain
and applying it to another could lead to a drop of 12% to
19% in WSD accuracy.
It was shown that one of the rea-
sons is due to different distributions of senses across domains,
as a 9% improvement in accuracy was observed when they
sense-balanced the instances from the different domains. Re-
cently, Agirre and Martinez [2004] conducted experiments
with training data for WSD which were automatically gath-
ered from the Internet. They reported a potential improve-
ment of 14% in accuracy if they have access to the sense dis-
tribution of their test data.

The work of these researchers showed that the different
sense distributions across domains have an important effect
on WSD accuracy. Therefore, estimation of the target do-
main’s sense distribution will be crucial, in order to build
WSD systems that are portable across different domains. A
partial solution to this would be to determine the predomi-
nant, or most frequently occurring sense of each word in a
corpus. Research has shown that the baseline heuristic of se-
lecting the most frequently occurring sense of a word gives
high WSD accuracy. This is due to the often skewed distribu-
tion of word senses. McCarthy et al. [2004b] addressed this
when they presented a method to automatically rank word
senses in a corpus, in order to determine the predominant
sense of each word. They obtained good results when the
method is applied on the SENSEVAL-2 English all-words
task [Palmer et al., 2001]. The same method is used in a re-
lated work [McCarthy et al., 2004a] to identify infrequently
occurring word senses.

In the machine learning literature, algorithms to estimate
class a priori probabilities have been developed. In this pa-
per, we present novel application of two distribution estima-
tion algorithms to provide estimates of the sense distribution,
or a priori probabilities of senses, and show that they are ef-
fective in improving the WSD accuracy of systems trained
and applied on different domains. Note that determining the
predominant sense is a special case of trying to estimate the
complete sense distribution. To the best of our knowledge,
these algorithms have not been used in NLP or for WSD.

We recognize that a WSD system should be scalable. This
is a problem faced by current supervised learning systems,
as they usually relied on manually annotated data for train-
ing. To tackle this problem, our prior work[Ng et al., 2003]
exploited parallel texts for WSD. Encouraging results were
obtained in our evaluation on the nouns of SENSEVAL-2 En-
glish lexical sample task [Kilgarriff, 2001], which used the
WordNet 1.7 sense inventory [Miller, 1990]. Note that the
usage of parallel texts as training data represents a natural
domain difference with the test data of SENSEVAL-2 English
lexical sample task, of which 91% is drawn from the British
National Corpus (BNC). In this paper, we chose to draw train-
ing data for our experiments from parallel texts, and similarly
base our evaluation on the nouns of SENSEVAL-2 English
lexical sample task.

This paper is structured as follows.

In the next section,

Parallel corpora

Hong Kong Hansards
Hong Kong News
Hong Kong Laws
Sinorama
Xinhua News
English translation of Chinese Treebank
Total

Size of English texts

(in million words (MB))

Size of Chinese texts

(in million characters (MB))

39.9 (223.2)
16.8 (96.4)
9.9 (53.7)
3.8 (20.5)
2.1 (11.9)
0.1 (0.7)

72.6 (406.4)

35.4 (146.8)
15.3 (67.6)
9.2 (37.5)
3.3 (13.5)
2.1 (8.9)
0.1 (0.4)

65.4 (274.7)

Table 1: Size of English-Chinese parallel corpora

we discuss the process of gathering training data from par-
allel texts. In section 3, we describe the two distribution es-
timation algorithms used. We also give a brief description
of the predominant sense method presented by [McCarthy
et al., 2004b].
In section 4, we evaluate the effectiveness
of the two algorithms and the predominant sense method in
improving WSD accuracy. Evaluation results on the nouns
in SENSEVAL-2 English lexical sample task are presented.
Discussions are made in section 5, before we conclude in sec-
tion 6.

2 Training Data from Parallel Texts
In this section, we brieﬂy describe the parallel texts used in
our experiments, and the process of gathering training data
from them.

2.1 Parallel Text Alignment
Table 1 lists the 6 English-Chinese parallel corpora (available
from Linguistic Data Consortium) used in our experiments.
The sentences of the 6 corpora were already pre-aligned, ei-
ther manually or automatically, when they were prepared. Af-
ter ensuring the corpora were sentence aligned, we tokenized
the English texts, and perform word segmentation on the Chi-
nese texts. Next, word alignment was done on the parallel
corpora using the GIZA++ software [Och and Ney, 2000].

2.2 Target Translations Selection
We took the same approach as described in our previous work
[Ng et al., 2003] to select some possible Chinese translations
for each sense of an English word. For instance, WordNet
1.7 lists 5 senses for the noun nature. Senses will be lumped
together if they are translated in the same way in Chinese.
For example, sense 2 and 3 of nature are lumped together
as they are both translated as “L(cid:7)l” in Chinese. Then,
from the word alignment output of GIZA++, we select those
occurrences of the noun nature which have been aligned to
one of the Chinese translations chosen. The English side of
these occurrences will then serve as training data for the noun
nature, as they are considered to have been disambiguated
and “sense-tagged” by the appropriate Chinese translations.
The time needed to perform manual target translations of
word senses is relatively short, compared to the effort needed
to manually sense annotate training examples. Also, this step
could potentially be automated with the use of an English-
Chinese dictionary.

bp(cid:3)

3 Distribution Estimation
To estimate the sense distribution, or a priori probabilities of
the different senses in a new data set, we made use of a con-
fusion matrix algorithm [Vucetic and Obradovic, 2001] and
an EM based algorithm [Saerens et al., 2002], which we de-
scribe in this section. A brief description of the predominant
sense method [McCarthy et al., 2004b] is also given, before
we discuss how to make use of the estimated a priori proba-
bilities to improve classiﬁcation accuracy.
3.1 Confusion Matrix
Let us assume that from a set of labeled data SL, a classiﬁer

which is an estimate of the probability of classifying an in-
stance as class !j , when in fact it belongs to class !i . Then,
one can apply this classiﬁer with known conditional prob-

is used to compute the conditional probabilities bpL(!jj!i) ,
abilities bpL(!jj!i) to a set of unlabeled data SU to obtain
note asbq(!j) . The a priori probabilitiesbp(!i) on SU are then

its predictions. From these predictions, one can estimate the
probability of predicting class !j on SU , which we will de-

estimated by solving the following equation:

j = 1; : : : ; n

(1)

bq(!j) =

nXi=1bpL(!jj!i)bp(!i);

where n represents the number of classes. Equation (1) can
be represented in matrix form as q = P (cid:1) p, from which the a
priori probabilities on SU , represented by p , can be estimated
by solving:

p = P(cid:0)1 (cid:1) q

(2)
To obtain estimates of P and q, bootstrap sampling [Efron
and Tibshirani, 1993] is employed. We will ﬁrst describe the
basic idea of bootstrap before giving the bootstrap algorithm.
In bootstrap sampling, given an original sample X with n
examples, bootstrap sample X(cid:3) is obtained by randomly sam-
pling n examples from X with replacement. To estimate the

conditional probabilitiesbpL(!jj!i), we need to split SL into

Strain and Stest. Further, let us deﬁne n(cid:3)
test(!j; !i) as the
test predicted to
number of examples in a bootstrap sample S (cid:3)
be of class !j when true class is !i. Also, we deﬁne n(cid:3)
U (!j)
U predicted
as the number of examples in a bootstrap sample S(cid:3)
to be of class !j. We now give the bootstrap algorithm below:
Given Stest, SU , and a classiﬁer, repeat the following for B

iterations (In our experiments, we set B to 200):

(cid:15) Generate a bootstrap sample from ntest examples of

Stest and calculate:
L(!jj!i) = n(cid:3)
Pj n(cid:3)

test(!j ;!i)

test(!j ;!i)

for i; j = 1; : : : ; n

(cid:15) Generate a bootstrap sample from nU examples of SU

estimate the a priori probabilities

and calculate:

U (!j )
nU

for j = 1; : : : ; n

bq(cid:3)(!j) = n(cid:3)
(cid:15) Use (2) to calculatebp(cid:3)(!i)(b)for i = 1; : : : ; n
BPB
bp(!i) = 1

b=1bp(cid:3)(!i)(b)

After B iterations,

3.2 EM based Algorithm
Once again, let us assume we train a classiﬁer on a set of
labeled data SL with n classes. Further, suppose we have a
set of N independent instances, or independent realizations of
the stochastic variable x, (x1; x2; : : : ; xN ) from a new data
set. The likelihood of these N instances can be deﬁned as:

L(x1; x2; : : : ; xN ) =

=

=

p(xk)

NYk=1
NYk=1" nXi=1
NYk=1" nXi=1

p(xk; !i)#
p(xkj!i)p(!i)# (3)

If we assume the generation of the instances within the
classes, and thus the within-class densities p(xkj!i) (i.e.,
the probabilities of observing xk given the class !i) do not
change from the training set SL to the new data set, we can
deﬁne p(xkj!i) = pL(xkj!i) . To determine the a priori

probability estimatesbp(!i) of the new data set that will max-

imize the likelihood of (3) with respect to p(!i) , we can ap-
ply the iterative procedure of the EM algorithm.
In effect,
through maximizing the likelihood of (3), we obtain the a pri-
ori probability estimates as a by-product.

Before we show the steps of the EM algorithm, it will be
helpful to deﬁne some notations. When we apply the classi-
ﬁer trained on SL on an instance xk drawn from the new data

probabilities of class !i in SL , which can be estimated by the

ity of instance xk being classiﬁed as class !i by the classiﬁer

set SU , we getbpL(!ijxk) , which we deﬁne as the probabil-
trained on SL. Further, let us deﬁne bpL(!i) as the a priori
class frequency of !i in SL. Also, we will deﬁnebp(s)(!i) and
bp(s)(!ijxk) as estimates of the new a priori and a posteriori
ing we initialize bp(0)(!i) by the a priori probabilities of the

probabilities at step s of the iterative EM procedure. Assum-

classes in the labeled data:

then for each new instance xk in SU , and each class !i, the
EM algorithm provides the following iterative steps:

bp(0)(!i) =bpL(!i)

bpL(!i)

bpL(!ijxk) bp(s)(!i)
bp(s)(!ijxk) =
Pn
j=1bpL(!jjxk) bp(s)(!j )
NXk=1bp(s)(!ijxk)
bp(s+1)(!i) =

bpL(!j )

1
N

(4)

(5)

where (4) represents the expectation E-step, (5) represents
the maximization M-step, and N represents the number of in-

(4) is simply a normalizing factor.

s are simply the a posteriori probabilities in the conditions of

for each particular instance xk and class !i . Observe that

stances in SU . Note that in (4), the probabilities bpL(!ijxk)
and bpL(!i) will stay the same through the iteration steps s
for (4), the new a posteriori probabilitiesbp(s)(!ijxk) at step
the labeled data,bpL(!ijxk), weighted by the ratio of the new
priorsbp(s)(!i) to the old priorsbpL(!i). The denominator in
At each iteration step s, both the a posteriori bp(s)(!ijxk)
and a priori probabilities bp(s)(!i) are re-estimated sequen-
convergence of the estimated probabilitiesbp(s)(!i). This it-

erative procedure will increase the likelihood of (3) at each
step.

tially for each new instance xk and each class !i, until the

3.3 Predominant Sense

A method of automatically ranking WordNet senses to deter-
mine the predominant, or most frequent sense, of a noun in
the BNC corpus is presented in [McCarthy et al., 2004b]. A
thesaurus is ﬁrst acquired from the parsed 90 million words of
written English from the BNC corpus to provide the k nearest
neighbors to each target noun, along with the distributional
similarity score (dss) between the target noun and its neigh-
bor. The WordNet similarity package [Pedersen et al., 2004]
is then used to give a WordNet similarity measure (wnss),
which is used to weight the contribution that each neighbor
makes to the various senses of the target noun. Through a
combination of dss and wnss, a prevalence score for each
sense of the target noun is calculated, from which the pre-
dominant sense of the noun in BNC is determined.

Though the focus of the method is to determine the pre-
dominant sense, we could obtain an estimated sense distribu-
tion by normalizing the prevalence score of each sense. As
noted in section 1, 91% of the test examples of SENSEVAL-
2 English lexical sample task were drawn from BNC. Thus,
it is reasonable to expect the sense distribution estimated by
this predominant sense method to be applicable to the test ex-
amples of SENSEVAL-2 English lexical sample task. We im-
plemented this method in order to evaluate its effectiveness in
improving WSD accuracy. Our implementation achieved ac-
curacies close to those reported by [McCarthy et al., 2004b].
McCarthy et al. [2004b] reported a jcn measure predominant
sense accuracy of 54% on the Semcor corpus, while we mea-
sured 53.3% using our implementation. On the SENSEVAL-
2 English all-words task, a 64% precision and 63% recall
was reported. Our implementation gave 66.2% precision and
63.4% recall. The differences could be due to some minor
processing steps which were not described in detail in [Mc-
Carthy et al., 2004b]. Finally, note that this predominant
sense method is only effective on a very large text corpus,
as the thesaurus can only be effectively generated from a very
large corpus.

Maximum
number of par-
allel examples
per noun

150
200
300
500

L

EM sample

TRUE (cid:0) L

EM (cid:0) L

ConfM (cid:0) L

Predominant (cid:0) L

68.29
69.29
70.29
72.21

69.35
71.33
72.07
73.57

adjust
1.76
1.58
1.55
1.37

sample
3.70
3.75
3.69
2.40

adjust
0.81
0.88
0.71
0.77

sample
1.06
2.04
1.78
1.36

adjust
0.92
0.90
0.76
0.79

sample
1.04
1.20
0.74
0.95

adjust
0.84
0.59
0.57
0.35

sample
0.59
0.29
0.62
0.09

Table 2: 5 trials micro-averaged scores for 22 SE2 nouns

Maximum number of parallel examples per noun

150
200
300
500

adjust
46.02
55.70
45.81
56.20

EM (cid:0) L

ConfM (cid:0) L

sample
28.65
54.40
48.24
56.67

adjust
52.27
56.96
49.03
57.66

sample
28.11
32.00
20.05
39.58

Predominant (cid:0) L
sample
adjust
15.95
47.73
7.73
37.34
16.80
36.77
25.55
3.75

Table 3: Relative improvement percentage from using the a priori probabilities estimated by the 3 methods

3.4

Improving Classiﬁcation Based on A Priori
Estimates

If a classiﬁer was trained to estimate posterior class probabil-

SU , it can be directly adjusted according to estimated class

itiesbpL(!ijxk) when presented with a new instance xk from
probabilitiesbp(!i) on SU .
Denoting predictions of the classiﬁer asbpL(!ijxk), a priori
probability of class !i from SL asbpL(!i), estimated a priori
probability of class !i from SU asbp(!i), adjusted predictions
bpadjust(!ijxk) can be calculated as:

(6)

bpadjust(!ijxk) = bpL(!ijxk) bp(!i)
PjbpL(!jjxk) bp(!j )

bpL(!i)

bpL(!j )

Note the similarity of (6) with (4).

4 Evaluation
In our experiments, we implemented the supervised WSD ap-
proach of [Lee and Ng, 2002], which was reported to achieve
state-of-the-art accuracy. The knowledge sources used in-
clude parts-of-speech, surrounding words, and local colloca-
tions. We use the naive Bayes algorithm as our classiﬁer,
which was reported to achieve good WSD accuracy and it is
fast to train and test with the naive Bayes classiﬁer. Also,
the naive Bayes classiﬁer outputs posteriori probabilities in
its classiﬁcations, which can be used directly to adjust the
predictions based on (6).

4.1 Results
We used the approach outlined in section 2 to obtain training
examples from parallel texts. Two senses s1 and s2 of a noun
are lumped together if they are translated into the same Chi-
nese word. Although there are 29 nouns in the SENSEVAL-2
English lexical sample task, the senses of 7 nouns are lumped
into one sense (i.e., all senses of each of these 7 nouns are
translated into one Chinese word). Thus, we limit our evalu-
ation to the remaining 22 nouns.

Saerens et al. [2002] reported that increasing the training
set size results in an improvement in the a priori estimation.
To investigate this issue, we gradually increased the maxi-
mum number of examples per noun selected from the parallel
texts. For each condition, we sampled training examples from
the parallel text using the natural distribution of the parallel
text, up to the maximum allowed number of examples per
noun. We noted the WSD accuracy achieved by the classiﬁer
without any adjustment, in the column labeled L in table 2. To
provide a basis for comparison, we adjusted the classiﬁer pos-
teriori probability output by the true sense distribution of the
test data, using equation (6). The increase in WSD accuracy
thus obtained is listed in the column adjust under TRUE(cid:0)L.
For example, if we knew exactly the true sense distribution of
the test data, we would have achieved an accuracy of 68.29%
+ 1.76% = 70.05%, when trained on a maximum of 150 ex-
amples per noun.

Besides adjusting the classiﬁer posteriori probability out-
put, one could also re-sample training examples from the par-
allel texts according to the true sense distribution. The in-
crease in WSD accuracy obtained using this approach is listed
in the column sample under TRUE(cid:0)L. Note that scores listed
under TRUE(cid:0)L indicate the upper-bound accuracy achiev-
able had we known the true sense distribution of the test data.
Increase in WSD accuracy by adjusting the classiﬁer out-
put using the sense distribution estimated by the EM algo-
rithm is listed in column adjust under EM(cid:0)L. Correspond-
ing increase by re-sampling training examples from the par-
allel texts according to the sense distribution estimated by
the EM algorithm is listed in column sample under EM(cid:0)L.
Similar increases by using sense distribution estimated by
the confusion matrix algorithm, and the predominant sense
method, are listed under the columns adjust and sample, un-
der ConfM(cid:0)L and Predominant(cid:0)L. The corresponding rel-
ative improvement of the various accuracy improvements
(comparing against using the true sense distribution) achieved
by the two algorithms and the predominant sense method is
given in Table 3. As an example, for 150 parallel examples

System Performance

9
8
.
9
7

2
6
.
7
7

5
1
.
7
7

4
9
.
6
7

0
6
.
6
7

0
7
.
5
7

1
4
.
5
7

0
1
.
5
7

9
0
.
5
7

4
9
.
4
7

2
7
.
4
7

0
0
.
4
7

0
0
.
4
7

7
5
.
3
7

5
5
.
3
7

2
3
.
1
7

0
0
.
1
7

5
4
.
0
7

0
1
.
4
6

6
8
.
3
6

5
1
.
7
4

4
8
.
5
3

9
0
.
5
3

1
3
.
4
3

3
7
.
7
3

1
6
.
2
3

0
1
.
4
2

y
c
a
r
u
c
c
a
 
D
S
W

 100

 90

 80

 70

 60

 50

 40

 30

 20

 10

 0

Figure 1: Comparison of Parallel-EM’s performance against SENSEVAL-2 participating systems. The single shaded bar represents
Parallel-EM, the empty white bars represent the supervised systems, and the pattern ﬁlled bars represent the unsupervised systems.

per noun, EM adjust gives an improvement of 0.81%, against
an improvement of 1.76% using TRUE adjust. Thus, relative
improvement is 0.81=1.76 = 46.02%.

The results show that the confusion matrix and EM algo-
rithms are effective in improving WSD accuracy. Also, the
two algorithms are most effective when a large training data
set (which is 500 examples per noun in our experiments) is
used. The results also indicate the potential of parallel text
scaling up to achieve better WSD accuracy as more training
examples are used, as shown by the increasing accuracy ﬁg-
ures under the L column of Table 2. This is a phenomenon not
explored in [Ng et al., 2003], which used the same number of
parallel text examples as the ofﬁcial SENSEVAL-2 training
data. At the same time, however, this caused a progressive re-
duction in the absolute amount of possible WSD accuracy im-
provement, as indicated by the reduction under the columns
adjust and sample of TRUE(cid:0)L in Table 2 when more parallel
texts examples were used. Our experiments also indicate that
application of the two algorithms achieved higher increases
in WSD accuracy when compared against the predominant
sense method, which has the primary aim of determining the
predominant sense of a word in a corpus.

We note that re-sampling the training examples gives bet-
ter WSD improvements than merely adjusting the posteriori
probabilities output by the classiﬁer. The number of training
examples for a sense will affect the accuracy of the classiﬁer
trained for that sense. Thus, if one has a reliable estimate
of the sense distribution, it might be more beneﬁcial to re-
sample the training examples. The results show that sampling
a maximum of 500 examples per noun from the parallel texts
according to the distribution estimated by the EM algorithm
gives our best achievable WSD accuracy of 73.57% (72.21%
+ 1.36%). In the column EM sample of Table 2, we show the
WSD accuracy resulting from sampling parallel text exam-
ples according to the EM estimated sense distribution, with
different maximum number of examples per noun (i.e., this

column is obtained by adding the ﬁgures from the columns L
and EM(cid:0)L sample).

4.2 Comparison with SE2 Systems
We compare our WSD accuracy achieved with the
SENSEVAL-2 English lexical sample task participating sys-
tems. We performed sense lumping on the publicly available
predictions of the SENSEVAL-2 participating systems. The
WSD accuracy of each system is then calculated over the set
of 22 nouns.
In the SENSEVAL-2 English lexical sample
task, there were 21 supervised and 51 unsupervised systems.
Figure 1 shows the relative ranking of our system (which we
named Parallel-EM) against the various SE2 systems.

We note that our system achieved better performance than
all the unsupervised systems, and would have ranked at the
64% percentile among the list of 22 (including ours) super-
vised systems.

5 Discussion
The score of 73.57% achieved by Parallel-EM is competi-
tive, considering that the SE2 supervised systems are trained
with the manually annotated ofﬁcial data, while our system
was trained with examples automatically extracted from par-
allel texts. Moreover, the following factors will also have to
be considered.

Ofﬁcial Training Data has Similar Sense Distribution as
Test Data
This is an advantage that the participating SENSEVAL-2 su-
pervised systems enjoy. Note that the results in Table 2 show
that if we sample according to the true distribution with a
maximum of 500 examples per noun, we could achieve a
score of 74.61%.

1There were 4 systems from IIT (2 were resubmissions), but sys-

tem answers were only available for 2 of them.

Some Ofﬁcial Training and Test Examples are from the
Same Document
Our prior work [Ng et al., 2003] highlighted the fact that
many of the ofﬁcial SENSEVAL-2 training and test exam-
ples are extracted from the same document, and that this has
inﬂated the accuracy ﬁgures achieved by SENSEVAL-2 par-
ticipating systems.

To investigate this effect, we trained our WSD classiﬁer us-
ing the ofﬁcial training examples. When evaluated on the ofﬁ-
cial test examples, a WSD accuracy of 79.71% was achieved.
Next, we followed the steps taken in [Ng et al., 2003]: We
took the ofﬁcial training and test examples of each noun w
and combined them together. The combined data is then ran-
domly split into a new training and a new test set such that
no training and test examples come from the same document.
While performing the split, we ensured the sense distribution
of the new training and test data sets followed those of the
ofﬁcial data sets. A WSD classiﬁer was then trained on this
new training set, and evaluated on this new test set. 5 ran-
dom trials were conducted, and our WSD classiﬁer achieved
an accuracy of 77.83% (a drop of 1.88% from 79.71%).

Lack of Parallel Training Examples for some Senses
A disadvantage faced by Parallel-EM is that, though the
senses are lumped, we are still lacking training examples for
9.64% of the test data. We believe that with the steady in-
crease in volume and variability of parallel corpora, this prob-
lem will improve in the near future.

Domain Difference
Lastly, we note that 91% of the SENSEVAL-2 English lexical
sample task test examples were drawn from BNC. This rep-
resents a domain difference with our parallel corpora, which
is mainly a mix of legislative, law, and news articles. Exam-
ples from different domains will contain different informa-
tion, thus presenting different classiﬁcation cues to the learn-
ing algorithm.

6 Conclusion
Differences in sense distribution between the training and test
sets will result in a loss of WSD accuracy. Addressing this is-
sue is important, especially if one is to build WSD systems
that are portable across different domains, of which the dif-
ference in sense distribution is an inherent issue. In this paper,
we used two distribution estimation algorithms to provide es-
timates of the sense distribution in a new data set. When ap-
plied on the nouns of the SENSEVAL-2 English lexical sam-
ple task, we have shown that they are effective in improving
WSD accuracy.

Acknowledgements
The ﬁrst author is supported by a Singapore Millennium
Foundation Scholarship (ref no. SMF-2004-1076). This re-
search is also partially supported by a research grant R252-
000-125-112 from National University of Singapore Aca-
demic Research Fund.

References
[Agirre and Martinez, 2004] E. Agirre and D. Martinez. Un-
supervised WSD based on automatically retrieved ex-
amples: The importance of bias.
In Proceedings of
EMNLP04, pages 25–32, Barcelona, Spain, July 2004.

[Efron and Tibshirani, 1993] B. Efron and R. J. Tibshirani.
An Introduction to the Bootstrap. Chapman & Hall, New
York, 1993.

[Escudero et al., 2000] G. Escudero, L. Marquez,

and
G. Rigau. An empirical study of the domain dependence
of supervised word sense disambiguation systems. In Pro-
ceedings of EMNLP/VLC’00, Hong Kong, October 2000.
[Kilgarriff, 2001] A. Kilgarriff. English lexical sample task
description. In Proceedings of SENSEVAL-2, pages 17–20,
Toulouse, France, July 2001.

[Lee and Ng, 2002] Y. K. Lee and H. T. Ng. An empiri-
cal evaluation of knowledge sources and learning algo-
rithms for word sense disambiguation.
In Proceedings
of EMNLP02, pages 41–48, Philadelphia, Pennsylvania,
USA, July 2002.

[McCarthy et al., 2004a] D. McCarthy,

R. Koeling,
J. Weeds, and J. Carroll. Automatic identiﬁcation of
infrequent word senses.
In Proceedings of COLING04,
pages 1220–1226, Geneva, Switzerland, August 2004.

[McCarthy et al., 2004b] D. McCarthy,

R. Koeling,
J. Weeds, and J. Carroll.
Finding predominant word
senses in untagged text. In Proceedings of ACL04, pages
280–287, Barcelona, Spain, July 2004.

[Miller, 1990] G. A. Miller. Wordnet: An on-line lex-
International Journal of Lexicography,

ical database.
3(4):235–312, 1990.

[Ng et al., 2003] H. T. Ng, B. Wang, and Y. S. Chan. Ex-
ploiting parallel texts for word sense disambiguation: An
empirical study. In Proceedings of ACL03, pages 455–462,
Sapporo, Japan, July 2003.

[Och and Ney, 2000] F. J. Och and H. Ney. Improved statis-
tical alignment models. In Proceeedings of ACL00, pages
440–447, Hong Kong, October 2000.

[Palmer et al., 2001] M. Palmer, C. Fellbaum, and S. Cotton.
English tasks: All-words and verb lexical sample. In Pro-
ceedings of SENSEVAL-2, pages 21–24, Toulouse, France,
July 2001.

[Pedersen et al., 2004] T. Pedersen, S. Patwardhan, and
J. Michelizzi. Wordnet::similarity - measuring the relat-
edness of concepts. In Proceedings of AAAI04, Intelligent
Systems Demonstration, San Jose, CA, July 2004.

[Saerens et al., 2002] M. Saerens, P. Latinne, and C. De-
caestecker. Adjusting the outputs of a classiﬁer to new
a priori probabilities: A simple procedure. Neural Com-
putation, 14(1):21–41, January 2002.

[Vucetic and Obradovic, 2001] S. Vucetic and Z. Obradovic.
Classiﬁcation on data with biased class distribution.
In
Proceedings of ECML01, pages 527–538, Freiburg, Ger-
many, September 2001.


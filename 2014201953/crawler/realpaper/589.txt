Approximating Optimal Policies for Agents with Limited Execution Resources 

Dmitri A. Dolgov  and  Edmund H. Durfee 

Department of Electrical Engineering and Computer Science 

University of Michigan 
Ann Arbor, MI 48109 

{ddolgov, durfee}@umich.edu 

Abstract 

An  agent  with  limited  consumable  execution  re(cid:173)
sources needs policies that attempt to achieve good 
performance  while  respecting  these  limitations. 
Otherwise,  an  agent  (such  as  a  plane)  might  fail 
catastrophically  (crash)  when  it  runs  out  of  re(cid:173)
sources  (fuel)  at  the  wrong  time  (in  midair).  We 
present a new approach to constructing policies for 
agents with limited execution resources that builds 
on  principles  of real-time  A l,  as  well  as  research 
in constrained Markov decision processes.  Specif(cid:173)
ically,  we  formulate,  solve,  and  analyze  the  pol(cid:173)
icy optimization problem where constraints are im(cid:173)
posed on the probability of exceeding the resource 
limits.  We  describe  and  empirically  evaluate  our 
solution  technique  to  show  that  it  is  computation(cid:173)
ally  reasonable,  and  that  it  generates  policies  that 
sacrifice some potential reward in order to make the 
kinds of precise guarantees about the probability of 
resource overutilization that are crucial for mission-
critical applications. 

Introduction 

1 
Optimality  is  the  gold  standard  in  rational  decision  making 
(e.g., [Russell and Subramanian,  1995]), and, consequently, 
the problem of constructing optimal policies  for autonomous 
agents  has  received  considerable  attention  over  the  years. 
Traditionally,  this problem has been viewed separately from 
the  problem  of actually  carrying  out the policies.  However, 
real agents have limitations as to what they can execute, and, 
clearly,  a  policy  is  less  useful  if an  agent  might  run  out  of 
resources while carrying out the policy. 

In  this  paper,  we  present  a  new  approach  for  construct(cid:173)
ing  policies  for  agents  that  have  limited  consumable  re(cid:173)
sources  where  running  out  of the  resources  can  have  neg(cid:173)
ative  consequences.  Whereas  AI  research  has  mostly  fo(cid:173)
cused on  (PO)MDP  [Boutilier et ai,  1999;  Dean et ai,1993; 
Howard,  1960;  Puterman,  1995]  methods  for  formulating 
policies  for agents  without  emphasizing  constraints  on  their 
execution  resources,  the  Operations  Research  literature  has 
developed  constrained  MDP  (CMDP)  [Altaian,  1999;  Put(cid:173)
erman,  1995]  techniques that can account for resource con(cid:173)
straints.  CMDP methods are particularly useful  for domains 

where  the  current  resource  amounts  are  unobservable  and 
cannot be easily  estimated  by the agent,  or where modeling 
resource amounts  in the  state description  is computationally 
infeasible.  In an aircraft scenario, some resources and situa(cid:173)
tions where such methods arc beneficial could include an air(cid:173)
plane with a broken fuel gauge (fuel amount is unobservable), 
pilot fatigue (attention is a resource that cannot be easily es(cid:173)
timated), or a combination of non-critical resources (ex.  var(cid:173)
ious refreshments) that should nevertheless not be exhausted, 
but explicit modeling of which unnecessarily complicates the 
optimization problem and increases policy size.  In the rest of 
the paper, we will use the fuel example, simply because it is 
a very intuitive instance of a consumable resource. 

However,  as  we  will  explain  later,  standard  risk-neutral 
CMDP  optimization  techniques  are  not  applicable  to  prob(cid:173)
lems where violating the constraints can have negative or, in 
the limit, catastrophic1  consequences.  The main contribution 
of this work is that it extends the standard CMDP techniques 
to handle the types of hard constraints that naturally arise  in 
problems  involving  critical  resources.  In particular,  we  for(cid:173)
mulate  an  optimization  problem  where  constraints  are  im(cid:173)
posed  on  improbability of resource  overutilization,  and  show 
how  the  problem  can  be  solved  using  standard  linear  pro(cid:173)
gramming  algorithms.  Our  formulation  yields  sub-optimal 
solutions to the constrained problem because it sacrifices po(cid:173)
tential  reward  to  make  guarantees  about  the  probability  of 
violating  the  resource  constraints.  As  we  later show,  when 
violating  constraints  incurs  dire  costs,  these  guarantees  are 
worthwhile ([Musliner et al,  1995] and references therein). 
We  introduce  our  model  in  section  2,  where  we  review 
Markov models, introduce notation, and specify our assump(cid:173)
tions  about  the  problem  domain.  Section  3  describes  and 
compares  several  candidate  solutions  for addressing  aspects 
of our problem. We then present in section 4 our new method, 
and empirically evaluate it in section 5.  We conclude with a 
discussion about  the  strengths and  limitations  of our results, 
and about our future directions. 

2  The Model 
The  stochastic  properties  of the  environments  in  the  prob(cid:173)
lems  that  we  are  addressing  lead  us  to  formulate  our  op-

!The term catastrophic is, of course, relative. We assume that the 
system designer is willing to accept certain risks to receive payoff. 

RESOURCE-BOUNDED REASONING 

1107 

timization  problem  as  a  stationary,  discrete-time,  Markov 
decision  process. 
In  this  section,  we  review  some  well-
known  facts  from  the  theory  of standard  [Puterman,  1995; 
Boutilier et a/.,  1999]  and constrained [Altman,  1999]  fully-
observable  Markov  decision  processes  and  also  discuss  the 
assumptions  that are  particular to  the class  of problems  that 
we  address  in  this  work.  This  section  provides  the  neces(cid:173)
sary background for the subsequent sections,  where we dis(cid:173)
cuss resource-constrained optimization problems. 

2.1  Markov Decision Processes 
A standard Markov decision process can be defined as a tuple 
where S  is  a  finite  set of states that the agent 
can  be  in,  is a finite set of actions that the agent can execute, 

R defines the transition function 

is the probability that the agent will go to state 
if it  executes 
action a in state i), and 
is the reward 
function (agent receives a reward of  ria  for executing action 

in state 
Clearly,  the  total  probability  of  transitioning  out  of  a 
state, given a particular action, cannot be greater than  1, i.e. 
As we discuss below, we are actually interested 

in domains where there exist states for which 

A  policy  is  defined  as  a  procedure  for  selecting  an  ac(cid:173)
tion  in  each  state.  A  policy  that  makes  its  choices  accord(cid:173)
ing  to  a  probability  distribution  over  the  set  of  actions  is 
called  randomized  and  can  be  described  as  a  mapping  of 
states  to probability  distributions  over  actions: 

A  deterministic  policy  that  always  chooses 
the  same  action  for  a  state  is,  of  course,  a  special  case 
of  a  randomized  policy. 
It  can  be  seen  (similarly  to  the 
case of standard CMDPs, as described in [Kallenberg,  1983; 
Puterman,  1995])  that  under  our  optimization  criterion  and 
constraints (section 4),  deterministic  policies can be subop-
timal.  Therefore,  in this  work,  we  focus  on  approximating 
optimal policies in the class of randomized ones. 

If, at time 0, the agent has an initial probability distribution 
over the state space, a Markov system follows the 

following trajectory: 

(1) 

where 

is the probability distribution at time 

2.2  Assumptions 
Typically,  Markov  decision  processes  are  divided  into  two 
categories:  finite-horizon  problems,  where the total number 
of  steps  that  the  agent  spends  in  the  system  is  finite  and 
is  known  a priori,  and  infinite-horizon  problems,  where  the 
agent is assumed to stay in the system forever (see [Puterman, 
1995] for a detailed discussion of both types of models). 

In this work we concentrate on dynamic real-time domains, 
where agents  have tasks  to  accomplish.  For example,  con(cid:173)
sider an agent flying a plane, whose goal is to safely get to its 
destination and land there.  This example does not naturally 
correspond to a  finite-horizon  problem, because the duration 
of executing various policies is not predetermined (unless we 
artificially impose such a finite duration, which is not easily 

justifiable).  On the other hand,  the problem does  not natu(cid:173)
rally  fit  the definition of the infinite-horizon model,  because 
the plane obviously cannot keep on  flying  forever. 

This leads us to make a slightly different and less common 
(although, certainly, not novel) assumption about how much 
time the agent spends executing  its  policy.  We assume that 
there is no predefined number of steps that the agent spends 
in the system, but that optimal policies always yield transient 
Markov processes (decision problems of this type were exten(cid:173)
sively studied by Kallenberg f 1983]).  A policy is said to yield 
a transient Markov process if the agent executing that policy 
will  eventually  leave the corresponding Markov  chain,  after 
spending  a  finite  number of time  steps  in  it.  Given  a finite 
state space, this assumption implies that there has to be some 
"leakage"  of probability  out  of the  system,  i.e. 
there  have 
to exist some state-action pairs 
for  which 
One particular case where the above assumption holds  is  in 
a  system  in  which  all  trajectories  lead  to absorbing  states. 
Once an agent enters  an absorbing  state,  it  has  finished  (or 
failed to finish) some task and has nowhere else to go, i.e. the 
probability of transitioning out of an absorbing state i is zero: 
In the plane-flying example, all trajectories 
lead to either a safe landing or a crash,  and once the agent 
enters  one of these  states,  the probability  of transitioning  to 
other states is zero. 

The transient nature of our problems leads us to adopt the 
expected total reward as the policy evaluation criterion. Given 
that an agent receives a reward whenever it executes an action, 
the total expected utility of a policy can be expressed as: 

(2) 

where T is the number of steps during which the agent accu(cid:173)
mulates utility. For a transient system with bounded rewards, 
the above sum converges for any T. 

3  Related Work 
In this  section,  we  briefly  survey  several  approaches  (based 
on well-known techniques) to solving the problem of finding 
optimal policies for agents with  limited resources, and point 
out the assumptions, strengths, and limitations of these meth(cid:173)
ods.  This establishes  a  landscape of solution algorithms,  to 
which we can compare our method (presented in the next sec(cid:173)
tion) in terms of complexity, efficiency, and solution quality. 

Fully  Observable  M D Ps 

3.1 
The  most  straightforward  way  of  handling  resource  con(cid:173)
straints  in  a  MDP  framework  is  to  explicitly  model  the  re(cid:173)
sources  by  making  their current  amounts  a  part  of the  state 
description.  This  yields  a  standard FOMDP,  which  can be 
solved  by  a  wide  variety  of efficient  methods.  The  benefit 

An alternative way of handling these states is to treat them as 
infinitely-recurrent, i.e.  once the agent gets there, it infinitely tran(cid:173)
sitions back to itself. We do not adopt this model, because it is less 
natural for our domains and also leads to unnecessary complications 
in the optimization problems. 

1108 

RESOURCE-BOUNDED REASONING 

of this approach is that it allows one to make use of all rele(cid:173)
vant information to construct the best possible policy, as the 
agent is able to base its action choice on the current state and 
resource  amounts.  The  downside  of this  approach  is  that  it 
requires  an a priori  discretization  of resource  amounts  to be 
made when the world model is constructed. Also, there is an 
additional burden of specifying the rewards and state transi(cid:173)
tions as functions of current resource amounts.  Furthermore, 
in this model,  the  size  of the  state space and,  consequently, 
the policy  size,  explodes  exponentially  in  the  number of re(cid:173)
sources,  as compared to the state  space where the resource 
amounts are not folded into the state description. 

The FOMDP approach relies on the fact that the agent can 
observe the exact amounts of all resources at runtime.  How(cid:173)
ever, this may not hold, especially in multiagent domains with 
shared resources,  when  an agent does not know what other 
agents have been doing and how much of the shared resources 
they have been consuming. 

3.2  Constrained  MDPs 
An alternative to the FOMDP approach described above is to 
treat the problem as a constrained Markov decision process 
[Altman,  1999], where the resources are not explicitly mod(cid:173)
eled, but rather are treated as constraints that are imposed on 
the space of feasible solutions (policies). 

and  Q 
resources (there is 

A  constrained  MDP  for  a  resource-limited  agent  differs 
from  a  standard  MDP  in  that  the  agent,  besides  getting  re(cid:173)
wards for executing actions, also incurs resource costs.  Con(cid:173)
sequently, a constrained MDP (CMDP) can be described as a 
tuple  (S, A, P, R, C, Q), where  C  =  _ 
defines  a  vector  of  actions'  costs 
are  used  when action 

units  of  resource 
is  executed  in  state  i), 
is  the  vector  of amounts  of available 
units  of resource 
initially  available). 
The  benefit  of the  CMDP  is  that  it  does  not  require  one 
to  explicitly  model  how  the  resources  affect  the  world.  In(cid:173)
deed, if all policies satisfy the resource constraints, one does 
not  have  to  worry  about  what  happens  when  the  resources 
are  overutilized.  Consequently,  the  state  space  and the  re(cid:173)
sulting policies are exponentially smaller than the ones in the 
FOMDP model.  The standard CMDP formulation constrains 
the expected usage of all resources to be below a certain limit 
and can be formulated as the following linear program: 

(3) 

where the variables 
number of times action a is executed in state i. 

have the interpretation of the expected 

A  weakness  of this approach  is that  it yields policies that 
can be suboptimal,  as compared to the ones constructed by 
the FOMDP method, because the agent does not base its de(cid:173)
cision on the current resource amounts, but rather completely 
ignores that aspect of the state. However, as mentioned in the 
previous section, in domains where the resources are not ob(cid:173)
servable, or if the policy size is of vital importance (for exam(cid:173)
ple if the agent's architecture imposes constraints on policies 
that it can store), this approach could prove fruitful. 

However,  the  biggest  problem  with  this  approach  (as 
pointed out, for example, by Ross and Chen in the telecom-

RESOURCE-BOUNDED REASONING 

munication domain [1988]) is that a standard CMDP imposes 
constraints  on  the  expected  amounts.  Clearly,  this  method 
does not work for critical resources, whose overutilization has 
negative consequences.  Indeed,  an agent that pilots aircraft 
would not be satisfied with a policy that on average uses an 
amount of fuel that does not exceed the tank capacity. 

3.3  Sample  Path  MDPs 
As just  mentioned,  Ross  and  Chen  pointed  out  the  weak(cid:173)
ness of the CMDP approach with constraints on the expected 
amounts. As a possible solution, Ross and Varadarajan [1989; 
1991]  propose an  approach where constraints are placed on 
the actual sample-path costs. In their work, the space of feasi(cid:173)
ble solutions is constrained to the set of policies whose proba(cid:173)
bility of violating the constraints (overutilizing the resources) 
in the long run is zero.  However, their work concentrates on 
the average per-step costs and rewards, whereas we are inter(cid:173)
ested in the total amounts, whose distributions are not easily 
calculable.  The  approach  of Ross  and  Varadarajan  has  the 
same benefits as the standard CMDP method, i.e.  no explicit 
modeling  of resources  is  required,  and  the  state  space  and 
policies  are  small.  In  addition,  unlike  the  standard CMDP, 
this method  is  suitable  for problems  with  critical  resources. 
However, a weakness of this approach is that for some prob(cid:173)
lems it might be too restrictive, in that it allows no possibility 
of overutilizing the resources.  Indeed,  policies produced by 
the sample path  method might have  significantly lower pay(cid:173)
off,  as  compared  to  policies  that  have  some  probability  of 
resource  overutilization.  Furthermore,  for some  domains  it 
might be desirable for the system designer to be able to con(cid:173)
trol the probability of resource overutilization, as a means of 
balancing optimality and risk. 
3.4  MDPs With Constraints on Variance 
Another approach  to  handling  deviations  from  the  expecta(cid:173)
tion in Markov models is to impose additional constraints on 
(or to  assign  additional  cost to)  the  variance.  Sobel  [1985] 
proposed to constrain the expected cost and to maximize the 
mean-variance  ratio  of the  reward.  Huang  and  Kallenberg 
[1994] proposed a unified approach to handling variance via 
an algorithm based on parametric linear programming. These 
approaches have the same benefits as the standard CMDP and 
the sample-path methods (compared to the FOMDP formula(cid:173)
tion)  in terms  of state  space  and policy  size,  as  well  as the 
complexity  of constructing  the  initial  world  model.  Addi(cid:173)
tionally,  they allow one to somewhat balance payoff and the 
deviation from the expected.  However, these methods do not 
allow  one  to  make  hard  guarantees  about  the  probability  of 
overutilizing the resources. 

4  Linear Approximation 
As hinted at in the previous sections, we would like to be able 
to constrain the  feasible  solution  space to the set of policies 
whose  probability  of overutilizing  the  resources  is  below  a 
user-specified threshold 
In  other  words,  we 
would like to be able to solve the following math program: 

(4) 

1109 

where 
policy, and 

is the total amount of resource 

that is used by the 
is the upper bound on that resource (as before). 

The trouble is that the optimization is in the space  of 

which can be interpreted as the expected number of times for 
executing the actions  in the corresponding states.  However, 
as a simple function of 
it  is  difficult to express 
the optimization variables 
because the latter contain no 
information  about  the  probability  distribution  of the  random 
variable of the number of times the action is actually executed 
in  the  corresponding  state  -  only  the  expected  values.3 
In 
this section,  we present a linear approximation to the above 
program, based on the Markov inequality:4 

(5) 
Using this inequality and the fact that the expected resource 
usage can be expressed as 
the opti(cid:173)
mization problem can be formulated as a linear program: 

A potential  weakness  of this  approach  is  that  the Markov 
inequality gives a very rough  upper bound,  and  the policies 
that correspond to solutions to this  LP can be too conserva(cid:173)
tive, in that their real probability of overutilizing the resources 
can be significantly lower than 
However,  if making  hard 
guarantees about the probability of overutilizing the resources 
is of vital importance, this method might prove valuable, as it 
yields policies that, in general, would have higher payoff than 
the ones obtained by the sample-path method (as the latter is 
often too restrictive).  On the other hand, unlike the standard 
CMDPs that impose constraints on the expected resource us(cid:173)
age,  and the MDPs that constrain the variance,  this method 
allows one to explicitly bound the allowable probability of re(cid:173)
source overutilization, and to make precise guarantees about 
the behavior of the system in that respect. 

It is also worth noting that, unlike the sample-path method 
or  the  methods  that  constrain  on  variance,  this  method  re(cid:173)
lies  on  solving  a  standard  linear program,  whereas  the  for(cid:173)
mer require  solving quadratic or parametric  linear programs. 
Therefore, the above formulation appears to be a reasonable 
approximation,  because  it should be no harder to solve than 
the standard CMDP (see section 5  for experimental results), 
while  providing  a  means  of balancing  solution  quality  with 
the precisely quantifiable risk of resource overutilization. 

5  Evaluation 
To verify our hypotheses about the properties of the approxi(cid:173)
mation described in the previous section, we have performed 
a  set  of numerical  experiments  that  compare  its  behavior to 
a standard CMDP with constraints on the expected resource 
amounts (section 3), and to an unconstrained MDP. 

3Generally speaking, the total cost is a sum of a random num(cid:173)
ber of differently-distributed random variables, and calculating its 
probability distribution is a nontrivial task. 

4This  inequality only holds  for nonnegative random variables. 
However, for a transient system, we can make the assumption that 
all costs are nonnegative, without any loss of generality. 

To reduce the bias that might arise from using a small num(cid:173)
ber of hand-crafted test cases,  we have  instead used a large 
number of randomly-generated constrained MDPs. All of the 
generated problems shared some common properties, among 
which the most interesting ones are the following (the values 
for our main experiments are given in parentheses): 

The  total  number  of  states,  actions,  and  re(cid:173)

sources, respectively. (20, 20, 2) 

Maximum  reward.  Rewards  are  assigned 

from a uniform distribution on 

(10) 

Mc  =  max(C)  Maximum  action  cost.  Resource  costs  are 
based on a 

assigned  to  state-action  pairs  from 
distribution  of R and 

(C, R)  (described below).  (10) 

Correlation  between  rewards  and  resource 

costs; better actions are typically more costly. 
Upper  bounds  on 

resource 
amounts  are  assigned  according  to  a  uniform  distribu(cid:173)
tion from this range.  ([200, 300]) 
Dissipation of probability - the probability that the agent 
exits the system at each time step. ([0.95,0.99]) 

The last parameter was used to ensure a transient chain.  In(cid:173)
stead  of providing  a  small  number  of sink  states,  we  have 
chosen  to  use  a  uniform  dissipation  of probability,  in  order 
to avoid additional randomization in our experiments, as the 
latter choice provides a more stable domain. 

Our  main  concern  was  the  behavior  of  our  approxima(cid:173)
tion, as a function of the probability threshold 
Therefore, 
we  have  run  a  number of experiments  for  various  values  of 
To be more precise, we have gradually increased 
from 0 to 1 in increments of 0.05, and for each value, gen(cid:173)
erated  50  random  models  and  solved  them  using  the  three 
methods:  1)  an  unconstrained MDP  without  any  regard  for 
resource limitations, 2) a standard CMDP with constraints on 
the expected usage of resources, and 3) our CMDP with con(cid:173)
straints on the probability  of overutilizing the resources  (eq. 
6). We then evaluated (using a Monte-Carlo simulation) each 
of the  three  solutions  (policies)  in  terms  of expected  reward 
and probability of overutilizing the resources. 

Figure  1  shows a plot of the actual probability of overuti(cid:173)
lizing  the  resources  for  the  policies  obtained  via  the  three 
methods  as  a  function  of the  probability threshold 
.  The 
data points are averaged over the runs  for a particular value 
of po. The curve that corresponds to the method that bounds 
the overutilization probability also shows the standard devia(cid:173)
tion  for the runs.  The other data have very similar variance, 
so we will use the plots of means (averaged over the runs for 
a given po) for our analyses. 

Obviously, po  has no effect on  the unconstrained and the 
standard  CMDP  (which  maintain  more  or  less  a  constant 
probability of overutilization), but it does affect to a large ex(cid:173)
tent the solutions to the problem with constraints on overuti(cid:173)
lization probability. One can see that the overutilization prob(cid:173)
ability for the  solutions produced by our approach  is always 
(as it should be).  Also, it is worth noting that when 
below 
approaches 1, our approximation yields the same results as 
should 

the other methods, which is good, since setting 
not constrain the space of feasible solutions. 

1110 

RESOURCE-BOUNDED REASONING 

Figure  1:  Probability  of resource  overutilization. 

Figure  3:  Average  rewards  with penalties  for overutilization. 

Moreover,  for  large  penalties  (IV  =  - 2 2 0 ),  the  conservative 
policy  outperforms  the  others  for  all  values  o f;  
.  Note  that 
here,  the  policy  is  re-evaluated post-factum.  Section  6  briefly 
discusses  an  approach  that  explicitly  models  the  penalties  in 
the  optimization  program. 

As  we  mentioned  in  section  4,  the  linear  approximation 
should  be  no  harder  to  solve  than  the  standard  constrained 
MDP,  because  both  are  formulated  as  linear  programs  with 
the  same  number  of  constraints.  To  experimentally  verify 
this,  we  have  timed  the  runs  of all  our  experiments.  Figure 
4  contains  a  plot  of the  times  that  it  took  to  solve  the  prob(cid:173)
lems  in  all  our  experiments.  One  can  see  that  the  running 
times  for  all  three  methods  are  not  appreciably  different.5  In 
particular,  the  average  ratio  of the  running  time  for  the  stan(cid:173)
dard  C M DP  to  the  running  time  of the  unconstrained  method 
is  1.25;  the  ratio  of the  running  time  of our  linear  approxi(cid:173)
mation  with  constraints  on  overutilization  probability  to  the 
running  time  of the  unconstrained  method  is  1.06.  The  slight 
downward  curvature  of  the  plot  of  the  running  time  of  our 
approximation  method  appears  to  be  a  consequence  of  the 
specific  implementation  of the  linear  programming  algorithm 
that  we  used  in  our  experiments. 

6  Discussion and Future Work 
Our  experiments  substantiate  the  claims  that  our  approxima(cid:173)
tion  provides  an  effective  and  efficient  method  that  agents 
can  use  to  formulate  policies  that  not  only  consider  limita(cid:173)
tions  on  execution  resources,  but  that  also  explicitly  bound 
the  probability  of resource  overutilization.  Our  new  approxi(cid:173)
mation  achieves  the  constraints  on  overutilization,  and  is  es(cid:173)
sentially no more expensive to use than  more standard CMDP 
and  unconstrained  M DP  methods.  Because  our  method  con(cid:173)
structs  policies  that  are  more  careful  about  avoiding  resource 
overutilization,  the  rewards  associated  with  its  policies  when 
resources  are  not  overutilized  tend  to  be  less  than  the  re(cid:173)
wards  for the  other methods'  policies  when  resources  are  not 
overutilized.  However,  as  we  illustrated  in  Figure  3,  when 
overutilization  incurs  penalties  our  new  method  can  outper-

5Here we have to note that we used the linear programming ap(cid:173)
proach to solving the unconstrained problem.  A different algorithm 
such as value or policy iteration might be more efficient. 

Figure 2:  Average rewards for solutions to the three problems. 

The  rewards  obtained  by  these  policies  are  shown  in  Fig(cid:173)
ure  2.  These  actual  rewards  do  not  necessarily  equal  the  ex(cid:173)
pected  rewards  (which  are  used  during  the  optimization  pro(cid:173)
cess).  This  is  because  only  the  runs  that  did  not  overutilize 
the  resources  were  included  in  the average,  and policies  were 
not penalized for violating  the  resource  constraints.  This  also 
explains  why  the  total  rewards  received  by  solutions  to  the 
standard  CMDPs  were  sometimes  greater  than  the  ones  ob(cid:173)
tained  by  solutions  to  the  unconstrained  problems. 

However,  realistically,  an agent always  incurs a penalty  for 
overutilizing  a  critical  resource,  where  the  penalty  amount  is 
based  on  the  consequences  of overutilization.  For  example, 
if the  agent  is  flying  a  plane,  and  the  resource  in  question  is 
fuel,  the  consequences  of trying  to  use  too  much  of that  re(cid:173)
source  are  catastrophic,  so  the  penalty  is  very  high. 
If we 
take  this  into  account  by  assigning  a  fixed  penalty  to  poli(cid:173)
cies that overutilize the  resources, we can update the graph  in 
Figure  2  to  get  a  more  realistic  picture.  Figure  3  shows  the 
average  rewards  for  solutions  obtained  via  the  three  meth(cid:173)
ods,  recalculated  to  reflect  the  penalties:  new  rewards  are 
is  the  overutilization 
probability,  R  is  the  average  reward  for successful  runs  of the 
given policy (as in  Figure 2),  and  W  is the penalty for overuti(cid:173)
lization.  We see that, if we take the penalty into account, there 
is  an  interval  of po  where  the  conservative  policy  produced 
by  our  linear  approximation  outperforms  the  other  policies. 

where 

RESOURCE-BOUNDED  REASONING 

1111 

the optimization variables.  This approximation and  the one 
based on the Chebyshev  inequality are more costly  to com(cid:173)
pute than the linear one. Our current efforts are to encode the 
approximations and evaluate their strengths and weaknesses. 

7  Acknowledgments 
This paper is based upon work supported by DARPA/ITO and 
the  Air  Force  Research  Laboratory  under  contract  F30602-
00-C-0017 as a subcontractor through Honeywell Laborato(cid:173)
ries.  The authors  thank  Kang  Shin,  Haksun Li,  and  David 
Musliner for their  valuable  contributions,  as  well  as  one of 
the anonymous reviewers whose comments inspired (eq. 7). 

References 
[Altman,  1999]  Eitan  Altman.  Constrained  Markov  Deci(cid:173)

sion Processes. Chapman and HALL/CRC, 1999. 

[Boutilier et al,  1999]  Craig  Boutilier,  Thomas  Dean,  and 
Steve Hanks.  Decision-theoretic planning:  Structural  as(cid:173)
sumptions  and  computational  leverage.  Journal  of Artifi(cid:173)
cial Intelligence Research,  11:1-94,  1999. 

lDeane/a/.,  1993]  Thomas  Dean,  Leslie  Pack  Kaelbling, 
Jak Kirman, and Ann Nicholson.  Planning with deadlines 
In  Proc.  of the Eleventh  National 
in  stochastic  domains. 
Confi  on Artificial Intelligence, pages 574-579, CA,  1993. 
[Howard,  1960]  R.  Howard.  Dynamic  Programming  and 

Markov Processes.  MIT Press, Cambridge, 1960. 

[Huang and Kallenberg, 1994]  Y. Huang and L.C.M. Kallen-
berg.  On  finding  optimal  policies  for  Markov  decision 
chains. Math, of Operations Research, 19:434-448, 1994. 
Linear  Program(cid:173)
ming  and Finite Markovian  Control  Problems.  Mathcma-
tisch Centrum, Amsterdam, 1983. 

iKallenberg,  1983]  L.C.M.  Kallenberg. 

[Musliner etal,  1995]  David John Musliner, James Hendler, 
Ashok  K.  Agrawala,  Edmund  H.  Durfee,  Jay  K.  Stros-
nider, and C. J. Paul. The challenges of real-time AI. IEEE 
Computer, 28(1), 1995. 

[Puterman,  1995]  M.  L.  Puterman.  Markov  decision  pro(cid:173)

cesses.  New York,  1995. John Wiley & Sons. 

[Ross and Chen,  1988]  K.W.  Ross  and  B.  Chen.  Optimal 
scheduling  of  interactive  and  non-interactive  traffic  in 
telecommunication  systems.  IEEE  Transactions on Auto 
Control, 33:261-267,  1988. 

[Ross and Varadarajan,  1989]  K.W.  Ross  and  R.  Varadara-
jan.  Markov  decision  processes  with  sample  path  con(cid:173)
straints: the communicating case.  OR, 37:780-790, 1989. 
[Ross and Varadarajan,  1991]  K.W.  Ross  and  R.  Varadara(cid:173)
jan.  Multichain Markov decision  processes  with  a sam(cid:173)
ple path constraint:  A decomposition approach.  Math,  of 
Operations Research, 16:195-207,1991. 

[Russell and Subramanian, 1995]  S. J. Russell and D. Subra-
manian.  Provably bounded optimal agents.  JAIR, 2:575-
609, 1995. 

[Sobel, 1985]  M.J. Sobel. Maximal mean/standard deviation 
ratio in undiscounted mdp.  OR Letters, 4:157-188, 1985. 

Figure 4:  Running time for the three methods 

form previous techniques.  Thus, our new method is particu(cid:173)
larly suited to agents engaged in mission-critical domains. 

Furthermore,  if penalties  for  resource  overutilization  are 
known at design-time and can be expressed in the same units 
as the rewards, an interesting modification of our LP formu(cid:173)
lation is to include the penalties in the policy evaluation cri(cid:173)
terion, as opposed to modeling the constraints on the overuti(cid:173)
lization probability.  This would yield an LP with the follow(cid:173)
ing objective function: 

(7) 

where Wk  is the penalty (in units of ria)  incurred for overuti-
lizing  resource  k.  The  maximization  is  subject  to just  the 
standard "conservation  of probability" constraints  as  in  (eq. 
3).  A  benefit  of this  formulation  is  that,  for  certain  initial 
probability  distributions,  deterministic  policies  are  optimal. 
A downside is that the formulation does not allow one to ex(cid:173)
plicitly control the acceptable overutilization probabilities. 

As can be seen in Figure  1, our linear approximation is in 
general overly conservative.  For example,  given permission 
to  overutilize  the resource  20%  of the  time  (p0  =  0.2),  the 
method generates a policy that overutilizes the resource only 
about  1% of the time.  Since rewards and resource usage are 
typically correlated, we would expect that a policy that under(cid:173)
shoots the permitted resource overutilization probability by a 
lower amount would also yield a higher expected reward. To(cid:173)
ward this end, we have formulated a quadratic programming 
approximation, based on the Chebyshev inequality, which al(cid:173)
lows us to put a better upper bound on the probability of re(cid:173)
source overutilization: 

(8) 

where 
amount of used resource 
allowable regions for the amounts of used resources. 

is the variance in the 
specify the widths of the 

and 

We are also investigating another formulation of the opti(cid:173)
mization program that should give a more accurate estimate 
of  the  resource  overutilization  probability.  The  method  is 
based  on  a  polynomial  approximation  of the  pdf of the  to(cid:173)
tal resource-usage cost and uses the moments of the cost as 

1112 

RESOURCE-BOUNDED REASONING 


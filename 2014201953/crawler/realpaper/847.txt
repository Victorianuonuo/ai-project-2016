Learning Classiﬁers When The Training Data Is Not IID

Murat Dundar, Balaji Krishnapuram, Jinbo Bi, R. Bharat Rao

Computer Aided Diagnosis & Therapy Group, Siemens Medical Solutions,

{murat.dundar, balaji.krishnapuram, jinbo.bi, bharat.rao}@siemens.com

51 Valley Stream Parkway, Malvern, PA-19355, USA

Abstract

Most methods for classiﬁer design assume that the
training samples are drawn independently and iden-
tically from an unknown data generating distribu-
tion, although this assumption is violated in several
real life problems. Relaxing this i.i.d. assumption,
we consider algorithms from the statistics litera-
ture for the more realistic situation where batches
or sub-groups of training samples may have inter-
nal correlations, although the samples from dif-
ferent batches may be considered to be uncorre-
lated. Next, we propose simpler (more efﬁcient)
variants that scale well to large datasets; theoretical
results from the literature are provided to support
their validity. Experimental results from real-life
computer aided diagnosis (CAD) problems indi-
cate that relaxing the i.i.d. assumption leads to sta-
tistically signiﬁcant improvements in the accuracy
of the learned classiﬁer. Surprisingly, the simpler
algorithm proposed here is experimentally found to
be even more accurate than the original version.

1 Introduction
Most classiﬁer-learning algorithms assume that the training
data is independently and identically distributed. For ex-
ample, support vector machine (SVM), back-propagation for
Neural Networks, and many other common algorithms im-
plicitly make this assumption as part of their derivation. Nev-
ertheless, this assumption is commonly violated in many real-
life problems where sub-groups of samples exhibit a high de-
gree of correlation amongst both features and labels.

In this paper we: (a) experimentally demonstrate that ac-
counting for the correlations in real-world training data leads
to statistically signiﬁcant improvements in accuracy; (b) pro-
pose simpler algorithms that are computationally faster than
previous statistical methods and (c) provide links to theoreti-
cal analysis to establish the validity of our algorithm.

1.1 Motivating example: CAD
Although overlooked because of the dominance of algorithms
that learn from i.i.d. data, sample correlations are ubiqui-
tous in the real world. The machine learning community fre-
quently ignores the non-i.i.d. nature of data, simply because

we do not appreciate the beneﬁts of modeling these corre-
lations, and the ease with which this can be accomplished
algorithmically. For motivation, consider computer aided di-
agnosis (CAD) applications where the goal is to detect struc-
tures of interest to physicians in medical images: e.g. to iden-
tify potentially malignant tumors in CT scans, X-ray images,
etc.
In an almost universal paradigm for CAD algorithms,
this problem is addressed by a 3 stage system: identiﬁcation
of potentially unhealthy candidate regions of interest (ROI)
from a medical image, computation of descriptive features
for each candidate, and classiﬁcation of each candidate (e.g.
normal or diseased) based on its features. Often many candi-
date ROI point to the same underlying anatomical structure at
slightly different spatial locations.

Under this paradigm, correlations clearly exist among both
the features and the labels of candidates that refer to the same
underlying structure, image, patient, imaging system, doc-
tor/nurse, hospital etc. Clearly, the candidate ROI acquired
from a set of patient images cannot be assumed to be IID.
Multiple levels of hierarchical correlations are commonly ob-
served in most real world datasets (see Figure 1).

1.2 Relationship to Previous Work
Largely ignored in the machine learning and data mining lit-
erature, the statistics and epidemiology communities have de-
veloped a rich literature to account for the effect of correlated
samples. Perhaps the most well known and relevant mod-
els for our purposes are the random effects model (REM) [3],
and the generalized linear mixed effects models (GLMM) [7].
These models have been mainly studied from the point of
view of explanatory data analysis (e.g. what is the effect of
smoking on the risk of lung cancer?) not from the point of
view of predictive modeling, i.e. accurate classiﬁcation of un-
seen test samples. Further, these algorithms tend to be com-
putationally impractical for large scale datasets that are com-
monly encountered in commercial data mining applications.
In this paper, we propose a simple modiﬁcation of existing al-
gorithms that is computationally cheap, yet our experiments
indicate that our approach is as effective as GLMMs in terms
of improving classiﬁcation accuracy.

2 Intuition: Impact of Sample Correlations
Simpliﬁed thought experiment Consider the estimation of
the odds of heads for a biased coin, based on a set of ob-

IJCAI-07

756

Figure 1: A Mixed effects model showing random and ﬁxed effects

servations. If every observation is an independent ﬂip of a
coin, then this corresponds to an i.i.d. assumption for the data;
based on this assumption one can easily build estimators for
our binomial data. This would work ﬁne so long as the obser-
vations reported to the statistician are really true to the under-
lying coin ﬂips, and provided the underlying data generating
mechanism was truly binomial (and not from some other dis-
tribution that generates “outliers” as per the binomial model).
However, suppose that the experimenter performing the ex-
periment reports something else to the statistician. After cer-
tain coin ﬂips, the experimenter reports the result of one coin
ﬂip observation as if it occurred many times (but only if he
observed “heads” on those occasions). For other observations
he reports the occurrences exactly once. Then the simple bi-
nomial estimator (designed using the i.i.d. assumption) would
not be appropriate. On the other hand, if every occurrence of
an i.i.d. sample is repeated the same number of times, then
we are essentially immune to this effect. Although this exam-
ple may seem simplistic, one should bear in mind that logistic
regression, Gaussian Processes and most other classiﬁers es-
sentially rely on the same binomial distribution to derive the
likelihood that they maximize in the training step.

Implications for Classiﬁer Learning The implicit as-
sumption in the machine learning community seems to be that
even if IID assumptions are violated, the algorithms would
work well in practice. When would this not be the case?

The ﬁrst intuition is that outliers (e.g. mis-labeled sam-
ples), do not systematically bias the estimation of the clas-
siﬁer during training, provided they are truly i.i.d. and drawn
from a fairly symmetric distribution. These outliers introduce
a larger variance in the estimation process, but this can be
largely overcome simply by increasing the sample sizes of the
training set. On the other hand, if outliers (and samples) are
systematically correlated, they do introduce a systemic bias
in the estimation process, and their effect remains even if we
have a large amount of training data.

For explaining the second intuition, let us ﬁrst consider a
practical situation occurring in CAD problems. Due to the
way the candidate generation (CG) algorithms for identify-
ing ROI are designed, some diseased structures (e.g. wall at-
tached nodules in a lung) may be identiﬁed by many candi-

dates that are spatially close in the image; i.e., all these can-
didates will refer to the same underlying physiological region
(e.g. the same lung nodule). Moreover, some other types of
structures (e.g. non-wall attached nodules) may be associated
with only one or two candidates, again due to the fundamen-
tal properties and biases of the candidate generation (CG) al-
gorithm. The occurrence frequency & other statistical prop-
erties of the underlying structural causes of the disease are
systematically altered if we naively treated all data as being
produced from i.i.d. data sources. As a result, a systematic
bias is introduced into the statistical classiﬁer estimation al-
gorithm that learns to diagnose diseases, and this bias remains
even when we have large amounts of training data.

When does the violation of i.i.d. assumptions not matter?
Clearly, if the correlations between samples is very weak,
we can effectively ignore them, treating the data as i.i.d. .
Further, even if the correlations are not weak, if we do not
much care for outlier immunity, and if each sub-type or sub-
population occurs with similar or almost identical frequency,
then we should be able to ignore these effects.

candidate in the jth

3 Random & Mixed Effects Models
Consider the problem shown in Figure 1. We are given a
training dataset D = {(xijk, yijk)}, where xijk ∈ (cid:3)d
is the
feature vector for the ith
patient of the
kth
hospital and yijk ∈ {−1, 1} are class labels. Let us also
assume without loss of generality that the indexing variable
follow i = {1, . . . , (cid:2)j}, j = {1, . . . , pk}, k = {1, . . . , K}.
In generalized linear models (GLM) [7] such as logistic or
probit regression, one uses a nonlinear link function—e.g. the
logistic sigmoid function σ(r) = 1/(1 + exp(−r)))—to ex-
press the posterior probability of class membership as:

P (yijk = 1|xijk, α) = σ(α(cid:2)xijk + α0).

(1)

However, this mathematically expresses conditional inde-
pendence between the classiﬁcation of samples and ignores
the correlations between samples from the same patient (j, k)
or from the same hospital k. This limitation is overcome in a
generalized linear mixed effects model (GLMM) [7] by postu-
lating the existence of a pair of random variables that explain
the patient speciﬁc effect δj,k and a hospital speciﬁc effect

IJCAI-07

757

(cid:2)

(cid:2)

δk. During training, one not only estimates the ﬁxed effect
parameters of the classiﬁer α, but also the distribution of the
random effects p(δk, δj,k|D). The class prediction becomes:

P (yijk = 1|xijk, α) =

σ(α(cid:2)xijk + α0 + δk + δj,k)p(δk, δj,k)dδkdδj,k.

(2)

In Bayesian terms, classiﬁcation of a new test sample x
with a previously trained GLM involves marginalization over
the posterior of the classiﬁer’s ﬁxed effect parameters:

(cid:2)

E[P (y = 1|x)] =

σ(α(cid:2)x + α0)p(α, α0|D)dαdα0.

(3)

The extension to ﬁnd the classiﬁcation decision on a test sam-
ple in a GLMM is quite straight-forward:

E[P (yijk = 1|xijk)] =

σ(α(cid:2)xijk + α0 + δk + δj,k)p(α, α0, δk, δj,k|D)

dαdα0dδkdδj,k.

(4)

Notice that the classiﬁcation predictions of sets of samples
from the same patient (j, k) or from the same hospital k are
no longer independent. During the testing phase, if one wants
to classify samples from a new patient or a new hospital not
seen during training (so the appropriate random effects are
not available), one may still use the GLM approach, but rely
on a marginalized version of the posterior distributions learnt
for the GLMM:

(cid:2)

E[P (y = 1|x)] =

σ(α(cid:2)x + α0)p(α, α0, δk, δj,k|D)

dαdα0dδkdδj,k.

(5)

A note of explanation may be useful to explain the above
equation. The integral in (5) is over δk and δj,k, for all j
and k, i.e. we are marginalizing over all the random effects in
order to obtain p(α, α0|D) and then relying on (2). Clearly,
training a GLMM amounts to the estimation of the posterior
distribution, p(α, α0, δk, δj,k|D).

3.1 Avoiding Bayesian integrals

Since the exact posterior can not be determined in analytic
form, the calculation of the above integral for classifying ev-
ery test sample can prove computationally difﬁcult. In prac-
tice, we can use Markov chain monte carlo (MCMC) meth-
ods but they tend to be too slow for data mining applica-
tions. We can also use approximate strategies for computing
Bayesian posteriors such as the Laplace approximation, vari-
ational methods or expectation propagation (EP). However,
as we see using the following lemma (adapted from a differ-
ent context [5]), if this posterior is approximated by a sym-
metric distribution(e.g. Gaussian) and we are only interested
in the strict classiﬁcation decision rather than the posterior
class membership probability, then a remarkable result holds:
The GLMM classiﬁcation involving a Bayesian integration
can be replaced by a point classiﬁer.

First we deﬁne some convenient notation (which will use
bold font to distinguish it from the rest of the paper). Let us
denote the vector combining α, α0 and all the random effect
variables as

w = [α(cid:2), α0, δ1, δ2, . . . , δK, δ11, . . . , δpK ,K](cid:2).

Let us deﬁne the augmented vector combining the fea-
ture vector x, the unit scalar 1 and a vector 0 of zeros—
whose length corresponds to the number of random effect
variables—as x = [x(cid:2), 1, 0(cid:2)](cid:2). Next, observe that if one
only requires a hard classiﬁcation (as in the SVM literature)
the sign(•) function is to be used for the link. Finally, note
that the classiﬁcation decision in (5) can be expressed as:
E[P (y = 1|x)] =
Lemma 1 For any sample x, the hard classiﬁcation decision
x) is identical to

of a Point Classiﬁer fPC(x, (cid:4)w) = sign((cid:4)w

x)p(w|D)dw.

sign(w

(cid:3)

(cid:2)

(cid:2)

that of a Bayesian Voting Classiﬁcation

(cid:5)(cid:2)

(cid:6)

(cid:2)

(6)

sign(w

x)q(w)dw

fBVC(x, q) = sign

if q(w) = q(w|(cid:4)w) is a symmetric distribution with respect to
(cid:4)w, that is, if q(w|(cid:4)w) = q((cid:7)w|(cid:4)w), where (cid:7)w ≡ 2 (cid:4)w − w is the
symmetric reﬂection of w about (cid:4)w.
Proof: For every w in the domain of the integral, (cid:7)w is also
in the domain of the integral. Since (cid:7)w = 2 (cid:4)w − w, we see
x +(cid:7)w
(cid:7)w
x = 0, the total contribution from w and (cid:7)w to the

x. Three cases have to be considered:
x =

that w
Case 1: When sign(w

x = 2(cid:4)w

x), or w

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

x), since w
x) = sign(w

(cid:2)

(cid:2)

x =
x) =

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

x) = − sign((cid:7)w
x)q(w|(cid:4)w) + sign((cid:7)w
x) = sign((cid:7)w
it follows that sign((cid:4)w
(cid:8)
x)q(w|(cid:4)w) + sign((cid:7)w
x = 0, (cid:7)w
x = 2(cid:4)w
x)q(w|(cid:4)w) + sign((cid:7)w

x + (cid:7)w

x (cid:5)= 0 or w

x). Thus,

sign(w

sign(w

(cid:8)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

x)q((cid:7)w|(cid:4)w) = 0.
x+(cid:7)w
(cid:9)
x)q((cid:7)w|(cid:4)w)
= sign((cid:4)w
x (cid:5)= 0, (cid:7)w
(cid:9)
x)q((cid:7)w|(cid:4)w)
= sign((cid:4)w

x that

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

x).

x = 0,

integral is 0, since

sign(w

Case 2: When sign(w

2(cid:4)w
sign((cid:7)w

x,

(cid:2)

(cid:2)

sign

sign

Unless (cid:4)w = 0 (in which case the classiﬁer in undeﬁned),

x).

(cid:2)

case 2 or 3 will occur at least some times. Hence proved. (cid:2)
Assumptions & Limitations of this approach: This Lemma
suggests that one may simply use point classiﬁers and avoid
Bayesian integration for linear hyper-plane classiﬁers. How-
ever, this is only true if our objective is only to obtain the
classiﬁcation: if we used some other link function to obtain a
soft-classiﬁcation then the above lemma can not be extended
for our purposes exactly (it may still approximate the result).
Secondly, the lemma only holds for symmetric posterior
distributions p(α, α0, δk, δj,k|D). However, in practice, most
approximate Bayesian methods would also use a Gaussian

Case 3: When w

(cid:2)

we have again from w

(cid:2)

IJCAI-07

758

approximation in any case, so it is not a huge limitation. Fi-
nally, although any symmetric distribution may be approxi-
mated by a point classiﬁer at its mean, the estimation of the
mean is still required. However, for computing a Laplace ap-
proximation to the posterior, one simply chooses to approxi-
mate the mean by the mode of the posterior distribution: the
mode can be obtained simply by maximizing the posterior
during maximum a posteriori (MAP) estimation. This is com-
putationally expedient & works reasonably well in practice.

4 Proposed Algorithm

4.1 Augmenting Feature Vectors (AFV)
In the previous section we showed that full Bayesian inte-
gration is not required and a suitably chosen point classiﬁer
would achieve an identical classiﬁcation decision. In this sec-
tion, we make practical recommendations about how to ob-
tain these equivalent point classiﬁers with very little effort,
using already existing algorithms in an original way.

We propose the following strategy for building linear clas-
siﬁers in a way that uses the known (or postulated) correla-
tions between the samples. First, for all the training samples,
construct augmented feature vectors with additional features
corresponding to the indicator functions for the patient & hos-
pital identity. In other words, to the original feature vector of
a sample x, append a vector that is composed of zero in all
locations except the location of the patient-id, and another
corresponding to the location of the hospital-id, and augment
the feature vector with the auxiliary features by, ¯x = [x(cid:2) ˜x(cid:2)](cid:2).
Next use this augmented feature vector to train a classi-
ﬁer using any standard training algorithm such as Fisher’s
Discriminant or SVMs. This classiﬁer will classify samples
based not only on their features, but also based on the ID of
the hospital and the patient. Viewed differently, the classiﬁer
in this augmented feature space not only attempts to explain
how the original features predict the class membership, but
it also simultaneously assigns a patient and hospital speciﬁc
“random-effect” explanation to de-correlate the training data.
During the test phase, for new patients/hospitals, the ran-
dom effects may simply be ignored (implicitly this is what we
used in the lemma in the previous section). In this paper we
implement the proposed approach for Fisher’s Discriminant.

4.2 Fisher’s Discriminant
In this section we adopt the convex programming formulation
of Fisher’s Discriminant(FD) presented in [8],

min
α, α0,ξ

s.t.

L(ξ|cξ) + L(α|cα)
ξi + yi = α(cid:2) xi + α0

e(cid:2)ξC = 0, C ∈ {±}

(7)

where L(z) ≡ − log p(z) be the negative log likelihood
associated with the probability density p. The constraint
ξi + yi = α(cid:2) xi + α0 ∀i, i = (1, 2, . . . , (cid:2)) where (cid:2) is the total
number of labeled samples, pulls the output for each sample
to its class label while the constraints e(cid:2)ξC = 0, C ∈ {±}
ensure that the average output for each class is the label, i.e.
without loss of generality the between class scattering is ﬁxed
to be two. The ﬁrst term in the objective function minimizes

the within class scattering whereas the second term penalizes
models α that are a priori unlikely.

, L(α|cα) = (cid:7)α(cid:7)2

Setting L(ξ|cξ) = (cid:7)ξ(cid:7)2

, i.e. assuming
a zero mean Gaussian density model with unit variance for
p(ξ|cξ) and p(α|cξ) and using the augmented feature vectors
we obtain the following optimization problem. Fisher’s Dis-
criminant with Augmented Feature Vectors (FD-AFV):

min

α, α0,ξ,δ,γ

s.t.

¯cξ+ (cid:7)ξ+(cid:7)2

+ ¯cξ− (cid:7)ξ−(cid:7)2

+ ¯cα (cid:7)α(cid:7)2

+ ¯cδ (cid:7)δ(cid:7)2

ξijk + yijk = α(cid:2)xijk + δ(cid:2) ˜xijk + α0

e(cid:2)ξC = 0, C ∈ {±}

where for the ﬁrst set of constraints, k runs from 1 to K, j
from 1 to pk for each k, i from 1 to (cid:2)jk for each pair of (j, k),
ξC
is a vector of ξijk corresponding to class C and δ is the
vector of model parameters for the auxiliary features.

4.3 Parameter Estimation
Depending on the sensitivity of the candidate generation
mechanism, a representative training dataset might have on
the order of few thousand candidates of which only few are
positive making the training data very large and unbalanced
between classes. To account for the unbalanced nature of the
data we used different tuning parameters, ¯cξ+ , ¯cξ− for the
positive and negative classes. To estimate these and ¯cα, ¯cδ,
we ﬁrst coarsely tune each parameter independently and de-
termine a range of values for that parameter. Then for each
parameter we consider a discrete set of three values. We use
10-fold cross validation on the training set as a performance
measure to ﬁnd the optimum set of parameters.

5 Experimental Studies
For the experiments in this section, we compare three tech-
niques: naive Fisher’s Discriminant (FD), FD-AFV, and
GLMM (implemented using approximate expectation propa-
gation inference). We compare FD-AFV and GLMM against
FD to see if these algorithms yield statistically signiﬁcant im-
provements in the accuracy of the classiﬁer. We also study if
the computationally much less expensive FD-AFV is compa-
rable to GLMM in terms of classiﬁer sensitivity.

For most CAD problems, it is important to keep the num-
ber of false positives per volume at a reasonable level, since
each candidate that is marked as positive by the classiﬁer will
then be visually inspected by a physician. For the projects
described below, we focus our attention on the region of the
Receiver Operating Characteristics (ROC) curve with fewer
than 5 false positives per volume. This roughly corresponds
to 90% speciﬁcity for both datasets.

5.1 Experiment 1: Colon Cancer
Problem Description: Colorectal cancer is the third most
common cancer in both men and women. It is estimated that
in 2004, nearly 147,000 cases of colon and rectal cancer will
be diagnosed in the US, and more than 56,730 people would
die from colon cancer [4]. While there is wide consensus that
screening patients is effective in decreasing advanced disease,
only 44% of the eligible population undergoes any colorectal
cancer screening. There are many factors for this, key being:
patient comfort, bowel preparation and cost.

IJCAI-07

759

Non-invasive virtual colonoscopy derived from computer
tomographic (CT) images of the colon holds great promise
as a screening method for colorectal cancer, particularly if
CAD tools are developed to facilitate the efﬁciency of radiol-
ogists’ efforts in detecting lesions. In over 90% of the cases
colon cancer progressed rapidly is from local (polyp adeno-
mas) to advanced stages (colorectal cancer), which has very
poor survival rates. However, identifying (and removing) le-
sions (polyp) when still in a local stage of the disease, has
very high survival rates [1], hence early diagnosis is critical.
This is a challenging learning problem that requires the
use of a random effects model due to two reasons. First,
the sizes of polyps (positive examples) vary from 1 mm to
all the way up to 60 mm: as the size of a polyp gets larger,
the number of candidates identifying it increases (these can-
didates are highly correlated). Second, the data is collected
from 152 patients across seven different sites. Factors such
as patient anatomy/preparation, physician practice and scan-
ner type vary across different patients and hospitals—the data
from the same patient/hospital is correlated.

Dataset: The database of high-resolution CT images used
in this study were obtained from NYU Medical Center, Cleve-
land Clinic Foundation, and ﬁve EU sites in Vienna, Belgium,
Notre Dame, Muenster and Rome. The 275 patients were
randomly partitioned into training (n=152 patients, with 126
polyps among 15596 candidates) and test (n=123 patients,
with 104 polyps among 12984 candidates) groups. The test
group was sequestered and only used to evaluate the perfor-
mance of the ﬁnal system. A combined total of 48 features
are extracted for each candidate.

Experimental Results: The ROC curves obtained for the
three techniques on the test data are shown in Figure 2. To
better visualize the differences among the curves, the en-
larged views corresponding to regions of clinical signiﬁcance
are plotted. We performed pair-wise analysis of the three
curves to see if the difference between each pair for the area
under the ROC-curve is statistically signiﬁcant (p values com-
puted using the technique described in [2]). Statistical anal-
ysis indicates that FD-AFV is more accurate than FD with a
p-value of 0.01, GLMM will be more accurate than FD with
a p-value of 0.07 and FD-AFV will be more accurate than
GLMM with a p-value of 0.18. The run times for each algo-
rithm are shown in Table 1.

5.2 Experiment 2: Pulmonary Embolism

Data Sources and Domain Description

Pulmonary embolism (PE), a potentially life-threatening con-
dition, is a result of underlying venous thromboembolic dis-
ease. An early and accurate diagnosis is the key to survival.
Computed tomography angiography (CTA) has merged as an
accurate diagnostic tool for PE. However, there are hundreds
of CT slices in each CTA study. Manual reading is laborious,
time consuming and complicated by various PE look-alikes
(false positives) including respiratory motion artifact, ﬂow-
related artifact, streak artifact, partial volume artifact, stair
step artifact, lymph nodes, vascular bifurcation among many
others [6], [10]. Several Computer-Aided Detection(CAD)
systems are developed to assist radiologists in this process by

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.9

FD−AFV
FD
GLMM

0.92

0.94

0.96

0.98

1

Figure 2: Comparing FD, FD-AFV on the Colon Testing data,
pF D
F D−AF V = 0.01, pF D

GLMM = 0.07, pGLMM

F D−AF V = 0.18.

Table 1: Comparison of training times in seconds for FD, FD-
AFV and GLMM for PE and Colon training data

Algorithm Time (Colon)
FD
FD-AFV
GLMM

518

5
7

Time (PE)

26
28
329

helping them detect and characterize emboli in an accurate,
efﬁcient and reproducible way [9], [11].

An embolus forms with complex shape characteristics in
the lung making the automated detection very challenging.
The candidate generation (CG) algorithm searches for inten-
sity minima. Since each embolus is usually broken into sev-
eral smaller units, CG picks up several points of interest in
the close neighborhood of an embolus from which features
are extracted. Thus multiple candidates are generated while
characterizing an embolus by the CG algorithm.

In addition to usual patient and hospital level random ef-
fects, here we observe a more concrete example of random
effects in CAD; the samples within the close neighborhood
of an embolus in a patient are more strongly correlated than
those far from the embolus. This constitutes a special case of
patient-level random effects. All the samples pointing to the
same PE are labeled as positive in the ground truth and are as-
signed the same PE id. We have collected 68 cases with 208
PEs marked by expert chest radiologists at two different in-
stitutions. They are randomly divided into two sets: training
(45 cases with 142 clots generating a total of 3017 candidates)
and testing (23 cases with 66 clots generating a total of 1391
candidates). The test group was sequestered and only used
to evaluate the performance of the ﬁnal system. A combined
total of 115 features are extracted for each candidate.

Experimental Design and Results:
The ROC curves obtained for the three techniques on the test
data are shown in Figure 3. The statistical analysis indicate

IJCAI-07

760

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.9

FD
GLMM
FD−AFV

0.92

0.94

0.96

0.98

1

Figure 3: Comparing FD, FD-AFV on the PE Testing data,
pF D
F D−AF V = 0.08, pF D

GLMM = 0.25, pGLMM

F D−AF V = 0.38.

that FD-AFV will be more sensitive than FD with a p-value of
0.08, GLMM will be more sensitive than FD with a p-value
of 0.25 and FD-AFV will be more sensitive than GLMM with
a p-value of 0.38. Even though statistical signiﬁcance can not
be proved here (a p-value less than 0.05 is required), a p-
value of 0.08 clearly favors FD-AFV over FD. Note also that
the ROC curve corresponding to FD-AFV clearly dominates
that of FD in the region of clinical interest. When the ROC
curves for FD-AFV and GLMM are compared we see that
the difference is not signiﬁcant. However, using the proposed
augmented feature vector formulation we were able account
for the random effects with much less of computational effort
than is required for GLMM.

6 Conclusion
Summary: The basic message of this paper is:
there is a
standard i.i.d. assumption that is implicit in the derivation of
most classiﬁer-training algortihms. By allowing the classiﬁer
to explain the data based on both the random effects (common
to many samples) and the ﬁxed effects speciﬁc to a each sam-
ple, the learning algorithm can achieve better performance. If
training data is limited, fully Bayesian integration via MCMC
algorithms can help improve accuracy slightly, but signiﬁ-
ciant gains can be reaped without even that effort. All that
a practitioner needs to do is to create a categorical indicator
variable for each random effect.

Contributions: Generalized Linear Mixed effects models
have been extensively studied in the statistics and epidemi-
ology communities. The main contribution of this paper is
to highlight the problem and solutions to the Machine learn-
ing & Data Mining community:
in our experiments, both
GLMMs and the proposed approach improve the classiﬁca-
tion accuracy as compared to ignoring the inter-sample cor-
relations. Our secondary contribution is to propose a very
simple and extremely efﬁcient solution that can scale well to
very large data mining problems, unlike the current statistics
approach to GLMMs that are computationally infeasible for

the large datasets. Despite the computational speedup, the
classiﬁcation accuracy of the proposed method was no worse
than that of GLMMs in our real-life experiments.

Applicability: Although we describes our approach in a
medical setting in order to make it more concrete, the algo-
rithm is completely general and is capable of handling any hi-
erarchical correlations structure among the training samples.
Though our experiments focussed on Fisher’s Discriminants
in this study, the proposed model can be incorporated into any
linear discriminant function based classiﬁer.

References
[1] L. Bogoni, P. Cathier, M. Dundar, A. Jerebko, S. Lakare,
J. Liang, S. Periaswamy, M. Baker, and M. Macari. Cad
for colonography: A tool to address a growing need.
British Journal of Radiology, 78:57–62, 2005.

[2]

J. A. Hanley and B. J. McNeil. A method of comparing
the areas under receiver operating characteristic curves
derived from the same cases. Radiology, 148:839–843,
1983.

[3] H. Ishwaran.

Inference for the random effects in
bayesian generalized linear mixed models. In ASA Pro-
ceedings of the Bayesian Statistical Science Section,
pages 1–10, 2000.

[4] D.

Jemal, R. Tiwari, T. Murray, A. Ghafoor,
A. Saumuels, E. Ward, E. Feuer, and M. Thun. Can-
cer statistics, 2004.

[5] B. Krishnapuram, L. C. M. Figueiredo,

and
Sparse multinomial logistic regres-
A. Hartemink.
sion:
Fast algorithms and generalization bounds.
IEEE Transactions on Pattern Analysis and Machine
Intelligence (PAMI), 27:957968, 2005.

[6] Y. Masutani, H. MacMahon, and K. Doi. Computerized
detection of pulmonary embolism in spiral ct angiogra-
phy based on volumetric image analysis. IEEE Trans-
actions on Medical Imaging, 21:1517–1523, 2002.

[7] P. McCullagh and J. A. Nelder. Generalized Linear

Models. Chapman & Hall/CRC, 1989.

[8] S. Mika, G. R¨atsch, and K.-R. M¨uller. A mathematical
programming approach to the kernel ﬁsher algorithm.
In NIPS, pages 591–597, 2000.

[9] M. Quist, H. Bouma, C. V. Kuijk, O. V. Delden, and
F. Gerritsen. Computer aided detection of pulmonary
embolism on multi-detector ct, 2004.

[10] C. Wittram, M. Maher, A. Yoo, M. Kalra, O. Jo-Anne,
M. Shepard, and T. McLoud. Ct angiography of pul-
monary embolism: Diagnostic criteria and causes of
misdiagnosis, 2004.

[11] C. Zhou, L. M. Hadjiiski, B. Sahiner, H.-P. Chan, S. Pa-
tel, P. Cascade, E. A. Kazerooni, and J. Wei. Comput-
erized detection of pulmonary embolism in 3D com-
puted tomographic (CT) images: vessel tracking and
segmentation techniques.
In Medical Imaging 2003:
Image Processing. Edited by Sonka, Milan; Fitzpatrick,
J. Michael. Proceedings of the SPIE, Volume 5032, pp.
1613-1620 (2003)., pages 1613–1620, May 2003.

IJCAI-07

761


Learning by Analogy : a Classiﬁcation Rule

for Binary and Nominal Data

Sabri Bayoudh and Laurent Miclet and Arnaud Delhay

IRISA/CORDIAL – Universit de Rennes 1, France

sabri.bayoudh, laurent.miclet, arnaud.delhay @enssat.fr

Abstract

This paper deals with learning to classify by using
an approximation of the analogical proportion be-
tween four objects. These objects are described by
binary and nominal attributes. Firstly, the paper re-
calls what is an analogical proportion between four
objects, then it introduces a measure called ”ana-
logical dissimilarity”, reﬂecting how close four ob-
jects are from being in an analogical proportion.
Secondly, it presents an analogical instance-based
learning method and describes a fast algorithm.
Thirdly, a technique to assign a set of weights to the
attributes of the objects is given: a weight is cho-
sen according to the type of the analogical propor-
tion involved. The weights are obtained from the
learning sample. Then, some results of the method
are presented. They compare favorably to standard
classiﬁcation techniques on six benchmarks. Fi-
nally, the relevance and complexity of the method
are discussed.
Keywords : instance-based learning, learning by
analogy, analogical proportion, analogical dissim-
ilarity.

fourth being the object to classify). One property of the ana-
logical dissimilarity between four objects is that it is null
when the objects are in analogical proportion. Let us take
a toy example to explain what is an analogical proportion and
how to learn with it. Let tomcat be the object to classify into
Feline or Ruminant. Suppose that there are only three in-
stances in the training set: calf, bull and kitten. The animals
are described by four binary attributes (1 stands for T RU E
and 0 for F ALSE) which are has claws(HC), is an adult(IA),
is a male(IM), feeds by suckling(FS).

Animals HC IA IM FS
1
0
1
0

calf
bull
kitten
tomcat

0
0
1
1

0
1
0
1

0
1
0
1

class

Ruminant
Ruminant

Feline

?

For each attribute, we notice that there is an analogical pro-
portion between the three objects, taken in this order, from the
training set and the object tomcat, since the four binary values
have one of the forms below (we will see later what are all the
possible analogical proportions between binary values).

0 is to 0 as 1 is to 1 (HC)
0 is to 1 as 0 is to 1 (IA and IM)
1 is to 0 as 1 is to 0 (FS)

1 Introduction
The aim of this article is to present a new non-parametric
classiﬁcation rule for objects described by binary and nom-
inal attributes. Some deﬁnitions and original algorithms will
be presented to explain this method, and results will be given.
The principle of non-parametric (or instance-based) learning
is to keep the whole learning set as a base for the decision in-
stead of inducing an explicit classiﬁcation concept. The best
known and simplest instance-based classiﬁcation rule is the
k-nearest neighbors (k-nn) rule, which requires only a met-
ric space. This rule, in its simplest form, computes the dis-
tance between the object to classify and all the instances in
the learning set and considers the k nearest instances as elec-
tors of their class [DUDA et al., 2001]. The class of the new
object is the class which has the majority given by the k elec-
tors.
Learning by analogy, as we present it here, is also an instance-
based technique which uses the concept of analogical dis-
similarity between four objects (three in the training set, the

Thus, the four objects are in analogical proportion since all
their binary attributes are in analogical proportion :

calf is to bull as kitten is to tomcat

We have to emphasize two points here. Firstly, taking these
three objects (triplet) in a different order will not produce any-
more an analogical proportion with tomcat, except when bull
and kitten are exchanged. Secondly, to make full use of the
training set, we must consider also the triplets containing two
or three identical objects. Consequently, a training set of m
objects produces m3 different triplets. In this example, it is
easy to verify that only two triplets among the 27 are in ana-
logical proportion with tomcat.
For the triplet (calf, bull, kitten), the resolution of an analogi-
cal equation on the classes gives the class of tomcat:

Ruminant is to Ruminant as Feline is to class(tomcat)

which solves in : class(tomcat) = Feline.
The second triplet (calf, kitten, bull) in analogy with tomcat
gives the same result. Hence, the classiﬁcation by analogy of

IJCAI-07

678

tomcat on this learning set is Feline.
Notice that the 1-nearest neighbor technique, using the Ham-
ming distance, concludes in this example that class(tomcat)
= Ruminant. The strength of the analogical classiﬁer stems
from the fact that it can use objects in the training set be-
longing to some class ω1 to conclude that the unknown class
object is ω2. In terms of distance, an analogical classiﬁer can
give importance to objects that are far from the object to be
classiﬁed. It actually makes a different use of the informa-
tion in the training set than distance classiﬁers or decision
trees, because it uses a (restricted) form of analogical reason-
ing [GENTNER et al., 2001].
Is the use of analogical pro-
portions actually relevant for classiﬁcation? The goal of this
article is to investigate in this direction, for data sets described
by binary and nominal data.

In Section 2, we give a formal deﬁnition of the notion of
analogical proportion and we introduce that of analogical dis-
similarity. In Section 3, we present a naive algorithm of learn-
ing a classiﬁcation rule followed by a faster version, which
makes use of the properties of analogical dissimilarity.
In
Section 4, we propose to learn from the set of instances three
different weights for each attribute, and to use one of them
to modify the computation of the analogical dissimilarity (a
weight is chosen among the three according to the type of
analogical proportion which is involved). We give results on
eight data bases from the UCI ML repository [NEWMAN et
al., 1998] and compare with various standard classiﬁcation
algorithms in Sect. 5. Finally, Section 6 discusses these re-
sults and the complexity of the algorithm.

2 Analogical Proportion and Analogical

Dissimilarity

2.1 Analogical Proportion between Four Binary

Objects

Generally speaking, an analogical proportion on a set X is
a relation between four elements of X, i.e. a subset of X 4,
which writes a : b :: c : d and reads:

a is to b as c is to d

The elements a, b, c and d are said to be in analogical pro-
portion [DELHAY and MICLET, 2005]. As deﬁned by Lep-
age [LEPAGE and ANDO, 1996][LEPAGE, 1996], an analog-
ical proportion a : b :: c : d implies two other analogical
proportions:

Symmetry of the ”as” relation:
Exchange of the means:

c : d :: a : b
a : c :: b : d

And the following property is also required :

Determinism: if a : a :: b : x then x = b

From the ﬁrst two properties one can deduce ﬁve more equiv-
alent analogical proportions, which gives eight equivalent
ways to write that the objects a, b, c and d are in analogical
proportion :

c : d :: a : b
a : c :: b : d
a : b :: c : d

c : a :: d : b
d : b :: c : a
d : c :: b : a
b : a :: d : c
b : d :: a : c

When X is the boolean set B = {0,1} there are only six 4-
tuples in analogical proportion among the sixteen possibilities
(and two special cases (a) and (b) in the other ten, as we shall
see in the next paragraph):

In analogical

proportion

0 : 0 :: 0 : 0
0 : 0 :: 1 : 1
0 : 1 :: 0 : 1
1 : 0 :: 1 : 0
1 : 1 :: 0 : 0
1 : 1 :: 1 : 1

Not in analogical

proportion

0 : 0 :: 0 :
0 : 0 :: 1 :
0 : 1 :: 0 :
0 : 1 :: 1 :
0 : 1 :: 1 :
1 : 0 :: 0 :
1 : 0 :: 0 :
1 : 0 :: 1 :
1 : 1 :: 0 :
1 : 1 :: 1 :

1
0
0
0
1
0
1
1
1
0

(a)

(b)

Now, if we take objects described by m binary attributes, we
can easily construct an analogical proportion on Bm

.

Deﬁnition 1 Four objects in Bm
are in analogical proportion
if and only if all their attributes are in analogical proportion:

a : b :: c : d ⇔ aj : bj :: cj : dj ∀ 1 ≤ j ≤ m

With this deﬁnition, it is straightforward to verify that the
above properties of analogical proportion are still veriﬁed.

2.2 Analogical Dissimilarity between Binary

Objects

Up to now, four objects are either in analogical proportion
or not. Here we introduce a new notion to measure, when
they are not in analogical proportion, how far are four ob-
jects from being in analogical proportion. This measure is
called Analogical Dissimilarity (AD). Another measure has
already been presented in the literature, for natural langage
processing, in the framework of semantic analogy [TURNEY,
2005] by considering similarity relations between two pairs
of words.
In B, we give the value of 1 to every four-tuple which is not in
analogical proportion, except for (a) and (b), which take the
value of 2. 1 and 2 are the number of binary values to switch
to produce an analogical proportion.
This deﬁnition leads to the following properties:

Property 1

1. ∀u, v, w, x ∈ B, AD(u, v, w, x) = 0 ⇔ u : v :: w : x
2. ∀u, v, w, x ∈ B,

AD(u, v, w, x) = AD(w, x, u, v) = AD(u, w, v, x)

3. ∀u, v, w, x, z, t ∈ B,

AD(u, v, z, t) ≤ AD(u, v, w, x) + AD(w, x, z, t)

4. In general, ∀u, v, w, x ∈ B :

AD(u, v, w, x) (cid:6)= AD(v, u, w, x)
(cid:2)m
In Bm
as
still hold true [MICLET and DELHAY, 2004].

, we deﬁne the analogical dissimilarity AD(a, b, c, d)
AD(aj, bj, cj, dj), and the four properties above
j=1

IJCAI-07

679

h(a) : h(b) :: h(c) : h(x)
ω0
ω1
ω1
ω0

:: ω0
:: ω1
:: ω1
:: ω0

: ω0
: ω0
: ω1
: ω1

?
?
?
?

:
:
:
:

resolution
h(x) = ω0

h(x) = ω1

Table 1: Possible conﬁgurations of a triplets

2.3 Extension to Nominal Attributes

To cope with nominal data, two approaches are possible:

• The ﬁrst one (one-per-value encoding) consists in split-
ting the nominal attribute. As a result, an n valued nom-
inal attribute is replaced by n binary attributes with ex-
actly one at 1.

• The second approach consists in keeping the nominal
attribute as a single attribute. This requires to deﬁne an
analogical proportion on the values of the attribute.

The second approach can be used when there is some knowl-
edge about the nominal values, like an order relation or a mea-
sure of distance between the values. Since it is not the case in
the data sets that we have worked on, we have chosen to use
the one-per-value encoding to treat the nominal attributes.

3 A Classiﬁcation Rule by Analogical

Dissimilarity

(cid:6)

(cid:3)(cid:4)

| 1 ≤ i ≤ m

(cid:5)
oi, h(oi)

3.1 Principle
Let S =
be a learning set, where
h(oi) is the class of the object oi. The objects are deﬁned by
binary attributes. Let x be an object not in S. The learning
problem is to ﬁnd the class of x, using the learning set S.
To do this, we deﬁne a learning rule based on the concept
of analogical dissimilarity depending on an integer k, which
could be called the k least dissimilar triplets rule. It consists
of the following steps:

1. Compute the analogical dissimilarity between x and all
the n triplets in S which produce a solution for the class
of x.

2. Sort these n triplets by the increasing value of their AD

when associated with x.

3. If the k-th object has the integer value p, then let k(cid:2)
be the greatest integer such that the k(cid:2)-th object has the
value p.

4. Solve the k(cid:2) analogical equations on the label of the
class. Take the winner of the votes among the k(cid:2) results.

To explain, we ﬁrstly consider the case where there are only
two classes ω0 and ω1. An example with 3 classes will follow.
Point 1 means that we retain only the triplets which have one
of the four1 following conﬁguration for their class (see Table
1). We ignore the triplets that do not lead to an equation with
a trivial solution on classes :

1There are actually two more, each one equivalent to one of the

four (by exchange of the mean objects).

h(a) : h(b) :: h(c) : h(x)
ω0
ω1

:: ω1
:: ω0

: ω1
: ω0

?
?

:
:

Point 2 is comparable to the k nearest neighbors method.
Since AD takes integer values, there are in general several
triplets for which AD = 0, AD = 1, etc. That is why point
3 increases the value of k to take into account all the triplets
with the same value for AD. Finally, point 4 uses the same
voting technique as the k-nn rule.
Multiclass problem: we proceed in the same way as in the
two classes problem, keeping only the triplets which solve on
the class label, for example:

Analogical equations

with a solution

ω1 : ω3 :: ω1 : ? ⇒ h(x) = ω3
ω4 : ω4 :: ω2 : ? ⇒ h(x) = ω2

with no solution

ω1 : ω3 :: ω0 : ? ⇒ h(x) =?
ω0 : ω1 :: ω3 : ? ⇒ h(x) =?

Missing values problem: we consider a missing value as a
value for which the distance to each valued nominal attribute
is the same. Hence, when splitting a nominal attribute, a
missing value would take the value 0 in all the split binary
attributes instead of taking the value 1 in exactly one of them
(Sect. 2.3).

Example
Let S = {(a, ω0), (b, ω0), (c, ω1), (d, ω1), (e, ω2)} be a set
of ﬁve labelled objects and let x (cid:6)∈ S be some object to be
classiﬁed. According to the analogical proportion axioms,
there is only 75 (= (Card(S)3 + Card(S)2)/2) non-
equivalent analogical equations among 125(= Card(S)3)
equations that can be formed between three objects from S
and x. Table (2) shows only the ﬁrst 14 lines after sorting
with regard to some arbitrarily analogical dissimilarity.
The following table gives the classiﬁcation of an object x
according to k :

k
k(cid:2)

1 2 3 4 5 6 7
1 3 3 5 5 7 7
votes for x 1 1 1 ? ? 2 2

3.2 A Fast Algorithm : FADANA*
As shown in Sect. 3.1, this learning by analogical proportion
technique needs to ﬁnd the k least dissimilar analogical pro-
portion triplets in S. The naive way is to compute the analogi-
cal dissimilarity of every possible triplet from the learning set
with the unknown class object as the fourth object of the ana-
logical proportion. This method has a complexity of O(m3)
based on the number of all the non equivalent triplets that can
be constructed from the learning set S. To make it faster, we
use the algorithm FADANA* (FAst search of the least Dis-
similar ANAlogy) inspired from LAESA [MORENO-SECO
et al., 2000, ] for the 1-nn classiﬁcation rule. This algo-
rithm speeds up the computation on line but has to make off
line some preprocessing of the learning set. The number of
off line computations depends on the number of ”base proto-
types” (complexity of O(bp∗m2)). The more base prototypes

IJCAI-07

680

o1 o2 o3 h(o1) h(o2) h(o3) h(x) AD k
b a d
1
b d e
c d e
a b d
c a e
e
d c
d b
c
e
a c
c
a c
a b
e
b a e
c d
b
c
c
c
a a c
...
...

ω1
ω2
ω2
ω1
ω2
ω2
ω1
ω2
ω1
ω2
ω2
ω1
ω1
ω1
...

ω0
ω1
ω1
ω0
ω0
ω1
ω0
ω1
ω1
ω0
ω0
ω1
ω1
ω0
...

ω0
ω0
ω1
ω0
ω1
ω1
ω1
ω0
ω0
ω0
ω0
ω0
ω1
ω0
...

ω1
⊥
ω2
ω1
⊥
ω2
ω0
⊥
⊥
ω2
ω2
⊥
ω1
ω1
...

0
1
1
1
2
2
2
2
3
3
3
3
4
4
...

8
9
...

6
7

2
3

4
5

...

Table 2: An example of classiﬁcation by analogical dissimi-
larity. Analogical proportions whose analogical resolution on
classes have no solution (represented by ⊥) are not taken into
account. AD is short for AD(o1, o2, o3, x).

we have the more computations we do off line and the less we
do on line. To show its efﬁciency, we give on the SPECT data
base (Sect. 5.1) the performance of FADANA* (the naive
method would need 512 000 AD computations).

bp

20 50 100 200
nAD 83365 41753 6459 1084 404 299 283 275

10

2

5

1

bp is the number of base prototypes, and nAD is the number
of AD computations in the online part.

4 Weighting the Attributes for an Analogical

Dissimilarity Decision Rule

The basic idea in weighting is that all the attributes do not
have the same importance in the classiﬁcation. The idea of
selecting interesting attributes for analogy is not new.
In
[TURNEY, 2005] a discrimination is also done by keeping
the most frequent patterns in words. Therefore, one should
give greater importance to the attributes that are actually dis-
criminant. However, in an analogical classiﬁcation, there are
several ways to ﬁnd the class of the unknown element. Let
us take again the preceding two class problem example (1).
We notice that there is two ways to decide between the class
ω0 and the class ω1 (there is also a third possible conﬁgu-
ration which is equivalent to the second by exchange of the
means (Sect. 2.1)). We therefore have to take into account
the equation used to ﬁnd the class. This is why we deﬁne
a set of weights for each attribute, according to the number
of classes. These sets are stored in an analogical weighting
matrix (Sect. 4.1).

4.1 Weighting Matrix
Deﬁnition 2 An analogical weighting matrix (W ) is a three
dimensional array. The ﬁrst dimension is for the attributes,
the second one is for the class of the ﬁrst element in an ana-
logical proportion and the third one is for the class of the last

element in an analogical proportion. The analogical propor-
tion weighting matrix is a d × C × C matrix, where d is the
number of attributes and C is the number of classes.

For a given attribute ak of rank k, the element Wkij of the
matrix indicates which weight must be given to ak when en-
countered in an analogical proportion on classes whose ﬁrst
element is ωi, and for which ωj is computed as the solution.
Hence, for the attribute ak:

Last element (decision)

class ωi class ωj
class ωi Wkii Wkij
class ωj Wkji Wkjj

First ele-
ment

Since we only take into account the triplets that give a solu-
tion on the class decision, all the possible situations are of one
of the three patterns:

Possible patterns

First

Decision

ωi : ωi
:: ωj
ωi : ωj :: ωi
:: ωi
ωi : ωi

: ωj
: ωj
: ωi

element

ωi
ωi
ωi

class
ωj
ωj
ωi

This remark gives us a way to compute the values Wkij from
the learning set.

4.2 Learning the Weights from the Training

Sample

The goal is now to ﬁll the three dimensional analogical
weighting matrix using the learning set. We estimate Wkij
by the frequency that the attribute k is in an analogical pro-
portion with the ﬁrst element class ωi, and solves in class ωj.
Firstly, we tabulate the splitting of every attribute ak on the

classes ωi:

ak = 0
ak = 1

. . . class ωi . . .
. . .
. . .
. . .
. . .

n0i
n1i

where ak is the attribute k and n0i (resp. n1i) is the number
of objects in the class i that have the value 0 (resp. 1) for the
binary attribute k. Hence,
1(cid:7)

C(cid:7)

nki = m

k=0

i=1

m is the number of objects in the training set.
Secondly, we compute Wkij by estimating the probability to
ﬁnd a correct analogical proportion on attribute k with ﬁrst
element class ωi which solves in class ωj.
In the following table we show all the possible ways of having
an analogical proportion on the binary attribute k. 0i (resp.
1i) is the 0 (resp. 1) value of the attribute k that has class ωi.

1st
2sd
3rd
4th
5th
6th

0i : 0i :: 0j : 0j
0i : 1i :: 0j : 1j
0i : 0i :: 1j : 1j
1i : 1i :: 1j : 1j
1i : 0i :: 1j : 0j
1i : 1i :: 0j : 0j

IJCAI-07

681

Pk(1st) estimates the probability that the ﬁrst analogical pro-
portion in the table above occurs.

• IBk (k=10): k Nearest-Neighbor classiﬁer with distance

weighting (weight by 1/distance).

Pk(1st) = n0in0in0jn0j/m4

...

From Wkij = Pk(1st) + . . . + Pk(6th), we compute
(cid:5)
/(6∗m4)

1j)+2∗n0in0jn1in1j

(cid:4)
(n2

Wkij =

0i +n2

1i)(n2

0j +n2

The decision algorithm (Sect. 3.1) is only modiﬁed at point
1, which turns into Weighted Analogical Proportion Classiﬁer
(W AP C):

• Given x, ﬁnd all the n triplets in S which can produce
a solution for the class of x. For every triplet among
these n, say (a, b, c), consider the class ωi of the ﬁrst
element a and the class ωj of the solution. Compute the
analogical dissimilarity between x and this triplet with
the weighted AD:

AD(a, b, c, x) =

d(cid:7)

k=1

Wkij AD(ak, bk, ck, xk)

Otherwise, if point 1 is not modiﬁed, the method is called
Analogical Proportion Classiﬁer (AP C).

5 Experiments and Results

5.1 Experiment Protocol
We have applied the weighted analogical proportion classi-
ﬁer (W AP C) to eight classical data bases, with binary and
nominal attributes, of the UCI Repository [NEWMAN et al.,
1998].

MONK 1,2 and 3 Problems (MO.1, MO.2 and MO.3),
MONK3 problem has noise added. SPECT heart data (SP.).
Balance-Scale (B.S) and Hayes Roth (H.R) database, both
multiclass database. Breast-W (Br.) and Mushroom (Mu.),
both data sets contain missing values.
In order to measure the efﬁciency of W AP C, we have ap-
plied some standard classiﬁers to the same databases, and
we have also applied AP C to point out the contribution of
the weighting matrix (Sect.4.1). We give here the parameters
used for the comparison method in Table 3:

• Decision Table: the number of non improving decision

tables to consider before abandoning the search is 5.

• Id3: unpruned decision tree, no missing values allowed.
• Part: partial C4.5 decision tree in each iteration and
turns the ”best” leaf into a rule, One-per-value encod-
ing.

• Multi layer Perceptron: back propagation training,
One-per-value encoding, one hidden layer with (number
of classes+number of attributes)/2 nodes.

• LMT (’logistic model trees’): classiﬁcation trees with
logistic regression functions at the leaves, One-per-value
encoding.

• IB1: Nearest-Neighbor classiﬁer with normalized Eu-

clidean distance.

• IB1 (k=5)[Guide, 2004]: k Nearest-Neighbors classi-
ﬁer with attributes weighting, similarity computed as
weighted overlap, relevance weights computed with gain
ratio.

5.2 Results
We have worked with the WEKA package [WITTEN and
FRANK, 2005] and with TiMBL [Guide, 2004], choosing 7
various classiﬁcation rules on the same data from WEKA
and the last classiﬁer from TiMBL. Some are well ﬁt to bi-
nary data, like ID3, PART, Decision Table. Others, like IB1
or Multilayer Perceptron, are more adapted to numerical and
noisy data. The results are given in Table 3. We have arbitrar-
ily taken k = 100 for our two rules. We draw the following
conclusions from this preliminary comparative study: ﬁrstly,
according to the good classiﬁcation rate of W AP C in Br. and
Mu. databases, we can say that W AP C handles the miss-
ing values well. Secondly, W AP C seems to belong to the
best classiﬁers for the B.S and H.R databases, which conﬁrms
that W AP C deals well with multiclass problems. Thirdly, as
shown by the good classiﬁcation rate of W AP C in the MO.3
problem, W AP C handles well noisy data. Finally, the re-
sults on MO. and B.S database are exactly the same with the
weighted decision rule W AP C than with AP C. This is due
to the fact that all AD that are computed up to k = 100 are of
null value. But on the other data bases, the weighting is quite
effective.

6 Discussion and FutureWork

Besides its original point of view, the weighted analogical
proportion classiﬁer seems, after these preliminary experi-
ments, to belong to the best classiﬁers on binary and nomi-
nal data, at least for small training sets. The reason is that it
can proﬁt from regularities in alternated values in the classes
in contrast to decision trees or metric methods which directly
correlate the attributes with the classes. This means that an
analogical proportion between four objects in the same class
is reinforced when taking into account a new attribute with
values (0, 1, 0, 1) on the four objects but is decreased if the
values are (0, 1, 1, 0).

We believe that there is still ample room for progress in
this technique. In particular, we intend to investigate a more
precise mode of weighting the attributes according to the type
of analogical proportion in which they are involved, and also
to test W AP C with numerical attributes. Obviously, more
work has to be done on the computational aspects. Even with
the FADANA* method (Sect. 3.2), the decision process still
takes too much time for realistic computation on large data
sets.

Another interesting question is the limits of this type of
classiﬁcation. We already know how to deﬁne an analogi-
cal proportion and an analogical dissimilarity on numerical
objects, and we have investigated on sequences of symbolic
or numerical objects [DELHAY and MICLET, 2005]. Still, we
do not know in which cases an analogical proportion between
classes is related to that of objects that constitute the classes

IJCAI-07

682

Methods
number of nominal attributes
number of binary attributes
number of train instances
number of test instances
number of classes
WAPC (k = 100)
APC (k = 100)
Decision Table
Id3
PART
Multi layer Perceptron
LMT
IB1
IBk (k = 10)
IB1 (k = 5)

MO.1 MO.2 MO.3 SP. B.S
4
20
187
438

7
15
124
432

7
15
169
432

7
15
122
432

22
22
80
172

2

2

2

2

3

2

Br. H.R Mu.
22
9
90
121
35
81
664

8043

4
15
66
66
4

2

98% 100% 96% 79% 86% 96% 82% 98%
98% 100% 96% 58% 86% 91% 74% 97%
100% 64% 97% 65% 67% 86% 42% 99%
78% 65% 94% 71% 54% mv 71% mv
93% 78% 98% 81% 76% 88% 82% 94%
100% 100% 94% 73% 89% 96% 77% 96%
94% 76% 97% 77% 89% 88% 83% 94%
79% 74% 83% 80% 62% 96% 56% 98%
81% 7% 93% 57% 82% 86% 61% 91%
73% 59% 97% 65% 78% 95% 80% 97%

Table 3: Comparison Table between W AP C and other classical classiﬁers on eight data sets. The best classiﬁcation rates,
within a signiﬁcance level = 5%, are in Boldface.

seminar on interactive desambiguation, pages 93–100,
Grenoble, 1996.

[MICLET and DELHAY, 2004] L. MICLET and A. DELHAY.
Relation d’analogie et distance sur un alphabet d´eﬁni par
des traits. Technical Report 5244, INRIA, July 2004. in
French.

[NEWMAN et al., 1998] D.J. NEWMAN, S. HETTICH, C.L.
BLAKE, and C.J. MERZ. UCI repository of machine learn-
ing databases, 1998.

[MORENO-SECO et al., 2000] F.

MORENO-SECO,
J. ONCINA-CARRATAL ´A,
and L. MICO-ANDRES.
Improving the Linear Approximating and Eliminating
Search Algorithm (LAESA) Error Rates, page 43.
IOS
Press, 2000.
Pattern Recognition and Applications.
Frontiers in Artiﬁcial Intelligence and Applications,.

[TURNEY, 2005] Peter D. TURNEY. Measuring semantic
similarity by latent relational analysis. Proceedings Nine-
teenth International Joint Conference on Artiﬁcial Intelli-
gence (IJCAI-05), 05:1136, 2005.

[WITTEN and FRANK, 2005] Ian H. WITTEN and Eibe
FRANK. Data Mining: Practical machine learning tools
and techniques, 2nd Edition. Morgan Kaufmann Publish-
ers, 2005.

in the learning sample. Are there data for which an analogi-
cal proportion classiﬁcation is ”natural” and data for which it
does not make sense?

7 Conclusion

In this article, we have shown a new method for binary and
nominal data classiﬁcation. We use the deﬁnition of an ana-
logical proportion between four objects. The technique of the
classiﬁer is based on the new notion of analogical dissimilar-
ity and on the resolution of analogical equations on the class
labels.

We also have shown the importance of weighting the at-
tributes according to the type of analogical proportion in
which they are involved. This has led us to good results in
all situations, compared to off the shelf standard classiﬁers,
on six classical data bases.

References
[DELHAY and MICLET, 2005] A. DELHAY and L. MICLET.
Analogie entre s´equences : D´eﬁnitions, calcul et utilisa-
tion en apprentissage supervis´e. Revue d’Intelligence Ar-
tiﬁcielle., 19:683–712, 2005.

[DUDA et al., 2001] R. DUDA, P. HART, and D. STORK.

Pattern classiﬁcation. Wiley, 2001.

[GENTNER et al., 2001] D. GENTNER, K. J. HOLYOAK, and
B. KOKINOV. The analogical mind: Perspectives from
cognitive science. MIT Press, Cambridge, MA, 2001.

[Guide, 2004] Version Reference Guide. Timbl: Tilburg

memory-based learner, 2004.

[LEPAGE and ANDO, 1996] Y. LEPAGE and S. ANDO. Saus-
surian analogy: a theoretical account and its applica-
tion.
In Proceedings of COLING-96, pages 717–722,
København, August 1996.

[LEPAGE, 1996] Y. LEPAGE. Ambiguities in analysis by
analogy. In Proceedings of MIDDIM-96, post–COLING

IJCAI-07

683


Automatic Evaluation of Text Coherence: Models and Representations

Regina Barzilay

Computer Science and Artiﬁcial Intelligence Laboratory

Massachusetts Institute of Technology

G-468, the Stata Center
Cambridge, MA 14853
regina@csail.mit.edu

Mirella Lapata

School of Informatics
University of Edinburgh

2 Buccleuch Place

Edinburgh EH8 9LW, UK

mlap@inf.ed.ac.uk

Abstract

This paper investigates the automatic evaluation of
text coherence for machine-generated texts. We in-
troduce a fully-automatic, linguistically rich model
of local coherence that correlates with human judg-
ments. Our modeling approach relies on shallow
text properties and is relatively inexpensive. We
present experimental results that assess the pre-
dictive power of various discourse representations
proposed in the linguistic literature. Our results
demonstrate that certain models capture comple-
mentary aspects of coherence and thus can be com-
bined to improve performance.

1 Introduction
The use of automatic methods for evaluating machine-
generated text is quickly becoming mainstream in machine
translation, text generation, and summarization. These meth-
ods target various dimensions of text quality, ranging from
sentence grammaticality to content selection [Bangalore et
al., 2000; Papineni et al., 2002; Hovy and Lin, 2003]. How-
ever, a fundamental aspect of text structure — its coherence
— is still evaluated manually.

Coherence is a property of well-written texts that makes
them easier to read and understand than a sequence of ran-
domly strung sentences. Although the same information can
be organized in multiple ways to create a coherent text, some
forms of text organization will be indisputably judged inco-
herent. The automatically-generated summary in Table 1 is
an example of a badly organized text: the presence of themat-
ically unrelated sentences and non-canonical time sequences
makes it nearly impossible to comprehend. We want to de-
velop an automatic method that can distinguish such a sum-
mary from a text with no coherence violations.

This paper focuses on local coherence, which captures text
organization at the level of sentence to sentence transitions,
and is undoubtedly necessary for global coherence. This topic
has received much attention in the linguistic literature [Grosz
et al., 1995; Morris and Hirst, 1991; Halliday and Hasan,
1976] and a variety of qualitative models have been proposed.
The emphasis of our work is on quantitative models of local
coherence that can be efﬁciently computed from raw text, and
readily utilized for automatic evaluation. Our approach relies
on shallow text properties that can be easily identiﬁed and

Newspapers reported Wednesday that three top Libyan of-
ﬁcials have been tried and jailed in the Lockerbie case.
Secretary-General Koﬁ Annan said Wednesday that he may
travel to Libya next week in hopes of closing a deal. The sanc-
tions were imposed to force Libyan leader Moammar Gadhaﬁ
to turn the men over. Louis Farrakhan, the leader of a U.S.
Muslim group, met with Gadhaﬁ and congratulated him on
his recovery from a hip injury.

Table 1: A low-coherence summary

quantiﬁed without recourse to elaborate world knowledge and
handcrafted rules.

The coherence models described in this paper fall into
two broad classes that capture orthogonal dimensions of en-
tity distribution in discourse. The ﬁrst class incorporates syn-
tactic aspects of text coherence and characterizes how men-
tions of the same entity in different syntactic positions are
spread across adjacent sentences. Inspired by Centering The-
ory [Grosz et al., 1995], our algorithm abstracts a collection
of coherent texts into a set of entity transition sequences, and
deﬁnes a probabilistic model over their distribution. Given a
new text, the model evaluates its coherence by computing the
probability of its entity transitions according to the training
data.

The second, semantic, class of models quantiﬁes local co-
herence as the degree of connectivity across text sentences.
This approach is motivated by the ﬁndings of Halliday and
Hasan [1976] who emphasized the role of lexical cohesion in
text coherence. The key parameter of these models is the def-
inition of semantic relatedness (as a proxy for connectivity)
among text entities. We explore a wealth of similarity mea-
sures, ranging from distributional to taxonomy-based, to ﬁnd
an optimal lexico-semantic representation for the coherence
assessment task.

We employ our models to evaluate the coherence of mul-
tidocument summaries produced by systems that participated
in the Document Understanding Conference. We acquire co-
herence ratings for a large collection of machine-generated
summaries by eliciting judgments from human subjects, and
examine the extent to which the predictions of various mod-
els correlate with human intuitions. Our experiments demon-
strate that while several models exhibit a statistically signif-
icant agreement with human ratings, a model that fuses the
syntactic and the semantic views yields substantial improve-
ment over any single model. Our contributions are twofold:

Modeling
We present a fully automatic, linguistically
rich model of local coherence that correlates with human
judgments. This model is the ﬁrst attempt, to our knowledge,
to automatically evaluate the local coherence of machine-
generated texts.
Linguistic Analysis
We present experimental results that
assess the predictive power of various knowledge sources and
discourse representations proposed in the linguistic literature.
In particular, we show that certain models capture comple-
mentary aspects of coherence and thus can be combined to
improve performance.

2 Related Work
Most of the work on automatic coherence evaluation has been
done in the context of automatic essay scoring. Higgins et
al. [2004] develop a system that assesses global aspects of
coherence in student essays. They use a manually annotated
corpus of essays to learn which types of discourse segments
can cause breakdowns in coherence. Other approaches focus
on local coherence. Miltsakaki and Kukich [2004] manually
annotate a corpus of student essays with entity transition in-
formation, and show that the distribution of transition types
correlates with human grades. Foltz et al. [1998] propose a
model of local coherence that presupposes no manual cod-
ing. A text is considered coherent if it exhibits a high degree
of meaning overlap between adjacent sentences. They employ
a vector-based representation of lexical meaning and assess
semantic relatedness by measuring the distance between sen-
tence pairs. Foltz et al. report that the model correlates reli-
ably with human judgments and can be used to analyze dis-
course structure. The success of this approach motivates our
research on semantic association models of coherence.

Previous work has primarily focused on human authored
texts and has typically utilized a single source of informa-
tion for coherence assesment. In contrast, we concentrate on
machine generated texts and assess which knowledge sources
are appropriate for measuring local coherence. We introduce
novel models but also assess whether previously proposed
ones (e.g., Foltz et al. [1998]) generalize to automatically
generated texts. As a byproduct of our main investigation, we
also examine whether humans can reliably rate texts in terms
of coherence, thus undertaking a large-scale judgment elici-
tation study.

3 Models of Local Coherence
In this section we introduce two classes of coherence models,
and describe how they can be used in automatic evaluation.
We motivate their construction, present a corresponding dis-
course representation, and an inference procedure.

3.1 The Syntactic View
Centering theory [Grosz et al.,
Linguistic Motivation
1995; Walker et al., 1998] is one of the most inﬂuential frame-
works for modeling local coherence. Fundamental in Center-
ing’s study of discourse is the way entities are introduced
and discussed. The theory asserts that discourse segments
in which successive utterances mention the same entities are
more coherent than discourse segments in which multiple en-
tities are discussed. Coherence analysis revolves around pat-
terns of local entity transitions which specify how the focus

1. [Former Chilean dictator Augusto Pinochet]o, was arrested in

[London]x on [14 October]x 1998.

2. [Pinochet]s, 82, was recovering from [surgery]x.
3. [The arrest]s was in [response]x to [an extradition warrant]x

4. [Pinochet]o was charged with murdering [thousands]o, in-

served by [a Spanish judge]s.

cluding many [Spaniards]o.

5. [Pinochet]s is awaiting [a hearing]o,

[his fate]x in [the

balance]x.

6. [American scholars]s applauded the [arrest]o.

Table 2: Summary augmented with syntactic annotations for
entity grid computation.

r
o
t
a
t
c
i
D

o
t
s
u
g
u
A

t
e
h
c
o
n
i

P

n
o
d
n
o
L

r
e
b
o
t
c
O

y
r
e
g
r
u
S

t
s
e
r
r

A

n
o
i
t
i
d
a
r
t
x
E

t
n
a
r
r
a

W

e
g
d
u
J

s
d
n
a
s
u
o
h
T

s
d
r
a
i
n
a
p
S

g
n
i
r
a
e
H

e
c
n
a
l
a
B

s
r
a
l
o
h
c
S

e
t
a
F

1 O O O X X – – – – – – – – – – – 1
2 – – S – – X – – – – – – – – – – 2
3 – – – – – – S X X S – – – – – – 3
4 – – O – – – – – – – O O – – – – 4
5 – – S – – – – – – – – – O X X – 5
6 – – – – – – O – – – – – – – – S 6

Table 3: An entity grid

of discourse changes from sentence to sentence. The key as-
sumption is that certain types of entity transitions are likely
to appear in locally coherent discourse. Centering also estab-
lishes constraints on the linguistic realization of focus, sug-
gesting that focused entities are likely to occupy prominent
syntactic positions such as subject or object.
Discourse representation
To expose entity transition
patterns characteristic of coherent texts, we represent a text
by an entity grid. The grid’s columns correspond to discourse
entities, while the rows correspond to utterances (see Table 3).
We follow Miltsakaki and Kukich [2004] in assuming that an
utterance is a traditional sentence (i.e., a main clause with ac-
companying subordinate and adjunct clauses).

Grid columns record an entity’s presence or absence in a
sequence of sentences (S1; : : : ; Sn). More speciﬁcally, each
grid cell represents the role ri j of entity e j in a given sen-
tence Si. Grammatical roles reﬂect whether an entity is a sub-
ject (s), object (o), neither (x) or simply absent (–). Table 3
illustrates an entity grid constructed for the text in Table 2.
Since the text contains six sentences, the grid columns are of
length six. As an example consider the grid column for the
entity arrest, [– – s – – o]. It records that arrest is present in
sentence 3 as a subject and in sentence 6 as an object, but is
absent from the rest of the sentences.

Ideally, each entity in the grid should represent an equiv-
alence class of coreferent nouns (e.g., FormerChileandicta-
torAugustoPinochet and Pinochet refer to the same entity).
However, the automatic construction of such an entity grid
requires a robust coreference tool, able to accurately process
texts with coherence violations. Since coreference tools are
typically trained on coherent texts, this requirement is hard to
satisfy. Instead, we employ a simpliﬁed representation: each
noun in a text corresponds to a different entity in the grid. The

simpliﬁcation allows us to capture noun-coreference, albeit
in a shallow manner — only exact repetitions are considered
coreferent. In practice, this means that named entities and
compound nouns will be treated as denoting more than one
entity. For instance, the NP FormerChileandictatorAugusto
Pinochet will be mapped to three entities: dictator, Augusto,
and Pinochet. We further assume that each noun within an
NP bears the same grammatical role as the NP head. Thus, all
three nouns in the above NP will be labeled as objects. When
a noun is attested more than once with a different grammati-
cal role in the same sentence, we default to the role with the
highest grammatical ranking (i.e., s > o > x).

Entity grids can be straightforwardly computed provided
that an accurate parser is available. In the experiments re-
ported throughout this paper we employed Collins’ [1998]
state-of-the-art statistical parser to identify discourse entities
and their grammatical roles. Entities involved in passive con-
structions were identiﬁed using a small set of patterns and
their corresponding deep grammatical role was entered in the
grid (see the grid cell o for Pinochet, Sentence 1, Table 3).
Inference
A fundamental assumption underlying our in-
ference mechanism is that the distribution of entities in coher-
ent texts exhibit certain regularities reﬂected in the topology
of grid columns. Grids of coherent texts are likely to have
some dense columns (i.e., columns with just a few gaps such
as Pinochet in Table 3) and many sparse columns which
will consist mostly of gaps (see London, judge in Table 3).
One would further expect that entities corresponding to dense
columns are more often subjects or objects. These character-
istics will be less pronounced in low-coherence texts.

We deﬁne the coherence of a text T (S1; : : : ; Sn) with en-
tities e1 : : : em as a joint probability distribution that governs
how entities are distributed across document sentences:

Pcoherence(T ) = P(e1 : : : em; S1 : : : Sn)

(1)
We further assume (somewhat simplistically) that an entity is
selected into a document independently of other entities:

Pcoherence(T ) (cid:25)

m(cid:213)

j=1

P(e j; S1 : : : Sn)

(2)

To give a concrete example, we will rate the coherence of the
text in Table 3 by multiplying together the probabilities of the
entities Dictator, Augusto, Pinochet, etc.

We deﬁne P(e j; S1 : : : Sn) as a probability distribution over
transition sequences for entity e j across all n sentences of
text T . P(e j; S1 : : : Sn) is estimated from grid columns, as ob-
served in a corpus of coherent texts:

To compare texts with variable lengths and entities, the
probabilities for individual columns are normalized by col-
umn length (n) and the probability of the entire text is nor-
malized by the number of columns (m):

Pcoherence(T ) (cid:25)

1

m (cid:1) n

m(cid:229)

n(cid:229)

j=1

i=1

log P(ri; jjr(i(cid:0)h); j : : : r(i(cid:0)1); j) (4)

Once the model is trained on a corpus of coherent texts,
it can be used to assess the coherence of unseen texts. Texts
that are given a high probability Pcoherence(T ) will be deemed
more coherent than those given low probability.

Notice that the model is unlexicalized, i.e., the estimation
of P(e j; S1 : : : Sn) is not entity-speciﬁc. In practice, this means
that entities with the same column topology (e.g., London,
October in Table 3) will be given the same probability.

3.2 The Semantic View
Linguistic motivation
An important factor in text com-
prehension is the degree to which sentences and phrases are
linked together. The observation dates back to Halliday and
Hasan [1976] who stressed the role of lexical cohesion in text
coherence. A number of linguistic devices — entity repeti-
tion, synonymy, hyponymy, and meronymy — are considered
to contribute to the “continuity of lexical meaning” observed
in coherent text. Morris and Hirst [1991] represent lexical
cohesion via lexical chains, i.e., sequences of related words
spanning a topical text unit, thus formalizing the intuition that
coherent units will have a high concentration of dense chains.
They argue that the distribution of lexical chains is a surface
indicator of the structure of coherent discourse.
Discourse representation
The key premise behind lex-
ical chains is that coherent texts will contain a high number
of semantically related words. This allows for a particularly
simple representation of discourse that does not take account
of syntactic structure or even word order within a sentence.
Each sentence is thus represented as a bag of words. These
can be all words in the document (excluding function words)
or selected grammatical categories (e.g., verbs or nouns). For
our models, we assume that each sentence is represented by a
set of nouns.

Central to this representation is the ability to measure se-
mantic similarity. Different coherence models can be there-
fore deﬁned according to the chosen similarity measure.
Inference
Measuring local coherence amounts to quan-
tifying the degree of semantic relatedness between sentences.
Speciﬁcally, the coherence of text T is measured by taking
the mean of all individual transitions.

P(e j; S1 : : : Sn) = P(r1; j : : : rn; j)

=

(cid:25)

n(cid:213)
i=1
n(cid:213)
i=1

P(ri; jjr1; j : : : r(i(cid:0)1); j)

P(ri; jjr(i(cid:0)h); j : : : r(i(cid:0)1); j)

where ri; j represents the grammatical role for entity e j in sen-
tence i (see the grid columns in Table 3). The estimates for
P(ri; jjr1; j : : :r(i(cid:0)1); j) are obtained using the standard Markov
assumption of independence, where h is the history size.
Assuming a ﬁrst-order Markov model, P(Pinochet; S1 : : : S6)
will be estimated by multiplying together P(O), P(SjO),
P(–jS), P(Oj–), P(SjO), and P(–jS) (see the column for
Pinochet in Table 3).

(3)

coherence(T ) =

n(cid:0)1(cid:229)
i=1

sim(Si; Si+1)

n (cid:0) 1

(5)

where sim(Si; Si+1) is a measure of similarity between sen-
tences Si and Si+1.

We have experimented with three broad classes of mod-
els which employ word-based, distributional, and taxonomy-
based similarity measures.

In its simplest form, semantic similarity can be operational-

ized in terms of word overlap:

sim(S1; S2) =

2jwords(S1) \ words(S2)j
(jwords(S1)j + jwords(S2)j)

(6)

where words(Si) is the set of words in sentence i. The
main drawback of this measure is that it will indicate low-
coherence for sentence pairs that have no words in common,
even though they may be semantically related.

Measures of distributional similarity, however, go beyond
mere repetition. Words are considered similar if they occur
within similar contexts. The semantic properties of words
are captured in a multi-dimensional space by vectors that are
constructed from large bodies of text by observing the dis-
tributional patterns of co-occurrence with their neighboring
words. For modeling coherence, we want to be able to com-
pare the similarity of sentences rather than words (see (4)).
Our computation of sentence similarity follows the method
described in Foltz [1998]: each sentence is represented by the
mean (centroid) of the vectors of its words, and the similarity
between two sentences is determined by the cosine of their
means.

sim(S1; S2) = cos(µ(~S1); µ(~S2))

µ j(~S1)µ j(~S2)

n(cid:229)
j=1
(µ j(~S1))2s n(cid:229)

j=1

(7)

(µ j(~S2))2

=

s n(cid:229)

j=1

where µ(~Si) = 1
jSij

(cid:229) ~w2Si ~w, and ~w is the vector for word w.

An alternative to inducing word similarity relationships
from co-occurrence statistics in a corpus, is to employ a man-
ually crafted resource such as WordNet [Fellbaum, 1998].
WordNet-based similarity measures have been shown to cor-
relate reliably with human similarity judgments and have
been used in a variety of applications ranging from the de-
tection of malapropisms to word sense disambiguation (see
Budanitsky and Hirst [2001] for an extensive survey). We em-
ployed ﬁve measures commonly cited in the literature. Two
of these measures [Hirst and St-Onge, 1998; Lesk, 1986] de-
ﬁne similarity solely in terms of the taxonomy, whereas the
other three are based on information theory [Resnik, 1995;
Jiang and Conrath, 1997; Lin, 1998] and combine taxonomic
information with corpus counts. By considering a broad range
of these measures, we should be able to distill which ones are
best suited for coherence assessment.

All WordNet-based measures compute a similarity score at
the sense level. We deﬁne the similarity of two sentences S1
and S2 as:

w12S1
w22S2

sim(S1; S2) =

sim(c1; c2)

argmax
c12senses(w1)
c22senses(w2)
jS1jjS2j

(8)

where jSij is the number of words in sentence i. Since the
appropriate senses for words w1 and w2 are not known, our
measure selects the senses which maximize sim(c1; c2).

4 Collecting Coherence Judgments
To comparatively evaluate the coherence models introduced
above, we ﬁrst needed to establish an independent measure of
coherence by eliciting judgments from human participants.
Materials and Design
For optimizing and testing our
models, we created a corpus representative of coherent and
incoherent texts. More speciﬁcally, we elicited coherence

judgments for multi-document summaries produced by sys-
tems and human writers for the Document Understanding
Conference (DUC, 2003). Automatically generated sum-
maries are prone to coherence violations [Mani, 2001] — sen-
tences are often extracted out of context and concatenated to
form a text — and are therefore a good starting point for co-
herence analysis.

We randomly selected 16 input document clusters1 and in-
cluded in our materials six summaries corresponding to each
cluster. One summary was authored by a human, whereas
ﬁve were produced by automatic summarization systems that
participated in DUC. Thus the set of materials contained
6 (cid:1) 16 = 96 summaries. From these, six summaries were re-
served as a development set, whereas the remaining 90 sum-
maries were used for testing (see Section 5).
Procedure and Subjects
Participants ﬁrst saw a set of
instructions that explained the task, deﬁned local coherence,
and provided several examples. Then the summaries were
presented; a new random order of presentation was generated
for each participant. Participants were asked to use a seven
point scale to rate how coherent the summaries were with-
out having seen the source texts. The study was conducted
remotely over the Internet and was completed by 177 unpaid
volunteers (approximately 23 per summary), all native speak-
ers of English.

The judgments were averaged to provide a single rating per
summary and all further analyses were conducted on means.
We report results on inter-subject agreement in Section 5.2.

5 Experiments
All our models were evaluated on the DUC summaries for
which we elicited coherence judgments. We used correlation
analysis to assess the degree of linear relationship between
human ratings and coherence as estimated by our models. In
this section we provide an overview of the parameters we ex-
plored, and present and discuss our results.

5.1 Parameter Estimation
Since the models we evaluated have different training require-
ments (e.g.,
some must be trained on a large corpus and
others are not corpus speciﬁc), it was not possible to have
a uniform training corpus for all models. Here, we give an
overview of the data requirements for our models and explain
how these were met in our experiments.

For the entity-based models, we need corpus data to se-
lect an appropriate history size and to estimate the proba-
bility of various entity transitions. History size was adjusted
using the development corpus described above. Entity tran-
sition probabilities were estimated from a different corpus:
we used 88 human summaries that were produced by DUC
analysts. We assumed that human authored summaries were
coherent and therefore appropriate for obtaining reliable esti-
mates. The grid columns were augmented with the start and
end symbols, increasing the size of the grid cell categories
to six. We estimated the probabilities of individual columns
using n-gram models (see (3)) of variable length (i.e., 2–4)
smoothed with Witten Bell discounting.

1Summaries with ungrammatical sentences were excluded from
our materials, to avoid eliciting judgments on text properties that
were not coherence-related.

(cid:229)
Egrid

Overlap

LSA

HStO

Lesk

JCon

Lin

Egrid
Overlap
LSA
HStO
Lesk
JCon
Lin
Resnik

Humans
:246*
:120
:230*
:322**
:125

(cid:0):341**

:042
:071
:227

(cid:0):290** (cid:0):392**

:485**

:173
:207

:074
(cid:0):003

:013
:093
(cid:0):032

(cid:0):107
:052

:037
:098
:035
:053
(cid:0):063

:380**
:625**
:776**
:746**

*p < :05 (2-tailed)

**p < :01 (2-tailed)

:270*
:421**
:410**

:526**
:606**

:809**

Table 4: Correlation between human ratings and coherence models, measured by Pearson coefﬁcient. Stars indicate the level of
statistical signiﬁcance.

Some of the models based on semantic relatedness pro-
vide a coherence score without presupposing access to cor-
pus data. These models are based on the measures proposed
by Hirst and St-Onge [1998], Lesk [1986] and on word over-
lap. Since they require no prior training, they were directly
computed on a lemmatized version of the test set. Other se-
mantic association models rely on corpus data to either auto-
matically construct representations of lexical meaning [Foltz
et al., 1998] or to populate WordNet’s representations with
frequency counts [Resnik, 1995; Jiang and Conrath, 1997;
Lin, 1998]. We used a lemmatized version of the North Amer-
ican News Text Corpus for this purpose. The corpus contains
350 million words of text taken from a variety of news sources
and is therefore appropriate for modeling the coherence of
news summaries. Vector-based representations were created
using a term-document matrix. We used singular value de-
composition [Berry et al., 1994] to reduce the vector space
to 100 dimensions and thus obtained a semantic space similar
to Latent Semantic Analysis (LSA, Foltz et al. [1998]).

5.2 Results
The evaluation of our coherence models was driven by two
questions: (a) What is the contribution of various linguistic
sources to coherence modeling? and (b) Are the two modeling
frameworks complementary or redundant?
Upper bound
We ﬁrst assessed human agreement on co-
herence judgments. Inter-subject agreement gives an upper
bound for the task and allows us to interpret model perfor-
mance in relation to human performance. To measure agree-
ment, we employed leave-one-out resampling [Weiss and Ku-
likowski, 1991]. The technique correlates the ratings of each
participant with the mean ratings of all other participants. The
average agreement was r = :768.
Model Performance
Table 4 displays the results of cor-
relation analysis on 58 summaries2 from our test set. Sig-
niﬁcant correlations are observed for the entity grid model
(Egrid), the vector-based model (LSA), and two WordNet-
based models, employing Hirst and St-Onge’s [1998] (HStO)
and Jiang and Conrath’s [1997] (JCon) similarity measures.
The signiﬁcant correlation of the Egrid model with human
judgments empirically validates Centering’s claims about the
importance of entity transitions in establishing local coher-
ence. The performance of the LSA model conﬁrms previous

2The rest of the test data —32 summaries —was used for eval-

uating our combined model.

claims in the literature [Foltz et al., 1998] that distributional
similarity is a signiﬁcant predictor of text coherence. The fact
that JCon correlates3 with human judgments is not surprising
either; its performance has been superior to other WordNet-
based measures on two tasks that are particularly relevant
for the problem considered here: modeling human similar-
ity judgments and the automatic detection of malapropisms,
a phenomenon that often results in locally incoherent dis-
courses (see Budanitsky and Hirst [2001] for details). It is
worth considering why HStO performs better than JCon. In
quantifying semantic similarity, HStO uses all available re-
lations in WordNet (e.g., antonymy, meronymy, hyponymy)
and is therefore better-suited to capture cohesive ties arising
from semantically related words than JCon which exploits
solely hyponymy. These results further suggest that the cho-
sen similarity measure plays an important role in modeling
coherence.

The correlations obtained by our models are substan-
tially lower when compared with the inter-subject agreement
of .768. Our results indicate that there is no single method
which captures all aspects of local coherence, although HStO
yields the highest correlation coefﬁcient in absolute terms. It
is worth noting that Egrid is competitive with LSA and the
semantic association models based on WordNet, even though
it is unlexicalized. We conjecture that Egrid makes up for the
lack of lexicalization by taking syntactic information into ac-
count and having a more global perspective of discourse. This
is achieved by explicitly modeling entity transitions spanning
more than two consecutive sentences.

As far as model intercorrelations are concerned, note that
Egrid is not signiﬁcantly correlated with the LSA model, thus
indicating that the two models capture complementary co-
herence properties. Interestingly, LSA displays no correla-
tion with the WordNet-based models. Although both types
of models rely on semantic relatedness, they employ dis-
tinct representations of lexical meaning (word co-occurrence-
based vs. taxonomy-based). Expectedly, the WordNet-based
coherence models are all intercorrelated (see Table 4).
Model Combination
An obvious question is whether a
better ﬁt with the experimental data can be obtained via model
combination. A standard way to integrate different predictors
(i.e., models) is multiple linear regression. Since several of

3The correlation in negative, because JCon is a measure of dis-
similarity; the coefﬁcient therefore has the opposite sign in compar-
ison with the other WordNet-based measures.

the models were intercorrelated, we performed stepwise re-
gression (using forward selection) to determine the best set
of predictors for coherence. The best-ﬁtting combined model
(obtained on the set of 58 summaries) included ﬁve variables:
Egrid, Overlap, LSA, HStO, and Lesk.

Next we tested the combined model’s performance on 32
unseen summaries, the output of two systems different from
those used for assessing the contribution of the individual
models. More speciﬁcally, we obtained a coherence score for
each unseen summary using the combined model and then
compared the estimated values and the human ratings. The
comparison yielded a correlation coefﬁcient of .522 (p < :01)
outperforming any single model. This is a linguistically richer
model which integrates several interrelated aspects of coher-
ence (both syntactic and semantic) such as repetition (Over-
lap), syntactic prominence (Egrid), and semantic association
(LSA, HStO, Lesk).

6 Conclusions
In this paper we have compared and contrasted two main
frameworks for representing and measuring text coherence.
Our syntactic framework operationalizes Centering’s notion
of local coherence using entity grids. We argued that this rep-
resentation is particularly suited for uncovering entity tran-
sition types typical of coherent and incoherent texts and in-
troduced a model that quantitatively delivers this assessment.
Our semantic framework capitalized on the notion of sim-
ilarity between sentences. We experimented with a variety
of similarity measures employing different representations of
lexical meaning: word-based, distributional, and taxonomy-
based. Our experiments revealed that the two modeling ap-
proaches are complementary: our best model retains aspects
of entity coherence as well as semantic relatedness.

The modeling approach taken in this paper relies on shal-
low text properties and is relatively inexpensive (assuming
access to a taxonomy and a parser). This makes the proposed
models particularly attractive for the automatic evaluation of
machine generated summaries. An important future direction
lies in the development of a lexicalized version of the entity-
grid model that combines the beneﬁts of grid column topol-
ogy and semantic similarity. Further investigations on differ-
ent languages, text types, and genres will test the generality
and portability of our models. We will also examine whether
additional linguistic knowledge (e.g., coreference resolution,
causality) will result in improved performance.

Acknowledgments
The authors acknowledge the support of EPSRC (Lapata;
grant GR/T04540/01) and the National Science Founda-
tion (Barzilay; CAREER grant IIS-0448168). Thanks to Eli
Barzilay, Frank Keller, Smaranda Muresan, Kevin Simler,
Caroline Sporleder, Chao Wang, and Bonnie Webber for help-
ful comments and suggestions.

References
[Bangalore et al., 2000] Srinivas Bangalore, Owen Rambow, and
In Pro-

Steven Whittaker. Evaluation metrics for generation.
ceedings of the INLG, pages 1–8, 2000.

[Berry et al., 1994] Michael W. Berry, Susan T. Dumais, and
Gavin W. O’Brien. Using linear algebra for intelligent informa-
tion retrieval. SIAM Review, 37(4):573–595, 1994.

[Budanitsky and Hirst, 2001] Alexander Budanitsky and Graeme
Semantic distance in WordNet: An experimental,
Hirst.
In Proceed-
application-oriented evaluation of ﬁ ve measures.
ings of ACL Worskhop on WordNet and Other Lexical Resources,
pages 29–34, 2001.

[Collins, 1998] Michael Collins. Head-driven Statistical Models
for Natural Language Parsing. PhD thesis, University of Penn-
sylvania, 1998.

[Fellbaum, 1998] Christiane Fellbaum, editor. WordNet: An Elec-

tronic Database. MIT Press, 1998.

[Foltz et al., 1998] Peter W. Foltz, Walter Kintsch, and Thomas K.
Landauer. Textual coherence using latent semantic analysis. Dis-
course Processes, 25(2&3):285–307, 1998.

[Grosz et al., 1995] Barbara Grosz, Aravind K. Joshi, and Scott
Weinstein. Centering: A framework for modeling the local co-
herence of discourse. Computational Linguistics, 21(2):203–225,
1995.

[Halliday and Hasan, 1976] M. A. K. Halliday and Ruqaiya Hasan.

Cohesion in English. Longman, 1976.

[Higgins et al., 2004] Derrick Higgins, Jill Burstein, Daniel Marcu,
and Claudia Gentile. Evaluating multiple aspects of coherence
in student essays. In Proceedings of the NAACL, pages 185-192,
2004.

[Hirst and St-Onge, 1998] Graeme Hirst and David St-Onge. Lexi-
cal chains as representations of context for the detection and cor-
rrection of malapropisms. In Christiane Fellbaum, editor, Word-
Net: An Electronic Lexical Database and Some of its Applica-
tions, pages 305–332. MIT Press, 1998.

[Hovy and Lin, 2003] Eduard Hovy and Chin-Yew Lin. Automatic
evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the HLT/NAACL, pages 71–78, 2003.

[Jiang and Conrath, 1997] Jay J. Jiang and David W. Conrath. Se-
mantic similarity based on corpus statistics and lexical taxonomy.
In Proceedings of ROCLING, 1997.

[Lesk, 1986] Michael Lesk. Automatic sense disambiguation: How
to tell a pine cone from an ice cream cone.
In Proceedings of
the 1986 Special Interest Group in Documentation, pages 24–26,
1986.

[Lin, 1998] Dekang Lin. An information-theoretic deﬁnition of
In Proceedings of the ICML, pages 296–304, 1998.

similarity.

[Mani, 2001] Inderjeet Mani. Automatic Summarization. John Ben-

jamins Pub Co, 2001.

[Miltsakaki and Kukich, 2004] Eleni Miltsakaki and Karen Kukich.
Evaluation of text coherence for electronic essay scoring systems.
natural language engineering. Natural Language Engineering,
10(1):25–55, 2004.

[Morris and Hirst, 1991] Jane Morris and Graeme Hirst. Lexical
cohesion computed by thesaural relations as an indicator of the
structure of text. Computational Linguistics, 1(17):21–43, 1991.
[Papineni et al., 2002] Kishore Papineni, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. BLUE: a method for automatic eval-
uation of machine translation. In Proceedings of the ACL, pages
311–318, 2002.

[Resnik, 1995] Philip Resnik. Using information content to eval-
uate semantic similarity.
In Proceedings of 14th International
Joint Conference on Artiﬁcial Intelligence, pages 448–453, 1995.
[Walker et al., 1998] Marilyn A. Walker, Arvind K. Joshi, and
Ellen F. Prince, editors. Centering Theory in Discourse. Claren-
don Press, Oxford, 1998.

[Weiss and Kulikowski, 1991] Sholom M. Weiss and Casimir A.
Kulikowski. Computer Systems that Learn: Classiﬁcation and
Prediction Methods from Statistics, Neural Nets, Machine Learn-
ing, and Expert Systems. Morgan Kaufmann, 1991.


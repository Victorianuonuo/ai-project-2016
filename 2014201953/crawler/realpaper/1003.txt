Word Sense Disambiguation with

Spreading Activation Networks Generated from Thesauri

George Tsatsaronis1∗

, Michalis Vazirgiannis1,2† and Ion Androutsopoulos1

1Department of Informatics, Athens University of Economics and Business, Greece

2GEMO Team, INRIA/FUTURS, France

Abstract

Most word sense disambiguation (WSD) meth-
ods require large quantities of manually annotated
training data and/or do not exploit fully the se-
mantic relations of thesauri. We propose a new
unsupervised WSD algorithm, which is based on
generating Spreading Activation Networks (SANs)
from the senses of a thesaurus and the relations be-
tween them. A new method of assigning weights to
the networks’ links is also proposed. Experiments
show that the algorithm outperforms previous un-
supervised approaches to WSD.

1 Introduction

Word Sense Disambiguation (WSD) aims to assign to every
word of a document the most appropriate meaning (sense)
among those offered by a lexicon or a thesaurus. WSD is im-
portant in natural language processing and text mining tasks,
such as machine translation, speech processing, information
retrieval, and document classiﬁcation. A wide range of WSD
algorithms and techniques has been developed, utilizing ma-
chine readable dictionaries, statistical and machine learning
methods, even parallel corpora. In [Ide and Veronis, 1998]
several approaches are surveyed; they address WSD either in
a supervised manner, utilizing existing manually-tagged cor-
pora, or with unsupervised methods, which sidestep the te-
dious stage of constructing manually-tagged corpora.

The expansion of existing, and the development of new
word thesauri has offered powerful knowledge to many text
processing tasks. For example, exploiting is-a relations (e.g.
hypernyms/hyponyms) between word senses leads to im-
proved performance both in text classiﬁcation [Mavroeidis et
al., 2005] and retrieval [Voorhees, 1993]. Semantic links be-
tween senses, as derived from a thesaurus, are thus impor-
tant, and they have been utilized in many previous WSD ap-
proaches, to be discussed below. Previous WSD approaches,
however, have not considered the full range of semantic links
between senses in a thesaurus. In [Banerjee and Pedersen,

∗

Partially funded by the PENED 2003 Programme of the EU and

the Greek General Secretariat for Research and Technology.

†

Supported by the Marie Curie Intra-European Fellowship.

2003] a larger subset of semantic relations compared to pre-
vious approaches was used, but antonymy, domain/domain
terms and all inter-POS relations were not considered.

In this paper we propose a new WSD algorithm, which
does not require any training, in order to explore the afore-
mentioned potential. A word thesaurus is used to construct
Spreading Activation Networks (SANs), initially introduced
in WSD by Quillian [1969].
In our experiments we used
WordNet [Fellbaum, 1998], though other thesauri can also
be employed. The innovative points of this new WSD algo-
rithm are: (a) it explores all types of semantic links, as pro-
vided by the thesaurus, even links that cross parts of speech,
unlike previous knowledge-based approaches, which made
use of mainly the “is-a” and “has-part” relations; (b) it in-
troduces a new method for constructing SANs for the WSD
task; and (c) it introduces an innovative weighting scheme for
the networks’ edges, taking into account the importance of
each edge type with respect to the whole network. We show
experimentally that our method achieves the best reported ac-
curacy taking into account all parts of speech on a standard
benchmark WSD data set, Senseval 2 [Palmer et al., 2001].

The rest of the paper is organized as follows. Section 2 pro-
vides preliminary information concerning the use of semantic
relations and SANs in the WSD task. Section 3 discusses our
SAN construction method and edge weighting scheme. Sec-
tion 4 presents our full WSD method, and Section 5 experi-
mental results and analysis. Section 6 discusses related WSD
approaches, and Section 7 concludes.

2 Background

2.1 Semantic Relations in WordNet

WordNet is a lexical database containing English nouns,
verbs, adjectives and adverbs, organized in synonym sets
the terms senses and synsets are
(synsets). Hereafter,
used interchangeably.
Synsets are connected with vari-
ous edges, representing semantic relations among them,
and the latest WordNet versions offer a rich set of
such links: hypernymy/hyponymy, meronymy/holonymy,
synonymy/antonymy, entailment/causality, troponymy, do-
main/domain terms, derivationally related forms, coordinate
terms, attributes, and stem adjectives. Several relations cross
parts of speech, like the domain terms relation, which con-
nects senses pertaining to the same domain (e.g. light, as a

IJCAI-07

1725

noun meaning electromagnetic radiation producing a visual
sensation, belongs to the domain of physics), and the at-
tributes relation, which connects a word with its possible val-
ues (e.g. light, as a noun, can be bright or dull). Our method
utilizes all of the aforementioned semantic relations.

2.2 WSD Methods that Exploit Semantic Relations
Several WSD approaches capitalize on the fact that thesauri
like WordNet offer important vertical (“is-a”, “has-part”)
and horizontal (synonym, antonym, coordinate terms) se-
mantic relations. Some examples of these approaches are
[Sussna, 1993; Rigau et al., 1997; Leacock et al., 1998;
Mihalcea et al., 2004; Mavroeidis et al., 2005; Montoyo et
al., 2005]. Most of the existing WSD approaches, however,
exploit only a few of the semantic relations that thesauri pro-
vide, mostly synonyms and/or hypernyms/hyponyms. For ex-
ample Patwardhan et al. [2003] and Banerjee and Pedersen
[2003] focus mostly on the “is-a” hierarchy of nouns, thus ig-
noring other intra- and all inter-POS relations. In contrast, our
method takes into account all semantic relations and requires
no training data other than the thesaurus.

2.3 Previous Use of SANs in WSD
Spreading Activation Networks (SANs) have already been
used in information retrieval [Crestani, 1997] and in text
structure analysis [Kozima and Furugori, 1993]. Since their
ﬁrst use in WSD by Quillian [1969], several others [Cotrell
and Small, 1983; Bookman, 1987] have also used them in
WSD, but those approaches required rather ad hoc hand-
encoded sets of semantic features to compute semantic sim-
ilarities. The most recent attempt to use SANs in WSD that
we are aware of, overcoming the aforementioned drawback,
is [Veronis and Ide, 1990].

Figure 1 illustrates how SANs were applied to WSD by
Veronis and Ide. Let W1 and W2 be two words that co-occur
(e.g. in a sentence or text) and that we want to disambiguate.
They constitute the network nodes (word nodes) depicted in
the initial phase of Figure 1; more generally, there would be
n word nodes, corresponding to the n words of the text frag-
ment. Next, all relevant senses of W1 and W2 are retrieved
from a machine readable dictionary (MRD), and are added
as nodes (sense nodes) to the network. Each word is con-
nected to all of its senses via edges with positive weights
(activatory edges). The senses of the same word (e.g. S11
and S12) are connected to each other with edges of negative
weight (inhibitory edges). This is depicted as phase 1 in Fig-
ure 1. Next, the senses’ glosses are retrieved, tokenized, and
reduced to their lemmas (base forms of words). Stop-words
are removed. Each gloss word (GW) is added as a node to
the network, and is connected via activatory links to all sense
nodes that contain it in their glosses (phase 2). The possible
senses of the gloss words are retrieved from the MRD and
added to the network. The network continues to grow in the
same manner, until nodes that correspond to a large part or
the whole of the thesaurus have been added. Note that each
edge is bi-directional, and each direction can have a different
weight.

Once the network is constructed, the initial word nodes
are activated, and their activation is spread through the net-

Index:

W1

W2

W1

W2

S.1.1

S.2.1

Initial Phase

S.1.2

S.2.2

Phase 1

= Word Node

= Sense Node

= Activatory Link

= Inhibitory Link

S.1.1

W1

S.1.2

GW
1.1.1
...
GW
1.1.n

GW
1.2.1
... 
GW
1.2.n

GW
2.1.1
...
GW
2.1.n

GW
2.2.1
... 
GW
2.2.n

Phase 2

S.2.1

S.2.2

W2

Figure 1: A previous method to generate SANs for WSD.

work according to a spreading strategy, ensuring that even-
tually only one sense node per initial word node will have a
positive activation value, which is taken to be the sense the
algorithm assigns to the corresponding word. Note that this
approach assumes that all occurrences of the same word in the
text fragment we apply the algorithm to have the same sense,
which is usually reasonable, at least for short fragments like
sentences or paragraphs.

We use a different method to construct the network, ex-
plained below. Note also that Veronis and Ide do not use
direct sense-to-sense relations; in contrast, we use all such
relations, as in section 2.1, that exist in the thesaurus. More-
over they weigh all the activatory and inhibitory edges with 1
and −1, whereas we propose a new weighting scheme, taking
into account the importance of each edge type in the network.

3 SAN Creation
In this section we propose a new method to construct SANs
for the WSD task, along with a new weighting scheme for
the edges. Our algorithm disambiguates a given text sentence
by sentence. We only consider the words of each sentence
that are present in the thesaurus, in our case WordNet. We
also assume that the words of the text have been tagged with
their parts of speech (POS). For each sentence, a SAN is con-
structed as shown in Figure 2. For simplicity, in this exam-
ple we kept only the nouns of the input sentence, though the
method disambiguates all parts of speech. The sentence is
from the d00 ﬁle in Senseval 2 data set:

“If both copies of a certain gene were knocked out, be-
nign polyps would develop.”

To construct the SAN, initially the word nodes, in our case
the nouns copies, gene and polyps, along with their senses are
added to the network, as shown in the initial phase of Figure
2. The activatory and inhibitory links are then added, as in the
previous section, but after this point the SAN grows in a very
different manner compared to Veronis and Ide. First, all the
senses of the thesaurus that are directly linked to the existing
senses of the SAN via any semantic relation are added to the
SAN, along with the corresponding links, as shown in expan-
sion round 1 of Figure 2. Every edge is bi-directional, since

IJCAI-07

1726

S.1.1

S.1.2
...
S.1.4

copies

gene

S.2.1

S.3.1

S.3.2

Synonym

S.1.1

S.1.2
...

S.1.4

...

polyps

...
S.2.1

Holonym

S.3.1

Attribute

...

Hypernym

...

Antonym

...

...

...

S.3.2

...

We use the function of equation 3, which incorporates fan-out
and distance factors to constrain the activation spreading; τ is
a threshold value.

(cid:3)

Oj (p) =

0
Fj
p+1 · Aj(p)

, if Aj(p) < τ
, otherwise

(3)

Initial Phase

Expansion Round 1

Index:

= Word Node

= Sense Node

= Activatory Link

= Inhibitory Link

Figure 2: Our method to construct SANs.

the semantic relations, at least in WordNet, are bi-directional
(e.g. if S1 is a hypernym of S2, S2 is a hyponym of S1). In
the next expansion round, the same process continues for the
newly added sense nodes of the previous round. The net-
work ceases growing when there is a path between every pair
of the initial word nodes. Then the network is considered as
connected. If there are no more senses to be expanded and the
respective SAN is not connected, we cannot disambiguate the
words of that sentence, losing in coverage. Note that when
adding synsets, we use breadth-ﬁrst search with a closed set,
which guarantees we do not get trapped into cycles.

3.1 The Spreading Activation Strategy
The procedure above leads to networks with tens of thousands
of nodes, and almost twice as many edges. Since each word
is eventually assigned its most active sense, great care must
be taken in such large networks, so that the activation is efﬁ-
ciently constrained, instead of spreading all over the network.
Our spreading activation strategy consists of iterations.
The nodes initially have an activation level 0, except for the
input word nodes, whose activation is 1. In each iteration,
every node propagates its activation to its neighbors, as a
function of its current activation value and the weights of the
edges that connect it with its neighbors. We adopt the acti-
vation strategy introduced by Berger et al. [2004], modifying
it by inserting a new scheme to weigh the edges, which is
discussed in section 3.2. More speciﬁcally, at each iteration
p every network node j has an activation level Aj(p) and an
output Oj (p), which is a function of its activation level, as
shown in equation 1.

Oj(p) = f (Aj(p))

(1)

The output of each node affects the next-iteration activation
level of any node k towards which node j has a directed edge.
Thus, the activation level of each network node k at itera-
tion p is a function of the output, at iteration p − 1, of every
neighboring node j having a directed edge ejk, as well as a
function of the edge weight Wjk, as shown in equation 2.
Although this process is similar to the activation spreading of
feed-forward neural networks, the reader should keep in mind
that the edges of SANs are bi-directional (for each edge, there
exists a reciprocal edge). A further difference is that no train-
ing is involved in the case of SANs.

(cid:2)

Ak(p) =

j

Oj(p − 1) · Wjk

(2)

Unless a function for the output O is chosen carefully, after a
number of iterations the activation ﬂoods the network nodes.

Equation 3 prohibits the nodes with low activation levels from
p+1 dimin-
inﬂuencing their neighboring nodes. The factor
ishes the inﬂuence of a node to its neighbors as the iterations
progress (intuitively, as “pulses” travel further). Function Fj
is a fan-out factor, deﬁned in equation 4. It reduces the inﬂu-
ence of nodes that connect to many neighbors.

1

Fj = (1 −

Cj
CT

)

(4)

CT is the total number of nodes, and Cj is the number of
nodes directly connected to j via directed edges from j.

3.2 Assigning Weights to Edges
In information retrieval, a common way to measure a token’s
importance in a document is to multiply its term frequency
in the document (TF) with the inverse (or log-inverse) of its
document frequency (IDF), i.e. with the number of docu-
ments the token occurs in. To apply the same principle to the
weighting of SAN edges, we consider each node of a SAN
as corresponding to a document, and each type of edge (each
kind of semantic relation) as corresponding to a token.

Initially each edge of the SAN is assigned a weight of −1
if it is inhibitory (edges representing antonymy and compet-
ing senses of the same word), or 1 if it is activatory (all other
edges). Once the network is constructed, we multiply the ini-
tial weight wkj of every edge ekj with the following quantity:

ETF (ekj) · INF (ekj )

(5)

ETF, deﬁned in equation 6, is the edge type frequency, the
equivalent of TF. It represents the percentage of the outgoing
edges of k that are of the same type as ekj . When comput-
ing the edge weights, edges corresponding to hypernym and
hyponym links are considered of the same type, since they
are symmetric. The intuition behind ETF is to promote edges
whose type is frequent among the outgoing edges of node k,
because nodes with many edges of the same type are more
likely to be hubs for the semantic relation that corresponds to
that type.

ETF (ekj ) =

|{eki|type(eki) = type(ekj)}|

|{eki}|

(6)

The second factor in equation 5, deﬁned in equation 7, is the
inverse node frequency (INF), inspired by IDF. It is the fre-
quency of ekj ’s type in the entire SAN.

INF (ekj) = log

N + 1

Ntype(ekj )

(7)

N is the total number of nodes in the SAN, and Ntype(ekj )
is the number of nodes that have outgoing edges of the same
type as ekj . As in IDF, the intuition behind INF is that we
want to promote edges of types that are rare in the SAN.

IJCAI-07

1727

4 The WSD Algorithm

Our WSD algorithm consists of four steps. Given a POS-
tagged text, a designated set of parts of speech to be disam-
biguated, and a word thesaurus:
Step 1: Fragment the text into sentences, and select the words
having a part of speech from the designated set. For each sen-
tence repeat steps 2 to 4.
Step 2: Build a SAN, according to section 3. If the SAN is not
connected, even after expanding all available synsets, abort
the disambiguation of the sentence.
Step 3: Spread the activation iteratively until all nodes are in-
active.1 For every word node, store the last active sense node
with the highest activation.2
Step 4: Assign to each word the sense corresponding to the
sense node stored in the previous step.

5 Experimental Evaluation

We evaluated our algorithm on a major benchmark WSD data
set, namely Senseval 2 in the “English all words” task. The
data set is annotated with senses of WordNet 2. We exper-
imented with all parts of speech, to be compatible with all
published results of Senseval 2 [Palmer et al., 2001]. Table 1
shows the number of occurrences of polysemous and monose-
mous words of WordNet 2 in the data set we used, per POS,
as well as the average polysemy.

Monosemous
Polysemous
Av. Polysemy

Nouns Verbs Adj. Adv. Total
464
1839

260
813

33
502

80
352

91
172

4

9

3

3

5

method is proposed and is evaluated on Senseval 2; it uses
thesauri-generated semantic networks, along with Pagerank
for their processing. We also report the accuracy of the best
reported unsupervised method that participated in the Sen-
seval 2 “English all words” task, presented in [Litkowski,
2001].

5.2 Performance of the Methods

Table 2 presents the accuracy of the six WSD methods, on the
three ﬁles of Senseval 2. The presented accuracy corresponds
to full coverage, and hence recall and precision are both equal
to accuracy. The results in Table 2 suggest that our method
outperforms that of Veronis and Ide, the hybrid method, and
the random baseline. Moreover, our method achieved higher
accuracy than the best unsupervised method that participated
in Senseval 2, and overall slightly lower accuracy than the
reported results of [Mihalcea et al., 2004].

y
c
a
r
u
c
c
A

0.400

0.380

0.360

0.340

0.320

0.300

0.280

0.260

0.240

0.220

0.279

0.259

0.239

0.387

0.365

0.343

0.368

0.346

0.325

0.329

0.308

0.287

Baseline

Synsets+Glosses

Veronis & Ide

Synsets

Table 1: Occurrences of polysemous and monosemous words
of WordNet 2 in Senseval 2.

Figure 3: Accuracy on polysemous words and the respective
0.95 conﬁdence intervals.

5.1 Methods Compared
In order to compare our WSD method to that of [Veronis and
Ide, 1990], we implemented the latter and evaluated it on Sen-
seval 2, again using WordNet 2. We also included in the com-
parison the baseline for unsupervised WSD methods, i.e. the
assignment of a random sense to each word. For the baseline,
the mean average of 10 executions is reported. Moreover, in
order to evaluate the possibility of including glosses in our
method, instead of only synset-to-synset relations, we imple-
mented a hybrid method which utilizes both, by adding to our
SANs the gloss words of the synsets along with their senses,
similarly to the method of Veronis and Ide. For the purposes
of this implementation, as well as for the implementation of
the original method of Veronis and Ide, we used the Extended
WordNet [Moldovan and Rus, 2001], which provides the POS
tags and lemmas of all WordNet 2 synset glosses. In the com-
parison, we also include the results presented in [Mihalcea
et al., 2004]. There, another unsupervised knowledge-based

1In equation 3, Fj · Aj(p) is bounded and, hence, as p increases,

eventually all nodes become inactive.

2If there is more than one sense node with this property per word,

we select randomly. This never occurred in our experiments.

Figure 3 shows the corresponding overall results for the
four methods we implemented, when accuracy is computed
only on polysemous words, i.e. excluding trivial cases, along
with the corresponding 0.95 conﬁdence intervals. There is
clearly a statistically signiﬁcant advantage of our method
(Synsets) over both the baseline and the method of Vero-
nis and Ide. Adding WordNet’s glosses to our method
(Synsets+Glosses) does not lead to statistically signiﬁcant
difference (overlapping conﬁdence intervals), and hence our
method without glosses is better, since it is simpler and re-
quires lower computational cost, as shown in section 5.3. The
decrease in performance when adding glosses is justiﬁed by
the fact that many of the glosses’ words are not relevant to
the senses the glosses express, and thus the use of glosses
introduces irrelevant links to the SANs.

Figure 3 does not show the corresponding results of Mihal-
cea et al.’s method, due to the lack of corresponding published
results; the same applies to the best unsupervised method of
Senseval 2. We note that in the results presented by Mihalcea
et al., there is no allusion to the variance in the accuracy of
their method, which occurs by random assignment of senses
to words that could not be disambiguated, nor to the num-
ber of these words. Thus no direct and clear statement can
be made regarding their reported accuracy. In Figure 4 we

IJCAI-07

1728

Words

SAN

SAN Glosses

SAN

Baseline

Mono

Poly

Synsets Veronis and Ide

Synsets+Glosses

File 1 (d00)
File 2 (d01)
File 3 (d02)

Overall

103
232
129
464

552
724
563
1839

0.4595
0.4686
0.5578
0.4928

0.4076
0.4592
0.4682
0.4472

0.4396
0.4801
0.5115
0.4780

0.3651
0.4211
0.4303
0.4079

Best Unsup.
Pagerank
Senseval 2 Mihalcea

unavailable
unavailable
unavailable

0.4510

0.4394
0.5446
0.5428
0.5089

Table 2: Overall and per ﬁle accuracy on the Senseval 2 data set.

compare the accuracy of our method against Mihalcea et al.’s
on each Senseval 2 ﬁle. In this case we included all words,
monosemous and polysemous, because we do not have results
for Mihalcea et al.’s method on polysemous words only; the
reader should keep in mind that these results are less infor-
mative than the ones of Figure 3, because they do not exclude
monosemous words. There is an overlap between the two

y
c
a
r
u
c
c
A

0.580

0.530

0.480

0.430

0.380

0.498

0.460

0.484

0.439

0.421

0.395

0.595

0.586

0.558

0.545

0.521

0.500

0.579

0.545

0.510

0.500

0.469

0.437

Synsets File 1

Synsets File 2

Synsets File 3

Mihalcea et al. File 1

Mihalcea et al. File 2

Mihalcea et al. File 3

Figure 4: Accuracy on all words and the respective 0.95 con-
ﬁdence intervals.

conﬁdence intervals for 2 out of 3 ﬁles, and thus the differ-
ence is not always statistically signiﬁcant.

Regarding the best unsupervised method that participated
in Senseval 2, we do not have any further information apart
from its overall accuracy, and therefore we rest on our advan-
tage in accuracy reported in Table 2. Finally, we note that to
evaluate the signiﬁcance of our weighting, we also executed
experiments without taking it into account in the WSD pro-
cess. The accuracy in this case drops by almost 1%, and the
difference in accuracy between the resulting version of our
method and the method of Veronis and Ide is no longer sta-
tistically signiﬁcant, which illustrates the importance of our
weighting. We have also conducted experiments in Senseval
3, where similar results with statistically signiﬁcant differ-
ences were obtained: our method achieved an overall accu-
racy of 46% while Ide and Veronis achieved 39,7%. Space
does not allow further discussion of our Senseval 3 experi-
ments.

5.3 Complexity and Actual Computational Cost
Let k be the maximum branching factor (maximum number
of edges per node) in a word thesaurus, l the maximum path
length, following any type of semantic link, between any two

nodes, and n the number of words to be disambiguated. Since
we use breadth-ﬁrst search, the computational complexity of
constructing each SAN (network) is O(n · kl+1). Further-
more, considering the analysis of constrained spreading acti-
vation in [Rocha et al., 2004], the computational complexity
of spreading the activation is O(n2 · k2l+3). The same com-
putational complexity ﬁgures apply to the method of Veronis
and Ide, as well as to the hybrid one, although k and l differ
across the three methods. These ﬁgures, however, are worst
case estimates, and in practice we measured much lower com-
putational cost.
In order to make the comparison of these
three methods more concrete with respect to their actual com-
putational cost, Table 3 shows the average numbers of nodes,
edges, and iterations per network (sentence) for each method.
Moreover, the average CPU time per network is shown (in
seconds), which includes both network construction and ac-
tivation spreading. The average time for the SAN Synsets
method to disambiguate a word was 1.37 seconds. Table 3

SAN

SAN Glosses

SAN

Synsets

Veronis and Ide

Synsets+Glosses

Nodes/Net.
Edges/Net.
Pulses/Net.
Sec./Net.

10,643.74
13,164.84

166.93
13.21

6,575.13
34,665.53

28.64
3.35

9,406.04
37,181.64

119.15
19.71

Table 3: Average actual computational cost.

shows that our method requires less CPU time than the hy-
brid method, with which there is no statistically signiﬁcant
difference in accuracy; hence, adding glosses to our method
clearly has no advantage. The method of Veronis and Ide has
lower computational cost, but this comes at the expense of a
statistically signiﬁcant deterioration in performance, as dis-
cussed in Section 5.2. Mihalcea et al. provide no comparable
measurements, and thus we cannot compare against them; the
same applies to the best unsupervised method of Senseval 2.

6 Related Work

The majority of the WSD approaches proposed in the past
deal only with nouns, ignoring other parts of speech. Some
of those approaches [Yarowsky, 1995; Leacock et al., 1998;
Rigau et al., 1997] concentrate on a set of a few pre-selected
words, and in many cases perform supervised learning. In
contrast, our algorithm requires no training, nor hand-tagged
or parallel corpora, and it can disambiguate all the words in a
text, sentence by sentence. Though WordNet was used in our
experiments, the method can also be applied using LDOCE

IJCAI-07

1729

or Roget’s thesaurus. Kozima and Furugori [1993] provide
a straightforward way of extracting and using the semantic
relations in LDOCE, while Morris and Hirst [1991] present
a method to extract semantic relations between words from
Roget’s thesaurus.

7 Conclusions

We have presented a new unsupervised WSD algorithm,
which utilizes all types of semantic relations in a word the-
saurus. The algorithm uses Spreading Activation Networks
(SANs), but unlike previous WSD work it creates SANs tak-
ing into account all sense-to-sense relations, rather than re-
lations between senses and glosses, and it employs a novel
edge-weighting scheme. The algorithm was evaluated on
Senseval 2 data, using WordNet as the thesaurus, though it
is general enough to exploit other word thesauri as well. It
outperformed: (i) the most recent SAN-based WSD method,
which overcame the problems older approaches faced, and
(ii) the best unsupervised WSD method that participated in
Senseval 2. It also matched the best WSD results that have
been reported on the same data.

References

[Banerjee and Pedersen, 2003] S. Banerjee and T. Pedersen.
Extended gloss overlaps as a measure of semantic relat-
edness. In Proc. of IJCAI-03, pages 805–810, Acapulco,
Mexico, 2003.

[Berger et al., 2004] H. Berger, M. Dittenbach,

and
D. Merkl. An adaptive information retrieval system based
on associative networks. In Proc. of the 1st Asia-Paciﬁc
Conference on Conceptual Modelling, pages 27–36,
Dunedin, New Zealand, 2004.

[Bookman, 1987] L. Bookman.

scheme for modelling semantics.
pages 611–614, Milan, Italy, 1987.

A microfeature based
In Proc. of IJCAI-87,

[Cotrell and Small, 1983] G. Cotrell and S. Small. A connec-
tionist scheme for modelling word sense disambiguation.
Cognition and Brain Theory, 6:89–120, 1983.

[Crestani, 1997] F. Crestani. Application of spreading acti-
vation techniques in information retrieval. Artiﬁcial Intel-
ligence Review, 11:453–482, 1997.

[Fellbaum, 1998] C. Fellbaum. WordNet – an electronic lex-

ical database. MIT Press, 1998.

[Ide and Veronis, 1998] N. M. Ide and J. Veronis. Word
sense disambiguation: the state of the art. Computational
Linguistics, 24(1):1–40, 1998.

[Kozima and Furugori, 1993] H. Kozima and T. Furugori.
Similarity between words computed by spreading activa-
tion on an english dictionary. In Proc. of EACL-93, pages
232–239, Utrecht, The Netherlands, 1993.

[Leacock et al., 1998] C. Leacock, M. Chodorow, and G. A.
Miller. Using corpus statistics and WordNet relations
for sense identiﬁcation.
Computational Linguistics,
24(1):147–165, 1998.

[Litkowski, 2001] K. Litkowski. Use of machine readable
dictionaries for word-sense disambiguation in senseval-2.
In Senseval-2, pages 107–110, Toulouse, France, 2001.

[Mavroeidis et al., 2005] D. Mavroeidis, G. Tsatsaronis,
M. Vazirgiannis, M. Theobald, and G. Weikum. Word
sense for exploiting hierarchical thesauri in text classiﬁ-
cation. In Proc. of PKDD-05, pages 181–192, Porto, Por-
tugal, 2005.

[Mihalcea et al., 2004] R. Mihalcea, P. Tarau, and E. Figa.
PageRank on semantic networks, with application to word
sense disambiguation. In Proc. of COLING-04, Geneva,
Switzerland, 2004.

[Moldovan and Rus, 2001] D. Moldovan and V. Rus. Ex-
In Proc. of

plaining answers with Extended WordNet.
ACL-01, Toulouse, France, 2001.

[Montoyo et al., 2005] A. Montoyo, A. Suarez, G. Rigau,
and M. Palomar. Combining knowledge- and corpus-based
word sense disambiguation methods. Journal of Artiﬁcial
Intelligence Research, 23:299–330, 2005.

[Morris and Hirst, 1991] J. Morris and G. Hirst. Lexical co-
hesion computed by thesaural relations as an indicator of
the structure of text. Comput. Linguistics, 17:21–48, 1991.
[Palmer et al., 2001] M. Palmer, C. Fellbaum, and S. Cot-
ton. English tasks: All-words and verb lexical sample.
In Senseval-2, pages 21–24, Toulouse, France, 2001.

[Patwardhan et al., 2003] S. Patwardhan, S. Banerjee, and
T. Pedersen. Using measures of semantic relatedness for
word sense disambiguation.
In Proc. of CICLING-03,
pages 241–257, Mexico City, Mexico, 2003.

[Quillian, 1969] R. M. Quillian. The teachable language
comprehender: a simulation program and theory of lan-
guage. Communications of ACM, 12(8):459–476, 1969.

[Rigau et al., 1997] G. Rigau, J. Atserias, and E. Agirre.
Combining unsupervised lexical knowledge methods for
word sense disambiguation.
In Proc. of ACL/EACL-97,
pages 48–55, Madrid, Spain, 1997.

[Rocha et al., 2004] C. Rocha, D. Schwabe, and M. Poggi
de Aragao. A hybrid approach for searching in the Se-
mantic Web. In Proc. of WWW-04, pages 374–383, New
York, NY, 2004.

[Sussna, 1993] M. Sussna. Word sense disambiguation for
free-text indexing using a massive semantic network. In
2nd International Conference on Information and Knowl-
edge Management, pages 67–74, November 1993.

[Veronis and Ide, 1990] J. Veronis and N. M. Ide. Word
sense disambiguation with very large neural networks ex-
tracted from machine readable dictionaries.
In Proc. of
COLING-90, pages 389–394, Helsinki, Finland, 1990.

[Voorhees, 1993] E. M. Voorhees. Using WordNet to disam-
In Proc. of ACM

biguate word senses for text retrieval.
SIGIR-93, pages 171–180, Pittsburgh, PA, 1993.

[Yarowsky, 1995] D. Yarowsky. Unsupervised word sense
disambiguation rivaling supervised methods. In Proc. of
ACL-95, pages 189–196, Cambridge, MA, 1995.

IJCAI-07

1730


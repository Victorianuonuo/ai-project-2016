Graph-Based Semi-Supervised Learning as a Generative Model 

Jingrui He               Jaime Carbonell               Yan Liu 

Carnegie Mellon University 
School of Computer Science 

5000 Forbes Avenue, Pittsburgh 15213 

jingruih@cs.cmu.edu           jgc@cs.cmu.edu          yanliu@cs.cmu.edu  

Abstract 

This  paper  proposes  and  develops  a  new 
graph-based  semi-supervised 
learning  method.  
Different from previous graph-based methods that 
are based on discriminative models, our method is 
essentially  a  generative  model  in  that  the  class 
conditional  probabilities  are  estimated  by  graph 
propagation  and  the  class  priors  are  estimated  by 
linear regression.  Experimental results on various 
datasets show that the proposed method is superior 
to  existing  graph-based  semi-supervised  learning 
methods, especially when the labeled subset alone 
proves  insufficient  to  estimate  meaningful  class 
priors. 

1  Introduction 
In many real world classification tasks, the number of labeled 
instances is very few due to the prohibitive cost of manually 
labeling every single data point, while the number of unla-
beled data can be very large since they are easy to obtain.  
Traditional  classification  algorithms,  known  as  supervised 
learning, only make use of the labeled data, therefore prove 
insufficient  in  these  situations.    To  address  this  problem, 
semi-supervised learning has been developed, which makes 
use of unlabeled data to boost the performance of supervised 
learning.  In particular, graph-based semi-supervised learn-
ing algorithms have proved to be effective in many applica-
tions,  such  as  hand-written  digit  classification  [Zhu  et  al., 
2003; Zhu et al., 2005], medical image segmentation [Grady 
and Funka-Lea, 2004], word sense disambiguation [Niu, Ji 
and Tan, 2005], image retrieval [He et al., 2004], etc. 
  Compared with other semi-supervised learning methods, 
such as TSVM [Joachims, 1999], which finds the hyperplane 
that separates both the labeled and unlabeled data with the 
maximum  margin,  graph-based  semi-supervised  learning 
methods make better use of the data distribution revealed by 
unlabeled data.  In graph-based semi-supervised learning, a 
weighted graph is first constructed in which both the labeled 
and unlabeled data are represented as vertices.  Then many of 
these methods can be viewed as estimating a function on the 
graph  [Zhu,  2005].    Based  on  the  assumption  that  nearby 
points in the feature space are likely to have the same label, 
the function is defined to be locally smooth and consistent 

with the labeled data.  Finally, the classification labels are 
obtained by comparing the function value and a pre-specified 
threshold.  For example, in the Gaussian random fields and 
harmonic function  method,  the learning  problem  is  formu-
lated in terms of a Gaussian random field on the graph, and 
the mean of the field serves as the function [Zhu et al., 2003].  
Another example is the local and global consistency method, 
in which the function at each point is iteratively determined 
by both the information propagated from its neighbors and its 
initial label [Zhou et al., 2004].  Yet another example is the 
graph mincut method whose function corresponds to parti-
tioning the graph in a way that roughly minimizes the number 
of  similar  pairs of  examples that  are given different  labels 
[Blum and Chawla, 2001].  In the mincut method, the func-
tion can only take binary values. 
  Up till now, graph-based semi-supervised learning meth-
ods  are  generally  approached  from  the  discriminative  per-
spective [Zhu, 2005] in that the function on the graph cor-
responds to posterior probabilities in one way or another.  In 
the discriminative setting, however, the use of unlabeled data 
does not necessarily guarantee better decision boundaries.  In 
addition, there is no clear explanation why the function on 
the graph should correspond to posterior probabilities from 
statistics point of view. 
 
this  paper,  we  propose  a  new  graph-based 
semi-supervised learning method from the generative model 
perspective.  Specifically, the class conditional probabilities 
and the class priors are estimated from the weighted graph.  
The potential advantages involve several aspects: first, it can 
be theoretically justified that in the ideal cases where the two 
classes are separable, the output functions in terms of certain 
eigenvectors of the graph converge to the class conditional 
probabilities as the number of training data goes to infinity.  
In non-ideal cases, our functions still provide a good estimate 
of the class conditional probabilities.  Finally, the estimated 
class priors make use of both the labeled and unlabeled data, 
which compensate for the lack of label information in many 
practical situations.  Experimental results show that our ap-
proach  leads  to  better  performance  than  other  existing 
graph-based methods on a variety of datasets.  Hence we can 
claim  both  stronger  theoretical  justification  and  better  em-
pirical results. 
  The rest of the paper is organized as follows.  In Section 2 
and Section 3, we introduce how to estimate the class condi-

In 

IJCAI-07

2492

tional probabilities and the class priors respectively.  Section 
4 deals with the out-of-sample problem, followed by an out-
line  of  the  algorithm  in  Section 5.   Then  the  experimental 
results are shown in Section 6.  Finally, we give conclusion 
and hint on future work in Section 7. 

2  Estimating Class Conditional Probabilities 

l

d

0

n
l

x
n

1,
x

.  The first 
,

1ln  positive (
0,
1,

2.1  Notation 
In a binary classification problem, suppose that we are given 
a set of  n  training samples: 
ln  sam-
ples are labeled, including 
n ) and 
1
n  negative  (
n )  samples.    The 
n
1
l
remaining 
n n  samples are unlabeled.  Our goal is to 
predict the class labels of these 
un  points by computing the 
P y x . 
posterior probability  
  By Bayes rule, we have 
(
|
p x
i
(
p x
i

)
(
P y
i
')
(
P y
i

P y
i

(1)

)
y
i

y
i
,

y
i
|

n
u

1,

1,

n
l

y
i

x
i

')

 

i

i

|

1

i

i

l

l

l

' 0,1

y
i

i

i

i

i

i

P y
i

p x y

p x y

1|
x
i
|i
P y

,
.  In our genera-
0.5
x , we need to esti-
 and  p y .  In this section, we focus on 
, and the 

iy  is predicted to be 1 iff 
tive model, in order to calculate 
mate both 
estimating the class conditional probability 
estimation of  p y  will be discussed in the next section. 
  We 
W
ij
measuring  the  direct  similarity  between 
define 
D
ii

 with 
 is  a  non-negative  function 
jx .    Then 
where 
D
.    Finally  define 
,
w i
 as two  n -dimensional vectors.  The element of 
) is set to 1 iff the corresponding point is a positive 

form  an  affinity  matrix 
,  where 

ix  and 
diagonal  matrix, 

1
 and  f
 ( f

the 
n ,  and 

first 
,
x x
i

S D WD

 as 
1,

f
f
(negative) labeled one. 

,i
x x

W

1/ 2

1/ 2

n n

ij

n

j

j

j

2.2  The Ideal Case 
To start with, let us first consider the ideal case where the two 
classes  are  far  apart.    In  this  case,  we  have  the  following 
equation: 
p x

p x y

p x y

P y

P y

1

1

0

0

 

(2)

P y

x

p x y

x

ix  and 

xy  is the observed class label of data point  x . 

where 
jx  are from two dif-
  Based on this assumption, if 
.    Therefore  if  we 
ferent  classes,  the  corresponding 
0
knew the labels of all the samples and put together the sam-
ples from the same class, the affinity matrix  W , and thus the 
symmetric  matrix  S  would  be block-diagonal.   To  be  spe-
cific, let 

ijW

u
,  lim

n  and 
 satisfies 
, 
1
,  as 
 con-

W

W
1
0

0
W
0

 

(3)

1W  and 

where 
0W  represent  the  sub-matrices  corresponding 
to  the  positive  and  negative  samples  respectively,  and  0 
represents  zero  matrix.    If  the  total  number  of  positive 
0W ) is 
1W  (
(negative) samples in the training set is 
an 
0D  be  two 
n )  square  matrix.    Let 
0
diagonal  matrices,  the  diagonal  elements  of  which  are  the 
row sums of 
S
1
0

0W .  Then  S  can be written as 
D W D
1

1n  ( 0n ), 
1D  and 

1W  and 

n  (
1

0
S

(4)

n
0

n
1

1/ 2

1/ 2

1/ 2

1/ 2

0

S

 

1

1
0

D W D
0

0

0

0

  The  following  theorem  connects  the  class  conditional 
probabilities with the diagonal elements of  D . 
Theorem  1.  If 
nV  are  positive  parameters,  and  the  function 
the 

conditions: 

V ,  where 

following 

u du

,i
x x

, 

x
i

0

x

n

n

j

j

i

i

i

i

n

n

u

u

d

ii

1

u

u

0

0

, 

u
i

V
n

lim

,  lim n
nV

yD n

p x y . 

sup
the  number  of  examples  n  goes  to  infinity, 
verges to 
  The proof of the theorem is straightforward and therefore 
we put it in the appendix.  Notice that this theorem is similar 
to a result in kernel density estimation.  The difference is that 
in kernel density estimation, we only have labeled data from 
a single class; while in our situation, we have both labeled 
and unlabeled data, and we could estimate the class condi-
tional distributions of the two classes at the same time. 
  Suppose that the labeled data are noise-free.  According to 
iiD  to approximate the class condi-
Theorem 1, we can use 
tional probability of 
iy .  How-
ix  given the observed label 
ever, for the unlabeled points, we do not know if 
iiD  corre-
.  To address this prob-
sponds to 
lem, we can make use of the eigenvectors of  S . 
 
It is easy to show that the largest eigenvalue of 
is 1, and if 
the  corresponding  eigenvectors  would  be 
v
construct two eigenvectors of  S  with eigenvalue 1: 

0S  
0W  form a connected graph respectively, 
 and 
 [Chung,  1997].    Based  on  v  and  v ,  we  can 

1W  and 

1S  and 

p x y
i

p x y
i

1/ 2
D
1

1/ 2
D
0

 or 

1

1

1

0

v

i

i

T

T

T

T

,

 

v

v

0

0

v
1

v
0

where  0  is a zero vector.  Notice that if we square  1v  and 
by elements to get 

(5)
0v  
1v  and 
0v , and then add them up, we get 
(6)
2
1
D
v
1
0
 and 
2
0v
 respectively,  and  their  non-zero  elements  are 

2
v
1
1v  and 

 correspond  to 

p x y
i

D  
1

D
0

1

1

2

2

2

i

  Obviously, 
p x y
i
equal to 

0
iiD . 

i

IJCAI-07

2493

2

f

f

if

S f

S f

 and  v

,  we  perform 

1v  (the  elements  of 

0v .  Therefore, upon convergence, 

 until convergence.  Since the initial value of  f

 and 
 will  converge  to  1v .    Similarly,  f
 (

 and 
  To  get  v
 
f
is  not  orthogonal  to 
1v  are 
non-negative),  f
 will 
2
converge to 
) 
is  in  proportion  to  the  class  conditional  probability  of  the 
) 
positive  (negative)  class.    After  normalizing 
so  that  it  sums  to  1,  we  have  an  empirical  estimation  of 
), which converges to its true value 
p x y
i
as  n  goes to infinity. 
  Figure  1  gives  an  example  of  density  estimation  in  the 
ideal case.  Figure 1(a) shows the training data, where the two 
moons represent two classes, and each class has one labeled 
example marked as star.  Figure 1(b) and 1(c) show the es-
timated class conditional distributions of the two classes. 

p x y
i

 (

 (

0

1

if

if

if

2

2

i

i

6

4

2

0

-2

-4

-5

0

5

10

 

(a) 

(b) 

(c) 

Figure 1. Density Estimation in the Ideal Case. (a): training data; (b) 
and (c) class conditional distributions 

2.3  The General Case 
In the general cases, the two classes are not far apart, and we 
have the following theorem. 
Theorem 2. If 
as  the  number  of  samples  n  goes  to  infinity, 
verges to 
0

 satisfies the conditions in Theorem 1, 
iiD n  con-
 

,i
x x

P y

P y

1

1

0

p x y
i

p x y
i

j

i

i

The proof to this theorem is quite similar to Theorem 1.  So 
we omit the details here.  It can be seen easily that Theorem 1 
is a special case of Theorem 2 when the two classes are far 
apart, i.e. 
lim

1 , if

D n

P y

1

1

p x y
i

i

y
i

ii

n

i

n

ii

0

lim

P y

D n

p x y
i

0 , if
  Equation (7), together with the fact that 
leads to Theorem 1. 
In the general cases, W  tends to form one connected graph 
 
instead of two, and  S  only has one eigenvector that corre-

y
i
lim

, 

1

n

(7)

 
 
0
n n P y
1

i

i

1

2

if

1

1

. 

ln
i n

S f

S f

S f

p x y
i

p x y
i

 and  f

 and  f

 and  f

 until  convergence,  both  f

 and 
sponds  to  eigenvalue  1.    If  we  still  iterate  f
S f
 will  con-
f
verge to the same eigenvector.  On the other hand, the op-
 can  be  seen  as  the 
eration  of  f
labeled data gradually spreading their information to nearby 
points.  If the iteration steps are unlimited, every data point 
will be equally influenced by the positive and negative la-
beled data, leading to the same value of  f
  To solve this problem, in our algorithm, we have designed 
a stopping criterion, and the iteration process is stopped once 
the  criterion  is  satisfied.    To  be  more  specific,  when  esti-
mating the class conditional probabilities of the positive class, 
 in each iteration step 
we could get an estimate of 
(by normalizing 
 so that it sums to 1).  By summing up 
this  probability  for  negative  labeled  samples,  we  have  the 
average  likelihood  of  these  samples  in  the  positive  class: 
n .  We stop the iteration when the 
L
second  derivative  of  L  with  respect  to  the  iteration  steps 
crosses 0.  This criterion can be justified as follows: in the 
initial iteration steps, only a few negative data get positive 
score from their nearby positive labeled points, so the rate at 
which  L  increases  is  very  low;  as  the  iteration  proceeds, 
those  negative  data  have  accumulated  high  scores  and 
propagate  to  the  majority  of  negative  points,  so  the  rate 
 begins  to  converge,  its 
gradually  increases;  finally,  as  f
value at each data point becomes stable, so the rate decreases 
until it reaches 0.  If we plot the curve of  L  with respect to 
the number of iteration steps, the shape would be convex first, 
and  then  concave  until  convergence  (Figure  2(b)).    Notice 
that in the initial iteration steps, the positive points, which are 
far away from the positive labeled points but connected to 
them via some kind of manifold, cannot get positive scores.  
If the algorithm stops at this stage, it may not fully explore 
the  data  distribution  and  cause  misclassification  on  certain 
clusters  of  data.    Therefore  we  choose  the  transition  point 
between convex and concave as the stopping point in order to 
trade  off  between  prematurity  and  excessive  propagation.  
The stopping criterion for the negative class can be derived 
n .  A key point in our 
similarly, i.e. 
1
algorithm  is  that  the  estimation  of  the  class  conditional 
probabilities  of  the  two  classes  is  independent,  i.e.  the 
numbers of iteration steps when the two stopping criterions 
are satisfied are not necessarily the same. 
  Figure  2  gives  an  example  of  density  estimation  in  the 
general case showing the effectiveness of our criterion.  This 
example is quite similar to the one shown in Figure 1 except 
that the two classes are not far apart.  Figure 2(b) shows the 
value  of  L  (the  upper  curve)  and  L  (the  lower  curve)  in 
each iteration step.  The arrows point to the positions in the 
curves where the two criterions are satisfied.  Figure 2(c) and 
2(d) show the estimated class conditional distributions of the 
two classes.  Although there are small gaps in the middle of 
the distributions, the moon structure is recovered fairly well. 

p x y
i

ln
i

0

L

1
1

i

l

IJCAI-07

2494

8

6

4

2

0

2

4
-5

0

5

(a) 

10

 

L+

L-

x 10-3

3

2.5

2

1.5

1

0.5

0
0

 

200

400

600

800

(b) 

(d) 

 

 

(c) 

Figure 2. Density Estimation in the Generation Case. (a): training 
data; (b):  L  and  L  in each iteration; (c) and (d): class conditional 
distributions. 
  Note that the stopping criterion discussed above is based 
on  simple  heuristics.    Currently  we  are  trying  to  design  a 
stopping criterion in a more principled manner. 

3  Estimating Class Priors 
In this section, we focus on estimating the class prior  P y .  
Existing graph-based semi-supervised learning methods only 
use the labeled set to estimate the class priors, either explic-
itly [Zhu et al., 2003] or implicitly [Zhou et al., 2004].  Ob-
viously, in real applications, the proportion of positive and 
negative labeled data is often far from the true class priors. 
 
In our algorithm, we use both the labeled and unlabeled 
data to estimate the class priors.  According to Theorem 2, 
once  we  have  estimated  the  class  conditional  probability 
, we can feed them into the following equations and 
p x y
form  a  linear  regression  problem,  the  solution  of  which  is 
equal to the least squares estimator of 
0 1

(8)
  However, when the number of labeled data is small, the 
estimated  class  conditional  probabilities  may  not  be  very 
accurate,  and  thus  ˆp  is  not  very  reliable.    To  solve  this 
problem, we use a beta distribution as the prior distribution 
1 p .  Then 
for 
the estimate of 
1

, the parameters of which are  ˆp  and 
 based on the labeled set: 
,
1

P y
ˆ
,
p i

p x y
i

p x y
i

. 
1,

D n

(9)

P y

P y

P y

P y

n  

1

1

0

1

1

ˆ
p

1

ˆ

,

ii

i

i

i

i

 

1
P y
ˆ
p n
l
1
n
l

which is equivalent to smoothing the proportion of the posi-
tive  and  negative  samples  in  the  labeled  set.    When  the 
number of labeled data is small, unlabeled data can be fully 
exploited to compensate for the proportion in the labeled set 
that is not the same as the class priors; when the number of 
labeled data is large, labeled data will dominate the estima-
tion of the class priors. 

4  Prediction of New Testing Data 
 that is not present during the 
To classify a data point 
training stage, we first calculate its class conditional prob-
abilities via kernel regression: 
,
x x
i
n

p x y

(10)

p x y

x

 

1

d

n

i

i

,
x x
i

i

1

  Using  these  class  conditional  probabilities  and  the  class 
priors obtained during the training stage, we can calculate the 
posterior probability and make a prediction. 

 and  P y  are  sum-

5  The Algorithm 
The  procedures  for  estimating 
marized in Table 1 and Table 2 respectively. 
1.  Form 

p x y

i

i

2. 

j

n n

,  where 

.  The element of  f

affinity  matrix 
W
.  Calculate  D  and  S . 
 and  f

the 
,
W
x x
i
ij
) is set 
Initialize  f
to 1 if the corresponding point is a positive (negative) 
labeled one, and 0 otherwise. 
S f
, 
 

,  and 
, 

normalize 

S f
1

p x y
i

p x y
i

 ( f

,  f

p x y
i

. 

1

1

0

(

)

f

2

2

n

i

i

i

i

i

i

1

3.  Update  f
4.  Assign 

f
that 
. 

so 
0

1

n

i

1

p x y
i

i

5.  Calculate the average likelihood of negative (positive) 

l

i

l

i

1

1

1
1

L

ln
i

0

1

n
l
i n
l

n (
0

p x y
i

p x y
i

labeled points in the positive (negative) class: 
n ).
L
1
Go to step 4 unless one of the following conditions is 
satisfied: 
a. L  ( L ) remains at 0, and  f
) has converged; 
b. L  ( L )  does  not  remain  at  0,  and  the  second  de-
rivative of  L  ( L ) with respect to the iteration steps 
crosses 0. 
p x y
i

. 
Table 1. Description of Estimation for 

p x y
i

 and 

p x y

i

i

 

 ( f

1

0

i

i

6.  Output 

1.  Solve the following linear regression problem for the 

least squares estimator  ˆp  of 
    ˆ
p x y
p p x y
i
i

ˆ
p

1

1

i

i

P y
0

1

: 
,
D n i

ii

1,

,

n  

2.  Calculate the class priors as the smoothed proportion of 

the positive and negative samples in the labeled set 

P y

1

1

,

P y

0

1

P y

1

 

ˆ
p n
l
1
n
l

Table 2. Description of Estimation for  P y  

6  Experimental Results 
In  this  section,  we  present  the  comparative  experimental 
results on two datasets: Cedar Buffalo binary digits database 
[Hull,  1994],  and  a  document  genre-classification  dataset 
[Liu et al., 2003].  Our algorithm is compared with two other 

IJCAI-07

2495

case, while the performance of our algorithm is comparable 
to  the  balanced  case.    This  is  because  the  class  mass  nor-
malization  procedure  adopted  in  Gaussian  random  fields 
depends on the labeled set only to estimate the class priors; 
while our algorithm makes use of both the labeled and the 
unlabeled  set  to  estimate  the  class  priors.    Therefore,  it  is 
more robust against the perturbation in the proportion of the 
positive and negative data in the labeled set. 

6.2  Genre Dataset 
Genre classification is to classify the documents based on its 
writing styles, such as political articles and movie reviews.  
The genre dataset that we use consists of documents from 10 
genres,  including  biographies  (b),  interview  scripts  (is), 
movie  reviews  (mr),  product  reviews  (pr),  product  press 
releases  (ppr),  product  descriptions on  store  websites  (pd), 
political  articles  on  newspapers  (pa),  editorial  papers  on 
politics  (ep),  news  (n),  and  search  results  from  multiple 
search  engines  using  10  queries  (sr).    We  randomly  select 
380  documents  from  each  category  to  compose  the  whole 
dataset of 3800 documents.  Each document is processed into 
a “tf.idf” vector, which is generated based on the top 10,000 
most frequent words in this dataset after stemming, with the 
header 
Here 
,  which  is  bor-
,
x x
i
rowed  from  [Zhu  et  al.,  2003]  and  roughly  measures  the 
similarity between documents.  The only difference is that we 
keep  all  the  edges  instead  of  keeping  edges  for  only  10 
nearest neighbors.  Next we perform experiments to compare 
the three algorithms.  The results are provided in Figure 5 and 
Figure 6 respectively. 

removed. 
0.03

words 
x x
i

stop 
x x
i

and 
exp

1

 

j

j

j

Our Algorithm
Gaussian Random Fields
Local and Global Consistency

0.65

Our Algorithm
Gaussian Random Fields
Local and Global Consistency

Our Algorithm
Gaussian Random Fields
Local and Global Consistency

Our Algorithm
Gaussian Random Fields
Local and Global Consistency

20

60

40
labeled set size
(a) 

80

60

40
labeled set size
(b) 

Figure 5. Classification between Random Partitions. (a): balanced; 
(b): unbalanced 

Our Algorithm
Gaussian Random Fields
Local and Global Consistency

Our Algorithm
Gaussian Random Fields
Local and Global Consistency

0.4
0

20

60

80

labeled set size

40
(a) 

40
labeled set size

60

80

1

(b) 

Figure 4. Unbalanced Classification. (a): 1 vs 2; (b) odd vs even 
  Figure 3(a) and 3(b) show the results of the two classifi-
cation tasks in the balanced case.  The performance of our 
algorithm  is  comparable with Gaussian  random  fields,  and 
both of them are much better than the local and global con-
sistency method.  Figure 4(a) and 4(b) show the results in the 
unbalanced  case.    In  this  situation,  the  performance  of 
Gaussian random fields is much worse than in the balanced 

50

labeled set size

100

(a) 

50

100

150

labeled set size
(b) 

Figure 6. Unbalanced Classification. (a): pa vs other; (b) b vs other 
  For Figure 5, we randomly partition the 10 categories into 
two classes, i.e. pa, pr, sr, b, and is, vs mr, ppr, pd, ep and n.  
Figure 5(a) and 5(b) correspond to the balanced and unbal-

y
c
a
r
u
c
c
a

0.6

0.55

0.5

20

80

 

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

y
c
a
r
u
c
c
a

 

150

graph-based  semi-supervised  learning  methods:  Gaussian 
random  fields  [Zhu  et  al.,  2003]  and  the  local  and  global 
consistency method [Zhou et al., 2004].  We did not compare 
with  supervised  learning  methods,  such  as  one  nearest 
neighbor,  since  they  have been proved  to be  less  effective 
than Gaussian random fields based on experimental results 
[Zhu et al., 2003]. 
  We have designed two kinds of experiments: balanced and 
unbalanced.  In the balanced case, the ratio of labeled points 
from each class is always the same as the class priors; in the 
unbalanced case, if not explained otherwise, we fix the total 
ln  of  labeled  points,  and  perturb  the  number  of 
number 
 with a Gaussian distri-
positive labeled points around 
.    In  each 
bution  of  mean  0  and  standard  deviation 
experiment, we gradually increase the number of labeled data, 
perform 20 trials for each labeled data volume, and average 
the accuracy at each volume point. 

2ln

10

ln

6.1  Cedar Buffalo Binary Digits Database 
We first perform experiments on Cedar Buffalo binary digits 
database  [Hull,  1994]  including  two  classification  tasks: 
classifying digits “1” vs “2”, with 1100 images in each class; 
and odd vs even digits, with 2000 images in each class (400 
images for each digit).  The data we use are the same as those 
used 
Here 
 is  the 
,
x x
i
average distance between each data point and its 10 nearest 
neighbors. 

2003]. 
,  where 

[Zhu 
2
2

al., 
2
2

in 
2

exp

et 

x
i

x

 

2

d

j

j

Our Algorithm

Gaussian Random Fields

Local and Global Consistency

Our Algorithm

Gaussian Random Fields

Local and Global Consistency

60

80

labeled set size

40
(a) 

40
labeled set size

60

80

1

(b) 

Figure 3. Balanced Classification. (a): 1 vs 2; (b) odd vs even 

1

0.9

y
c
a
r
u
c
c
a

0.8

0.7

0.6

0.5
0

20

1

0.9

0.8

0.7

0.6

0.5

y
c
a
r
u
c
c
a

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

y
c
a
r
u
c
c
a

1
 

20

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

y
c
a
r
u
c
c
a

1
 

20

y
c
a
r
u
c
c
a

0.7

0.65

0.6

0.55

0.5

1

y
c
a
r
u
c
c
a

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

IJCAI-07

2496

j

,i
x x

anced  cases  respectively.    In  the  balanced  case,  Gaussian 
random fields is better than our algorithm and the local and 
global  consistency  method.    This  might  be  because  the 
 does not have some of the nice properties 
function 
required by Theorem 2.  However, in the unbalanced case, 
Gaussian random fields tends to suffer a lot.  On the contrary, 
our algorithm is quite robust despite of the perturbation. 
 
In Figure 6, we try to classify pa and b against all the other 
categories.  In these experiments, the class priors are 0.1 for 
the positive class and 0.9 for the negative class.  However, 
here  we  provide  equal  numbers  of  positive  and  negative 
points in the labeled set.  From the figures, we can see that the 
performance  of  our  algorithm  is  rather  stable,  while  the 
performance  of  both  Gaussian  random  fields  and  the  local 
and  global  consistency  method  is  largely  affected  by  the 
misleading labeled set, since they only depend on the labeled 
set to estimate the class priors, either explicitly or implicitly. 

7  Conclusion and Future Work 
In 
this  paper,  we  propose  a  novel  graph-based 
semi-supervised learning method to estimate both the class 
conditional probabilities and the class priors.  It is a genera-
tive  model,  in  contrast  to  existing  graph-based  methods, 
which are essentially discriminative.  In the ideal case, the 
estimated class conditional probabilities have been proved to 
converge to the true value.  In the general case, our algorithm 
can still output reasonable estimates of the class conditional 
probabilities.    For  data  points  outside  the  training  set,  the 
class  conditional  probabilities  are  estimated  via  kernel  re-
gression.  When estimating the class priors, we effectively 
use the unlabeled data to make up for the labeled data with 
unrepresentative  class  prior  distributions.    Experimental 
results  on  two  datasets  demonstrate  the  superiority  of  our 
algorithm over recent existing graph-based semi-supervised 
learning  methods,  especially  when  the  proportion  in  the 
labeled set is not the same as the class priors. 
 
In our experiments, we notice that in some cases, adding 
even a single labeled point into the labeled set brings about 
significant improvement in classification accuracy; while in 
other cases, adding many labeled points into the labeled set 
does not help improve the performance.  Currently we are 
incorporating active learning into our framework.  Particu-
larly, we are interested in determining when to invoke active 
learning (not just which instances to label) in order to achieve 
the biggest gain while minimizing incremental labeling cost. 

References 
[Blum and Chawla, 2001] Blum, A., & Chawla, S. (2001). 
Learning  from  labeled  and  unlabeled  data  using  graph 
mincuts. ICML. 

[Chung, 1997] Chung, F. R. K. (1997). Spectral graph the-
ory,  regional  conference  series  in  mathematics,  no.  92. 
American Mathematical Society. 

[Grady and Funka-Lea, 2004] Grady, L., & Funka-Lea, G. 
(2004). Multi-label image segmentation for medical ap-

plications based on graph-theoretic electrical potentials. 
ECCV04, workshop on Computer Vision Approaches to 
Medical  Image  Analysis  and  Mathematical  Methods  in 
Biomedical Image Analysis. 

[He et al., 2004] He, J., Li, M., Zhang, H. J., Tong, H., & 
Zhang,  C.  (2004).  Manifold-ranking  based  image  re-
trieval.  Proc.  12th  ACM  International  Conf.  on  Multi-
media. 

[Hull, 1994] Hull, J. J. (1994). A database for handwritten 
text recognition research. IEEE Transactions on Pattern 
Analysis and Machine Intelligence, 16. 

[Joachims, 1999] Joachims, T. (1999). Tranductive Inference 
for  text  classification  using  Support  Vector  Machines. 
ICML. 

[Liu et al., 2003] Liu, Y., Carbonell, J., & Jin, R. (2003). A 
New  Pairwise  Ensemble  Approach  for  Text  Classifica-
tion. ECML. 

[Niu, Ji and Tan, 2005] Niu, Z. Y., Ji, D. H., & Tan, C. L. 
(2005). Word sense disambiguation using label propaga-
tion based semi-supervised learning. Proc. 43rd Meeting 
of the Association for Computational Linguistics. 

[Zhou et al., 2004] Zhou, D., bousquet, O., Lal, T., Weston, 
J., & Schlkopf, B. (2004). Leaning with local and global 
consistency. NIPS. 

[Zhu  et  al., 2003] Zhu, X., Ghahramani, Z., & Lafferty, J. 
(2003).  Semi-supervised  learning  using  Gaussian  fields 
and harmonic functions. ICML. 

[Zhu et al., 2005] Zhu, X., & Lafferty, J. (2005). Harmonic 
mixtures:  combining  mixture  models  and  graph-based 
methods  for  inductive  and  scalable  semi-supervised 
learning. ICML. 

[Zhu, 2005] Zhu, X. (2005). Semi-supervised learning with 
graphs.  Doctoral  dissertation,  School  of  Computer  Sci-
ence, Carnegie Mellon University. 

Appendix 
Proof of Theorem 1: suppose 

ix  is from the positive class: 

lim

n

D n
1

ii

lim

n

n

j

W n
1
1

ij

lim

n

E

x Y

n
1
j

1

x
i

x

j

n V
1
n

n

,
x x
i

 

1
,

x x p x y
i

1

dx

(11)

p x y

i

1

  Equation (11) reduces the number of terms in the summa-
jx  is  from  the  negative 
tion  from  n  to 
x .    A  corre-
class.   
sponding proof applies if 

1n  since 
 is  a  delta  function  at 

ix  is from the negative class. 

 if 

,i
x x

j

ijW

0

x
i

j

IJCAI-07

2497


A Weighted Polynomial Information Gain Kernel for Resolving Prepositional 

Phrase Attachment Ambiguities with Support Vector Machines 

Bram Vanschoenwinkel* 

Bernard Manderick 

Vrije Universiteit Brussel, Department of Computer Science, Computational Modeling Lab 

Pleinlaan 2, 1050 Brussel, Belgium 
{bvschoen® - bernard@arti}.vub.ac.be 

Abstract 

We introduce a new kernel for Support Vector Ma(cid:173)
chine learning in a natural language setting.  As a 
case study to incorporate domain knowledge into 
a  kernel,  we  consider  the  problem  of  resolving 
Prepositional Phrase attachment ambiguities.  The 
new  kernel  is  derived  from  a  distance  function 
that proved to be succesful in memory-based learn(cid:173)
ing. We start with the Simple Overlap Metric from 
which we derive a Simple Overlap Kernel and ex(cid:173)
tend it with Information Gain Weighting.  Finally, 
we combine it with a polynomial kernel to increase 
the dimensionality of the feature space.  The clo(cid:173)
sure properties of kernels guarantee that the result 
is again a kernel.  This kernel achieves high clas(cid:173)
sification accuracy and is efficient in both time and 
space usage. We compare our results with those ob(cid:173)
tained by memory-based and other learning meth(cid:173)
ods.  They  make  clear  that  the  proposed  kernel 
achieves a higher classification accuracy. 

Introduction 

1 
An important issue in natural language analysis is the resolu(cid:173)
tion of structural ambiguity.  A sentence is said to be struc(cid:173)
turally ambiguous when it can be assigned to more than one 
syntactic structure  [Zavrel  et al. , 1997].  In Prepositional 
Phrase (PP) attachment one wants to disambiguate between 
cases where it is uncertain whether the PP attaches to the verb 
or to the noun. 
Example 1 Consider the following two sentences: 

1. I bought the shirt with pockets. 
2. 1 washed the shirt with soap. 
In sentence 1, with modifies the noun shirt because with 

pockets (PP) describes the shirt. In sentence 2 however, with 
modifies the verb washed because with soap (PP) describes 
how the shirt is washed [Ratnaparkhi, 1998]. 

This  type of attachment ambiguity  is  easy  for people to 
resolve because they can use their world knowledge [Stetina 
* Author funded by a doctoral grant of the institute for advance(cid:173)

ment of scientific technological research in Flanders (1WT). 

and Nagao, 1997].  A computer program usually cannot rely 
on that kind of knowledge. 

This  problem  has  already  been  tackled  using  memory-
based learning like for example K-nearest neighbours.  Here, 
the training examples are first stored in memory and classi(cid:173)
fication of a new example is done based on the closest ex(cid:173)
ample stored in memory.  Therefore, one needs a function 
that expresses the distance or similarity between examples. 
There already exist several dedicated distance functions to 
solve all kind of natural language problems using memory-
based  learning  [Veenstra  et al,  2000; Zavrel  et al,  1997; 
Daelemans et al, 2002]. 

We will use a Support Vector Machine (SVM) to tackle the 
problem of PP attachment disambiguation.  Central to SVM 
learning is the kernel function K  :  .X  x  X  ->  R  where X 
contains the examples and the kernel K calculates an inner 
product in a second space, the feature space F. This product 
expresses how similar examples are. 

Our goal is to combine the power of SVMs with the dis(cid:173)
tance functions that arc well-suited for the probem for which 
they were designed.  Deriving a distance from a kernel is 
straightforward, see Section 2.1. However, deriving a kernel 
from a distance is not trivial since kernels must satisfy some 
extra conditions, i.e.  being a kernel is a much stronger con(cid:173)
dition than being a distance.  In this paper we will describe 
a method that shows how such dedicated distance functions 
can be used as a basis for designing kernels that sequentially 
can be used in SVM learning. 

We use the PP attachment problem as a case study to illus(cid:173)
trate our approach.  As a starting point we take the Overlap 
Metric that has been succesfully used in memory-based learn(cid:173)
ing for the same problem [Zavrel et al, 1997]. 

Section 2 will give a short overview of the theory of SVMs 
together with some theorems and definitions that are needed 
in Section 4.  Based on [Zavrel et al, 1997], section 3 gives 
an overview of metrics developed for memory-based learning 
applied to the PP attachment problem. In Section 4 the new 
kernels will be introduced. Finally Sections 5 and 6 give some 
experimental results and a conclusion of this work. 
2  Support Vector Machines 
For simplicity, in our explanation we will consider the case 
of binary classification only, i.e.  we consider an input space 
X with input vectors x and a target space D = {1, -1}. The 

CASE-BASED  REASONING 

133 

goal  of the  SVM  is  to  assign  every  x  to  one of two  classes 
D  =  { 1 , - 1 }.  The decision  boundary  that  separates  the  in(cid:173)
put vectors belonging to different classes  is  usually an  arbi(cid:173)
trary n  —  1-dimensional  manifold  if the  input space  X  is n-
dimensional. 

2.1  Some Properties  of Kernels 

even if we don't know the exact  form of the features that are 
used  in  F.  Moreover,  the  kernel  expresses prior knowledge 
about the  patterns being  modelled,  encoded  as  a  similarity 
measure between two vectors  [Brown et al,  1999]. 

But not all maps over X  x X  are kernels.  Since a kernel K 
is related to an inner product, cfr.  the definition above, it has 
to satisfy some conditions that arise naturally from the defini(cid:173)
tion of an  inner product and are given by Mercer's theorem: 
the  map  must  be  continuous  and  positive  definite  I  Vapnik, 
1998]. 

In  this  paper we  will  use  following  methods  to  construct 

kernels iCristianini and Shawe-Taylor, 2000]: 
M1  Making kernels from kernels:  Based on the fact that ker(cid:173)
nels satisfy a number of closure properties.  In this case, 
the Mercer conditions follow naturally from the closure 
properties of kernels. 

M2  Making  kernels  from  features:  Start  from  the  features 
of the input vectors and obtain a kernel by  working out 
their inner product.  A feature is a component of the in(cid:173)
put  vector. 
In  this  case,  the  Mercer conditions  follow 
naturally from the definition of an  inner product. 

3  Metrics for Memory-based Learning 
In this section, we will focus on the distance functions [Zavrel 
et al,  1997; Cost and Salzberg,  1993] used for memory-based 
learning with symbolic values.  First,  we will  have a look at 
the  Simple  Overlap Metric  (SOM)  and  next we  will  discuss 
Information  Gain  Weighting  (1GW).  Memory-based learning 
is a class  of machine learning techniques where training in(cid:173)
stances  are  stored  in  memory  first  and classification  of new 
instances later on is based on the distance (or similarity) be(cid:173)
tween the new instance and the closest training instances that 
have already been stored in memory.  A well-known example 
of memory-based learning is k-nearest neighbours classifica(cid:173)
tion.  We  will  not go  into further detail,  the  literature offers 

SVMs  sidestep  both  difficulties  [Vapnik,  1998].  First, 
overfitting is avoided by choosing the unique maximum mar(cid:173)
gin hyperplane among all possible hyperplanes that can sep(cid:173)
arate the data in  F.  This hyperplane maximizes the distance 
to the closest data points. 

7b  be  more  precise,  once  we have chosen  a  kernel K  we 
can  represent  the  maximal  margin  hyperplane  (or  decision 
boundary)  by a linear equation  in x 

convex  quadratic  objective  function  with  linear  constraints. 
Moreover, most of the cti  prove to be zero.  By definition the 
vectors  xi  corresponding with non-zero a, are called the sup(cid:173)
port vectors SV and this set consists of those data points that 
lie closest to the hyperplane and thus are the most difficult to 
classify. 

In order to classify a new point  xnew,  one determines the 

sign of 

If this sign  is positive xnew, belongs to class  1,  if negative to 
class  - 1,  if zero  xnew  lies  on  the  decision  boundary.  Note 
that we  have now restricted the summation to the  set  SV  of 
support vectors because the other a,  are zero anyway. 

134 

CASE-BASED  REASONING 

many books and articles that provide a comprehensive intro(cid:173)
duction to memory-based learning, e.g. [Mitchell,  1997]. For 
our purpose, the important thing to remember is that we will 
be working with symbolic values like strings over an alphabet 
of characters.  Instances are n-dimensional vectors a and b in 

A  Kernel  for  Natural  Language  settings 

3.1  Simple Overlap Metric 
The most basic metric for vectors with symbolic values is the 
Simple Overlap Metric(SOM)  [Zavrel  et a/.,  1997]: 

Here  dS OM  (a,b)  is  the distance between  the  vectors a and 
b, represented by n features and S is the distance per feature. 
The k;-ncarest neighbour algorithm equipped with this metric 
is called  IB1  [Aha et  al,  1991].  The IB1  algorithm simply 
counts  the  number of  (mis)matching  feature  values  in  both 
vectors. This is a reasonable choice if we have no information 
about the  importance of the  different features.  But  if we  do 
have  information about  feature  importance then we can  add 
linguistic bias to weight the different features. 
3.2 
Information Gain  Weighting  (1GW) measures  for every fea(cid:173)
ture i  separately how much  information  it contributes to our 
knowledge of the correct class  label.  The Information Gain 
(IG) of a feature i is measured by calculating the entropy be(cid:173)
tween the cases with  and without knowledge of the value of 
that feature: 

Information Gain Weighting 

weights  Wi  can  be  used to extend Equation  2  with  weights 
(see Equation 6). The k-nearest neigbour algorithm equipped 
with this metric is called 1B1-1G [Zavrel et  al,  1997] and the 
corresponding  distance  called  dIG: 

We don't have to proof that the kernel ks ok satisfies the Mer(cid:173)
cer conditions because this follows naturally from the defini(cid:173)
tion  of the  inner product.  We  started  from the  features, de-

inner product between them, see M2 from Section 2.1. How(cid:173)
ever, to show that the kernel really corresponds to the distance 
function given in Equations 3 and 4 we have to verify the dis(cid:173)
tance formula for kernels given in Section 2.1: 

CASE-BASED  REASONING 

135 

4.2  Extending the SOK to n Dimensions 

4.4  A  Polynomial  Information  Gain  Kernel 
In this section, we will increase the dimensionality of the fea(cid:173)
ture  space  F  by  making use  of a  polynomial  kernel  Kpoly. 
We  will  start  this  section  by giving  an  example  [Cristianini 
and Shawe-Taylor, 2000]: 

We  don't  have  to  proof  that  the  kernel  KSOK  is  a  valid 
kernel  as  this  follows  naturally  from  the  definition  of 
the  inner  product.  However,  we  again  have  to  show  that 
it  is  a  kernel  that  corresponds  to  the  distance  function 
dSOM 
In  the  following  we  will  show 

from  Equation  2. 

However,  this does  not  impose  any  problems for the  kernel 
we are aiming to develop. 
4.3  Adding Information  Gain  to  the SOK 

136 

CASE-BASED  REASONING 

5.2  Experimental Setup 
The experiments have been done with L I B S V M, a C/C++ and 
Java  library for  SVMs  [Chih-Chung  and Chi-Jen,  2002].  The 
machine  we  have  used  is  a  Pentium  I II  with  256MB  R AM 
memory,  running  Windows  XP.  We  choose  to  implement  the 
kernels  KIG  and  KPIG 
The  type  of  S VM  learning  we  have  used  is  C-SVM  [Chih-
Chung  and  Chi-Jen,  2002].  The  parameter  C  controls  the 
model  complexity  versus  the  model  accuracy  and  has  to  be 
chosen  based  on  the complexity  of the  problem. 

in  Java. 

The experiments  in  this  section  are  conducted on  a  simpli(cid:173)
fied  version  of the  full  PP  attachment  problem  (see  Example 
3).  The  data  consists  of four-tuples  of words,  extracted  from 
the  Wall  Street  Journal  Treebank  [Marcus  et  al.,  1993]  by  a 
group at I BM  [Ratnaparkhi  et al. , 1994]1. 

The  data  set  contains  20801  training  patterns,  3097  test 
patterns  and  an  independant  validation  set  of 4093  patterns 
for  parameter  optimization.  A ll  of the  models  described  be-
low  were  trained  on all  training examples  and  the  results  are 
given  for the  3097  test  patterns.  For the  benchmark compari(cid:173)
son  with other models from  the  literature, we  use only results 
for  which  all  parameters  have  been  optimized  on  the  valida(cid:173)
tion  set.  For  more  details  concerning  the  data  set  we  refer 
to  [Zavrel  et al. , 1997]. 

5.3  Results 

From  the  closure  properties  of  kernels,  it  follows  naturally 
that  KPIG 
indeed  is  a  valid  kernel  which  calculates  the  in(cid:173)
ner  product  of two  vectors  transformed  by  a  feature  mapping 

5  Experimental  Results 

5.1  PP attachment disambiguation problem 
If it is uncertain in a sentence whether the preposition attaches 
to the verb or to the  noun  then  we  have a prepositional phrase 
(PP)  attachment  problem.  For example,  in  sentence  1  of Ex(cid:173)
ample  1,  with  modifies  the  noun  shirt  because  with pockets 
(PP) describes the shirt.  In contrast,  in sentence 2, with mod(cid:173)
ifies  the  verb  washed  because  w i th  soap  (PP)  describes  how 
the shirt  is  washed  [Ratnaparkhi,  1998]. 

we  only 

keep 

those  words 

fact, 
importance 

In 
any 
(V(erb),N(oun),P(reposition),N(oun)). 

to 

the  PP  attachment  problem, 

that  are  of 
i.e. 

In  the  case  where  sentences  are  reduced  to  quadruples  as 
illustrated  in  Example  3,  the  human  performance  is  approx(cid:173)
imately  88.2%  [Ratnaparkhi  et  al,  1994].  This  performance 
rate  gives  us  an  acceptable  upper  limit  for  the  maximum 
performance  of  a  computer  because  it  seems  unreasonable 
to  expect  an  algorithm  to  perform  much  better  than  a  hu(cid:173)
man.  As  we  will  show  in  our experimental  results  the  kernel 
KIG  achieves a classification  accuracy up to  82,9%, see  Sec(cid:173)
tion  5.3.  However,  in  [Zavrel  et al. , 1997]  the 1B1-1G  attains 
a  maximum  classification  accuracy  of 8 4 . 1 %,  this  is  a  good 
indication  of the  classification  accuracy  that  should  be  possi(cid:173)
ble  to  obtain  with  a  kernel  based  on  the  distance  defined  in 
Equation  6. 

The  fact  that  the  kernel  KIG  performs worse  than  IB1-IG, 
although  it  is  equipped  with  the  very  same  distance  metric, 
may  seem  somehow  surprising.  We  believe  this  is  because 
SVMs  perform  linear  separation  in  the  feature  space  F.  The 
decision  boundary of IB1-IG  on  the  other hand  is  non-linear. 
Due  to  the  linearity  of  the  decision  boundary  of  the  S V M, 
some  points  get  misclassified.  The  number  of misclassifica-
tions  is  controlled  by  the  parameter  C.  Choosing  a  larger 

CASE-BASED  REASONING 

137 

value  for  C  forces  the  SVM  to  avoid  classification  errors. 
However, choosing a  value for C  that is  too high  will  result 
in overfitting.  Increasing the dimensionality of F makes lin(cid:173)
ear separation easier (Cover's theorem). This can be seen by 
comparing the classification  accuracies  between the  kernels 
KIG  and  KPIG-  The  results  also  show  that  our  kernel  KPIG 
outperforms all other methods in the comparison. 

6  Conclusion 
In this paper, we have proposed a method for designing ker(cid:173)
nels for SVM learning in natural language settings. As a start(cid:173)
ing point we  took distance  functions that have  been  used  in 
memory-based learning. The resulting kernel  KPIG  achieves 
high classification  accuracy compared to other methods that 
have been applied on the same data set.  In our approach we 
started  from  the  vectors  of the  input  space,  defined  a  map(cid:173)
ping on  those vectors and worked out their inner product in 
the feature space.  All  necesary kernel conditions follow nat(cid:173)
urally from the definition of the  inner product.  We used the 
distance formula for kernels to show that the kernels are ac(cid:173)
tually  based on  the  distance  functions  from  Section  3.  The 
experimental results of the kernel  KPIG  show that increasing 
the dimensionality  in  the  feature  space  yields better results. 
We increased the dimensionality of the feature space by mak(cid:173)
ing  combinations of the  features 
,„  through  a  polynomial 
kernel.  In this way, we do not only take into account the sim(cid:173)
ilarity between corresponding features of vectors, but we also 
take  into  account  the  similarity  between  non-corresponding 
features of the vectors.  In fact this comes down to taking into 
account, to a greater extend, the context in wich a word oc(cid:173)
curs. 

We used the resolvement of PP attachment ambiguities as a 
case-study to validate our findings, but it is our belief that the 
kernel  KPIG  can be used for a wide range of natural language 
problems.  In  the  future  we  will  apply  our  kernels  in  more 
complex natural language settings and compare the results to 
other methods that have been applied on the same problems. 
At  the  moment  we  are  already  running experiments on  lan-
guage  independent  named-entity  recognition  [Sang,  2002]. 
The  first  results  look  promising,  but  it  is  too  early  to  draw 
any  significant  conclusions.  We  will  also  try  to  further ex(cid:173)
tend the kernels proposed in this paper to achieve even higher 
accuracies.  Moreover, there are still many distance functions 
reported in the literature that have not yet been investigated to 
see whether they are applicable in  SVM  learning, we intend 
to investigate such distance functions and if possible derive a 
kernel from them. 

References 
[Aha et  al.,  1991]  D. Aha, D. Kibler, and M. Albert. Instance 
based  learning algorithms.  Machine Learning,  6:37  - 66, 
1991. 

[Brown et  al.,  1999]  Michael  P.S.  Brown,  William  Noble 
Grundy,  David  Lin,  Nello  Cristianini,  Charles  Sugnet, 
Manuel Ares, and David Haussler. Support vector machine 
classification of microarray gene expression data,  1999. 

[Chih-Chung and Chi-Jen, 2002]  Chang  Chih-Chung  and 
Lin  Chi-Jen.  Libsvm:  a  library  for  support  vector  ma(cid:173)
chines, 2002. 

[Collins and Brooks,  1995]  M.J  Collins  and  J.  Brooks. 
Prepositional  phrase  attachment  through  a  backed-off 
model. 
In  Proceedings  of the  Third  Workshop  on  Very 
Large Corpora,  Cambridge,  1995. 

[Cost and Salzberg,  1993]  Scott  Cost  and  Steven  Salzberg. 
A weighted nearest neighbour algorithm for learning with 
symbolic features.  Machine Learning,  10:57-78,  1993. 

[Cristianini and Shawe-Taylor, 2000]  Nello  Cristianini  and 
John  Shawe-Taylor.  An  Introduction  to  Support  Vec(cid:173)
tor Machines  and  other  Kernel-based  Learning  Methods. 
Cambridge University Press, 2000. 

[Daelemans et al., 2002]  Walter  Daelemans,  Jakub  Zavrel, 
Ko  van  der  Sloot,  and  Antal  van  den  Bosch.  Timbl: 
Tilburg memory-based learner, version 4.3.  Technical re(cid:173)
port, Tilburg University and University of Antwerp, 2002. 
[Marcus et al,  1993]  M.  Marcus,  Santorini  B,  and  M.A. 
Marcinkiewicz.  Building a large annotated corpus of en-
glish:  The  penn  treebank.  Computational  Linguistics, 
19(2):313- 330, 1993. 

[Mitchell,  1997]  Tom  Mitchell.  Machine  Learning.  The 

McGraw-Hill Companies, Inc.,  1997. 

[Ramon, 2002]  Jan  Ramon.  Clustering and instance based 
learning  in  first  order  logic.  PhD  thesis,  K.U.  Leuven, 
Belgium, 2002. 

[Ratnaparkhi et al., 1994]  Adwait  Ratnaparkhi,  J.  Reynar, 
and  S.  Roukos.  A  maximum  entropy  model  for  prepo(cid:173)
sitional  phrase  attachment. 
In  Proceedings  of the ARPA 
Workshop  on  Human  Language  Technology,  Plainsboro, 
NJ,  1994. 

[Ratnaparkhi,  1998]  Adwait  Ratnaparkhi.  Maximum  En(cid:173)
tropy  Models for  Natural  Language  Ambiguity  Resolution. 
PhD thesis,  University of Pennsylvania, Philadelphia, PA, 
1998. 

[Sang, 2002]  Erik  F.  Tjong  Kim  Sang.  Introduction  to  the 
conll-2002 shared task:  Language-independent named en(cid:173)
tity  recognition.  In  Proceedings  of CoNLL-2002,  Taipei, 
Taiwan, pages 155 - 158, 2002. 

[Stetina and Nagao, 1997]  Jiri  Stetina  and  Makoto  Nagao. 
Corpus  based pp  attachment  ambiguity  resolution  with  a 
semantic  dictionary.,  1997.  Kyoto  University,  Yoshida 
Honmachi, Kyoto 606, Japan. 

[Vapnik,  1998]  Vladimir  Vapnik.  The  Nature  of Statistical 

Learning Theory.  Springer-Verlag, New York,  1998. 

[Veenstra et al., 2000]  Jorn  Veenstra,  Antal  van den  Bosch, 
Sabine  Buchholz,  Walter  Daelemans,  and  Jakub  Zavrel. 
Memory-based word senese disambiguation.  Computers 
and the Humanities, 34:1 -2:171 -  177, 2000. 

[Zavrel et al,  1997]  Jakub  Zavrel,  Walter  Daelemans,  and 
Jorn Veenstra.  Resolving pp attachment ambiguities with 
memory-based learning.  In  Proceedings CoNNL,  Madrid, 
pages  136 -  144. Computational Linguistics, Tilburg Uni(cid:173)
versity, 1997. 

138 

CASE-BASED  REASONING 


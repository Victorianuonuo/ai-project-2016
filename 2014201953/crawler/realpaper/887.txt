A Tighter Error Bound for Decision Tree Learning Using PAC Learnability

Chaithanya Pichuka, Raju S. Bapi, Chakravarthy Bhagvati,

Arun K. Pujari, B. L. Deekshatulu

Department of Computer and Information Sciences

University of Hyderabad

Gachibowli, Hyderabad, 500046, India

pssgrk438@yahoo.com; {bapics, chakcs, akpcs, bldcs}@uohyd.ernet.in

Abstract

Error bounds for decision trees are generally based
on depth or breadth of the tree. In this paper, we
propose a bound for error rate that depends both on
the depth and the breadth of a speciﬁc decision tree
constructed from the training samples. This bound
is derived from sample complexity estimate based
on PAC learnability. The proposed bound is com-
pared with other traditional error bounds on sev-
eral machine learning benchmark data sets as well
as on an image data set used in Content Based Im-
age Retrieval (CBIR). Experimental results demon-
strate that the proposed bound gives tighter estima-
tion of the empirical error.

1 Introduction
Computational learning theory (CoLT) deals with the char-
acterization of the difﬁculty of learning problems and the
capabilities of machine learning algorithms [Vapnik, 1998;
Vidyasagar, 1997]. The probably approximately correct
(PAC) framework is formulated to characterize classes of hy-
potheses that can be reliably learned from a reasonable (poly-
nomial) number of randomly drawn training examples with a
reasonable amount of computation. Sample complexity speci-
ﬁes the number of training examples needed for a classiﬁer to
converge, with a high probability, to a successful hypothesis.
Within the PAC framework, it is possible to derive bounds
on sample complexity by combining the expression for the
size of the hypothesis space and the empirical error [Mitchell,
1997]. Thus characterization of the hypothesis space is an im-
portant issue in CoLT.

In order to understand the representational capability of
various classiﬁers, we ran several algorithms such as percep-
tron, linear and polynomial support vector machines (SVM),
Bayes maximum likelihood (ML), decision tree (DT), oblique
decision tree, and k-nearest neighbor (kNN) classiﬁers on the
standard Iris benchmark dataset. Iris is a four-dimensional
dataset comprising three classes, of which two classes are not
linearly separable. Figure 1 shows a two-dimensional projec-
tion of the decision regions described by the trained classi-
ﬁers on the benchmark dataset. Perceptron, linear SVM and
DT deﬁne elementary decision regions comprising linear and
axis-parallel rectangular surfaces. Oblique DT, polynomial

SVM, Bayesian ML and kNN describe increasingly more
complex decision regions and hence have richer hypothesis
representation capability. It is to be noted that although lim-
ited empirical characterization of classiﬁers is possible as de-
picted in Figure 1, theoretical characterization of classiﬁers,
in general, is difﬁcult. In this paper we shall consider DTs for
further theoretical and empirical analysis.

Several techniques have been proposed for estimating the
future error rate of a decision tree classiﬁer [Kaariainen and
Langford, 2005; Mansour, 2000]. There are two primary
classes of error estimation methods available. The ﬁrst class
of methods utilize the empirical error on the training samples
in predicting the future error. Empirical error could be based
on the training set or on the test set or on both. For example,
we can use k-fold cross-validation on a dataset and then trans-
form the cross-validation estimate into an estimate or heuris-
tic conﬁdence interval of the error of the ﬁnal decision tree
learned from all examples. Another approach is the sample
compression bound which considers the test set after labelling
it, that is, it estimates the next label based on the training set
and the labelled previous test set [Langford, 2005]. There are
several other methods such as Microchoice bound [Langford
and Blum, 1999], Test set bound and Occam bounds [Lang-
ford, 2005]. Microchoice bound inherently depends on the
structure of the DT by calculating the choice spaces at every
node of the DT. Test set bound is a test set-based bound and
is entirely characterized by the errors on the labelled test set.
As the test set bound incorporates test set error directly, its es-
timate is usually good. Occam bound assumes the underlying
distribution to be Binomial and computes an estimate based
on the empirical errors observed on the training dataset.

The second class of methods utilize structural aspects of
the classiﬁer in order to arrive at an estimate. For example, in
the case of DTs, one could consider the depth or breadth of
the DT constructed on the training samples in the estimation
of future error.

In this paper we propose a structure-based error bound
for DTs using the PAC learning framework. The new error
bound considers both depth and breadth of the DT learned
from training examples. We conducted several experiments
on benchmark datasets to compare the results of various error
bound estimation methods and found that the proposed esti-
mation method works well. The rest of the paper is organized
as follows. Firstly, we describe both the structure-based and

IJCAI-07

1011

Figure 1: Decision Regions of Classiﬁers: Comparison of the hypotheses learned by various classiﬁcation methods on the Iris
machine learning benchmark dataset.

empirical error-based estimation methods. Experimental re-
sults are presented subsequently. Discussion of the results is
followed by conclusions.

2 Classiﬁer Structure-Based Error Bounds
In this section, before introducing classiﬁer structure based
bounds, we will describe PAC learning in detail and explore
how generalization bounds could be formulated based on
PAC learning framework.

2.1 PAC Learning and Error Bounds
In this section we review the relation between Probably Ap-
proximately Correct (PAC) learning framework and general-
ization error bounds [Valiant, 1984; Mitchell, 1997]. PAC
tackles the questions related to the number of examples and
the amount of computation required to learn various classes
of target functions. PAC learning framework has two major
assumptions. One that assumes that the classiﬁcation error
is bounded by some constant, ε, that can be made arbitrarily
small. The second assumption requires that the classiﬁer’s
probability of failure is bounded by some constant, δ, that
can be made arbitrarily small. In short, we require that the
classiﬁer probably learns a hypothesis that is approximately
correct – hence it is called the Probably Approximately Cor-
rect (PAC) learning framework. Deﬁning the error on entire
instance distribution (T ) of data as true error (errorT ) and
the error on training set (D) as empirical error (errorD),
PAC learnability can be deﬁned formally as follows.

Deﬁnition [Mitchell, 1997]: Consider a concept class C
deﬁned over a set of instances X of length n and a learner L
using hypothesis space H. C is PAC-Learnable by L using

1
2 , and δ such that 0 < δ <

H if for all c  C, distribution T over X, ε such that 0 <
1
2 , learner L will with
ε <
probability at least (1 − δ) output a hypothesis h ∈ H such
that errorT(h) ≤ ε, in time that is polynomial in 1
δ , n,
and size(c). This can also be represented mathematically as
follows.

ε , 1

P [errorT(h) > ε] ≤ δ
P [errorT(h) < ε] ≥ (1 − δ)

(or)

(1)

PAC-Learnability concept can be used to derive a general
lower bound on the size m of the training set required by a
learner. There are two possible scenarios. In the ﬁrst sce-
nario, we assume that the learner is consistent, that is, the
learner outputs hypotheses that are consistent with the tar-
get concept on the training set. The probability that a single
hypothesis having true error greater than ε and is consistent
with the training set is at most (1 − ε). And the probability
that this hypothesis will be consistent with m individuals is at
most (1 − ε)m. If, instead of one, there are k such hypotheses
whose true error is greater than ε and are consistent with the
training set, then the upper bound on the probability that at
least one of these k hypotheses will be consistent with all the
m randomly drawn individuals is, k ∗ (1 − ε)m. Using the fact
that k ≤ |H|, we can write down the following inequality in
Equation 2.

P [errorT (h) > ε] ≤ H ∗ (1 − ε)m ≤ δ

(2)
We can rewrite this as shown in Equation 3 and solving for
m (the number of training examples needed by the learning
algorithm) results in the inequality shown in Equation 4.

IJCAI-07

1012

p[errorT (h) > ε] ≤ H ∗ e

(−mε) ≤ δ

m ≥

1

ε

(ln|H| + ln(

))

1

δ

(3)

(4)

In the second scenario when the learner is agnostic, that
is, the true error on the training set is not necessarily zero,
then we can use Chernoff approximation to estimate the er-
ror bound in the PAC learning framework [Mitchell, 1997].
Analogous to Equation 4, the error bound for agnostic learner
can be derived and is shown in Equation 5.

m ≥

1
2ε2

(ln|H| + ln(

))

1

δ

(5)

Now, we can use Equation 5 to derive the generalization

error bound as shown in Equation 6.

(cid:2)

ε ≥

1
2m

(ln|H| + ln(

))

1

δ

(6)

We can write this in terms of true error (errorT(h)) and

training error (errorD(h)) as shown in Equation 7.

(cid:2)

errorT(h) ≤ errorD(h) +

1
2m

(ln|H| + ln(

))

1

δ

(7)

Coming back to the original problem of deriving error
bounds for DTs, Equation 7 can be used if only we know
the size of the hypothesis space, |H|. Thus for PAC based
error bounds, we need an estimate of |H|. There are three
ways of estimating |H| for DTs. The ﬁrst approach is to es-
timate based on the depth of DT, the second method is to
estimate based on the breadth of DT, and the third possibility
is to use both of these measures. We propose a bound based
on the third approach wherein recursive estimation of |H| is
done. In the subsequent sub-sections, we describe the three
structure-based error bounds for DTs.

2.2 Depth-Based Error Bound for DT (PAC I)
Computation of |H| is based on depth of the tree and the
approach is akin to the one shown in the lecture notes of
Guestrin [Guestrin, 2005]. We assume that the DTs are bi-
nary trees and use the idea that every k-depth decision tree
will have (k − 1)-depth decision trees as its children.
Approximation of |H| for PAC I
Given n attributes,

= N umber of decision trees of depth k
= 2
= (choices of root attribute)∗

Hk
H0
Hk+1

Hk+1
Hk
if Lk
Lk+1
So, Lk

(possible lef t subtrees)∗
(possible right subtrees)

2k

= n ∗ Hk ∗ Hk
= 22k ∗ n
−1
= log2Hk and L0 = 1
= log2n + 2Lk
= (2k − 1)(1 + log2n) + 1

If we substitute this into Equation 7, then we get the expres-
sion for error bound for decision tree using the depth feature
as shown in Equation 9.

errorT(h) ≤ errorD(h)+

(cid:3)

ln2
2m

((2k − 1)(1 + log2n) + 1 + ln( 1

δ

))

(9)

2.3 Breadth-Based Error Bound for DT (PAC II)

In this method, computation of |H| is based on breadth of the
tree and the approach is akin to the one shown in the lecture
notes of Guestrin [Guestrin, 2005]. We assume that the DTs
are binary trees and use the idea that every k-leaves decision
tree will have (k − 1)-leaves decision trees in it.

Approximation of |H| for PAC II

Given n attributes,

Hk = N umber of decision trees of leaves k
H0 = 2

k−1(cid:4)

Hk = n

HiHk−i

(10)

Hk = n

i=1
k−1(k + 1)2k−1

The last step in Equation 10 is a rough (crude) approxi-
mation to Hk from the previous step. If we substitute this
into Equation 7, then we get the expression for error bound
for decision tree using the breadth feature as shown in Equa-
tion 11. The crude approximation will be replaced by a better
approximation obtained by recursive estimation in PAC(III).

errorT(h) ≤ errorD(h)+

(cid:3)

1
2m

((n − 1)ln(n) + (2k − 1)ln(k + 1) + ln( 1

δ

))
(11)

2.4 Depth and Breadth-Based Error Bound for DT

(PAC III)

We propose this new approach for calculating error bounds
for DTs. This approximation is very much similar to PAC II
although the resulting bound will be tighter as we will show
empirically. The simple idea behind this approach is that it
is important to consider several structural features of the DT
to get a closer approximation of |H| which in turn leads to a
tighter estimate for the error bound.

(8)

IJCAI-07

1013

Approximation of |H| for PAC III
Given n attributes,

Hk
H0
H1
H2

Hk

W here,

Fk

F1

= N umber of decision trees of leaves k
= 2
= n ∗ H0 ∗ H0
= n ∗ H1 ∗ H1
...
= n

k−1 ∗ Fk ∗ H
k
1

HiHk−i = n

k−1(cid:4)
k−1(cid:4)

i=1

=

FiFk−i

i=1

= 1 (Initial condition)

(12)
We have to substitute the expression for H into Equation 7

to get error bound for DT using this approximation.

3 Empirical Error-Based Bounds
In this section, we will describe generalization bounds for-
mulated based on the empirical error observed on the dataset.
Unlike the structure-based error bounds discussed in the pre-
vious section, bounds reviewed in this section explicitly in-
corporate empirical errors observed on the datasets — train-
ing and test sets.

3.1 Microchoice Bound
Microchoice approach tries to estimate an a priori bound
on the true error based on the sequential trace of the algo-
rithm [Langford and Blum, 1999]. When a learning algo-
rithm is applied on training set of n instances, the algorithm
successively makes a sequence of choices c1, . . . , cn from a
sequence of choice spaces, C1, . . . , Cn ﬁnally producing a
hypothesis, h ∈ H. Speciﬁcally, the algorithm looks at the
choice space C1 to produce choice c1. The choice c1 in turn
determines the next choice space C2. It is to be noted here
that at every stage the previous choices are eliminated from
consideration at the next stage. The algorithm again looks at
the data to make the next choice c2 ∈ C2. This choice then
determines the next choice space C3, and so on. These choice
spaces can be thought of as nodes in a choice tree, where
each node in the tree corresponds to some internal state of
the learning algorithm, and a node belonging to some choice
space Ci.

We can apply this Microchoice bound to decision tree by
taking into account the choice spaces available at every node
of the decision tree and then using PAC bound technique to
get the error bound as shown in Equation 13.

k(cid:4)

i=1

3.2 Test Set Bound
In this bound assumptions about the error distribution are
made. In particular, classiﬁcation error distribution is mod-
elled as coin ﬂips distribution or the Binomial distribution as
shown in Equation 16.

Bin( k
m

, cD) =

( m
j

)c

j

D(1 − cD)m−j

(14)

The expression in Equation 14 computes the probability
that m examples (coins) with error rate cD produce k or fewer
errors. We can interpret the Binomial tail as the probability
of an empirical error greater than or equal to k
m . But we are
interested in error bound for a classiﬁer given the probability
of error δ and m, so we deﬁne Binomial tail inversion as given
in Equation 15 which gives the largest true error such that the
probability of observing k

m or more errors is at least δ.

, errorT(h)) = maxp{p : Bin( k

Bin( k
, p) ≥ δ} (15)
m
Now, given the number of test instances m, test set bound

m

can be formulated as shown in Equation 16.
Test Set Bound :

P [errorT(h) ≤ Bin(errortest, δ)] ≥ 1 − δ

(16)
One serious drawback of the test set bound is that it is
possible that the test set and training sets are incompatible
and thereby introduce inaccuracies in the error bound estima-
tion [Kaariainen and Langford, 2005].
3.3 Occam’s Razor Bound
This is can be termed as a training set-based bound as it takes
the training set performance into consideration. This is rea-
sonable as many learning algorithms implicitly assume that
the train set accuracy behaves like the true error. Addition-
ally, in this bound we need to know the prior probability of
the hypotheses.
Occam’s Razor Bound:
For all priors P (h), over all classiﬁers h, for all δ ∈ (0, 1),

P [errorT(h) ≤ Bin(errorD(h), δP (h))] ≥ 1 − δ
(17)
We obtain the Occam’s razor bound by negating the above

Equation 17.

P [errorT(h) ≥ Bin(errorD (h), δP (h))] < δ

(18)
It is very important to notice that the prior P (h) must be
selected before looking at the instances. We can relax the
Occam’s Razor bound with the entropy Chernoff bound to
get a somewhat more tractable expression [Langford, 2005].
Chernoff Occam’s Razor Bound:
For all priors P(h), over all classiﬁers h, for all δ ∈ (0, 1),

(cid:8)

errorT(h) ≤ errorD(h) +

(cid:5)(cid:6)(cid:6)(cid:7) 1

(

2m

(cid:4)

i∈N odes(DT )

Ci + ln(

))

1

δ

(13)

P [errT(h) ≥ errD(h) +

1
2m

(ln(

1

P (h)

) + ln(

1

δ

))] < δ

(19)
The application of the Occam’s Razor bound is somewhat
more complicated than the application of the test set bound.

IJCAI-07

1014

Table 1: Results of experiments for error bound calculating on 15 different data sets.

Data Set
Weather

Yellow-small-

Adult-
Adult+

Yellow-small+
Contact Lenses

Labor
Monk1
Monk2
Monk3

Voting-records

CRX

Tic-tac-toe
Segment

CBIR

S
14
16
20
20
20
24
57
432
432
432
435
690
958
1500
696

P(I)
0.489
0.521
0.462
0.462
0.462
0.532
0.368
0.374
0.423
0.374
0.32
0.419
0.958
0.168
0.812

P(II)
0.356
0.409
0.362
0.362
0.362
0.354
0.220
0.211
0.271
0.212
0.18
0.129
0.247
0.154
0.296

P(III)
0.356
0.409
0.362
0.362
0.362
0.347
0.220
0.167
0.206
0.168
0.149
0.107
0.201
0.123
0.219

MC
0.490
0.473
0.425
0.425
0.4294
0.466
0.394
0.296
0.325
0.276
0.219
0.150
0.237
0.184
0.321

Occ
0.953
0.889
0.793
0.793
0.793
0.734
0.424
0.288
0.288
0.274
0.215
0.150
0.230
0.113
0.271

Test
0.485
0.263
0.427
0.427
0.388
0.394
0.294
0.190
0.188
0.255
0.158
0.139
0.221
0.077
0.190

Emp ATT
0.25
0.263
0.05
0.05
0.04
0.096
0.071
0.138
0.131
0.106
0.056
0.104
0.087
0.025
0.184

4
5
5
5
5
5
17
7
7
7
17
16
10
20
15

B
1
1
1
1
1
1.1
0.7
5
7.2
4.5
3.5
3.5
12.5
8.6
11

D
1
1
1
1
1
1.7
1.5
4.9
5.2
4.9
4.6
5.5
4.1
9.4
11

E
3.5
4.2
1
1
0.8
2.3
4.1
59.9
56.6
46.2
24.5
72.3
84
38.8
128

S: Size; P(I): PAC(I); P(II): PAC(II); P(III): PAC(III); MC: Microchoice; Occ: Occam’s Razor; Test: Test set; Emp: Empirical

Error; ATT: Number of Attributes; B: Average Breadth of DTs; D: Average Depth of DTs; E: Average count of

misclassiﬁcation errors

lustrate how various error bounds are computed. In this case
the breadth and depth are one each and the choice space size
at each node is also indicated in Figure 2. The number of
attributes, n, is 5.

schemes.

theoretical estimates of

The H value computed for various error bounds in
Equation 20 are now substituted in Equation 7 to ob-
tain the ﬁnal
the error bound
Taking m = 14 and
under different
δ = 0.05, we obtain the following values for er-
ror bounds:
and
PAC(III) = 0.362. For the Microchoice bound, we need to
take Equation 13 and substitute the choice-space sizes from
the DT in Figure 2. Then the Microchoice error works out to
be 0.524.

PAC(II) = 0.362;

PAC(I) = 0.462;

5 Empirical Results
We have done experiments on 14 different machine learning
benchmark datasets from the UCI repository. The results re-
ported are averaged over 10 experiments conducted on ran-
domly chosen subsets of the datasets. Training was done on
66% of the data and testing on the remaining 33%. In the case
of image dataset from content based image retrieval (CBIR)
system, due to the complexity of the experiment only one run
was conducted. The results are summarized in Table 1. Ta-
ble reports the observed error rates (on training set and test
set) as empirical error. The goal of the experimentation is to
see if the theoretical bounds estimated from various methods
come close to the observed empirical error. The bold-faced
entries in Table 1 correspond to the best estimate of the true
error. From the table, as expected we can see that PAC(III)
always gives better estimate than PAC(I) and PAC(II). Depth
based measures overestimate the size of the tree since they
assume a complete binary tree and in reality, the DT may be

Figure 2: Example DT: corresponding to Adult- dataset
where breadth and depth are 1 and the quantity in parenthesis
denotes the size of the choice space at every node.

4 Illustrative Example

P AC − I :

P AC − II :

−1 [Eq 9]
−1 = 20

2k
Hk = 5k ∗ n
H1 = 51 ∗ 421
Hk = n
H1 = 50(2)2∗1−1 = 1 ∗ 2 = 2

k−1(k + 1)2k−1 [Eq 11]

P AC − III : Hk = n

HiHk−i [Eq 12]

k−1(cid:4)
(cid:4)

i=1

H1 = 2 [k = 1, Init. Cond.]

M icrochoice : H =

Ci

i∈N odes(DT )

H = 14 + 8

(20)
Here we consider an example experimental dataset,
namely, the Adult- dataset. The DT structure obtained from
one of the ten experiments on Adult- dataset is chosen to il-

IJCAI-07

1015

[Langford, 2005] J. Langford. Tutorial on practical predic-
tion theory for classiﬁcation. Journal of Machine Learning
Research, 6:273–306, September 2005.

[Mansour, 2000] Y. Mansour. Generaliztion bounds for de-

cision tree. In COLT, pages 69–74, 2000.

[Mitchell, 1997] T. M. Mitchell. Machine Learning. The

McGraw-Hill Companies, Singapore, 1997.

[Valiant, 1984] L. Valiant. A theory of the leranable. Com-

munications of the ACM, 27:1134–1142, 1984.

[Vapnik, 1998] V. N. Vapnik. Statistical Learning Theory.

Wiley & Sons, Inc., 1998.

[Vidyasagar, 1997] M. Vidyasagar. A Theory of Learning

and Generalization. Springer-Verlag, New York, 1997.

far from complete. Thus PAC(I) estimates will be inferior to
those of PAC(III). This can be clearly observed in the table
where the PAC(I) estimates become worse with increasing
depth. In PAC(II), breadth of DT is considered. However,
since a crude approximation of the actual value of H is con-
sidered in PAC(II) as discussed in Section 2.3, PAC(II) does
not give a better estimate compared to PAC(III).

From Table 1 we can also observe that PAC(III) always out-
performs the microchoice and Occam’s razor bounds. Mi-
crochoice bound is an estimate of H based on choice spaces
available at every node of DT and the choice space sizes can
lead to overestimation of H. Test set and Occam’s bounds
rely on binomial distribution based measures which take total
number of instances and observed errors into consideration.
Conceptually, it is not straight forward to compare PAC(III)
and binomial theory based bounds. However it can be ob-
served that whenever empirical error is low, we tend to get
good estimation from Occam and Test set bounds. Test set
bound performed well in ﬁve out of the ﬁfteen experiments
whereas PAC(III) fared well in the remaining 10 experiments.
It appears that in the majority of these ﬁve cases, empirical
(test) error was low and the average breadth of the DT was
high. These may be the possible reasons for poorer estimation
by the PAC(III) approach. Further, it is easy to see that these
empirical results seem reasonable from theoretical consider-
ations also when different expressions for |H| are compared
for the three PAC-based bounds.

Results in Table 1 suggest a possible subset relation
among the various error bounds studied in this paper.

P AC(I) ≥ P AC(II) ≥ P AC(III)
M icro choice ≥ P AC(III)
Occam ≥ P AC(III)

6 Conclusion
In this paper, we proposed a bound for error rate that depends
both on the depth and the breadth of a speciﬁc decision tree
constructed from the training samples. PAC leaning frame-
work is used to derive this bound. The proposed bound is
compared with other traditional error bounds on several ma-
chine learning benchmark data sets and on an image data set.
Experimental results demonstrate that the proposed bound
gives tighter estimation of the empirical error. The bound
we have obtained here is considerably tighter than previous
bounds for Decision Tree classiﬁers. We arrived at a possible
subset relations among various structure-based and empirical
error-based bounds.

References
[Guestrin, 2005] C. Guestrin. Lecture notes, Carnegie Mel-
lon University (ML course No: 10701/15781). February
2005.

[Kaariainen and Langford, 2005] M.

and
A comparison of tight generalization
In International conference on Machine

J. Langford.
error bounds.
Learning, volume 119, pages 409–416, August 2005.

Kaariainen

[Langford and Blum, 1999] J. Langford and A. Blum. Mi-
crochoice bounds and self bounding learning algorithms.
In COLT, pages 209–214, 1999.

IJCAI-07

1016


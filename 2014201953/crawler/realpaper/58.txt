Repairing Concavities in ROC Curves

Peter A. Flach

Department of Computer Science

University of Bristol

Bristol, United Kingdom

Shaomin Wu

School of Construction Management

and Engineering, University of Reading

Reading, United Kingdom

Peter.Flach@bristol.ac.uk

Shaomin.Wu@reading.ac.uk

Abstract

In this paper we investigate methods to detect and
repair concavities in ROC curves by manipulating
model predictions. The basic idea is that, if a point
or a set of points lies below the line spanned by two
other points in ROC space, we can use this informa-
tion to repair the concavity. This effectively builds
a hybrid model combining the two better models
with an inversion of the poorer models; in the case
of ranking classiﬁers, it means that certain inter-
vals of the scores are identiﬁed as unreliable and
candidates for inversion. We report very encour-
aging results on 23 UCI data sets, particularly for
naive Bayes where the use of two validation folds
yielded signiﬁcant improvements on more than half
of them, with only one loss.

Introduction

1
There is an increasing amount of work on model selection and
model combination in the machine learning and data mining
literature: for instance, model selection based on ROC space
[Provost and Fawcett, 2001], model combination by means
of bagging [Breiman, 1996], boosting [Freund and Schapire,
1996], arcing [Breiman, 1998], the mixture of experts method
[Jacobs et al., 1991], to name just a few. A review on en-
sembles of learning machines can be found in [Valentini and
Masulli, 2002].

Typically, these methods assume a set of given models with
ﬁxed performance, and the issue is how best to combine these
models to obtain a better ensemble model. There is no at-
tempt to analyse the performance of the given models to de-
termine a region where performance is sub-standard. This pa-
per investigates methods to improve given models using ROC
analysis.

ROC (Receiver Operating Characteristic) analysis is usu-
ally associated with classiﬁer selection when both class and
misclassiﬁcation cost distribution are unknown at training
time. However, ROC analysis has a much broader scope
that is not limited to cost-sensitive classiﬁcation. A categor-
ical classiﬁer is mapped to a point in ROC space by means
of its false positive rate on the X-axis and its true positive
rate on the Y-axis. A probabilistic classiﬁer results in a ROC

curve, which aggregates its behaviour for all possible deci-
sion thresholds. The quality of a probabilistic classiﬁer can be
measured by the Area Under the ROC Curve (AUC), which
measures how well the classiﬁer separates the two classes
without reference to a decision threshold. A good classiﬁer
should have a large AUC, and AUC=1 means that there is
a decision threshold such that the corresponding categorical
classiﬁer has 100% accuracy.

We use the term model repair to denote approaches that
modify given models in order to obtain better models.
In
contrast, ensemble methods produce hybrid models that leave
the original models intact. An approach to model construc-
tion using ROC space is given in [Blockeel and Struyf, 2002],
where the authors identify and assemble parts of a decision
tree that perform well in different areas of ROC space. Our
approach in this paper is to identify ‘bad’ areas, or concav-
ities, in a ROC curve and repair them by manipulating the
corresponding low-quality predictions. The approach is ex-
perimentally validated using both naive Bayes and decision
tree, but the approach has much wider scope as it can be ap-
plied to any classiﬁer that computes class scores.

To illustrate the approach, we describe in Section 2 the Re-
pairPoint algorithm that combines three models based on dif-
ferent thresholds of the same probabilistic classiﬁer, and cre-
ates a new model which theoretically should improve upon
the worst of the three models. In Section 3 we introduce the
main algorithm RepairSection, that mirrors an entire con-
cave region (a region of the curve that is below its convex
hull).
In Section 4 we present experimental results on 23
data sets from the UCI repository. Section 5 reviews some
related work on model ensembles, gives the main conclusion
and suggests further work.

2 Basics of repairing classiﬁers in ROC space
Assume that the confusion matrix of a classiﬁer evaluated on
a test set is as in Table 1. Then the true positive rate of the
classiﬁer is a/(a + b) and the false positive rate of the classi-
ﬁer is c/(c + d). The point (c/(c + d), a/(a + b)) in the XY
plane (i.e., ROC space) will be used to represent the perfor-
mance of this classiﬁer.

If a model is under the ascending diagonal in ROC space,
this means that it performs worse than random. Models
A and B in Figure 1 are such worse-than-random models.

predicted positive

predicted negative

actual positive
actual negative

a
c

b
d

Table 1: A confusion matrix.

However, there is a very useful trick to obtain better-than-
random models: simply invert all predictions of the origi-
nal model. This corresponds to exchanging the columns in
the contingency table, leading to a new true positive rate of
b/(a + b) = 1 − a/(a + b), i.e. one minus the original true
positive rate; similarly we obtain a new false positive rate of
d/(c + d) = 1− c/(c + d). Geometrically, this corresponds
to mirroring the original ROC point through the midpoint on
the ascending diagonal.

Given three models Model 1, Model 2 and Model 3, output
Model 4 that operates as follows:
1.

If both Model 1 and Model 2 predict negative, then pre-
dict negative;
If both Model 1 and Model 2 predict positive, then predict
positive;
If Model 1 predicts negative and Model 2 predicts posi-
tive, then predict the opposite of what Model 3 predicts;

2.

3.

4. Otherwise, predict what Model 3 predicts.

Table 2: Algorithm RepairPoint. The last clause does not
apply under the inclusion constraints and is only added for
completeness.

siﬁed negative by Model 1 and positive by Models 3 and 2
((T P3 − T P1) ∪ (FP3 − FP1)); those classiﬁed negative by
Models 1 and 3 and positive by Model 2 ((T P2 − T P3) ∪
(FP2 − FP3)); and those classiﬁed negative by all three mod-
els ((POS− T P2)∪ (NEG− FP2), where POS and NEG are
the sets of all positive and all negative examples, respec-
tively). By construction, Model 4 classiﬁes the ﬁrst group as
positive, the second group as negative, the third group as pos-
itive, and the fourth group as negative. The true positives of
Model 4 are thus T P1∪(T P2−T P3); because of the inclusion
constraints the result follows (analogous for false positives).

Figure 1: By inverting their predictions, worse-than-random
models A and B below the diagonal can be transformed into
better-than-random models -A and -B above the diagonal.

Notice that the ascending diagonal really connects two
classiﬁers:
the classiﬁer which always predicts negative in
(0,0), and the classiﬁer which always predicts positive in
(1,1). This suggests that the above repair procedure can be
generalised to line segments connecting arbitrary classiﬁers.
For instance, consider Figure 2. Denote the sets of true and
false positives of Model i by T Pi and FPi, then we can con-
struct Model 4 under the condition that T P1 ⊆ T P3 ⊆ T P2
and FP1 ⊆ FP3 ⊆ FP2.
In particular, these inclusion con-
straints are satisﬁed if Models 1, 2 and 3 are obtained by
setting thresholds on the same probabilistic model, which is
what we assume throughout the paper.

Model 4 operates as indicated in Table 2. The inclusion
constraints guarantee that the geometric conﬁguration of Fig-
ure 2 holds. This is formally stated in the following theorem.
Theorem 1 Assuming that T P1 ⊆ T P3 ⊆ T P2 and FP1 ⊆
FP3 ⊆ FP2, the model produced by Algorithm RepairPoint
has true and false positive rates T Pr4 = T Pr1 + T Pr2− T Pr3
and FPr4 = FPr1 + FPr2 − FPr3, where T Pri and FPri de-
note true and false positive rates of Model i.
Proof. Under the inclusion constraints expressed in the the-
orem, there are four disjoint groups of examples: those clas-
siﬁed positive by Models 1, 3 and 2 (T P1 ∪ FP1); those clas-

Figure 2: Model 3 is mirrored to Model 4 with the help of
Models 1 and 2.

An equivalent construction is the following. Remove from
the test set all instances classiﬁed positive by Model 1, and all
instances classiﬁed negative by Model 2. We can imagine this
as a smaller nested ROC space in which Model 1 represents
(0,0) and Model 2 represents (1,1); because of the inclusion
constraints the position of Model 3 remains unchanged. Note
that Model 3 performs worse than a random model in this
nested ROC space. We then construct Model 4 as in Figure 1,
by inverting the predictions of Model 3 on the remaining test
examples.

We end this section by noting that the true and false posi-
tive rates derived for Model 4 only hold for the same test set
on which Models 1, 2 and 3 were evaluated. On a second, in-
dependent test set the ROC locations of the 4 classiﬁers may
be different – in particular, Model 3 may not be below the

line connecting Models 1 and 2, in which case Model 4 will
be evaluated worse than Model 3.
In our experiments, we
therefore use validation sets to decide whether concavities are
stable across different samples. This will be further discussed
in Section 4.

3

Identifying and repairing concavities in a
ROC curve

The previous section outlined the main ideas underlying re-
pairing concavities in ROC curves. However, preliminary ex-
periments indicated that the three-point approach is too crude
to work well in practice.
In this section we introduce our
main algorithm, which manipulates a whole section of a ROC
curve. A ROC curve is obtained by evaluating a probabilistic
classiﬁer on a test set and varying the decision threshold, re-
sulting in a step curve [Hand and Till, 2001]. An efﬁcient
way of constructing this curve is by ranking the instances
corresponding to their predicted probability of being positive
[Fawcett, 2003]. Figure 3 shows both the ROC curve for a
probabilistic model evaluated on a small test set1 and its con-
vex hull [Provost and Fawcett, 2001].

score calculated by the probabilistic model in this interval,
and output a constant score (e.g., the mid-point of the inter-
val). Assuming that ties are broken by assigning a random
rank, this would replace the concave region of the ROC curve
with the line segment cd. It is interesting to note that if this
procedure is followed for all concavities, this corresponds to
constructing the convex hull by discretising the probability
scores.

However, in theory we should be able to do better than that:
we can invert the ranking of the instances in the probability
interval, which can be seen as applying Algorithm Repair-
Point to all thresholds in this interval. For instance, by ap-
plying this algorithm to the model corresponding to threshold
c2, this point would be point-mirrored through the midpoint
on line segment cd to the other side of the convex hull. The
same can be done for the other thresholds. The resulting ROC
curve is shown in Figure 4. We can note that the area C under
the curve has been replaced by an equally large area C0 above
the curve. The AUC of the repaired curve is therefore larger
than both the AUC of the original curve and the AUC of its
convex hull.

Figure 3: A probabilistic ROC curve and its convex hull.

Figure 4: Mirroring a concave part of the ROC curve.

Four points of the ROC curve (point a, b, c and d) are lo-
cated on the convex hull. Each of these points corresponds
to a probability threshold, and thus each segment of the con-
vex hull corresponds to a probability interval. For instance,
the convex hull in Figure 3 has three segments corresponding
to three disjoint probability intervals. Three out of these ﬁve
segments delineate concave regions of the ROC curve, indi-
cated as A, B and C. For example, area A is delineated by line
ab and the ROC curve between point a and point b. In gen-
eral, a concave area means that the ranking obtained from the
probabilistic model in this probability interval is worse than
random. For instance, consider area C which is the largest
concavity. One way to repair this concavity is by ignoring the

1For illustration purposes, the test set contains only a few ex-
amples and the resolution of the ROC curve is low.
In practical
circumstances a step curve with much higher resolution is obtained.

The algorithm to produce the model with the repaired ROC
curve is given in Table 3. The procedure works for any model
that calculates a score; a probabilistic model is a special case.

Given a scoring model M and two thresholds T1 > T2, construct
a scoring model M0 predicting scores as follows. Let S be the
score predicted by M:
1.
2.
3. Otherwise, predict T1 + T2 − S.

If S > T1, then predict S;
If S < T2, then predict S;

Table 3: Algorithm RepairSection. The algorithm effec-
tively inverts the ranking of all instances whose score as pre-
dicted by M falls in the interval T1 < S < T2.

4 Experimental evaluation
We describe a number of experiments to evaluate our ap-
proach. We used 23 two-class data sets from the UCI repos-
itory [Blake and Merz, 1998]. Table 4 shows their numbers
of attributes, numbers of examples, and relative size of the
majority class.

Dataset
Australia
Sonar
Glass
German
Car
Anneal
Monk1
Monk2
Monk3
Hepatitis
House
Tic-tac-toe
Heart
Ionosphere
Breast Cancer
Lymphography
Primary Tumor
Soybean-large
Solar-Flare
Hayes-Roth
Credit
Balance
Bridges

#Attrs
14
60
9
20
6
38
6
6
6
19
16
9
13
34
9
17
17
35
12
4
15
4
12

#Exs %MajClass
690
208
214
1000
1728
798
566
601
554
155
435
958
270
351
286
148
339
683
323
133
690
625
108

55.51
51.92
67.29
69.40
69.68
74.44
50.00
65.72
55.41
78.71
62.07
64.20
55.56
64.10
70.28
56.81
55.75
55.51
56.35
60.91
55.51
53.92
66.67

Table 4: UCI data sets used in our experiments.

The experimental procedure for the ﬁrst experiment was
as follows. We split a data set into ten folds and use eight
of them for training, one for validation and one for testing.
We trained a naive Bayes model and a decision tree model2
on the training data, and chose two thresholds that delineate
a concavity. We then produced a new model by repairing
the probabilities between the thresholds; we only use the new
model if it improves AUC on the validation set. The detailed
procedure is given in Table 5.

We ran Experiment RepairSection ten times and obtained
m pairs of AUC values. Since we use a validation set, we are
able to decide whether or not repair resulted in a better model
– if not, we discard it and use the original, unrepaired model.
For this reason, we only report results on the non-discarded
models, so m may be smaller than 100. The average AUCs of
the unrepaired curve and the repaired curve for each data set
are given in Tables 6 and 7. We also performed a paired t-test
with m − 1 degrees of freedom and level of conﬁdence 0.1
to test the signiﬁcance of the average difference in AUC. The
results are favourable: the signiﬁcance tests yield 10 wins and
3 losses for repaired naive Bayes models and 11 wins and 5
losses for repaired decision tree models.

Experiments without using a validation set yielded worse
results, so the use of a validation set appears crucial. We

2Notice that decision trees can be viewed as scoring classiﬁers

[Ferri et al., 2002; Provost and Domingos, 2003].

1.

2.

3.

4.

5.

Train a naive Bayes or decision tree model M on the train-
ing data; construct a ROC curve C and its convex hull H
on the training data.
Find adjacent points on H such that in this interval the
area between C and H is largest. Let T1 and T2 be the
corresponding score thresholds.
Produce a new probabilistic model M0 by calling Repair-
Section(T1, T2).
Evaluate M and M0 on the validation set, construct their
If AUC(M0) ≤
ROC curves and calculate their AUCs.
AUC(M) then go to 6.
Evaluate M and M0 on the test set, construct their ROC
curves and calculate their AUCs.

6. Go to 1. until each fold has been used as a test set.

Table 5: Experiment RepairSection.

Dataset

Australia
Sonar
Glass
German
Car
Anneal
Monk1
Monk2
Monk3
Hepatitis
House
Tic-Tac-Toe
Heart
Ionosphere
Breast Cancer
Lymphography
Primary Tumor
Soybean-Large
Solar-Flare
Hayes-Roth
Credit
Balance
Bridges*
Average

AUC
(Original)
90.9 ± 0.88
77.5 ± 1.30
76.6 ± 1.43
79.3 ± 0.88
99.0 ± 0.079
86.7 ± 0.48
75.4 ± 0.70
64.6 ± 0.71
96.2 ± 0.29
86.3 ± 1.94
96.3 ± 0.35
74.1 ± 0.61
91.0 ± 1.04
93.2 ± 0.69
76.1 ± 1.63
90.3 ± 1.39
79.6 ± 1.02
91.9 ± 0.46
91.1 ± 0.71
89.4 ± 1.53
90.8 ± 0.68
98.4 ± 0.27
92.7 ± 1.66
86.41

AUC
(Repaired)
90.6 ± 0.94
76.8 ± 1.31
80.6 ± 1.36
78.9 ± 0.88
99.0 ± 0.08
90.2 ± 0.47
77.4 ± 0.70
66.1 ± 0.92
97.7 ± 0.24
85.1 ± 1.22
96.3 ± 0.35
75.6 ± 0.58
90.2 ± 1.08
93.1 ± 0.690
77.7 ± 1.57
91.0 ± 1.49
80.6 ± 1.03
91.9 ± 0.46
91.2 ± 0.74
91.3 ± 1.58
90.7 ± 0.65
98.4 ± 0.29
92.9 ± 1.89
87.1

Better
?
×
∨
×
∨
∨
∨
∨

∨
×
∨
∨
∨

∨

Table 6: Results of Experiment RepairSection with naive
Bayes.
*For the Bridges data set we used 5-fold cross-
validation.

therefore conducted an experiment with two validation folds:
we would only use the repaired model if its AUC was higher
on both validation folds. The results for naive Bayes are
shown in Table 8. We now obtain 12 signiﬁcant wins and
only 1 signiﬁcant loss, and the average increase in accuracy
is more than a percentage-point. Interestingly, two validation
folds didn’t work well for decision trees.

Finally, we conducted an experiment with naive Bayes
whereby we selected all concavities, and repaired those that
occurred on two validation sets. The results were similar to
the results in the ﬁrst experiment (11 signiﬁcant wins and
three losses), with some of the wins and losses occurring on

Dataset

Australia
Sonar
Glass
German
Car
Anneal
Monk1
Monk2
Monk3
Hepatitis
House
Tic-Tac-Toe
Heart
Ionosphere
Breast Cancer
Lymphography
Primary Tumor
Soybean-Large
Solar-Flare
Hayes-Roth
Credit
Balance
Bridges
Average

AUC
(Original)
82.27 ±0.50
87.35 ±1.20
76.09 ±1.34
80.78 ±0.55
99.05 ±0.074
86.86 ±0.41
75.82 ±0.74
66.46 ±0.66
96.30 ±0.32
91.92 ±2.84
96.18 ±0.31
74.81 ±0.54
91.72 ±1.34
96.57 ±0.91
74.82 ±1.71
87.71 ±1.86
76.63 ±1.23
90.93 ±0.50
86.61 ±1.59
86.87 ±2.24
91.09 ±0.65
99.37 ±0.18
85.57 ±4.36
86.16

AUC
Better
(Repaired)
?
82.28 ±0.49
88.92 ±1.01
∨
80.69 ±1.15
∨
81.01 ±0.54
∨
99.07 ±0.081
90.64 ±0.37
∨
78.05 ±0.704 ∨
67.95 ±0.662 ∨
98.14 ±98.14 ∨
91.92 ±2.84
96.66 ±0.29
∨
75.97 ±0.56
∨
90.77 ±1.21
×
95.88 ±0.62
75.75 ±1.51
87.88 ±1.89
75.51 ±1.36
91.10 ±0.52
85.65 ±1.65
88.33 ±2.08
90.92 ±0.64
99.42 ±0.189
81.71 ±3.35
86.71

∨
×
×
×
∨

×

Table 7: Results of Experiment RepairSection with decision
trees.

Dataset

Australia
Sonar
Glass
German
Car
Anneal
Monk1
Monk2
Monk3
Hepatitis
House
Tic-Tac-Toe
Heart
Ionosphere
Breast Cancer
Lymphography
Primary Tumor
Soybean-Large
Solar-Flare
Hayes-Roth
Credit
Balance
Average

AUC
(Original)
87.17±2.28
81.34±2.46
74.67±1.61
82.33±0.62
99.25±0.091
87.06±0.53
75.68±1.12
65.74±0.89
96.75±0.47
92.67±3.79
96.76±0.37
75.62±0.69
92.91±1.29
97.07±0.77
78.20±2.43
86.94±2.41
78.14±1.31
91.36±0.45
89.69±1.90
84.89±3.56
89.03±1.56
99.53±0.027
86.49

AUC
(Swapped)
86.86±2.33
84.61±2.66
74.75±1.42
82.72±0.60
99.29±0.098
90.32±0.48
78.65±0.92
67.24±0.95
98.54±0.25
93.19±3.37
96.76±3.44
76.92±0.71
92.98±1.42
97.53±0.57
79.16±2.12
90.49±1.94
80.19±1.27
91.63±0.44
89.94±1.88
88.33±3.09
88.67 ±1.70
99.10±0.078
87.63

Better
?
×
∨
∨
∨
∨
∨
∨
∨
∨
∨

∨
∨

∨

Table 8: Results with naive Bayes using two validation folds.

different data sets. It appears that repairing only the largest
concavity and using two validation sets is the best strategy (at
least for naive Bayes).

5 Discussion and conclusions

The work reported in this paper bears some similarity with en-
semble methods. Bagging and boosting are two well-known
ensemble approaches. Both approaches are implemented by
re-sampling methods. In bagging [Breiman, 1996], the en-
semble is formed by making bootstrap replicates of the train-
ing data sets and then multiple generated hypotheses are used
to get an aggregated predictor. Boosting algorithms [Freund
and Schapire, 1996] assign different weights to training in-
stances depending on whether they are correctly classiﬁed.
The approaches presented in this paper do not make use of
re-sampling techniques.

Another relevant ensemble method is majority voting
[Kimura and Shridar, 1991; Lam and Sue, 1997], in which the
class predicted by the ensemble is the most predicted class
among the base classiﬁers. Algorithm RepairPoint in this
paper uses a kind of voting: when both Model 1 and Model
2 (see Figure 2) agree on the classiﬁcation of an instance,
then we choose that class. Otherwise, we choose the class
not predicted by Model 3. In majority voting, on the other
hand, we would choose the class predicted by Model 3. The
difference is that majority voting does not take the quality of
the different models into account, whereas our repair scheme
knows that Model 3 is sub-optimal and therefore corrects it
predictions in the relevant region.

ROC curves contain a wealth of information about the per-
formance of one or more classiﬁers, which can be utilised to
construct better models. They have been used to ﬁnd optimal
labelling of decision trees [Ferri et al., 2002] and to ﬁnd good
decision thresholds for probabilistic classiﬁers [Lachiche and
Flach, 2003].
In this paper we have proposed a novel ap-
proach to construct new models by repairing concavities in
a ROC curve. The ﬁrst method, RepairPoint, works on a
probabilistic classiﬁer with three probability thresholds, and
tries to improve the poorest model with help of the other two.
Preliminary experimental results (not reported) showed that
this didn’t work too well, but this may be due to the fact that
the selection of the threshold for Model 3 is not easy. The
threshold is a point chosen based on the ROC curve of the
training data set; this point (see point c3 in Figure 3) has the
farthest distance to the convex hull. If this threshold is not
optimal on the test data (for instance, the position c3 in the
ROC curve on the test data set unfortunately is located in the
position c2), the AUC after repair becomes worse. Still, we
believe that the idea of mirroring models around lines in ROC
space will prove to be very useful. An interesting investiga-
tion for future work is whether a similar method can be made
to work if the models are not obtained from a single scoring
model (this would invalidate Theorem 1, i.e., the position of
Model 4 may be different from the point obtained by point-
mirroring).

The second method, RepairSection, locates and repairs
an entire concave region of a ROC curve. Experimental re-
sults were very encouraging for both naive Bayes and de-
cision trees. For naive Bayes we were able to improve re-
sults even further by using two validation folds, but this didn’t
work for decision trees. We are currently investigating why
this is so. One possible explanation is that ROC curves ob-

[Freund and Schapire, 1996] Y. Freund and R. E. Schapire.
Experiments with a new boosting algorithm. In L. Saitta,
editor, Proceedings of the 13th International Conference
on Machine Learning, pages 148–156. Morgan Kaufmann,
1996.

[Hand and Till, 2001] D. Hand and R. Till. A simple gener-
alisation of the area under the ROC curve for multiple class
classiﬁcation problems. Machine Learning, 45(2):171–
186, 2001.

[Jacobs et al., 1991] R. Jacobs, M. Jordan, S. Nowlan, and
G. Hinton. Adaptive mixtures of local experts. Neural
Computation, 3(1):79–87, 1991.

[Kimura and Shridar, 1991] F. Kimura and M. Shridar.
Handwritten numerical recognition based on multiple al-
gorithms. Pattern Recognition, 24(10):969–983, 1991.

[Lachiche and Flach, 2003] N. Lachiche and P. Flach.

Im-
proving accuracy and cost of two-class and multi-class
probabilistic classiﬁers using ROC curves. In T. Fawcett
and N. Mishra, editors, Proceedings of the 20th Interna-
tional Conference on Machine Learning, pages 416–423.
AAAI Press, 2003.

[Lam and Sue, 1997] L. Lam and C. Sue. Application of ma-
jority voting to pattern recognition: An analysis of its be-
haviour and performance. IEEE Transactions on Systems,
Man and Cybernetics, 27(5):553–568, 1997.

[Provost and Domingos, 2003] F. Provost and P. Domingos.
Tree induction for probability-based ranking. Machine
Learning, 52(3):199–215, 2003.

[Provost and Fawcett, 2001] F. Provost and T. Fawcett. Ro-
bust classiﬁcation for imprecise environments. Machine
Learning, 42(3):203–231, 2001.

[Valentini and Masulli, 2002] G. Valentini and F. Masulli.
Ensembles of learning machines.
In M. Marinaro and
R. Tagliaferri, editors, 13th Italian Workshop on Neural
Nets, volume 2486 of Lecture Notes in Computer Science,
pages 3–22. Springer-Verlag, 2002.

tained from decision trees have lower resolution (because all
instances in a leaf receive the same predicted probability),
which may mean that concavities are less stable across sam-
ples. Pruning may be another factor, as it has been shown that
pruning is detrimental for probability prediction [Provost and
Domingos, 2003; Ferri et al., 2003].

There are several other ways in which this work could be
taken further. One is to investigate how much repair is possi-
ble, by concentrating on ROC curves with large concavities,
possibly from artiﬁcial data sets. Another is to work with
averaged ROC curves that are obtained by cross-validation
(since each instance occurs in the test fold exactly once, an
averaged ROC curve can be simply constructed by combin-
ing all instances with their predicted probabilities).

Acknowledgments
A preliminary version of this paper (without the experimen-
tal results with decision trees and two validation folds) ap-
peared as [Flach and Wu, 2003]. We gratefully acknowledge
the constructive comments made by the anonymous review-
ers. We would also like to thank Rich Roberts for performing
additional experiments.

References
[Blake and Merz, 1998] C.

repository

UCI
1998.
MLRepository.html.

C. Merz.
databases,
http://www.ics.uci.edu/˜mlearn/

Blake
of machine

learning

and

[Blockeel and Struyf, 2002] H. Blockeel and J. Struyf. De-
riving biased classiﬁers for improved ROC performance.
Informatica, 26(1):77–84, 2002.

[Breiman, 1996] L. Breiman. Bagging predictors. Machine

Learning, 24(2):123–140, 1996.

[Breiman, 1998] L. Breiman. Arcing classiﬁers. Annals of

Statistics, 26(3):801–849, 1998.

[Fawcett, 2003] T. Fawcett. ROC graphs: Notes and prac-
tical considerations for data mining researchers. Tech-
nical Report HPL-2003-4, HP Laboratories, Palo Alto,
CA, USA, 2003. www.purl.org/net/tfawcett/
papers/HPL-2003-4.pdf.

[Ferri et al., 2002] C. Ferri, P. Flach, and J. Hernandez-
Orallo. Decision tree learning using the area under the
ROC curve. In C. Sammut and A. Hoffman, editors, Pro-
ceedings of the 19th International Conference on Machine
Learning, pages 139–146. Morgan Kaufmann, 2002.

[Ferri et al., 2003] C. Ferri, P. Flach, and J. Hernandez-
Improving the AUC of probabilistic estimation
Orallo.
In N. Lavraˇc, D. Gamberger, L. Todorovski, and
trees.
H. Blockeel, editors, Proceedings of the 14th European
Conference on Machine Learning, volume 2837 of Lec-
ture Notes in Computer Science, pages 121–132. Springer-
Verlag, 2003.

[Flach and Wu, 2003] P. Flach and S. Wu. Repairing concav-
ities in ROC curves. In J. Rossiter and T. Martin, editors,
Proceedings of the 2003 UK Workshop on Computational
Intelligence, pages 38–44. University of Bristol, 2003.


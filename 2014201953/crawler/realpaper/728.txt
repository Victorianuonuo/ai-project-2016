Cooperating Reasoning Processes:
More than just the Sum of their Parts

∗

Alan Bundy

School of Informatics
University of Edinburgh

A.Bundy@ed.ac.uk

Abstract

Using the achievements of my research group over
the last 30+ years, I provide evidence to support the
following hypothesis:

By complementing each other, cooperat-
ing reasoning process can achieve much
more than they could if they only acted
individually.

Most of the work of my group has been on pro-
cesses for mathematical reasoning and its applica-
tions, e.g. to formal methods. The reasoning pro-
cesses we have studied include:
Proof Search: by meta-level

inference, proof
planning, abstraction, analogy, symmetry, and
reasoning with diagrams.

Representation Discovery, Formation and Evolution:

by analysing, diagnosing and repairing failed
proof and planning attempts, forming and
repairing new concepts and conjectures, and
forming logical representations of informally
stated problems.

∗

I would like to thank the many colleagues with whom I have
worked over the last 30+ years on the research reported in this paper:
MECHO/PRESS (Lawrence Byrd, George Luger, Chris Mellish, Rob
Milne, Richard O’Keefe, Martha Palmer (n´ee Stone), Leon Sterling,
Bernard Silver and Bob Welham) ECO (Robert Muetzelfeldt, Mandy
Haggith, Dave Robertson and Mike Uschold), Proof Planning (Dave
Barker-Plummer, David Basin, Richard Boulton, James Brotherston,
Francisco Cantu, Claudio Castellini, Ewen Denney, Louise Den-
nis, Lucas Dixon, Jacques Fleuriot, Jason Gallagher, Jeremy Gow,
Ian Green, Jane Hesketh, Christian Horn, Andrew Ireland, Predrag
Janiˇci´c, Helen Lowe, Ina Kraan, Ewen Maclean, Pete Madden, R´aul
Monroy, Julian Richardson, Alan Smaill, Andrew Stevens, Frank
van Harmelen, Lincoln Wallen and Geraint Wiggins), ISANEWT
Hazel Duncan and Amos Storkey, TM/HR (Alison Pease, Simon
Colton, Alan Smaill, Graham Steel, John Lee and Toby Walsh) and
ORS (Fiona McNeill, Marco Schorlemmer and Chris Walton). I’m
grateful for feedback on an earlier draft from David Basin, James
Brotherston, Claudio Castellini, Simon Colton, Priya Gopalan, Jane
Hesketh, Andrew Ireland, Predrag Janiˇci´c, Erica Melis, Chris Mel-
lish, R´aul Monroy, J¨org Siekmann and Sean Wilson. The research
reported in this paper was supported by numerous grants and stu-
dentships, the most recent of which is EPSRC GR/S01771.

Other: learning of new proof methods from exam-
ple proofs, ﬁnding counter-examples, reason-
ing under uncertainty, the presentation of and
interaction with proofs, the automation of in-
formal argument.

In particular, we have studied how these different
kinds of process can complement each other, and
cooperate to achieve complex goals.
We have applied this work to the following areas:
proof by mathematical induction and co-induction;
analysis; equation solving, mechanics problems;
the building of ecological models; the synthesis,
veriﬁcation,
transformation and editing of both
hardware and software, including logic, functional
and imperative programs, security protocols and
process algebras; the conﬁguration of hardware;
game playing and cognitive modelling.

1 Introduction

“Many hands make light work” (John Heywood)

Much research in Artiﬁcial Intelligence consists of invent-
ing or developing a new technique: analysing and establish-
ing its properties, implementing and testing it, comparing
it with rival techniques for the same task, etc. Other work
consists of combining several techniques into a complex sys-
tem that solves problems, models natural systems, etc. It is
commonly observed that complementary techniques can as-
sist each other, extending their range of automation and/or
effectiveness. For instance, this was the theme of a previous
IJCAI paper of mine [Bundy, 1985]. It is not just that different
techniques are needed to tackle different aspects of a complex
task; one technique can also assist another by addressing its
shortcomings. In the current paper we examine this observa-
tion in more detail, giving examples of how techniques can
complement each other: achieving more than the sum of their
parts. The examples are drawn from the work of my research
group, over more than 30 years, aimed at automating mathe-
matical reasoning.

This same point could be illustrated by many pieces of
work. However, given the context of this paper and the space
limitations of the IJCAI-07 proceedings, I have limited my-
self to work in which I played some role, albeit a small one

IJCAI-07

2

in some cases. My apologies to those people whose work I
have not surveyed; profuse apologies to those I have not even
cited.

2 Object-level and Meta-level Inference

“Not all who wander are lost.” (John Ronald Reuel
Tolkien)

inductive proofs, but we have found widespread applications
in other types of problem. Typically, we want to rewrite the
goal in such a way as to separate the similarities and dif-
ferences while preserving the similarities. This enables the
given(s) to be used to prove all or part of the transformed
goal. We can visualise this process by annotating the goal
and the given(s) with their similarities and differences (see
Figure3 1).

Much of our work has been on automatic inference, es-
pecially proving mathematical theorems. Most work in this
area builds on mathematical logic, which is used to represent
the conjectures to be proved, the axioms of the theories in
which they are to be proved, and the rules of inference with
which they are to be proved. These logical formulae pro-
vide a search space of potential proof steps. Typically, proof
search is conducted backwards, starting with the original con-
jecture as the main goal and applying the rules of inference
backwards to exchange each goal for a set of subgoals. Nav-
igating this search space is the major problem in automated
theorem proving. It is called the combinatorial explosion, be-
cause the number of subgoals increases super-exponentially
with the length of the proof, rapidly swamping the computer’s
memory for all but trivial problems.

Human mathematicians succeed in navigating these huge
search spaces by stepping back from the low-level details and
thinking at a higher-level about the nature of the problem and
the suitability of their tools for solving such problems. We
have been trying to emulate this high-level thinking in our
automated provers. In particular, we have implemented proof
search as simultaneous and cooperating inference at two lev-
els:
the object-level, in which the original conjecture, the-
ory and rules are expressed, and the meta-level, in which the
proof methods are speciﬁed and the conjectures analysed. In
meta-level inference, the conjecture is analysed, a suitable
proof method identiﬁed and applied, progress is measured
and the process recurses. In this way, inference at the meta-
level directs inference at the object-level, avoiding unproduc-
tive parts of the search space and making proof search com-
putationally tractable.

We have applied this two-level approach many times, but
most notably in the domains of equation solving using the
PRESS1 system [Bundy and Welham, 1981; Sterling et al.,
1989] and inductive theorem proving using Clam/Oyster2
[Bundy et al., 1991; Bundy, 2001]. The latest realisation of
this idea is proof planning , [Bundy, 1988; 1991], in which
proof methods are speciﬁed as STRIPS-like plan operators and
plan formation is used to custom-build a proof plan for each
conjecture.

To illustrate these ideas I will describe rippling, a proof
method for reducing the difference between a goal and one
or more givens [Bundy et al., 2005a]. It frequently occurs in
mathematics that the goal to be proved is syntactically simi-
lar to a “given”, i.e. a hypothesis, assumption or axiom. This
occurs, most notably, in inductive proof, where the induction
conclusion and induction hypothesis have strong similarities
by construction. Rippling was originally developed for such

t <> (y <> z) = (t <> y) <> z

↑

h :: t

<> (y <> z) = ( h :: t

h :: t <> (y <> z)

↑

= h :: t <> y)

↑

↑

<> y) <> z

(cid:3)
(cid:3)
(cid:3)
(cid:3)

↑

h :: t <> (y <> z)
h = h ∧ t <> (y <> z) = (t <> y) <> z

= h :: (t <> y) <> z
↑

<> z
↑

The example shows the step case of an inductive proof
of the associativity of list append, where <> is the inﬁx
notation for append and :: is the inﬁx notation for cons.
(cid:2) separates the induction hypothesis (the given) from the
induction conclusion (the goal). The formulae are anno-
tated to show the differences and similarities between the
given and the goal. The grey boxes indicate the parts of
the goal which differ from the given. They are called
wave-fronts. Each wave-front has one or more wave-
holes indicating sub-terms of the wave-fronts which cor-
respond to parts of the given. The parts of the goal out-
side the wave-fronts or inside the wave-holes, are called
the skeleton. The skeleton always matches the induction
hypothesis. The arrows indicate the direction of move-
ment of the wave-fronts — in this case outwards through
the goal until they completely surround the skeleton.
The rules are also annotated and annotation must also
match when rules are applied. Such annotated rules are
called wave-rules. The main wave-rule used in this ex-
ample is:

H :: T

↑

<> L ⇒ H :: T <> L

↑

which is taken from the step case of the recursive deﬁni-
tion of <>.
Note how the grey boxes get bigger at each wave-rule
application with more of the skeleton embedded within
them, until they contain a complete instance of the given.

Figure 1: The Associativity of Append using Rippling

Rippling successfully guides proof search by ensuring that
the skeleton gets larger until it matches the givens. At this
point the givens can be used to prove the goal. It has been
successfully applied in induction, summing series, analysis
and a variety of other areas. More importantly, if and when

1Prolog Equation Solving System.
2And in later proof planners, such as λClam and ISAPLANNER

3Tip to reader: the mathematical details are optional and have all

been put in ﬁgures separated from the main text.

IJCAI-07

3

it fails, the nature of the failure can be used to suggest a way
to patch the proof attempt and to recover from the failure (see
§3).

Meta-level inference provides least-commitment devices,
such as meta-variables, which can be used to postpone search
decisions. For instance, in inductive proof, choosing an
appropriate induction rule is an inﬁnite branching point in
the search: there is an induction rule corresponding to each
well-ordering of each recursive data-structure, such as nat-
ural numbers, lists, trees, sets, etc. The key to a success-
ful induction is often to choose an induction rule that inserts
wave-fronts into the induction conclusion for which there are
matching wave-rules, so that rippling is successful. We can
turn this requirement on its head by postponing the choice
of induction rule, inserting higher-order meta-variables into
the induction conclusion to stand for the unknown wave-
front, using higher-order uniﬁcation to instantiate these meta-
variables during rippling, and retrospectively choosing an in-
duction rule when the wave-fronts are fully instantiated. This
signiﬁcantly reduces the search problem by moving it from
a place (the choice of induction rule) where the branching
is inﬁnite, to a place (rippling) where it is highly constrained.
Because we are effectively developing the middle of the proof
before the beginning or the end, we call this search moving
technique middle-out reasoning, in contrast to top-down or
bottom-up reasoning.

We have applied middle-out reasoning to the choice of in-
duction rule in two PhD projects. Initially, Ina Kraan used it
to select an appropriate induction rule from a pre-determined,
ﬁnite set of rules, as part of a project to synthesise logic
programs from their speciﬁcations [Kraan et al., 1996]. In
this project, Kraan also used middle-out reasoning to syn-
thesise the logic program. More recently, Jeremy Gow ex-
tended middle-out reasoning to create and verify new induc-
tion rules, customised to the current conjecture [Gow, 2004;
Bundy et al., 2005b].

applications and extensions, for instance:

Proof planning is now a thriving research area, with many
• The ΩMEGA proof planner [Siekmann et al., 2006],
which has been applied to a wide range of areas in math-
ematics;
• Applications to the combination and augmentation of
• Multi-agent proof planning [Benzm¨uller and Sorge,
• Reasoning about feature interaction in ﬁrst-order tempo-
• Multi-strategy proof planning [Melis et al., 2006].

decision procedures [Janiˇci´c and Bundy, 2002];

ral logic [Castellini and Smaill, 2005]

2001];

I have also proposed it as the basis for a “Science of Reason-
ing”, in which proof plans provide a multi-level understand-
ing of the structure of proofs [Bundy, 1991]

Meta-level inference, and proof planning in particular,
shows how two complementary inference processes can inter-
act to reduce the amount of search and overcome the combi-
natorial explosion. Meta-level analysis matches object-level
goals to the proof methods best suited to solve them, cutting

out a lot of legal, but unproductive, object-level, rule applica-
tions. Middle-out reasoning can rearrange the search space,
postponing difﬁcult search decisions until they are made eas-
ier as a side-effect of other search decisions. The object-level
inference then provides the goals that form the ammunition
for the next stage of meta-level analysis.

3 Inference and Fault Diagnosis

“The best laid plans of mice and men gang aft
aglay.” (Rabbie Burns)

Like all plans, proof plans are not guaranteed to succeed.
When they fail, recovery requires another kind of reasoning
process: fault diagnosis. When standard object-level proof
search fails, the usual response is for the proof search to back-
up to some earlier choice point and retry. Proof planning
enables a more productive use to be made of failure. The
mismatch between the situation anticipated at the meta-level
and the situation obtaining at the object-level provides an op-
portunity to analyse the failure and propose a patch. Human
mathematicians also make a productive use of failure. See,
for instance, [van der Waerden, 1971] for an example of re-
peated plan failure and patching in an, eventually successful,
attempt to prove a challenging conjecture.
Consider, as another example, a failure of the rippling
method outlined in §2. This might occur, for instance, be-
cause there is no rule available to ripple the wave-front out
one more stage. From the meta-level inference that formed
the rippling-based proof plan, we can extract a partial descrip-
tion of the form that the missing wave-rule should take. This
is best illustrated by an example. See Figure 2.

We have implemented this kind of fault diagnosis and re-
pair within our proof planning framework using proof critics
[Ireland, 1992; Ireland and Bundy, 1996]. The original work
explored ways to recover from different kinds of failure of
rippling, suggesting, variously, new forms of induction, case
splits, generalisations and intermediate lemmas. Later work
by Ireland and his co-workers has extended this to discover-
ing loop invariants in the veriﬁcation of imperative programs
[Ireland and Stark, 2001]. They have recently applied this
work to industrial veriﬁcation by extending and successfully
evaluating Praxis’ automated prover [Ireland et al., 2006].
Deepak Kapur and M. Subramaniam have also developed
similar methods for lemma discovery in induction [Kapur and
Subramaniam, 1996]. R´aul Monroy has used critics for the
correction of false conjectures [Monroy et al., 1994], which
requires the additional reasoning process of abduction. More
recently, Monroy has applied his techniques to higher-order
faulty conjectures, leading to the automatic formation of new
theories, such as monoids [Monroy, 2001]. Andreas Meier
and Erica Melis have adapted their multi-strategy MULTI sys-
tem to include failure analysis and the automatic proposal
of “recommendations” to overcome them [Meier and Melis,
2005].

Fault diagnosis of an earlier failure suggests the most
promising alternative proof attempt. It removes the need for
blind backtracking, removing legal, but unproductive choices
from the search space and helping to defeat the combinatorial
explosion. Moreover, the proof patches often provide inter-

IJCAI-07

4

rev(rev(l)) = l

↑

(cid:3) rev(rev( h :: t
(cid:3) rev( rev(t) <> (h :: nil)

)) = h :: t
↑

)

↑

↑

= h :: t

(cid:2)

(cid:3)(cid:4)

blocked

(cid:5)

rev is a list reversing function. The example is taken
from a failed proof that reversing a list twice will give
you the original list. Suppose that, initially, the only rule
available is:

rev( H :: T

↑

) ⇒ rev(T ) <> (H :: nil)

↑

The ripple attempt will fail because no rule matches
the left hand side of the goal, so the wave-front can-
not be moved one stage further outwards. However, by
analysing the blocked ripple, fault diagnosis can work out
that the missing wave-rule should take the form:
) ⇒ F ( rev(X) , X, Y )

rev( X <> Y

↑

↑

where F stands for some unknown function, repre-
sented by a higher-order meta-variable. If we allow the
proof to continue using this missing wave-rule schema,
then the proof will succeed, instantiating F (u, v, w) to
rev(w) <> u in the process. We now discover the miss-
ing wave-rule to be:

rev( X <> Y

↑

) ⇒ rev(Y ) <> rev(X)

↑

which must be proved as a lemma to complete the proof.
Note that this lemma is a distributive law of <> over
rev, and is an interesting theorem in its own right, rather
than just an arbitrary hack needed just to get the original
proof to go through.

Figure 2: Lemma Speculation Using a Rippling Failure

esting information. As the example in Figure 2 illustrates,
lemma speculation often produces lemmas that are of math-
ematical interest in their own right and prove to be useful in
future proofs. Generalisation, which is another way of patch-
ing failed inductive proofs, also often generalises the original
conjecture in a mathematically interesting way.

In the other direction, the kind of fault diagnosis illustrated
in Figure 2 is only possible because of the interaction of meta-
level and object-level inference. It is the discrepancy between
the meta-level expectation and the object-level reality that fo-
cuses the fault diagnosis to suggest a patch that will resolve
this discrepancy and allow the original proof plan to be re-
sumed. Thus, again, we see complementary reasoning pro-
cesses focus their attention to solve a problem that none of
them could manage alone — nor even acting simultaneously
but without interaction.

4 Inference and Learning

“To this day, we continue to ﬁnd new techniques
and methods. There’s always something new to
learn.” (Joseph Cocking)

We have seen the value of proof methods when used by
meta-level inference or fault diagnosis to guide object-level
proof search, but where do such proof methods come from?
Mostly, they have come from detailed analysis of a fam-
ily of related proofs, and reﬂection on the thinking that in-
forms experienced mathematical reasoners. But this is a
time-consuming, highly-skilled and error-prone process. The
proof methods that my group developed for inductive proof,
for instance, took more than a decade to develop, evaluate and
reﬁne. It is necessary to maintain constant vigilance against
quick, ad hoc ﬁxes that will not have general utility and ex-
planatory power. It would be much better if the process of
developing new proof methods could be automated.

My group has explored various machine learning tech-
niques to automate the construction of proof methods. These
start with successful object-level proofs whose meta-level
structure they try to analyse and abstract, in order to formulate
new proof methods. Our earliest attempt to do this was the
application of explanation-based generalisation (EBG) to the
equation-solving domain [Silver, 1985]. Silver’s LEARNING
PRESS system was able to learn (sometimes simpliﬁed) ver-
sions of all the proof methods that had been hand-coded into
our PRESS equation solving system, just by applying EBG to
examples of successfully solved equations. Later, Desimone
did the same for early versions of our inductive proof methods
[Desimone, 1989].

Recently, we have explored the use of data-mining tech-
niques to extract new methods4 from large corpuses of proofs
[Duncan et al., 2004]. Duncan’s IsaNewT5 system ﬁrst iden-
tiﬁes frequently occurring sequences of proof steps using
variable-length Markov models (VLMM) [Ron et al., 1996].
Then it combines these sequences using genetic algorithms
to join them with branching, repetition and macros, using a
language developed in [Jamnik et al., 2002]. The resulting
proof methods have been evaluated successfully by compar-
ing proof success and timings on a large test corpus of proofs
with and without the newly learnt tactics. Currently, the learnt
methods consist of simple, generic combinations of object-
level rules. They do not approach the sophistication nor tar-
geting of many hand-crafted methods, such as rippling. Nor
do they provide dramatic search reductions. However, this
use of data-mining has made a promising start and has the
potential for much more development.

We have introduced a new class of reasoning processes
from machine learning. These complement meta-level in-
ference by constructing new proof methods from example
proofs, thus enabling meta-level inference to guide object-
level proof search. The learning methods are, in turn, in-
formed by previous success of object-level inference in pro-

4Technically, it is just tactics that are learnt, rather than methods,
since, as discussed below, we are currently unable to learn the meta-
language for specifying these tactics: methods being tactics together
with their meta-level speciﬁcations.

5Is a New Tactic.

IJCAI-07

5

ducing proofs. Their analysis is informed by the patterns they
identify in these proofs. Thus we have a virtuous triangle of
reasoning processes, each process in the triangle using infor-
mation from one of its neighbours to assist the other neigh-
bour.

However, there is a missing process in this story: a learn-
ing process that can construct the meta-level language used to
specify the proof methods. Examples of such a language are
the concepts of wave-fronts, wave-holes and skeletons, intro-
duced and illustrated in Figure 1. Silver’s and Desimone’s
EBG-based systems assumed the prior existence of an appro-
priate meta-level language and used it in the analysis of the
object-level proofs. Duncan’s IsaNewT does not assume the
existence of such a meta-language, so is, thereby, restricted
to an impoverished language for expressing proof methods. It
cannot use conditional branching, since the conditions would
have to be expressed in the missing meta-language. It uses
non-deterministic branching instead. Similarly, it cannot use
until-loops, since again the exit condition would have to be
expressed in the missing meta-language. It just uses repeti-
tion. It cannot combine the proof methods with proof plan-
ning or analyse failures with fault diagnosis, as the method
speciﬁcations necessary to do this would need to be expressed
in the missing meta-language. Instead, it is reduced to con-
ducting exhaustive search at the method level, which reduces
search, but not in as directed a way as proof planning is capa-
ble of.

Current AI learning mechanisms appear not to be capable
of performing the learning task required here: constructing a
new ontology. There are plenty of mechanisms for deﬁning
new concepts in terms of old ones, but that is not sufﬁcient
to form a new meta-language. The concept of wave-front,
for instance, cannot be deﬁned in terms of the functions and
predicates in the object-language. Here’s a hard challenge for
the next generation of machine learning researchers. For a
start on tackling this challenge, see §6.
5 Inference and Representation Formation
“Once you know the formula, it’s a guiding light
that will give you everything you want to know.”
(Mark Lallemand)

As discussed in §2, the automation of inference requires
that the axioms, rules and conjectures of a theory be repre-
sented in the computer. Where do such representations come
from? In most work in automated inference, either they are
adopted/adapted from a textbook or from previous research,
or they are the outcome of careful and highly-skilled hand-
crafting. However, developing an appropriate representation
is one of the most important parts of problem solving. Ap-
plied mathematics, for instance, mainly consists of exploring
and developing mathematical representations of physical en-
vironments and problems. So, in a thorough investigation of
automatic reasoning, the formation of formal representations
ought to be a central concern.

In the MECHO6 Project ([Bundy et al., 1979]) we ad-
dressed this issue by building a program for solving mechan-
ics problems stated in English, with examples drawn from the

6Mechanics Oracle.

English GCE A-Level, applied-mathematics papers, intended
for 16-18 year old pre-university entrants. MECHO took me-
chanics problems stated in English, parsed them, constructed
a semantic representation in the form of ﬁrst-order logic as-
sertions, extracted equations from this logical representation
and then solved them. This process is illustrated in Figure 3.
During this process, cooperation between representation
formation and inference occurs at a number of levels and in
several directions.
• The whole process of representation enables inference,
in the form of equation solving, to solve the mechanics
problem.
• A form of non-monotonic inference is needed to “ﬂesh
out” the explicit problem statement. Real-world objects
must be idealised, and this can be done in a variety of
ways. For instance, a ship might be idealised as a parti-
cle on a horizontal plane in a relative velocity problem,
but in a speciﬁc gravity problem it will be idealised as
a 3D shell ﬂoating on a liquid. The string in a pulley
problem is usually idealised as inelastic and the pulley
as frictionless. MECHO uses cues to recognise the prob-
lem type and then matches a problem-type schema to the
extracted assertions, which then provides the implicit,
unstated assertions.
• MECHO uses a form of plan formation7 to extract a set
of equations from the assertions. A physical law is iden-
tiﬁed that relates the goals to the givens, e.g., F = m.a.
These equations may introduce intermediate variables,
which must also be solved for, causing the planning pro-
cess to recurse. Preconditions of the physical laws relate
the variables to the assertions, enabling the laws to be in-
stantiated to equations describing the problem situation.
• Equation solving is used to express the goal variable in
terms of the givens. This inferential process is imple-
mented using meta-level inference, as described in §2.
• Inference is also required during natural language pro-
cessing, e.g., to resolve ambiguities, such as pronoun
reference. The meta-level inference mechanisms used
in MECHO were motivated by the problems of control-
ling inference both in problem-solving and semantic in-
terpretation, and were successful in both areas. For in-
stance, the inferential choices arising during semantic
processing are limited by idealisation, especially by the
framework imposed by problem-type schemata.

The ideas from the MECHO project were subsequently
adapted to the ECO program, which built an ecological simu-
lation program in response to a dialogue with a user [Robert-
son et al., 1991]. ECO incorporated a further example of
collaborative reasoning: a meta-level analysis of the user’s
requirements was used to propose two kinds of idealisation.
ECO was able to suggest both idealisations of real world eco-
logical objects, e.g., animals, plants, environments, etc., and
idealisations of the processes by which they interacted, e.g.,

7We called it the “Marples Algorithm” in honour of David
Marples, who was the ﬁrst person we were aware of to describe this
process in some teaching notes.

IJCAI-07

6

The input is a mechanics problem, posed in English, taken from an applied mathematics textbook. This is ﬁrst parsed to identify
its syntactic structure and then turned into a set of logical assertions by semantic analysis. These assertions must be “ﬂeshed out”
by cuing schemata to provide unstated, but implicit, default information. The givens and goals of the problem are identiﬁed and
a planning process is used to instantiate general physical laws into a set of simultaneous equations from which the goals can be
derived from the givens. The PRESS equation solver is then used to express the goals in terms of the givens.

Figure 3: Representations Used by the MECHO Program

logistic growth, predator-prey equation, competition equa-
tion, etc.

6 Inference, Fault Diagnosis and Ontology

Repair

“I shall try to correct errors when shown to be er-
rors, and I shall adopt new views so fast as they
shall appear to be true views” (Abraham Lincoln )

MECHO and ECO formed representations with the aid of
natural language processing tools and inference, but from a
ﬁxed signature, by which I mean the variables, functions and
predicates, along with their types, that were used to form the
logical formulae. However, as we saw in §4, it is sometimes
necessary to invent new concepts and to create new functions,
predicates, types, etc. with which to represent these new con-
cepts.

6.1 Repairing Faulty Conjectures
Alison Pease and Simon Colton’s TM8 program, [Colton and
Pease, 2005], implements some ideas of Lakatos [Lakatos,
1976] for repairing faulty mathematical theories. In particu-
lar, it takes nearly true conjectures and repairs them into the-
orems. TM achieves this by orchestrating: the OTTER the-
orem prover [McCune, 1990]; the MACE counter-example
ﬁnder [McCune, 1994] and the HR9 machine learning system
[Colton et al., 1999; Colton, 2002].

Given a false conjecture, ﬁrst MACE is used to ﬁnd both
a set of counter-examples and a set of supporting examples.
TM then implements Lakatos’s methods of strategic with-
drawal and counter-example barring to modify the original,
false conjecture into new conjectures that it hopes are true.
In strategic withdrawal, HR ﬁnds a concept that describes a
maximal subset of supporting examples and the theorem is
specialised to just that concept. In counter-example barring,
HR ﬁnds a concept that covers all the counter-examples, and a

8Theorem Modiﬁer.
9Hardy & Ramanujan.

IJCAI-07

7

minimal subset of supporting examples, then makes the nega-
tion of that concept a precondition of the revised theorem.
These new conjectures are given to OTTER to verify. The TM
system thus exhibits the interaction of three kinds of reason-
ing: model generation, concept formation and inference, to
do something that neither of them could do alone: correcting
faulty conjectures. An example is given in Figure 4.

planning agent must diagnose the fault with its version of the
rule; repair the rule; and replan with the amended rule. This
repair may just consist of adding or deleting a precondition,
or it may go deeper, requiring a change to the signature with
which the preconditions are expressed. An example is given
in Figure 5.

TM was given the following faulty conjecture in Ring
Theory:

∀x, y. x2 ∗ y ∗ x2 = e

where e is the multiplicative identity element.
MACE found 7 supporting examples and 6 counter-
examples to this conjecture. Given these two sets of ex-
amples, HR invented a concept that can be simpliﬁed to:

∀z. z2 = z + z

and used it for strategic withdrawal. OTTER was then
able to prove the original conjecture for just those rings
with this new property.

Figure 4: Correcting a Faulty Conjecture in Ring Theory

HR has also been used a component of a system for the
automatic generation of classiﬁcation theorems for algebra
models up to isomorphism and isotopism, [Colton et al.,
2004; Sorge et al., 2006]. This system employs an intri-
cate interplay of machine learning, computer algebra, model
generation, theorem proving and satisﬁability solving meth-
ods, and has produced new results in pure mathematics. For
instance, it generated an isotopic classiﬁcation theorem for
loops of size 6, which extended the previously known result
that there are 22. This result was previously beyond the capa-
bilities of automated reasoning techniques.

6.2 Repairing Faulty Signatures
However, note that HR does not extend the signature of the
representation language.
Its new concepts are deﬁned in
terms of a ﬁxed set of functions, relations and types. In con-
trast, Fiona McNeill’s ORS10 modiﬁes the underlying signa-
ture, e.g., by changing the arity and types of functions and
predicates, and by merging or separating functions and pred-
icates [Bundy et al., 2006; McNeill, 2005]. ORS is designed
for a virtual world of interacting software agents. It achieves
its goals by forming and executing plans that combine the
services of other agents.

However, these plans are fallible and may fail on execu-
tion. Plan formation can be viewed as an automated proof of
an existential theorem, where the witness of the existentially
quantiﬁed variable is a plan, and the proof shows that this
plan is guaranteed to achieve its goal. This plan may, nev-
ertheless, fail to execute, because the planning agent’s world
model is faulty. In particular, its theory of the circumstances
under which other agents will perform certain services are ex-
pressed as preconditions of action rules. However, the other
agents may have different versions of these action rules. The

10Ontology Repair System.

Suppose that the planning agent’s plan contains an action
to pay a hotel bill, represented as

(Pay PA Hotel $200)

where PA is the planning agent. This action fails on ex-
ecution.
Just prior to failure there is a short dialogue
with the Hotel agent. Such inter-agent dialogues are the
normal way to check those preconditions of whose truth
value the service providing agent is unsure. The planning
agent is expecting to be asked

(Money PA $200)

However, it is surprised to be asked
(Money PA $200 credit card)

instead. This suggests that the Hotel agent has a ternary
version of the Money predicate, rather than the planning
agent’s binary predicate. In order to communicate suc-
cessfully with the Hotel agent, the planning agent must
change the arity of its Money predicate accordingly and
replan.

Figure 5: Repairing an Ontology Signature

ORS employs interactions between three kinds of reasoning
process: inference in the form of plan formation; fault diag-
nosis of the failures of plan execution and any dialogue prior
to this failure; and mechanisms for repairing faults in both
the theory and the signature of its ontology. The fault diag-
nosis works by a simple decision tree, in which the notion of
being asked a surprising question (or not) is a key diagnostic
cue. It assumes a context in which there is a large measure of
ontological agreement between interacting agents, but some
critical differences that, uncorrected, will cause the agents’
plans to fail. This context might arise, for instance, where the
agents have tried to adhere to some ontological standard, but
different versions and local customisation have created criti-
cal differences. It can help where the ontological differences
only require localised repairs, but where such repairs must be
done at runtime and without human intervention, for instance,
where very large numbers of software agents are interacting
over the internet.

An agent can use inference to form plans to achieve its
goals, but these are only guaranteed to succeed when its world
model is perfect. Since perfect world models are, in practice,
unattainable, a successful agent must also be able to repair a
faulty world model. This requires the interaction of inference
with fault detection, diagnosis and repair. Fault detection re-
quires inference to provide a failed plan. Fault diagnosis re-
quires a fault to be detected and the failed plan for analysis,
together with further inference about the state of the world
and how it differs from the world model. Fault repair requires
a diagnosis of the fault. Successful inference requires the re-

IJCAI-07

8

paired world model. These reasoning processes thus consti-
tute a virtuous circle, which incrementally improves the suc-
cess of the agent in attaining its goals.

7 Conclusion

“Come together” (John Lennon & Paul McCart-
ney)

In this paper we have been defending the hypothesis that:

By complementing each other, cooperating reason-
ing process can achieve much more than they could
if they only acted individually.

We have presented evidence in favour of this hypothesis accu-
mulated over more than 30 years within my research group11.
In particular, we have seen how processes of inference, at
both object and meta-level, planning, fault diagnosis, learn-
ing, counter-example ﬁnding, representation formation and
ontology repair can interact. It is not just that different pro-
cesses deal with different parts of the task, but that the weak-
nesses in one process are complemented by strengths in an-
other, leading to the more efﬁcient and effective running of
each process and of their combination. Search that would
overwhelm an inference process running on its own, is effec-
tively guided when two inference processes run in tandem.
Faults that would otherwise remain undetected, are made
manifest by the mismatching expectations of two inference
processes. Formal representations required to perform infer-
ence are formed with the aid of inference. New proof methods
needed to guide inference are created by learning from previ-
ously successful examples of inference. Faulty world models
are detected, diagnosed and repaired.

AI has become fragmented over the last 30 years. The de-
velopment of robust, scalable AI systems to emulate the many
facets of intelligence has proven to be much more difﬁcult
than envisaged by the AI pioneers of the 50s and 60s. To
make the task tractable, we divided ourselves into smaller,
specialised communities, organised our own conferences and
journals, and developed a separate series of techniques for
each of the different facets of intelligence. We have made
enormous progress. Our techniques are theoretically well un-
derstood, robust and applicable to important practical prob-
lems. But, taken individually, they each suffer from severe
limitations of range and autonomy.

One lesson from this paper is that perhaps the time has
come to reintegrate Artiﬁcial Intelligence; to discover how
the limitations of one kind of technique can be overcome by
complementing it with another; to realise that together we
have already achieved much more than we had realised when
we were apart.

References
[Benzm¨uller and Sorge, 2001] Christoph Benzm¨uller and
Volker Sorge. OANTS – an open approach at combining
interactive and automated theorem proving. In M. Kerber

11Lots of other groups have accumulated similar evidence, but

surveying that is beyond the scope of the current paper (see §1).

and M. Kohlhase, editors, 8th Symposium on the Integra-
tion of Symbolic Computation and Mechanized Reasoning
(Calculemus-2000), pages 81–97, St. Andrews, UK, 6–7
August 2001. AK Peters, New York, NY, USA.

[Bundy and Welham, 1981] A. Bundy and B. Welham. Us-
ing meta-level inference for selective application of mul-
tiple rewrite rules in algebraic manipulation. Artiﬁcial In-
telligence, 16(2):189–212, 1981. Also available from Ed-
inburgh as DAI Research Paper 121.

[Bundy et al., 1979] A. Bundy, L. Byrd, G. Luger, C. Mel-
lish, R. Milne, and M. Palmer. Solving mechanics prob-
lems using meta-level inference. In B. G. Buchanan, edi-
tor, Proceedings of IJCAI-79, pages 1017–1027. Interna-
tional Joint Conference on Artiﬁcial Intelligence, 1979.
Reprinted in ‘Expert Systems in the microelectronic age’
ed. Michie, D., pp. 50-64, Edinburgh University Press,
1979. Also available from Edinburgh as DAI Research Pa-
per No. 112.

[Bundy et al., 1991] A. Bundy, F. van Harmelen, J. Hesketh,
and A. Smaill. Experiments with proof plans for induc-
tion. Journal of Automated Reasoning, 7:303–324, 1991.
Earlier version available from Edinburgh as DAI Research
Paper No 413.

[Bundy et al., 2005a] A. Bundy, D. Basin, D. Hutter, and
A. Ireland. Rippling: Meta-level Guidance for Mathemat-
ical Reasoning, volume 56 of Cambridge Tracts in The-
oretical Computer Science. Cambridge University Press,
2005.

[Bundy et al., 2005b] A. Bundy, J. Gow, J. Fleuriot, and
L. Dixon. Constructing induction rules for deductive syn-
thesis proofs. In S. Allen, J. Crossley, K.K. Lau, and I Po-
ernomo, editors, Proceedings of the ETAPS-05 Workshop
on Constructive Logic for Automated Software Engineer-
ing (CLASE-05), Edinburgh, pages 4–18. LFCS University
of Edinburgh, 2005. Invited talk.

[Bundy et al., 2006] A. Bundy, F. McNeill, and C. Wal-
ton. On repairing reasoning reversals via representational
reﬁnements.
In Proceedings of the 19th International
FLAIRS Conference, pages 3–12. AAAI Press, 2006. In-
vited talk.

[Bundy, 1985] Alan Bundy. Discovery and reasoning in
mathematics. In A. Joshi, editor, Proceedings of IJCAI-85,
pages 1221–1230. International Joint Conference on Arti-
ﬁcial Intelligence, 1985. Also available from Edinburgh as
DAI Research Paper No. 266.

[Bundy, 1988] A. Bundy. The use of explicit plans to guide
inductive proofs. In R. Lusk and R. Overbeek, editors, 9th
International Conference on Automated Deduction, pages
111–120. Springer-Verlag, 1988. Longer version available
from Edinburgh as DAI Research Paper No. 349.

[Bundy, 1991] Alan Bundy. A science of reasoning. In J.-
L. Lassez and G. Plotkin, editors, Computational Logic:
Essays in Honor of Alan Robinson, pages 178–198. MIT
Press, 1991. Also available from Edinburgh as DAI Re-
search Paper 445.

IJCAI-07

9

[Bundy, 2001] Alan Bundy. The automation of proof by
mathematical induction. In A. Robinson and A. Voronkov,
editors, Handbook of Automated Reasoning, Volume 1. El-
sevier, 2001.

[Castellini and Smaill, 2005] Claudio Castellini and Alan
Smaill.
Proof planning for ﬁrst-order temporal logic.
In Robert Nieuwenhuis, editor, Automated Deduction –
CADE-20, volume 3632 of Lecture Notes in Computer Sci-
ence, pages 235–249. Springer, 2005.

[Colton and Pease, 2005] S. Colton and A. Pease. The TM
system for repairing non-theorems. Electronic Notes in
Theoretical Computer Science, 125(3), 2005. Elsevier.

[Colton et al., 1999] S Colton, A Bundy, and T Walsh. HR:
Automatic concept formation in pure mathematics. In Pro-
ceedings of the 16th International Joint Conference on Ar-
tiﬁcial Intelligence, Stockholm, Sweden, pages 786–791,
1999.

[Colton et al., 2004] S. Colton, A. Meier, V. Sorge, and
R. McCasland. Automatic generation of classiﬁcation the-
orems for ﬁnite algebras”. In Proceedings of the Interna-
tional Joint Conference on Automated Reasoning, 2004.

[Colton, 2002] S Colton. Automated Theory Formation in

Pure Mathematics. Springer-Verlag, 2002.

[Desimone, 1989] R. V. Desimone.

Explanation-Based
Learning of Proof Plans. In Y. Kodratoff and A. Hutchin-
son, editors, Machine and Human Learning. Kogan Page,
1989. Also available as DAI Research Paper 304. Previous
version in proceedings of EWSL-86.

[Duncan et al., 2004] H. Duncan, A. Bundy, J. Levine,
A. Storkey, and M. Pollet. The use of data-mining for the
automatic formation of tactics. In Workshop on Computer-
Supported Mathematical Theory Development. IJCAR-04,
2004.

[Gow, 2004] J. Gow. The Dynamic Creation of Induction
Rules Using Proof Planning. PhD thesis, School of In-
formatics, University of Edinburgh, 2004.

[Ireland and Bundy, 1996] A. Ireland and A. Bundy. Produc-
tive use of failure in inductive proof. Journal of Automated
Reasoning, 16(1–2):79–111, 1996. Also available from
Edinburgh as DAI Research Paper No 716.

[Ireland and Stark, 2001] A. Ireland and J. Stark. Proof plan-
ning for strategy development. Annals of Mathematics and
Artiﬁcial Intelligence, 29(1-4):65–97, February 2001. An
earlier version is available as Research Memo RM/00/3,
Dept. of Computing and Electrical Engineering, Heriot-
Watt University.

[Ireland et al., 2006] A.

Ireland, B.J. Ellis, A. Cook,
R. Chapman, and J Barnes. An integrated approach to
high integrity software veriﬁcation. Journal of Automated
Reasoning: Special Issue on Empirically Successful Auto-
mated Reasoning, 2006. To appear.

[Ireland, 1992] A. Ireland. The Use of Planning Critics in
In A. Voronkov, editor,
Mechanizing Inductive Proofs.
International Conference on Logic Programming and Au-
tomated Reasoning – LPAR 92, St. Petersburg, Lecture

Notes in Artiﬁcial Intelligence No. 624, pages 178–189.
Springer-Verlag, 1992. Also available from Edinburgh as
DAI Research Paper 592.

[Jamnik et al., 2002] M. Jamnik, M. Kerber, and M. Pollet.
Automatic learning in proof planning. In F. van Harme-
len, editor, Proceedings of 15th ECAI, pages 282–286. Eu-
ropean Conference on Artiﬁcial Intelligence, IOS Press,
2002.

[Janiˇci´c and Bundy, 2002] Predrag Janiˇci´c and Alan Bundy.
A general setting for the ﬂexible combining and augment-
ing decision procedures. Journal of Automated Reasoning,
28(3):257–305, 2002.

[Kapur and Subramaniam, 1996] D. Kapur and M Subrama-
niam. Lemma discovery in automating induction. In M. A.
McRobbie and J. K. Slaney, editors, 13th International
Conference on Automated Deduction (CADE-13), pages
538–552. CADE, Springer, 1996.

[Kraan et al., 1996] I. Kraan, D. Basin, and A. Bundy.
Middle-out reasoning for synthesis and induction. Journal
of Automated Reasoning, 16(1–2):113–145, 1996. Also
available from Edinburgh as DAI Research Paper 729.

[Lakatos, 1976] I. Lakatos. Proofs and Refutations: The
Logic of Mathematical Discovery. Cambridge University
Press, 1976.

[McCune, 1990] W. McCune. The Otter user’s guide. Tech-
nical Report ANL/90/9, Argonne National Laboratory,
1990.

[McCune, 1994] W. McCune. A Davis-Putnam program and
its application to ﬁnite ﬁrst-order model search. Techni-
cal Report ANL/MCS-TM-194, Argonne National Labs,
1994.

[McNeill, 2005] Fiona McNeill. Dynamic Ontology Reﬁne-
ment. PhD thesis, School of Informatics, University of
Edinburgh, 2005.

[Meier and Melis, 2005] A. Meier and E. Melis. Failure rea-
soning in multiple-strategy proof planning. Electronic
Notes in Theoretical Computer Science, 125:67–90, 2005.
[Melis et al., 2006] E. Melis, A. Meier, and J. Siekmann.
Proof planning with multiple strategies. Artiﬁcial Intel-
ligence, 2006. To appear.

[Monroy et al., 1994] R. Monroy, A. Bundy, and A. Ireland.
Proof Plans for the Correction of False Conjectures.
In
F. Pfenning, editor, 5th International Conference on Logic
Programming and Automated Reasoning, LPAR’94, Lec-
ture Notes in Artiﬁcial Intelligence, v. 822, pages 54–68,
Kiev, Ukraine, 1994. Springer-Verlag. Also available from
Edinburgh as DAI Research Paper 681.

[Monroy, 2001] Ra´ul Monroy. Concept formation via proof
planning failure. In R. Nieuwenhuis and A. Voronkov, ed-
itors, 8th International Conference on Logic for Program-
ming, Artiﬁcial Intelligence and Reasoning, volume 2250
of Lecture Notes in Artiﬁcial Intelligence, pages 718–731.
Springer-Verlag, 2001.

[Robertson et al., 1991] D. Robertson,

Alan Bundy,
R. Muetzelfeldt, M. Haggith, and M Uschold. Eco-Logic:

IJCAI-07

10

Logic-Based Approaches to Ecological Modelling. MIT
Press, 1991.

[Ron et al., 1996] D. Ron, Y. Singer, and N. Tishby. The
power of amnesia: Learning probabilistic automata with
variable memory length. Machine Learning, 25, 1996.

[Siekmann et al., 2006] J. Siekmann, C. Benzm¨uller, and
Computer supported mathematics with
Journal of Applied Logic, Special Issue on

S. Autexier.
ΩMEGA.
Mathematics Assistance Systems, 2006. To appear.

[Silver, 1985] B. Silver. Meta-level inference: Represent-
ing and Learning Control Information in Artiﬁcial Intel-
ligence. North Holland, 1985. Revised version of the au-
thor’s PhD thesis, Department of Artiﬁcial Intelligence, U.
of Edinburgh, 1984.

[Sorge et al., 2006] V. Sorge, A. Meier, R. McCasland, and
S. Colton. The automatic construction of isotopy invari-
ants. In The proceedings of the International Joint Confer-
ence on Automated Reasoning., 2006.

[Sterling et al., 1989] L. Sterling, Alan Bundy, L. Byrd,
R. O’Keefe, and B. Silver. Solving symbolic equations
with PRESS. J. Symbolic Computation, 7:71–84, 1989.
Also available from Edinburgh as DAI Research Paper
171.

[van der Waerden, 1971] B.L. van der Waerden. How the
Proof of Baudet’s Conjecture Was Found, pages 251–260.
Academic Press, 1971.

IJCAI-07

11


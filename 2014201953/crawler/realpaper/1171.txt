Logistic Regression Models for a Fast CBIR Method

Based on Feature Selection

R. Ksantini1, D. Ziou1, B. Colin2, and F. Dubeau2

University of Sherbrooke

(1) Computer Science Department

{riadh.ksantini, djemel.ziou}@usherbrooke.ca

(2) Mathematic Department

{bernard.colin, francois.dubeau}@usherbrooke.ca

Abstract

Distance measures like the Euclidean distance have
been the most widely used to measure similarities
between feature vectors in the content-based image
retrieval (CBIR) systems. However, in these sim-
ilarity measures no assumption is made about the
probability distributions and the local relevances
of the feature vectors. Therefore, irrelevant fea-
tures might hurt retrieval performance. Probabilis-
tic approaches have proven to be an effective solu-
tion to this CBIR problem. In this paper, we use
a Bayesian logistic regression model, in order to
compute the weights of a pseudo-metric to improve
its discriminatory capacity and then to increase im-
age retrieval accuracy. The pseudo-metric weights
were adjusted by the classical logistic regression
model in [Ksantini et al., 2006]. The Bayesian
logistic regression model was shown to be a sig-
niﬁcantly better tool than the classical logistic re-
gression one to improve the retrieval performance.
The retrieval method is fast and is based on feature
selection. Experimental results are reported on the
Zubud and WANG color image databases proposed
by [Deselaers et al., 2004].

1 Introduction
The rapid expansion of the Internet and the wide use of
digital data in many real world applications in the ﬁeld of
medecine, security, communications, commerce and acad-
emia, increased the need for both efﬁcient image database
creation and retrieval procedures. For this reason, content-
based image retrieval (CBIR) approach was proposed. In this
approach, each image from the database is associated with a
feature vector capturing certain visual features of the image
such as color, texture and shape. Then, a similarity measure
is used to compare these feature vectors and to ﬁnd similar-
ities between images with the assumption that images that
are close to each other in the feature space are also visu-
ally similar. Distance measures like the Euclidean distance
have been the most widely used for feature vector compar-
ison in the CBIR systems. However, these similarity mea-
sures are only based on the distances between feature vec-
tors in the feature space. Therefore, because of the lack of

information about the relative relevances of the featurebase
feature vectors and because of the noise in these vectors, dis-
tance measures can fail and irrelevant features might hurt re-
trieval performance. Probabilistic approaches are a promis-
ing solution to this CBIR problem, that when compared to
the standard CBIR methods based on the distance measures,
can lead to a signiﬁcant gain in retrieval accuracy. In fact,
these approaches are capable of generating probabilistic sim-
ilarity measures and highly customized metrics for comput-
ing image similarity based on the consideration and distinc-
tion of the relative feature vector relevances. As to previ-
ous works based on these probabilistic approaches, [Peng et
al., 2004] used a binary classiﬁcation to classify the database
[Cae-
color image feature vectors as relevant or irrelevant,
nen and Pauwels, 2002] used the classical quadratic logistic
regression model, in order to classify database image feature
[Aksoy et al., 2000] used
vectors as relevant or irrelevant,
weighted L1 and L2 distances, in order to measure the simi-
larity degree between two images and [Aksoy and Haralick,
2001] measure the similarity degree between a query image
and a database image using a likelihood ratio derived from a
Bayesian classiﬁer.

In this paper, we investigate the effectiveness of a Bayesian
logistic regression model based on a variational method, in
order to adjust the weights of a pseudo-metric used in [Ksan-
tini et al., 2006], and then to improve its discriminatory ca-
pacity and to increase image retrieval accuracy. This pseudo-
metric makes use of the compressed and quantized versions
of the Daubechies-8 wavelet decomposed feature vectors, and
its weights were adjusted by the classical logistic regression.
We will show that thanks to the variational method, the used
Bayesian logistic regression model is a signiﬁcantly better
tool than the classical logistic regression model to compute
the pseudo-metric weights and to improve the querying re-
sults. The retrieval method is fast, efﬁcient and based on
feature selection. The evaluation of the retrieval method us-
ing both models, separately, is performed using precision and
scope curves as deﬁned in [Kherﬁ and Ziou, 2006].

In the next section, we brieﬂy deﬁne the pseudo-metric. In
section 3, we brieﬂy describe the pseudo-metric weight com-
putation using the classical logistic regression model, while
showing the limitations of this latter and that the Bayesian
logistic regression model is more appropriate for the pseudo-
metric weight computation. Then, we detail the Bayesian lo-

IJCAI-07

2790

gistic regression model. Moreover, we will describe the data
training performed for both models. The feature selection
based image retrieval method and the feature vectors used to
represent the database images are presented in section 4. Fi-
nally, in section 5, we will perform some experiments to val-
idate the Bayesian logistic regression model and we will use
the precision and scope, in order to show the advantage of the
Bayesian logistic regression model over the classical logistic
regression one, in terms of querying results.

2 The pseudo-metric
Given a query feature vector Q and a featurebase of |DB|
feature vectors Tk (k = 1, ..., |DB|) having 2J
components
each, our aim is to retrieve in the featurebase the most similar
feature vectors to Q. To achieve this, Q and the |DB| feature
vectors are Daubechies-8 wavelets decomposed, compressed
to m coefﬁcients each and quantized. Then, to measure the
similarity degree between Q and a target feature vector Tk
of the featurebase, we use the one-dimensional version of the
pseudo-metric used in [Ksantini et al., 2006] and given by
the following expression
(cid:2) Q, Tk (cid:2)= ˜w0| ˜Q[0]− ˜Tk[0]|−

wbin(i)( ˜Qc

q[i] = ˜T c

(cid:2)

kq[i]),

i: ˜Qc

q[i](cid:2)=0

where (cid:3)
˜Qc

q[i] = ˜T c

(cid:4)
kq[i]

(cid:5)

=

1
0

˜Qc

q[i] = ˜T c

if
otherwise,

kq[i]

(1)

(2)

˜Tk[0] are the scaling factors of Q and Tk,

˜Q[0] and
q[i]
˜T c
kq[i] represent the i-th coefﬁcients of their Daubechies-
and
8 wavelets decomposed, compressed to m coefﬁcients and
quantized versions, ˜w0 and the wbin(i)’s are the weights to
compute, and the bucketing function bin() groups these lat-
ters according to the J resolution levels, such as

˜Qc

bin(i) = (cid:3)log2(i)(cid:4)

with

i = 1, ..., 2J − 1.

(3)

3 The weight computation

In order to improve the discriminatory power of the pseudo-
metric, we compute its weights ˜w0 and {wk}J−1
k=0 using a clas-
sical logistic regression model and a Bayesian logistic regres-
sion model, separately. We deﬁne two classes, the relevance
class denoted by Ω0 and the irrelevance class denoted by Ω1,
in order to classify the feature vector pairs as similar or dis-
similar. The basic principle of using the Bayesian logistic re-
gression model and the classical logistic regression one is to
allow a good linear separation between Ω0 and Ω1, and then
to compute the weights which represent the local relevances
of the pseudo-metric components.

˜X0,i is the
the two feature vectors are intended to be similar.
absolute value of the difference between the scaling factors
of the Daubechies-8 wavelets decomposed, compressed and
quantized versions of the two feature vectors and {Xk,i}J−1
k=0
are the numbers of mismatches between the J resolution level
coefﬁcients of these latter. We suppose that we have n0 pairs
of similar feature vectors and n1 pairs of dissimilar ones.
Thus, the class Ω0 contains n0 explanatory vectors and their
associated binary target variables {X r
i=1 to repre-
sent the pairs of the similar feature vectors, and the class Ω1
contains n1 explanatory vectors and their associated binary
target variables {X ir
j=1 to represent the pairs of
the dissimilar feature vectors. The pseudo-metric weights ˜w0
and {wk}J−1
k=0 and an intercept v are chosen to optimize the
following conditional log-likelihood.

j = 1}n1

i = 0}n0

j , Sir

i , Sr

L( ˜w0, w0, ..., wJ−1, v) =

log(pr

i ) +

log(pir

j ),

(4)

n0(cid:2)

n1(cid:2)

i=1

j=1

i and pir

where pr
ties, respectively, and given by

j are the relevance and irrelevance probabili-

pr
i = F (− ˜w0

˜X r

wkX r

k,i − v),

J−1(cid:2)
0,i −
J−1(cid:2)

k=0

pir
j = F ( ˜w0

˜X ir

0,j +

wkX ir

k,j + v),

k=0

where F (x) = ex
1+ex is the logistic function. For this rea-
son, standard optimization algorithms such as Fisher scoring
and gradient ascent algorithms [Clogg et al., 1991], can be
invoked. However, in several cases, especially because of
the exponential in the likelihood function or because of the
existence of many zero explanatory vectors, the maximum
likelihood can fail and estimates of the parameters of interest
(weights and intercept) may not be optimal or may not ex-
ist or may be on the boundary of the parameter space. Also,
as there is complete or quasicomplete separation between Ω0
and Ω1, the function L is made arbitrarily large and standard
optimization algorithms diverge [Krishnapuram et al., 2005].
Moreover, as Ω0 and Ω1 are large and high-dimensional,
these standard optimization algorithms have high computa-
tional complexity and take long time to converge. The ﬁrst
two problems can be solved by smoothing the parameter of
interest estimates, assuming a certain prior distribution for
the parameters, thereby reducing the parameter space, and
the third problem can be solved by using variational trans-
formations which simplify the computation of the parameter
of interest estimates [Jaakkola and Jordan, 2000]. This mo-
tivates the adoption of a Bayesian logistic regression model
based on variational methods.

3.1 The classical logistic regression model
In this model, each feature vector pair is represented by an ex-
planatory vector and a binary target variable. Speciﬁcally, for
the i-th feature vector pair, we associate an explanatory vec-
tor Xi = ( ˜X0,i, X0,i, ..., XJ−1,i, 1) ∈ RJ × {1} and a binary
target Si which is either 0 or 1, depending on whether or not

3.2 The Bayesian logistic regression model

In the Bayesian logistic regression framework, there are three
main components which are a chosen prior distribution over
the parameters of interest, the likelihood function and the pos-
terior distribution. These three components are formally com-
bined by Bayes’ rule. The posterior distribution contains all

IJCAI-07

2791

the available knowledge about the parameters of interest in
the model. Among many priors having different distributional
forms, gaussian prior has the advantage of having low com-
putational intensity and of smoothing the parameter estimates
toward a ﬁxed mean and away from unreasonable extremes.
However, when the likelihood function is not conjugate of
the gaussian prior, the posterior distribution has no tractable
form and its mean computation involves high-dimensional
integration which has high computational cost. According
to [Jaakkola and Jordan, 2000], it’s possible to use accurate
variational transformations in order to approximate the like-
lihood function with a simpler tractable exponential form. In
this case, thanks to the conjugacy, with a gaussian prior dis-
tribution over the parameters of interest combined with the
likelihood approximation, we obtain a closed gaussian form
approximation to the posterior distribution. However, as the
number of observations is large, the number of variational
parameters updated to optimize the posterior distribution ap-
proximation is also large, thereby the computational cost is
high. In the Bayesian logistic regression model that we pro-
pose, we use variational transformations and the Jensen’s in-
equality in order to approximate the likelihood function with
tractable exponential form. The explanatory vectors are not
observed but instead are distributed according to two speciﬁc
distributions. The posterior distribution is also approximated
with a gaussian which depends only on two variational pa-
rameters. The computation of the posterior distribution ap-
proximation mean is fast and has low computational com-
plexity. In this model, we denote the random vectors whose
i }n0
realizations represent the explanatory vectors {X r
i=1 of the
j }n1
relevance class Ω0 and the explanatory vectors {X ir
j=1 of
the irrelevance class Ω1, by X0 = ( ˜
X0,0, X0,0, ..., XJ−1,0, 1)
and X1 = ( ˜
X0,1, X0,1, ..., XJ−1,1, 1), respectively. We sup-
pose that X0 ∼ q0(X0) and X1 ∼ q1(X1), where q0 and q1
are two chosen distributions. For X0 we associate a binary
random variable S0 whose realizations are the target vari-
ables {Sr
i=1, and for X1 we associate a binary ran-
dom variable S1 whose realizations are the target variables
{Sir
j=1. We set S0 equal to 0 for similarity and we set
S1 equal to 1 for dissimilarity. Parameters of interest (weights
and intercept) are considered as random variables and are de-
noted by the random vector W = ( ˜w0, w0, ..., wJ−1, v). We
assume that W ∼ π(W), where π is a gaussian prior with
prior mean μ and covariance matrix Σ. Using Bayes’ rule,
the posterior distribution over W is given by
P (W|S0 = 0, S1 = 1) =
ˆ P

i = 0}n0

j = 1}n1

Q

˜

x0∈Ω0,x1∈Ω1

1
i=0

P (Si = i|Xi = xi, W)qi(Xi = xi)
P (S0

= 0, S1

= 1)

π(W)

,

txi) for each
where P (Si = i|Xi = xi, W) = F ((2i − 1)W
i ∈ {0, 1}. Using a variational approximation [Jaakkola and
Jordan, 2000] and the Jensen’s inequality, the posterior distri-
bution is approximated as follows
P (W|S0 = 0, S1 = 1)

(cid:6)
W|S0 = 0, S1 = 1,

(cid:7)

(cid:8)1

(cid:7)

(cid:8)1

(cid:9)

i

i=0,

qi

i=0

π(W)

,

P (S0 = 0, S1 = 1)

P

≥

(cid:6)
∝ P

W|S0 = 0, S1 = 1,

(cid:8)1

i=0

(cid:9)
π(W)

i

i=0,

qi

(cid:8)1
(cid:8)1

(cid:7)
(cid:7)
(cid:14)

(cid:7)
(cid:9)
(cid:13)

=

(cid:6)

(cid:8)1

(cid:7)

i

i=0,

qi

i=0

Eqi

[Hi]−i

2

P1

i=0

−

ϕ(i)

Eqi

[H2

i ]−2

i

(cid:9)(cid:14)(cid:12)

,

P

where

(cid:6)
W|S0 = 0, S1 = 1,
(cid:13)
(cid:10) 1(cid:11)

(cid:10)

P1

(cid:12)
F (i)

i=0

e

i=0

(cid:8)1

where Eq0 and Eq1 are the expectations with respect to the
(cid:7)
distributions q0 and q1, respectively, ϕ(i) = tanh( i
2 )
i

i=0 are the variational parameters. Therefore, the ap-
proximation of the posterior distribution is considered as an
adjustable lower bound and as a proper Gaussian distribu-
tion with a posterior mean μpost and covariance matrix Σpost
which are estimated by the following Bayesian update equa-
tions

and

4i

1(cid:2)

(cid:13)

i=0

(Σpost)−1 = (Σ)−1 + 2

(cid:10)

μpost = Σpost

(Σ)−1

μ +

ϕ(i)Eqi
1(cid:2)
(cid:13)

i=0

(cid:14)
[xi(xi)t]

,

(i −

1
2

)Eqi

[xi]

(cid:14)(cid:12)

(5)

. (6)

i

(cid:7)

(cid:8)1

The weight and intercept computation algorithm is in two
phases. The ﬁrst phase is the initialization of q0, q1 and
the gaussian prior π(W), and the second phase is iterative
and allows the computation of Σpost and μpost through the
Bayesian update equations (5) and (6), respectively, while
using an EM type algorithm [Jaakkola and Jordan, 2000], in
i=0 at each iter-
order to ﬁnd the variational parameters
ation to have an optimal approximation to the posterior dis-
tribution. In the initialization phase, q0 and q1 are chosen to
model Ω0 and Ω1, respectively, and because of the absence of
prior knowledge about the weights and the intercept, π(W) is
chosen univariate with zero mean and large variances [Con-
gdon, 2001]. The values of μpost components are the desired
estimates of the pseudo-metric weights ˜w0 and {wk}J−1
k=0 and
the intercept v. Once the parameters of the posterior distrib-
ution approximation are computed, its magnitude is given by
i=0 F (i). This latter becomes very close to 1 as
the term
Ω0 and Ω1 are linearly separated or quasi separated and tends
towards 0 as Ω0 and Ω1 become more and more overlapped.
Analogically, in the classical logistic regression model, the
term e2L
i=0 F (i)
[Caenen and Pauwels, 2002]. These two terms will be used
to perform feature selection in the retrieval method.

has almost the same characteristics as

(cid:15)1

(cid:15)1

3.3 Training

Let us consider a color image database which consists of sev-
eral color image sets such that each set contains color images
which are perceptually close to each other in terms of object
shapes and colors.
In order to compute the pseudo-metric
weights and the intercept by the classical logistic regression
model, we have to create the relevance class Ω0 and the ir-
relevance class Ω1. To create Ω0, we draw all possible pairs
of feature vectors representing color images belonging to the

IJCAI-07

2792

same database color image sets, and for each pair we com-
pute an explanatory vector and we associate to this latter a
binary target variable equal to 0. Similarly, to create Ω1, we
draw all possible pairs of feature vectors representing color
images belonging to different database color image sets, and
for each pair we compute an explanatory vector and we asso-
ciate to this latter a binary target variable equal to 1. For the
Bayesian logistic regression model, we create the Ω0 and Ω1
with the same way, but instead of associating a binary target
variable value to each explanatory vector of Ω0 and Ω1, we
associate a binary target variable S0 equal to 0 to all Ω0 ex-
planatory vectors and we associate a binary target variable S1
equal to 1 to all Ω1 explanatory vectors.

4 Color image retrieval method
The querying method is in two phases. The ﬁrst phase is a
preprocessing phase done once for the entire database con-
taining |DB| color images. The second phase is the querying
phase.

4.1 Color image database preprocessing
We detail the preprocessing phase done once for all the data-
base color images before the querying in a general case by
the following steps.

1. Choose N feature vectors for comparison.
2. Compute the N feature vectors Tli (l ∈ {1, ..., N })
for each i-th color image of the database, where i ∈
{1, ..., |DB|}.

3. The feature vectors representing the database color
images are Daubechies-8 wavelets decomposed, com-
pressed to m coefﬁcients each and quantized.

(cid:6)
4. Organize the decomposed, compressed and quantized
l =
which are used to optimize the pseud-metric

feature vectors into search arrays Θl
1, ..., N
computation process [Ksantini et al., 2006].

+ and Θl
−

(cid:9)

5. Adjustment of the metric weights ˜wl

k=0 for
each featurebase Tli (i = 1, ..., |DB|) representing the
database color images, where l ∈ {1, ..., N }.

0 and {wl

k}J−1

4.2 The querying algorithm
We detail the querying algorithm in a general case by the fol-
lowing steps.

(cid:6)

(cid:9)

1. Given a query color image, we denote the feature vectors

representing the query image by Ql

l = 1, ..., N

.

3. The similarity degrees between Ql

2. The feature vectors representing the query image are
Daubechies-8 wavelets decomposed, compressed to m
(cid:9)
(cid:6)
coefﬁcients each and quantized.
(cid:6)
l = 1, ..., N
(cid:9)
l =
and the database color image feature vectors Tli
(cid:6)
(i = 1, ..., |DB|) are represented by the arrays
1, ..., N
such that Scorel[i] =(cid:2) Ql, Tli (cid:2)
l = 1, ..., N
(cid:9)
Scorel
for each i ∈ {1, ..., |DB|}. These arrays are returned by
the procedure Retrieval(Ql, m, Θl
,
respectively. The procedure Retrieval is used to optimize
the querying process [Ksantini et al., 2006].

(cid:6)
l = 1, ..., N

+, Θl

(cid:9)

−)

(cid:16)N

4. The similarity degrees between the query color image
and the database color images are represented by a re-
sulted array T otalScore, such as, T otalScore[i] =
l=1 γlScorel[i] for each i ∈ {1, ..., |DB|}, where
{γl}N
l=1 are weightfactors used to down-weight the fea-
ture which has low discriminatory power. γl = e2Ll
when the weights are computed by the classical logis-
tic regression model, and γl =
i) when the
weights are computed by the Bayesian logistic regres-
sion model.

i=0 F (l

(cid:15)1

5. Organize the database color images in order of increas-
ing resulted similarity degrees of the array T otalScore.
The most negative resulted similarity degrees corre-
spond to the closest target images to the query image.
Finally, return to the user the closest target color images
to the query color image and whose number is denoted
by RI and chosen by the user.

4.3 Used feature vectors
In order to describe the luminance, colors and the edges of a
color image, we use luminance histogram and weighted his-
tograms. The image texture description is performed by kur-
tosis and skewness histograms. Given an M × N pixel LAB
color image, its luminance histogram hL contains the number
of pixels of the luminance L, and can be written as follows

hL(c) =

δ(IL(i, j) − c),

(7)

M −1(cid:2)

N −1(cid:2)

i=0

j=0

for each c ∈ {0, ..., 255}, where IL is the luminance image
and δ is the Kronecker symbol at 0. The weighted histograms
are the color histogram constructed after edge region elimina-
tion and the multispectral gradient module mean histogram.
The former is given by

M −1(cid:2)

N −1(cid:2)

hh
k(c) =

δ(Ik(i, j) − c)χ[0,η]

i=0

j=0

and the latter is given by

k(c) = he
¯
he

k(c)
Np,k(c) ,

(cid:3)
(cid:4)
λmax(i, j)

,

(8)

(9)

where Np,k(c) is the number of the edge region pixels and is
(cid:3)
(cid:4)
deﬁned as
λmax(i, j)

δ(Ik(i, j) − c)χ]η,+∞[

Np,k(c) =

M −1(cid:2)

N −1(cid:2)

,

i=0

j=0

and
he
k(c) =
N −1(cid:2)
M −1(cid:2)

i=0

j=0

δ(Ik(i, j) − c)λmax(i, j) χ]η,+∞[

(10)

(cid:3)
(cid:4)
λmax(i, j)

,

(11)
for each c ∈ {0, ..., 255} and k = a, b, where λmax represents
the multispectral gradient module [Ksantini et al., 2006], η is

IJCAI-07

2793

a threshold deﬁned by the mean of the multispectral gradient
modules computed over all image pixels, Ia and Ib are the
images of the chrominances a red/green and b yellow/blue,
respectively, and χ is the characteristic function. The multi-
spectral gradient module mean histogram provides informa-
tion about the overall contrast in the chrominance and the
edge region elimination allows the avoidance of overlappings
or noises between the color histogram populations caused by
the edge pixels. The LAB color image kurtosis and skewness
histograms are given by

M −1(cid:2)

N −1(cid:2)

i=0

j=0

M −1(cid:2)

N −1(cid:2)

i=0

j=0

and

hκ
k(c) =

hs
k(c) =

δ(I κ

k (i, j) − c),

(12)

δ(I s

k(i, j) − c),

(13)

a and I κ

respectively, for each c ∈ {0, ..., 255} and k = L, a, b, where
I κ
L, I κ
b are the kurtosis images of the luminance L
and the chrominances a and b, respectively, and I s
a and
I s
b are the skewness images of these latter. They are obtained
by local computations of the kurtosis and skewness values at
the luminance and chrominance image pixels. Then, a linear
interpolation is used to represent the kurtosis and skewness
values between 0 and 255. Since each used feature vector is a
histogram having 256 components, we set J equal to 8 in the
following section.

L, I s

5 Experimental results
In this section, we will discuss the choices of the distributions
q0 and q1, in order to validate the Bayesian logistic regression
model in the image retrieval context. Finally, we will use
the precision and scope as deﬁned in [Kherﬁ and Ziou,
2006], to evaluate the querying method using both models
separately. The choices of the distributions q0 and q1 and the
querying evaluation will be conducted on the WANG and
Zubud color image databases proposed by [Deselaers et al.,
2004]. The WANG database contains |DB| = 1000 color
images which were selected manually to form 10 sets (e.g.
Africa, beach, ruins, food) of 100 images each. The Zurich
Building Image Database (ZuBuD) contains a training part
of |DB| = 1005 color images and query part of 115 color
images. The training part consists of 201 building image sets,
where each set contains 5 color images of the same building
taken from different positions. Before the feature vector
extractions, we represent the WANG and Zubud database
color images in the perceptually uniform LAB color space.
Since from each color image of the Zubud and WANG
databases we extract N = 11 histograms which are given
by (7), (8), (9), (12) and (13) respectively, each database is
represented by eleven featurebases. The choices of q0 and q1
will be separately performed for each featurebase. For each
˜
X0,0 and (X0,0, ..., XJ−1,0) are
featurebase, we assume that
˜
independent. We make the same assumption for
X0,1 and
(X0,1, ..., XJ−1,1). Moreover, we suppose that the random
vector (X0,0, ..., XJ−1,0) random variables whose realiza-
tions are positive integers, are independent and each one of

them follows a truncated poisson distribution at its greatest
realization, to have a best ﬁt. Analogically, we make the
same choice for (X0,1, ..., XJ−1,1). Also, we assume that the
˜
random variable
X0,0 whose realizations are positive reals,
follows a gaussian mixture distribution, which is the same
˜
choice for
X0,1. Generally, to carry out an evaluation in the
image retrieval ﬁeld, two principal issues are required: the
acquisition of ground truth and the deﬁnition of performance
criteria. For ground truth, we use human observations. In
fact, three external persons participate in the below evalu-
ation. Concerning performance criteria, we represent the
evaluation results by the precision-scope curve P r = f (RI),
where the scope RI is the number of images returned to the
user. In each querying performed in the evaluation experi-
ment, each human subject is asked to give a goodness score to
each retrieved image. The goodness score is 2 if the retrieved
image is almost similar to the query, 1 if the retrieved image
is fairly similar to the query and 0 if there is no similarity
between the retrieved image and the query. The precision
is computed as follows: P r = the sum of goodness scores
for retrieved images/RI. Therefore, the curve P r = f (RI)
gives the precision for different values of RI which lie
between 1 and 20 when we perform the querying evaluation
on the WANG database, and lie between 1 and 5 when we
perform the querying evaluation on the ZuBuD database.
When the human subjects perform different queryings in the
evaluation experiment, we compute an average precision for
each value of RI, and then we construct the precision-scope
curve.
In our evaluation experiment, each color image of
the WANG and Zubud databases is represented by N = 11
histograms which are hL, hh
b , hs
L,
hs
a and hs
b. In order to evaluate the querying in the WANG
database, each human subject is asked to formulate a query
from the database and to execute a querying, using weights
computed by the classical logistic regression model, and
to give a goodness score to each retrieved image, then to
reformulate a query from the database and to execute the
querying, using weights computed by the Bayesian logistic
regression model, and to give a goodness score to each
retrieved image. Each human subject performs the querying
ﬁfty times by choosing a new query from the database
each time. We repeat this experience for different orders of
compression m ∈ {30, 20, 10}. To evaluate the querying in
the ZuBuD database, each human subject is asked to follow
the preceding steps, while formulating the queries from the
database query part. For the WANG and Zubud databases,
the resulted precision-scope curves are given in Figure 1
for compression orders m ∈ {30, 20, 10}. The Figure 2
illustrates two retrieval examples in the Zubud database
comparing the performances of the regression models for
m = 30. In each example the query is located at the top-left
of the dialog box.

¯
he
b, hκ

a, hh
b ,

L, hκ

a, hκ

¯
he
a,

IJCAI-07

2794

pseudo-metric and to allow feature selection. Evaluations of
the querying method showed that the Bayesian logistic re-
gression model is a better tool than the classical logistic re-
gression one to compute the pseudo-metric weights and to
improve the querying results.

References
[Aksoy and Haralick, 2001] S. Aksoy and R. M. Haralick.
Feature Normalization and Likelihood-based Similarity
Measures for Image Retrieval. Pattern Recognition Let-
ters, 22(5):563-582, 2001.

[Aksoy et al., 2000] S. Aksoy, R. M. Haralick, F. A. Cheikh,
and M. Gabbouj. A Weighted Distance Approach to
Relevance Feedback. In 15th International Conference
on Pattern Recognition, page 4812, Barcelona, Spain,
2000.

[Caenen and Pauwels, 2002] G. Caenen and E. J. Pauwels.
Logistic Regression Models for Relevance Feedback
in Content-Based Image Retrieval. In Storage and Re-
trieval for Media Databases 2002, Proceedings of SPIE,
pages 49-58, San Jose, California, USA, 2002.

[Clogg et al., 1991] C. C. Clogg, D. B. Rubin, N. Schenker,
B. Schultz, and L. Widman. Multiple Imputation of
Industry and Occupation Codes in Census Public-Use
Samples Using Bayesian Logistic Regression. Journal
of the American Statistical Association, 86:68-78, 1991.
[Congdon, 2001] P. Congdon. Bayesian Statistical Mod-

elling. John Wiley, Chichester, UK, 2001.

[Deselaers et al., 2004] T. Deselaers, D. Keysers, and H.
Ney. Classiﬁcation Error Rate for Quantitative Evalua-
tion of Content-based Image Retrieval Systems. In 17th
International Conference on Pattern Recognition, pages
505-508, Cambridge, UK, 2004.

[Jaakkola and Jordan, 2000] T. S. Jaakkola and M. I. Jordan.
Bayesian parameter estimation via variational methods.
Statistics and Computing, 10(1):25-37, 2000.

[Kherﬁ and Ziou, 2006] M. L. Kherﬁ and D. Ziou. Rele-
vance feedback for CBIR: a new approach based on
probabilistic feature weighting with positive and nega-
tive examples. IEEE Transactions on Image Processing,
15(4):1017-1030, 2006.

[Krishnapuram et al., 2005] B. Krishnapuram, L. Carin, M.
A. T. Figueiredo, and A. J. Hartemink. Sparse Multino-
mial Logistic Regression: Fast Algorithms and General-
ization Bounds. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 27(6):957-968, 2005.

[Ksantini et al., 2006] R. Ksantini, D. Ziou, and F. Dubeau.
Image Retrieval Based on Region Separation and Mul-
tiresolution Analysis. International Journal of Wavelets,
Multiresolution and Information Processing, 4(1):147-
175, 2006.

[Peng et al., 2004] J. Peng, B. Bhanu, and S. Qing. Learn-
ing Feature Relevance and Similarity Metrics in Im-
age Databases. In Proceedings of the IEEE Workshop
on Content-Based Access of Image and Video Libraries,
pages 14-18, Santa Barbara, California, USA, 2004.

(a)

(b)

Figure 1: Evaluation ((a) ZuBud database and (b) WANG
database): precision-scope curves for retrieval using weights
computed by the classical logistic regression model and
weights computed by the Bayesian logistic regression model.

(a)

(b)

Figure 2: Comparison (ZuBud database): a) ﬁrst 8 color im-
ages retrieved using weights computed by the classical logis-
tic regression model, b) ﬁrst 8 color images retrieved using
weights computed by the Bayesian logistic regression model.

6 Conclusion

We presented a simple, fast and effective color image query-
ing method based on feature selection. In order to measure
the similarity degree between two color images both quickly
and effectively, we used a weighted pseudo-metric which
makes use of the one-dimensional Daubechies decomposition
and compression of the extracted feature vectors. A Bayesian
logistic regression model and a classical logistic regression
one were used to improve the discriminatory capacity of the

IJCAI-07

2795


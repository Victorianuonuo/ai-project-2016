Stereotype Extraction with Default Clustering

Julien Velcin and Jean-Gabriel Ganascia

LIP6, Universit´e Paris VI
8 rue du Capitaine Scott
75015 PARIS, FRANCE

Abstract

The concept of stereotype seems to be really
adapted when wishing to extract meaningful de-
scriptions from data, especially when there is a high
rate of missing values. This paper proposes a log-
ical framework called default clustering based on
default reasoning and local search techniques. The
ﬁrst experiment deals with the rediscovering of ini-
tial descriptions from artiﬁcial data sets, the second
one extracts stereotypes of politicians in a real case
generated from newspaper articles. It is shown that
default clustering is more adapted in this context
than the three classical clusterers considered.

Introduction
Conceptual clustering [Michalski, 1980] is a fundamental
machine learning task that is applied in various areas such
as image analysis, analytical chemistry, biology, sociology.
It takes a set of object descriptions as input and creates a
classiﬁcation scheme. The conceptual descriptions of clus-
ters are of particular interest to reason about the categories
themselves, to compare different data sets and to predict new
observations. This work focuses on the extraction of such
conceptual descriptions, especially in the speciﬁc context of
missing data.

Automatic inductive techniques have to deal with missing
information, due to voluntary omissions, human error, bro-
ken equipment [Newgard and Lewis, 2002]. In the context of
sparse data, i.e. with a huge amount of missing values, the
concept of stereotype seems more appropriate than the usual
one of prototype to describe data clusters. Therefore, our goal
is to extract stereotype sets that represent the data sets as well
as possible. By analogy to default logic [Reiter, 1980], which
is a speciﬁc logic for default deduction, we make use of de-
fault subsumption, which is a speciﬁc logic for default induc-
tion, to build such stereotypes.

Section 1 presents a new approach to conceptual clustering
when missing information exists. Section 2 proposes a gen-
eral framework in the attribute-value formalism. The new no-
tion of default subsumption is introduced, before seeing how
the concept of stereotype makes it possible to name clusters.
A stereotype set extraction algorithm based on local search
techniques is then presented. Section 3 concerns experiments,

ﬁrst on artiﬁcial data sets and secondly with a real data case
generated from newspaper articles.

1 Conceptual clustering with sparse data
1.1 Dealing with missing values
This paper proposes a clustering method that deals with high
rates of missing values. But contrary to algorithms such as k-
modes (categorical version of k-means) or EM, that can eas-
ily lead to local optima, we have chosen to achieve the clus-
tering using a combinatorial optimization approach, like in
[Figueroa et al., 2003] or [Sarkar and Leong, 2001]. Note that
our goal is not only to cluster examples but also and mainly
to describe the cluster in a way that is simple and easy to
understand. The problem can thus be stated as ﬁnding read-
able, understandable, consistent and rich descriptions from
the data.

1.2 Overview of default logic
During the eighties, there were many attempts to model de-
ductive reasoning when missing information exists. A lot of
formalisms were developed to encompass the inherent difﬁ-
culties of such models, especially their non-monotony: close-
world assumption, circumscription, default logic, etc. Since
our goal is to deal with missing values, it seems natural to
take advantage of this work. The default logic formalism, in-
troduced by R. Reiter in 1980, was chosen because it seemed
to correspond well to our problem.

This logic for default reasoning is based on the notion of
default rule, through which it is possible to infer new formu-
las when the hypotheses are not inconsistent with the current
context. More generally, a default rule always has the fol-
lowing form: A : B1; B2 : : : Bn=C where A is called the
prerequisite, Bi the justiﬁcations and C the conclusion. This
default rule can be interpreted as follows: if A is known to be
true and if it is consistent to assume B1; B2 : : : Bn then con-
clude C. For instance, let us consider the default rule below
related to the experiments of the last section:

politician(X) ^ introducedAbroad(X) : :diplomat(X)

traitor(X)

This rule translates an usual way of reasoning for many peo-
ple living in France at the end of the 19th century. It states
that the conclusion traitor(X) can be derived if X is a
politician who is known to be introduced abroad and that we
cannot prove that he is a diplomat.

The key idea here is to use similar observations and their
descriptions to infer new information instead of default rules,
but the underlying mechanism is the same. The following
subsection explains the transition from default logic to default
induction.

1.3 Default clustering
E. Rosch saw the categorization itself as one of the most im-
portant issues in cognitive science [Rosch, 1975]. She in-
troduced the concept of prototype as the ideal member of
a category. Whereas categorization makes similar observa-
tions ﬁt together and dissimilar observations be well sepa-
rated, clustering is the induction process in data mining that
actually build such categories. More speciﬁcally, conceptual
clustering is a machine learning task deﬁned by R. Michalski
[Michalski, 1980] which does not require a teacher and uses
an evaluation function to discover classes that have appropri-
ate conceptual descriptions. Conceptual clustering was prin-
cipally studied in a probabilistic context (see, for instance, D.
Fisher’s Cobweb algorithm [Fisher, 1987]) and rarely on re-
ally sparse data sets. For instance, the experiments done by
P.H. Gennari do not exceed 30% of missing values [Gennari,
1990].

As seen above, default logic is a logic for deduction de-
pending on background knowledge. This paper proposes a
new technique called default clustering which uses a similar
principle but for induction when missing information exists.
The main assumption is the following: if an observation is
grouped with other similar observations, you can use these
observations to complete unknown information in the original
fact if it remains consistent with the current context. Whereas
default logic needs implicit knowledge expressed by default
rules, default clustering only uses information available in the
data set. The next section presents this new framework. It
shows how to extract stereotype sets from very sparse data:
ﬁrst it extends classical subsumption (see section 2.1), next
it discusses stereotype choice (see section 2.2), and ﬁnally it
proposes a local search strategy to ﬁnd the best solution (see
section 2.4).

2 Logical framework
This section presents the logical framework of default clus-
tering in the attribute-value formalism (an adaptation to con-
ceptual graphs can be found in [Ganascia and Velcin, 2004]).
The description space is noted D, the descriptor space (i.e.
the values the attributes can take) V and the example set E.
The function (cid:14) maps each example e 2 E to its description
(cid:14)(e) 2 D.

2.1 Default subsumption
Contrary to default logic, the problem here is not to deduce,
but to induce knowledge from data sets in which most of the
information is unknown. Therefore, we put forward the no-
tion of default subsumption, which is the equivalent for sub-
sumption of the default rule for deduction. Saying that a de-
scription d 2 D subsumes d0 2 D by default means that there
exists an implicit description d00 such that d0 completed with
d00, i.e. d0 ^ d00, is more speciﬁc than d in the classical sense,

which signiﬁes that d0 ^ d00 entails d. The exact deﬁnition
follows:

Deﬁnition 1 d subsumes d0 by default (noted d (cid:20)D d0)
iff 9 dc such that dc 6=? and d (cid:20) dc and d0 (cid:20) dc where
t (cid:20) t0 stands for t subsumes t0 in the classical sense. dc is a
minorant of d and d0 in the subsumption lattice.

To illustrate our deﬁnition, here are some descriptions
based on binary attributes that can be compared with respect
to the default subsumption:

d1 = f(T raitor = yes); (Internationalist = yes)g
d2 = f(T raitor = yes); (Connection with jews = yes)g
d3 = f(P atriot = yes)g
d1 (cid:20)D d2 and d2 (cid:20)D d1 because 9 dc such that d1 (cid:20) dc
dc = f(T raitor = yes); (Internationalist =

and d2 (cid:20) dc:
yes); (Connection with jews = yes)g.

However, considering that a patriot cannot be an interna-
tionalist and vice-versa, i.e. :((Patriot=yes) ^ (Internation-
alist=yes)), which was an implicit statement for many people
living in France at the end of the 19th century, d1 does not
subsume d3 by default, i.e. :(d1 (cid:20)D d3).

Property 1 The notion of default subsumption is more gen-
eral than classical subsumption since, if d subsumes d0, i.e.
d (cid:20) d0, then d subsumes d0 by default, i.e. d (cid:20)D d0. The
converse is not true.

Property 2 The default subsumption relationship is sym-

metrical, i.e. 8d 8d0 if d (cid:20)D d0 then d0 (cid:20)D d.

Note that the notion of default subsumption may appear
strange for people accustomed to classical subsumption be-
cause of the symmetrical relationship. As a consequence, it
does not deﬁne an ordering relationship on the description
space D. The notation (cid:20)D may be confusing with respect
to this symmetry, but it is relative to the underlying idea of
generality.

2.2 Concept of stereotype
In the literature of categorization, Rosch introduced the con-
cept of prototype [Rosch, 1975; 1978] inspired by the fam-
ily resemblance notion of Wittgenstein [Wittgenstein, 1953]
(see [Shawver, 2004] for an electronic version and [Narboux,
2001] for an analysis focused on family resemblance). Even
if our approach and the original idea behind the concept of
prototype have several features in common, we prefer to refer
to the older concept of stereotype that was introduced by the
publicist W. Lippman [Lippman, 1922]. For him, stereotypes
are perceptive schemas (a structured association of character-
istic features) shared by a group about other person or object
categories. These simplifying and generalizing images about
reality affect human behavior and are very subjective. Below
are three main reasons to make such a choice.

First of all, the concept of prototype is often misused in
data mining techniques. It is reduced to either an average ob-
servation of the examples or an artiﬁcial description built on
the most frequent shared features. Nevertheless, both of them
are far from the underlying idea in family resemblance. Es-
pecially in the context of sparse data, it seems more correct to
speak about a combination of features found in different ex-
ample descriptions than about average or mode selection. The
second argument is that the notion of stereotype is often de-
ﬁned as an imaginary picture that distorts the reality. Our goal

is precisely to generate such pictures even if they are caricat-
ural of the observations. Finally, these speciﬁc descriptions
are better adapted for fast classiﬁcation (we can even say dis-
crimination) and prediction than prototypes. This last feature
is closely linked to Lippman’s deﬁnition.

In order to avoid ambiguities, we restrict the notion of
stereotype to a speciﬁc description d 2 D associated to (we
can say “covering”) a set of descriptions D (cid:26) D. How-
ever, the following subsection does not deal just with stereo-
types but with stereotype sets to cover a whole description set.
The objective is therefore to automatically construct stereo-
type sets, whereas most of the studies focus on already ﬁxed
stereotype usage [Rich, 1979; Amossy and Herschberg Pier-
rot, 1997]. Keeping this in mind, the space of all the possible
stereotype sets is browsed in order to discover the best one,
i.e.
the set that best covers the examples of E with respect
to some similarity measure. But just before addressing the
search itself, we should consider both the relation of relative
cover and the similarity measure used to build the categoriza-
tion from stereotype sets.

2.3 Stereotype sets and relative cover
Given an example e characterized by its description d =
(cid:14)(e) 2 D, consider the following statement: the stereotype
s 2 D can cover e if and only if s subsumes d by default. It
means that in the context of missing data each piece of infor-
mation is so crucial that even a single contradiction prevents
the stereotype from being a correct generalization. Further-
more, since there is no contradiction between this example
and its related stereotype, the stereotype may be used to com-
plete the example description.

In order to perform the clustering, a very general similarity
measure Msim has been deﬁned, which counts the number
of common descriptors of V belonging to two descriptions,
ignores the unknown values and takes into account the default
subsumption relationship:
Msim: D (cid:2)D ! N+

(di; dj) 7! Msim(di; dj) = jfv 2 d=d = di ^ dj gj

if di (cid:20)D dj and

Msim(di; dj) = 0 if :(di (cid:20)D dj),

where di ^ dj is the least minorant of di and dj in the sub-

sumption lattice.

Let us now consider a set S = fs;; s1; s2 : : : sng (cid:26) D of
stereotypes. s; is the absurd-stereotype linked to the set E;.
Then, a categorization of E can be calculated using S with an
affectation function that we called relative cover:

Deﬁnition 2 The relative cover of an example e 2 E, with
respect to a set of stereotypes S = fs;; s1; s2 : : : sng, noted
CS(e), is the stereotype si if and only if:

(cid:15) si 2 S,
(cid:15) MS((cid:14)(e); si) > 0,
(cid:15) 8k 2 [1; n]; k 6= i; Msim((cid:14)(e); si) > Msim((cid:14)(e); sk).
It means that an example e 2 E is associated to the most
similar and “covering-able” stereotype relative to the set S.
If there are two competitive stereotypes with an equal higher
score or if there is no covering stereotype, then the example is
associated to the absurd-stereotype s;. In this case, no com-
pletion can be calculated for e.

2.4 Stereotype extraction
In this paper, default reasoning is formalized using the no-
tions of both default subsumption and stereotype set. Up to
now, these stereotype sets were supposed to be given. This
section shows how the classiﬁcation can be organized into
such sets in a non-supervised learning task. It can be summa-
rized as follows. Given:
1. An example set E.
2. A description space D.
3. A description function (cid:14): E (cid:0)! D which associates a
description (cid:14)(e) 2 D to each example belonging to the train-
ing set E.
The function of a non-supervised learning algorithm is to or-
ganize the initial set of individuals E into a structure (for
instance a hierarchy, a lattice or a pyramid). In the present
case, the structure is limited to partitions of the training set,
which corresponds to searching for stereotype sets as dis-
cussed above. These partitions may be generated by (n + 1)
stereotypes S = fs;; s1; s2 : : : sng: it is sufﬁcient to asso-
ciate to each si the set Ei of examples e belonging to E and
covered by si relative to S. The examples that cannot be cov-
ered by any stereotype are put into the E; cluster and associ-
ated to s;.

To choose from among the numerous possible partitions,
which is a combinatorial problem, a non-supervised algo-
rithm requires a function for evaluating stereotype set rele-
vance. Because of the categorical nature of data and the pre-
vious deﬁnition of relative cover, it appears natural to make
use of the similarity measure Msim. This is exactly what we
do by introducing the following evaluation function hE:

Deﬁnition 3

E being an example set, S =
fs;; s1; s2 : : : sng a stereotype set and CS the function that
associates to each example e its relative cover, i.e. its closest
stereotype with respect to Msim and S, the evaluation func-
tion hE is deﬁned as follows:

hE(S) = X
e2E

Msim((cid:14)(e); CS(e))

While k-modes and EM algorithms are straightforward, i.e.
each step leads to the next one until convergence, we re-
duce here the non-supervised learning task to an optimization
problem. This approach offers several interesting features:
avoiding local optima (especially with categorical and sparse
data), providing “good” solutions even if not the best ones,
better control of the search. In addition, it is not necessary to
specify the number of expected stereotypes that is also dis-
covered during the search process.

There are several methods for exploring such a search
space (hill-climbing, simulated annealing, etc.). We have
chosen the meta-heuristic called tabu search which improves
the local search algorithm. Remember that the local search
process can be schematized as follows: 1. An initial solution
Sini is given (for instance at random). 2. A neighborhood P
is calculated from the current solution Si with the assistance
of permitted movements. These movements can be of low
inﬂuence (enrich one stereotype with a descriptor, remove a
descriptor from another) or of high inﬂuence (add or retract
one stereotype to or from the current stereotype set). 3. The

Sc   fs;g ; the current solution
Sb   Sc ; the best up-to-now solution
T   ; ; the list that contains tabu-attributes
for i   1 to N Step do f

let Sc be fs;; s1; s2 : : : sk g
P   ; ; initialize the neighborhood
for all Ai =2 T do f

for m   1 to k do f

for all v   (Ai = Vij ) do
if v does not belong to a stereotype of Sc f

S   fs1 : : : sm(cid:0)1; sm [ v; sm+1 : : : skg
P   P [ S g

if v belongs to a stereotype of Sc f

s0   sm n v
if s0 6= ; then S   fs1 : : : sm(cid:0)1; s0; sm+1 : : : sk g
else S   fs1 : : : sm(cid:0)1; sm+1 : : : sk g
if S 6= ; then P   P [ S g

g
for all v   (Ai = Vij )
if v does not belong to a stereotype of Sc do f

sn   fvg
S   fs1; s2 : : : sm; sng
P   P [ S g

g
SN   argmaxS2P (hE (S))
T is updated, depending on the chosen attribute Ai

that permits to pass from Sc to SN .

Sc   SN
if hE (Sc) > hE (Sb) then Sb   Sc g

return Sb

Figure 1: The default clustering algorithm

best movement, relative to the evaluation function hE, is cho-
sen and the new current solution Si+1 is computed. 4. The
process is iterated a speciﬁc number of times N Step and the
best up-to-now discovered solution is recorded. Then, the
solution is the stereotype set Sb that best maximizes hE in
comparison to all the crossed sets.

As in almost all local search techniques, there is a trade-
off between exploitation, i.e. choosing the best movement,
and exploration, i.e. choosing a non optimal state to reach
completely different areas. The tabu search extends the basic
local search by manipulating short and long-term memories
which are used to avoid loops and to intelligently explore the
search space. This meta-heuristic is detailed in [Glover and
Laguna, 1997] and its application to clustering can be found
in [Al-Sultan, 1995]. Note that only the short-term memory
was used at this stage of our work.

2.5 Default clustering algorithm
Fig. 1 is the main frame of the default clustering algorithm.
It is based on a very basic version of tabu search that tries
to maximize our function hE. Ai denotes the ith attribute
and E is the example set. Sc and Sb stands respectively for
the current and for the best solution. N Step is the maximal
number of iterations, P the current neighborhood and T the
tabu-list that contains tabu-attributes. If an attribute Ai is in
the tabu-list then no descriptor (Ai = Vij ) can be used to
calculate the neighborhood of the current solution.

A “no-redundancy” constraint has been added in order to
obtain a perfect separation between the stereotypes. In the
context of sparseness, it seems really important to extract
contrasted descriptions which are used to quickly classify the
examples, as does the concept of stereotype introduced by
Lippman.

A new constraint called cognitive cohesion is now deﬁned.
It veriﬁes cohesion within a cluster, i.e. an example set Ej (cid:26)
E, relative to the corresponding stereotype sj 2 S. Cognitive
cohesion is veriﬁed if and only if, given two descriptors v1
and v2 2 V of sj, it is always possible to ﬁnd a series of
examples that makes it possible to pass by correlation from
v1 to v2. Below are two example sets with their covering
stereotype. The example on the left veriﬁes the constraint,
the one on the right does not.
s1 : a0 , b1 , d5 , f0 , h0
?, h0
e1 : a0, ?,
?,
e2 : a0, b1, ?,
?,
?
?, d5, ?,
e6 : ?,
?
b1, d5, f0, ?
e8 : ?,
e42: a0, ?, d5, ?,
?

s2 : a0 , b1 , d5 , f0 , h0
e0 : a0, b1, ?,
e8 : ?,
?,
e9 : a0, b1, ?,
e51: ?,
e98: ?,

?, d5, ?, h0
?, d5, ?, h0

?,

?,
?
f0, ?
?,
?

Hence, with s2 it is never possible to pass from a0 to d5,
whereas it is allowed by s1 (you begin with e2 to go from a0
to b1, and then you use e8 to go from b1 to d5). It means that,
in the case of s1, you are always able to ﬁnd a “correlation
path” from one descriptor of the description to another, i.e.
examples explaining the relationship between the descriptors
in the stereotype.

3 Experiments
This section presents experiments performed on artiﬁcial data
sets. This is followed by an original comparison in a real
data case using three well-known clusterers. Default clus-
tering was implemented in a Java program called PRESS
(Programme de Reconstruction d’Ensembles de St´er´eotypes
Structur´es). All the experiments for k-modes, EM and Cob-
web were performed using the Weka platform [Garner, 1995].

3.1 Validation on artiﬁcial data sets
These experiments use artiﬁcial data sets to validate the ro-
bustness of our algorithm. The ﬁrst step is to give some con-
trasted descriptions of D. Let us note ns the number of these
descriptions. Next, these initial descriptions are duplicated
nd times. Finally, missing data are artiﬁcially simulated by
removing a percentage p of descriptors at random from these
ns (cid:2) nd artiﬁcial examples. The evaluation is carried out
by testing different clusterers on these data and comparing
the discovered cluster representatives with the initial descrip-
tions. We verify what we call recovered descriptors, i.e. the
proportion of initial descriptors that are found. This paper
presents the results obtained with ns = 5 and nd = 50 over
50 runs. The number of examples is 250 and the descriptions
are built using a langage with 30 binary attributes. The tabu-
list length is equal to 10 and N Step to 100. Note that the ﬁrst
group of experiments are placed in the Missing Completely
At Random (MCAR) framework.

Fig. 2 shows ﬁrstly that the results of PRESS are very good
with a robust learning process. The stereotypes discovered
correspond very well to the original descriptions up to 75%
of missing descriptors. In addition, this score remains good
(nearly 50%) up to 90%. Whereas Cobweb seems stable rel-
ative to the increase in the number of missing values, the re-
sults of EM rapidly get worse above 80%. Those obtained us-
ing k-modes are the worst, although the number of expected
classes has to be speciﬁed.

k-Modes
(3)
(2)
6
6
0
27
44
0
.74
.60
0
63
(cid:2)
(cid:2)

EM

(4)
6
0
0
.50
0
(cid:2)

(1)
2
48
56
.85
17
(cid:2)

(2)
2
0
0
.66
7
(cid:2)

(1)
6
27
42
.89
70
(cid:2)

n

ex. contradiction
av. contradiction

hE

redundancy
cog. cohesion

EM

n

ex. cont.
av. cont.

hE
red.

cog. coh.

(3)
2
48
56
.83
0
(cid:2)

(4)
2
0
0
.65
0
(cid:2)

(1)
2
56
52
.82
72
(cid:2)

Cobweb
(2)
(3)
2
2
57
0
51
0
.68
.56
0
55
(cid:2)
(cid:2)

(4)
2
0
0
.46
0
(cid:2)

PRESS

6
0
0
.79
0
X

Figure 2: Proportion of recovered descriptors.

3.2 Studying social misrepresentation
The second part of the experiments deals with real data ex-
tracted from a newspaper called “Le Matin” at the end of the
19th century in France. The purpose is to automatically dis-
cover stereotype sets from events related to the political dis-
order in the ﬁrst ten days of September 1893. The results of
PRESS are compared to those of the three clusterers k-modes,
EM and Cobweb. It should be pointed out that our interest
focuses on the cluster descriptions, which we call representa-
tives to avoid any ambiguity, rather than on the clusters them-
selves.

The articles linked to the chosen theme were gathered and
represented using a language with 33 attributes. The terms of
this language, i.e. attributes and associated values, were ex-
tracted manually. Most of the attributes are binary, 4 accept
more than two values and 4 are ordinals. The number of ex-
tracted examples is 63 and the rate of missing data is nearly
87%, which is most unusual.

3.3 Evaluation of default clustering
In order to evaluate PRESS, a comparison was made with
three classical clusterers: k-modes, EM and Cobweb. Hence,
a non-probabilistic description of the clusters built by these
algorithms was extracted using four techniques: (1) using the
most frequent descriptors (mode approach); (2) the same as
(1) but forbidding contradictory features between the exam-
ples and their representative; (3) dividing the descriptors be-
tween the different representatives; (4) the same as (3) but
forbidding contradictory features. Two remarks need to be
made. Firstly, the cluster descriptions resulting from k-modes
correspond to technique (1). Nevertheless, we tried the other
three techniques exhaustively. Secondly, representatives re-
sulting from extraction techniques (3) and (4) entail by con-
struction a redundancy rate of 0%. The comparison was made
according to the following three points:

The ﬁrst approach considers the contradictions between an
example and its representative. The example contradiction is
the percentage of examples containing at least one descriptor
in contradiction with its covering representative. In addition,
if you consider one of these contradictory examples, average
contradiction is the percentage of descriptors in contradiction

Figure 3: Comparative results on Le Matin.

with its representative. This facet of conceptual clustering is
very important, especially in the sparse data context.

Secondly, we check if the cognitive cohesion constraint
(see 2.5) is veriﬁed. The rate of descriptor redundancy is also
considered. These two notions are linked to the concept of
stereotype and to the sparse data context.

Finally, we consider the degree of similarity between the
examples and their covering representatives. This corre-
sponds to the notion of compactness within clusters, but with-
out penalizing the stereotypes with many descriptors. The
function hE seems really adapted to give an account of rep-
resentative relevance. In fact, we used a version of hE nor-
malized between 0 and 1, by dividing by the total number of
descriptors.

3.4 Results
Fig. 3 gives the results obtained from the articles published
in Le Matin. Experiments for the k-modes algorithm were
carried out with N = 2 : : : 8 clusters, but only N = 6 re-
sults are presented in this comparison. The rows of the ta-
ble show the number n of extracted representatives, the two
scores concerning contradiction, the result of hE, the redun-
dancy score and whether or not the cognitive cohesion con-
straint is veriﬁed. The columns represent each type of experi-
ment (k-modes associated with techniques (1) to (4), EM and
Cobweb as well, and ﬁnally our algorithm PRESS).

Let us begin by considering the contradiction scores.
They highlight a principal result of default clustering: us-
ing PRESS, the percentage of examples having contradictory
features with their representative is always equal to 0%. In
contrast, the descriptions built using techniques (1) and (3)
(whatever the clusterer used) possess at least one contradic-
tory descriptor with 27% to 57% of the examples belonging
to the cluster. Furthermore, around 50% of the descriptors
of these examples are in contradiction with the covering de-
scription, and that can in no way be considered as a negligible
noise. This is the reason why processes (1) and (3) must be
avoided, especially in the sparse data context, when building
such representatives from k-modes, EM or Cobweb cluster-
ing. Hence, we only consider techniques (2) and (4) in the
following experiments.

Let us now study the results concerning clustering qual-
ity. This quality can be expressed thanks to the compactness

function hE, the redundancy rate and cognitive cohesion.

PRESS marked the best score (0.79) for cluster compact-
ness with six stereotypes. That means a very good homo-
geneity between the stereotypes and the examples covered.
It is perfectly consistent since our algorithm tries to maxi-
mize this function. The redundant descriptors rate is equal
to 0%, according to the no-redundancy constraint. Further-
more, PRESS is the only algorithm that is able to verify cog-
nitive cohesion. EM obtains the second best score and re-
dundant descriptor rate remains acceptable. However, the
number of expected classes must be given or guessed using
a cross-validation technique, for instance. K-modes and Cob-
web come third and fourth and also have to use an external
mechanism to discover the ﬁnal number of clusters.

Note that the stereotypes extracted using PRESS corre-
spond to the political leanings of the newspaper. For instance,
the main stereotype produces a radical, socialist politician,
corrupted by foreign money and Freemasonry, etc. It corre-
sponds partly to the difﬁculty in accepting the major changes
proposed by the radical party and to the fear caused in France
since 1880 by the theories of Karl Marx. We cannot explain
here in more detail the semantics of discovered stereotypes,
but these ﬁrst results are really promising.

4 Conclusion
Conceptual clustering is seldom studied with such a high
number of missing values. However, it is really important
to be able to extract readable, understandable descriptions
from such type of data in order to complete information, to
classify new observations quickly and to make predictions.
In this way, default clustering presented in this paper tries
to provide an alternative to the usual clusterers. Moreover,
based on local optimization techniques, it proposes a very
general easy-to-extend framework for stereotype set discov-
ering: new movements, constraints added relative to the prob-
lematic chosen, adapted control indexes, etc. The results ob-
tained, on both artiﬁcial data sets and a real case extracted
from newspaper articles, are really promising and should lead
to other historical studies concerning social stereotypes.

For instance, we are currently applying these techniques
to the study of social representations, a branch of social
psychology introduced by S. Moscovici [Moscovici, 1961].
More precisely, this approach is really useful for press content
study which up to now is done manually by experts. Hence,
future work could be done on choosing key dates of the Drey-
fus affair and automatically extracting stereotypical charac-
ters from different newspapers. These results will then be
compared and contrasted with the work of sociologists and
historians of this period.

References
[Al-Sultan, 1995] K. Al-Sultan.

A tabu search ap-
proach to the clustering problem. Pattern Recognition,
28(9):pp.1443–1451, 1995.

[Amossy and Herschberg Pierrot, 1997] R. Amossy

and
A. Herschberg Pierrot. St´er´eotypes et clich´es: langues,
discours, soci´et´e. Nathan Universit´e, 1997.

[Figueroa et al., 2003] A. Figueroa,

and
T. Jiang. Clustering binary ﬁngerprint vectors with miss-
ing values for DNA array data analysis. IEEE Computer
Society Bioinformatics Conference, 2003.

J. Borneman,

[Fisher, 1987] D.H. Fisher. Knowledge Acquisition Via In-
cremental Conceptual Clustering. Machine Learning,
(2):pp.139–172, 1987.

[Ganascia and Velcin, 2004] J.-G. Ganascia and J. Velcin.
Clustering of conceptual graphs with sparse data. Proceed-
ings of the 12th International Conference on Conceptual
Structures, 2004.

[Garner, 1995] S.R. Garner. Weka: The waikato environ-
ment for knowledge analysis. Proc. of the New Zealand
Computer Science Research Students Conference, pages
pp.57–64, 1995.

[Gennari, 1990] J.H. Gennari. An experimental study of con-

cept formation. 1990. Doctoral dissertation.

[Glover and Laguna, 1997] F. Glover and M. Laguna. Tabu

Search. Kluwer Academic Publishers, 1997.

[Lippman, 1922] W. Lippman. Public Opinion. Ed. MacMil-

lan, NYC, 1922.

[Michalski, 1980] R.S. Michalski. Knowledge acquisition
through conceptual clustering : A theorical framework and
algorithm for partitioning data into conjunctive concepts.
International Journal of Policy Analysis and Information
Systems, (4):pp.219–243, 1980.

[Moscovici, 1961] S. Moscovici. La psychanalyse : son im-

age et son public. PUF, Paris, 1961.

[Narboux, 2001] J.-P. Narboux. Ressemblances de famille,

caract`eres, crit`eres. pages pp.69–95. PUF, 2001.

[Newgard and Lewis, 2002] C.D. Newgard and R.J. Lewis.
The Imputation of Missing Values in Complex Sampling
Databases: An Innovative Approach. Academic Emer-
gency Medicine, 9(5484), 2002.

[Reiter, 1980] R. Reiter. A logic for default reasoning. Arti-

ﬁcial Intelligence, (13):pp.81–132, 1980.

[Rich, 1979] E. Rich. User Modeling via Stereotypes.

In-
ternational Journal of Cognitive Science, 3:pp.329–354,
1979.

[Rosch, 1975] E. Rosch. Cognitive representations of se-
mantic categories. Journal of Experimental Psychology:
General, (104):pp.192–232, 1975.

[Rosch, 1978] E. Rosch. Principles of categorization.

In
E. Rosch and B. Lloyd, editors, Cognition and Catego-
rization, pages 27–48. NJ: Lawrence Erlbaum, Hillsdale,
1978.

[Sarkar and Leong, 2001] M. Sarkar and T.Y. Leong. Fuzzy
Proc AMIA

k-means clustering with missing values.
Symp., pages pp.588–92, 2001.

[Shawver, 2004] L.

Shawver.

Commentary

Wittgenstein’s
http://users.rcn.com/rathbone/lw65-69c.htm.

Philosophical

Investigations,

on
2004.

[Wittgenstein, 1953] L. Wittgenstein. Philosophical Investi-

gations. Blackwell, Oxford, UK, 1953.


Dynamic Veriﬁcation of Trust in Distributed Open Systems

Nardine Osman and David Robertson

The University of Edinburgh

School of Informatics

N.Osman@sms.ed.ac.uk, dr@inf.ed.ac.uk

Abstract

In open and distributed systems, agents must en-
gage in interactions of which they have no previous
experience. Deontic models are widely used to de-
scribe aspects of permission, obligation, and trust
anticipated by such agents, but no practical mech-
anism has been developed for testing deontic trust
speciﬁcations against models of multi-agent inter-
actions. This paper describes a way of doing this;
an implementation of it via model checking; and
some preliminary results on a realistic example.

1 Introduction
In large-scale open distributed systems, trust remains a funda-
mental challenge. Despite much research, the notion of trust
remains vague and there is no consensus on what exactly trust
is in systems such as multiagent systems (MAS). This is be-
cause trust may be addressed at different levels. At the low
system level, trust is associated with network security, such
as authentication (determining the identity of the user entity),
access permissions (deciding who has access to what), con-
tent integrity (determining whether the content has been mod-
iﬁed), content privacy (ensuring that only authorised entities
can access the content), etc. At higher levels, the focus is
on trusting entities — these may be human users, software
agents, services, directories, etc. — to perform actions as
requested, provide correct information, not to misuse infor-
mation, execute protocols correctly, etc.

Available research has mainly focused on analysing, im-
proving, and developing strategies that address trust issues
at various system levels. In this paper we focus not on the
strategies, but on the possibility of specifying and verifying
such strategies. We therefore inherit the general deﬁnition of
trust; trust is deﬁned as the problem of who to interact with,
when to interact with them, and how to interact with them
[Ramchurn et al., 2004]. We then show how the speciﬁca-
tion and veriﬁcation methods of [Osman et al., 2006] may be
used to specify and verify trust models at various system lev-
els. We use the Lightweight Coordination Calculus (LCC) of
[Robertson, 2004] for modelling global interaction models.
Local trust constraints, on the agents level, are modelled via a
simple trust policy language introduced in Section 3.2. These
models may then be fed to a lightweight dynamic model

checker [Osman et al., 2006] for verifying interesting trust
properties, which go beyond liveness and safety properties
veriﬁed by traditional veriﬁcation techniques. The result is
a powerful, yet simple, veriﬁcation mechanism. The veriﬁer
itself is lightweight, delegating the complexity of managing
the search space to the underlying XSB tabled Prolog system
[Sagonas et al., 1994].

We open with a motivating example in Section 2. Section 3
provides an overview of our system model and the languages
used for speciﬁcation. The veriﬁcation process is introduced
in Section 4, before concluding with our results in Section 5.

2 Motivating Example: an Auction System
Section 3 presents our 2-layered architectural approach for
distributed open systems. Similar to web service architec-
tures, the basic idea is that a global interaction model is
used to specify the rules of the interaction, irrespective of the
agents engaged in this interaction. Then each agent, based
on its local constraints, tries to ﬁnd the most suitable inter-
action protocol along with the most suitable group of agents
to interact with. We call the agents’ local constraints the de-
ontic constraints, since they specify the agents permissions,
prohibitions, and obligations.

Now let us consider the case where an agent is interested in
engaging in an auction scenario for either selling or buying a
speciﬁc item. Trust issues automatically arise on two levels.
These may be summarised by the following two questions:

! Which interaction protocol should the agent engage in?

! In such an interaction, which agents does it engage with?

For example, before selling its item, the auctioneer will
have to pick the appropriate interaction protocol, where ap-
propriateness is measured by the satisﬁability of certain prop-
erties. Traditional properties to check for are usually liveness
and safety properties. For example, the auctioneer may de-
cide that the interaction protocol is trusted only if it is dead-
lock free (trust issue TI 1 of Figure 1). A much more interest-
ing set of properties may be obtained when tackling domain
speciﬁc issues. For example, a more challenging trust issue
to verify is whether the interaction protocol enforces truth-
telling by the bidders or not (trust issue TI 2 of Figure 1).

For a given interaction protocol, each agent will then have
to select the appropriate agents for such an interaction. The
goal is to achieve a set of agents that trust each other. For

IJCAI-07

1440

TI 1:

Is the interaction protocol deadlock free?

TI 2:

In such an interaction, can the bidders be better off if they bid either a lower or a
higher value than their true valuation?

TI 3:

If from previous experience the agent knows that DVDs from auctioneer A are
not original, then A is not trusted in delivering good quality DVDs.

TI 4:

If the auctioneer agent A is not trusted in delivering good quality DVDs, then it
is not trusted in delivering good quality CDs.

TI 5: Agent A is trusted to take the role of the auctioneer only if it has decent ratings
and holds a good reputation, with more importance given to the most recent
ratings.

Figure 1: The auction scenario: some trust issues

example, one bidder may trust auctioneer A in selling any-
thing except DVDs — possibly, due to previous experience,
it now knows that these DVDs are not original. It may also
use socio-cognitive models of trust to learn that if the DVDs
are not original, then most probably the CDs will not be too
(trust issues TI 3 and TI 4 of Figure 1). Another widely used
trust mechanism is the use of ratings and reputations. The
agents should be capable of collecting (or having access to)
each other’s rating. It is then up to each agent to aggregate
these ratings as they see ﬁt. For example, a bidding agent
might decide not to trust new auctioneers with no selling his-
tory. An average rating is then required, possibly giving more
importance to the latest ratings (trust issue TI 5 of Figure 1).

The trust issues of Figure 1 cover a wide sample of the
various trust mechanism in the literature [Ramchurn et al.,
2004]: from socio-cognitive models (TI 4 of Figure 1), to
evolutionary and learning models (TI 3 of Figure 1), repu-
tation mechanism (TI 5 of Figure 1), trustworthy interaction
mechanisms ((TI 2 of Figure 1)), etc. In this paper, we do not
specify how trust is learned. We focus on the agent’s individ-
ual aggregation mechanisms and their speciﬁcation, which is
essential for verifying trust. For example, while we do not
focus on how the agent obtains the ratings of another (TI 5 of
Figure 1), we do require the speciﬁcation of how these ratings
are aggregated.

The main goal of this paper is to show how agents may
answer these questions by using a dynamic model checker. In
our running example, the agent’s constraints used (or the trust
constraints) are those of Figure 1. The interaction protocol
veriﬁed is presented by the state-space graph of Figure 2.

The interaction of Figure 2 is that of a Vickrey auction. The
interaction starts at state s0 when the auctioneer A sends an
invite to a set of bidders for bidding on item I with a reserve
price R. The interaction remains at state s0 until invites are
sent to all bidders. Then the bidders send their sealed bids
back to the auctioneer. This is represented by state s1 of Fig-
ure 2. When all bids are collected, the interaction moves to
the new state s2. The auctioneer informs the winner of the
price V to be paid, moving the interaction to state s3. Finally,
the winning bidder sends its payment P, and the interaction is
completed at state s4.

Before we present the veriﬁcation mechanism used in Sec-
tion 4, Section 3 introduces our system model, its architec-
ture, and its speciﬁcation languages.

message(a(auctioneer,A), a(bidder,Bi), invite(I,R))

message(a(auctioneer,A), a(bidder,Bi), invite(I,R))

message(a(bidder,Bi), a(auctioneer,A), bid(Vi))

message(a(bidder,Bi ), a(auctioneer,A), bid(Vi))

message(a(auctioneer,A), a(bidder,Bi), won(I,V))

message(a(bidder,Bi ), a(auctioneer,A), payment(P))

s0

s1

s2

s3

s4

where message(A,B,M) represents the transmission of message M from A to B and

a(R,I) represents an agent with id I playing the role R

Figure 2: The auction scenario: the interaction’s state-space

3 System Modelling

We view MAS as a collection of autonomous agents. The sys-
tem is open and distributed. Various agents may join or leave
the system at any time. Interactions become the backbone that
holds the system together. Agents group themselves into dif-
ferent, and possibly multiple, interactions. Figure 3 provides
such an example, where a collection of agents are grouped
into three different interactions (or scenarios): two auction
scenarios and a trip planning scenario.

Due to the dynamic nature of the system, we believe inter-
action groups should be created dynamically and automati-
cally by the agents. We also believe everything should be dis-
tributed. This implies that there should be no higher layer for
coordination, control, synchronisation, etc. It is the agents’
responsibility to group themselves into different scenarios.
As a result, we split the MAS model into two layers: the inter-
action layer and the agents layer.

The interaction model speciﬁes the rules and constraints
on the interaction. This indicates how exactly the interaction
may be carried out. The agents’ models specify the rules and
constraints on the agents. These are the agents’ permissions,
prohibitions, and obligations; we therefore call this model the
deontic model (Figure 3). Note that for one scenario there is
one global interaction model and several local deontic mod-
els. While agents need to share the interaction model in order
to know the rules of the interaction they’re engaged in, each
agent will have its own local constraints in its deontic model.
In what follows, we introduce the languages used in speci-

fying these models.

3.1 The Interaction Model

We choose the Lightweight Coordination Calculus (LCC)
[Robertson, 2004] for modelling the interaction’s state-space
graph, since it is the only executable process calculus for MAS
that we are aware of1. Having an executable process calcu-
lus for modelling the interaction’s state-space graph is very
useful for dynamically verifying the executable models of in-
teraction. Figure 4 presents the syntax of LCC.

1Note that we are aware of other coordination systems, but none

of these are based directly on a process calculus

IJCAI-07

1441

auction
scenario

auction
scenario

trip planning

scenario

rules of the
interaction

≡

interaction

model

engaged in

agent

constraints

≡

deontic
model

Figure 3: The MAS model – a 2-layered architecture model

Interaction
Clause
Agent
ADef

:= {Clause, . . .}
:= Agent :: ADef
:= a(Role, Id)
:= null ← C | Agent ← C | M essage ← C |

ADef then ADef | ADef or ADef |
ADef par ADef

M essage

:= M ⇒ Agent | M ⇐ Agent

C := T erm | C ∧ C | C ∨ C

Role

:= T erm
M := T erm

null denotes an event which does not involve message passing.
T erm is a structured term in Prolog syntax.
Id is either a variable or a unique agent identiﬁer.

Figure 4: LCC syntax

Agents, in LCC, are deﬁned by their roles and identiﬁers.
An interaction is deﬁned by a set of clauses. A clause gives
each agent role a deﬁnition that speciﬁes its acceptable be-
haviour. An agent can either do nothing (usually used for
internal computations), take a different role, or send/receive
messages (M ⇒ A, M ⇐ A). Agent deﬁnitions are constructed
using the sequential (then), choice (or), parallel composition
(par), and conditional (←) operators. The conditional opera-
tor is used for linking constraints to message passing actions.

Example Revisited
Figure 5 presents the speciﬁcation of the state-space graph of
Figure 2 via LCC. The auctioneer A — knowing the item I,
the reserve price R, and the set of bidders Bs — recursively
sends an invite to all bidders in set Bs. It then takes the role
of auctioneer2 to collect bids, send the winner a message,
collect payment, and deliver the item won. On the other side,
each bidder agent receives an invite from the auctioneer and
sends its bid based on its valuation. The winner receives a
win message and then sends its payment P.

3.2 Deontic Models

In this paper, we focus on the trust constraints imposed by
the agents. We propose a trust policy language for the speci-
ﬁcation of trust rules. The language is similar to other logic-
based policy languages which are built on deontic concepts,
such as ASL [Jajodia et al., 1997], RDL [Hayton et al., 1998],
and Rei [Kagal et al., 2003]. The syntax of our language is
presented by Figure 6.

The syntax states that

trust rules might either hold
in general or under certain conditions: T rustSpecs [←
Condition]. The interaction’s trustworthiness is modelled by

a(auctioneer(I, R, Bs), A) ::

( invite(I, R) ⇒ a(bidder, B) ← Bs = [B|T ]then

a(auctioneer(I, R, T ), A) )

or
a(auctioneer2(Bs, []), A) ← Bs = [].

a(auctioneer2(Bs, V s), A) ::

append([B, V ], V s, V n) ← bid(B, V ) ⇐ a(bidder, B) then
( a(auctioneer2(Bs, V n), A) ← not(all bid(Bs, V n))

or
( win(B1, V 2) ⇒ a(bidder, B1)

← all bid(Bs, V n) and

highest(V n, B1, ) and second highest(V n, , V 2) then

deliver(I, B1) ← payment(P ) ⇐ a(bidder, B1) ) ).

a(bidder, B) ::

invite(I, R) ⇐ a(auctioneer( , , ), A) then
bid(B, V ) ⇒ a(auctioneer2( , ), A ← valuation(I, V ) then
win(Bi, V i) ⇐ a(auctioneer2( , ), A) then
payment(P ) ⇒ a(auctioneer2( , ), A) ← Bi = B and payment(P ).

Figure 5: LCC speciﬁcation for the interaction of Figure 2

T rustRule
T rustSpecs

Agent
Sign
Action
MPA

Condition

Role,N-MPA, M essage

:= T rustSpecs [← Condition]
:= trust(interaction(IP ), Sign) |

trust(Agent, Sign) |
trust(Agent, Sign, Action)

:= a(Role, Id)
:= + | −
:= MPA | N-MPA | T rustSpecs
:= M essage ⇒ Agent |

M essage ⇐ Agent

:= Condition ∧ Condition |
Condition ∨ Condition |
T emporal | T erm

:= T erm

where, [X] denotes zero or one occurrence of X,
IP is an interaction protocol speciﬁed in LCC,
Id is either a variable or a unique agent identiﬁer,
T emporal is a temporal property whose syntax is speciﬁed in [Osman et al., 2006],
and
T erm is either a variable or a structured term in Prolog syntax.

Figure 6: Syntax of our trust policy language

trust(interaction(IP ), Sign), where IP is the LCC speciﬁ-
cation of the interaction protocol in question (e.g.
the in-
teraction protocol of Figure 5). Sign could take the values
‘+’ and ‘−’ to model trust and distrust, respectively. The
agent’s trustworthiness is modelled by trust(Agent, Sign),
where Agent represents the agent in question. Only if the
agent is trustworthy, it can engage in an interaction. Trusting
or distrusting agents to perform speciﬁc actions is modelled

IJCAI-07

1442

by trust(Agent, Sign, Action). Actions could either be mes-
sage passing actions (MPA) — such as sending (M essage ⇒
Agent) or receiving (M essage ⇐ Agent) messages — or
non-message passing actions (N-MPA) — such as perform-
ing computations. We also allow actions to take the form of
another trust rule (note the T rustSpecs in the Action deﬁni-
tion). This supports the delegation of trust, since it permits the
speciﬁcation of whether an agent’s trust itself is to be trusted
or not.

Example Revisited

Trust issues TI 3, TI 4, and TI 5 of Figure 1 address trust at
the agent level. In what follows, we present the speciﬁcation
of the more complex trust rule, TI 5, in our trust policy lan-
guage. The rule is presented by Figure 7(a). It speciﬁes that
agent A is trusted as an auctioneer only if it has a selling his-
tory of at least 50 items and an average rating above 70%,
going up to 90% for the latest 20 transactions. The mecha-
nism presented here distrusts new entrants and focuses on the
agent’s latest ratings rather than the overall one.

Trust issues TI 1 and TI 2 of Figure 1 address trust at the
interaction level. The conditions for trusting an interaction
protocol are speciﬁed via a temporal language. Since the
‘deadlock free’ property of TI 1 is a straightforward property
to model via a temporal language, we show how the more
challenging property of ‘enforcing truth-telling by the bid-
ders’ (TI 2) may be speciﬁed.

To prove the protocol enforces truth-telling by the bidders,
we prove that, for the interaction protocol of Figure 5, the
bidders cannot do any better than bidding their true valua-
tion V: the maximum value they are willing to pay. Note that
we do not verify whether the agent will actually bid V or
not, but whether the interaction protocol provides an incen-
tive for bidding a different value than V. For this, we study
the two cases: (1) if the competing agent bids a higher value
Ch, and (2) if the competing agent bids a lower value Cl. In
Figure 8, the grey circle represents the bidder and its valua-
tion V. The other two circles represent the two cases of the
competing agent, which may bid either a higher or a lower
value: Ch and Cl, respectively. For each of these two cases,
the bidder may either bid its true valuation, a value between
its true valuation and that of its competitor, or a value be-
yond that of the competitor (Figure 8). The trust rule of Fig-
ure 7(b) studies all 6 cases, respectively. For each bid, the
winner and the price won at are computed through the tempo-
2.
ral property [bid(Bidder,Bid1), bid(Competitor,Bid2)] <win(Winner,Price)> tt
The bidder should, naturally, be expected to lose to its com-
petitor in cases 1, 2, and 6. It should win in cases 3, 4, and
5, where case 4 would be the only case where the bidder bids
its true valuation and wins the item for the price Y. The trust
rule requires that the bidder is not better off when winning
in cases 3 and 5, i.e. where the item is won for prices X and
Z, respectively. This is expressed by the conditions X≥Y and
Z≥Y.

The InteractionProtocol of the trust rule presented by Fig-
ure 7(b) is in fact the LCC protocol of Figure 5. This rela-

2The temporal property [X,Y]<Z>tt speciﬁes that if messages X

and Y are sent, then message Z will eventually be received.

trust(a(auctioneer,A), +) ←

rating count(a(auctioneer,A), Total) and Total > 50 and
rating average(a(auctioneer,A), Average) and Average > 0.7 and
rating latest(a(auctioneer,A), 20, Latest) and Latest > 0.9.

(a) Trust rule TI 5

trust(a(interaction(InteractionProtocol)), +) ←

Vl’<Cl and Cl<Vl and Vl<V and V<Vh and Vh<Ch and Ch<Vh’ and
[bid(Bidder,V), bid(Competitor,Ch)] <win(Competitor, )> tt and
[bid(Bidder,Vh), bid(Competitor,Ch)] <win(Competitor, )> tt and
[bid(Bidder,Vh’), bid(Competitor,Ch)] <win(Bidder,X)> tt and
[bid(Bidder,V), bid(Competitor,Cl)] <win(Bidder,Y)> tt and
[bid(Bidder,Vl), bid(Competitor,Cl)] <win(Bidder,Z)> tt and
[bid(Bidder,Vl’), bid(Competitor,Cl)] <win(Competitor, )> tt and
X≥Y and Z≥Y.

(b) Trust rule TI 2

Figure 7: Speciﬁcation of trust rules

case 3: agent bids Vh’
where Vh’ > Ch

case 2: agent bids Vh

where Ch > Vh > V

case 1: agent bids V

i

d
b
 
g
n
i
t
e
p
m
o
c
 
r
e
h
g
h

i

Ch

V

Cl

i

d
b
g
n

 

i
t

e
p
m
o
c
 
r
e
w
o

l

case 4: agent bids V

case 5: agent bids Vl

where V > Vl > Cl

case 6: agent bids Vl’
where Cl > Vl ’

Figure 8: The bidding strategies: the 6 cases

tively complex LCC structure3 is passed entirely as a parame-
ter to the trust rule. The interaction protocol is then said to be
trusted if the condition — the collection of temporal proper-
ties — is satisﬁed. In this example, the condition veriﬁes the
possible occurrence of transmitting the bid( , ) and win( , )
messages in the LCC interaction protocol of Figure 5.

It is worth noting that the trust rule is restricted to the auc-
tions domain, yet independent of the speciﬁc auction proto-
col it is veriﬁed upon. For example, the six cases of Fig-
ure 8 are comprehensive, even for auction systems consist-
ing of more than two agents. This is because verifying the
utility of one agent should take into consideration only the
competing agent’s bid — all other agents’ bids are irrelevant.
Furthermore, the veriﬁcation of such a trust rule will termi-
nate with correct results, whether positive or negative, for any
auction protocol that requires agents to place their bids be-
fore a message is transmitted informing who the winner is.
It will probably fail when veriﬁed against more complex auc-
tion protocols, such as those selling multiple items and having
several winners. However, the veriﬁcation will be success-

3For a comparison between the LCC process calculus and tra-
ditional process calculi, such as Milner’s CCS [Milner, 1989] and
Hoar’s CSP [Hoare, 1985], the interested reader may refer to [Os-
man et al., 2006]. From this comparison, we may assert that the
complexity of LCC is similar to that of traditional process calculi,
such as CCS and CSP.

IJCAI-07

1443

ful in the most common auctions, such as the English, Dutch,
sealed ﬁrst-price, and sealed second-price auctions. Allowing
agents to automatically verify such properties (which are rel-
atively independent of the speciﬁc interaction protocol), aids
the agents in making their protocol selection when faced with
new and unexplored protocols.

4 The Dynamic Model Checker

We believe it is solely the agents’ responsibility to answer the
question of how, when, and who to interact with. Ideally, in
a highly dynamic system, this should be done at run time by
deciding which combination of interaction and deontic (trust)
models is currently suitable. But is this feasible?

We choose model checking from amongst other veriﬁcation
techniques because it provides a fully automatic veriﬁcation
process which could be carried out by the agents during in-
teraction time. We show how interaction time veriﬁcation is
made possible with the use of a remarkably efﬁcient (see Fig-
ure 11) and lightweight model checker. The concerned agent
can then feed the model checker with the system model: the
combination of the global interaction model and agents’ lo-
cal trust rules. The model checker should then verify whether
the trust rules are consistent amongst themselves as well as
consistent with respect to the interaction model. It should be
capable of detecting non-suitable (distrusted) agents and/or
interaction protocols.

4.1 Model Checking: an Overview

The model checking problem can be deﬁned as follows:
Given a ﬁnite transition system S and a temporal formula
φ, does S satisfy φ? The model checking process is divided
into three stages: modelling, speciﬁcation, and veriﬁcation.
The system to be veriﬁed must ﬁrst be modelled in the lan-
guage of the model checker S. The properties (φ) to which
the system model is veriﬁed upon should be speciﬁed using
the model checker’s temporal logic. Both the system model
and the properties speciﬁcation are fed to the model checker
for veriﬁcation. The model checker is, essentially, an algo-
rithm that decides whether a model S satisﬁes a formula φ.
Figure 9 illustrates the model checking process.

System model:

S

Property specification:

φ

Model

Checking
Algorithm

Result:
true /
false

Figure 9: The model checking process

4.2 Model Checking: the Implementation

In traditional model checking, the system model represents
a state-space graph. The constraints on a given state-space
are then speciﬁed through temporal properties in the prop-
erty speciﬁcation section. In our case, the system model is
a combination of the shared interaction’s state-space and the

agents’ local trust constraints. Trust constraints based on tem-
poral properties, such as those constraining the interaction
(e.g. TI 2 of Figure 7(b)), are modelled in the property speci-
ﬁcation section. Other trust rules constraining the agents (e.g.
TI 5 of Figure 7(b)) are kept on the deontic level of the system
model. Figure 10 provides an overview of our trust verifying
model checker. The sample input data is that of the auction
system scenario. The interaction model sample in Figure 10
is a copy of that of Figure 5. The deontic model sample, or
the trust constraints on agents, is a copy of Figure 7(a). The
property speciﬁcation is the trust constraint on the interaction
protocol (Figure 7(b)).

System model:

a(auctioneer(I,R,Bs),A) ::

( invite(I,R) ⇒ a(bidder,B) ← Bs=[B|T] then
...

a(auctioneer2(Bs,Vs), A) ::

append([B,V], Vs, Vn) ← bid(V) ⇐ a(bidder,B) then
...

a(bidder,B) ::

invite(I,R) ⇐ a(auctioneer( , , ),A) then
...

s0

s1

s2

s3

trust(a(auctioneer,A), +) ←

rating count(a(auctioneer,A), Total) and Total > 50 and
...

...

Property specification:

trust(a(interaction(InteractionProtocol)), +) ←

Vl ’<Cl and Cl<Vl and Vl<V and
V<Vh and Vh <Ch and Ch<Vh ’ and
[bid(Bidder,V), bid(Competitor,Ch)] <win(Competitor, )> tt and
...

...

Model

Checking
Algorithm

Result:
true /
false

n
o

i
t
c
a
r
e

l

e
d
o
m

t

n

i

l

e
d
o
m

c
i
t

n
o
e
d

l

a
r
o
p
m
e

t

s
e

i
t
r
e
p
o
r
p

Figure 10: The auction scenario: the model checker’s input

The model checker should then verify that the interac-
tion model is trusted and that the agents are trusted to en-
gage in the given interaction. To verify this, the concerned
agent feeds the model checker with the appropriate interac-
tion model along with the trust rules that are split between
the deontic model and the property speciﬁcation. The veriﬁ-
cation process is explained in the following section.

The Veriﬁcation Process
In this section, we present an overview of the model check-
ing algorithm — the black box of Figure 10. We refer the
interested reader to [Osman et al., 2006] for the details of the
model checker and its operational semantics.

The veriﬁcation process is carried out as follows. The in-
teraction model, the deontic model, and the temporal property
to be veriﬁed are fed to the model checker. Veriﬁcation starts
at the initial state s0 of the interaction model, and the model
checker tries to verify that the temporal property φ is satisﬁed
at s04.
If it succeeds, the veriﬁer terminates and the prop-
erty is said to be satisﬁed. Otherwise, the veriﬁer attempts to
make a transition(s) to the next state(s) in the state-space5. If
the transition(s) violates any of the trust (deontic) rules, then
the veriﬁcation process terminates and the property is not sat-
isﬁed. Otherwise, the satisfaction of the property φ is veriﬁed
with respect to the new state(s), and the whole process is re-
peated all over again.

4Satisfaction is veriﬁed by applying the modal μ-calculus proof

rules presented in [Osman et al., 2006].

5Transitions are made based on the LCC transition rules pre-

sented in [Osman et al., 2006].

IJCAI-07

1444

The result is a remarkably compact model checker built on
top of the XSB system [Sagonas et al., 1994], a tabled Prolog
system. This compactness is achieved by placing the burden
of searching the state-space on the underlying XSB system.

5 Conclusion and Results

Trust is the key to the success of large-scale distributed sys-
tems. Most of the work carried on in the ﬁeld of trust has
focused on the strategies for ensuring trusted interactions. In
this paper, we have presented a mechanism for specifying and
verifying such strategies. Our mechanism allows the agents
involved to dynamically and automatically invoke the model
checker at run-time, when the conditions for veriﬁcation are
met, and verify that certain trust requirements will not be
broken for a given scenario with a given set of collaborating
agents.

The complexity of verifying multi-agent systems depends
on the complexity of the different roles in the interaction
rather than the number of agents involved in such an inter-
action. As shown in Section 3.1, interaction protocols are de-
ﬁned in terms of agents’ roles instead of the individual agents
that might be engaged in the interaction. For example, there
are two roles for our auction scenario: the role of the auction-
eer and that of the bidder. All bidders then share the same
protocol speciﬁcation (Figure 5), irrespective of their differ-
ent local deontic constraints. The complexity of verifying the
ﬁve trust issues of Figure 1 for the auction scenario of Fig-
ure 2 (or Figure 5) depends on the complexity of the auction-
eer’s and bidder’s role deﬁnitions. In our scenario, if the trust
issues are veriﬁed for a set of one auctioneer agent and two
bidder agents, then the results will hold for a set of one auc-
tioneer agent and n bidder agents, where n ≥ 2. The trick
is to know the minimum number of agents required for ver-
ifying certain properties of a given interaction protocol. We
assume such information is provided with the interaction pro-
tocol.

In our example, we set the scene to incorporate one auc-
tioneer and two bidders. The dynamic model checker is then
invoked for verifying the ﬁve trust issues of Figure 1 against
the auction scenario of Figure 5. The CPU time and memory
usage, from the moment the model checker is invoked until
the results are returned to the user, are presented by Figure 11.

CPU time (sec)

Memory usage (MB)

TI 1
0.017
1.327

TI 2
0.204
14.052

TI 3
0.020
1.356

TI 4
0.021
1.376

TI 5
0.010
0.917

Figure 11: Preliminary results

Note that results for TI 2, in Figure 11, differ drastically
from others since TI 2 is essentially constructed from 6 tem-
poral properties (see Section 3.2: Example Revisited), while
the others are single temporal properties.

The model checker presented in this paper has been tested
on several benchmark scenarios, such as a variety of auction
scenarios and a travel agency scenario [Osman et al., 2006].
These results were not presented in this paper since they are

not concerned with the trust application, which this paper fo-
cuses on, but with more general deontic constraints. How-
ever, the CPU time in all cases never exceeded one second.
These preliminary results prove that dynamic veriﬁcation is
made possible in such realistic scenarios. The limits of the
model checker still need to be tested. This requires a compre-
hensive study of possible scenarios, their interesting temporal
properties, and the level of complexity that these could reach.
In conclusion, the novelty of our work is in introducing
interaction time veriﬁcation and applying it to the ﬁeld of
trust in multi-agent systems. This is made feasible by us-
ing a relatively compact (implemented in 150 lines of Prolog
code), yet efﬁcient (as shown by the results of Figure 11), dy-
namic model checker. This allows agents to invoke the model
checker at run time for verifying various trust issues.

References
[Hayton et al., 1998] Richard Hayton, Jean Bacon, , and Ken
Moody. Access control in an open distributed environ-
ment. In Symposium on Security and Privacy, pages 3–14,
Oakland, CA, 1998. IEEE Computer Society Press.

[Hoare, 1985] Charles Hoare. Communicating Sequential

Processes. Prentice Hall, 1985.

[Jajodia et al., 1997] Sushil Jajodia, Pierangela Samarati,
and V. S. Subrahmanian. A logical language for expressing
authorizations. In Proceedings of the 1997 IEEE Sympo-
sium on Security and Privacy (SP ’97), page 31, Washing-
ton, DC, USA, 1997. IEEE Computer Society.

[Kagal et al., 2003] Lalana Kagal, Tim Finin, and Anupam
Joshi. A policy language for a pervasive computing envi-
ronment.
In IEEE 4th International Workshop on Poli-
cies for Distributed Systems and Networks. IEEE Com-
puter Society, June 2003.

[Milner, 1989] Robin Milner. Communication and Concur-

rency. Prentice Hall, 1989.

[Osman et al., 2006] Nardine Osman, David Robertson, and
Christopher Walton. Dynamic model checking for multi-
agent systems. In Proceedings of the Fourth International
Workshop on Declarative Agent Languages and Technolo-
gies (DALT’06), Hakodate, Japan, May 2006.

[Ramchurn et al., 2004] Sarvapali D. Ramchurn, Dong
Hunyh, and Nicholas R. Jennings. Trust in multi-agent
systems. Knowledge Engineering Review, 2004.

[Robertson, 2004] David Robertson. A lightweight coordi-
nation calculus for agent social norms. In Proceedings of
Declarative Agent Languages and Technologies workshop,
New York, USA, 2004.

[Sagonas et al., 1994] Konstantinos

Sagonas,

Terrance
Swift, and David S. Warren. XSB as an efﬁcient deductive
database engine.
the 1994 ACM
SIGMOD international conference on Management of
data (SIGMOD ’94), pages 442–453. ACM Press, 1994.

In Proceedings of

IJCAI-07

1445


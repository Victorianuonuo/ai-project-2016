Gaussian Process Models of Spatial Aggregation Algorithms 

Naren Ramakrishnan 

Department of Computer Science 
Virginia Tech, VA 24061, USA 

naren@cs.vt.edu 

Chris Bailey-Kellogg 

Department of Computer Sciences 
Purdue University, IN 47907, USA 

cbk@cs.purdue.edu 

Abstract 

Multi-level  spatial  aggregates  are  important  for 
data mining in  a variety of scientific  and engineer(cid:173)
ing applications, from analysis of weather data (ag(cid:173)
gregating temperature and pressure data into ridges 
and fronts) to performance analysis of wireless sys(cid:173)
tems  (aggregating  simulation  results  into  config(cid:173)
uration  space  regions  exhibiting  particular  perfor(cid:173)
mance  characteristics).  In  many  of these  applica(cid:173)
tions,  data  collection  is  expensive  and  time  con(cid:173)
suming,  so  effort  must  be  focused  on  gathering 
samples  at  locations  that  will  be  most  important 
for the  analysis.  This  requires  that  we  be  able  to 
functionally  model  a data mining algorithm in  or(cid:173)
der  to  assess  the  impact  of  potential  samples  on 
the mining of suitable spatial  aggregates.  This pa(cid:173)
per  describes  a  novel  Gaussian  process  approach 
to  modeling  multi-layer  spatial  aggregation  algo(cid:173)
rithms, and demonstrates the ability of the resulting 
models to capture the essential underlying qualita(cid:173)
tive  behaviors  of the  algorithms.  By  helping  cast 
classical  spatial  aggregation algorithms in  a rigor(cid:173)
ous  quantitative  framework,  the  Gaussian  process 
models support diverse uses such as directed sam(cid:173)
pling, characterizing the sensitivity of a mining al(cid:173)
gorithm  to  particular parameters,  and  understand(cid:173)
ing how variations in input data fields percolate up 
through a spatial aggregation hierarchy. 

Introduction 

1 
Many  important tasks  in  data  mining,  scientific  computing, 
and qualitative modeling involve the successive and system(cid:173)
atic spatial aggregation and redescription of data into higher-
level  objects.  For instance,  consider the characterization of 
WCDMA  (wideband code-division multiple access) wireless 
system  configurations  for  a  given  indoor  environment. 
In 
noisy channels, the performance goal is to quantitatively as(cid:173)
sess the relationship between the signal-to-noise ratio (SNR) 
and  the  bit  error  rate  (BER)  or  bit  error  probability  (BEP) 
of the realized configuration.  To improve performance in of(cid:173)
fice environments  (characterized  by  doorways,  walls,  cubi(cid:173)
cles), a common trick used is to incorporate space-time trans(cid:173)
mit diversity (STTD). Instead of a single transmitter antenna, 

SNRl,dB 

Figure  1:  Mining  configuration  spaces  in  wireless  system 
configurations.  The  shaded  region  denotes  the  largest  por(cid:173)
tion  of  the  configuration  space  where  we  can  claim,  with 
confidence at least 99%, that the average bit error probability 
(BEP) is acceptable for voice-based system usage.  Each cell 
in the plot is the result of the spatial and temporal aggregation 
of hundreds of time-consuming wireless  system  simulations. 

the base station uses two transmitter antennas separated by a 
small distance. If the signal from one of the antennas is weak, 
the  signal  from  another  is  likely  to  be  high,  and  the  over(cid:173)
all  performance is expected to improve.  In this application, 
it  is  important  to  assess  how  the  power-imbalance  between 
the  two  branches  impacts  the  BEP of the  simulated  system, 
across a range of SNRs (see Fig. 1; [ Verstak et al, 2002]). 

Characterizing  the  performance  of WCDMA  systems  re(cid:173)
quires  the  identification  of  multi-level  spatial  aggregates  in 
the  high-dimensional  configuration  spaces  of  wireless  sys(cid:173)
tems.  The  lowest  level  (input)  contains  individual  Monte 
Carlo simulation runs providing unbiased estimates of BEPs. 
This space is high-dimensional (e.g. 
10), owing to the mul(cid:173)
titude  of  wireless  system  parameters  (e.g.  channel  models, 
fading  characteristics,  coding  configurations,  and  hardware 
controls). Wireless design engineers prefer to work in at most 
two or three dimensions (e.g. to study the effect of power im(cid:173)
balance on system performance) for ease of tunability and de(cid:173)
ployment. The next level of spatial aggregation thus contains 
buckets which aggregate data in terms of two dimensions, us(cid:173)
ing various consistency constraints and design specifications. 
Finally, the third level aggregates buckets into regions of con-

QUALITATIVE  REASONING 

1045 

strained shape; the shape of the regions illustrates the nature 
of joint  influence  of the  two  selected  configuration  parame(cid:173)
ters on performance. Specific region attributes, such as width, 
provide estimates for the thresholds of sensitivity of configu(cid:173)
rations to variations in parameter values. 

The results of such mining are important for both qualita(cid:173)
tive and quantitative analysis. For instance, when the average 
SNRs of the two branches are equal, the BEP is minimal and 
the width of the mined region in Fig.  1  depicts the largest ac(cid:173)
ceptable power imbalance (in this case, approximately 12dB). 
However, the width is not uniform and the region is narrower 
for smaller values of the SNRs.  The qualitative result is  that 
system  designs  situated  in  the  lower  left  corner of the  con(cid:173)
figuration space are more sensitive to power imbalance in the 
two branches. 

Each input data point captures the results of a wireless sys(cid:173)
tem simulation  which takes hours or even days (the simula(cid:173)
tions in Fig. 1 were conducted on a 200-node Beowulf cluster 
of workstations). Thus it is imperative that we focus data col(cid:173)
lection in  only those regions that are most important to sup(cid:173)
port our data mining objective, viz. to qualitatively assess the 
performance  in  configuration  spaces.  This  requires  that  we 
model the functioning of the data mining algorithm, in order 
to optimize sample selection for utility of anticipated results. 
Modeling data mining algorithms in this manner is useful for 
closing the loop, characterizing the effects of the data mining 
algorithm's parameters, and improving our understanding of 
how variations in data fields percolate up through the layers. 
A particularly interesting application is to use such modeled 
structures to design  information-theoretic measures for eval(cid:173)
uating experimental  designs  [MacKay,  1992]  and  for active 
data selection [Cohn et  al,  1996; Denzler and Brown, 2002J. 
In order to address these goals, this paper develops a novel 
Gaussian process approach to modeling algorithms that mine 
spatial aggregates. We first overview the Spatial Aggregation 
mechanism for spatial data mining and the Gaussian process 
approach to Bayesian modeling.  We then show how to inte(cid:173)

grate the two approaches in order to achieve our goal of prob(cid:173)
abilistically modeling spatial data mining algorithms.  We il(cid:173)
lustrate  this  ability  within  the  context of identifying  pockets 
underlying the gradient in  a field â€” an application that cap(cid:173)
tures many of the interesting characteristics of more complex 
studies like the wireless application. 

2  Spatial Aggregation 
The Spatial Aggregation Language (SAL) [Bailey-Kcllogg et 
al,  1996;  Yip  and  Zhao,  1996J,  provides  a  set  of operators 
and data types, parameterized by domain-specific knowledge, 
for  uncovering  and  manipulating  multi-layer  geometric  and 
topological  structures  in  spatially distributed  data.  SAL  ap(cid:173)
plications  construct  increasingly  abstract  descriptions  of the 
input data by utilizing knowledge of physical properties such 
as continuity  and  locality,  expressed  with  the  vocabulary of 
metrics, adjacency relations, and equivalence predicates.  To 
understand the  SAL approach  (see  Fig.  2),  consider a  SAL 
program for analyzing flows  in  a vector field (e.g.  wind  ve(cid:173)
locity or temperature gradient). 

In the first level, the goal is to group input vectors (a) into 
paths  so  that  each  sample  point  has  at  most  one  predeces(cid:173)
sor and at most one successor.  SAL breaks the process  into 
two key steps, one capturing locality in the domain space (i.e. 
sample  location),  and  the  other  capturing  similarity  in  the 
feature space  (i.e.  vector direction).  A  neighborhood graph 
aggregates  objects  with  a  specified  adjacency  predicate  ex(cid:173)
pressing the notion of locality appropriate for a given domain. 
As  shown  (b),  a  sample  point's  neighbors  include  all  other 
points  within  some  specified  radius  r.  Feature  comparison 
then must consider only neighbors in  this graph, thereby ex(cid:173)
ploiting physical knowledge to gain computational efficiency 
while maintaining correctness.  Here we  break feature com(cid:173)
parison  into  a  sequence  of predicates  and  graph  operations. 
In particular, we first filter the graph (c), applying a predicate 
that keeps only those edges whose direction is similar enough 

1046 

QUALITATIVE  REASONING 

(within  some angle tolerance 0) to the directions of the vec(cid:173)
tors at the endpoints.  The remaining graph has some "junc(cid:173)
tion" points where vector direction suggests multiple possible 
neighbors, and the most appropriate path extension from the 
point must be chosen.  A  similarity metric sums the distance 
between the junction and a neighbor, weighted by a constant 
d,  and  the difference  in  vector direction  at the junction and 
the neighbor.  The  most  similar  neighbor for the junction  is 
selected ((d) and (e), for successor and predecessor junctions, 
respectively). 

The remaining graph edges are collected and redescribed 
as more abstract streamline curve objects (f), for the second 
level  of analysis.  Again,  computation  is  localized  so  that 
only  neighboring  streamlines  are  compared.  The  neighbor(cid:173)
hood graph here (not shown) uses an adjacency predicate that 
declares streamlines neighbors if their constituent points were 
in the first level.  It is then straightforward to identify conver(cid:173)
gent  flows  (g) with  an equivalence predicate that tests when 
constituent points form a junction  in  the graph  in  (c).  If de(cid:173)
sired, these flow bundles can be abstracted and analyzed at an 
even higher level. 

SAL's  uniform  spatial  reasoning  mechanism,  instantiated 
with  appropriate  domain  knowledge,  has  proved  success(cid:173)
ful  in  applications  ranging  from  decentralized  control  de(cid:173)
sign iBailey-Kellogg and Zhao, 1999; 2001 J, to weather data 
analysis  [Huang  and  Zhao,  1999],  to  analysis  of diffusion-
reaction morphogenesis [Ordonez and Zhao,  2000].  Recent 
work has focused on optimizing sample selection for applica(cid:173)
tions where data collection is expensive, including identifying 
flows  in  multi-dimensional  gradient  fields  [Bailey-Kellogg 
and Ramakrishnan, 2001 ] and analyzing matrix properties via 
perturbation  sampling  [Ramakrishnan  and  Bailey-Kellogg, 
2002].  This  paper  provides  the  mathematical  foundations 
necessary for the modeling of such SAL programs to support 
the meta-level reasoning tasks outlined in the introduction. 

3  Gaussian Processes 
Gaussian  processes  have  become  popular  in  the  last  few 
years, especially as a unifying framework for studying mul(cid:173)
tivariate  regression  [Rasmussen,  1996],  pattern  classifica(cid:173)
tion  [Williams  and  Barber,  1998],  and  hierarchical  model(cid:173)
ing [Menzefricke, 2000].  The underlying idea can be traced 
back  to  the  geostatistics  technique  called  kriging  [Journel 
and Huijbregts,  1992],  named after the South African miner 
Danie Krige. In kriging, the unknown function to be modeled 
(e.g., ozone concentration) over a (typically) 2D spatial field 
is expressed as the realization of a stochastic process. A prior 
is placed over the function space represented by this stochas(cid:173)
tic process, by suitably selecting a covariance function. Given 
measured  function  values  at  sample  locations,  kriging  then 
proceeds to  estimate  the  parameters of the  covariance  func(cid:173)
tion (and any others pertaining to the random process).  Using 
such values a prediction of the response variable can then be 
made for a new sample point, typically using MAP or ML in(cid:173)
ference.  This basic approach is still popular in many tasks of 
spatial data analysis. 

Even though parameters are estimated in this approach, it 
is important to note that kriging is fundamentally a memory-

based technique, since the estimated parameters only describe 
the  underlying  covariance  function  of  a  stochastic  process. 
Thus,  predictions  of  the  response  variable  for  new  sample 
points  are  conditionally  dependent  on  the  measured  values 
and  their  sample  points;  by  unrolling  the  effect  of  the  pa(cid:173)
rameters of the random process, we can directly express this 
dependency. 

Kriging is often motivated as a local modeling technique, 
capable of approximating or interpolating functions with mul(cid:173)
tiple  local  extrema,  and  generalizes  well  to applications ex(cid:173)
hibiting anisotropics and trends.  The stochastic prior is also 
viewed as a mathematically elegant mechanism to impart any 
available  domain  knowledge to  the  modeling  technique.  In 
1989,  Sacks  et  al.  [Sacks  et  al,  1989]  showed  how  krig(cid:173)
ing can  actually  be used to  model  processes with determin(cid:173)
istic  outcomes,  especially  in  the  context  of computer exper(cid:173)
iments.  The justification  for modeling  a  deterministic  code 
as  a  stochastic  process  is  often  that  even  though  the  re(cid:173)
sponse  variable  is  deterministic,  it  may  'resemble  the  sam(cid:173)
ple  path  of a  suitably  chosen  stochastic  process'  [Sacks  et 
al.,  1989].  Alternatively, using a stochastic process prior can 
be  viewed  as  a  Bayesian  approach  to  data  analysis  [Sivia, 
19961,  and this is the idea emphasized by most recent com(cid:173)
puter science  research  in  Gaussian  processes  [Gibbs,  1997; 
Rasmussen,  1996].  The  stochastic  process  can  be  suitably 
formulated to ensure that the model reproduces the same re(cid:173)
sponse value for repeated invocations of a given sample input 
(i.e.,  absence  of random error).  For  instance,  the  Gaussian 
prior can be chosen so that the diagonal entries of the covari(cid:173)
ance matrix are  1, meaning that the model should interpolate 
the data points. 

In the recent past, Gaussian processes have become popu(cid:173)
lar in the statistical pattern recognition community [MacKay, 
1997]  and  graphical  models  literature  [Jordan  (ed.),  1998]. 
Neal established the connection between Gaussian processes 
and  neural  networks  with  an  infinite  number  of  hidden 
units [Neal,  1996].  Such relationships allow us to take tradi(cid:173)
tional learning techniques and re-express them as imposing a 
particular covariance structure on the joint distribution of in(cid:173)
puts.  For instance, we can take a trained neural network and 
mine the covariance structure  implied by the weights (given 
mild  assumptions  such  as  a  Gaussian  prior over the  weight 
space).  Williams motivates the usefulness of such studies and 
describes common covariance functions [Williams,  1998]. 

Williams and Barber [Williams and Barber,  1998] describe 
how the Gaussian process framework can be extended to clas(cid:173)
sification, where the modeled variable is categorical.  Essen(cid:173)
tially, the idea is to (i) use a logistic function to conduct tradi(cid:173)
tional Gaussian regression modeling, and (ii) adopt a softmax 
function to bin the logistic output into a given set of classes. 
This means that the logistic function uses a "latent variable" 
as input in its computation, since its values are not provided 
by the dataset. 

4  Gaussian Processes for Spatial Aggregation 
SAL programs construct multi-layer spatial aggregates based 
on specified local adjacency relations, similarity metrics, and 
consistency  checks.  We  describe  here  how  to  capture  the 

QUALITATIVE  REASONING 

1047 

determine the extent and stringency of this neighborhood re(cid:173)
lation â€” one of the defining parameters  of a  SAL  program. 
Specifically, we posit a process such as: 

The  idea  then  is  to  estimate  a  model 
form  as  /,  on  the  basis  of  a  given  set  of 

(1) 
of  the  same 
observations 
A  typical  choice  for  Z  in 

is a random process with zero mean and covariance 

is  the  estimated  variance and 

where scalar 
is a matrix 
that captures the correlation between the inputs (i.e., the given 
locations).  Notice that even though the  input is one dimen(cid:173)
sion,  the  size  of  R  depends  on  the  number of locations  for 
which gradient measurements are available. The above model 
for  also includes the constant term 
this can be estimated 
based on the k observations, or we can substitute more com(cid:173)
plex terms (e.g. linear), or even omit it altogether. 

The  functional  form  of R  (including  its  parameterization) 
in effect defines the stochastic process and must be carefully 
chosen to reflect the underlying data's fidelity or any domain-
specific assumptions about local variation. The parameters of 
the process are then  estimated  using  multidimensional  opti(cid:173)
mization involving a suitable objective function. For instance, 
given the following form for /?: 

(2) 
the  problem  reduces  to  estimating 
from  the  given  data. 
Notice  that  this  formulation  for  R  implicitly  enforces  that 
the  model  exactly  interpolate  the  given  data  points,  since 
A  common objective  function  for estimating 

is to minimize the mean squared error (MSE), 

between  and 
solution to the optimization problem: 

. The p that minimizes MSE is given by the 

(3) 

where R is the symmetric correlation matrix formed from 
For a new sample point 
variable is given by: 

a prediction for the regressed 

(4) 

where r is the correlation vector between the response at 
and all the other  points (derived from /?), 
vector of dimension 

and Â« is the estimate of a given by: 

is the identity 

The variance in the estimate is given by: 

(5) 

(6) 

In  this case,  the optimization  is  one-dimensional  due to the 
presence of the  single  parameter  With  a different param(cid:173)
eterization,  we  will  employ  multi-dimensional  optimization 
over the  entire  set  of hyperparameters.  When  dimensional(cid:173)
ity is large, the hyperparameters are estimated using MCMC 
methods. Once such a modeling is complete, as discussed in 
the previous section, we can relate a categorical class variable 

Figure 3:  Modeling the reversal of gradients in a  ID field us(cid:173)
ing Gaussian processes,  (top) Original field,  (bottom) Given 
measured  values  of  gradient  vector  angles  at  specific  data 
points (blue),  the model posits that the conditional  distribu(cid:173)
tion of the angle at unseen data points is a Gaussian (shown 
in red). 

qualitative behaviors of such aggregates using Gaussian pro(cid:173)
cesses.  The essence of a Gaussian process is  its covariance 
structure,  so  we  focus on determining covariance structures 
in  a SAL program.  For example, in the two-layer SAL pro(cid:173)
gram of Sec.  2,  the parameters (r, 0, d)  impose a covariance 
structure by specifying the reach of the neighborhood graph, 
enforcing the similarity of angles in  the vector field, and pe(cid:173)
nalizing for the distance at decisions involving junctions. 

4.1  Covariance Structure 
We now describe how to model the covariance structure of a 
given  SAL  program.  We  give  the  mathematical  framework 
for the case of mining a 1D field to determine if there is a re(cid:173)
versal of gradient as we move along the spatial dimension, but 
essentially the same machinery applies to two and higher di(cid:173)
mensional spaces.  The basic problem is one of classifying  ID 
points to determine the qualitative structure of same-direction 
flows. Fig. 3 (top) depicts the given input field along the x di(cid:173)
mension.  As is shown, the field consists of unit vectors with 
different orientation.  The Gaussian process approach is first 
to model an underlying regressed variable and then to use a 
logistic or softmax function to bin the output into classes.  In 
our application, the regressed variable represents the gradient 
and can be simply summarized as the angle of unit vector ori(cid:173)
entation y in Fig. 3 (top). In other applications, the regressed 
variable could be an unobserved  'latent'  variable.  In either 
case, it is modeled as a function / of the input x. 

First, assume / to be a Gaussian process on x, meaning that 
the conditional probability distribution of y given a value of x 
is a Gaussian. For instance, Fig. 3 (bottom) depicts measured 
values of y  superposed with distributions of y at two unseen 
points.  A covariance structure among the y values could, for 
example, capture the intuition that adjacent values of y should 
agree  more  than  distant values.  The  goal  of modeling is  to 

1048 

QUALITATIVE  REASONING 

to y using softmax functions. For instance, the reversal of the 
gradient in Fig. 3 can be captured by first using the Gaussian 
process model to make predictions of the gradient at untested 
points  and  then  determining  if (and  where)  a  zero  crossing 
occurs. 

The above equations extend naturally to a 2D case such as 
that described in Sec. 2.  The covariance prior has to be suit(cid:173)
ably parameterized and we also have the option of taking into 
account  any  interactions  between  the  two  dimensions  (both 
linear and nonlinear). 

4.2  Modeling  M a ny  Layers 
When SAL programs consist of many layers, we need to de(cid:173)
velop  a  sequence  of Gaussian  process  models,  each  with  a 
suitable covariance function,  which can  then be superposed 
to  yield  a composite covariance function.  Recall  that  while 
one could simply assess the covariance of the output field for 
sample values of the parameters and a given  input  field,  the 
real purpose of a Gaussian process model is to express the co-
variance  of the  output  as  a function  of the  characteristics  of 
the input. This is the key property that allows reasoning about 
closing the loop and selecting optimal  samples.  In addition, 
Gaussian process models help capture the randomness inher(cid:173)
ent in some of SAL's computations, e.g. non-determinism in 
labeling, and variations due to how ties are broken for aggre(cid:173)
gation purposes.  Refer again to Sec. 2 for an example of the 
types of operations that the covariance model must capture. 
At the very bottom of the hierarchy is the input data  field. 
For  applications  characterized  by  expensive  data  collection 
(as in the introduction), it can be advantageous to start with a 
sparse set of sample data.  The Gaussian modeling approach 
to regression is ideal for creating surrogate representations of 
data fields from such a sparse dataset.  That is, given a sparse 
set of samples,  interpolate a dense  field  satisfying those val(cid:173)
ues and incorporating any appropriate domain knowledge, as 
discussed above regarding kriging.  Such surrogate functions 
can then been used as the starting points for qualitative anal(cid:173)
ysis [Bailey-Kellogg and Ramakrishnan, 2001]. 

The operators in a SAL level deal with both locality (which 
object  locations  are close  to  which  other ones,  as  encapsu(cid:173)
lated  in  a neighborhood graph) and similarity  (which object 
features are close to which other ones, as encapsulated in met(cid:173)
rics and predicates).  For instance,  in  the example of Sec.  2, 
two  points  are  assigned  to  the  same  pocket  if they  are  spa(cid:173)
tially proximate and their flows converge.  Here the Gaussian 
process  is  classification  (or  more  generally,  density  estima(cid:173)
tion).  A  popular covariance  structure  for  an  n-dimensional 
input field captures locality: 

(7) 

and 

where the expression relates the function values at positions 
then the covariance function will 
be positive  definite,  satisfying  the  normalization constraints 
of a posteriori  inference. 

To see how to capture similarity, consider when two sam(cid:173)
ple locations are classified into the same trajectory in Sec. 2. 
In addition to being spatially proximate (as inferred by SAL's 

Figure 4:  A 2D pocket function. 

neighborhood calculations), the underlying vector fields must 
also  be  similar  in  direction.  Expressing  the  covariance  in 
terms of position alone can cause the resulting estimated hy-
perparameters  to  be  misleading  or  difficult  to  interpret,  as 
their  effect  is  confounded  with  the  underlying  vector  field. 
One  solution  is  to  artificially  inflate  the  dimensionality,  so 
that position and direction together describe the data. Besides 
increasing the dimensionality, this approach spells trouble for 
estimation  using  MCMC  methods  since  significant  portions 
of the sample  space  will  remain unsampled and  it  would  be 
difficult  to  assess  their effects  on  the  minimized  functional. 
An  alternative solution  is  to use the fact that  the vector field 
is itself a surrogate and add a term to the covariance outside 
the above structure, capturing the contribution due to similar(cid:173)
ity in  the vector  field.  We place a Gamma prior on this term 
with a shape parameter that ensures that its role is secondary 
to  the  covariance  structure  on  position  (directional  similar(cid:173)
ity alone is not enough for high covariance at the output; the 
sample  locations  also  must be  spatially  proximate).  This  is 
recognized in the statistics community as a hierarchical prior 
and described in detail in [Neal,  1997]. 

5  Experimental Results 
In  order to  test  our  approach,  we  studied  de  Boor's  pocket 
function (see Fig. 4): 

at which 
where X is the n-dimensional point 
is evaluated,  I  is the  identity n-vector, 
the pocket function 
is the L2  norm.  This function exploits the fact that 
and 
the volume of a high dimensional cube is concentrated in  its 
corners and p is designed so that it has a "dip" in each corner. 
It embodies many aspects  of datasets  like  those encountered 
in the wireless simulation study, including multiple local ex-
trema, non-systematic variation in the location of the pockets, 
and regional variation.  The pocket function is also important 
as a benchmark for high-dimensional data exploration, where 
the goal is to identify the most interesting regions of the de(cid:173)
sign  space  without necessarily  conducting  a  (costly)  global 
optimization over the entire design space.  Data mining pro(cid:173)
grams are hence required to identify the  most promising re(cid:173)
gions using as few function evaluations as possible. 

QUALITATIVE REASONING 

1049 

Figure 5:  Modeling a SAL program to mine pockets in gradient fields,  (a) Variation in number of pockets mined by the SAL 
program for various  values  of 
Covariance 
contribution in  dimension for various values  of 

In all charts,  varies by group and  varies within group. 

(b) Covariance contribution in 

dimension  for various  values  of 

A  SAL  program  to  identify  the  number  of pockets  starts 
with some samples of the pocket function. The lowest level of 
modeling involves a kriging interpolation over a uniform grid 
(we chose size 
for testing). Then the approach of Sec. 2 is 
applied to the gradient vector field of this scalar field: the sec(cid:173)
ond level bundles points into curves, and the third aggregates 
these into flows. Each convergent flow represents one pocket. 
One could mine the covariance structures for each layer sep(cid:173)
arately; we unfold these mappings here to obtain a single co-
variance structure summarizing  all  three  layers.  This is  be(cid:173)
cause the structure (esp. the contributions of each dimension) 
is easiest to interpret in terms of the original spatial field. 

We conducted a parameter sweep over 

as: 

and used NeaPs Bayesian modeling software fNeal,  1997] to 
construct  Gaussian  process  classifiers  for  the  flow  classes. 
Covariance  contributions  in  the 
terms  (Eq.  7)  from  both 
the dimensions was estimated using hybrid Monte Carlo (ag(cid:173)
gressive schemes to evolve the system state by adding higher 
order terms).  This procedure uses a leapfrog scheme to sup(cid:173)
press  random  walk  behavior  by  selective  iteration  between 
Gibbs sampling scans and latent value updates. 

Our results indicated a strong positive correlation between 
the  and 
covariance contributions, bringing out the sym(cid:173)
metry  in  the  underlying  SAL computations.  The  number of 
pockets mined was constant across the values of d (other pa(cid:173)
rameters fixed), and one of the goals of our study was to de(cid:173)
termine if this negligible effect of d is captured in the covari(cid:173)
ance structures.  (The effect of  would actually be more pro(cid:173)
nounced in other spatial fields but not so much in the pocket 
function  due  to  the  inherent  symmetry.)  Parameters  r  and 
produced the most variation in the covariance contributions 

with 
0.95 causing an abrupt jump in the number of mined 
pockets.  This is due to the rather stringent limit imposed on 
vector similarity  arising  from  the  nonlinearity  of the  cosine 
metric.  Fig. 5  summarizes the results for a 2D pocket func(cid:173)
tion,  where  we  have  averaged  the  covariance contributions 
across all ds, for given  and 

As  the  number  of pockets  increases  (Fig.  5(a)),  the  co-
variance  contributions  increase  (Fig.  5(b,c))  approximately 

quadratically.  In other words,  as the underlying latent func(cid:173)
tion  varies  rapidly  along  the  given  dimensions,  we  cannot 
stray  "too far"  away  from  a given  sample point when  mak(cid:173)
ing  predictions  at  test  points.  The reciprocal  of the  covari(cid:173)
ance scale  term  is  often referred  to  the  characteristic length 
of a dimension.  This gives an estimate of "how far" a given 
dimension's effect holds.  When only four pockets are mined, 
the characteristic length is about 1, meaning pockets occupy a 
width  of 
(exactly one fourth of the total space).  As more 
pockets  are  mined,  the  characteristic  length  drops  to  about 
0.4.  It is  also  interesting to note that the  abrupt jump in  the 
number of pockets  for  =  0.95  is reflected  by a similar in(cid:173)
crease in the covariance contributions for this value.  Essen(cid:173)
tially, vector and edge directions have to be so similar that few 
long "runs" can be aggregated as streamlines. This brings out 
the capability of the Gaussian process approach to capture the 
essentials of a spatial aggregation algorithm. 

6  Discussion 
This  paper has  demonstrated  a  novel  Gaussian  process  ap(cid:173)
proach  to  modeling  the  qualitative  behavior  of  SAL  pro(cid:173)
grams;  in  contrast  to  much  of  the  literature  where  Gaus(cid:173)
sian  processes are used for pattern classification and regres(cid:173)
sion [Rasmussen, 1996; Gibbs, 1997], our work takes existing 
data mining algorithms and recasts them in terms of Gaussian 
priors.  To  the  best  of our knowledge,  this  is  the  first  study 
to  completely  model  a  qualitative  data  mining  algorithm  in 
terms of a process framework,  summarizing the transforma(cid:173)
tion  from data to higher-level aggregates.  This is an impor(cid:173)
tant step in  firmly  establishing a probabilistic basis for spatial 
aggregation  computations.  The  modeling  undertaken  here, 
while expensive, is justifiable for studies such as the wireless 
system characterization described in the introduction. 

There are several immediate gains from the work presented 
here; due to space limitations we only mention them briefly. 
First,  the  Gaussian  process  model  can  characterize  experi(cid:173)
mental design criteria such as entropy as a functional w.r.t. the 
input space,  allowing us to  use the mined covariance struc(cid:173)
ture  to  focus  sampling  at  the  most  informative points  (e.g., 
see  LBailey-Kellogg and Ramakrishnan, 2001]).  It is impor(cid:173)
tant  to  note,  however,  that  the  approach  taken  in  [Bailey-
Kellogg and Ramakrishnan, 2001] only addresses the lowest 

1050 

QUALITATIVE  REASONING 

levels  of a  hierarchy  and  is  unable  to  reason  about  higher-
level,  more abstract processes  of redescription and aggrega(cid:173)
tion  as  is  done  here.  Second,  Gaussian  process  models  al(cid:173)
low us to study the effects of different SAL parameters for a 
given  class  of datasets,  e.g.  the  inference  above of the  neg(cid:173)
ligible  role  of d  in  the  mining  process.  Finally,  it  allows  us 
to take algorithms that function  in differing ways  (and using 
different  sets  of parameters)  and  places  them  on  a common 
footing,  namely  the  language of covariance structures.  This 
means that we can  reason  about the applicability of different 
algorithms by studying the constraints they impose on spatial 
locality and field similarity. 

Gaussian  processes  have  recently  been  linked  to  kernel-
based methods, as used in support vector machines [Cristian-
ini  and Shawe-Taylor, 2000];  we intend to explore this con(cid:173)
nection  in  future work.  Kernel-based methods  are attractive 
in  their promise  to  overcome the  curse of dimensionality  by 
the use of nonlinear projections, a facet that is of critical  im(cid:173)
portance for mining data from large parameter sweeps.  As the 
need  for data mining  in  computational  science  gains  promi(cid:173)
nence, process models will be crucial to achieve effective uti(cid:173)
lization  of data for mining purposes. 

Acknowledgements 
The authors thank Feng Zhao and  Layne  Watson  for helpful 
comments.  This  work  is  supported by US  NSF grants  EIA-
9974956, EIA-9984317, and EIA-0103660. 

References 
[Bailey-Kellogg and Ramakrishnan, 2001]  C.  Bailey-Kellogg  and 
N. Ramakrishnan.  Ambiguity-Directed Sampling for Qualitative 
Analysis of Sparse Data from Spatially Distributed Physical Sys(cid:173)
tems. In Proceedings of the Seventeenth International Joint Con(cid:173)
ference  on  Artificial Intelligence  (IJCAl'OJ),  pages  43-50,  2001. 
[Bailey-Kellogg and Zhao,  1999]  C.  Bailey-Kellogg  and  F.  Zhao. 
In  Proceedings  of 
Influence-Based  Model  Decomposition. 
the  Sixteenth  National  Conference  on  Artificial  Intelligence 
(AAAI'99),  pages 402-409,  1999. 

[Bailey-Kellogg and Zhao, 2001]  C.  Bailey-Kellogg  and  F.  Zhao. 
Influence-Based Model Decomposition for Reasoning about Spa(cid:173)
tially  Distributed  Physical  Systems.  Artificial  Intelligence,  Vol. 
130(2):pagcs 125-166, 2001. 

[Bailey-Kellogg etal,  1996]  C.  Bailey-Kellogg,  F.  Zhao,  and 
K.  Yip.  Spatial  Aggregation:  Language  and  Applications. 
In 
Proceedings  of the  Thirteenth  National  Conference  on  Artificial 
Intelligence  (AAAI'96),  pages  517-522,  1996. 

[Connera/.,  1996]  D.A.  Cohn,  Z.  Ghahramani,  and  M.I.  Jordan. 
Active  Learning  with  Statistical  Models.  Journal  of  Artificial  In(cid:173)
telligence Research, Vol. 4:pages  129-145,  1996. 

[Cristianini and Shawe-Taylor, 2000]  N.  Cristianini  and  J.  Shawe-
Taylor.  An  Introduction  to  Support  Vector Machines  and  Other 
Kernel-Based  Learning  Methods.  Cambridge  University  Press, 
2000. 

[Dcnzlcr and Brown, 2002]  J.  Denzler and C M.  Brown.  Informa(cid:173)
tion Theoretic Sensor Data Selection for Active Object Recogni(cid:173)
tion and State Estimation. IEEE Transactions on Pattern Analysis 
and Machine Intelligence, Vol. 24(2):pages  145-157, Feb 2002. 

[Gibbs,  1997]  M.N.  Gibbs.  Bayesian  Gaussian  Processes for Re(cid:173)
gression  and  Classification.  PhD  thesis,  University  of  Cam(cid:173)
bridge, 1997. 

[Huang and Zhao,  1999]  X.  Huang  and  F.  Zhao.  Relation-Based 
Aggregation:  Finding Objects in Large Spatial Datasets.  In Pro(cid:173)
ceedings  of the  3rd International  Symposium  on  Intelligent  Data 
Analysis,  1999. 

[Jordan (cd.),  1998]  M.I. Jordan (ed.).  Learning in Graphical Mod(cid:173)

els.  MIT Press,  1998. 

[Journel and Huijbrcgts,  1992]  A.G.  Journel  and  C.J.  Huijbregts. 

Mining Geostatistics.  Academic  Press, New York,  1992. 

[MacKay,  1992]  D.J. MacKay.  Information-Based Objective Func(cid:173)
tions  for  Active  Data  Selection.  Neural  Computation,  Vol. 
4(4):pages 590-604,  1992. 

[MacKay,  1997]  D.J.  MacKay.  Gaussian  Processes:  A  Replace(cid:173)
ment for Supervised Neural Networks? In Lecture Notes of Tuto(cid:173)
rial at Neural Information  Processing  Systems  (NIPS'97),  1997. 
Hierarchical  Modeling 
with  Gaussian  Processes.  Communications  in  Statistics,  Vol. 
29(4):pages  1089-1108, 2000. 

iMenzefricke, 2000]  U.  Menzefricke. 

[Ncal,  1996]  R.M.  Neal.  Bayesian  Learning for Neural Networks. 
Springer-Verlag, NY,  1996.  Lecture Notes in Statistics No.  118. 
[Neal,  1997]  R.M.  Neal.  Monte  Carlo  Implementations  of Gaus(cid:173)
sian Process Models for Bayesian Regression and Classification. 
Technical  Report  9702,  Department  of Statistics,  University  of 
Toronto, Jan 1997. 

[Ordonez and Zhao, 2000]  I.  Ordonez  and  F.  Zhao. 

STA: 
Spatio-Temporal  Aggregation  with  Applications  to  Analysis  of 
In  Proceedings  of the  Seven(cid:173)
Diffusion-Reaction  Phenomena. 
teenth  National  Conference  on  Artificial  Intelligence  (AAAI'00), 
pages 517-523, 2000. 

[Ramakrishnan and Bailey-Kellogg, 2002]  N.  Ramakrishnan  and 
C.  Bailey-Kellogg.  Sampling  Strategies  for  Mining  in  Data-
Scarce Domains. IEEE/A IP Computing in Science and Engineer(cid:173)
ing, Vol. 4(4):pages 31-43, July/Aug 2002. 

[Rasmussen,  1996]  C.E. Rasmussen.  Evaluation of Gaussian Pro(cid:173)
cesses and other Methods for Non-Linear Regression. PhD thesis, 
University of Toronto,  1996. 

[Sacks et al,  1989]  J.  Sacks,  W.J.  Welch,  T.J.  Mitchell,  and  H.P. 
Wynn.  Design and Analysis of Computer Experiments.  Statisti(cid:173)
cal Science, Vol. 4(4):pages 409-435,  1989. 

[Sivia,  1996]  D.S.  Sivia.  Data Analysis:  A  Bayesian  Tutorial.  Ox(cid:173)

ford University Press,  1996. 

[Verstak etal., 2002]  A.  Verstak,  N.  Ramakrishnan,  K.K.  Bae, 
W.H.  Tranter,  L.T  Watson,  J.  He,  C.A.  Shaffer,  and T.S.  Rap-
paport.  Using  Hierarchical  Data  Mining  to  Characterize  Per(cid:173)
formance  of Wireless System  Configurations.  Technical  Report 
cs.CE/0208040, Computing Research Repository, Aug 2002. 

[Williams and Barber,  1998]  C.K.I.  Williams  and  D.  Barber. 
Bayesian  Classification  with  Gaussian  Processes.  IEEE Trans(cid:173)
actions  on  Pattern  Analysis  and  Machine  Intelligence,  Vol. 
20(12):pages  1342-1351, Dec 1998. 

[Williams,  1998]  C.K.I.  Williams.  Prediction  with  Gaussian  Pro(cid:173)
cesses:  From  Linear  Regression  to  Linear  Prediction  and  Be(cid:173)
In  M.I.  Jordan,  editor,  Learning  in  Graphical  Models, 
yond. 
pages 599-621. MIT Press, Cambridge, MA, 1998. 

[Yip and Zhao,  1996]  K.M.  Yip and F Zhao.  Spatial Aggregation: 
Theory  and  Applications.  Journal  of Artificial  Intelligence  Re(cid:173)
search, Vol. 5:pages  1-26,  1996. 

QUALITATIVE  REASONING 

1051 


Mining Complex Patterns across Sequences with Gap Requirements∗

1Dept. of Computer Science & Eng., Florida Atlantic University, Boca Raton, FL 33431, USA 

Xingquan Zhu1, 3    and   Xindong Wu2

2Dept. of Computer Science, University of Vermont, Burlington, VT 05405, USA 

3Graduate University, Chinese Academy of Sciences, Beijing 100080, China 

xqzhu@cse.fau.edu; xwu@cs.uvm.edu

Abstract 

The  recurring  appearance  of  sequential  patterns, 
when confined by the predefined gap requirements, 
often implies strong temporal correlations or trends 
among pattern elements. In this paper, we study the 
problem of mining a set of gap constrained sequen-
tial patterns across multiple sequences. Given a set 
of sequences S1, S2,., SK constituting a single hyper-
sequence S, we aim to find recurring patterns in S,
say P, which may cross multiple sequences with all 
their matching characters in S bounded by the user 
specified gap constraints. Because of the combina-
torial  candidate  explosion, 
traditional  Apriori-
based  algorithms  are  computationally  infeasible. 
Our research proposes a new mechanism to ensure 
pattern growing and pruning. When combining the 
pruning  technique  with  our  Gap  Constrained 
Search  (GCS)  and  map-based  support  prediction 
approaches, our method achieves a speed about 40 
times faster than its other peers. 

1  Introduction 
Many real-world applications involve data characterized by 
continuous  sequences  and  streams.  Examples  include  data 
flows in medical ICU (Intensive Care Units), network traffic 
data, stock exchange rates, and DNA and protein sequences. 
Since  real-world  events  rarely  happen  independently  but 
rather associate with each other, to some degree, discover-
ing structures of interest in multiple sequences provides us 
an effective means to explore patterns trivial in a single data 
stream but significant when unifying all observed data into 
one view. For example, the information from multiple data 
streams in ICU (such as the oxygen saturation, chest volume 
and  hear  rate)  may  indicate  or  predicate  the  state  of  a  pa-
tient’s situation, and an intelligent agent with the ability to 
discover  knowledge  from  multiple  sensors  can  automati-
cally  acquire  and  update  its  environment  models  [Oats  & 
Cohen 96]. In microbiology, it is now well known that the 
genomes of most plants and animals contain a large quantity 

                                                

∗ This material is based upon work supported by the US Na-
tional Science Foundation under Grant No. 0514819. The support 
of NSF of China under Grant No. 60674109 is also acknowledged. 

of  repetitive  DNA  fragments.  Examples  include  recurring 
short base pairs (BP) in protein coding DNA and repetitive 
DNA/RNA motifs in genomes, where recurring patterns of 
different  lengths  and  types  are  commonly  found,  at  both 
genomic  and  proteomic  levels, to  have  significant  biologi-
cal/medical values. For example, the 10-11 BP periodicities 
in  complete  genomes  reflect  protein  structure  and  DNA 
folding  [Herzel  et  al.  99]  and  some tandem  repeats  are now 
discovered to be influential to the bacterial virulence to hu-
man [Belkum et al. 97]. Studying correlations among multi-
ple  gene  sequences,  their  associations  with  environments 
and disease phenotypes, thus provides a means of predicting 
and preventing fatal diseases [Rigoutsos & Floratos 98].  

Although recurring patterns convey important and useful 
knowledge, in reality, they rarely just reproduce and repeat 
themselves, but rather appear with a slight shift in the pat-
tern  letters.  For  example,  the  tandem  repeats  in  DNA  or 
protein sequences often involve a phase shift incurred by the 
insertion or deletion of a short sequence [Belkum et al. 97]. 
A practical solution is to allow the mining or search process 
to bear a certain degree of flexibility. Consider sequences in 
Fig.  1(a),  where  a  pattern  across  three  sequences  repeats 
three times but with each appearance slightly different from 
the  others.  If  we  can  allow  that  each  time  the  pattern  ap-
pears, any two of its successive pattern letters’ appearances 
are within a range, rather than a fixed value, we may then be 
able  to  find the  pattern  in  Fig.  1(a).  The introduction  of a 
variable period (gap) thus provides a flexible way to capture 
interesting patterns hidden in sequences. 

Mining recurring patterns from sequences essentially re-
lies  on  a  counting  mechanism  to  check  patterns’  occur-
rences. This, however, is inherently complicated by the se-
quential  order  of  the  underlying  sequences.  Considering 
sequence 
“AAAGGGTTTTCCCTTTTCCCTTTTCCCC”, 
to  find  the  number  of  complete  occurrences  of  pattern 
P=AGTC (we ignore gap constraints at this stage), there are 
3  ×  3  combinations  for  “A”  and “G”,  and  4×10+4×7+4×4 
combinations  for  “T” and “C”.  So  in total,  there  are  9×84 
occurrences  for  AGTC.  Although  this  number  does  not 
sound  scary,  considering  complete  occurrences,  however, 
brings one of the most difficult challenges to our problem: 
the  deterministic  Apriori  theory  (the  support  of  a  pattern 
cannot  exceed  the  support  of any  of  its  sub-patterns)  does 
not  hold.  Considering  S=“AGCTTT”,  pattern  P=AGCT 

IJCAI-07

2934

appears  three  times  in  S,  but  P’s  subpattern  AGC  appears 
once  only.  Therefore,  traditional  Apriori-based  algorithms 
are computationally infeasible to handle our problem.  

Motivated by the above observations, we propose MCPaS
to  Mine  Complex  Patterns  across  Sequences  with  gap  re-
quirements. We will review related work in the next section, 
and  state  our  research  problem  in  Section  3.  In  Section  4, 
we will study pattern frequencies and propose a determinis-
tic  pruning technique  for  pattern  mining.  In  Section  5,  we 
discuss our unique Gap Constrained Search and Map-based 
Support Prediction processes to accelerate the pattern min-
ing  process.  Based  on the proposed  pruning and  searching 
techniques, we elaborate, in Section 6, the mining algorithm 
details. Comparative studies are reported in Section 7, fol-
lowed by the concluding remarks in Section 8. 

 1     2   3    4    5     6    7    8   9    10    11   12  13  14
A    .   .    .   A    .    .    .    .    A    .    .    .    .
 .    e  g    .    .     .    .   e    g     .     .    e   .    g 
 .    .   3    .    .     .    .    .    3    .    .     .    .    3 

S1    
 S2    
 S3    

(a) 

    1           2           3         4         5         6    . . .
(A . .) (. e .) ( . g 3) (. . .) (A . .) (. . .) . . 

 Shyb    

Figure 1. (a) Patterns across three sequences; (b) The hybrid se-

quence generalized from the sequences in (a) 

(b)

2  Related Work 

Existing  research  in  mining  patterns  from  data  sequences 
can  be  distinguished  into  two  categories.  (1)  Mining  pat-
terns frequently appearing in a certain number of (relatively 
short) sequences with a Boolean count, i.e., whether a pat-
tern occurs in the sequence or not. Traditional pattern min-
ing in market baskets [Srikant & Agrawal 96, Pei et al. 01, 
Zaki 98] and Gene Motif search [Murray et al. 02] fall into 
this  category.  (2)  Mining  recurring  patterns  from  long  se-
quences such as episode mining [Yang et al. 00, Méger & 
Rigotti 04, Das et al. 98] and tandem repeats and base pair 
oscillation detection in DNA sequences [Herzel et al. 99].  

Any  mining  process  will  have  to  rely  on  a  counting 
mechanism to check patterns’ frequency information, from 
which frequent patterns can be found. It is worth noting that 
the  selected  counting  mechanism  crucially  impacts  on  the 
mining approach. For research efforts relying on a Boolean 
count, because a pattern’s appearances are counted for only 
once  w.r.t. each  sequence, the  deterministic  Apriori theory 
well holds, and is therefore commonly adopted in the min-
ing process. On the other hand, when one has to determine a 
pattern’s actual number of occurrences in the sequences, the 
situation  becomes complicated,  simply  because the  mining 
process won’t follow Apriori theory at all. Existing efforts 
in  the  field  have  therefore  proposed  confined  occurrence 
counting, such as minimal occurrences [Mannila et al. 97], 
one-off occurrences [Chen et al. 05], and windowing occur-
rences [Mannila et al. 97]. The objective is to confine pat-
terns’ occurrences such that Apriori theory can apply.  

When  considering  complex  patterns  across  multiple  se-
quences, the work relevant to our problem here comes from 
Oates et al. [96] and Mannila et al. [97]. Both efforts tried 
to search frequent episodes across sequences, as well as the 
prediction rules in the form of x indicating y, where x and y
are  the  frequent  episodes  in  the  sequences.  For  example, 
after event A happens, exactly two time points later event B
happens,  and  then  exactly  three  time  points  later  event  C
happens. Notice that this is a very restrictive constraint, and 
we  are trying  to  find  episodes  like:  after  event  A  happens, 
within  two  time  points  event  B  happen,  and  within  three 
time  points  later  event  C  happens.  This  loose  constraint 
leaves a great flexibility to explore useful patterns.  

The  problem of  complex pattern  mining  across  multiple 
sequences is similar to multi-dimensional sequential pattern 
mining [Pino et al. 01], where a normal practice is to aggre-
gate values from all sequences to form a single sequence, as 
shown in Figure 1(b), so traditional sequential pattern min-
ing methods can apply.  

In  the  domain  of  DNA  or  protein  sequences,  BLAST 
[Altschul 90] is one of the most famous algorithms. Given a 
query  sequence  (pattern),  it  searches  for  a  matching  se-
quence from the databases, while what we are pursuing here 
is on mining the patterns. To find patterns, the TEIRESIAS 
algorithm [Rigoutsos & Floratos 98] is designed for pattern 
discovery  from  biological  sequences  with  the  number  of 
wild-cards that can be  present in  the extracted  patterns  re-
stricted and fixed by the users. Similar approaches such as 
Pratt [Jonassen 97] is also proposed to mine restricted pat-
terns (in terms of the maximum numbers of characters and 
wild-cards in each pattern) from a single sequence. 

3  Problem Statement 
The sequences S1,.., Si, .., SK from which we extract patterns 
are called a hyper-sequence, denoted by S, with each single 
sequence Si called an element sequence. We use S[i] to rep-
resent the characters at the ith time point of S. Without losing 
the  generality,  we  assume  all  element  sequences  share  the 
same alphabet size, denoted by •, and |•| represents the size 
of  •.  For  simplicity,  we assume  that  all element  sequences 
have the same lengths, denoted by L.

A  wild-card  (denoted  by  a  single  dot  “.”)  is  a  special 
symbol  that  matches  any  character  in  •.  A  gap  g  is  a  se-
quence of wild-cards, bounded by the maximal and minimal 
values.  The  size  of  the  gap  refers  to  the  number  of  wild-
Ng to  represent  a  gap  whose  size  is 
M
cards  in  it.  We  use 
within  the  range  [N, M].  We  also call  W=M-N+1,  the  gap 
flexibility. A pattern, P=p1g1p2g2,…,gl-1pl, is a set of charac-
ters  from  different  element  sequences  and  gaps  that  begin 
and  end  with  characters,  where  pj is  an  element  pattern 
which  consists  of  letters  from  element  sequences.  An  ele-
ment pattern should consist of at least one character. gj is the 
gap requirement between element patterns pj-1 and pj. Figure 
2 pictorially shows a pattern P with three element patterns. 
The  number  of  element  patterns  in  P,  denoted  by  |P|,  is 

IJCAI-07

2935

called  the  length  of  P, i.e.,  the  wild-card  symbols  are  not 
counted towards a pattern’s length.  

The problem of mining complex patterns across multiple 

l

l

1

−

 

 

p

g

p

=

M
N

P

M
N

M
N

...

gp
1

gp
2

sequences is to find patterns of the following form: 
 

(1)
which means that gaps between any two successive element 
Ng .  An  occur-
patterns  are  the  same,  i.e., g1=g2=…=g1= M
rence of a pattern P in S is defined as a match between the 
characters  of  the  element  patterns  and  the  element  se-
quences,  under  the  gap  constraints.  The  occurrences  are 
considered different, as long as the locations of one pair of 
the matched characters are different. For example, we con-
0g , i.e.,
2
sider P=ATG appears 2 times in S=”ATTG”, w.r.t. 
S[1]S[2]S[4] and S[1]S[3]S[4]. The support of P in S (de-
noted by sup(P)) is the number of times P occurring in S.

Given a length-l pattern P and a length-L hybrid-sequence 
S, we first calculate the total number of possible occurrences 
of  a  length-l  pattern  in  S, Ll, then  we  count  the  actual  ap-
pearances of P, sup(P). We consider P a frequent pattern, iff 
sup(P)/Ll is larger than the user-specified threshold value  .

A
.
 .  

 . 
 e   
 .   

 . 
 g  
 3 

P = 

pgpgp
1

1
0

1
0

2

 

3

 

p1 =                      p2 =                  p3=

A    .    .    
.    e   g     
.    .    3    

        

P =   

Pattern letters corresponding to element sequence S1
…. 
Pattern letters corresponding to element sequence S3

p1

0g
1

p2

Figure 2. A pattern denotation 

4  Pattern Frequency & Deterministic Pruning 
4.1  Pattern Frequency  
Given  pattern  P=p1p2…pl and  its  support  in  a length-L  hy-
per-sequence  S (with  L  >>  l),  sup(P),  to  assess  P’s  fre-
quency,  we  need to  find  Ll, the possible  number  of  occur-
Ng , each 
rences of P in S. Considering pattern P with gap  M
time P appears in S, its actual spans in S vary from l+(l-1)N
to l+(l-1)M, which correspond to the cases that whenever P
appears in  S,  the gap  between any two  successive  element 
patterns  exactly  equal  to  N  and  M  respectively.  Assuming 
the first element pattern p1 matches S at S[δ], then for ele-
ment pattern p2, its valid occurrences may possibly appear in 
the range from S[δ+N] to S[δ+M], i.e., with W=M-N+1 pos-
sibilities. The same situation holds for all other element pat-
terns p3,…pl. So in total, a length-l pattern P starting at S[δ]
may have Wl-1 possible occurrences in S. Assuming p1 has   
possible appearances in S, the total number of possible ap-
pearances of P in S is Ll=  ·Wl-1. Because the average span 
between successive element patterns is (N+M)/2+1, the av-
erage span of P in S is (l-1)·((N+M)/2+1). Then, the possi-
ble  number  of  occurrences  of  p1  equals 
to  [L-(l-
1)·((N+M)/2+1)], where [♦] means the maximal integer no 
larger than ♦. So the value of Ll is defined by Eq. (2).  

L

l

=

[

L

−

(

l

−

)(1

M

N

+

)]1

⋅

W

−

1

l

     

(2) 

+
2

Eq (2) holds only if (l+(l-1) ·M) • L, i.e., the maximal span 
of the pattern is less than the length of the hyper-sequence.  

4.2  Deterministic Pruning 
In this subsection, we derive one theorem and two lemmas 
for the deterministic pruning of our mining process. 
THEOREM 1. Gien a length-l pattern P and its length l-3 
subpatterns Q, we have the supports of Q and P satisfy the 
inequality 
Proof: Because Q is a length l-3 subpattern of P, denoting 
Q by q1q2…ql-3, there are four possible relationships between 
them: (1) P=p1Qpl-1pl; (2) P=p1p2Qpl; (3) P=Qpl-2pl-1pl; and (4) 
P=p1p2p3Q. Let’s first prove that Theorem 1 is true for (1). 
The same proof applies to all other possibilities.  

3 WWQ

sup(

sup(

⋅
()

2)

P

+

≤

)

2

Assuming N=0 and the gap flexibility is W, the first ele-
ment pattern of Q, q1, appears at time slot S[ ]. It is easy to 
know that p1 has W possibilities to appear between S[ -W]
and S[ -1]. So the maximal support of p1Q is sup(p1Q)=W·
sup(Q). Now assuming further that the last element pattern 
of Q, ql-3, appears at S[ + ], it is clear that pl-1 has W possi-
bilities  to  appear  at  the  range  between  S[ + +1]  and 
S[ + +W],  as  shown  in  Figure  3.  If  pl-1  indeed  appears  at 
S[ + +W], the element pattern p1 will have W possibilities 
to appear between S[ + +W+1] and S[ + +2W]. Denoting 
,  if  pl-1  appears  at 
this  region  by 
S[ + +W-1], we know that pl may possibly appear between 
S[ + +W]  and  S[ + +2W-1].  Notice  that  S[ + +W]  has 
been  reserved  for  the  possible  appearance  of  pl-1,  so  the 
number  of  possible  appearances  of  pl  (w.r.t.  to  pl-1  at 
S[ + +W-1]) is W-1, unless pl-1 and pl are the same, which 
is not a generic case in reality. Similarly, the number of pos-
sible appearances of pl (w.r.t. to pl-1 at S[ + +W-2]) is W-2. 
As  a  result,  for  all  possible W  appearances  of  pl-1  between 
S[ + +1] and S[ + +W], the sum of their possible match-
1−lϕ =W+W-1+W-2…+0=W(W+1)/2.
ing pl’s occurrences is 

βαϕ
+++⊂−
W
1
l

βα
++

W
]2

,1

[

1

So the maximal support for length-l pattern P=p1Qpl-1pl is 
• 

sup(P) 

that 

is, 

, 

2

W·sup(Q)·
WQ
sup(
(
+1… +

)

⋅

+

sup(

1−lϕ =
+
3 W
+ +1   + +2… + +W    + +W+1

3 WWQ
2)

⋅
()
.

2)

2

+ +W+2 …   + +W+W …
 x    x    …  x        x             x        …  x           x              …    x                  x              …. 

Pattern Q

W

pl-1

W

pl

Figure 3. Pattern growing  

LEMMA 1. Given a threshold  , we say that a length-l pat-
tern P is frequent iff sup(P)/Ll • 
. If P’s any length l-3 sub-
,
pattern Q’s frequency, Freq(Q), is less than 
ρ

ω ⋅
+
)1
ω
+
)1
where ω=(N+M)/2, then P cannot be a frequent pattern.  
Proof: 

⋅
()1
⋅
()4

l
(
l
(

L
L

−
−

−
−

IJCAI-07

2936

l

3

−

(

)

≤

Q

ρ

l
(
l
(

,  since 

we 
()4(

−
−
−
−
QSup
(

⋅+⋅−−=
lL
(
))1

Freq(Q)=Sup(Q)/Ll-3, 
L
ω ⋅
⋅
+
)1
()1
−
l
3
ω
⋅
+
)1
()4
<
−
L
)
(

Because 
Sup
L
L
L
We have 
Because  Q  is  a  length  l-3  subpattern  of  P,  according  to 
Theorem 1, we know that 
ω
+
So 

3 WWQ
+

≤
sup(
ρ
⋅
W
(

know 
W
l
,

⋅
()
W

⋅−
()1

ω

2)

ω
+

lW

2)

))1

)
⋅

ρ

+

L

−
4

L

−

<

−

−4

(

)

(

⋅

⋅

2

2

3

Sup
=

(

L

P
−

(
−

(

L

⋅
()1

L
(
ω
+

⋅
()1
⋅

l

4

−

P

Sup
(
⋅
W
))1
⋅+
W
1
⋅
W
2

ρ

−

l

1

))1

W
Sup
P
(
)
ω
⋅−
+
()1

+
W
W
2
+
2)1
. Therefore, P is not frequent.          

Because  the  gap  flexibility  W  •1,  we  know 
i.e.,

Sup
(
L
l

−

(

L

(

L

ρ<

ρ
⋅<

W
(

))1

W

P

=

1

−
1

)

⋅

l

Sup

(

)
lLP

      (3) 

W

1

,

≤
 

     

⋅
⋅

2

L
L

−
−

−
−

l
(
l
(

W
(

)1
)4

⋅−
)1

WL

ω ⋅
+
)1
(
ω
+
(
)1

LEMMA 2: If the average span of the longest pattern in S is 
less than 
, i.e., about a half of a length-L hyper-
sequence  S,  given  a  length-l  pattern  P,  for  any  length  l-3 
,
subpattern of P, Q, if Freq(Q) is less than
ρ
where  ω=(N+M)/2,  then  patterns,  with  P  as  their  subpat-
terns, are not frequent. 
Justification: 
According to Lemma 1, we know that given the conditions 
in  Lemm2,  a length-l  pattern  P  will  not  be  frequent.  Now 
assume pattern P is a subpattern of a length-l+k pattern F.
According  to  Eq.  (2),  we  know 
,
W
for  any  length-l+k  pattern  F,  with  P  as  its  subpattern,  the 
maximal support of F is less than Wk times of Sup(P). 
So 

. The frequency of F is 

WL
l

⋅< ρ

2)1

Sup

P

+

(

)

(

⋅

k

Sup
(

<

k

)

P
SupWF
)(
F

)

Freq

(

F

)

=

Sup
L

Freq
(

F

)

⋅<
ρ

⋅+
W
1
W
2

(

(

⋅<
ρ

⋅+
W
1
W
2
+
W
1
W
2
+
k
l
+
⋅−−
ω
lL
(
))1
(
()1
⋅−+−
+
ω
klL
(
()1

))1

ρ

<

⋅

⋅

WL
l
L
L

⋅

+

l

k

l

⋅

W

k

,  where 

(

l

⋅−+
()1

k

ω
+

  is  the 

)1

average span of the length-l+k pattern. Given that the long-
est pattern is less than 
⋅
ρ

)1
1

<

⋅

⋅

Freq

(

F

)

W
(
W
2
W
+

−
+
W

1

L

⋅

WL
−
(
−
(
−
l

2
L
L
(

(

, we have 
ω
+
⋅
l
))1
−
W
2)1
ω
+
)1
<

(
)1
⋅
WL
−
(
)1
L

−
(
⋅

)
ρ

=

ρ

⋅

W
2

+
W

1

⋅

2
W

So pattern F is not frequent. 

2

⋅−
)1

WL

In  reality,  we  may  not  know  in  apriori  that  whether  the 
average  span  of  the  longest  pattern  in  S  is  less  than 
 or not (since the longest patterns are yet to be 
W
(
found), so Lemma 2 does not seem to be useful in the min-
ing  process.  Nevertheless,  because  we  are  dealing  with  a 
long hyper-sequence S, it is almost certain that the average 
span (even the maximal span) of the longest pattern in S is 
less than a half of |S|. For the DNA sequences we are using 
(in Section 7), the average span of the longest pattern is less 
than 10 percent of the sampled length-1000 sequence S. So 
we can safely assume that this prerequisite always holds. 

5  Pattern Search with Gap Requirements 
Consider  a  length-l  pattern  P  with  a  gap  flexibility  W,  an 
exhaustive search will start from the first pattern letter p1 to 
find its first match in S. Denoting this matching location by 
x, the search process then starts from  x to match p2 within 
the  range  [x+1, x+W].  Such  a  process  iteratively  repeats 
until  all  possible  locations  starting  from  x  have  been 
checked,  then  it  moves  one  step  forward  (x+1).  The  time 
complexity  is  O(L•Wl-1),  which  is linear w.r.t. L,  but  expo-
nential w.r.t. W and l. In the case that S consists of K ele-
ment sequences, this complexity increases to O(K•L•Wl-1).

 

 

5.1  Gap Constrained Search (GCS) 
 
In  this  subsection,  we  propose  a  Dynamic  Programming 
[Bellman  57]  oriented  search  mechanism,  which  is able  to 
achieve a linear time complexity in gap constrained pattern 
search.  The  algorithm  consists  of  three  steps.  Given  a 
length-l pattern P= p1p2 …pl, we first build a length L list for 
lPO . We initiate 
each of the element patterns pl, denoted by 
lPO   to  0  before  the  search  process  (For  easy 
the  value  of 
understanding,  we  pictorially  show  a  simple  example  in 
Figure 4 with P=AGTC and gap 
1.  GCS sequentially scans S from the left to right. For any 
current position x, if S[x] matches the first element pat-
tern p1, set the value of 

0g ). 
2

1PO  [x] to 1.  

j

j

−

)

1

p

p

x

v

=

−

O

O

−
Wx

[1

,1max(
=
v

jPO  to 

2.  At any location x, if S[x] matches any element pattern 
pj, j>l (i.e., excluding the first element pattern), we up-
]
.
date the value of 
As  shown  in  Figure  4,  when  x=3,  S[3]  matches  p2
(which is G), then the value of OG[3] is updated to the 
sum  of  OA[1]  and  OA[2].  The  above  process  indicates 
that for any matches of the element pattern letter, pj j>1, 
at location x, we backtrack W steps to find the number 
of  times  pj’s  last  successive  pattern  letter  pj-1  has  ever 
appeared.  If all element  patterns  (pj, j>1)  were able  to 
jPO in such a 
iteratively regulate and update their lists 
jPO [x] will indicate the number 

way, then the value in 
of times that pattern p1p2…pj ever ends at position x.

3.  We iteratively repeat the above process, until we finish 
the  whole  sequence  S.  Then  we  sum  up  all  elements 
lPO , which is the number of times P=p1p2…pl appears 
in
lPO [x] will indicate the 
in S. In addition, the values in 
number of times P ending at position x.

The time complexity of GCS consists of two parts: (1) scan-
ning the whole sequence L, and (2) at each location x, com-
paring  S[x]  with  all  element  patterns,  and  backtracking  W
steps  if  necessary.  Because  each  backtracking  can  be 
achieved  through  a  sum  operation,  so  the  total  time  com-
plexity is O(KL(l-1)), which is linear w.r.t. L, l, and K, and 
much more efficient than the exhaustive search O(KLWl-1).  

IJCAI-07

2937

out rescanning S. For all generated length-l candidates in Cl,
we calculate a value l′ =l+3 and a threshold,  
 
ρ

=′

ρ

(5) 

 

 

 

 

⋅

L
L

−′−
l
(
−′−
l
(

⋅
()1
⋅
()4

ω
+
ω
+

)1
)1

All length-l candidates with their frequencies larger than ρ
are forwarded to a frequent set Fl. If any length-l candidate’s 
frequency is less than ρ′, we mark it as “suspicious”, which 
means  that  this  pattern  is  unlikely  to  grow  further,  so  we 
will  keep an eye  on  it.  Meanwhile,  for  any  length-l  candi-
date P in Cl, if any of P’s length-l-2 subpattern is suspicious,
we will remove P from the candidate set Cl (on lines 11 to 
12 in Figure 6). According to Lemmas 1 and 2, if a pattern 
Q  is  suspicious,  then  any length-l  patterns  with  Q  as  their 
length l-3 patterns are not going to be frequent. Therefore, if 
P’s length-l-2 subpattern is suspicious, then any length l+1 
(and  beyond)  patterns  containing  such  subpatterns  are  not 
going to be frequent. So there is no need to put them into the 
candidate  set  Cl  for  growing.  As  a  result,  we  may  safely 
remove P from the candidate set Cl.

After MCPaS prunes out candidates from Cl, it builds RM
for all remaining length-l patterns in Cl by rescanning S (on 
line 13 in Figure 6). MCPaS grows length-l+1 candidates by 
using all patterns in Cl (on line 5 in Figure 6). This can be 
achieved  through  the  following  two  techniques:  (1)  trying 
all combinations by attaching any possible element pattern 
to the patterns in Cl, or (2) using the popular Apriori candi-
date generation procedure.  

Input: (1) Hyper-sequence S and gap  M
sequences K; (3) alphabet •; and (4) frequency threshold 
Output: Frequent pattern set  

Ng , (2) # of element 

1.  W M-N+1 
2.  Build length-2 pattern set C2, build BM and HM maps 

for all patterns in C2.
l

 3 

3. 
4.  While (Cl-1 • φ)
5. 
6. 

PatternGen(Cl-1);  

Cl
Predict support values for all candidates in Cl
(Eq. (4)) 
l′=l+3 AND calculate threshold ρ′
For any pattern y in Cl

If Freq(y) •      Then   Fl
If Freq(y) < ρ′   Then   y
If  any length l-2 subset of y is suspicious

Fl
suspicious

∪ y

                   Then  Cl

Cl \ y

Rescan S and build RM for all patterns in Cl
l

l +1;  

7. 
8. 
9. 
10. 
11. 
12. 
13. 
14. 
15.  Return (F3

1     2      3      4    5      6    7     8    9   10    11   12   13   14   15   16   17   18 
  S    A   A   G   G   C   A   T   C   T   C   A   G    A   T    C   T   T   C 
  A    1    1    0    0    0   1    0    0   0    0    1    0    1   0    0    0   0   0 
  G    0    0    2    2    0   0    0    0   0    0    0    1    0   0    0    0   0    0 
  T    0    0    0    0    0   0    2    0   0    0    0    0    0    1    0   0    0   0  
  C    0    0    0    0    0   0    0    2   0    2    0    0    0    0    1   0    0   0  
0g )
2

Figure 4. Gap constrained pattern search (gap constraint 

5.2  Map-based Support Prediction 
Notice  that  each  time  we  grow  a  length-l  pattern  P  to  a 
length l+1 pattern F, we will have to check F’s frequency by 
searching  its  occurrences  in  S  again.  This  reexamination 
mechanism costs a considerable amount of system runtime, 
as there are possibly millions of candidates. Nevertheless, if 
we can reuse P’s occurrence information, we may be able to 
speed up the search process dramatically. For this purpose, 
after  we  find  pattern  P’s  occurrences,  we  generate  a  rear-
map (RM) for P which records the number of times P ap-
pears in S  and all  its  ending  positions  (this  RM  is  actually 
the 
lPO list in the above section). As shown in Figure 4, if P
ever ends at position x, the value of RM[x] will indicate the 
number of times P ends at x; otherwise, RM[x] equals to 0.  
With  the  RM  of  pattern  P=plp2…pl,  denoted  by  the 
RM lp
,  we  may  just  search  P’s  RM,  instead  of  scan-
p
...1
ning the  whole  sequence S, to  find the  number  of times F
appearing in S. This can be achieved by a simple production 
and  sum  procedure.  More  specifically,  for  pattern  F=
plp2…plpl+1, if we can build a head-map (HP) which records 
locations  and  times  of  the  length-2  pattern  plpl+1’s  starting 
 indicates the number of times 
information, where 
plpl+1  starts  at  position  x,  then,  the  support  of  pattern  F  is 
determined by Eq. (4) 
=
)

   (4) 
As  shown  in  Figure  5,  when  predicting  the  support  of 
AGTC,  we  first  find  RMAGT  and  HMTC,  the  production  and 
sum  of  the  corresponding  elements  of  these  two  lists  will 
exactly equal to AGTC’s support.  

HP l
l pp

][1 x

pp
1

Sup

RM

HP

x
][

L
=
x
1

x
][

x
][

...

pp
l
l

p

+
1

(

p
l

p
1

..

⋅

+
1

+

2

l

1     2      3      4    5      6    7     8    9     10    11   12   13   14   15   16   17   18 
      S       A   A    A   G   C   G   T   C   T    C   A   G   G   T    C   T   T   C 
  RMAGT   0    0     0    0    0   0    4    0   1    0    0    0    0    2    0    1    0   0 
  HM TC   0    0     0    0    0   0    2    0   1    0    0    0    0    1     0    1    1   0 

 RMAGTC  0    0     0    0    0   0    0    4   0    5    0    0    0    0     2    0    0   1 
0g )
2

Figure 5. Map-based support prediction (gap constraint 

6  Algorithm 
The  system  framework  of  MCPaS  is  shown  in  Figure  6. 
MCPaS first generates all length-2 patterns, denoted by C2.
After  the  first  step,  MCPaS  begins  pattern  growing  and 
pruning. Assuming now at a certain step, we have generated 
a set of length-l candidates from length-l-1 patterns (on line 
5  in  Figure  6),  then  a  length-l  candidate’s  support  can  be 
easily predicted by plugging its length-l-1 patterns’ RM and 
the corresponding length-2 patterns’ HM into Eq. (4), with-

∪ F4…∪Fl-1)

Figure 6. MCPaS Algorithm 

7  Experimental Results 
The  data  used  in  our  experiments  are  nucleotide  DNA  se-
quences downloaded from the National Center for Biotech-
nology  Information  website [NCBI],  we choose  four  DNA 
sequences  as  our 
test  bed  (AX829168,  AX829170, 
AX829174,  and  AX829178).  When  using  multiple  se-
quences  to  form  a  hyper-sequence,  we  truncate  sequences 
into equal length ones. Because we use DNA sequences, the 

IJCAI-07

2938

alphabet for all element sequences is Σ={A, C, T, G}. For 
comparisons, we implement the MPPm method in [Zhang et 
al.  05]  in  finding  frequent  patterns  with  gap  constraints. 
This MPPm is the most relevant (and most recent as well) 
method we can find from all other peers.  

Table 1 Candidate numbers scanned by different methods  

Pattern  Enumerate All MPPm  MCPaSPruning  MCPaS 

64 
256 
1024 
4088 
15535 
39728 
16108 
2653 
350 
38 
3 

64 
256 
1018 
3997 
11461 
7138 
1581 
273 
41 
13 
2 

C3
C4
C5
C6
C7
C8
C9
C10
C11
C12
C13

64 
256 
1024 
4096 
16384 
65536 
262144 
1048576 
4194304 
16777216 

413 

64 
256 
1024 
4096 
16381 
54072 
19675 
3459 
414 
42 
3 

7.1  Pruning Efficiency Comparison 
We provide in this subsection a pruning efficiency compari-
son with MPPm by using a single DNA sequence AX829174 
(by randomly sampling a L=1000 subsequence). 

In Table 1, we report the experimental results (by the av-
erage  results  of  10  executions),  where  the  first  column 
means the candidate pattern set with different lengths. The 
second column indicates the number of candidates one has 
to evaluate, if enumerating all combinations. The third col-
umn  means  the  number  of  candidates  evaluated  by  MPPm.
Because  MCPaS  uses  two  approaches,  map-based  support 
prediction  and  Lemma  2,  for  pruning,  we’d  like  to  assess 
their  efficiency  separately. We  first  discard the map-based 
support prediction in the algorithm by replacing line 6 with 
line 13 in Figure 6. The results are denoted by MCPaSPrunning.
After  that,  we  use  both  map-based  support  prediction  and 
Lemma 2 for pruning, with results denoted by MCPaS.

When  comparing  MCPaSPrunning  and  MPPm,  we  find  that 
MCPaSPrunning has about 20% or fewer candidates than MPPm.
A further study on MCPaSPrunning and MMPm reveals that they 
have  opposite  pruning  mechanisms.  In  MCPaSPrunning,  pat-
terns are growing and pruned orderly, which means that we 
generate length-l candidates, prune out unlikely ones, grow 
candidates and repeat the algorithm until the candidate set is 
empty.  On  the  other  hand,  MPPm  uses  reverse  pruning 
mechanisms.  It  first  determines  the  maximal  length  of  the 
frequent  pattern  n,  and  based  on  this  value,  works  out  the 
minimal support values for different lengths of patterns. Not 
only the value n might be determined inaccurately (an inac-
curate n will therefore reduce the pruning efficiency), even 
if n is perfectly determined, it will leave the selected thresh-
old  (for  length  n-1,  n-2,  …,  3)  to  be  relatively  small,  be-
cause it has to consider the worst scenarios.  

When  combined  with the map-based  frequent  prediction 
mechanism, MCPaS makes dramatic improvement in reduc-
ing the number of candidates in Cl. For example, the number 
of patterns needs to be scanned in C8 is 7138, which is about 
86.8% less than the number of patterns scanned by MMPm.

7.2  Pattern Search Efficiency Comparison 

s)
n
r
e

t
t

 

a
p
0
0
0
1
/
s
d
n
o
c
e
s
(
 
e
m

i
t
 

h
c
r
a
e
s
 
e
v
i
t
s
u
a
h
x
E

4.5

4

3.5

3

2.5

2

1.5

1

0.5

3
3

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

4
4

5
5

6
6

7
7

8
8

9
9

10
10

0.1

11
11

Pattern length

)
s
n
r
e

t
t

 

a
p
0
0
0
1
/
s
d
n
o
c
e
s
(
 
e
m

i
t
 

h
c
r
a
e
s
 
S
C
G

Figure 7. Pattern search efficiency (W=4, L=1000, AX829174) 

1500

1000

500

30

20

10

)
s
d
n
o
c
e
s
(
 
e
m

i
t

n 
u
r
 
s
a
P
C
M

)
s
d
n
o
c
e
s
(
 
e
m

i
t

n 
u
r
m 
P
M
M

0
1.5
1.5

2
2

2.5
2.5

3
3

3.5
3.5

4
4

4.5
4.5

frequency  threshold (%)

0
5
5
-3
-3

x 10
x 10

Figure 8. Pattern mining performance from single sequence 

To assess the performances of the proposed GCS mecha-
nism  in  comparison  with  exhaustive  search,  we  sample  a 
length L=1000 subsequence from AX829174, then use two 
search mechanisms and record their average search time (in 
seconds)  for  every  1000  patterns,  and  report  the  results  in 
Figure 7 (the average results of 10 executions). In Figure 7, 
the  x-axis  denotes  the  length  of  the  patterns,  the  dash line 
indicates the results of exhaustive search (corresponding to 
the y-axis on the left side of the figure), and the solid line 
with crosses represents the results from GCS (corresponding 
to the y-axis on the right side of the figure).  

Comparing to exhaustive search, GCS is normally 5 to 10 
times faster in searching (depending on the actual length of 
the patterns). Because exhaustive search’s time complexity 
exponentially increases along with the pattern length and the 
gap  flexibility,  GCS  can  possibly  achieve  more  improve-
ments for longer patterns or larger gap constraints.  

7.3  Pattern Mining Performance Comparison 
To assess overall pattern mining performances, we first use 
a  single  sequence  AX829174  with  a  fixed value  W=4. We 
randomly  sample  a  length  L=1000  subsequence  from 
AX829174, then use MPPm and MCPaS to mine the results 
by  specifying  different  threshold  values  ρ.  We  report  the 
average runtime (from 10 executions) in Figure 8, where the 
x-axis means the value of ρ, the dash line indicates the re-
sults from MPPm and the solid line with crosses denotes the 
results from MCPaS (corresponding to the y-axis on the left 
and right side of Fig. 8 respectively).  

Both  MPPm  and  MCPaS  nonlinearly  respond  to  the 
threshold  ρ  with  pretty  similar  shapes. This  is  because  the 
value of ρ nonlinearly determines the number of candidates 
of the system. On average, MCPaS is about 40 times faster 
than MPPm, with its improvement mainly comes from three 
aspects:  (1)  an  ordinal  pruning  from  Lemma  2;  (2)  map-
based  support  prediction;  and  (3)  GCS  based  search.  GCS

IJCAI-07

2939

alone can enhance the search speed for about 8 times (com-
paring to exhaustive search), and the other two aspects will 
generally contribute a speed improvement of about 5 times.  
In Figure 9, we report the results from the hyper-sequence 
consisting of one to up to four sequences (due to the inten-
sive  time  consumption,  we  were  only able  to  run  the  pro-
grams for only one time for 3 or 4 sequences). The x-axis in 
Figure  9  represents  the  number  of  element  sequences,  and 
the y-axis denotes the average runtime of MCPaS. Because 
MPPm and MCPaS have huge runtime differences, we report 
MPPm’s  runtime  in a  small table in  Figure  9.  Meanwhile, 
for a hyper-sequence with K=4 element sequences, MPPm’s 
runtime is too big, so we omit the value at this point.  

4
x 10

MPPm Runtime 

113988

K

Runtime (s) 

1 
570 

2 

3 

13751  387754 

12

10

8

6

4

2

)
s
d
n
o
c
e
s
(
 
e
m
nt
u
r
 
s
a
P
C
M

i

13

0
1

236
2

4956

3

4

Number of element sequences K

Figure 9. Pattern mining performance from multiple sequences 

The results  in  Figure  9  show  that MCPaS’s  runtime  expo-
nentially increases by the number of element sequences K.
Although this sounds disappointing, to understand the chal-
lenge of our problem, let’s assume that S merely consists of 
two  DNA  element  sequences,  then  each  element  pattern  pi
has  (|Σ|+1)2-1=(4+1)2-1=24  possibilities  (excluding  the  one 
consisting  of  wildcards  only).  So  the  number  of  length-5 
candidate patterns is 245=7962624, if no pruning techniques 
are involved. Although we can transform element sequences 
to  form  a  hybrid  sequence  and  apply  MPPm  to  solve  the 
problem, because of the large alphabet size and the less ef-
fective reverse pruning technique, most of the length-5 can-
didates are going to be treated as frequent and used to grow 
next level candidates. MCPaS on the other hand, will start to 
prune candidates from length-4 candidates, and for length-5 
patterns it will reduce about 80% of candidates (if S consists 
of  2  element  sequences).  The  above  observations  make  us 
believe that although MCPaS is nonlinear w.r.t. the number 
of  element  sequences  K,  when  mining  complex  patterns 
from hyper-sequences, it is much more practical in reality.  

8  Conclusions  

We have studied in this paper the problem of mining com-
plex  patterns  across  multiple  sequences  with  gap  require-
ments,  where  patterns  repetitively  appear  in  multiple  se-
quences  and  their  matching  appearances  are  flexibly  con-
fined by users’ gap requirements. Because of the exponen-
tial candidate explosion, traditional Apriori-based solutions 
are  technically  infeasible  to  solve  the  problem.  We  have 
proposed  MCPas  with  three  unique  features  to  fulfill  the 
task:  (1)  an  Apriori-like  mining  framework  which  allows 
pattern  generation  and  growing  to  be  conducted  step  by 

step;  (2)  map-based  support  predication  to  predict  candi-
dates’  frequency  without  rescanning;  and  (3)  a  gap  con-
strained  linear-time  pattern  search.  Experimental  compari-
sons  have  shown  that  each  of  the  above  techniques  has 
made  a  contribution,  and  the  overall  performances  of 
MCPas have been about 40 times faster than its other peers. 

References 
[Altschul  90]  S.  Altschul,  W.  Gish,  W.  Miller,  E.  Myers,  &  D. 
Lipman,  Basic local  alignment  search  tool.  Journal  of Molecular 
Biology, 215:403-410, 1990. 
[Bellman  57]  R.  E.  Bellman.  Dynamic  Programming,  Princeton 
University Press, 1957. 
[Belkum et al. 97] A. van Belkum, S. Scherer amd W. van Leeu-
wen, D. Willemse, L. van Alphen, & H. Verbrugh. Variable num-
ber of tandem repeats in clinical strains of haemophilus influenzae. 
Infection and Immunity, 65(12):5017-5027, 1997. 
[Chen  et  al.  05]  G.  Chen,  X.  Wu,  &  X.  Zhu,  Sequential  pattern 
mining in multiple data streams, Proc. of ICDM, TX, 2005. 
[Das et al. 98] G. Das, K. Lin, H. Mannila, G. Renganathan, & P. 
Smyth, Rule discovery from time series, Proc. of KDD, 1998. 
[Herzel  et  al.  99]  H.  Herzel,  O. Weiss, &  E.  Trifonov,  10-11  BP 
periodicities  in  complete  genomes  reflect  protein  structure  and 
DNA folding, Bioinformatics, 15(3):187-193, 1999. 
[Jonassen  97]  I.  Jonassen,  Efficient  discovery  of  conserved  pat-
terns  using  a  pattern  graph,  Computer  Applications in the  Biosci-
ences, 13:509-522, 1997. 
[Mannila  et  al.  97]  H.  Mannila,  H.  Toivonen,  and  A.I.  Verkamo. 
Discovery of frequent episodes in event sequences. Data Mining & 
Knowledge Discovery, 1(3), 1997. 
[Méger & Rigotti 04] N. Méger, C. Rigotti: Constraint-based min-
ing of episode rules and optimal window. Proc. of PKDD, 2004. 
[Murray  et  al.  02]  K. Murray,  D.  Gorse, & J. Thornton,  Wavelet 
transforms for the characterization and detection of repeating mo-
tifs, Journal of Molecular Biology, 316:341-363, 2002. 
[NCBI] NCBI: http://www.ncbi.nlm.nih.gov
[Oats & Cohen 96] T. Oates & P. Cohen, Searching for structure in 
multiple streams of data, Proc. of ICML, 1996. 
[Pei et al. 01] J. Pei, J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, 
U. Dayal, & M. Hsu. PrefixSpan: Mining Sequential Patterns Effi-
ciently by Prefix-Projected Pattern Growth, Proc. of ICDE, 2001. 
[Pino et al. 01] H. Pinto, J. Han, J. Pei, K. Wang, Q. Chen, & U. 
Dayal,  Multi-dimensional  sequential  pattern  mining,  Proc.  of 
CIKM, 2001. 
[Rigoutsos & Floratos 98] I. Rigoutsos & A. Floratos. Combinato-
rial  pattern  discovery  in  biological  sequences:  the  teiresias  algo-
rithm. Bioinformatics, 14(1), 1998. 
[Srikant & Agrawal 96] R. Srikant & R. Agrawal, Mining Sequen-
tial  Patterns:  Generalizations  and  Performance  Improvements, 
Proc. of the International Conf. on Extending DB Tech., 1996.  
[Yang et al. 00] J. Yang, W. Wang, & P. Yu, Mining asynchronous 
periodic patterns in time series data, Proc. of KDD, MA, 2000.  
[Zaki  98]  M.  Zaki,  Efficient  enumeration  of  frequent  sequences, 
Proc. of CIKM, November 2-7, 1998, Bethesda, Maryland. 
[Zhang et al. 05] M. Zhang, B. Kao, D. Cheung, & K. Yip, Mining 
periodic  patterns  with  gap  requirement  from  sequences,  Proc.  of 
ACM SIGMOD, Baltimore Maryland, 2005. 

IJCAI-07

2940


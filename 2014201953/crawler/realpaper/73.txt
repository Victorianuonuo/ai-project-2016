A ﬂexible and robust similarity measure based on contextual probability

Hui Wang ∗

School of Computing and Mathematics

University of Ulster at Jordanstown

Northern Ireland

h.wang@ulster.ac.uk

Abstract

Arguably, analogy is one of the most important
aspects of intelligent reasoning.
It has been hy-
pothesized that, given suitable background knowl-
edge, analogy can be viewed as a logical infer-
ence process. This study follows another school
of thought that argues that similarity can provide a
probabilistic basis for inference and analogy. Most
similarity measures (which are frequently viewed
as being conceptually equivalent to distance mea-
sures) are restricted to either nominal or ordinal
attributes, and some are conﬁned to classiﬁcation
tasks. This paper proposes a ﬂexible similarity
measure that is task-independent and applies to
both nominal and ordinal data in a conceptually
uniform way. The proposed similarity measure
is derived from a probability function and corre-
sponds to the intuition that if we consider all neigh-
borhoods around a data point, the data points closer
to this point should be included in more of these
neighborhoods than more distant points. Experi-
ments we have conducted to demonstrate the use-
fulness of this measure indicate that it fares very
competitively with commonly used similarity mea-
sures.

Introduction

1
Natural and artiﬁcial intelligent agents rely on different in-
ference and reasoning strategies to achieve their goals and
maintain their intended behavior. Arguably, two of the more
successful strategies are based on the notions of analogy 1
∗Also LITA, Université de Metz, Ile du Saulcy, 57045 Metz

Cedex, France.

1Here we consider the subject of analogy in the following, rel-
atively narrow, sense: objects are represented by feature vectors
rather than the typical graphs; we do not consider transfer of knowl-
edge from one object to another; we do not consider cross-domain
knowledge but assume all objects share common set of attributes.

Werner Dubitzky

School of Biomedical Science
University of Ulster at Coleraine

Northern Ireland

w.dubitzky@ulster.ac.uk

and similarity. This is evidenced by the large volume of AI
research in the areas of analogical reasoning and case-based
reasoning [Vosniadou and Ortony, 1989; Kolodner, 1993]. In
the context of available background knowledge, analogy can
be viewed as a logical inference process [Davies and Russell,
1987]. Another way of looking at analogy holds that similar-
ity can provide a probabilistic basis for inference, and that a
quantitative framework can be developed for the probability
that an analogy is correct as a function of the degree of sim-
ilarity measured or observed [Russell, 1986]. This study is
concerned with the latter view of modeling analogy.

Distance measures (or, equivalently, similarity measures)
are central to many areas related to AI, including reasoning
under uncertainty, knowledge-based systems, machine learn-
ing, pattern recognition, data mining, analogical, case-based,
instance-based reasoning, and to other ﬁelds such as statis-
tics, operations research and decision theory. A large number
of distance metrics and measures are available and a particu-
lar choice may have considerable implications for the success
of the problem that is to be addressed.

Distance functions are broadly categorized into those that
can handle ordinal input data, nominal input data, and het-
erogeneous input data, consisting of both ordinal and nominal
data 2. Ordinal distance functions make use of the intrinsic to-
tal ordering relation in the underlying attribute values, which
are either continuous (e.g. weight of an object) or discrete
(e.g. number of obstacles). A nominal or symbolic attribute
is a discrete attribute whose values do not necessarily exhibit
a total order relation. For example, an attribute representing
the role of crew member may have the values scientist, mis-
sion specialist and commander. Using an ordinal distance
measurement on such values is meaningless.

Because the notion of ‘distance’ is intrinsically numeri-
cal, most available distance measures are deﬁned for data
with ordinal attributes. However, distance measures that can

2Here we consider two scales of measurement: ordinal and nom-
inal. In an ordinal scale, values (discrete or continuous) are ordered
whereas in a nominal scale, (discrete) values are unordered.

process nominal attributes are required by many modern AI
algorithms. Some measures that handle nominal attributes do
exist. The most well known measure of this kind is the Value
Difference Metric (VDM) [Stanﬁll and Waltz, 1986]. It is de-
ﬁned in terms of attribute values that are conditioned by pos-
terior probabilities of a class. Hence, like some other distance
measures, the VDM is restricted to classiﬁcation tasks.

To cope with heterogeneous data, containing both nominal
and ordinal attributes, two approaches are commonly taken:
(1) The data is transformed into one of the two data types
that complies with the distance measure used, and (2) two
types of distance measures are combined and handle the data
separately in accordance with the data type (see, for example,
[Wilson and Martinez, 1997]). However, both approaches are
problematic when it comes to the interpretation of the results.
The novel distance measure proposed in this study is called
Neighborhood Counting Metric (NCM). It is derived from a
probability function and it can handle both nominal and or-
dinal attributes in a conceptually uniform way. Intuitively, it
can be understood or interpreted as follows (see Figure 1):
If we consider all neighborhoods, N, around a data point,
t, then those data points closer to t should be included in
more of these neighborhoods, N, than points that are not
close to t. Usually neighborhoods are interpreted in terms
of distance. However, if we adopted this interpretation, we
would deﬁne one distance function in terms of another. To
avoid this dilemma, we interpret neighborhood without dis-
tance through the concept of hypertuples [Wang et al., 1999]
for both nominal and ordinal attributes. As a result, our dis-
tance measure applies to both ordinal and nominal attributes
in a uniform way.

The new NCM is conceptually simple, it is straightforward
to implement, and it has the added property that it is inde-
pendent of the underlying analytical or reasoning task (e.g.
classiﬁcation). The measure’s clear and unambiguous mean-
ing is deﬁned by the number of neighborhoods of a query that
include or cover a given data point.

Paper outline: Section 2 presents a short review of distance
measures relevant to this study; it is followed by Sections 3
and 4, which present mathematical details of the new simi-
larity measure. In Section 5 the empirical evaluation of the
method is presented and its results discussed. The paper con-
cludes with a summary and a brief discussion of future work.

2 A brief review of important distance

functions

Two of the most common distance functions are Euclid-
ean distance and the Hamming distance. The former is re-
stricted to ordinal and the latter is usually used for nomi-
nal attributes. The Heterogeneous Euclidean-Overlap Metric
(HEOM) [Wilson and Martinez, 1997] combines the Ham-

Figure 1: An illustrative example. Each of the 7 concentric
circles represents a neighborhood of data point t deﬁned by
some distance measure. Data point a is covered by 5 neigh-
borhoods whereas b by only 2. Geometrically, a is clearly
closer (more similar) to t than b.

ming and the normalized Euclidean distance functions and
can therefore be used on heterogeneous data.

The Value Difference Metric (VDM) [Stanﬁll and Waltz,
1986] is designed to handle nominal attributes in classiﬁca-
tion tasks only.
It uses pre-computed statistical properties
from available training data and can be interpreted in terms
of probabilities. The Heterogeneous Value Difference Metric
(HVDM) [Wilson and Martinez, 1997] combines the Euclid-
ean distance and the VDM. It is therefore able to handle het-
erogeneous data, but because of its VDM heritage it is con-
ﬁned to classiﬁcation tasks.

The Interpolated Value Difference Metric (IVDM) [Wilson
and Martinez, 1997] extends the VDM to scenarios involving
ordinal attributes.
In the learning phase, it employs a dis-
cretization step to collect statistics and determine the proba-
bility for the discretized values (Pa,x,c in the VDM formula),
but then retains the continuous values in the training data for
later use in the application or testing phase. The IVDM re-
quires a non-parametric probability density estimation to de-
termine the probability values for each class. The Discretized
Value Difference Metric (DVDM) is the same as the IVDM
except that it avoids the retention of the original continuous
values but uses only the discretized values.

[Blanzieri and Ricci, 1999] introduced the Minimal Risk
Metric (MRM), a probability-based distance measure for
classiﬁcation and case-based reasoning. It minimizes the risk
of misclassiﬁcation and depends on probability estimation
techniques. As a result, the MRM exhibits a high compu-
tational complexity and is task-dependent.

1987654231019876542310tab3 A probability function
Let V be a (non-empty) universe of discourse. A Σ-ﬁeld F
on V is a collection of subsets of V , such that:
1. V ∈ F ;
2. If A ∈ F then A(cid:48) ∈ F , where A(cid:48) is the complement of A;
3. If A,B,··· ∈ F , then A∪ B∪··· ∈ F .
A probability function P over V is a mapping from F to
[0,1] satisfying the three axioms of probability [Ash, 1972].
If P is restricted to V , it is called a probability mass function
(discrete) or probability density function (continuous).
Suppose we have a probability function P as deﬁned above.
For X ∈ F let f (X) be a non-negative measure of X satisfying
f (X1 ∪ X2) = f (X1) + f (X2) if X1 ∩ X2 = /0. As an example,
we can take f (X) for the cardinality of X.
Consider a function G : F → [0,1] such that, for X ∈ F ,

G(X) = ∑
E∈F

P(E) f (X ∩ E)/K

where K = ∑E∈F P(E) f (E). It can be easily shown that G is
a probability function.
G(X) is calculated from all those E ∈ F that overlap with
X (i.e., f (X ∩ E) (cid:54)= 0). These E’s are relevant to X and serve
as the contexts in which G(X) is induced. Each such E is
called a neighborhood of X. In other words a neighborhood
of X is an element E of F , i.e., a subset of V , such that E
overlaps X.

In practice P is usually not known, but a sample of data
drawn from the data space according to P is commonly avail-
able. P can be estimated from the sample by either paramet-
ric or non-parametric techniques. In some tasks (e.g., clas-
siﬁcation) the point-wise probability is needed. When there
is insufﬁcient data, which is not uncommon, the point-wise
probability may be difﬁcult to estimate using non-parametric
methods. However, it is likely that probability can be esti-
mated for some regions (contexts) in the data space based on
the sample. Using the G probability function, we can estimate
the G probability for any single data point from knowledge
of the P probabilities of various regions or contexts. This
provides us with a formal way of inference about probability
under incomplete (hence uncertain) situations.
To calculate the G probability for X ∈ F in practice, we

need to

1. Find the set of all neighborhoods appropriate to the prob-

lem at hand,

2. Estimate the P probability for every neighborhood,
3. Finally, estimate (calculate) the G probability for X ac-

cording to its deﬁnition.

In classiﬁcation tasks, we can further calculate the conditional
G probability of a class given a single data point in a similar

fashion. While this may appear to be computationally expen-
sive, it is often possible to derive a simple formula for G.

Depending on what F is and how neighborhoods are de-
ﬁned, we can use the G probability for different tasks.
In
Section 4 we take F to be the set of all regions in a multi-
dimensional space deﬁnable as hypertuples and our similarity
measure is derived from this interpretation.

3.1 Estimation of G
This section describes how G is estimated for a data point
from data samples. Let D be a sample of data drawn from V
according to an unknown probability distribution P. Our aim
is to calculate ˆG(t|D) for any t ∈ V .

To calculate G we need P, which can be estimated from
data assuming the principle of indifference as follows: For
any E ∈ F ,

ˆP(E|D) = |E|/n

where |E| is the number of elements in E and n is the number
of elements in D. Additionally we assume that f (X) = |X|
for X ∈ F .

ˆP(E|D)× f (E ∩t)

K

ˆP(E|D)×|E ∩t|

We then have,
ˆG(t|D) = ∑
E∈F
= ∑
E∈F
= ∑
E∈F ,t∈E
1
nK ∑
E∈F ,t∈E
1
nK ∑
x∈D

=

K
ˆP(E|D)

K
|E|

deﬁnition

speciﬁcation of f

assumption that t is in E so E ∩t = t

estimation of P by principle of indifference

expansion and then re-organisation

=

cov(t,x)

where K is a normalisation factor, and cov(t,x) is the number
of such E ∈ F that covers both t and x. We call this number
the cover of x with respect to t.
To obtain cov(t,x), a straightforward approach would be
to iterate over all E ∈ F and check if E covers both t and x.
Clearly, such a process is undesirable because of its exponen-
tial complexity. In Section 4 we will present an efﬁcient way
of calculating cov(t,x).

4 Measuring distance through counting
This section considers an interpretation of neighborhood and
derives a formula for calculating cov(t,x), which is then taken
as a measure of similarity or distance.

4.1 Neighborhood
Pursuing a non-distance-based conceptualization of neigh-
borhood, we interpret a neighborhood as a hypertuple [Wang
et al., 1999], and a neighbor as a data point (or tuple) cov-
ered by some neighborhood. So a neighborhood of a tuple is
a hypertuple that covers the tuple.

Hypertuples
Let R = {a1,a2,··· ,an} be a set of attributes, and dom(a)
be the domain of attribute a ∈ R. Furthermore let V def=
i=1 dom(ai) and L def= ∏n
∏n
i=1 2dom(ai). V is the data space de-
ﬁned by R, and L an extended data space. A (given) data set
is D ⊆ V – a sample of V .

The attributes can be either ordinal or nominal. For sim-
plicity, we assume the domain of any attribute is ﬁnite, but
the results are not limited to ﬁnite domains.
If we write an element t ∈ V by (cid:104)v1,v2,··· ,vn(cid:105) then vi ∈
dom(ai). If we write h ∈ L by (cid:104)s1,s2,··· ,sn(cid:105) then si ∈ 2dom(ai)
or si ⊆ dom(ai).

Consider

An element of L is called a hypertuple, and an element of
V a simple tuple. The difference between the two is that a
ﬁeld within a simple tuple is a value while a ﬁeld within a
hypertuple is a set. If we interpret vi ∈ dom(ai) as a singleton
set {vi}, then a simple tuple is a special hypertuple. Thus we
can embed V into L, so V ⊆ L.
two hypertuples h1 and h2, where h1 =
(cid:104)s11,s12,··· ,s1n(cid:105) and h2 = (cid:104)s21,s22,··· ,s2n(cid:105). We say h1 is
covered by h2 (or h2 covers h1), written h1 ≤ h2, if for
i ∈ {1,2,··· ,n},

(cid:40)
{x ∈ dom(ai) : min(s1i ∪ s2i) ≤ x ≤ max(s1i ∪ s2i)},

Furthermore the sum of h1 and h2, written by h1 + h2
(cid:104)s1,s2,··· ,sn(cid:105), is: for each i ∈ {1,2,··· ,n},

∀x ∈ s1i,min(s2i) ≤ x ≤ max(s2i)
s1i ⊆ s2i

if ai is ordinal
if ai is nominal

si =

def=

if ai is ordinal
if ai is nominal

s1i ∪ s2i,

The product operation ∗ can be similarly deﬁned. It turns out
that (cid:104)L,≤,+,∗(cid:105) is a lattice.

For a simple tuple t, t(ai) represents the projection of t onto

i

If ai is nominal, then Si = 2dom(ai) and so N(cid:48)(cid:48)

attribute ai. For a hypertuple h, h(ai) is similarly deﬁned.
A hypertuple can be generated by taking one subset from
each attribute. Let ai be an attribute, i = 1,2,··· ,n; Si be the
def= |Si|. Then a
set of all subsets of the domain of ai; and N(cid:48)(cid:48)
hypertuple h is an element of ∏i Si. Therefore the number of
all hypertuples is ∏i N(cid:48)(cid:48)
i .
i = 2mi, where
mi = |dom(ai)|. If ai is ordinal, then an element s ∈ Si cor-
responds to an interval [min(s),max(s)]. As a result, some
elements of Si may correspond to the same interval, and
hence become equivalent.
In general we have the follow-
ing number of distinctive intervals for an ordinal attribute:
i = ∑mi−1
j=0 (mi − j) = ∑mi
N(cid:48)(cid:48)
Neighborhoods as hypertuples
For any simple tuple t, a neighborhood of t is taken to be a
hypertuple h such that t ≤ h; and a neighbor of t with respect

j=1 j = mi(mi+1)

2

.

to h is x ∈ V such that x ≤ h. By this deﬁnition any simple
tuple has a neighborhood. At least the maximal hypertuple
in the extended data space is a neighborhood of any simple
tuple since the maximal hypertuple covers all simple tuples.
For a query t ∈ V , not all hypertuples in ∏i Si are neigh-
borhoods of t. For a hypertuple h to be a neighborhood of
t we must have t(ai) ∈ h(ai) for all i. Therefore, to gen-
erate a neighborhood of t, we can take an si ∈ Si such that
t(ai) ∈ si for all i, resulting in a hypertuple (cid:104)s1,s2,··· ,sn(cid:105).
If ai is nominal, the number of si ∈ Si such that t(ai) ∈ si is
i = ∑mi−1
= 2mi−1 since si is any subset of dom(ai)
N(cid:48)
that is the super set of t(ai).
If ai is ordinal, this number
i = (max(ai) − t(ai) + 1) × (t(ai) − min(ai) + 1) since
is N(cid:48)
(max(ai)− t(ai) + 1) is the number of ordinal values above
t(ai), and (t(ai)− min(ai) + 1) is such a number below t(ai).
Any pair of values from the two parts respectively forms an
interval.
To summarize, the number of neighborhoods of t is ∏i N(cid:48)
i ,

mi−1
i

(cid:179)

(cid:180)

i=0

where
(1)

N(cid:48)
i =

2mi−1,

if ai is nominal
(max(ai)−t(ai) + 1)× (t(ai)− min(ai) + 1),
if ai is ordinal.

Cover of simple tuples
Under the above interpretation of neighborhood, we know ex-
actly the number of all neighborhoods of a given simple tuple
t. Here we present an efﬁcient way of calculating cov(t,x),
the number of neighborhoods of t that cover x, which is
needed in Section 3.1.
Consider two simple tuples t = (cid:104)t1,t2,··· ,tn(cid:105) and x =
(cid:104)x1,x2,··· ,xn(cid:105). A neighborhood h of t covers t by deﬁni-
tion, i.e., t ≤ h. What we need to do is to check if h covers x
as well. In other words, we want to ﬁnd all hypertuples that
cover both t and x.

Eq.1 speciﬁes the number of all simple tuples that cover
t only. We take a similar approach here by looking at each
attribute and explore the number of subsets that can be used
to generate a hypertuple covering both t and x. Multiplying
these numbers across all attributes gives rise to the number
we require.

Consider attribute ai. If ai is ordinal, then the number of
intervals that can be used to generate a hypertuple covering
both xi and ti is as follows:
Ni = (max(ai)−max({xi,ti})+1)×(min({xi,ti})−min(ai)+1).
If ai is nominal, the number of subsets for the same purpose
is:

(cid:40)

Ni =

2mi−1,if xi = ti
2mi−2,otherwise

Recall that mi = |dom(ai)|.

x is cov(t,x) = ∏i Ni, where

To summarize, the number of neighborhoods of t covering

(2)

Ni =



2mi−1,
2mi−2,

(max(ai)− max({xi,ti}) + 1)× (min({xi,ti})− min(ai) + 1),

if ai is ordinal
if ai is nominal and xi = ti
if ai is nominal and xi (cid:54)= ti

4.2 Use of cover as similarity measure
We use cov(t,x) as a measure of similarity between t and x,
which we call the Neighborhood Counting Metric or simply
NCM. That is, for any two tuples x and y, the NCM between
them is

(3)

NCM(x,y) = cov(x,y) =

n∏

i=1

Ni

where n is the number of attributes and Ni is given by Eq.2.
It is clear that NCM(x,y) ≥ 0, NCM(x,x) ≥ NCM(x,y) and
NCM(x,y) = NCM(y,x). Therefore the NCM is reﬂexive and
symmetric, the properties generally required for a similarity
measure [Osborne and Bridge, 1997].

This measure can be interpreted intuitively as follows. If
we consider all neighborhoods around a data tuple, those tu-
ples closer to this tuple should be included in more neigh-
borhoods and those farther away should be included in fewer
neighborhoods (see Figure 1). As a result, closer tuples
should be assigned higher cover values.

In contrast to its usual interpretation in terms of distance,
we interpret the notion of a neighborhood without distance
through the concept of hypertuples for both nominal and ordi-
nal attributes. As a consequence, our novel distance measure
applies to both ordinal and nominal attributes in a conceptu-
ally uniform way.

Incidentally, the NCM intrinsically handles missing values
in a fashion consistent with other measures. Recall, that the
NCM is a product of all Ni’s, where i is attribute index. For
two data tuples, t and x, if there is a missing value in t or x for
attribute i, then Ni is set to 1. As a result, this attribute does
not contribute towards the NCM value.

5 Evaluation
The new Neighborhood Counting Metric is task-independent
and can therefore be used for classiﬁcation, clustering and
other analytical tasks involving distances or similarities. We
empirically evaluated the NCM in the context of a classiﬁ-
cation task, using the k-nearest neighbor (k-NN) classiﬁca-
tion algorithm with and without distance-based (neighbor)

weighting [Baily and Jain, 1978]. The purpose of the eval-
uation is to compare the new measure with some of the com-
monly used distance measures in a setup involving heteroge-
neous data. The evaluation uses public benchmark data sets
from UC Irvine Machine Learning Repository, which were
selected with respect to their balance of ordinal and nominal
attributes.

We implemented a k-NN algorithm with the novel NCM as
well as the measures HEOM, HVDM, IVDM, and DVDM.
The computational runtimes of the MRM turned out too ex-
cessive, so it was excluded from the study. In the experiments
k was set to 1,6,11,16,21,16,31 and to ‘MaxK’ (i.e., k =
the number of data tuples in the training data). We adopted
a 10-fold cross-validation procedure, and for each measure
and each k value we ran the cross-validation 10 times and
recorded the results for subsequent analysis.

Due to space limitations, we report only some of the results
in detail. Table 1 shows the results when weighting was used
and k = MaxK.

A statistical signiﬁcance analysis was carried out using
the Student t-test (two samples, assuming unequal variances)
with α = 0.05 (at a 95% conﬁdence level). For each data
set, each measure and each k value, we ran a 10-fold cross-
validation using k-NN 10 times with random partitioning of
data, resulting in a sample of 10 values. For a pair of samples,
by two different measures, we have a total of 20 values. So
the critical value is 2.1. We then calculate the ‘t’ values. If
t ≥ 2.1 or t ≤ −2.1 the two samples are signiﬁcantly different
(the classiﬁcation rate of one sample is signiﬁcantly higher or
lower than that of the other). Notice that every value in Table
1 is the average of a sample of 10 values.

From Table 1 we can see that based on our experimental
design the NCM achieved 17 out of 20 ‘signiﬁcantly’ higher
classiﬁcation success rates compared to the other methods
(last row in table). Looking at the subtotals for ‘nominal’,
‘ordinal’ and ‘mixture’ categories (reﬂected by subtotals from
top to bottom), the NCM achieves the largest margin over its
competitors for the ‘nominal’ data sets, followed by ‘mixture’
and then ‘ordinal’. This suggests that, when all data tuples are
taken into consideration, the NCM is clearly superior under
all circumstances.

The details of the results involving other values for k are
not shown.
In terms of k-changes, we observe that with-
out weighting there was no signiﬁcant difference between the
used measures. When weighting was used, there was still lit-
tle difference for small k values. The general trend for all
measures was: as k got larger the classiﬁcation success rate
increased slightly but soon started to decline. The difference
then showed up as the NCM displayed a much slower rate
of decline and, after k > 11, the NCM consistently outper-
formed the other four measures (see k = MaxK as discussed

above). We can conclude that the NCM produces less vari-
able or more robust results than the other four measures.

6 Conclusion
Starting from a probability function, we have developed
a novel similarity or distance measure, the Neighborhood
Counting Metric, which can be used with ordinal, nominal
and heterogeneous attributes in a conceptually uniform way.
This measure is deﬁned without reference to any particular
analytical task (e.g., classiﬁcation). This means that the mea-
sure is potentially useful for many reasoning, inferential and
reasoning tasks and systems modeling analogy or similarity.
Because the NCM is based on a simple, easy-to-implement
mathematical formulation and has a computational complex-
ity in the same order as the Euclidean distance measure, it is
a prime candidate for practical AI methods and tools.

Our empirical evaluation demonstrates that in a k-NN-
based classiﬁcation task, the measure signiﬁcantly outper-
forms its competitors when distance-based neighbor weight-
ing is used and when k is not too small, in particular when all
data tuples are taken into consideration. As k gets larger, the
NCM displayed a consistently superior performance over the
reference methods. The difference is most signiﬁcant when
all data tuples in a training data set are considered. This im-
plies that the NCM is less sensitive to k than the other mea-
sures considered in this study. Given an application based
on the k-NN algorithm, if the optimal value for k is not
known, we can simply consider all or a relatively large set
of data tuples without a signiﬁcantly compromising perfor-
mance. Therefore, the performance of this measure is more
predictable than the other methods investigated in this study.
In our experiments, all attributes were assumed to have
equal weights. Future work will investigate how to determine
the best attribute weights to achieve improved performance,
as well as an application of the NCM to clustering tasks.

References
[Ash, 1972] R.B. Ash. Real Analysis and Probability. Aca-

demic Press, New York, 1972.

[Baily and Jain, 1978] T. Baily and A. K. Jain. A note on
distance-weighted k-nearest neighbor rules. IEEE Trans.
Syst. Man Cyber., 8(4):311–313, 1978.

[Blanzieri and Ricci, 1999] Enrico Blanzieri and Francesco
Ricci. Probability based metrics for nearest neighbor clas-
siﬁcation and case-based reasoning.
Lecture Notes in
Computer Science, 1650:14–29, 1999.

[Davies and Russell, 1987] Todd R. Davies and Stuart J.
Russell. A logical approach to reasoning by analogy. In
Proc. of IJCAI87, pages 264–270, Milan, Italy, 1987.

Data
Audiology (Standardized)
Bridges2 (Standardized)
Primary Tumor
Soybean (Large)
TTT
Vote
Yeast
Zoo
avg (nominal)
Diabetes
Ecoli
Glass
Iono
Iris
Pima
Sonar
Vehicle
Wine
avg (ordinal)
Anneal
Australian
Auto
Breast-Cancer
Bridges1
Credit Screening
German
Heart
Hepatitis
Hors-Colic
avg (heterogeneous)
Sig. count

DVDM
22.9
44.3
25.6
84.6
65.8
90.9
32.3
78.8
59.3
63.5
61.1
47.9
68.1
93.2
63.0
74.3
46.3
94.7
73.9
75.3
85.5
49.1
75.7
44.3
60.8
68.7
79.2
76.6
77.9
69.3
1

NCM
66.3
60.8
44.9
86.7
68.0
91.0
33.5
95.0
70.5
63.6
50.8
61.1
80.5
91.3
63.0
85.7
60.1
96.7
77.3
75.3
87.5
67.9
76.0
62.2
88.3
69.0
82.3
81.7
84.7
77.5
17

HEOM
44.2
52.9
26.0
84.6
65.3
88.8
32.0
69.5
61.3
63.5
43.1
47.2
63.2
90.7
63.0
73.1
42.3
94.6
70.1
75.3
82.8
46.4
75.7
49.1
82.0
68.7
80.6
76.6
64.4
70.2
0

IVDM
23.0
44.3
25.6
84.6
65.8
90.9
33.0
80.0
59.5
63.5
50.1
55.6
64.3
94.3
63.1
71.6
48.2
98.1
74.0
75.3
85.3
48.2
75.7
44.3
64.7
68.7
81.7
76.6
79.4
70.0
2

HVDM
22.9
44.3
25.6
84.6
65.8
90.9
32.0
78.8
59.3
63.5
43.1
47.2
63.2
90.7
63.0
73.1
42.3
94.6
70.1
75.3
82.8
46.4
75.7
44.3
59.5
68.7
72.0
76.6
64.4
66.6
0

Table 1: Average results (classiﬁcation success rates) over 10
runs of the algorithms, where neighbor weighting was used
and k=‘MaxK’. Best values are shown in bold, and statisti-
cally signiﬁcant values in bold italic.

[Kolodner, 1993] Janet Kolodner. Case-Based Reasoning.

Morgan Kaufmann Publishers, San Mateo, CA, 1993.

[Osborne and Bridge, 1997] Hugh Osborne

and Derek
Bridge. Models of similarity for case-based reasoning.
In Procs. of the Interdisciplinary Workshop on Similarity
and Categorisation, pages 173–179, 1997.

[Russell, 1986] Stuart J. Russell. Quantitative analysis of
analogy. In Proc of AAAI86, pages 284–288, Philadelphia,
PA, 1986. Morgan Kaufmann.

[Stanﬁll and Waltz, 1986] C. Stanﬁll and D. Waltz.

To-
ward memory-based reasoning. Communication of ACM,
29:1213–1229, 1986.

[Vosniadou and Ortony, 1989] Stella Vosniadou and Andrew
Similarity and Analogical Reasoning.

Ortony, editors.
Cambridge University Press, New York, USA, 1989.

[Wang et al., 1999] H. Wang, W. Dubitzky, I. Düntsch, and
D. Bell. A lattice machine approach to automated case-
base design: Marrying lazy and eager learning. In Proc.
IJCAI99, pages 254–259, Stockholm, Sweden, 1999.

[Wilson and Martinez, 1997] D. Randal Wilson and Tony R.
Improved heterogeneous distance functions.
Martinez.
Journal of Artiﬁcial Intelligence Research, 6:1–34, 1997.


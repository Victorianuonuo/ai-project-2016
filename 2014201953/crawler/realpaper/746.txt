Nogood Recording from Restarts

Christophe Lecoutre and Lakhdar Sais and S´ebastien Tabary and Vincent Vidal

CRIL−CNRS FRE 2499

Universit´e d’Artois

{lecoutre, sais, tabary, vidal}@cril.univ-artois.fr

Lens, France

Abstract

In this paper, nogood recording is investigated
within the randomization and restart framework.
Our goal is to avoid the same situations to occur
from one run to the next one. More precisely, no-
goods are recorded when the current cutoff value
is reached, i.e. before restarting the search algo-
rithm. Such a set of nogoods is extracted from the
last branch of the current search tree. Interestingly,
the number of nogoods recorded before each new
run is bounded by the length of the last branch of
the search tree. As a consequence, the total number
of recorded nogoods is polynomial in the number
of restarts. Experiments over a wide range of CSP
instances demonstrate the effectiveness of our ap-
proach.

1 Introduction
Nogood recording (or learning) has been suggested as a
technique to enhance CSP (Constraint Satisfaction Problem)
solving in [Dechter, 1990]. The principle is to record a
nogood whenever a conﬂict occurs during a backtracking
search. Such nogoods can then be exploited later to pre-
vent the exploration of useless parts of the search tree. The
ﬁrst experimental results obtained with learning were given
in the early 90’s [Dechter, 1990; Frost and Dechter, 1994;
Schiex and Verfaillie, 1994].

Contrary to CSP, the recent impressive progress in SAT
(Boolean Satisﬁability Problem) has been achieved using
nogood recording (clause learning) under a randomization
and restart policy enhanced with a very efﬁcient lazy data
structure [Moskewicz et al., 2001].
Indeed, the interest of
clause learning has arisen with the availability of large in-
stances (encoding practical applications) which contain some
structure and exhibit heavy-tailed phenomenon. Learning
in SAT is a typical successful technique obtained from the
cross fertilization between CSP and SAT: nogood recording
[Dechter, 1990] and conﬂict directed backjumping [Prosser,
1993] have been introduced for CSP and later imported into
SAT solvers [Bayardo and Shrag, 1997; Marques-Silva and
Sakallah, 1996].

Recently, a generalization of nogoods, as well as an elegant
learning method, have been proposed in [Katsirelos and Bac-

chus, 2003; 2005] for CSP. While standard nogoods corre-
spond to variable assignments, generalized nogoods also in-
volve value refutations. These generalized nogoods beneﬁt
from nice features. For example, they can compactly cap-
ture large sets of standard nogoods and are proved to be more
powerful than standard ones to prune the search space.

As the set of nogoods that can be recorded might be of ex-
ponential size, one needs to achieve some restrictions. For ex-
ample, in SAT, learned nogoods are not minimal and are lim-
ited in number using the First Unique Implication Point (First
UIP) concept. Different variants have been proposed (e.g. rel-
evance bounded learning [Bayardo and Shrag, 1997]), all of
them attempt to ﬁnd the best trade-off between the overhead
of learning and performance improvements. Consequently,
the recorded nogoods cannot lead to a complete elimination
of redundancy in search trees.

In this paper, nogood recording is investigated within the
randomization and restart framework. The principle of our
approach is to learn nogoods from the last branch of the
search tree before a restart, discarding already explored parts
of the search tree in subsequent runs. Roughly speaking, we
manage nogoods by introducing a global constraint with a
dedicated ﬁltering algorithm which exploits watched literals
[Moskewicz et al., 2001]. The worst-case time complexity of
this propagation algorithm is O(n2γ) where n is the number
of variables and γ the number of recorded nogoods. Besides,
we know that γ is at most ndρ where d is the greatest do-
main size and ρ is the number of restarts already performed.
Remark that a related approach has been proposed in [Bap-
tista et al., 2001] for SAT in order to obtain a complete restart
strategy while reducing the number of recorded nogoods.

2 Technical Background
A Constraint Network (CN) P is a pair (X , C ) where X is
a set of n variables and C a set of e constraints. Each variable
X ∈ X has an associated domain, denoted dom(X), which
contains the set of values allowed for X. Each constraint C ∈
C involves a subset of variables of X , denoted vars(C), and
has an associated relation, denoted rel(C), which contains the
set of tuples allowed for vars(C).

A solution to a CN is an assignment of values to all the vari-
ables such that all the constraints are satisﬁed. A CN is said
to be satisﬁable iff it admits at least one solution. The Con-
straint Satisfaction Problem (CSP) is the NP-complete task

IJCAI-07

131

of determining whether a given CN is satisﬁable. A CSP in-
stance is then deﬁned by a CN, and solving it involves either
ﬁnding one (or more) solution or determining its unsatisﬁa-
bility. To solve a CSP instance, one can modify the CN by
using inference or search methods [Dechter, 2003].

The backtracking algorithm (BT) is a central algorithm for
solving CSP instances. It performs a depth-ﬁrst search in or-
der to instantiate variables and a backtrack mechanism when
dead-ends occur. Many works have been devoted to improve
its forward and backward phases by introducing look-ahead
and look-back schemes [Dechter, 2003]. Today, MAC [Sabin
and Freuder, 1994] is the (look-ahead) algorithm considered
as the most efﬁcient generic approach to solve CSP instances.
It maintains a property called Arc Consistency (AC) during
search. When mentioning MAC, it is important to indicate
which branching scheme is employed. Indeed, it is possible
to consider binary (2-way) branching or non binary (d-way)
branching. These two schemes are not equivalent as it has
been shown that binary branching is more powerful (to refute
unsatisﬁable instances) than non-binary branching [Hwang
and Mitchell, 2005]. With binary branching, at each step of
search, a pair (X,v) is selected where X is an unassigned
variable and v a value in dom(X), and two cases are consid-
ered: the assignment X = v and the refutation X (cid:3)= v. The
MAC algorithm (using binary branching) can then be seen
as building a binary tree. Classically, MAC always starts by
assigning variables before refuting values. Generalized Arc
Consistency (GAC) (e.g. [Bessiere et al., 2005]) extends AC
to non binary constraints, and MGAC is the search algorithm
that maintains GAC.

Although sophisticated look-back algorithms such as CBJ
(Conﬂict Directed Backjumping) [Prosser, 1993] and DBT
(Dynamic Backtracking) [Ginsberg, 1993] exist, it has been
shown [Bessiere and R´egin, 1996; Boussemart et al., 2004;
Lecoutre et al., 2004] that MAC combined with a good vari-
able ordering heuristic often outperforms such techniques.

3 Reduced nld-Nogoods

From now on, we will consider a search tree built by a
backtracking search algorithm based on the 2-way branching
scheme (e.g. MAC), positive decisions being performed ﬁrst.
Each branch of the search tree can then be seen as a sequence
of positive and negative decisions, deﬁned as follows:
Deﬁnition 1 Let P = (X , C ) be a CN and (X,v) be a pair
such that X ∈ X and v ∈ dom(X). The assignment X = v
is called a positive decision whereas the refutation X (cid:3)= v is
called a negative decision. ¬(X = v) denotes X (cid:3)= v and
¬(X (cid:3)= v) denotes X = v.
Deﬁnition 2 Let Σ = (cid:4)δ1, . . . , δi, . . . , δm(cid:5) be a sequence
of decisions where δi is a negative decision. The sequence
(cid:4)δ1, . . . , δi(cid:5) is called a nld-subsequence (negative last deci-
sion subsequence) of Σ. The set of positive and negative de-
cisions of Σ are denoted by pos(Σ) and neg(Σ), respectively.
Deﬁnition 3 Let P be a CN and Δ be a set of decisions.
P |Δ is the CN obtained from P s.t., for any positive decision
(X = v) ∈ Δ, dom(X) is restricted to {v}, and, for any
negative decision (X (cid:3)= v) ∈ Δ, v is removed from dom(X).

Deﬁnition 4 Let P be a CN and Δ be a set of decisions. Δ
is a nogood of P iff P |Δ is unsatisﬁable.

From any branch of the search tree, a nogood can be ex-
tracted from each negative decision. This is stated by the fol-
lowing proposition:
Proposition 1 Let P be a CN and Σ be the sequence of de-
cisions taken along a branch of the search tree. For any nld-
subsequence (cid:4)δ1, . . . , δi(cid:5) of Σ, the set Δ = {δ1, . . . , ¬δi} is
a nogood of P (called nld-nogood)1.

Proof. As positive decisions are taken ﬁrst, when the negative
decision δi is encountered, the subtree corresponding to the
opposite decision ¬δi has been refuted. 2

These nogoods contain both positive and negative deci-
sions and then correspond to the deﬁnition of generalized no-
goods [Focacci and Milano, 2001; Katsirelos and Bacchus,
2005]. In the following, we show that nld-nogoods can be re-
duced in size by considering positive decisions only. Hence,
we beneﬁt from both an improvement in space complexity
and a more powerful pruning capability.

By construction, CSP nogoods do not contain two opposite
decisions i.e. both x = v and x (cid:3)= v. Propositional resolution
allows to deduce the clause r = (α∨β) from the clauses x∨α
and ¬x ∨ β. Nogoods can be represented as propositional
clauses (disjunction of literals), where literals correspond to
positive and negative decisions. Consequently, we can extend
resolution to deal directly with CSP nogoods (e.g. [Mitchell,
2003]), called Constraint Resolution (C-Res for short). It can
be deﬁned as follows:
Deﬁnition 5 Let P be a CN, and Δ1 = Γ ∪ {xi = vi} and
Δ2 = Λ ∪ {xi (cid:3)= vi} be two nogoods of P . We deﬁne Con-
straint Resolution as C-Res(Δ1, Δ2) = Γ ∪ Λ.

It is immediate that C-Res(Δ1, Δ2) is a nogood of P .

1 = (cid:4)δ1, . . . , δg1 , . . . , δgk

Proposition 2 Let P be a CN and Σ be the sequence of de-
cisions taken along a branch of the search tree. For any nld-
subsequence Σ(cid:2) = (cid:4)δ1, . . . , δi(cid:5) of Σ, the set Δ = pos(Σ(cid:2)) ∪
{¬δi} is a nogood of P (called reduced nld-nogood).
Proof. We suppose that Σ contains k ≥ 1 negative de-
cisions, denoted by δg1 , . . . , δgk ,
they
appear in Σ. The nld-subsequence of Σ with k negative
decisions is Σ(cid:2)
Its corre-
sponding nld-nogood is Δ1 = {δ1, . . . , δg1 , . . . , δgk−1 ,
. . . , ¬δgk
}, δgk−1 being now the last negative decision.
The nld-subsequence of Σ with k − 1 negative decisions is
(cid:5). Its corresponding nld-nogood
Σ(cid:2)
is Δ2 = {δ1, . . . , δg1 , . . . , ¬δgk−1
}. We now apply C-Res
between Δ1 and Δ2 and we obtain Δ(cid:2)
1 = C-Res(Δ1, Δ2) =
{δ1, . . . , δg1 , . . . , δgk−2, . . . , δgk−1−1, δgk−1+1,
}.
The last negative decision is now δgk−2 , which will be
eliminated with the nld-nogood containing k − 2 negative
decisions. All the remaining negative decisions are then
eliminated by applying the same process. 2

2 = (cid:4)δ1, . . . , δg1 , . . . , δgk−1

in the order that

(cid:5).

. . . , ¬δgk

One interesting aspect is that the space required to store
all nogoods corresponding to any branch of the search tree is

1The notation {δ1, . . . , ¬δi} corresponds to {δj ∈ Σ | j <

i} ∪ {¬δi}, reduced to {¬δ1} when i = 1.

IJCAI-07

132

δ2

¬δ3

δ3

δ1

¬δ2

δ6

δ4

¬δ4

δ7

¬δ7

δ5

¬δ5

¬δ6

δ8

δ9

¬δ9

δ10

¬δ10

δ11 ¬δ11

Figure 1: Area of nld-nogoods in a partial search tree

polynomial with respect to the number of variables and the
greatest domain size.
Proposition 3 Let P be a CN and Σ be the sequence of de-
cisions taken along a branch of the search tree. The space
complexity to record all nld-nogoods of Σ is O(n2d2
) while
the space complexity to record all reduced nld-nogoods of Σ
is O(n2d).

Proof. First, the number of negative decisions in any branch
is O(nd). For each negative decision, we can extract a (re-
duced) nld-nogood. As the size of any (resp. reduced) nld-
nogood is O(nd) (resp. O(n) since it only contains positive
decisions), we obtain an overall space complexity of O(n2d2
)
(resp. O(n2d)). 2

4 Nogood Recording from Restarts

In [Gomes et al., 2000], it has been shown that the runtime
distribution produced by a randomized search algorithm is
sometimes characterized by an extremely long tail with some
inﬁnite moment. For some instances, this heavy-tailed phe-
nomenon can be avoided by using random restarts, i.e. by
restarting search several times while randomizing the em-
ployed search heuristic. For constraint satisfaction, restarts
have been shown productive. However, when learning is
not exploited (as it is currently the case for most of the aca-
demic and commercial solvers), the average performance of
the solver is damaged (cf. Section 6).

Nogood recording has not yet been shown to be quite con-
vincing for CSP (one noticeable exception is [Katsirelos and
Bacchus, 2005]) and, further, it is a technique that leads,
when uncontrolled, to an exponential space complexity. We
propose to address this issue by combining nogood recording
and restarts in the following way: reduced nld-nogoods are
recorded from the last (and current) branch of the search tree
between each run. Our aim is to beneﬁt from both restarts and
learning capabilities without sacriﬁcing solver performance
and space complexity.

Figure 1 depicts the partial search tree explored when the
solver is about to restart. Positive decisions being taken

ﬁrst, a δi (resp. ¬δi) corresponds to a positive (resp. neg-
ative) decision. Search has been stopped after refuting δ11
and taking the decision ¬δ11. The nld-nogoods of P are
the following: Δ1 = {δ1, ¬δ2, ¬δ6, δ8, ¬δ9, δ11}, Δ2 =
{δ1, ¬δ2, ¬δ6, δ8, δ9}, Δ3 = {δ1, ¬δ2, δ6}, Δ4 = {δ1, δ2}.
The ﬁrst reduced nld-nogood is obtained as follows:

Δ(cid:2)

1 = C-Res(C-Res(C-Res(Δ1, Δ2), Δ3), Δ4)

= C-Res(C-Res({δ1, ¬δ2, ¬δ6, δ8, δ11}, Δ3), Δ4)
= C-Res({δ1, ¬δ2, δ8, δ11}, Δ4)
= {δ1, δ8, δ11}

Applying the same process to the other nld-nogoods, we

obtain:

Δ(cid:2)
Δ(cid:2)
Δ(cid:2)

2 = C-Res(C-Res(Δ2, Δ3), Δ4) = {δ1, δ8, δ9}.
3 = C-Res(Δ3, Δ4) = {δ1, δ6}.
4 = Δ4 = {δ1, δ2}.

In order to avoid exploring the same parts of the search
space during subsequent runs, recorded nogoods can be ex-
ploited. Indeed, it sufﬁces to control that the decisions of the
current branch do not contain all decisions of one nogood.
Moreover, the negation of the last unperformed decision of
any nogood can be inferred as described in the next section.
For example, whenever the decision δ1 is taken, we can infer
¬δ2 from nogood Δ(cid:2)

4 and ¬δ6 from nogood Δ(cid:2)
3.

Finally, we want to emphasize that reduced nld-nogoods
extracted from the last branch subsume all reduced nld-
nogoods that could be extracted from any branch previously
explored.

5 Managing Nogoods

In this section, we now show how to efﬁciently exploit re-
duced nld-nogoods by combining watched literals with prop-
agation. We then obtain an efﬁcient propagation algorithm
enforcing GAC on all learned nogoods that can be collec-
tively considered as a global constraint.

5.1 Recording Nogoods

Nogoods derived from the current branch of the search tree
(i.e. reduced nld-nogoods) when the current run is stopped
can be recorded by calling the storeN ogoods function (see
Algorithm 1). The parameter of this function is the sequence
of literals labelling the current branch. As observed in Sec-
tion 3, a reduced nld-nogood can be recorded from each neg-
ative decision occurring in this sequence. From the root to
the leaf of the current branch, when a positive decision is en-
countered, it is recorded in the set Δ (line 4), and when a
negative decision is encountered, we record a nogood from
the negation of this decision and all recorded positive ones
(line 9). If the nogood is of size 1 (i.e. Δ = ∅), it can be di-
rectly exploited by reducing the domain of the involved vari-
able (see line 7). Otherwise, it is recorded, by calling the
addN ogood function (not described here), in a structure ex-
ploiting watched literals [Moskewicz et al., 2001].

We can show that the worst-case time complexity of
storeN ogoods is O(λpλn) where λp and λn are the num-
ber of positive and negative decisions on the current branch,
respectively.

IJCAI-07

133

else

Δ ← Δ ∪ {δi}

if δi is a positive decision then

Algorithm 1 storeNogoods(Σ = (cid:4)δ1, . . . , δm(cid:5))
1: Δ ← ∅
2: for i ∈ [1, m] do
3:
4:
5:
6:
7:
8:
9:
10:
end if
11:
12: end for

with δi = (X, v), remove v from dom(X)

addNogood(Δ ∪ {¬δi})

if Δ = ∅ then

else

end if

pick and delete X from Q
if | dom(X) | = 1 then

let v be the unique value in dom(X)
if checkWatches(X = v) = f alse then return false

Algorithm 2 propagate(S : Set of variables) : Boolean
1: Q ← S
2: while Q (cid:6)= ∅ do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end while
14: return true

if dom(Y ) = ∅ then return false
else Q ← Q ∪ {Y }

end if
for each C | X ∈ vars(C) do

for each Y ∈ V ars(C) | X (cid:6)= Y do

if revise(C,Y ) then

5.2 Exploiting Nogoods

Inferences can be performed using nogoods while establish-
ing (maintaining) Generalized Arc Consistency. We show it
with a coarse-grained GAC algorithm based on a variable-
oriented propagation scheme [McGregor, 1979]. The Algo-
rithm 2 can be applied to any CN (involving constraints of any
arity) in order to establish GAC. At preprocessing, propagate
must be called with the set S of variables of the network
whereas during search, S only contains the variable involved
in the last positive or negative decision. At any time, the prin-
ciple is to have in Q all variables whose domains have been
reduced by propagation.

Initially, Q contains all variables of the given set S (line
1). Then, iteratively, each variable X of Q is selected (line
If dom(X) corresponds to a singleton {v} (lines 4 to
3).
7), we can exploit recorded nogoods by checking the consis-
tency of the nogood base. This is performed by the function
checkW atches (not described here) by iterating all nogoods
involving X = v as watched literal. For each such nogood,
either another literal not yet watched can be found, or an in-
ference is performed (and the set Q is updated).

The rest of the algorithm (lines 8 to 12) corresponds to the
body of a classical generic coarse-grained GAC algorithm.
For each constraint C binding X, we perform the revision
of all arcs (C, Y ) with Y (cid:3)= X. A revision is performed by
a call to the function revise, speciﬁc to the chosen coarse-
grained arc consistency algorithm, and entails removing val-
ues that became inconsistent with respect to C. When the re-
vision of an arc (C, Y ) involves the removal of some values
in dom(Y ), revise returns true and the variable Y is added
to Q. The algorithm loops until a ﬁxed point is reached.

The worst-case time complexity of checkW atches is
O(nγ) where γ is the number of reduced nld-nogoods stored
in the base and n is the number of variables2. Indeed, in the
worst case, each nogood is watched by the literal given in pa-
rameter, and the time complexity of dealing with a reduced
nld-nogood in order to ﬁnd another watched literal or make
an inference is O(n). Then, the worst-case time complexity
of propagate is O(er2dr + n2γ) where r is the greatest con-
straint arity. More precisely, the cost of establishing GAC
(using a generic approach) is O(er2dr) when an algorithm
such as GAC2001 [Bessiere et al., 2005] is used and the cost

2In practice, the size of reduced nld-nogoods can be far smaller

than n (cf. Section 6).

of exploiting nogoods to enforce GAC is O(n2γ).
Indeed,
checkW atches is O(nγ) and it can be called only once per
variable.

The space complexity of the structures introduced to man-
age reduced nld-nogoods in a backtracking search algorithm
is O(n(d + γ)). Indeed, we need to store γ nogoods of size at
most n and we need to store watched literals which is O(nd).

6 Experiments
In order to show the practical interest of the approach de-
scribed in this paper, we have conducted an extensive experi-
mentation (on a PC Pentium IV 2.4GHz 512Mo under Linux).
We have used the Abscon solver to run M(G)AC2001 (de-
noted by MAC) and studied the impact of exploiting restarts
(denoted by MAC+RST) and nogood recording from restarts
(denoted by MAC+RST+NG). Concerning the restart policy,
the initial number of allowed backtracks for the ﬁrst run has
been set to 10 and the increasing factor to 1.5 (i.e., at each
new run, the number of allowed backtracks increases by a 1.5
factor). We used three different variable ordering heuristics:
the classical brelaz [Brelaz, 1979] and dom/ddeg [Bessiere
and R´egin, 1996] as well as the adaptive dom/wdeg that has
been recently shown to be the most efﬁcient generic heuris-
tic [Boussemart et al., 2004; Lecoutre et al., 2004; Hulubei
and O’Sullivan, 2005; van Dongen, 2005]. Importantly, when
restarts are performed, randomization is introduced in brelaz
and dom/ddeg to break ties. For dom/wdeg, the weight
of constraints are preserved from each run to the next one,
which makes randomization useless (weights are sufﬁciently
discriminant).

In our ﬁrst experimentation, we have tested the three al-
gorithms on the full set of 1064 instances used as bench-
marks for the ﬁrst competition of CSP solvers [van Don-
gen, 2005]. The time limit to solve an instance was ﬁxed
to 30 minutes. Table 1 provides an overview of the results
in terms of the number of instances unsolved within the time
limit (#timeouts) and the average cpu time in seconds (avg
time) computed from instances solved by all three methods.
Figures 2 and 3 represent scatter plots displaying pairwise
comparisons for dom/ddeg and dom/wdeg. Finally, Table 2
focuses on the most difﬁcult real-world instances of the Radio
Link Frequency Assignment Problem (RLFAP). Performance
is measured in terms of the cpu time in seconds (no timeout)

IJCAI-07

134

dom/ddeg

brelaz

dom/wdeg

#timeouts
avg time
#timeouts
avg time
#timeouts
avg time

365
125.0
277
85.1
140
47.8

MAC

+ RST
378
159.0
298
121.7
123
56.0

+ RST + N G
337
109.1
261
78.2
121
43.6

Table 1: Number of unsolved instances and average cpu time
on the 2005 CSP competition benchmarks, given 30 minutes
CPU.

and the number of visited nodes. An analysis of all these re-
sults reveals three main points.

Restarts (without learning) yields mitigated results. First,
we observe an increased average cpu time for all heuristics
and fewer solved instances for classical ones. However, a
close look at the different series reveals that MAC+RST com-
bined with brelaz (resp. dom/ddeg) solved 27 (resp. 32)
less instances than MAC on the series ehi. These instances
correspond to random instances embedding a small unsatis-
ﬁable kernel. As classical heuristics do not guide search to-
wards this kernel, restarting search tends to be nothing but
an expense. Without these series, MAC+RST would have
solved more instances than MAC (but, still, with worse per-
formance). Also, remark that dom/wdeg renders MAC+RST
more robust than MAC (even on the ehi series).
Nogood recording from restarts improves MAC perfor-
mance. Indeed, both the number of unsolved instances and
the average cpu time are reduced. This is due to the fact that
the solver never explores several times the same portion of
the search space while beneﬁting from restarts.

Nogood recording from restarts applied to real-world in-
stances pays off. When focusing to the hardest instances [van
Dongen, 2005] built from the real-world RLFAP instance
scen-11, we can observe in Table 2 that using a restart policy
allows to be more efﬁcient by almost one order of magnitude.
When we further exploit nogood recording, the gain is about
10%.

Finally, we noticed that the number and the size of the re-
duced nld-nogoods recorded during search were always very
limited. As an illustration, let us consider the hardest RL-
FAP instance scen11 − f 1 which involves 680 variables and
a greatest domain size of 43 values. MAC+RST+NG solved
this instance in 36 runs while only 712 nogoods of average
size 8.5 and maximum size 33 were recorded.

7 Conclusion
In this paper, we have studied the interest of recording no-
goods in conjunction with a restart strategy. The beneﬁt of
restarting search is that the heavy-tailed phenomenon ob-
served on some instances can be avoided. The drawback is
that we can explore several times the same parts of the search
tree. We have shown that it is quite easy to eliminate this
drawback by recording a set of nogoods at the end of each
run. For efﬁciency reasons, nogoods are recorded in a base
(and so do not correspond to new constraints) and propaga-
tion is performed using the 2-literal watching technique in-

scen11-f12

scen11-f10

scen11-f8

scen11-f7

scen11-f6

scen11-f5

scen11-f4

scen11-f3

scen11-f2

scen11-f1

cpu

nodes

cpu

nodes

cpu

nodes

cpu

nodes

cpu

nodes

cpu

nodes

cpu

nodes

cpu

nodes

cpu

nodes

cpu

nodes

0.85
695
0.95
862
14.6
14068
185
207K
260
302K
1067
1327K
2494
2826K
9498
12M
29K
37M
69K
93M

MAC

+ RST
0.84
477
0.82
452
1.8
1359
9.4
9530
21.8
22002
105
117K
367
419K
1207
1517K
3964
5011K
9212
12M

+ RST + N G
0.84
445
1.03
636
1.9
1401
8.4
8096
16.9
16423
82.3
90491
339
415K
1035
1286K
3378
4087K
8475
10M

Table 2: Performance on hard RLFAP Instances using the
dom/wdeg heuristic (no timeout)

troduced for SAT. One can consider the base of nogoods as a
unique global constraint with an efﬁcient associated propaga-
tion algorithm.

Our experimental results show the effectiveness of our
approach since the state-of-the-art generic algorithm MAC-
dom/wdeg is improved. Our approach not only allows to
solve more instances than the classical approach within a
given timeout, but also is, on the average, faster on instances
solved by both approaches.

Acknowledgments
This paper has been supported by the CNRS and the ANR
“Planevo” project noJC05 41940.

References
[Baptista et al., 2001] L. Baptista, I. Lynce, and J. Marques-Silva.
Complete search restart strategies for satisﬁability. In Proceed-
ings of SSA’01 workshop held with IJCAI’01, 2001.

[Bayardo and Shrag, 1997] R.J. Bayardo and R.C. Shrag. Using
CSP look-back techniques to solve real-world SAT instances. In
Proceedings of AAAI’97, pages 203–208, 1997.

[Bessiere and R´egin, 1996] C. Bessiere and J. R´egin. MAC and
combined heuristics: two reasons to forsake FC (and CBJ?) on
hard problems. In Proceedings of CP’96, pages 61–75, 1996.

[Bessiere et al., 2005] C. Bessiere, J.C. R´egin, R.H.C. Yap, and
Y. Zhang. An optimal coarse-grained arc consistency algorithm.
Artiﬁcial Intelligence, 165(2):165–185, 2005.

[Boussemart et al., 2004] F. Boussemart, F. Hemery, C. Lecoutre,
and L. Sais. Boosting systematic search by weighting constraints.
In Proceedings of ECAI’04, pages 146–150, 2004.

[Brelaz, 1979] D. Brelaz. New methods to color the vertices of a

graph. Communications of the ACM, 22:251–256, 1979.

[Dechter, 1990] R. Dechter. Enhancement schemes for constraint
processing: backjumping, learning and cutset decomposition. Ar-
tiﬁcial Intelligence, 41:273–312, 1990.

[Dechter, 2003] R. Dechter. Constraint processing. Morgan Kauf-

mann, 2003.

IJCAI-07

135

 1000

 100

T
S
R
+
C
A
M

 10

 1000

 100

G
N
+
T
S
R
+
C
A
M

 10

 1000

 100

G
N
+
T
S
R
+
C
A
M

 10

 1

 1

 10

 100

 1000

MAC

 1

 1

 10

 100

 1000

MAC

 1

 1

 10

 100

 1000

MAC+RST

Figure 2: Pairwise comparison (cpu time) on the 2005 CSP competition benchmarks using the dom/ddeg heuristic

 1000

 100

T
S
R
+
C
A
M

 10

 1000

 100

G
N
+
T
S
R
+
C
A
M

 10

 1000

 100

G
N
+
T
S
R
+
C
A
M

 10

 1

 1

 10

 100

 1000

MAC

 1

 1

 10

 100

 1000

MAC

 1

 1

 10

 100

 1000

MAC+RST

Figure 3: Pairwise comparison (cpu time) on the 2005 CSP competition benchmarks using the dom/wdeg heuristic

[Focacci and Milano, 2001] F. Focacci and M. Milano. Global cut
framework for removing symmetries. In Proceedings of CP’01,
pages 77–92, 2001.

[Lecoutre et al., 2004] C. Lecoutre, F. Boussemart, and F. Hemery.
Backjump-based techniques versus conﬂict-directed heuristics.
In Proceedings of ICTAI’04, pages 549–557, 2004.

[Frost and Dechter, 1994] D. Frost and R. Dechter. Dead-end

driven learning. In Proc. of AAAI’94, pages 294–300, 1994.

[Ginsberg, 1993] M. Ginsberg. Dynamic backtracking. Artiﬁcial

Intelligence, 1:25–46, 1993.

[Gomes et al., 2000] C.P. Gomes, B. Selman, N. Crato, and
H. Kautz. Heavy-tailed phenomena in satisﬁability and constraint
satisfaction problems. Journal of Automated Reasoning, 24:67–
100, 2000.

[Hulubei and O’Sullivan, 2005] T. Hulubei and B. O’Sullivan.
Search heuristics and heavy-tailed behaviour. In Proceedings of
CP’05, pages 328–342, 2005.

[Hwang and Mitchell, 2005] J. Hwang and D.G. Mitchell. 2-way
In Proceedings of CP’05, pages

vs d-way branching for CSP.
343–357, 2005.

[Katsirelos and Bacchus, 2003] G. Katsirelos and F. Bacchus. Un-
In Proceedings of

restricted nogood recording in CSP search.
CP’03, pages 873–877, 2003.

[Katsirelos and Bacchus, 2005] G. Katsirelos and F. Bacchus. Gen-
In Proceedings of AAAI’05, pages

eralized nogoods in CSPs.
390–396, 2005.

[Marques-Silva and Sakallah, 1996] J.P. Marques-Silva and K.A.
Sakallah. Conﬂict analysis in search algorithms for propositional
satisﬁability. RT/4/96, INESC, Lisboa, Portugal, 1996.

[McGregor, 1979] J.J. McGregor. Relational consistency algo-
rithms and their application in ﬁnding subgraph and graph iso-
morphisms. Information Sciences, 19:229–250, 1979.

[Mitchell, 2003] D.G. Mitchell. Resolution and constraint satisfac-

tion. In Proceedings of CP’03, pages 555–569, 2003.

[Moskewicz et al., 2001] M. W. Moskewicz, C. F. Madigan,
Y. Zhao, L. Zhang, and S. Malik. Chaff: Engineering an Efﬁ-
cient SAT Solver. In Proc. of DAC’01, pages 530–535, 2001.

[Prosser, 1993] P. Prosser. Hybrid algorithms for the constraint sat-
isfaction problems. Computational Intelligence, 9(3):268–299,
1993.

[Sabin and Freuder, 1994] D. Sabin and E. Freuder. Contradicting
conventional wisdom in constraint satisfaction. In Proceedings
of CP’94, pages 10–20, 1994.

[Schiex and Verfaillie, 1994] T. Schiex and G. Verfaillie. Nogood
recording for static and dynamic constraint satisfaction problems.
IJAIT, 3(2):187–207, 1994.

[van Dongen, 2005] M.R.C. van Dongen, editor. Proceedings of

CPAI’05 workshop held with CP’05, volume II, 2005.

IJCAI-07

136


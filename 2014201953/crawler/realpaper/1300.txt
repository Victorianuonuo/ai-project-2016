Session  10  Natural  Language: 
Systems 

IDIOLECTIC  LANGUAGE-ANALYSIS  FOR 

UNDERSTANDING DOCTOR-PATIENT DIALOGUES* 

Horace Enea and Kenneth Mark Colby 

Department  of Computer  Science 

Stanford University 
Stanford, California 

ABSTRACT 

A  programming  language  is  described  which  is  designed to 
simplify  the  construction  of  computer  programs  to  analyze 
English.  This  system  attempts  to  merge  the  best  features  of 
pattern  matchers  and 
the  phrase  structure  approach  to 
language  analysis.  Several  practival  problems  which  occur  in 
dealing with such a system are described. 

INTRODUCTION 

multiple  word-senses  each  possessing  semantic 
features, 
programs  or  rules  for  restricting  word  combinations.  Such 
parsers  perform  a  detailed  analysis  of  every  word,  valiantly 
disambiguating  at  each  step  in  an  attempt  to  construct  a 
meaningful 
interpretation.  While  It  may  be  sophisticated 
computationally,  a  conventional  parser  is  quite  at  a  loss  to 
deal  with  the  caprice  of  ordinary  conversation. 
In  everyday 
discourse  people  speak  colloquially  and  idiomatically  using  all 
sorts  of  pat  phrases,  slang  and  cliches.  The  number  of 
special-case  expressions  is  indefinitely  large.  Humans  are 
cryptic  and  elliptic.  They  lard  even  their  written  expressions 
with  meaningless 
fragments.They  convey  their 
intentions  and  ideas  in  idiosyncratic  and  metaphorical  ways, 
blithely  violating  rules  of 'correct'  grammar  and syntax.  Given 
these  difficulties,  how is  it  that  people carry on conversations 
easily  most  of  the  time  while  machines  thus  far  have  found  it 
extremely  difficult  to  continue  to  make  appropriate  replies 
indicating  some  degree  of  understanding? 

fillers  and 

Why  is  it  so  difficult  for  machines  to  understand 
natural 
language?  Perhaps  it  is  because  machines  do  not 
simulate  sufficiently  what  humans  do  when  humans  process 
language.  Several  years  of experience with computer science 
and 
linguistic  approaches  have  taught  us  the  scope  and 
limitations  of  syntactic  and  semantic  parsers.(Schank,Tesler and 
Weber,8  Simmons,9  Winograd,13  Woods1").  While  extant 
linguistic  parsers  perform  satisfactorily  with  carefully  edited 
text  sentences  or  with  small  dictionaries  ,  they  are  unable  to 
deal  with  everyday  language  behavior  characteristic  of  human 
conversation. 
for  certainty  and 
attracted  by  an  analogy  from  the  proof  theory  of  logicians  in 
which  provability  implied  computability,  computational  linguists 
hoped  to  develop  formalisms  for  natural  language  grammars. 
But  the  hope  has  not  been  realized  and  perhaps  in  principle 
cannot  be. 
(It  is  difficult  to  formalize  something  which  can 
hardly  be  formulated). 

rationalistic  quest 

In  a 

Linguistic  parsers  use  morphographemic  analyses, 
parts-of-speech  assignments  and  dictionaries  containing 

*  This  research is supported by Grant  PHS MH 06645-12 from 
the  National  Institute  of  Mental  Health,  by  (in  part)  Research 
Scientist  Award  (No.  1-K05-K14.433) 
from  the  National 
Institute  of  Mental  Health  to  the  second  author  and  (in  part) 
by  the  Advanced  Research  Projects  Agency  of  the  Office  of 
the  Secretary  of  Defense(SD-183). 

It  seems  that  people  'get  the  message1  without 
always  analyzing  every  single  word  in  the  input.  They  even 
ignore  some  of  its  terms.  People  make  individualistic  and 
idiosyncratic  selections  from  highly  redundant  and  repetitious 
communications.  These  personal  selective  operations,  based 
on  idiosyncratic  intentions,  produce  a  transformation  of  the 
input  by  destroying  and  even  distorting  information.  In  speed 
reading,  for  example,  only  a  small  percentage  of  contentive 
words  on  each  page  need  be  looked  at.  These  words 
somehow  resonate  with  the  readers  relevant  conceptual-
belief  structure  whose  processes  enable  him  to  'understand1 
not  simply  the  language  but  all  sorts  of  unmentioned  aspects 
about  the  situations  and  events  being  referred  to  in  the 
language.  Normal  written  English text  is estimated  to  be  5/6 
redundant 
Spoken 
than  507. 
conversations 
redundant(Carroll').  Words  can  be  garbled  and  listeners 
nonetheless  get  the  gist  or  drift  of  what  is  being  said.  They 
see the "pattern" and thus can supply much of what is missing 

(Rubenstein 
in  English  are  prooably  better 

Haberstroh7). 

and 

from 

incorporate 

that  of  current 

To  approximate  such  human  achievements  we 
require  a  new  perspective  and  a  practical  method  which 
linguistic  approaches.  This 
differs 
alternate  approach  should 
those  aspects  of 
parsers  which  have  been  found  to  work  well,  e.g.,  detecting 
embedded  clauses.  Also  individualistic  features  characteristic 
of  an 
idiolect  should  have  dominant  emphasis.  Parsers 
represent  complex  and  refined algorithms.  While  on one hand 
they  subject  a  sentence 
to  a  detailed  and  sometimes 
overkilling  analysis,  on 
finicky  and 
oversensitive.  A  conventional  parser  may  simply  halt  if  a 

the'  other,  they  are 

278 

word  in  the  input  sentence  is  not  present  in  its  dictionary.  It 
finds  ungrammatical  expressions  such  as  double  prepositions 
('Do  you  want 
to  get  out  of  from  the  hospital?')  quite 
confusing.  Parsers  constitute  a  tight  conjunction  of  tests 
rather  than  a  loose  disjunction.  A6  more  and  more tests are 
added  to  the  conjunction,  the  parser  behaves  like  a  finer and 
finer 
for  an 
expression  to  pass  through.  Parsers  do  not  allow  for  the 
exclusions typical  of everyday human dialogues. 

filter  which  makes 

increasingly  difficult 

it 

Finally,  it  is  difficult  to  keep  consistent  a  dictionary 
of  over  500  multiple-sense  words  classified  by  binary 
semantic  features  or  rules.  For  example,  suppose  a  noun (Ni) 
is  used  by  some  verbs  as  a  direct  object  in  the  semantic 
sense  of  a  physical  object.  Then  it  is  noticed  that  Ni  is  also 
used  by  other  verbs  in the sense of a location so 'location1 is 
added  to  Ni's  list  of  semantic  features.  Now  Ni  suddenly 
qualifies  as  a  direct  object  for  a lot  of other  verbs.  But  some 
of  the  resultant  combinations  make  no  sense  even  in  an 
idiolect.  If  a  special  feature  is  then  created  for  Ni,  then  one 
loses 
the  power  of  general  classes  of  semantic  features. 
Adding  a  single  semantic  feature can  result  in  the propagation 
of  hidden  inconsistencies  and  unwanted  side-effect.,  as  the 
dictionary  grows  it  becomes  increasingly  unstable  and  difficult 
to  control. 

Early  attempts 

to  develop  a  pattern-matching 
special-purpose  heuristics  have  been 
approach  using 
described  by  Colby,  Watt  and  Gilbert,2  Weizenbaum"  and 
Colby  and  Enea.3  The  limitations  of  these  attempts  are  well 
known  to  workers  in  artificial  intelligence.  The  man-machine 
conversations  of  such  programs  soon  becomes  impoverished 
and  boring.  Such  primitive  context-restricted  programs  often 
grasp  a  topic  well  enough  but  too  often  do  not  understand 
quite  what  is  being  said  about  the  topic,  with  amusing  or 
disastrous consequences. This shortcoming is a consequence of 
the  limitations  of  a  pattern-  matching  approach  lacking  a  rich 
conceptual  structure  into  which  the  pattern  abstracted  from 
the  input  can  be  matched  for  inferencing.  These  programs 
also  lack  a  subroutine  structure,  both  pattern  directed  and 
specific, desirable  for generalizations and further analysis. 

The  strength  of  these  pattern  matching  approaches 
lies  in  their  ability  to  ignore  some  of  the  input.  They  look  for 
patterns,  which  means  the  emphasis  of  some  detail  to  the 
exclusion  of  other  detail.  Thus  they can get  something out  of 
nearly  every sentence--  sometimes more, sometimes less. 

interesting  pattern-matching  approach 

An 
for 
machine 
translation  has  been  developed  by  Wilks.12  His 
program  constructs  a  pattern  from  English  text  input  which is 
matched  against  templates  in  an  interlingual  data  base  from 
which,in  turn,  French  output  is  generated  without  using  a 
generative  grammar. 

In 

the  course  of  constructing  an 

interactive 
simulation  of  paranoia  we  were  faced  with  the  problem  of 
dealing with unedited and unrestricted natural language as it is 
the  doctor-patient  situation  of  a  psychiatric 
used 

in 

domain  of  discourse  admittedly 

interview.(Colby,  Hilf,  Weber,  and  Kraemer,"  Colby  and  Hilf5). 
This 
contains  many-
psychiatricelly  stereotyped  expressions  and  is  constrained  in 
topics  (Newton's  laws  are  rarely  discussed).  But  it  is  rich 
enough  in  verbal  behavior  to  be  a  challenge  to  a  language 
understanding  algorithm  since  a  variety  of  human  experiences 
are  discussed domain  including the  interpersonal  relation which 
develops  between  the  interview  participants,  A  look  at  the 
contents  of  a  thesaurus  reveals that  words  relating to  people 
and their interrelations make up roughly  707 of English words. 

interview, 

The  diagnosis  of  paranoia  is  made  by  psychiatrists 
relying  mainly  on  the  verbal  behavior  of  the  interviewed 
patient. 
If  a  paranoid  model  is  to  exhibit  paranoid  behavior in 
a  psychiatric 
it  must  be  capable  of  handling 
dialogues  typical  of  the  doctor-patient  context.  Since  the 
model  can  communicate  only  through  teletyped  messages.the 
vis-a-vis  aspects  of 
interview  are 
absent.  Therefore  the  model  must  be  able  to  deal  with 
unedited  typewritten  natural  language  input  and  to  output 
replies  which  are  indicative  of  an  underlying  paranoid thought 
process during the episode of  a psychiatric interview. 

the  usual  psychiatric 

In  an  interview  there  is  always  a  who  saying 
something to  a whom with definite intentions  and expectations. 
There  are  two  situations  to  be  taken  into  account,  the  one 
being  talked  about  and  the  one  the  participants  are  in. 
Sometimes  the  latter  becomes  the  former.  Participants  in 
dialogues  have  intentions  and  dialogue  algorithms  must  take 
this  into  account.  The  doctor's  intention  is  to  gather  certain 
kinds  of  information  while  the  patient's  intention  is  to  give 
information  and  get  help.  A  job  is to  be done;  it  is not  small 
talk.  Our  working  hypothesis  is  that  each  participant  in  the 
the  other  by  matching  selected 
dialogue  understands 
input  against 
idiosyncratically-  significant  patterns 
conceptual  patterns  which  contain 
the 
linguistically.  This 
situation  or  event  being  described 
understanding 
linguistic 
responses 
intentions  and 
expectations  of  the  participants  and  to  the  requirements  of 
the  situation.  In  this  paper  we  shall  describe  only  our current 
input-analyzing  processes  used  to  extract  a  pattern  from 
In  a  later  communication  we  shall 
natural  language  input. 
describe 
the 
conceptual  level  once a pattern has been received by memory 
from  the  input-analysing  processes. 

is  communicated 
judged  appropriate 

inferential  processes  carried  out  at 

in  the 
information  about 

reciprocally  by 
to 

the 

the 

Studies  of  our  1971  model  of  paranoia  (PARRY) 
indicated  that  about  thirty  percent  of  the  sentences were not 
understood  at  all  ,  that  is,  no  concept  in  the  sentence  was 
recosnized. 
in  a  somewhat  larger  number  of  cases  some 
concepts,  but  not  all,  were  recognized.  In  many  cases these 
partially  recognized  sentences  lead  to  a  partial  understanding 
that  was  sufficient  to  gather  the  intention  of  the  speaker  end 
thus 
to  output  an  appropriate  response.  However, 
misunderstandings occurred too often. For example: 

lead 

279 

DOCTOR:  How  o ld 

is  your  mother  ? 

PARRY:  T w e n t y - e i g ht 

("dec").  The  inverse  operation,  recomposition,  constructs  an 
output  stream  under  the  direction  of  a  recomposition  pattern 
("rec"). 

PARRY  has  interpreted  the  question  as  referring  to  his  own 
age  and  answered by giving  his age.  The purpose of our new 
language  analysis  system  is  to  significantly  raise  the  level  of 
understanding  by  preventing such  misunderstandings while  not 
restricting  what  can  be  said  to  PARRY.  We  do  not  expect 
complete  under-  standing  from  this  system  ~  even  native 
speakers  of  the  language  do  not  completely  understand  the 
language. 

By 'understanding  we mean me system can do some 

or  all  of the  following: 

1)  Determine 
interviewer 
utterance. 

intention  of 

the 
a  particular 

the 

in  making 

2)  Make  common  logical  deductions  that 
follow from the interviewers utterance 

3)  Form  an  idioletic  internal  representation 
of  the  utterance  so that  questions  may  be 
answered,  commands  carried  out,  or  data 
added to  memory. 

4}  Determine  references  for  pronouns,  and 
other  anaphora. 

5)  Deduce  the  tone  of  the  utterance,!.e., 
hostile, insulting... 

6)  Classify 
rejoinder.command,... 

the 

input  as  a  question, 

The  approach  we  are  taking consists of  merging the 
best  features  of  pattern  directed  systems  such  as  the  MAD 
DOCTOR,2  ELIZA"  and  parsing  directed  systems  for  example, 
Winograd,13  Woods.14.  By  merging  the  BNF  phrase  structure 
approach  ot  analyzing  English  with  the  pattern  matching 
approach,  with  its  attendant  emphasis of some concepts to the 
exclusion  of  others.  The  programs  to  accomplish  this  are 
written  in  ML1SP2,  an  extensible  version  of  the  programming 
language  ML[$P,5,10  and  uses  an  interpreted  version  of  the 
pattern  matcher  designed  for  a  new  programming  language 
LISP 70. 

The  following  is  a  basic  description  of  the  pattern 
matcher.  We  shall  illustrate  its  operation  using  examples  of 
problems  common to teletyped psychiatric dialogues. 

PATTERN  MATCHING 

Pattern  directed  computation  involves  two  kind  of 
operations 
and 
recomposition.  Decomposition  breaks  down  an  input  stream 
into components under the direction of a decompostion pattern 

decomposition 

structures: 

data 

on 

280 

A rewrite rule is of the form; 

dec  -->  r ec 

It  defines  a  partial  function on streams as follows: if the input 
stream  matches  the dec,  then the  output  stream  Is  generated 
by  the  rec.  The  following  rule  (given  as  an  example  only) 
could be  part  of  a question answering function: 

How  a re  you  ?  -»  Very  well  and  you  ? 

If  the  input  stream consists  of the four tokens: 

How  are  you  ? 

the  output  stream  will  consist  of the  five tokens: 

Very  w e II  and  you  ? 

REWRITE  FUNCTIONS 

A 

rewrite  rule  defines  a  partial 

for 
example,  the  mapping  of  some  particular  token  into  some 
other  particular  token.  A  broader  partial  function  can  be 
defined  as  the  union  of  several  rewrite  rules.  A  rewrite 
function definition is of the form: 

function, 

RULES  OF  <name>  -

decl 
-*  r e e l, 
dec2  -»  rec2, 

decn  -»  r e c n; 

VARIABLES 

A  function  is  difficult  to  define  if  every  case  must 
be  enumerated.  Therefore,  rewrite  rules  allow  variables  to 
appear  in  patterns.  The  value  of  a  variable  can  be  either  a 
list  or  an  atom.  In this paper the notation: 

:X 

where  X  ia  any  identifier,  will  denote  the  variable  X.  The 
variables  of  each  rule  are  distinct  from  the  variables  of  all 
other  rules,  even if their names are the same. 

The  following definition has only ihree rewrite rules, 

but  handles  an unlimrted number of input  streams: 

281 

surrounding  a  pattern  means  that  the  current  input 
The 
stream  is  to  be  pushed  down,  that  the  function  indicated  by 
the  first  token  within  the  brackets  is  to  be  entered  with the 
rest  of  the  pattern  appended  to  the  front  of the  input  stream, 
and  that  the  output  stream  is  to  be  placed  into  the  restored 
current  input  stream.  Note  that  MLISP2  functions  may  be 
called as well  as  rewrite functions. 

GOALS 

To  gain  the  advantage  of  goal  directed  pattern 
matching  and  computing, as  well  as the  full  power  of context 
sensitive grammars, the following form may be used: 

names  a 
The  identifer  between  the  angled  brackets 
rewrite  function  the  rules  of  which are to  be  matched  against 
the  input  stream.  When  a  match occurs the  output  stream of 
the  goal  will  be bound to the associated variable. Example; 

occurs  in  the  input  stream  it  will  be  bound to  :N.  :N1  will  be 
bound  to  2. 
If  the  <NEGATIVE>  does  not  occur,  :N1  will  be 
bound to  1. On the rec side of the rules if :Nl  is 2 then :N will 
be  placed  in  the  output  stream. 
If  it  is  1  then  nothing  is 
placed  in  the  output  stream at  that  point.  Example, given the 
rule  above: 

MORE  EXAMPLES 

We  have collected a large number of dialogues using 
our  previous  program  PARRY.  These  dialogues  form  a  large 
body  of  examples  of  the  kind  of  English which we can expect. 
Martin  Frost,  a  graduate  student 
in  Computer  Science, 
.Stanford  University,  has  written  a  keyword  in  context 
program  which  enables  us  to  isolate  examples  centered  on 
particular  words  so  that  uses  of  thoses  words  in  context 
become  more  apparent.  Our  general  approach  is  to  build  a 
system  which  can  produce  desired  intreptations  from  these 
examples  and  to  incrementally  add  to  the  rules  in  the  system 
as  new  cases  are  discovered  during  the  running  of  the 
program. 

Following  are  some  examples  of  commonly  occuring 
situations  and  examples of  the  kind  of rules we  use to handle 
them. 

OPTIONALS 

Many  other  shorthands  exist 

to  simplify  writing 
rules.  One  useful  feature  that  will  be  mentioned  here  is  the 
optional. 

RULES  OF  AUXILARY_PHRASE  -

(AUX_PH  :A  [:"N):Nl  >: 

<AUXlLARY>iA 

[<NEGATIVE>:N3  N1 

If  the  optional  pattern,  enclosed  in  square  brackets  {"[]"), 

Although  it  is  conceivable  that  there  are  an infinite 
number  of  ways  to  introduce  a  question  in  this  manner,  we 
have  found  only  about  six  literal  strings  are  actually  used  in 
our  data  base  of  dialogues.  When  we  discover  a new  string 
we  incrementally add a rule.  When we have  enough examples 

282 

to  dectect  a  more  genera) 
farm  we  replace  the  rules  for 
<QUESTIONJNTR0DUCER>  by  a  more  elegant  and  general 
formulation.  This  approach  allows  us  to  process  dialogues 
before  we  have  a  complete  analysis  of  all  possible  sentence 
constructions,  and  it  allows  us  to  build  a  language  analyzer 
based  on  actually  occurring  forms. 

dialogues  is  made  up  of  a  great  number  of  cliches  and  idioms, 
therefore  we  anticipate  a  large  number  of  rules  devoted  to 
them.  For  example: 

RULES  OF  TIME_PHRASES  -

A COUPLE OF  <TIME_UNIT>;T  AGO 
*  (TIHE  (RELATIVE  PAST)(REF  PRESENT)  :T); 

it 

Notice  that 

is  possible  to  make  more  than  one 
analysis  of  any  given  sentence  depending  on  what  is  being 
looked 
for.  A  poet  might  be  interested  in  the  number  of 
syllables  per  word  and.  the  patterns  of  stress.  A  "full" 
analysis  of  English  must  allow  for  this  possibility,  but  it  it 
clearly  foolish  to  produce  this  kind  of  analysis  for  PARRY.  Our 
analysis  will  be  partial  and  idiosyncratic  to  the  needs  of  our 
program.  This  is  what  is  meant  by  idiolectic. 

F I L L E RS 

it 

is  quite  common 

to  introduce 
words  of  fittle  significance  to  PARRY  into  the  sentence.  For 
example: 

interviewers 

for 

HELL,  WHAT  IS  YOUR  NAHE? 

The  "well"  in  this  sentence  serves  no  purpose  in  PARRY'S 
analysis,  although  it  might  to  a  linguist  interested  in  hesitation 
phenomena.  These  fillers  can  be  ignored  The  following  rules 
accomplish  this: 

RULES  OF  SENTENCE  -

<F]LLERS>;F  <SENTENCE>:S  - 

:S; 

RULES  OF  FILLERS  -

WELL  - 
OK  -  ; 

, 

PUNCTUATION 

Interviewers  use  little  intra-sentence  punctuation  in 
talking  to  PARRY.  When  it  is  used  it  is  often  to  seperate 
phrases  that  might  otherwise  be  ambiguous.  Example: 

WHY WEREN'T  YOU  VERY  CLOSE,  FRANK 

Here  the  comma  clearly  puts  "CLOSE"  in  a  different  phrase 
from  "FRANK".  Punctuation,  when  used  in  PARRY'S  rules, 
is 
generally  enclosed  in  optional  brackets  ("[]").  This  has  the 
effect  of  separating  phrases  when  punctuation  is  used,  but  not 
requiring  full  punctuation  for  the  system  to  work,  Example; 

RULES  OF  SENTENCE  -

<SENTENCE>:S1  C.JtC  <SENTENCE_CONNECTOR>:SC 

<SENTENCE>:S2 

-  (CONUNCTION  SC  :S1  :S2)| 

CLICHES  AND 

IDIOMS 

RULES  OF  TINE_UN1T  -

SECONDS  -  WITHIN  CONVERSATION), 
MOMENTS  -  WITHIN  CONVERSATION), 
DAYS  -  (BEFORE  CONVERSATION  DAYS): 

REPRESENTATION  CORRECTION 

Intermediate  results  are  often  produced  which  are 
misleading  in  meaning  or  are  in  the  wrong  form  for  further 
processing.  We,  therefore,  incorporate  at  various  points  rules 
which  detect  certain  undesired 
results  and 
convert  them  to  the  desired  form.  Example: 

intermediate 

RULES  OF  CORRECT FORM -

(QUESTION  . .. 

(SENTENCE  . . , ))  -

(QUESTION 

): 

U N K N O WN  WORDS 

Rules  can  be  derived  to  handle  words  which  were 

previously  unknown  to  the  system.  For  example: 

RULES OF UNKNOWN_WORD -

OR'.  )X  -  <NEW_W0RD  NAME  :X>, 
THE  :X  <VERB_PHRASE>:Y  * 

<NEU_U0RD NOUN  :X>, 

I  :X  YOU  -»  <NBJ_U0R0  VERB  :X>i 

Here  "NEW_WORD"  is  a  function  which  adds  new  words  to the 
dictionary. 

CONCLUSION 

improved 

We  are  faced  with  the  problems  of  natural  language 
being  used  to  interview  people  in  a  doctor-patient  context. 
We  have  developed  a  language  processing  system  which  we 
believe  is  capable  of  performing  in  these  interviews  at  a 
significantly 
level  of  performance  compared  to 
systems  used  in  the  past.  We  have  developed  techniques 
which can  measure performance in comparison with the ideal 
of  a  real  human  patient  in  the  same  context,"'5,7  We  are 
designing our  system with the realization that  a long period of 
development 
levels  of 
performance,  This  is  a  system  that  can  work  at  a  measured 
level  of  performance  and  be  improved  over  time  with  new 
rules  having  minimum  interaction  with  those  already  existing. 
Our  system  is  designed  so  that  a  complete  analysis  of  every 
word  or  phrase  of  an utterance  is  not  neceesary. 

reach  desired 

is  necessary 

to 

The  English  we 

encounter 

in  doctor-patient 

The  basis  of  this  system  is  a  rewrite  interpreter 
which  will  automatically  merge  new  rules  into  the  set  of 

263 

already  existing  rules  so  that  the  system  will  continue  to 
handle  sentences which It  handled in the past. 

REFERENCES 

'Carroll,  J.B.  Language  and  Thought.  Prentice-Hall,  Englewood 

Cliffs,  New Jersey, p.  59. 

2Colby,  K.M.,  Watt, J.  and  Gilbert, J.P.  A  computer  method 
of  psychotherapy.  Journal  of  Nervous  and  Mental  Disease, 
142,148-152,1966. 

3Colby,K.M.  and  Enea,H.  Heuristic  methods 

for  computer 
understanding 
of  natural  language  in  context  restricted 
on-line  dialogues.  Mathematical  Biosciences,l,  1-25,1967. 

4Colby,  K.M.,  Hilf,  F.D.,  Weber,  S.,  and  Kraener,  H.  Turing-like 
indistinguishability  tests  for  the  validation  of  a  computer 
simulation 
Artificial 
lntelligence.3,199-221,1972. 

processes. 

of 

paranoid 

5Colby,  KM.  and  Hilf,  F.D.  Multidimensional  analysis 

in 
the  adequacy  of  a  simulation  of  paparnoid 
evaluating 
processes.  Memo  AIM-194.  Stanford  Artificial  Intelligence 
project,  Stanford  University. 

6Enea,  K  MLISP,  Technical  report  no.  CS-92,  1968,  Computer 

Science Department,  Stanford University. 

7Rubenstein,  A.H.  and  Haberstroh,  C.  J,,  Some  Theories  of 

Organization, Dorsey Press, Homewood Hl.,1960, p. 232. 

8Schank,  R.C.,  Tesier,  L  and  Weber.S.  Spinora  ii:  Conceptual 
case-based natural  language analysis. Memo AIM-109,  1970, 
Stanford Artificial  Intelligence Project, Stanford University. 

9Simmons,  R.f.  Some  semantic  structures 

for  representing 
English  meanings.  Preprint,  1970,  Computer  Science 
Department, University of Texas, Austin. 

l0Smith,  D.C.,  MLISP,  Memo  AIM-135,  1970,  Stanford  Artificial 

Intelligence  Project,  Stanford University. 

11 'Weizenbaum, J.  Eliza-  a computer program for the study of 
communication  between  man  and  machine. 

natural 
Communications of the ACM,  9,36-45,1966. 

12Wilke, Y.A. Understanding without proofs. (See this volume). 

13Winograd,  T.  A  program  for  understanding  natural  language. 

Cognitive  Psychology,3,l-l91,1972 

14Woods,  W.A.  Transition  network  grammars 

for  natural 
language  analysts.  Communications  of  the  ACMJ 3,591-
606,1970. 

264 

MECHANISM  OF  DEDUCTION 
A  QUESTION  ANSWERING  SYSTEM 
W I TH  N A T U R AL  LANGUAGE 
INPUT 

IN 

Makoto  Nagao  and  Jun-ichi  T s u j ii 
Kyoto  U n i v e r s i t y,  Dept.  Elec.  Eng. 
Kyoto,  Japan 

ABSTRACT 

We  have  constructed  a  deductive  question  answer(cid:173)

ing  system  which  accepts  natural  language  input  in 
Japanese. 
The  semantic  trees  of  aseertional  input 
sentences  are  stored  in  a  semantic  network  and  i n t e r(cid:173)
relationships  — c o n d i t i o n a l,  i m p l i c a t i o n a l,  and  so 
f o r t h—  are  established  among  them. 
A  matching  r o u t(cid:173)
ine  looks  for  the  semantic  trees  which  have  some  r e l a(cid:173)
tions  to  a  query,  and  returns  the  mismatch  information 
(difference) 
routine  produces  sub-goals  to  diminish  t h is  difference. 
This  process 
takes  place  recursively  u n t il  the  d i f f e r(cid:173)
ence  is  completely  resolved  (success),  or  there  is  no 
other  p o s s i b i l i ty  of  matching  in  the  semantic  network 
( f a i l u r e ). 
Standard  problem  solving  techniques  are 
As  the  result  the  system  is 
used  in  t h is  process. 
very  powerful  in  handling  deductive  responses. 
In 
t h is  paper  only  the  part  of  the  l o g i c al  deduction  is 
explained  in  d e t a i l. 

to  a  deduction  r o u t i n e. 

The  deduction 

DESCRIPTIVE  TERNS:  question  answering,  deduction, 
natural  language,  semantic  network,  problem  solving. 

I 

INTRODUCTION 

There  are  a  few  deductive  question  answering  systems 
using  natural  language,  almost  a ll  of  which  use  l o g i c al 
expressions,  especially  the 
culus  expression,  as  an  intermediate  language. 
ever  systems  which  use  formal  logics  have  problems: 
(1)  Syntactic  and  semantic  analyses  of  natural  language 
input  are  necessary  to  transform  the  input  to  l o g i(cid:173)
cal  expression  without  ambiguity. 

f i r st  order  predicate  c a l(cid:173)
How(cid:173)

(2)  The  axiom  set  must  be  clearly  defined  and  must  not 

be  contradictory. 

(5)  Predicates  and  variables  must  be  fixed  beforehand. 

This  is  a  problem  for  the  system's  expansion. 
Also  t h is  prevents  mixing  the 
f i r st  and  higher 
order  predicate  calculus  systems. 

(4)  Deduction  using  the  resolution  principle 

is  cumber(cid:173)
Usually  question  answering  does  not  require 

some. 
a  deep  deductive  process. 

(5)  Good  q u a l i ty  of  natural  language  output  is  very  . 

hard  to  obtain  from  a  l o g i c al  expression. 

To  avoid  the  above  problems  we  have  used  a  kind  of 

Session  10  Natural  Language:  Systems 

semantic  representation  of  natural 
as  an  intermediate  expression. 
following  characteristic 
(1)  The  question  answering  system  is  a  composite  of  sub(cid:173)
systems  for  language  analysis,  deduction,  and  lang(cid:173)
uage  generation. 

language  sentences 

pur  systern  has  the 

features. 

(2)  The  parsed  trees  of  sentences  are  permitted  to  have 
Ambiguities  are  resolved  in  the 

some  ambiguities. 
process  of  l o g i c al  deduction, 

(3)  During  the  question  answering  process, 

the  deduction 

a b i l i ty  is  increased  and  the  area  which  the  system 
can  deal  with  is  also  expanded. 
a b i l i ty  of  a  system  depends  on  how  many  theorems 
the  system  can  use,  and  on  how  e f f i c i e n t ly  it  can 
deal  with  them.  We  have  constructed  a  system  in 
which  the  available  theorems  increase  during  the 
question  answering  process. 

The  deduction 

(4)  Facts  can  play  the  role  of  theorems.  We  think  the 
d i s t i n c t i on  between  facts  and  theorems  is  not  clear 
enough. 
one  time  and  as  a  fact  at  another  time. 
example, 

A  statement  can  be  used  as  a  theorem  at 

For 

A  human  is  an  i n t e l l i g e nt  animal, 

plays  the  role  of  a  theorem  to  answer 

Is  Smith  i n t e l l i g e nt  ? 

because  Smith  is  an  instance  of  a  variable 
On  the  contrary  it  plays  the  role  of  a  fact  to  the 
question 

'human'. 

Is  a  man  an  animal  ? 

'a  human' 
'man'. 

is  treated  as  an  instance  of  a 

because 
variable 
In  our  system  the  assertions  given  by  a  user,  which 
correspond  to  facts  in  usual  systems,  can  play  the 
role  of  theorems. 
a  higher  concept  term  to  be  a  variable 
concept  term. 
There  is  no  d i s t i n c t i on  between 
them,  and  both  facts  and  theorems  have  the  same 
structures  in  the  data  base. 
This  is  the  most 
s i g n i f i c a nt  character  of  the  system  we  have  develop(cid:173)
ed. 

This  is  accomplished  by  allowing 

to  i ts  lower 

(5)  In  order  to  deal  with  a  large  data  base, 

the  system 

has  a  well  organized  data  structure  and  relevant 
information  to  a  question  is  accessed  by  a  t e c h n i(cid:173)
que  of  indexing  and  r e t r i e v a l, 

(6)  The  deduction  process  is  similar  to  that  of  humans. 

It  allows  introducing  many  heuristics  i n to  the 
deduction  process. 

In  t h is  paper  the  d e t a i ls  of  deduction  subsystem  alone 
are  explained. 
published  elsewhere  in  the  near 

The  other  two  subsystems  w i ll  be 

future, 

II  SYSTEM  ORGANIZATION 

A  block  diagram  of  our  system  is  shown  in  F i g.  1. 

The  internal  data  base  of  the  system  is  divided  i n to 
two  parts: 
( 1;  semantic  representations  (semantic 

trees)  of  input 

sentences. 

2S5 

(2)  network  (mutual  connection)  of  ( 1 ). 
The  mutual  connection  consists  of  i n t e r r e l a t i o n s h i ps 
such  as  c o n d i t i o n a l,  i m p l i c a t i o n a l,  and  so  f o r t h. 
input  sentence 
is  read  i n to 
and  is  not  in  the  network  y e t. 
lates  in  a  very  natural  way  in  the  question  answering 
process. 
An  inverted  f i le  of  keywords  makes  it  easy 
to  extract  information  relevant  to  the  question. 

An 
t r e e,  and  it 
the  semantic  network  if  it  is  an  assertion 
Thus  knowledge  accumu-

is  analyzed  i n to  a  semantic 

The  parsing  routine  performs  syntactic  and  seman(cid:173)
t ic  analyses  of  an  input  query  sentence,  and  produces 
the  parse  t r e e. 
accepts  the 
which  contains  sentences  already  accepted. 

tree  and  r e l a t es  it  to  the  semantic  network 

A  network  administration  routine 

To  accomplish  a  deduction,  there  are  two  main 

parts: 
the  execution  routine  and  the  deduction  r o u t i n e. 
The  execution  r o u t i n e,  which  plays  the  central  role  in 
the  deduction  process,  searches  through  the  network  for 
sentences  relevant  to  the  current  goal  and  matches  them 
one  by  one  against  i t. 
The  deduction  routine  manages 
the  global  information  in  the  problem  solving  process 
such  as  goal-subgoal  relationships,  variable  bindings 
( f or  example 
•Smith'),  and  so  f o r t h. 
execution  routine  to  determine  which  sentence  must  be 
v e r i f i ed 

This  routine  also  directs  the 

is  bound  to  the  word 

the  word 

f i r s t. 

'man' 

IIl  KNOWLEDGE  STRUCTURE 

3.1  Semantic  Trees. 

We  have  applied  a  kind  of  dependency  analysis  to 
the  input  Japanese  sentences. 
A  noun  modified  by  an 
adjective  is  transformed  i n to  a  kernel  sentence  having 
another  kernel  sentence  related  to  the  noun. 
sentence 

The 

KINBEN MA WITO WA SEIKO SURU 
(  A  d i l i g e nt  man  w i ll  succeed.) 

is  divided  i n to  two  sentences  l i ke 

HITO WA SEIKO SURU 
(  A  man  w i ll  Bucceed.) 

and 

HITO WA KINBEN DA 
(  A  man  is  d i l i g e n t .) 

The  parsed  tree  stfueture  of  t h is  sentence  is  shown  in 
Fig. 2. 

Some  sentences  in  Japanese  have  two  possible  sub(cid:173)
that  i s,  one  which  contains  the  reference 

j e ct  phrases, 
p a r t i c le  'GA'  and  the  other  which  contains  'WA*.  We 
consider  the  r e l a t i o n al  phrase  w i th  the  p a r t i c le 
i n d i c a t i ng  what  the  sentence  t a l ks  about; 
with 
predicate  in  the  sentence. 

the  phrase 
is  the  subject  phrase  corresponding  to  the 

'GA' 

'WA'  as 

ZO  WA HANA  GA  NAGAI 
(  Elephant  has  a  long  nose.) 

is  a  t y p i c al  example. 
I ts  l i t e r al 
"  As  for  elephant  the  nose  is  l o n g ." 
ture  of  it 

is  shown  in  F i g,  3. 

t r a n s l a t i on  is 

The  tree  s t r u c(cid:173)

the 

Sentences  connected  by  AND  or  OK  are  represented  in 
tree  structure  as  shown  in  F i g.  4. 
A  sentence  which  contains  upper  concept  terms 

replaceable  by  t h e ir  lower  concept 
terms  is  considered 
as  a  theorem  available  to  prove  a  statement  which  has 
the  lower  concept  terms  in  i t. 
So  upper-lower  concept 
r e l a t i o n s h ip  among  words  plays  an  important  role  in  our 
system. 
meaning  A  is  a  lower  concept  of  B,  and  B  is  an  upper 
concept  of  A,  has  a  special  structure 
to  express  the 
"  NINGEN  WA  KASHIKOI  DOBUTSU  DA" 
r e l a t i o n s h ip  c l e a r l y. 
(  A  man  is  an  i n t e l l i g e nt  animal.)  is  parsed  as  shown  in 
F i g.  5. 

The  input  sentence  in  the  form  of  "  A  WA  B  DA" 

Properties  of  sentences  are  attached  to  the  top 

node  of  the  parsed  tree  s t r u c t u r e. 
treated  are  p o t e n t i a l,  a c t i v e,  passive,  subjenctive, 
tense,  and  so  f o r t h. 
ed  as  true,  so  that  a  sign  T  is  given  to  the  property 
part  of  the  parsed  t r e e. 
property  part  indicate 
i v e l y. 

The  signs  F  and  U  in  the 

false  and  undetermined  respect(cid:173)

The  assertion  sentence  is  regard(cid:173)

The  properties  we 

3.2  Semantic  Network. 

The  network  is  constructed  in  the 
In  the  case  of  an  assertion  sentence  S„, 
1'. 
ed  in  the 

form  shown  in  F i g.  6a. 

following  way. 

it  is  s t o r(cid:173)

(1) 

286 

{1)  Branches  in  the  network  and  trees  are  b i - d i r e c t i o n(cid:173)

for 

al 
search  in  the  deduction  process. 

transformation  and 

f l e x i b le 

f or  e f f i c i e nt 

(2)  Words  are  not  stored  in  nodes  of  the  parsed  trees 
but  by  a  pointer  to  the  l e x i c al  entry  of  the  word 
( F i g.  7 ). 

( 3)  The 

l e x i c al  e n t ry  of  a  w o r d,  c a l l ed  NLIST,  c o n t a i ns 

n ot  o n ly 
a l so  a 
of 
NLIST 

l e x i c al 

l i st  of  sentences 

i n f o r m a t i on  about 
( p o i n t e rs 

the  w o r d,  b ut 
the  e n t r i es 
to 

the  sentences 

in  SLIST)  which  c o n t a i ns 

t he  w o r d. 

is  a  k i nd  of 

f i le  of  keywords. 

CO  The  node  of 

the  network 

i n d i c a t ed  by  a  p o i n t er 

t a b l e,  c a l l ed  SLIST,  which  c o n t a i ns  i n f o r m a(cid:173)

i n v e r t ed 
is 

from  a 
t i on  about 
whether 
d e t e r m i n ed 

the  sentence 

the  s e n t e n c e. 
is 
( U ),  and  so 

(5)  D i f f e r e nt  nodes 

in 

The 

i n f o r m a t i on  of 

t r ue 
f o r th 

( P ),  or  u n(cid:173)
in 
l i s t. 
the  network  correspond 

( T ), 
f a l se 
is  s t o r ed 

t h is 
to  d i f(cid:173)

f e r e nt  s e n t e n c e s. 
a  sentence  can  be  r e t r i e v ed 
the  n e t w o r k. 

As  a  r e s u l t, 

i n f o r m a t i on  about 
in 

from  a  s i n g le  node 

IV  EXECUTION  ROUTINE 

' t he 

T h is 

is  r e a l i z ed  by 

t h is  s t u dy 
"the 

the  d e d u c t i on  r o u t i n e. 

law  of  s u b s t i t u t i o n'  and 

in 
the  use  of 
i m p l i c a t i o n .' 

Among  many  i n t e l l e c t u al  a b i l i t i es  of  humans,  we 
the  d e d u c t i on  a b i l i ty 
implemented 

have 
baaed  on 
law  of 
r o u t i ne  and 
r o u t i ne 
t r i es 
a n o t h er  one, 
l o w er  c o n c e p t s. 
over 
duces  subgoals  and 
t e l ls 
sentence  must  be  v e r i f i ed 
searches 
are  e q u i v a l e nt 
t i on  r o u t i n e. 
s e a r c h,  m a t c h i n g,  and  r e s o l v i ng  d i f f e r e n c e s. 

The  e x e c u t i on 
to  match  a  sentence  s t r u c t u re  a g a i n st 
r e g a r d i ng  an  upper  concept  as  a  v a r i a b le 
The  d e d u c t i on  r o u t i ne  p r o(cid:173)
the  e x e c u t i on  r o u t i ne  which 
f i r s t. 
f or 

the  network 
the  g o al  sentence  g i v en  by 

the  sentences  which 

t h r o u gh 
to 

It  c o n s i s ts  of 

i ts 

the  deduc(cid:173)

t h r ee  main  p a r t s:  keyword 

The  e x e c u t i on  r o u t i ne 

the  e x e c u t i on 

4.1  Keyword  S e a r c h. 

The  system  has  an  i n v e r t ed 

f i le  of  words  c a l l ed 

By  u s i ng 

t h is 

f i l e, 

the  e x e c u t i on  r o u t i ne 

t a k es 

the  sentences  which  c o n t a in  words 

in 

the  g o al  s e n t(cid:173)

These  s e l e c t ed  sentences  a re  presumed  to  be 
to 

the  c u r r e nt  s e n t e n c e. 

NLIST. 
out 
e n c e. 
r e l e v a nt 

(2)  In  the  case  of  a  negation  sentence,  schematically 
form 
w r i t t en  as 
ae  F i g.  6a,  but  the  property  part  is  w r i t t en  as  F. 

it  is  stored  in  the  same 

'not  S2 

(3)  If  a  sentence  is  - If  s1, 

in  the 

form  shown  in  F i g.  6b. 

then  S 

. ', 

it  is  stored 

(4)  If  a  sentence 

is 

'Because  S1,  S2. 

it 

is  stored 

in  the 

form  shown  in  F i g.  6c. 

(5)  If  the  sentences  S1  and  S2  in  <1)--(4)  are 

found  in 

they  are  not  stored  newly, 
the  semantic  network, 
but  the  stored  ones  are  used. 
For  example  the 
following  sentences  are  stored  in  the  network  as 
shown  in  F i g.  6d. 

Because  S1,  S2. 
then  S3. 
If  S1, 

In  t h is  case  because  S1  is  asserted  as  t r u e,  S3  is  also 
t r u e. 

5 

The  network  and  parsed  trees  have  the 

i n t e r n al  constructions. 

following 

287 

4.2  Hatching  Method 

The  matching  algorithm  is  constructed  so  that  two 

parsed  trees  which  are  d i f f e r e nt  in  the  sequence  of 
branches  <Fig.  8)  w i ll  be  matched  successfully  by  the 
branch  labels  on  the  parsed  trees.  Matching  between 
two  parsed  trees  f a i ls  for  various  reasons. 
causes  of  mismatch,  named  differences,  are  c l a s s i f i ed 
i n to  the 
(1)  N-difference:  The  words  which  are  attached  to  the 
corresponding  node  are  d i f f e r e nt  in  the  two  sent(cid:173)
ences. 
F i g.  9a  shows  an  example,  where  the  d i f(cid:173)
ference  is  expressed  as  (N  (*C  * D ) ). 
pointer  to  the  node  C. 

following  four  classes. 

The 

(2)  S1-differenee:  One  structure  ( f i r st  argument)  has 

extra  branches  which  the  other  does  not  have. 
F i g.  9b  shows  an  example  of  t h is  category,  abbre(cid:173)
viated  as  CS1  ((*R4)  - B ) ),  which  shows  the  branch 
R4 is  the  extra  one. 

*C  shows  the 

(3)  S2-difference:  One  structure  (second  argument)  has 

extra  branches. 
difference  is  shown  by  (s2  (*C  (*R5))). 

F i g.  9c  is  an  example  and  t h is 

(4)  SO-difference:  Both  structures  have  extra  branches. 

An  example  is  shown  is  F i g.  9d. 
The  matching  subroutine 
t r i es 

argument  against  i ts  second  one. 
succeeds, 
the  subroutine  returns 
uction  r o u t i n e. 

If  not, 

it  returns 

to  match  i ts 
If  the  matching 

f i r st 

'success' 

to  the  ded(cid:173)

the  differences. 

(N  CC  'D)) 

(S1  ((•R4) 

'B)) 

286 

F i g.  11  is  an  example. 
the 
two  structures,  S-structure  and  T-structure,  are  equi(cid:173)
valent  and  the  difference  is  resolved. 

If  the  matching  succeeds, 

V  DEDUCTION  ROUTINE 

The  deduction  routine  controls  the  whole  of  the 
This  routine  has  a  global  know(cid:173)
This  knowledge  contains  the 

deduction  process. 
ledge  of  the  process. 
goal-subgoal  organization,  variable  binding  and  so  f o r t h, 
The  deduction  routine 
sentence  must  be  v e r i f i ed  and  which  sentence, 
f i r st  t r i al 

t e l ls  the  execution  routine  which 

f a i l s,  has  to  be  v e r i f i ed  next. 

if  the 

5.1  Goal  Organization 

The  deduction  method  in  our  system  takes  a  ques(cid:173)

t r i al 

f a i l s, 

the  deduction  routine  searches 

the  sentences  stored  in  the  network. 

t i on  Q  as  a  .goal  and  t r i es  to  v e r i fy  it  by  means  of 
matching  it  with 
If  the 
through  the  network  for  such  sentences  as  P-*Q. 
Those  sentences  P's, 
to  accomplish  the  previous  goal. 
In  the  same  manner 
sub-subgoals  are  produced  to  accomplish  the  subgoals. 
As  the  process  advances,  many  goals  are  produced  h i e r(cid:173)
a r c h i c a l l y. 
remember  the  h i e r a r c h i c a l ly  organized  relationships 
among  goals. 

An  AND-OR  tree  structure  is  used  to 

if  any,  are  considered  as  subgoals 

Subgoals  are  created  in  various  cases. 

(1)  If  a  goal  sentence  G  can  not  be  determined  to  be 
true  or  f a l s e,  subgoals  are  created  by  means  of 
searching  through  the  network  for  the  sentences 
which  are  antecedents  of  G. 

(2)  In  the  same  case  of  (1), 

the  negations  of  conseque(cid:173)

nces  of  G  are  taken  as  subgoals. 
proved  to  be  true, 
be  f a l s e. 

the  sentence  G  is  determined  to 

If  they  are 

(3)  If  the  matching  between  two  parsed  trees  is  i n(cid:173)

complete,  subgoals  to  diminish  the  mismatches  are 
created. 
In  addition  to 

these  cases,  subgoals  are  also 

produced  when  a  goal  is  divided  i n to  several  subgoals. 
For  example  'KARE  WA  KINBEN  DE  SHOJIKI  DA'  (He  is 
d i l i g e nt  and  honest)  is  divided  i n to  'KARE  WA  KINBEN 
DA'  (He  is  d i l i g e n t ),  and  'KARE  WA  SHOJIKI  DA'  (He  is 
honest). 

The  goals  are  t r i ed  one  by  one,  and  when  there 
the  deduction  process  stops  with  a 

remains  no  goal, 
f a i l u re  message. 
A  goal  which  has  several  subgoals 
w i ll  succeed  or  not,  depending  upon  whether  the  sub-
goals  w i ll  succeed  or  not. 
A  goal  keeps  some  i n f o r(cid:173)
the  information 
mation 
of  whether  it  is  an  AND-type  or  an  OR-type. 
Depth  of 
goal  shows  the  depth  between  the  top-goal  (that  i s,  a 
question  given  by  a  user)  and  the  present  goal. 
The 
top-goal  is  0  and  the  depth  of  the  immed(cid:173)
depth  of  the 
iate  subgoal 
is  1. 

For  example 

for  i t s e l f. 

it  has 

The  indicators  such  as 

The  deduction  routine  chooses  a  goal, 

the  depth 
of  which  is  the  smallest  of  a l l,  and  t e l ls  the  execu(cid:173)
tion  routine  to  v e r i fy  i t. 
KOTEI  (positive  assertion),  HITEI  (negative  assertion), 
MATCH  (to  be  matched)  and  so  f o r th  show  the  effects  of 
the  goals'  r e s u l ts  to  be  transferred  to  t h e ir  previous 
goals. 
the  sentence  corresponding  to  i ts  previous  goal  is 
proved  to  be  true  ( f a l s e ). 
produced  in  order  to  resolve  the  mismatch  between  two 
parsed  trees  have  the  indicator  MATCH. 

KOTEI  (HITEI)  shows  that  if  t h is  goal  succeeds, 

The  subgoals  which  are 

5.2  Variable  Binding. 

To  use  the  law  of  s u b s t i t u t i on  is  one  of  most 

important  a b i l i t i es  in  t h is  system. 
out  by  considering  an  upper  concept  as  a  variable  over 
i ts  lower  concepts. 
it  is  a  lower  concept  of  another  word,  and  as  a  v a r i(cid:173)
able  when  it  is  an  upper  concept  of  another  word. 
We  do  not  introduce  unary  predicates  such  as'human(x)', 

A  word  behaves  as  a  constant  when 

This  is  carried 

289 

' a n l m a l ( x ) ',  which  are  usually  used  in  the  predicate 
calculus  system  in  order  to  r e s t r i ct  the  range  of  v a r i(cid:173)
ables. 

We  regard  a ll  words  as  variables  which  have  t h e ir 

own  domains  of  values.  We  i l l u s t r a te  t h is  by  the 
following  example. 
(1)  HITO  GA  KENKO  NARA-BA  HITO  WA  SEIKO  SURU 
the  man  w i ll  succeed.) 

( If  a  man  is  healthy, 

(Q)  Smith  WA  SEIKO  SURU  KA  ? 

(Will  Smith  succeed  ?) 

f i r s t. 

The  system  searches  through  the  network  to  find  out  the 
sentence  (1)  which  is  expected  to  answer  the  given  ques(cid:173)
The  matching  between  the  consequent  part  of  (1) 
t i o n. 
and  the  question  f a i ls  at 
The  cause  of  mismatch 
is  N  difference  between  'Smith*  and  'HITO  (man)'. 
routine  is  called  to  find  out  that  HITO  is  an  upper 
concept  of  Smith,  which  is  proved  by  the  information 
'Smith  is  a  man.' 
in  the  network. 
Thus  a  subgoal,  the 
antecedent  of  (1), 
in  which  HITO  is  replaced  by  Smith  is 
produced, 
that  i s, 
'Is  Smith  healthy  ? '. 
As  the  deduc(cid:173)
t i on  process  proceeds,  several  such  bind  conditions  are 
produced. 
deration  the  related  bind  conditions  produced  during  the 
former  process. 

Each  goal  must  be  t r i ed  taking  i n to  consi(cid:173)

N 

The  deduction  routine  has  a  stack  to  remember  these 

This  stack  is  i l l u s t r a t ed  in  F i g,  10. 

conditions. 
Each  goal  has  a  pointer  to  t h is  stack  and  the  routine 
can  retrieve 
If  a  goal 
during  the  t r i al  of  the  goal  is  abandoned. 
other  hand  if  a  goal  succeeds, 
ed  for  use  in  the  succeeding  process. 

then  the  bind  condition  generated 
On  the 

the  corresponding  bind  condition  of  a  goal. 

i ts  condition  is  memoriz(cid:173)

f a i l s, 

VI  COMPARISON  WITH  THE  SYSTEMS  USING  PREDICATE  CALCULUS 

formulas. 

In  those  systems 

formula,  store  it 

Those  systems  which  use  predicate  calculus  translate 
input  i n to  a  predicate  calculus 

the 
in  the  data  base,  and  use  a  universal  method  of  deduct(cid:173)
ion  such  as  the  resolution  method. 
common  subexpressions  appearing  in  d i f f e r e nt  sentences 
are  stored  as  many  times  as  they  appear  in  d i f f e r e nt 
l o g i c al 
system  the  same  subexpressions  are  stored  only  once  and 
t h e ir  r e l a t i o ns  to  the  other  parts  of  sentences  are 
stored  by  l i n k s. 
u t i l i z ed  in  the  deduction  process. 
system  deals  with  a  great  amount  of  data  and  only  a 
r e l a t i v e ly  small  portion  of  the  data  has  a  direct  r e l a t(cid:173)
ion  to  the  given  question, 
the  quick  access  to  these  r e(cid:173)
lated  expressions  is  very  important 
process. 

So  these  i n t e r r e l a t i o n s h i ps  can  be 

This  is  not  e f f i c i e n t. 

Especially  when  the 

in  the  deduction 

In  our 

Which  sentences  or  formulas  are  available  for  the 

current  problem  needs  to  be  recognized  e a s i l y,  and  to  do 
t h i s,  a  well  organized  data  base  is  necessary. 
tempting  to  t ry  to  incorporate  the  use  of  property  l i s ts 
to  speed  up  r e s o l u t i o n. 
useful  for  each  object  symbol  c  to  have  access  to  a 
chained  l i st  of  a ll  l i t e r a ls  or  clauses  where  c  occurs. 

For  example  one  may  find  it 

It  is 

A  d i f f i c u lt  but  more  important  problem  is  to 

It  is  desirable 

recognize  how  a  meaningful  unit  is  related  to  another 
u n i t. 
to  contain 
information  about  the  i n t e r r e l a t i o n s h i ps  among  the 
meaningful  u n i t s. 
In  our  system  the  deduction  procedure 
can  retrieve  from  a  node  those  sentences  which  have  some 

for  the  data  base 

290 


Generalized Additive Bayesian Network Classiﬁers

Jianguo Li† ‡ and Changshui Zhang‡ and Tao Wang† and Yimin Zhang†

†Intel China Research Center, Beijing, China

‡Department of Automation, Tsinghua University, China

{jianguo.li, tao.wang, yimin.zhang}@intel.com, zcs@mail.tsinghua.edu.cn

Abstract

Bayesian network classiﬁers (BNC) have received
considerable attention in machine learning ﬁeld.
Some special structure BNCs have been proposed
and demonstrate promise performance. However,
recent researches show that structure learning in
BNs may lead to a non-negligible posterior prob-
lem, i.e, there might be many structures have sim-
ilar posterior scores. In this paper, we propose a
generalized additive Bayesian network classiﬁers,
which transfers the structure learning problem to a
generalized additive models (GAM) learning prob-
lem. We ﬁrst generate a series of very simple BNs,
and put them in the framework of GAM, then adopt
a gradient-based algorithm to learn the combin-
ing parameters, and thus construct a more power-
ful classiﬁer. On a large suite of benchmark data
sets, the proposed approach outperforms many tra-
ditional BNCs, such as naive Bayes, TAN, etc, and
achieves comparable or better performance in com-
parison to boosted Bayesian network classiﬁers.

1 Introduction
Bayesian networks (BN), also known as probabilistic graph-
ical models, graphically represent the joint probability distri-
bution of a set of random variables, which exploit the con-
ditional independence among variables to describe them in
a compact manner. Generally, a BN is associated with a di-
rected acyclic graph (DAG), in which the nodes correspond to
the variables in the domain and the edges correspond to direct
probabilistic dependencies between them [Pearl, 1988].

Bayesian network classiﬁers (BNC) characterize the con-
ditional distribution of the class variables given the attributes,
and predict the class label with the highest conditional prob-
ability. BNCs have been successfully applied in many ar-
eas. Naive Bayesian (NB) [Langley et al., 1992] is the sim-
plest BN, which only consider the dependence between each
feature xi and the class variable y. Since it ignores the de-
pendence between diﬀerent features, NB may perform not
well on data sets which violate the independence assump-
tion. Many BNCs have been proposed to overcome NB’s
[Sahami, 1996] proposed a general framework
limitation.
to describe the limited dependence among feature variables,

called k-dependence Bayesian network (kdB). [Friedman et
al., 1997] proposed tree augmented Naive Bayes (TAN), a
structure learning algorithm which learns a maximum span-
ning tree (MST) from the attributes. Both TAN and kdB have
tree-structure graph. K2 is an algorithm which learns general
BN for classiﬁcation purpose [Cooper and Herskovits, 1992].
The key diﬀerences between these BNCs are their structure
learning algorithms. Structure learning is the task of ﬁnding
out one graph structure that best characterizes the true den-
sity of given data. Many criteria, such as Bayesian scoring
function, minimal description length (MDL) and conditional
independence test [Cheng et al., 2002], have been proposed
for this purpose. However, it is inevitable to encounter such a
situation: several candidate graph structures have very close
score value, and are non-negligible in the posterior sense.
This problem has been pointed out and presented theoretic
analysis by [Friedman and Koller, 2003]. Since candidate
BNs are all approximations of the true joint distribution, it
is natural to consider aggregating them together to yield a
much more accurate distribution estimation. Several works
have been done in this manner. For example, [Thiesson et
al., 1998] proposed mixture of DAG, and [Jing et al., 2005]
proposed boosted Bayesian network classiﬁers.

In this paper, a new solution is proposed to aggregate can-
didate BNs. We put a series of simple BNs into the framework
of generalized additive models [Hastie and Tibshirani, 1990],
and adopt a gradient-based algorithm to learn the combining
parameters, and thus construct a more powerful learning ma-
chine. Experiments on a large suite of benchmark data sets
demonstrate the eﬀectiveness of the proposed approach.

The rest of this paper is organized as follows. In Section
2, we brieﬂy introduce some typical BNCs, and point out the
non-negligible problem in structure learning. In Section 3,
we propose the generalized additive Bayesian network classi-
ﬁers. To evaluate the eﬀectiveness of the proposed approach,
extensive experiments are conducted in Section 4. Finally,
concluding remarks are given in Section 5.

2 Bayesian Network Classiﬁers

A Bayesian network B is a directed acyclic graph that en-
codes the joint probability distribution over a set of random
variables x = [x1, · · · , xd]T . Denote the parent nodes of xi
by Pa(xi), the joint distribution PB(x) can be represented by

IJCAI-07

913

factors over the network structures as follows:

(cid:2)

PB(x) =

d

i=1

P(xi|Pa(xi)).

Given data set D = {(x, y)} in which y is the class vari-
able, BNCs characterize D by the joint distribution P(x, y),
and convert it to conditional distribution P(y|x) for predicting
the class label.

2.1 Several typical Bayesian network classiﬁers
The Naive Bayesian (NB) network assumes that each attribute
variable only depends on the class variable, i.e,

P(x, y) = P(y)P(x|y) = P(y)

(cid:2)

d

i=1

P(xi|y).

Figure 1(a) illustrates the graph structure of NB.

Since NB ignores the dependencies among diﬀerent fea-
tures, it may perform not well on data sets which violate
the attribute independence assumption. Many BNCs have
been proposed to consider the dependence among features.
[Sahami, 1996] presented a more general framework for
limited dependence Bayesian networks, called k-dependence
Bayesian classiﬁers (kdB).

Deﬁnition 1: A k-dependence Bayesian classiﬁer
is a
Bayesian network which allows each feature xi to have a max-
imum of k feature variables as parents, i.e., the number of
variables in Pa(xi) equals to k+1 (‘+1’ means that k does not
count the class variable y).

According to the deﬁnition, NB is a 0-dependence BN. The
kdB [Sahami, 1996] algorithm adopts mutual information
I(xi; y) to measure the dependence between the ith feature
variable xi and the class variable y, and conditional mutual in-
formation I(xi, x j|y) to measure the dependence between two
feature variables xi and x j. Then kdB employs a heuristic rule
to construct the network structure via these two measures.

kdB does not maximize any optimal criterion in structure
learning. Hence, it yields limited performance improvement
over NB. [Keogh and Pazzani, 1999] proposed super-parent
Bayesian networks (SPBN), which assumes that there is an
attribute acting as public parent (called super-parent) for all
the other attributes. Suppose xi is the super parent, denote the
corresponding BN as Pi(x, y), we have
(cid:2)

Pi(x, y) = P(y)P(xi|y)P(x\i|xi, y)

= P(y)P(xi|y)

P(x j|xi, y).

(1)

n

j=1, j(cid:2)i

It is obvious that SPBN structure is a special case of kdB
(k=1). Figure 1(b) illustrates the graph structure of SPBN.
The SPBN algorithm adopts classiﬁcation accuracies as the
criterion to select out the best network structure.

[Friedman et al., 1997] proposed tree augmented Naive
Bayes (TAN), which is also a special case of kdB (k=1). TAN
attempts to add edges to the Naive Bayesian network in order
to improve the posterior estimation. In detail, TAN ﬁrst com-
putes the conditional mutual information I(xi, x j|y) between
any two feature variables xi and x j, and thus obtain a full
adjacency matrix. Then TAN employs the minimum span-
ning tree algorithm (MST) on the adjacency matrix to obtain
a tree-structure BN. Therefore, TAN is optimal in the sense of

MST. Many experiments show that TAN signiﬁcantly outper-
forms NB. Figure 1(c) illustrates one possible graph structure
of TAN.

Both kdB and TAN generate tree-structure graph. [Cooper
and Herskovits, 1992] proposed the K2 algorithm, which
adopts the K2 score measure and exhaustive search to learn
general BN structures.

2.2 The structure learning problem
Given training data D, structure learning is the task of ﬁnd-
ing a set of directed edges G that best characterizes the true
density of the data. Generally, structure learning can be cat-
egorized into two levels: macro-level and micro-level. In the
macro-level, several candidate graph structures are known,
and we need choosing the best one out.
In order to avoid
overﬁtting, people often use model selection methods, such
as Bayesian scoring function, minimum descriptive length
(MDL), etc [Friedman et al., 1997].
In the micro-level,
structure learning cares about whether one edge in the graph
should be existed or not.
In this case, people usually em-
ploy the conditional independence test to determine the im-
portance of edges [Cheng et al., 2002].

However, in both cases, people may face such a situa-
tion that several candidates (graphs or edges) have very close
scores. For instance, suppose MDL is used as the crite-
rion, people may encounter a situation that two candidate BN
structure G1 and G2 have MDL score 0.899 and 0.900, re-
spectively. Which one should be chosen? Someone may say
that it is natural to select G1 out since it has a bit smaller MDL
score, but practice may show that G1 and G2 have similar per-
formance, and G2 may perform even better in some cases. In
fact, both of them are non-negligible in the posterior sense.

This problem has been pointed out and presented theoretic
analysis by [Friedman and Koller, 2003]. It shows that when
there are many models that can explain the data reasonably
well, model selection makes a somewhat arbitrary choice be-
tween these models. Besides, the number of possible struc-
tures grows super-exponentially with the number of random
variables. For these two reasons, we don’t want to do struc-
ture learning directly. We hope aggregating a series of sim-
pler and weaker BNs together to obtain a much more accurate
distribution estimation of the underlying process.

We note that several researchers have proposed some
schemes for this purpose, for examples, learning mixtures of
DAG [Thiesson et al., 1998], or ensembles of Bayesian net-
works by model averaging [Rosset and Segal, 2002; Webb et
al., 2005]. We brieﬂy introduce them in the following.

2.3 Model averaging for Bayesian networks
Since candidate BNs are all approximations of the true distri-
bution, model averaging is a natural way to combine candi-
dates together for a more accurate distribution estimation.

Mixture of DAG (MDAG)
Deﬁnition 2: If P(x| θc, Gc) is a DAG model, the following
equation deﬁnes a mixture of DAG model

(cid:3)

P(x|θs) =

πcP(x|θc, Gc),

c

(cid:4)
where πc is the prior for the c-th DAG model Gc, and πc ≥ 0,

πc = 1, θc is the parameter for graph Gc.

c

IJCAI-07

914

y

x1

x2

x3

. . .

xd

y

y

xi

xj

xj+1

. . .

xd

x1

x2

x3

x4

. . .

xd

(a) Naive Bayesian

(b) Super-parent kdB (k=1)

(c) One possible structure of TAN

Figure 1: Typical Bayesian network structures

GABN is an extensible framework since many diﬀerent
link functions can be considered. In this paper, we study a
special link function: fi(·) = log(·). Deﬁning z = (x, y) and
taking exponent on both sides of the above equation, we have

exp[F(z)] = exp

(cid:5) (cid:3)

(cid:6)

(cid:2)

n

i

λi fi(z)

=

n

i

P

λ

i

i (z).

This is in fact a potential function. It can also be written as a
probabilistic distribution when given a normalization factor,

n(cid:2)

P(z) =

1

S λ(z)

i

P

λ

i

i (z),

where S λ(z) is the normalization factor:

(cid:3)

(cid:7) n(cid:2)

(cid:8)

=

P

i

λ
i (z)

S λ(z) =

(cid:3)

(cid:7) n(cid:3)

exp

λi log Pi(z)

z

i

z

i=1

The likelihood of P(z) is called quasi-likelihood:

L(λ) =

=

=

(cid:3)

k=1

(cid:3)

log P(zk)
(cid:7) n(cid:3)

N

N

λi log Pi(zk) − log S λ(zk)

i=1

(cid:7)

λ · f(zk) − log S λ(zk)

(cid:8)
,

(cid:3)

k=1

N

k=1

(cid:8)
.

(cid:8)

(5)

(6)

(7)

MDAG learns the mixture models via maximizing the
posterior likelihood of given data set.
In detail, MDAG
combining uses the Chesseman-Stutz approximation and the
Expectation-Maximization algorithm for both the mixture
components structure learning and the parameter learning.

[Webb et al., 2005] presented a special and simple case
of MDAG for classiﬁcation purpose, called the average one
dependence estimation (AODE). AODE adopts a series of ﬁx-
structure simple BNs as the mixture components, and directly
assumes that all mixture components in MDAG have equal
mixture coeﬃcient. Practices show that AODE outperforms
Naive Bayes and TAN.

Boosted Bayesian networks
Boosting is another commonly used technique for combin-
ing simple BNs. [Rosset and Segal, 2002] employed the gra-
dient Boosting algorithm [Friedman, 2001] to combine BNs
for density estimation. [Jing et al., 2005] proposed boosted
Bayesian network classiﬁers (BBN), and adopted general Ad-
aBoost algorithm to learn the weight coeﬃcients.

Given a series of simple BNs: Pi(x, y),

i = 1, · · · , n,
BBN aims to construct the ﬁnal approximation by linear ad-
n
αiPi(x, y), where αi ≥ 0 are
ditive models: P(x, y) =
i=1
αi = 1. More generally, the
weight coeﬃcients, and
constraint on αi can be relaxed, but only αi ≥ 0 is kept:
αiPi(x, y). In this case, the posterior can be
F(x, y) =
deﬁned as follows

(cid:4)
(cid:4)

n
i=1

(cid:4)

i

For general binary classiﬁcation problem y ∈ {-1,1}, this prob-
lem can be solved by the exponent loss function

(cid:3)

L(α) =

exp{−yF(xk, y)}

k

(3)

via the AdaBoost algorithm [Friedman et al., 2000].

3 Generalized additive Bayesian networks
In this section, we present a novel scheme that can aggregate
a series of simple BNs to a more accurate density estima-
tion of the true process. Suppose Pi(x, y), i = 1, · · · , n are the
given simple BNs, we consider putting them in the framework
of generalized additive models (GAM) [Hastie and Tibshi-
rani, 1990]. The new algorithm is called generalized additive
Bayesian network classiﬁers (GABN).

In the GAM framework, Pi(x, y) are considered to be linear

additive variables in the link function space:
λi fi[Pi(x, y)].

F(x, y) =

n

(cid:3)

i

P(y|x) =

(cid:4)

exp{F(x, y)}
y(cid:3) exp{F(x, y(cid:3))}

.

(2)

where λ = [λ1, · · · , λn]T , f(zk) = [ f1(zk), · · · , fn(zk)]T .

3.1 The Quasi-likelihood optimization problem

Maximizing the quasi-likelihood, we can obtain the solution
of the additive parameters. To make the GAM model mean-
ingful and tractable, we add some constraints to the parame-
ters. The ﬁnal optimization problem turns to be:

max L(λ)
(cid:4)
s.t.

(1) 0 ≤ λi ≤ 1
λi = 1
(2)

i

(8)

For equation constraint, the Lagrange multiplier can be
adopted to transfer the problem into an unconstraint one;
while for inequation constraints, classical
interior point
method (IPM) can be employed. In detail, the IPM utilizes
barrier functions to transfer inequation constraints into a se-
ries of unconstraint optimization problems [Boyd and Van-
denberghe, 2004].

(4)

IJCAI-07

915

Here, we adopt the most used logarithmic barrier function,
and obtain the following unconstraint optimization problem:

L(λ, rk, α) = rk

(cid:3)

i=1

log(λi) + rk
n

(cid:3)

n

(cid:3)

n

i=1

log(1 − λi)

(cid:9)

i=1

+ α(1 −

λi) + L(λ)
log(λ) + log(1n − λ)

= rk
+ α(1 − λ · 1n) + L(λ),

(cid:10)

· 1n

(9)

where 1n indicates a n-dimensional vector with all elements
equal to 1, rk is the barrier factor in the kth step of the IPM
iteration, and α is the Lagrange multiplier.

Therefore, in the kth IPM iteration step, we need to max-
imize an unconstraint problem L(λ, rk, α). Quasi-Newton
method is adopted for this purpose.

3.2 Quasi-Newton method for the unconstraint

optimization problem

To solve the unconstraint problem: max L(λ, rk, α), we must
have the gradient of L w.r.t λ.
Theorem 1: The gradient of L(λ, rk, α) w.r.t λ is

∂L(λ, rk, α)

∂λ

(cid:3)

(cid:7)

N

k=1
1

1n − λ

= gλ =
(cid:9) 1

+ rk

λ

−

(cid:8)

f(zk) − EP(z)[f(zk)]
(cid:10)

· 1n − α1n.

(10)

Proof: In Equation (10), it is easy to obtain the gradient of
the ﬁrst summation term and non-summation terms. Here, we
only present the gradient solution of the second summation
term, i.e., log S λ(zk) in L(λ).

∂log S λ(z)

∂λ

=
(cid:3)

∵ S λ(z) =
(cid:3)

∂S λ(z)

1

∂S λ(z)

S λ(z)
(cid:7)

exp

z

∂λ

(cid:8)

λ · f(z)
(cid:7)

(cid:8)

(cid:8)

3.5 Discussions

∂λ

∴

∂log S λ(z)

∂λ

=

=

=

λ · f(zk)
(cid:7)
λ · f(zk)

f(zk) exp

f(zk) exp

zk

(cid:3)

zk

1
S λ(z)
(cid:3)

P(zk)f(zk) = EP(z)

zk

(cid:10)

(cid:9)
f(zk)

.

(cid:4)

For computational cost consideration, we did not further
compute the second order derivative of L(λ, rk, α), while
adopted the quasi-Newton method [Bishop, 1995] to solve
the problem. In this paper, the L-BFGS procedure provided
by [Liu and Nocedal, 1989] is employed for this task.

3.3 The IPM based training algorithm

The interior point method starts from a point in the feasible
region, sequentially adjusts the barrier factor rk in each itera-
tion, and solves a series unconstraint problem L(λ, rk, α), k =
1, 2, · · · . The detailed training algorithm is shown in Table 1.

Table 1: The training algorithm for GABN

Input: Given training set D = {(xi
Training Algorithm

, yi)}N
i=1

S0: set convergence precision  > 0, and the maximal step M;
S1: initialize the interior point as λ = [λ1, · · · , λ
= 1/n;
S2: generate a series of simple BNs: Pi(x, y), i = 1, · · · , n;
S3: for k = 1 : M
S4: select rk

> 0 and rk

< rk−1,

n]T , λ

i

obtain the kth step optimization problem L(λ, rk

, α);

S5: calculate gλ and the quasi-likelihood L(λ);
S6: employ L-BFGS procedure to solve: max L(λ, rk
S7: test of the barrier term

, α);

(cid:9)

(cid:10)

ak

= rk

log(λ) + log(1n

− λ)

· 1n;

S8: if ak

<  jump to S9, else continue the loop;

S9: Output the optimal parameter λ∗,

and obtain the ﬁnal generalized models P(z; λ∗).

3.4 A series of ﬁx-structure Bayesian networks

There are one unresolved problem in the algorithm listed in
Table 1, which is in the S2 step, i.e, how to generate a series
of simple BNs as the weak learner. There are many methods
for this purpose. In our experiments, we take super parent
BN as the weak learner. Readers may consider other possible
strategies to generate simple BNs.

For a d-dimensional data set, when setting diﬀerent at-
tribute as the public parent node according to Equation (1),
it can generate d diﬀerent ﬁx-structure super-parent BNs:
Pi(x, y), i = 1, · · · , d. Figure 1(b) depicts one example of
this kind of simple BNs. To improve performance, mutual in-
formation I(xi, y) is computed for removing several BNs with
lowest mutual information score. In this way, we obtain n
very simple BNs, and adopt them as weak learners in GABN.
Parameters (conditional probabilistic table) learning in
BNs is common, and thus details are omitted here. Note that
for robust parameter estimation, Laplacian correction and m-
estimate [Cestnik, 1990] are adopted.

GABN has several advantages over the typical linear additive
BN models: Boosted BN (BBN). First, GABN is much more
computational eﬃcient than BBN. Given d-dimensional and
N samples training set, it is not hard to prove that the com-
putational complexity of GABN is O(Nd2 + MNd), where M
is the IPM iteration steps. On the contrary, BBN requires se-
quentially learning BN structures in each boosting step. This
leads to a complexity of O(KNd2), where K is the boosting
step, which is usually very large (in 102 magnitude). There-
fore, GABN dominates BBN on scalable learning task. Prac-
tice also demonstrates this point.

Furthermore, GABN presents a new direction for combin-
ing weaker learners since it is a highly extensible framework.
We present a solution for logarithmic link function. It is not
hard to adopt other link functions under the GAM framework,
and thus propose new algorithms. Many existing GAM prop-
erties, optimization methods can be seamlessly adopted to ag-

IJCAI-07

916

gregate simple BNs for more powerful learning machines.

4 Experiments

This section evaluates the performance of the proposed algo-
rithm, compared it with other BNCs such as NB, TAN, K2,
kdB, SPBN; model averaging methods such as AODE, BBN;
and decision tree algorithm CART [Breiman et al., 1984].

The benchmark platform was 30 data sets from the UCI
machine learning repository [Newman et al., 1998]. One
point should be indicated here: for BNCs, when data sets have
continuous features, we ﬁrst adopted discretization method to
transfer them into discrete features [Dougherty et al., 1995].
We employed 5-fold cross-validation for the error estimation,
and kept all compared algorithms having the same fold split.
The ﬁnal results are shown in Table 2, in which the results
by TAN and K2 are obtained by the Java machine learning
toolbox Weka [Witten and Frank, 2000].

To present statistical meaningful evaluation, we conducted
the paired t-test to compare GABN with others. The last row
of Table 2 shows the win/tie/lose summary in 10% signiﬁ-
cance level of the test. In addition, Figure 2 illustrates the
scatter plot of the comparison results between GABN and
other classiﬁers. We can see that GABN outperforms most
other BNCs, and achieves comparable performance to BBN.
Specially note, the SPBN column shows results by the best
individual super-parent BN, which are signiﬁcant worse than
GABN. This demonstrates that it is eﬀective and meaningful
to use GAM for aggregating simple BNs.

5 Conclusions

In this paper, we propose a generalized additive Bayesian
network classiﬁers (GABN). GABN aims to avoid the non-
negligible posterior problem in Bayesian network structure
learning. In detail, we transfer the structure learning problem
to a generalized additive models (GAM) learning problem.
We ﬁrst generate a series of very simple Bayesian networks
(BN), and put them in the framework of GAM, then adopt
a gradient-based learning algorithm to combine those sim-
ple BNs together, and thus construct a more powerful clas-
siﬁers. Experiments on a large suite of benchmark data sets
demonstrate that the proposed approach outperforms many
traditional BNCs such as naive Bayes, TAN, etc, and achieves
comparable or better performance in comparison to boosted
Bayesian network classiﬁers. Future work will focus on other
possible extensions within the GABN framework.

References
[Bishop, 1995] C. M. Bishop. Neural Networks for Pattern Recog-

nition. Oxford University Press, London, 1995.

[Boyd and Vandenberghe, 2004] S. Boyd and L. Vandenberghe.

Convex Optimization. Cambridge University Press, 2004.

[Breiman et al., 1984] L. Breiman, J. Friedman, R. Olshen, and
C. Stone. Classiﬁcation And Regression Trees. Wadsworth In-
ternational Group, 1984.

[Cestnik, 1990] B. Cestnik. Estimating probabilities: a crucial task
in machine learning. In the 9th European Conf. Artiﬁcial Intelli-
gence (ECAI), pages 147–149, 1990.

[Cheng et al., 2002] J. Cheng, D. Bell, and W. Liu. Learning be-
lief networks from data: An information theory based approach.
Artiﬁcial Intelligence, 137:43–90, 2002.

[Cooper and Herskovits, 1992] G. Cooper and E. Herskovits. A
Bayesian method for the induction of probabilistic networks from
data. Machine Learning, 9:309–347, 1992.

[Dougherty et al., 1995] J. Dougherty, R. Kohavi, and M. Sahami.
Supervised and unsupervised discretization of continuous fea-
tures. In the 12th Intl. Conf. Machine Learing (ICML), San Fran-
cisco, 1995. Morgan Kaufmann.

[Friedman and Koller, 2003] N. Friedman and D. Koller. Being
Bayesian about network structure: a Bayesian approach to struc-
ture discovery in Bayesian networks. Machine Learning, 50:95–
126, 2003.

[Friedman et al., 1997] N. Friedman, D. Geiger, and M. Gold-
Bayesian network classiﬁers. Machine Learning,

szmidt.
29(2):131–163, 1997.

[Friedman et al., 2000] J. Friedman, T. Hastie, and R. Tibshirani.
Additive logistic regression: a statistical view of boosting. Annals
of Statistics, 28(337-407), 2000.

[Friedman, 2001] J. Friedman. Greedy function approximation: a

gradient boosting machine. Annals of Statistics, 29(5), 2001.

[Hastie and Tibshirani, 1990] T. Hastie and R. Tibshirani. General-

ized Additive Models. Chapman & Hall, 1990.

[Jing et al., 2005] Y. Jing, V. Pavlovi´c, and J. Rehg. Eﬃcient dis-
criminative learning of Bayesian network classiﬁers via boosted
augmented naive Bayes. In the 22nd Intl. Conf. Machine Learn-
ing (ICML), pages 369–376, 2005.

[Keogh and Pazzani, 1999] E. Keogh and M. Pazzani. Learning
augmented Bayesian classiﬁers: A comparison of distribution-
based and classiﬁcation-based approaches. In 7th Intl. Workshop
Artiﬁcial Intelligence and Statistics, pages 225–230, 1999.

[Langley et al., 1992] P. Langley, W. Iba, and K. Thompson. An
analysis of Bayesian classiﬁers. In the 10th National Conf. Arti-
ﬁcial Intelligenc (AAAI), pages 223–228, 1992.

[Liu and Nocedal, 1989] D. Liu and J. Nocedal. On the limited
memory BFGS method for large-scale optimization. Mathemati-
cal Programming, 45:503–528, 1989.

[Newman et al., 1998] D. Newman, S. Hettich, C. Blake, and

C. Merz. UCI repository of machine learning databases, 1998.

[Pearl, 1988] J. Pearl. Probabilistic Reasoning in Intelligent Sys-
tems: Networks of Plausible Inference. Morgan Kaufmann, 1988.

[Rosset and Segal, 2002] S. Rosset and E. Segal. Boosting density
estimation. In Advances in Neural Information Processing Sys-
tem (NIPS), 2002.

[Sahami, 1996] M. Sahami. Learning limited dependence Bayesian
classiﬁers. In the 2nd Intl. Conf. Knowledge Discovery and Data
Mining (KDD), pages 335–338. AAAI Press, 1996.

[Thiesson et al., 1998] B. Thiesson, C. Meek, D. Heckerman, and
et al. Learning mixtures of DAG models. In Conf. Uncertainty
in Artiﬁcial Intelligence (UAI), pages 504–513, 1998.

[Webb et al., 2005] G. Webb, J. R. Boughton, and Zhihai Wang.
Not so naive Bayes: aggregating one-dependence estimators.
Machine Learning, 58(1):5–24, 2005.

[Witten and Frank, 2000] I. Witten and E. Frank. Data Mining:
Practical Machine Learning Tools and Techniques with Java Im-
plementations. Morgan Kaufmann Publishers, 2000.

IJCAI-07

917

dataset
australia
autos
breast-cancer
breast-w
cmc
cylinder-band
diabetes
german
glass
glass2
heart-c
heart-stat
ionosphere
iris
letter
liver
lymph
page-blocks
post-operative
satimg
segment
sonar
soybean-big
tae
vehicle
vowel
waveform
wavef+noise
wdbc
yeast
average
win/tie/lose

BBN
.1352
.2185
.2551
.0337
.4589
.2345
.2408
.2510
.3311
.2154
.1611
.1815
.0884
.0333
.1371
.3333
.1243
.0585
.3096
.1218
.0636
.2353
.0696
.4506
.2814
.1232
.1496
.1652
.0441
.3895
.1965
11/10/9

Table 2: Testing error on 30 UCI data sets
GABN AODE
.1328
.1313
.2195
.1760
.2621
.2520
.0293
.0264
.4630
.4683
.2479
.1994
.2642
.2569
.2640
.2560
.3497
.2990
.2277
.2400
.1710
.1678
.2037
.1989
.0911
.0711
.0400
.0333
.1437
.1170
.3420
.3228
.1483
.1425
.0634
.0572
.3202
.3096
.1221
.1172
.0732
.0429
.2397
.2108
.0696
.0690
.4848
.4448
.2773
.2860
.1303
.1152
.1630
.1600
.1708
.1586
.0545
.0370
.4028
.3949
.2049
.1928
17/9/4

TAN
.1449
.1903
.3077
.0358
.4705
.2204
.2552
.2510
.2851
.2617
.1716
.2000
.0798
.0633
.1656
.3376
.1284
.0468
.2889
.1234
.0568
.2452
.0569
.5497
.2967
.1636
.2132
.1850
.0439
.4023
.2080
19/6/5

kdB
.1797
.1952
.2935
.0358
.4787
.2535
.2943
.2700
.3586
.2740
.2651
.2813
.0896
.0667
.1935
.3862
.2104
.0639
.3850
.1265
.0623
.2770
.0793
.5170
.2493
.1626
.1838
.2336
.0440
.4598
.2323
29/0/1

K2

.1435
.2439
.2797
.0272
.5017
.2686
.2526
.2510
.2897
.1964
.1783
.1741
.1054
.0800
.2583
.4232
.1419
.0636
.3444
.1799
.0897
.2452
.0689
.5232
.3095
.2566
.1954
.2000
.0545
.4050
.2250
21/6/3

SPBN
.1566
.2333
.2727
.0472
.4840
.2478
.2643
.2940
.3464
.2516
.2017
.1815
.1303
.0667
.1842
.3623
.1572
.0650
.3531
.1430
.0571
.2643
.0851
.5101
.3272
.1162
.1982
.2156
.0673
.3881
.2224
27/1/2

NB
.1478
.2760
.2656
.0386
.4935
.3252
.2486
.2680
.3511
.2637
.1944
.2047
.1082
.0400
.3088
.3246
.1837
.0674
.3406
.1851
.0913
.2789
.0943
.5433
.3594
.2919
.1880
.2022
.0458
.3955
.2375
26/1/3

2
K

T
R
A
C

0.5

0.4

0.3

0.2

0.1

0
0

0.5

0.4

0.3

0.2

0.1

0
0

CART
.1580
.2293
.2832
.0586
.4820
.3343
.2799
.2720
.3228
.2331
.2083
.2148
.1111
.0533
.1722
.3420
.1902
.0373
.3111
.1422
.0355
.2213
.0740
.4585
.2612
.1990
.2284
.2420
.0597
.4185
.2211
23/4/3

0.1

0.2

GABN

0.3

0.4

0.5

(d) GABN vs K2

0.1

0.2

GABN

0.3

0.4

0.5

–

N
B
B

B
d
k

0.5

0.4

0.3

0.2

0.1

0
0

0.5

0.4

0.3

0.2

0.1

0
0

0.1

0.2

GABN

0.3

0.4

0.5

(a) GABN vs BBN

0.1

0.2

GABN

0.3

0.4

0.5

E
D
O
A

N
B
P
S

0.5

0.4

0.3

0.2

0.1

0
0

0.5

0.4

0.3

0.2

0.1

0
0

0.1

0.2

GABN

0.3

0.4

0.5

(b) GABN vs AODE

0.1

0.2

GABN

0.3

0.4

0.5

N
A
T

B
N

0.5

0.4

0.3

0.2

0.1

0
0

0.5

0.4

0.3

0.2

0.1

0
0

0.1

0.2

GABN

0.3

0.4

0.5

(c) GABN vs TAN

0.1

0.2

GABN

0.3

0.4

0.5

(e) GABN vs kdB

(f) GABN vs SPBN

(g) GABN vs NB

(h) GABN vs CART

Figure 2: Scatter plots for experimental results on 30 UCI data sets. Each plot shows the relative error rate of GABN and one compared
algorithm. Points above the diagonal line correspond to data sets where GABN performs better than the compared algorithm.

IJCAI-07

918


Continuous  nonlinear dimensionality  reduction  by  kernel  eigenmaps 

Matthew  Brand 

Mitsubishi Electric Research Laboratories 

Cambridge, MA 02460 USA 

Abstract 

We  equate  nonlinear  dimensionality  reduction 
(NLDR) to graph embedding with side information 
about  the  vertices,  and  derive  a  solution  to  either 
problem  in  the  form  of a  kernel-based  mixture  of 
affine  maps  from  the  ambient  space  to  the  target 
space.  Unlike  most  spectral  NLDR  methods,  the 
central eigenproblem can be made relatively small, 
and the result is a continuous mapping defined over 
the entire space, not just the datapoints.  A demon­
stration  is  made  to  visualizing  the  distribution  of 
word  usages  (as  a  proxy  to  word  meanings)  in  a 
sample of the machine learning literature. 

1  Background:  Graph embcddings 
Consider a connected graph with weighted undirected edges 
specified  by  edge  matrix  W.  Let 
be  the  posi­
tive  edge  weight  between  connected  vertices  i  and  j  zero 
otherwise.  Let  D =  diag(Wl)  be  a  diagonal  matrix  where 
the cumulative edge weights into vertex /.  The 
following points are well known or easily derived in spectral 
graph theory [Fiedler,  1975; Chung,  1997]: 

1.  The generalized eigenvalue decomposition (EVD) 

W V = D VA 

has  real  eigenvectors 

(1) 
and  eigenvalues 

with minimal distortion vis-a-vis the weights, in the 
stipulates  a  shorter  embedding 

sense  that  a  larger 
distance.  Formally, the embedding 

(4) 

(5) 

for any integer 
= 
I  sets the scale of the embedding and  causes  vertices of 
high  cumulative  weight  to  be  embedded  nearer  to  the 
origin. 

. The norm constraint 

4.  Y can be rigidly rotated in 

without changing its dis­
tortion.  The distortion measure is also invariant to rigid 
translations,  but  the  eigenproblem  is  not,  thus  there  is 
an  unwanted  degree  of freedom  (DOF)  in  the  solution. 
Due to stochasticity, this DOF is isolated in the uniform 
eigenvector \\, which is suppressed  from the embedding 
without  error  (because 
.  A d d i n g to   Y 
rigidly translates the embedding by 

5.  Premultiplying  by 

, 

and  rearranging  equates  equa­

tion  1  to the EVD of the graph Laplacian D - W: 

6.  Premultiplying  by 

(symmetric)  KVD of the normalized Laplacian: 

(6) 
connects  equation  1  to  the 

2.  Premultiplying equation (1) by D_1  makes the general­

ized eigenproblem into a stochastic eigenproblem 

(2) 
where 
is  a  stochastic  transition  matrix  having 
nonnegative  rows  that  sum  to  one.  The  largest  eigen­
therefore  stochastic  and 
value  of  equation  (1) 
its paired eigenvector is  uniform 

is 

3.  Expanding and collecting terms  in WiJ reveals the geo­

metric meaning of the eigenvalues: 

The  d  eigenvectors  paired  to  eigenvalues 

(3) 
through 
therefore  give  an  embedding  of  the  vertices  in 

(7) 

with 

In  summary:  Equation  1  gives  an  optimal  embedding  of 
a  graph  in  Rc/  via  eigenvectors 
eigenvalue Ω-iis 
stochastic  and  the  corresponding  eigenvector  \\  is  uniform; 
this  is  an  important property  of the  EVD  solution  because  it 
isolates the problem's unwanted translational degree of free­
dom in a single eigenvector, leaving the remaining eigenvec­
tors unpolluted. 

Many embedding algorithms can be derived from this anal­
ysis, including the Fiedler vector [Fiedler,  1975], locally lin­
ear embeddings (LLE)  [Roweis and  Saul,  2000], and Lapla­
cian eigenmaps [Belkin and Niyogi, 2002].  For example, di­
rect solution of equation  1  gives the Laplacian eigenmap; as 

LEARNING 

547 

a historical note, the symmetrized formulation was proposed 
by Fiedler in the  1970s and has been used for heuristic graph 
layout since the  1980s [Mohar,  1991]. 

2  Transformational embeddings 
Now consider a more general problem:  We  are given  some 
information about the vertices in a matrix Z =  [z\, • • • ,zN] c 
RJxN,  whose  columns  are  generated  by  applying  a  vector-
valued  function z(-) —► z £ Rd to each vertex of the graph. 
We seek a linear operator which transforms Z to the optimal 
graph embedding:  G(Z) —♦ Y.  We will call this the "transfor­
mational  embedding,"  to distinguish  it from  the  "direct em­
bedding" discussed above. 

A natural candidate for the algebraic statement of the trans­

formational embedding problem is the generalized EVD 

(ZWZT)V =  (ZDZT)VA, 

(8) 

because  setting  Y  = VTZ  makes  this  equivalent to  the  orig­
inal  direct  embedding  problem.  Again,  there  is  an  equiva­
lent symmetric eigenproblem:  Make Cholesky1  decomposi­
tion  RTR  <-  ZDZT  into  upper-triangular R  G  Rdxd  and  let 

B=  (R-  '  Z W Z1  I T1)  eRuxd. 

Then 

BV' = V'A 

(9) 

(10) 

with 

is a short  m a t r i x,

(in 
, and a computational 
This gives an embedding 
advantage: 
 the origi­
nal N x N eigenproblem can be reduced to a very small d xd 
problem, and the matrix multiplications also scale as 0(d2N) 
rather than  0(N3),  due to the  sparsity  of W  and  D. 
2.1  Correcting problematic eigenstructure 
It is generally the case that 
—there is no lin­
ear combination of the rows of Z giving Y, so the desired lin­
ear mapping G(Z) —* Y does not exist.  Equations 8-11  give 
the  optimal  least-squares  approximation 
This approximation can have a serious 
I 
then the first eigenvector vi  is not uniform;  it cannot be dis­
carded  as  the  unwanted  translational  DOF.  Worse,  all  the 
other eigenvectors will be variously contaminated by the un­
wanted  DOF,  resulting  in  an  embedding  polluted  with  arti­
facts.  For this reason, we call direct solution of equation 8 a 
raw approximation. 

flaw: 

Our  options  for  remedy  are  limited  to  those  that  modify 
the  row-space  of Z  to  reintroduce  the  uniform  eigenvector. 
For reasons that will become obvious below,  we will restrict 
ourselves to operations that can be applied to any column of 
Z without knowing any other column. 

The simplest such operation is to append a uniform row to 
.  This makes the relation between  Z 

Z, so that 

1 Any gram-like factorization will work. For example, given EVD 
.  The Cholesky is especially attrac­

tive for its numerical stability, sparsity, and easy invertibility. 

and  Y  affine and  guarantees  that 
is  uniform,  but  it  can 
also force the eigenvectors to model additional  variance that 
is not part of the problem. 

Working backward from the desiderata that the leading col­
umn  of V  Z should  be  uniform,  let 
such 
that ZK is a  modified representation of the vertices with  val­
ues of z()  reweighted on a  per-vertex basis: 
Clearly 
is divided by its first element. 

has a uniform first column, since each row 

It follows immediately that the related eigenproblem 

(12) 

, 

is  stochastic,  and 
is  an  embedding 
with the unwanted translational degree of freedom totally re­
moved.  Note that the raw and stochastic approximations are 
orthogonal  (under  metric 
is a diagonal matrix; 
the other methods are not. 

It should  be noted  that—when  scaled to have equal  norm 
—  none of these approximations has uniformly 
superior distortion scores; but in Monte Carlo trials with ran­
dom graphs,  wc  find  a clear ordering  from  lowest to highest 
distortion:  reweighted,  affine, stochastic,  raw (see  figure  1). 

Figure  1:  Comparison  of methods  for  modifying  the  row-
space  of Z.  The  graph  shows  distortion  from  the  optimal 
embedding,  averaged  over  106  trials  with  50-node  matrices 
having random edge weights and random Z e_ R4x50. 

The  raw  approximation  is  suboptimal  because  information 
about  the  d-dimensional  embedding  is  spread  over  d  +  1 
eigenvectors,  no  subset  of which  is  optimal.  The  stochas­
tic  approximation  is  also  suboptimal—it  optimizes  a  differ­
ent measure implied by equation  12.  In practice, when com­
puting  embeddings  of graphs  whose  embedding  structure  is 
known  a priori,  wc  find  that  the  reweighted  and  stochastic 
approximations give results that are clearly very similar, and 
superior to the other approximations. 

The  need  for  any  such  correction  stems  from  the  fact 
that—the  literatures  of  spectral  graph  theory  and  NLDR 
is  not  a  completely  correct 
notwithstanding—equation  1 
statement  of the  embedding  problem.  We  will  show  in  a 
forthcoming  paper  that,  as  a  statement  of  the  embedding 
problem,  equation  1  is  both  algebraically  underconstraincd 
and  numerically  ill-conditioned. 
In  particular,  point  #2  is 

548 

LEARNING 

not  strictly  true:  The  stochastic  eigenvalue  is  not  always 
paired  to  a  uniform  eigenvector.  This  leads  to  patholo(cid:173)
gies that can ruin the embedding, whether obtained from the 
basic  or  derived  formulations.  NLDR  algorithms  that  can 
be derived from equation  1  (e.g.,  [Roweis  and  Saul,  2000; 
Belkin and Niyogi, 2002; Teh and Roweis, 2003]) do not re(cid:173)
mediate the problem. 

A  forthcoming  paper  makes  a  full  analysis  of  these  is(cid:173)
sues, identifies the correct problem statements for both equa(cid:173)
tions  1  & 8, and gives closed-form optimal solutions to both 
problems.  The approximation methods discussed in this sec(cid:173)
tion are still useful in that they are faster and give reasonably 
high-quality embeddings. For the NLDR method and datasets 
considered  below,  the  result  of  the  reweighted  approxima(cid:173)
tion is almost numerically indistinguishable from the optimal 
embedding,  and requires  substantially  less calculation.  The 
reweighting method can also be justified as a Pade approxi(cid:173)
mation of the optimal solution. 

] 

, 

. 

3  Nonlinear dimensionality  reduction 
be a set of points sampled from a 
Let 
low-dimensional  manifold  embedded  in  a  high-dimensional 
A  reduced-dimension  embedding  Y  = 
ambient  space. 
is a set of low-dimensional 
points  with  the same  local  neighborhood  structure.  We de(cid:173)
sire  instead  a  mapping 
which  will  general(cid:173)
ize  the  correspondence  to  the  whole  continuum,  with  rea(cid:173)
sonable interpolation and extrapolation to be expected in the 
neighborhood  of the  data.  Spectral  methods  for  NLDR  typ(cid:173)
ically  require  solution  of many  and/or  very  large eigenvalue 
or generalized eigenvalue problems [Kruskal and Wish, 1978; 
Kambhatla and Leen,  1997; Tcnenbaum et al., 2000; Roweis 
and Saul, 2000; Belkin and Niyogi, 2002], and with the ex(cid:173)
ception of [Teh and Roweis,  2003; Brand, 20031, offer em-
beddings of points rather than mappings between spaces. 

Here we show how a leverage the transformational embed(cid:173)
ding of section 2 into a continuous NLDR  algorithm, specifi(cid:173)
cally a kernel-based mixture of affine maps from the ambient 
space to the target space.  To do so,  we must  show how the 
edge weight matrix W and vertex matrix Z are specified.  Let 
iff x;  and  Xj  satisfy  some  locality  criterion, 
As stated above, an 

otherwise 

embedding Y of X  should satisfy 

where larger Wy penalize large distances between y,- and y,. 
How  should  Wij  be computed?  / is  a measure of similar(cid:173)
ity:  The  graph-theoretic  literature  usually  takes  / ( • ,)  =  1. 
while  NLDR  methods  typically  take 

to be  a Gaussian  kernel,  on  analogy  to heat dif(cid:173)
fusion  models  [Belkin  and  Niyogi,  2002].  The  uninforma-
tive setting Wij =  1  is only usable when there is a very large 
number of points  (and edges),  so  that connectivity informa(cid:173)
tion alone suffices to determine metric properties of the em(cid:173)
bedding.  The Gaussian setting has a complementary weak(cid:173)
ness:  It can be very  sensitive to small  variations  in distance 
to neighbors (that may be introduced by the curvature of the 
data  manifold  or measurement  noise  in  the  ambient  space). 

/  should  be  monotonically  decreasing,  relatively  insensitive 
to noise ( d/ should be small), and it should lead to exact re(cid:173)
constructions of data sampled from manifolds that are already 
flat. Straightforward calculus shows that equation  13 has the 
desired minimum when 
, or more gen(cid:173)
erally,  the  multiplicative  inverse  of whatever  distance  mea(cid:173)
sure  is appropriate in the ambient space2.  (By contrast,  the 
LLE  weightings are not correlated  with distances.)  To make 
the problem scale invariant,  we scale W such that its largest 
nonzero off-diagonal value is  1  (consequently 
every(cid:173)
where  / is  computed). 

Let  us  now  situate  some  Gaussian  kernels  p*(x)  = 
on  the  manifold.  In  this  paper,  we  will  take  a 
random  subset  of data  points  as  kernel  centers,  and  set  all 
; these kernels are radial basis functions. Let vector 

(14) 

be  the  kth  local  homogeneous  coordinate  of  Xi  scaled  by 
the  posterior  of  the  kth  kernel.  K;  is  an  optional  local 
dimensionality-reducing linear projection.  Let representation 
vector 

(15) 
be the vertical concatenation of all such local coordinate vec(cid:173)
tors.  Collect  all  such  column  vectors  into  a  basis  matrix 

To  summarize  thus  far,  our  goal  now  is  to  find  a  linear 
transform 
of the  basis  (kernel-weighted  coor(cid:173)
dinates)  that is  maximally consistent  with our local  distance 
constraints, specifically 

(16) 

This is isomorphic to the graph embeddings of section 2; the 
methods developed there apply directly to W and Z. The con(cid:173)
tinuous mapping from ambient to embedding space immedi-
atly follows from the continuity and smoothness of z{-): 

where 

the  E VD  determines 

transformation  G  = 
of  the  continuous  kernel  representation  de(cid:173)

the 

fined over the entire ambient space: 

(17) 

t

s

i

n

on  a  1D 

2Proof:  Consider  three  p

causes the 
to have a global minimum 
Without loss of generality, we fix the global location and 

o
manifold.  What similarity measure 
distortion 
at 
scale of the embedding by  fixing  the endpoints: 
Solving for the unique zero of the distortion's first derivative,  we 
obtain the optimum at y2 = W23 W12 -I- W23). Since this is a har(cid:173)
monic relation, the unique continuous satisficing measure is 
; some 
.at the optimum.  The 

simple algebra confirms that indeed 
induction to multiple points in multiple dimensions is direct. 

.  This sets 

and 

LEARNING 

549 

As a matter of numerical  prudence,  we recommend  using 

the reweighted approximation: 

We now show some kernel eigenmaps computed using the 
transformational  embedding  of  section  2.  All  embedding 
methods are given the same inputs. 

(18) 

At first blush,  it would seem  that reweighting should not be 
necessary:  By construction, 
— 
and  the denominator—should  be  uniform  at  the datapoints. 
However, as mentioned above, even when the algebra predicts 
this structure, numerical eigensolvers may not find it. 

), thus 

To  obtain  an  approximate  inverse  mapping,  we  map  the 
into  the  target 
means  and  covariances  of each  kernel 
there.  Then, 
space  to  obtain  kernels 
breaking 
j into blocks corresponding to each 
kernel, take the Moore-Penrose pseudo-inverse of each, and 
set 
If using the reweighted map, the ap­
proximate inverse map is 

fc 

4 

Illustrative example 

We  will  use  a  variant  of  the 
"swiss roll", a standard test man­
ifold  in the  NLDR  community,  to 
illustrate the arguments and meth­
ods  developed in  this  paper.  We 
sampled  a  twisted  version  of the 
manifold  regularly  on  a  30 x 30 
grid  and  added  a  small  amount 
of Gaussian  isotropic noise.  Fig­
ure  2  shows the  ideal  R2  param­
eterization  and  two  views  of the 
ambient  R3  embedding.  Points 
are shown joined into a line to aid 
visual  interpretation  of  the  em-
beddings.  All experiments in this 
section use a W matrix that was generated using the 12 near­
est neighbors to each point and the inverse distance function. 

Figure 2: The swiss roll. 

The Laplacian eigenmap 
embedding (figure 3) shows 
the embedding specified by 
the  W  matrix. 
Note 
that  it  exhibits  some  fold­
ing  at 
the  corners  and 
top and  bottom edges,  due 
partly to problems  with the 
uniform  eigenvector  and 
exacerbated  by 
fact 
that  spectral  embeddings 
tend  to  compress  near  the 
boundaries.  The Laplacian 
eigenmap  requires  solution  Figure  3:  Laplacian  eigenmap 
of a large 900 x 900 eigen- 
problem, and offers no mapping off the points.  Kernel eigen-
maps will be approximations to this embedding. 

embedding, 

the 

Figure  4  shows  a  raw 
kernel eigenmap embedding 
computed  using  a  basis  (Z 
matrix)  created 
from  64 
Gaussian  unit-a  kernels 
placed  on  random  points. 
This 
required  solving  a 
much  more  manageable 
256  x  256  eigenproblem. 
100  trials  were  performed 
with  different  sets  of  ran­
domly placed kernels.  In all 
trials,  the  reweighted  and 
stochastic  maps  gave  the 

Figure  4:  Kernel  eigenmap 
embedding, raw result. 

the raw  and  affine maps exhibited  substantial  folding  at the 
edges and corners of the embedding. 

5 

shows 

Figure 

a 
reweighted  kernel  eigen­
map  computed  from 
the 
same W and Z as  figures  3 
& 4.  The result is smoother 
and  actually  exhibits  less 
folding  than  the  original 
Laplacian eigenmap. 

The problem can be reg­
ularized by putting positive 
mass on  the diagonal  of W 
(e.g.,  W +W +1),  thereby 
making  the  recovered  ker­
nel eigenmap more isomet­
ric (bottom figure 5).  This 
regularization  is  appropri­
ate  when 
is  believed 
that  all  neighborhoods  are 
roughly the same size. 

it 

The  recently  proposed 
Locality Preserving Projec­
tion (LPP)  [He and Niyogi, 
2002], is essentially the raw 
approximation (direct solu­
tion  of  equation  8)  with 

Figure  5:  Kernel  eigenmap 
embedding, 
reweighted  and 
regularized results. 

and  Z  =  X,  thereby  giving  a  linear  pro­
jection  from the ambient  space  to  the  target  space  that  best 
preserves local relationships. 

LPP is admirably simple, 
but it can be shown that the 
affine  approximation  from 
section  2  will  always  have 
less distortion. LPP can also 
suffer from  loss of the uni-
fo rm eigenvector.  Figure 6 
Figure 6:  LPP embedding and 
embeddings  of  the 
our affine upgrade. 
show 
swiss roll produced by LPP and by an affine modification of it 
that is equivalent to our method with a trivial single uniform-

550 

LEARNING 

density kernel.  Upgrading  LPP to an  affinc projection cap(cid:173)
tures more of the data's structure.  Even so, there is no affine 
"view" of this manifold that avoids folding. 

5  Visualizing word usages 
In statistical analyses of natural language, similar usage pat(cid:173)
terns for two words are taken to indicate that they have sim(cid:173)
ilar meanings or strongly related meanings.  Latent semantic 
analysis (LSA) is a linear dimensionality reduction of a term-
document co-occurence matrix. The principal components of 
this matrix give an embedding in which similarly used words 
are similarly located. Literally, co-location is a proxy for col(cid:173)
location  (the  propensity  of words  to  be  used  together)  and 
synonymy.  We may expect that the kernel eigenmap offers a 
more powerful nonlinear analysis: 

The  NIPS 12  corpus3  features  a  matrix  counting  occur(cid:173)
rences  of  13000+  words  in  1700+  documents.  We  mod(cid:173)
eled the first  1000 words and the last 200 documents in the 
matrix.  This  roughly  corresponds  to  one  year's  papers,  a 
reasonable  "snapshot"  of the  ever-changing  terminology  of 
the  field.  We  stemmed the words and combined counts for 
the same roots, then determined distance between two word 
roots as the cosines of the angles between their log-domain-
transformed occurrence vectors (x,, —* log2(l +  xij)).  The W 
matrix was generated by adding an edge from each word to 
its 30 closest neighbors in cosine-space.  The representation 
Z was made using 4 random  words as kernel  centers.  Fig(cid:173)
ure 7 discusses the resulting 2D embedding, in which techni(cid:173)
cal terms arc clearly grouped by field and many of the more 
common English words arc tightly clustered by common se(cid:173)
mantics.  The first two  LSA  dimensions  (also  shown  in fig(cid:173)
ure 7) of the same data arc reveal significantly less semantic 
structure. 

6  Discussion 
The  kernel  eigenmap  generates  continuous  nonlinear  map(cid:173)
ping functions for dimensionality reduction and manifold re(cid:173)
construction.  Suitable choices of kernels can  reproduce the 
behavior of several other NLDR methods. One could put a ker(cid:173)
nel at every local group of points, perform local dimensional(cid:173)
ity reduction (e.g., a PC A) at each kernel, and thereby obtain 
from equations 8 and 17 an NLDR algorithm much like chart(cid:173)
ing [Brand. 2003] or automatic alignment [Teh and Roweis, 
2003].  Or, as in the demonstrations above, the kernel eigen(cid:173)
map can  simultaneously  determine the  local  dimensionality 
reductions and their global merger. 

The  kernel  eigenmap  typically  substitutes  a  small  dense 
CVD for the the large sparse LVD of graph embedding prob(cid:173)
lems. 
In  the  sparse  case,  a  specialized  power method  can 
compute the desired eigenvectors in significantly less than the 
0(N3)  time  required  for  a  full  EVD.  In  the  kernel  setting, 
similar efficiencies apply because both W and Z are typically 
sparse, allowing fast construction of the reduced EVD prob(cid:173)
lem Z W Z1; this too is amenable to fast power methods.  Of 
course,  the  most  important  efficiency  of the  kernel  method 
is  its  ability  to  embed  new  points  quickly  via  the  function 

3Courtesy S. Roweis, available from the U. Toronto website. 

G(x)—there is no need to compute a new global embedding 
or revise the EVD. 

The reweighting scheme, although theoretically mooted by 
our subsequent discovery of a better problem formulation and 
closed-form solution,  is still practically viable as a fast ap(cid:173)
proximation for large problems,  and as a post-conditioning 
step for unavoidable numerical  error of any  NLDR  algorithm 
based on eigenvalue decompositions. 

In this paper we have used random kernels.  There are nu(cid:173)
merous avenues to discovering stronger methods by investi(cid:173)
gating placement and  tuning  of the  kernels,  stability  of the 
embedding  and  its  topological  structure,  and  sample  com(cid:173)
plexity. 
In  short,  all  the  issues  that  proved  fertile  ground 
for research  in  classification  and  regression  can  be  studied 
anew in the context of estimating the geometry and topology 
of manifolds. 

References 
[Belkin and Niyogi, 2002]  Mikhail  Belkin  and  Partha 
Laplacian  eigenmaps  for  dimensionality  re(cid:173)
Niyogi. 
Technical  Report 
duction  and  data  representation. 
TR-2002-01,  University  of Chicago  Computer  Science, 
2002. 

[Brand, 2003]  Matthew Brand. Charting a manifold. In Proe. 

NIPS-15, 2003. 

[Chung, 1997]  Fan R.K. Chung. Spectral graph theory, vol(cid:173)
ume 92 of CBMS Regional Conference Series in Mathe(cid:173)
matics. American Mathematical Society, 1997. 

[Fiedler,  1975]  Miroslav Fiedler.  A property of eigenvectors 
of nonncgative  symmetric  matrices and  its application to 
graph theory.  Czech. Math. Journal 25:619-633, 1975. 

[He and Niyogi, 2002]  Xiafei He and Partha Niyogi.  Local(cid:173)
ity preserving projections.  Technical Report TR-2002-09, 
University of Chicago Computer Science, October 2002. 
[Kambhatla and Leen,  1997]  N.  Kambhatla and Todd Leen. 
Dimensionality  reduction  by  local  principal  component 
analysis. Neural Computation, 9, 1997. 

[Kniskal and Wish, 1978]  J. B. Kruskal and M. Wish.  Mul(cid:173)
tidimensional Scaling.  Sage Publications, Beverly Hills, 
CA,  1978. 

[Mohar,  1991]  B. Mohar.  The laplacian spectrum of graphs. 
In Y. Alavi, editor, Graph Theorv, Combinatorics and Ap(cid:173)
plications, pages 871-898. J. Wiley, New York, 1991. 

[Roweis and Saul, 2000]  Sam  T.  Roweis  and  Lawrence  K. 
Saul.  Nonlinear dimensionality  reduction  by  locally  lin(cid:173)
ear embedding.  Science,  290:2323-2326,  December 22 
2000. 

[Teh and Roweis, 2003]  Yee Whye Teh and Sam T. Roweis. 
Automatic alignment of hidden representations.  In Proc. 
NIPS-15, 2003. 

[Tenenbaum e/ al., 2000]  Joshua  B.  Tenenbaum,  Vin 
de  Silva,  and  John  C.  Langford.  A  global  geomet(cid:173)
ric  framework  for  nonlinear  dimensionality  reduction. 
Science, 290:2319-2323, December 22 2000. 

LEARNING 

551 

Figure 7:  ABOVE:  A 2D kernel eigenmap of word usages in recent NIPS papers.  To improve legibility we show just a subset 
of the data; some labels have been shifted slightly to reduce overlap.  Word roots are shown in their first occurring unstemmed 
variant.  The three lobes of the distribution roughly correspond to favored terminology in the submission areas of Algorithms & 
Architectures (left), Neuroscience (right), and Theory (top).  Words with broader usage are more tightly distributed in the center 
(presumably  because  they  are  more  likely  to  co-occur  in  general  discourse),  with  several  clusters  of words  having  strongly 
related  meanings.  Three  of these  clusters  have  been  colored:  red  for publishing  terms  (IEEE,  conference,  number,  paper, 
proceedings, volume), green for probability terms (b.iyc>ian..  NhnwK\  independent.  map.  ni.ji.iiin.tL posterior, joint, statistical), 
and blue for locations (Cambridge, department,  institute,  university).  BELOW,  SMALLER:  A  linear embedding obtained from 
a latent semantic analysis of the same data.  Though collocated words are often co-located,  when compared with the kernel 
eigenmap result, semantic structures are far less obvious. 

552 

LEARNING 


Grounding Abstractions in Predictive State Representations

Brian Tanner and Vadim Bulitko and Anna Koop and Cosmin Paduraru

Department of Computing Science
Edmonton, Alberta, Canada T6G 2E8

{btanner, bulitko, anna, cosmin}@cs.ualberta.ca

Abstract

This paper proposes a systematic approach of rep-
resenting abstract features in terms of low-level,
subjective state representations. We demonstrate
that a mapping between the agent’s predictive state
representation and abstract features can be derived
automatically from high-level training data sup-
plied by the designer. Our empirical evaluation
demonstrates that an experience-oriented state rep-
resentation built around a single-bit sensor can rep-
resent useful abstract features such as “back against
a wall”, “in a corner”, or “in a room”. As a result,
the agent gains virtual sensors that could be used
by its control policy.1

Introduction

1
It is often useful for intelligent agents to reason at a level
more abstract than their perceptions. Abstractions aggregate
many sensory situations into a single conﬁguration of abstract
features, allowing behavior expressed in abstract terms that
generalize across similar situations. This generalization al-
low humans designers to encode control policies closer to the
level of human reasoning. For instance, using abstractions, an
agent’s designer can deﬁne a policy such as “avoid navigating
through open areas” or “don’t stop near a doorway”.

The desire to write control policies in abstract terms is of-
ten thwarted because the policies must be implemented over
the set of state variables and sensor readings available to the
agent. In simulation, one option is to directly augment the
environment with high-level information (e.g., each tree in
a forest may be annotated as a possible cover for a soldier
agent) [Paull and Darken, 2004].
If access to the environ-
ment is not available, a programmer may write code to map
low-level state (e.g., coordinates and obstacle detectors) to
higher-level abstract terms (e.g., being in a narrow corridor
where the unit cannot move sideways) [Orkin, 2005].

As realism of simulations used for real-life training and
education [Dini et al., 2006] increases, both approaches are
becoming expensive. Indeed, the number of abstract features
and the amount of underlying software engineering needed to

1This work was supported in part by RLAI, NSERC, iCORE,

and Alberta Ingenuity.

deﬁne them has been compared to construction of the Tower
of Babel [McDonald et al., 2006]. Likewise, manually an-
notating modern games and simulations is prohibitively time-
consuming and error-prone.

This paper makes several contributions to the aforemen-
tioned problems. We propose an automated approach for
learning a mapping from agent state representations to
human-deﬁned abstractions using a small set of data labeled
by a human expert. We use a “I know it when I see it” ap-
proach to deﬁning abstract features; instead of programmat-
ically deﬁning or assigning values to the abstract features,
the human expert examines a number of examples and labels
them appropriately. A machine learning classiﬁcation algo-
rithm, such as C4.5 [Quinlan, 1993], can then create a classi-
ﬁer that will appropriately label novel future experiences.

For this approach to be useful, it is important that the
agent’s state representation allows the values of the abstract
features to generalize well between the labeled examples and
unseen data. We argue that agents using subjective, grounded
representations are well suited to making these generaliza-
tions. A subjective, grounded approach represents the agent’s
state in terms of experience with its sensors and effectors.
Speciﬁcally, we use a predictive representation of state: the
agent’s state is stored as a vector of probabilities of outcome
sensations given various sequences of action [Littman et al.,
2002]. We believe that both concise and vague abstractions
can be characterized by patterns of interaction: abstractions
learned in terms of a predictive representation are portable to
novel situations with similar patterns of interaction.

In a small navigation domain, we demonstrate that an
agent using a simple classiﬁcation algorithm can learn to
identify intuitive abstractions like BACK-TO-WALL, CORNER,
NARROW-CORRIDOR, and ROOM using both subjective and
objective state representations. Further, we show that in the
subjective case, the learned classiﬁer allows the agent to iden-
tify these abstractions in a novel environment without any ad-
ditional training.

2 Background and Related Work
In a typical situated agent framework, the agent (decision
maker) interacts with an environment at a series of time steps.
The agent receives perceptions or observations from the envi-
ronment at each moment and responds with actions that may
affect the environment. In this paper, we model the world as a

IJCAI-07

1077

partially observable Markov Decision Process (POMDP). We
assume that at each discrete time step, there is a set of data
(called the state) that is sufﬁcient to make accurate proba-
bilistic predictions of future experience (the Markov assump-
tion). The state is not directly observable by the agent, the
observation received from the environment may correspond
to multiple different states.

The standard POMDP model represents the agent’s current
state as a probability distribution over a set of unobservable
situations that the agent may be in. Each of these unobserv-
able situations is called a nominal state; distributions over
nominal states are called belief states. POMDPs use the con-
ditional probability rule, a nominal state transition model, and
an observation model to update the belief state with each new
action and observation. The transition and observation mod-
els are usually determined by a human expert and then later
these parameters are optimized with data.

Alternatively, history-based methods also model partially
observable environments by considering either a ﬁxed or vari-
able length window of recent interactions with the environ-
ment [McCallum, 1995].

Predictive state representations (PSR) are a third type
of representation that model both observable and partially-
observable environments [Littman et al., 2002]. Instead of
focusing on previous interaction with the environment, the
predictive model represents state as a set of predictions about
future interactions.
In a PSR, the agent’s state is repre-
sented as answers to questions about future experience in the
world. For example, one element of the state representation
might be “What is the probability of receiving the sequence
of observations (o1o2 . . . oi) if I take the sequence of actions
(a1a2 . . . aj)?”. Singh et al. [2004] proved that linear predic-
tive state representations can represent all environments that
can be represented by a ﬁnite-state POMDP or a ﬁnite-length
history method.

A PSR has two components: structure and parameters. The
structure is the set of events that the PSR makes predictions
about, called the core tests. Each core test predicts the prob-
ability of observing a particular sequence of observations if
a particular sequence of actions is chosen by the agent. The
parameters are used to update the core test values after each
new action and observation. Algorithms have been proposed
for learning the PSR parameters [Singh et al., 2003] and dis-
covering and learning PSR structure and parameters together
[McCracken and Bowling, 2006].

3 Abstractions
A state representation stores each state as a conﬁguration of
variables. In a traditional MDP, the state is often represented
by a single discrete variable with a unique value for each state
in the state space. In a POMDP, the state is represented by a
vector of n continuous-valued variables ∈ [0, 1], where n is
the number of underlying nominal states. In a linear PSR, the
state is also a vector of n continuous-valued variables ∈ [0, 1],
where n is the number of core tests.

In this paper, an abstraction is a many-to-one operator φ
that maps several conﬁgurations of state variables to a sin-
gle conﬁguration of a different set of variables. For exam-

ple, consider an MDP for a navigation task where the state is
represented using three discrete variables, (cid:3)x, y, θ(cid:4). The ﬁrst
two variables, (cid:3)x, y(cid:4) represent a location, while θ denotes ro-
tation or orientation in a coordinate system. One abstraction
φi might identify the NORTH-EAST-CORNER, deﬁned as all
states within 5 units of the (cid:3)0, 0(cid:4) corner of the environment.
In this case:
if x ∈ {1, 2, 3, 4} and y ∈ {1, 2, 3, 4}
φi((cid:3)x, y, θ(cid:4)) =
otherwise

(cid:2)

1
0

We generally refer to the outcomes of abstractions as ab-
stract features. Abstract features allow a control algorithm to
make useful generalizations: to behave similarly in states that
have different state variable combinations but similar abstract
features. For example, our agent may know that NORTH-
EAST-CORNERis a good place to recharge its batteries.
3.1 Representing Abstractions
Useful abstract features can often be invented by a human ex-
pert leveraging knowledge about a particular domain or task.
No matter what the agent’s representation, these abstract fea-
tures are understood in the expert’s terms. Thus, a mechanism
must be found that reliably converts the agent’s state repre-
sentation into the abstract features. This is usually an exten-
sive software engineering effort involving lengthy tuning via
trial and error [Paull and Darken, 2004]. In games and virtual
reality trainers the resulting abstraction is sometimes referred
to simply as “sensors” [Orkin, 2005] or “situation awareness
components” [Kunde and Darken, 2006].

The agent’s state representation has a strong impact on
the complexity and robustness of any script that converts the
agent’s state into abstract features. For instance, abstract fea-
tures such as BACK-TO-WALL or NARROW-CORRIDOR are
difﬁcult to concisely deﬁne as a function of (cid:3)x, y, θ(cid:4). These
features can have value 1 in various disjoint locations in the
map, making their values difﬁcult to know without explicit
reasoning about walls or other obstacles. On the contrary, a
predictive representation that stores the state as predictions
about future perceptions can succinctly capture these abstrac-
tions in terms of patterns of interaction with the environment.
3.2 Learning Abstractions
We propose a machine learning approach for automatically
ﬁnding a mapping between an agent’s state representation and
an expert’s understanding of abstract features. The expert can
look at a number of situations and appropriately label the ab-
stract feature values in each case. We call this set of labeled
examples the training set. For example, in the case of our
mobile agent, the expert looks at a map of a particular envi-
ronment and labels examples of the agent being in a narrow
corridor. The agent then uses an off-the-shelf machine learn-
ing algorithm such as a decision tree learner to learn a set
classiﬁer that takes state variables as inputs and emits abstract
feature values as outputs.

4 Experimental Results
For the demonstration, we chose a simple model of a path-
ﬁnding environment that allows us to evaluate our approach

IJCAI-07

1078

Room

Corridor

Neither

Figure 1: Map A, used to train the agent’s virtual sensors to
recognize various abstract features.

in detail. Speciﬁcally, our agent has four actions and a single,
binary observation. It is situated in a discrete grid environ-
ment where some cells are open and some are blocked. The
agent occupies exactly one cell and has four possible orien-
tations (up, down, left, and right). The agent’s actions are:
go forward (F), turn left (L), turn right (R) , and go backward
(B). If the agent moves forward or backward and collides with
an obstacle, the agent’s position remains unchanged. At any
time step, the agent’s sensor reports 1 if the agent faces a
blocked cell and 0 otherwise.

We chose to make the grid world deterministic: the actions
always succeed. Determinism greatly simpliﬁes the perfor-
mance measures and visualization of our experiments without
weakening the results.

We use two different maps in our experiments. The ﬁrst,
Map A (Figure 1), has a total of 943 reachable grid locations
with four orientations each, for a total of 3,772 states. The
second environment, Map B (Figure 2), has a total of 1,063
reachable grid locations with four orientations each, for a to-
tal of 4,252 states.
State Representations
The agent uses two different state representations, one that
is based on objective coordinates (cid:3)x, y, θ(cid:4) and another that is
a predictive representation. As we have discussed, the ob-
jective (cid:3)x, y, θ(cid:4) coordinate system is useful for characteriz-
ing certain abstractions that are tied to physical location on a
map. If certain areas of the map have a certain abstract fea-
ture value (such as “in a room”), then a classiﬁer learned over
the (cid:3)x, y, θ(cid:4) state will correctly classify other states within the
same room with very few training examples. However, if the
agent has seen no examples of a particular room, the classiﬁer
will have no means to identify it even if it is identical in size
and shape to another room seen previously.

The agent’s predictive representation is a vector of test val-
ues. Each test value corresponds to the probability of the
test’s observations matching those generated by the environ-
ment, conditioned on the test’s actions being executed.

Room

Corridor

Neither

Figure 2: Map B, used for testing the agent’s virtual sensors.

As an example, consider a predictive state representation
with only two core tests. Core test t1 is deﬁned as seeing (1)
after executing action (F ). The value of t1 will be 1 if and
only if the agent is facing an obstacle or can reach it within
one step forward. Core test t2 is deﬁned as seeing (1, 1, 1)
while executing (R, R, R). The value of t2 will be 1 if the
agent had obstacles on its right, behind it, and on the left
side of it. In a deterministic environment any core test has
the value of 1 or 0. Thus, this micro representation has four
states: [t1 = 0, t2 = 0], [t1 = 0, t2 = 1], [t1 = 1, t2 = 0],
and [t1 = 1, t2 = 1]. These tests are compactly denoted as
F1 and R1R1R1.

We construct the PSR representation from an accurate
POMDP model of the environment using an adaptation of
an algorithm introduced by Littman et al. for creating lin-
ear PSRs from POMDP models [2002]. The PSR construc-
tion algorithm is a polynomial algorithm that uses depth-ﬁrst
search to ﬁnd the set of PSR core tests and their values in all
of the POMDP nominal states. Our straightforward adapta-
tion of Littman et al.’s algorithm uses breadth-ﬁrst rather than
depth-ﬁrst search, yielding considerably shorter core tests.
Abstract Features
We chose four abstract features for our experiments, ranging
from the formally and succinctly deﬁned BACK-TO-WALL to
the more vague ROOM:

1. BACK-TO-WALL is a feature that is on when the agent
would hit a wall if the backward action was taken, and
off otherwise.

2. CORNER is a feature that is on in every grid location that

has a corner where two blocked grid cells meet.

IJCAI-07

1079

3. ROOM is a feature that is on when the agent is in a room.
This is a difﬁcult feature to program manually due to
the fact that “room” is a vague human-level concept. In
our approach we circumvent this problem by simply al-
lowing a human designer to label certain states as room
without formally deﬁning it (Figures 1 and 2).

4. NARROW-CORRIDOR is a feature that is on in all 1-cell
wide corridors, labeled by a human designer. This and
the ROOM abstraction are not labeled entirely consis-
tently within or across the two environments because of
subjective decisions made by the human designer.

5. Finally, some grid cells are labeled as “neither” because
the expert knew they should not be classiﬁed as ROOM or
NARROW-CORRIDOR, but there was no other abstract
feature to label them with.

4.1 Experiment 1: (cid:3)x, y, θ(cid:4) vs. PSR on Map A
In this experiment, we compare the classiﬁcation accuracy
of decision trees learned from varying amounts of training
data within a single map (Map A) using both representa-
tions. We hypothesize that with few training examples, the
(cid:3)x, y, θ(cid:4) representation will generally perform better on ab-
stract features that are well characterized by spatial locality
(e.g., the ROOM feature). Conversely, we expect the predic-
tive representation to perform well for abstract features that
are well characterized by patterns of interaction (e.g., BACK-
TO-WALL).

Experimental Method
All states in Map A are labeled as either positive or negative
examples of each abstract feature. The agent then uses the J48
(similar to C4.5 [Quinlan, 1993]) classiﬁcation algorithm to
create a separate decision tree for each abstract feature in both
representations using the WEKA machine learning software
package [Witten and Frank, 2005]. In all of our experiments,
the reported results are for a PSR created using some subset of
the core tests generated by our POMDP to PSR conversion al-
gorithm. This subset is the ﬁrst k tests found by the algorithm.
Using the full set of thousands of core tests is simply not nec-
essary. For various sample sizes P = {1%, 10%, 75%}, P
of the examples are randomly sampled from Map A and put
into the training set, the other examples are left out and used
as a test set. The classiﬁer learned on the training set is then
applied to the test set and classiﬁcation accuracy is measured.
The proportion of positive examples to negative examples is
often quite biased, so all or our results report the accuracy
of a baseline classiﬁer that simply predicts the majority class
from the training data. Precision, recall, and speciﬁcity statis-
tics were also measured but are not reported as they exhibited
similar trends to the accuracy. This procedure is repeated 10
times and the mean and standard deviation of the accuracies
is reported.

This experiment was performed on all four abstract fea-
tures, here we present the two results that represent either end
of the performance spectrum: ROOM and BACK-TO-WALL,
shown in Table 1.

% Data Used for Training

1%

10%

ROOM

82.0% ± 0.0
80.1% ± 3.9
90.7% ± 2.5

82.0% ± 0.0
95.8% ± 2.6
94.0% ± 1.0

BACK-TO-WALL

79.1% ± 0.0
76.8% ± 5.2
93.0% ± 3.7

79.1% ± 0.0
79.1% ± 0.1
99.0% ± 0.5

75%

82.0% ± 0.0
99.9% ± 0.2
96.7% ± 0.5

79.1% ± 0.0
83.6% ± 1.7
99.6% ± 0.2

Baseline
(cid:3)x, y, θ(cid:4)
750 tests

Baseline
(cid:3)x, y, θ(cid:4)
750 tests

Table 1: Accuracy of a classiﬁer learned with (cid:2)x, y, θ(cid:3) repre-
sentation against a classiﬁer learned with a 750 core test PSR
on the ROOM and BACK-TO-WALL abstract features. Test and
training data sets are disjoint.

Discussion
As expected, the (cid:3)x, y, θ(cid:4) classiﬁer was generally more ac-
curate than the PSR classiﬁer on the ROOM feature. Surpris-
ingly, for small amounts of data, the PSR classiﬁer is actually
more accurate. This indicates that the subjective state repre-
sentation is able to identify the most canonical examples of
the ROOM feature with very little data.
When testing the BACK-TO-WALL feature, the PSR clas-
siﬁer was substantially better than the (cid:3)x, y, θ(cid:4) classiﬁer.
BACK-TO-WALL can be characterized by a very simple pat-
tern of interaction even though it occurs in various locations
on the map in all orientations. This makes it an ideal candi-
date for being represented in a subjective representation in-
stead of a coordinate-based one. The decision tree that is
learned for the PSR classiﬁer with 750 core tests is notice-
ably lengthier (31 leaf nodes) than a manually crafted clas-
siﬁer would be (3 leaf nodes). We believe this to be due to
overﬁtting. When the same experiment is run on the BACK-
TO-WALL feature using fewer core tests (250 or fewer), the
3-node decision tree is found (Figure 3).

L 0 L 0

ON

OFF

NOT
WALL

L 1 L 0

ON

OFF

NOT
WALL

WALL

Figure 3: Decision tree classiﬁer for the BACK-TO-WALL fea-
ture learned using a predictive state representation with 250
or fewer core tests.

4.2 Experiment 2: Transfer using PSR
In this experiment, we investigate the degradation in classiﬁ-
cation accuracy when a decision tree learned on training data
from Map A is used to predict the abstract feature values on

IJCAI-07

1080

The results in Table 2 are produced using a classiﬁer
learned over different numbers of core tests. These tests are
the ﬁrst k found by a breadth-ﬁrst search, meaning they may
be quite short and ask questions only about states in the im-
mediate neighborhood. Most of the abstract features (BACK-
TO-WALL, NARROW-CORRIDOR and CORNER) are well char-
acterized by short tests as evidenced by their high accuracy
with only a 10 test PSR.

Map B. Good performance in Map B is evidence that a classi-
ﬁer learned over a subjective state representation can be suc-
cessful with novel experiences that share patterns of interac-
tion with previous experiences. Note that we will not com-
pare the PSR classiﬁer’s ability to transfer to (cid:3)x, y, θ(cid:4) classi-
ﬁer in this experiment because the (cid:3)x, y, θ(cid:4) decision tree has
no basis for being used in a different map. Indeed, (cid:3)x, y, θ(cid:4)
locations for the abstract features are highly map-speciﬁc and
not even translation or rotation invariant.

We perform this set of experiments using varying amounts
of core tests as features for the classiﬁer. We anticipate that
some abstractions (BACK-TO-WALL and CORNER) will be
easily identiﬁed with a small number of short tests as features.
The vague, larger area abstractions like ROOM and NARROW-
CORRIDOR may require more, longer tests. If this hypothesis
holds, accuracy on the ROOM and NARROW-CORRIDOR ab-
stractions should be worse for classiﬁers built using fewer
core tests, and better when more core tests are used.
Experimental Setup
All states in Map A and B are labeled as either positive or
negative examples of each abstract feature. Like the previous
experiment, the agent then uses the J48 algorithm to induce
decision tree classiﬁers from the training set, in this case all of
the states from Map A. The decision tree is then tested on all
examples from Map B. Because the agent has access to all of
the available training and testing data, only a single learning
trial is used and no standard deviation values are reported in
Table 2. We present the results of using 10, 50, 250, and 750
core tests.

The results labeled Baseline correspond to predicting the
value of the abstract feature that occurred most frequently in
the training data: it is the accuracy that a classiﬁer would get
with no information about each training example except its
label.

Baseline
10 tests
50 tests
250 tests
750 tests

ROOM
70.1%
87.1%
88.3%
91.5%
94.8%

CORRIDOR

BACK-WALL

78.9%
96.2%
97.6%
97.6%
97.6%

75%
100%
100%
100%
99.5%

CORNER
90.6%
98.6%
100%
99.2%
99.8%

Table 2: Accuracy of PSR-based decision trees when training
on Map A and testing on Map B with various amounts of tests.

Discussion
The transfer results, shown in Table 2, show the accuracy of
the decision trees for all four abstract features when tested on
Map B. All of the accuracies are very good, close to perfect
with the exception of the ROOM abstraction. ROOM is par-
ticularly hard, not only because it is vague, but because the
ROOM feature is locally aliased. That is, there are places in
Map B labeled as not ROOM that look very similar to areas
in Map A labeled as ROOM. This is evident in the bottom-
left and middle-right rooms in Figure 1. Both of these rooms
have sections that look locally very much like the double wide
area from the top of Map B in Figure 2, which is labeled as
not ROOM.

Figure 4: Errors identifying the value of the abstract feature
ROOM. Grid locations with marks indicate errors; ranging from
small circles (error in a single orientation) to large circles (error
in all four orientations).

The bottom rows in Table 2 (50, 250, and 750 tests) pro-
vide empirical support for the conjecture that larger area ab-
stractions may need more, longer core tests. Classiﬁcation
accuracy of the ROOM abstraction increases substantially as
more core test values are provided to the classiﬁcation algo-
rithm. The number of leaf nodes in the decision tree grows
from 9 leaves with 10 core tests, to 20, 44, and ﬁnally 55
with 50, 250, and 750 core tests respectively. This seems to
indicate that it is not the sheer volume of tests that is improv-
ing the quality of the classiﬁer: which tests are used makes a
difference in the accuracy of the classiﬁer.

A small amount of classiﬁcation accuracy was lost as we
moved up to 750 core tests, presumably because the low ra-
tio of features to training examples allowed the classiﬁer to
overﬁt the training data.
Figure 4 shows the location of misclassiﬁed ROOM feature in
Map B. The decision tree induced from 250 tests makes mis-
takes in cells that appear locally similar but have the opposite
labeling in Map A. The decision tree also misclassiﬁes certain

IJCAI-07

1081

[Kunde and Darken, 2006] Dietmar Kunde and Christian J.
Darken. A mental simulation-based decision-making ar-
chitecture applied to ground combat.
In Proceedings of
the Behavior Representation in Modeling & Simulation
(BRIMS), 2006.

[Littman et al., 2002] Michael L. Littman, Richard S. Sut-
ton, and Satinder Singh. Predictive representations of
state. In Advances in Neural Information Processing Sys-
tems 14, Cambridge, MA, 2002. MIT Press.

[McCallum, 1995] Andrew Kachites McCallum. Reinforce-
ment Learning with Selective Perception and Hidden State.
PhD thesis, University of Rochester, Rochester, New York,
1995.

[McCracken and Bowling, 2006] Peter McCracken

and
Michael Bowling. Online discovery and learning of
predictive state representations.
In Advances in Neural
Information Processing Systems 18, pages 875–882, 2006.
[McDonald et al., 2006] David McDonald, Alice Leung,
William Ferguson, and Talib Hussain. An abstraction
framework for cooperation among agents and people in a
virtual world. In Proceedings of the Artiﬁcial Intelligence
and Interactive Digital Entertainment conference (AIIDE),
Marina del Rey, California, 2006.

[Orkin, 2005] Jeff Orkin. Agent architecture considerations
for real-time planning in games. In Proceedings of the Ar-
tiﬁcial Intelligence and Interactive Digital Entertainment
conference (AIIDE), 2005.

[Paull and Darken, 2004] Gregory H. Paull and Christian J.
Darken. Integrated on- and off-line cover ﬁnding and ex-
ploitation. In Proceedings of GAME ON conference, 2004.
[Quinlan, 1993] J. Ross Quinlan. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Francisco, CA,
USA, 1993.

[Singh et al., 2003] Satinder Singh, Michael L. Littman,
Nicholas Jong, David Pardoe, and Peter Stone. Learn-
ing predictive state representations. In Proceedings of the
Twentieth International Conference on Machine Learning,
pages 712–719, 2003.

[Singh et al., 2004] Satinder Singh, Michael R. James, and
Matthew R. Rudary. Predictive state representations: A
new theory for modeling dynamical systems. In Proceed-
ings of the Twenthieth Conference on Uncertainty in Arti-
ﬁcial Intelligence, pages 512–519, 2004.

[Witten and Frank, 2005] Ian H. Witten and Eibe Frank.
Data Mining: Practical machine learning tools and tech-
niques. Morgan Kaufmann, San Francisco, 2nd edition
edition, 2005.

states that are not similar to anything seen in Map A.

5 Limitations
Our results provide evidence that subjective representations
have desirable generalization properties for representing ab-
stractions. However, our experiments were limited to a sim-
ple grid world and a linear predictive state representation.
Further substantiation of our claim will require wider experi-
mentation with more abstractions, various environments, and
more general subjective representations.

Our approach does not eliminate human experts: they are
still required to identify which abstractions will be useful.
This expert must also manually label a small amount of data
before the learning algorithm can be applied. In the near fu-
ture, an automated approach like reinforcement learning may
be used to learn the control policies for these agents. When
the expert is no longer creating control policies manually, it
will not be necessary for these abstractions to have an inter-
pretation suitable for humans. In this case, it would be very
useful to have the agent automatically discover useful ab-
stractions through its own experience [Grollman et al., 2006].

6 Conclusion
In this paper we have proposed a method for automatically
learning a mapping from low-level state representations to
high-level abstract features. In particular, we have demon-
strated that a subjective, predictive state representation can
generalize from a small sample of designer-labeled examples
to novel experience. Our experiments also show that a clas-
siﬁer learned over a predictive representation can effectively
classify new experience in a different environment with sim-
ilar high-level characteristics.

This work is a step along the path to the automatic con-
struction of virtual sensors that will be useful for creating
strong control policies. This generalization ability will en-
able the design of control policies applicable in a wide variety
of situations, allowing for richer domains and more complex
agents.

7 Acknowledgments
The authors gratefully acknowledge the ideas and encourage-
ment they received from Richard Sutton, Mike Bowling, Dan
Lizotte, Mark Ring and especially Martha Lednicky for help-
ing compile the results.

References
[Dini et al., 2006] Don M. Dini, Michael Van Lent, Paul Car-
penter, and Kumar Iyer. Building robust planning and
execution systems for virtual worlds.
In Proceedings of
the Artiﬁcial Intelligence and Interactive Digital Enter-
tainment conference (AIIDE), Marina del Rey, California,
2006.

[Grollman et al., 2006] D. H. Grollman, O. C. Jenkins, and
F. Wood. Discovering natural kinds of robot sensory ex-
periences in unstructured environments. Journal of Field
Robotics, In Press, 2006.

IJCAI-07

1082


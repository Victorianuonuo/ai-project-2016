Self-Adjusting Ring Modules (SARMs) for Flexible Gait Pattern Generation

† Institut f¨ur Informatik, Humboldt-Universit¨at zu Berlin, hild@informatik.hu-berlin.de
‡ Fraunhofer Institut AIS, Sankt Augustin, Germany, frank.pasemann@ais.fraunhofer.de

Manfred Hild†, Frank Pasemann‡

Abstract

Using the principle of homeostasis, we derive a
learning rule for a speciﬁc recurrent neural network
structure, the so-called Self-Adjusting Ring Mod-
ule (SARM). Several of these Ring Modules can be
plugged together to drive segmented artiﬁcial or-
ganisms, for example centipede-like robots. Con-
trolling robots of variable morphologies by SARMs
has major advantages over using Central Pattern
Generators (CPGs). SARMs are able to immedi-
ately reconﬁgure themselves after reassembly of
the robot’s morphology.
In addition, there is no
need to decide on a singular place for the robot’s
control processor, since SARMs represent inher-
ently distributed control structures.

Introduction

1
In order for an organism to move, it is necessary that its limbs
and its body itself move in a coordinated manner. A special
case of coordination is found in segmented organisms like
centipedes, where similar parts of the body move in the same
way, but phase-shifted. If we want to implement an artiﬁcial
organism that moves, i.e. an autonomous robot, we can use
a central pattern generator (CPG) to drive the motor joints
and thus select one of the well documented architectures from
the literature [Golubitsky et al., 1998]. But when a modular
structure that can be assembled during runtime is asked for,
it is unclear where a CPG should be located, so a distributed
approach will better ﬁt the requirements.

In the following sections we introduce a concept of dis-
tributed control for the generation of locomotion, based on a
single neural building block, the Self-Adjusting Ring Module
(SARM).

2 Two Neurons Form an SO(2)-Oscillator
Using a discrete time, fully connected recurrent neural net-
work with two neurons, no bias terms, and tanh as transfer
function, the only parameters to specify are the weights which
form a 2×2-matrix W2. This matrix is based on a rotation
matrix [Thompson and Steward, 2002], i.e an element of the
unitary group SO(2) and has the form

(cid:2)

W2 = (1 + )

(cid:3)

,

s

r−r

s

where

and

s = cos(2πφ),
r = sin(2πφ),

0 ≤ φ ≤ 1/2,
0 < .

Here s stands for the neurons’ recurrent self-connections,
and r for the ring-connections between the neurons. If we
choose  (cid:3) 1 and thus compensate for the damping prop-
erty of the transfer function tanh, the two neurons approx-
imately oscillate with frequency f0 = φfs, relative to the
update or sampling frequency fs of the network [Pasemann
et al., 2003]. Since the two neurons produce almost sine
waves that differ in phase by π/2, it is possible to gener-
ate a sine wave and other waveforms of the same frequency,
but with arbitrary amplitude and phase, just using a weighted
sum of the two neurons’ output signals [Hornik et al., 1989;
Cybenko, 1989].
2.1 A Simple Ring Module
We can extend the SO(2)-Oscillator to the more general form
of a ring oscillator by introducing more neurons [Pasemann,
1995]. Using m neurons (m ≥ 2), the weights can be written
as the following m×m-matrix
⎛
⎜⎜⎝ s

Vm = (wi,j) =

⎞
⎟⎟⎠ ,

r
...

...
s

r
s

where again s stands for the self-connections, r for the ring-
connections, and all other entries are zero. For the ring to
oscillate we choose the condition

−r

(cid:12)

(cid:10) (cid:11)

wi−1,i

wm,1 < 0,

i=2...m

which for n ≥ 1 and m = 2n is fulﬁlled by the following
alternative arrangement of weights:

IJCAI-07

848

frequency. The proportion ratio depends on the number of
neurons. This is exactly what we will need to implement a
ring module that is able to self-adjust its weights in order to
maintain a certain frequency and amplitude.

β = (−1)n.

3 Introducing the Principle of Homeostasis
A central tenet of modern physiology is homeostasis [Der and
Pantzer, 1999]. It describes the principle, that every living
organism tries to settle down to a healthy internal state af-
ter it has been disturbed. This desired state is also called an
equilibrium. We deﬁne the ring module to have reached its
equilibrium, if it oscillates with target frequency f0 and tar-
get amplitude A0. The phase shift will not be of concern
here. In order to apply homeostatic mechanisms to the ring
module, we ﬁrst have to introduce an appropriate measure for
the equilibrium [Urzelai and Floreano, 2001].
3.1 Discrete Sampled Sine Waves
If we sample one full period of a sine wave at N discrete time
steps we can study the following two sums:

(cid:2)
(cid:2)

(cid:14)(cid:14)(cid:14)(cid:14)A sin
(cid:14)(cid:14)(cid:14)(cid:14)A sin

N−1(cid:13)
N−1(cid:13)

k=0

k=0

(cid:3)(cid:14)(cid:14)(cid:14)(cid:14) ,

2πk
N

2π(k + 1)

N

(cid:3)

− A sin

(cid:2)

(cid:3)(cid:14)(cid:14)(cid:14)(cid:14) .

2πk
N

⎛
⎜⎜⎜⎜⎝

s

βr

U2n =

⎞
⎟⎟⎟⎟⎠ ,

r
s −r
...

...
s

r
s

Clearly we have W2 = V2 = U2, but for n > 1 always
V2n (cid:5)= U2n. If n is odd, then β = −1, therefore U2n can
be implemented as n identical copies of the following ring
module:

Figure 1: Structure of a Ring Module

SA(A, N) =

Sf (A, N) =

1
N

1
N

The modules have a switch at each end which connects the
two outputs as long as no other module is plugged in, thus
single modules oscillate stand-alone. When modules are
plugged together, their switches open up, so that the former
two rings unite to one. For even n one needs to introduce
a negation at some position in the ring, e.g. by means of a
simple switch.

Interesting applications of those modules emerge, if they
have motor joints included, which are driven by a linear com-
bination of the modules’ internal neurons. Flipping of one
ring module within a chain of modules changes the phase re-
lationships of all modules.
2.2 Varying Frequency and Amplitude
The mapping s = cos(2πφ), r = sin(2πφ) no longer holds
for the ring oscillators of type U2n, because the additional
neurons introduce a delay which in turn considerably lowers
the frequency. At the same time the amplitude rises, since
there are more self-connections present which boost up the
energy of the whole system.

There does not exist a simple formula for calculating s and
r as a function of φ, but if the desired frequency is rela-
tively low compared to the network’s update frequency, i.e.
for small φ, we ﬁnd that:

s = cos(2πφ) ≈ 1,
r = sin(2πφ) ≈ 2πφ.

In other words, the weights of the self-connections always
stay close to 1, mainly controlling the level of energy ﬂow in
the system and therefore the amplitude, whereas the weights
of the ring-connections are approximately proportional to the

The ﬁrst sum SA(A, N) calculates the average amplitude of
the rectiﬁed signal and can easily be approximated by:

SA(A, N) ≈ lim
(cid:15) 2π
N→∞ SA(A, N)
1
2π
2
π

· A.

=

=

0

|A sin x| dx

Thus, a sine wave of peak amplitude A results in an aver-
age absolute neural activity of 2A/π, independent of the fre-
quency. For N > 3 the second sum Sf (A, N) can be approx-
imated by
Sf (A, N) ≈ 4

(cid:14)(cid:14)(cid:14)(cid:14)A sin

− A sin

2π(k + 1)

(cid:3)

(cid:2)

(cid:3)(cid:14)(cid:14)(cid:14)(cid:14)

2πk
N

N

(cid:4)N/4(cid:5)−1(cid:13)
(cid:2)

k=0

(cid:2)
(cid:3)

2π(cid:7)N/4(cid:8)

N

N
4A
=
N
≈ 4A
N

sin

.

The approximation error

Ef (A, N) =

4A
N

− Sf (A, N) ≥ 0

is zero if N is a multiple of four. In general, the relative error
is below 3% for N > 10, and below 1% for N > 18. Alto-
gether, assuming a constant amplitude, the average variation
of neural activity is reciprocally proportional to the oscilla-
tion period.

IJCAI-07

849

3.2 The Learning Rules
Having deﬁned an equilibrium condition, we now need some
example sets for weights of stable oscillating ring networks.
As a starting point, we assume a network update frequency
of fs = 50Hz, which corresponds to the timing of stan-
dard servo motors. Choosing an oscillation frequency of
f0 = 1Hz as moderate speed for a walking machine, we have
φ = f0/fs = 1/50.

The larger the amplitude, the larger the distortions of the
sine wave, due to the non-linearity of tanh, so using A0 =
0.4 as target amplitude is a good choice. Varying the number
of ring modules n ∈ {1, 2, . . . , 6} with U2n as correspond-
ing weight matrix, we get the following results by numerical
simulation:

2n
2
4
6
8
10
12

s2n
1.036
0.905
0.814
0.721
0.642
0.550

r2n
0.132
0.187
0.260
0.345
0.420
0.510

s2n + r2n

1.168
1.092
1.074
1.066
1.062
1.060

Table 1: Weight Settings

If we start with three coupled ring modules and then add
one module, the amplitude grows by ΔA = 0.2 from 0.4 to
0.6. In order to compensate for this effect, we have to adjust
the self-coupling weights by s8 − s6 = −0.093. If we leave
out one module, the weights have to be adjusted accordingly
by s4 − s6 = +0.091, so the average absolute adjustment is
Δs = 0.092. Putting this together, we end up with a simple
learning rule for the self-coupling weights, which stabilizes
the amplitude:

s(t + 1) = s(t) + sδ(t),
2
π

sδ(t) = εs

Δsf0
ΔAfs

(cid:2)

(cid:3)
A0 − |σ(t)|

,

where s(t) is the weight and σ(t) the neuron’s output at time
t, A0 and f0 are target amplitude and frequency, and Δs, ΔA,
and fs are deﬁned as above. εs is the normalized learning
rate, i.e. εs = 1.0 means, that after changing the number
of connected ring modules, the target amplitude A0 will be
reached again after the next full oscillation period.

Two things should be noted: ﬁrstly, the appearance of f0
in the learning rule is just part of the learning rate’s normal-
ization and not essential for the proper amplitude regulation.
Secondly, since all parameters are known in advance, the for-
mula can easily be implemented as

sδ(t) = η (μ − |σ(t)|) ,

with pre-calculated constants η and μ, according to A0, ΔA,
f0, fs, εs, and Δs. Thus, only a multiplication, an addition,
and a sign ﬂip is needed. Because of the computationally
low cost, this learning rule can run even on simplest 8 bit-
microprocessors [Hikawa, 2003].

In an analogous manner we get a learning rule for the
weights of the ring-connections, which stabilizes the oscil-
lation frequency:

r(t + 1) = r(t) + rδ(t),

rδ(t) = εr

Δr

ΔSf fs

f0

4A0
fs

(cid:2)

(cid:3)
f0 − |σ(t) − ˆσ(t)|

,

where r(t) is the weight at time t, σ(t) and ˆσ(t) are the out-
puts of the neurons after and before the r-weighted connec-
tion, respective, εr is the normalized learning rate, and the
rest as deﬁned above. Again, this learning rule can also be
written in a simple form as

rδ(t) = η (μ − |σ(t) − ˆσ(t)|) ,

with pre-calculated constants η and μ. Note, that despite the
term −ˆσ(t), both formulas are identical. Clearly, ˆσ(t) must
be zero in the ﬁrst learning rule, since there is only one neuron
involved with a self-connection.

Since rδ(t) controls the oscillation frequency, but depends
on A0 and f0 at the same time, this learning rule will only
work properly combined with the learning rule for the self-
connections. Care has to be taken with the choice of εs
and εr, because too high a value will deform the waveform’s
shape and cause uncontrolled interactions between the two
learning rules. Good parameter choices are given at the end.
3.3 Diffusion Processes
If we assemble some ring modules with well-tuned parame-
ters and then switch them on, the learning rules reliably adjust
oscillation frequency and amplitude within a few oscillation
periods. This will work equally well for a single module or
up to six and more modules.

Now, if we separate some modules and reassemble them
all during runtime, something special may happen. Since
the separated modules go on oscillating and adjusting their
weights independently from each other, we have an unpre-
dictable distribution of weight values and neural activities at
the time we reconnect the modules. The modules then end
up with an unequal weight distribution. Frequency and am-
plitude will be well preserved by the learning rules, but the
relative phase shift between the modules on one side, and be-
tween the two neurons within one module on the other side
will no longer be the same. In order to compensate for this,
we introduce two independent diffusion processes, one for
each weight pair s, s(cid:6) and r, r(cid:6) within a module, as follows:

¯s(t) = s(t) + s(cid:6)(t)

,

s(t + 1) = δs¯s(t) + (1 − δs) s(t),
s(cid:6)(t + 1) = δs¯s(t) + (1 − δs) s(cid:6)(t),

2

2

and similarly for the ring-connections:
¯r(t) = r(t) + r(cid:6)(t)

,

r(t + 1) = δr¯r(t) + (1 − δr) r(t),
r(cid:6)(t + 1) = δr¯r(t) + (1 − δr) r(cid:6)(t).

IJCAI-07

850

The diffusion processes have to be applied immediately after
the learning rule’s weight update. Generally it is sufﬁcient to
use small values for δs and δr, i.e. 0 ≤ δs, δr (cid:3) 1.

When implemented on a simple 8 bit-microprocessor, as
mentioned above, only additions are needed, since a division
by two can be implemented as a right shift. The same is true
for smart choices of δs and δr.

4 Results and Discussion
To illustrate the effect of the homeostatic learning rules and
diffusion processes, we simulate hot plugging and unplug-
ging of one to ﬁve continuously operating SARMs, ﬁrst with
ﬁxed weights, then with homeostatic control of the weights.
For the ﬁrst simulation we set s = 0.814, r = 0.260, and
start with three concatenated SARMs. The result can be seen
in the following ﬁgure:

t

u
p
u
o

t

 
l

a
r
u
e
n

1.5

1

0.5

0

-0.5

0

o
s
r

500

1000

1500

2000

2500

3000

3500

4000

time steps

Parameter

Target Amplitude
Target Frequency

Network Update Frequency

Learning Rate Self-Connections
Learning Rate Ring-Connections

Diffusion Self-Connections
Diffusion Ring-Connections

Symbol

A0
f0
fs
εs
εr
δs
δr

Value
0.4
1Hz
50Hz
1.0
5.0
0.05
0.1

Table 2: Parameter Settings

depends on the amplitude, whereas the amplitude controlling
adjustment of the self-connections is frequency independent.
Table 2 shows which parameter settings have been proven to
be robust. Using these settings we get the following system
behavior:

t

u
p
u
o

t

 
l

a
r
u
e
n

1.5

1

0.5

0

-0.5

0

o
s
r

500

1000

1500

2000

2500

3000

3500

4000

time steps

Figure 2: Neural Output When Using Fixed Weights

Figure 3: Neural Output of Self-Adjusting Ring Modules

As expected, the three ring modules oscillate with amplitude
A0 = 0.4 and frequency ratio f0/fs = 1/50, which corre-
sponds to an output frequency of 1.0Hz, assuming 50 time
steps per second. After 10 seconds (at time step 500) two ad-
ditional SARMs are plugged in. In consequence, the ampli-
tude slightly increases and the frequency considerably lowers
down from 1.0Hz to 0.6Hz. After further 20 seconds (at time
step 1500) four of the ﬁve SARMs are removed and almost
immediately the oscillation completely breaks down. 20 sec-
onds later again two SARMs are added (at time step 2500),
but the system is not able to recover from zero to oscillation.
Theoretically oscillation should restart, but due to the ﬁnite
accuracy (even) of double precision numbers this is practi-
cally never the case.

We now repeat the whole sequence of plugging and un-
plugging, but with learning rules and diffusion processes
switched on.
In order to have comparable conditions, we
use appropriate parameters for target amplitude, target fre-
quency, and network update frequency. As pointed out ear-
lier, it is crucial to quickly reach a stable amplitude, since
the frequency controlling adjustment of the ring-connections

Clearly the SARMs now ﬁnd back to their equilibrium, after
being disturbed by hot reassembly. As can be seen, ﬁrst the
amplitude is back to 0.4 after a few periods, whereas the fre-
quency settles down afterwards (see Figure 3, time steps 500
to 700, 1600 to 2000, and 2500 to 3000). Three ﬁndings are
to be mentioned in comparison to the simulation with ﬁxed
weights:
• Depending on the number of active SARMs, all weights
target their optimal value as listed in Table 1. This impli-
cates, that each SARM in principle is able to derive the
exact number of attached SARMs – without dedicated
communication and regardless of their distance. The
starting weights s = 0.814 and r = 0.260 are reached
again, once the starting number of SARMs is restored.
Loosely spoken, each part of the body statically feels
the rest of the body and is dynamically aware of mor-
phological changes to the whole body.
• The oscillation never breaks down (see time steps 1500
to 1700). More loosely spoken and in analogy to physi-

IJCAI-07

851

tive behavior in new environments. Evolutionary Com-
putation, 9(4):495–524, 2001.

ology it can be stated, that homeostasis prevents the (ar-
tiﬁcial) organism from death.
• Close inspection of the slow varying self-connections re-
veals, that the target frequency shines through slightly
(can be best seen shortly after time step 2500). This in-
dicates an optimal setting of the self-connection’s learn-
ing rate. A higher rate would lead to destabilization,
whereas lower rates would prolongate reaction time.

Despite these more analytical and philosophical reﬂec-
tions, the Self-Adjusting Ring Modules can practically be
built in hardware and used to make artiﬁcial creatures move.
An actuated modular body construction system as described
in [Rafﬂe, 2004] forms an optimal application. It then is a
good idea to have the modules equipped with a switch which
introduces a negation before one ring-connection, as nec-
essary for an even number of interconnected ring modules.
Now, this switch can also be used to turn off the oscillation
of the whole network and settle down into a conﬁguration of
constant neural output signals. Different steady output con-
ﬁgurations are possible, just by switching on the negation at
different modules.

References
[Cybenko, 1989] G. Cybenko. Approximation by superpo-
sition of a sigmoidal function. Mathematics of Control,
Signal and Systems, 2:303–314, 1989.

[Der and Pantzer, 1999] R. Der and T. Pantzer. Emergent
robot behavior from the principle of homeokinesis. Pro-
ceedings of the 1st International Khepera Workshop, 1999.
I. Stewart, P.-L.
Buono, and J. J. Collins. A modular network for legged
locomotion. Physica D, 105:56–72, 1998.

[Golubitsky et al., 1998] M. Golubitsky,

[Hikawa, 2003] H. Hikawa.

A digital hardware pulse-
mode neuron with piecewise linear activation function.
IEEE Transactions on Neural Networks, 14(5):1028–
1037, 2003.

[Hornik et al., 1989] K. Hornik, M. Stinchcombe,

and
H. White. Multilayer feedforward networks are universal
approximators. Neural Networks, 2:359–366, 1989.

[Pasemann et al., 2003] F. Pasemann, M. Hild, and K. Za-
hedi.
SO(2)-networks as neural oscillators. Proc. of
Int. Work-Conf. on Artiﬁcial and Natural Neural Networks
(IWANN), 2003.

[Pasemann, 1995] F. Pasemann. Characterization of peri-
odic attractors in neural ring networks. Neural Networks,
8:421–429, 1995.

[Rafﬂe, 2004] H. S. Rafﬂe. Topobo: A 3-D constructive as-
sembly system with kinetic memory. Master’s thesis, Pro-
gram in Media Arts and Sciences, MIT, 2004.

[Thompson and Steward, 2002] J. M. T. Thompson and
John

H. B. Steward. Nonlinear Dynamics and Chaos.
Wiley & Sons, Ltd, 2002.

[Urzelai and Floreano, 2001] J. Urzelai and D. Floreano.
Evolution of adaptive synapses: Robots with fast adap-

IJCAI-07

852


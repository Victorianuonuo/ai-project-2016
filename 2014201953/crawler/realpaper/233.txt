Algebraic Markov Decision Processes

Patrice Perny

LIP6 - Université Paris 6

4 Place Jussieu

Olivier Spanjaard

LIP6 - Université Paris 6

4 Place Jussieu

Paul Weng

LIP6 - Université Paris 6

4 Place Jussieu

75252 Paris Cedex 05, France

75252 Paris Cedex 05, France

75252 Paris Cedex 05, France

patrice.perny@lip6.fr

olivier.spanjaard@lip6.fr

paul.weng@lip6.fr

Abstract

In this paper, we provide an algebraic approach to
Markov Decision Processes (MDPs), which allows
a uniﬁed treatment of MDPs and includes many ex-
isting models (quantitative or qualitative) as par-
ticular cases. In algebraic MDPs, rewards are ex-
pressed in a semiring structure, uncertainty is rep-
resented by a decomposable plausibility measure
valued on a second semiring structure, and prefer-
ences over policies are represented by Generalized
Expected Utility. We recast the problem of ﬁnd-
ing an optimal policy at a ﬁnite horizon as an alge-
braic path problem in a decision rule graph where
arcs are valued by functions, which justiﬁes the
use of the Jacobi algorithm to solve algebraic Bell-
man equations. In order to show the potential of
this general approach, we exhibit new variations
of MDPs, admitting complete or partial preference
structures, as well as probabilistic or possibilistic
representation of uncertainty.

Introduction

1
In the ﬁeld of planning under uncertainty,
the theory of
Markov Decision Processes (MDPs) has received much at-
tention as a natural framework both for modeling and solving
complex structured decision problems, see e.g. [Dean et al.,
1993; Kaebling et al., 1999]. In the standard MDP approach,
the utilities of actions are given by scalar rewards supposed
to be additive, uncertainty in the states of the world and in
the consequences of the actions are represented with proba-
bilities, and policies are evaluated using the expected utility
(EU) model. Although these choices are natural in various
practical situations, many other options are worth investigat-
ing for several reasons :
• Rewards of actions are not necessarily scalar nor additive.
In multi-agent planning or in multicriteria MDPs, the utility
of any action is given by a vector of rewards (one per agent or
criterion) and actions are compared according to Pareto dom-
inance [Wakuta, 1995]. In qualitative frameworks, rewards
are valued on an ordinal scale and therefore are not additive.
The sum is then replaced by the min, max or any reﬁnement
of them, depending on the context.

• Non-probabilistic representation of uncertainty might be
of interest. In practice, it is sometimes difﬁcult to quantify
precisely the plausibility of states and consequences of ac-
tions. Assessing probabilities in such situations seems difﬁ-
cult. For this reason, alternative approaches based on quali-
tative representations of uncertainty have been proposed (see
e.g. [Darwiche and Ginsberg, 1992; Dubois and Prade, 1995;
Wilson, 1995]) and might be used in the context of MDPs.
• Non-EU theories offer also interesting descriptive possibil-
ities. Despite the appeal of the expected utility model and
its theoretical foundations [von Neumann and Morgenstern,
1947; Savage, 1954], recent developments of decision theory
have shown the descriptive potential of alternative representa-
tions of preferences under uncertainty. For example, the rank
dependent expected utility theory (RDEU) is a sophistication
of EU theory using probability transforms to better account
for actual decision making behaviors under risk [Quiggin,
1993]. Besides this extension, various alternatives to EU have
been proposed for decision making in a non-probabilistic set-
ting. Among them, let us mention the qualitative expected
utility (QEU) theories proposed in [Dubois and Prade, 1995]
and [Giang and Shenoy, 2001], and very recently, the gen-
eralized expected utility theory (GEU) proposed in [Chu and
Halpern, 2003a; 2003b] which generalizes the notion of ex-
pectation for general plausibility measures.

Despite the diversity of models proposed for decision mak-
ing under uncertainty, very few of them are used in the con-
text of dynamic decision making. This gap can be explained
by the inconsistencies entailed by the use of non-EU criteria
(typically RDEU) in the dynamic context [Machina, 1989;
Sarin and Wakker, 1998]. Indeed, when using a given non-
linear utility criterion at each decision stage, the Bellman
principle is generally violated, so that backward induction is
likely to generate a dominated policy; further, there is in gen-
eral no operational way to determine an optimal policy. This
simple statement largely explains the predominance of the EU
model in dynamic decision making under risk.

In the last decade however, some alternative models to EU
have been proved to be dynamically consistent, thus provid-
ing new possibilities for sequential decision making. This
is the case of qualitative expected utility theory [Dubois and
Prade, 1995] that has led to a possibilistic counterpart of
MDPs [Sabbadin, 1999] with efﬁcient algorithms adapted

from backward induction and value iteration, substituting op-
erations (+, ×) by (max, min) in computations. In the same
vein, [Littman and Szepesvári, 1996] propose a generalized
version of MDPs where max and + are substituted by ab-
stract operators in Bellman equations, and [Bonet and Pearl,
2002] propose a qualitative version of MDPs. Besides these
positive results, very few alternatives to standard MDPs have
been investigated.

Among the diversity of possible choices for deﬁning a re-
ward system, a plausibility measure over events and a pref-
erence over lotteries on rewards, we need to know which
combination of them can soundly be used in the context of
MDPs and which algorithm should be implemented to deter-
mine an optimal policy. For this reason, we propose in this pa-
per an algebraic generalization of the standard setting, relying
on the deﬁnition of a semiring structure on rewards, a semi-
ring structure on plausibilities of events, and a generalized
expectation model as decision criterion [Chu and Halpern,
2003a]. The generalization power of semirings have been al-
ready demonstrated in AI by [Bistarelli et al., 1995] in the
context of constraint satisfaction problems. Within this gen-
eral setting, our aim is to present a uniﬁed treatment of MDPs
and to provide algorithmic solutions based on the general Ja-
cobi algorithm (initially introduced to solve systems of linear
equations).

The paper is organized as follows: in Section 2, we show
by example how to recast an MDP as an optimal path prob-
lem in a decision graph. Then we introduce an algebraic
framework for MDPs, relying on the deﬁnition of algebraic
structures on rewards, plausibilities and expectations (Section
3). In Section 4 we justify the use of the Jacobi algorithm
as a general procedure to determine an optimal policy in an
Algebraic MDP (AMDP). Finally, we consider in Section 5
some particular instances of AMDPs, including new proba-
bilistic and possibilistic MDPs using partial preferences over
rewards.

2 Decision Rule Graph in MDPs
We brieﬂy recall the main characteristics of a Markov Deci-
sion Process (MDP) [Puterman, 1994]. It can be described as
a tuple (S, A, T, R) where:
• S = {s1, . . . , sn} is a ﬁnite set of states,
• A = {a1, . . . , am} is a ﬁnite set of actions,
• T : S × A → Pr(S) is a transition function, giving for each
state and action, a probability distribution over states (in the
sequel, we write T (s, a, s′) for T (s, a)(s′)),
• R : S × A → R is a reward function giving the immediate
reward for taking a given action in a given state.
A decision rule is a function from the set of states S to the
set of actions A. There are N = mn available decision rules
at each step. We write ∆ = AS = {δ1, . . . , δN } the set of
decision rules. A policy at step t (i.e., the tth-to-last step) is
a sequence of t decision rules. For a policy π and a decision
rule δ, we note (δ, π) the policy which consists in applying
ﬁrst decision rule δ and then policy π.

A history is a realizable sequence of successive states and
actions. The accumulated reward corresponding to a his-
tory γt = (st, at, st−1, . . . , a1, s0) (with initial state st) is

δ2
a1
a2

δ1
a1
a1

δ4
s1
a2
s2
a2
Table 1: Decision rules.

δ3
a2
a1

R(γt) = Pt

i=1 R(si, ai). We denote Γt(s) the set of t-step
histories starting from s. For an initial state s, a t-step policy
π = (δt, . . . , δ1) induces a probability distribution Prπ
t (s, ·)
over histories. The t-step value of being in state s and exe-
cuting policy π is given by (expected accumulated reward):

t (s) = Pγ∈Γt(s) Prπ

vπ
the vector whose ith component is vπ

t (s, γ)R(γ)

Denoting vπ
t (si), any
t
two t-step policies π, π′ can be compared using the compo-
nentwise dominance relation ≥Rn deﬁned by:

t ≥Rn vπ′
vπ

t ⇐⇒ (cid:0)∀s ∈ S, vπ

t (s) ≥ vπ′

t (s)(cid:1)

t

0

The t-step value of a policy of the form (δi, π) is deﬁned re-
cursively by v(δi,π)
t−1),
where f i : Rn → Rn is the update function which associates
to any vector x = (x1, . . . , xn) the vector (f i
n(x))
where f i

= (0, . . . , 0) and v(δi,π)

k=1 T (sj, δi(sj), sk)xk.

1(x), . . . , f i

= f i(vπ

j (x) = R(sj, δi(sj)) +Pn

Example 1 Consider an MDP with S = {s1, s2}, A =
{a1, a2}, T (si, aj, sk) = 1 if i = j = k and 0.5 other-
wise, R(s1, a1) = 8, R(s1, a2) = 7, R(s2, a1) = 12 and
R(s2, a2) = 11. Thus, there are N = 4 available decision
rules at each step (see Table 1). For instance, decision rule
δ2 consists in applying action a1 in states s1 and action a2
in state s2. In this example, the functions f i are given by:
f 1(x1, x2) = (8 + x1, 12 + 0.5x1 + 0.5x2)
f 2(x1, x2) = (8 + x1, 11 + x2)
f 3(x1, x2) = (7 + 0.5x1 + 0.5x2, 12 + 0.5x1 + 0.5x2)
f 4(x1, x2) = (7 + 0.5x1 + 0.5x2, 11 + x2)
for all (x1, x2) ∈ R2.

Given a ﬁnite horizon H, the optimal policy π∗ can be

found thanks to the following Bellman equations:

vπ∗
0 = (0, . . . , 0)
vπ∗
t = maxi=1...N f i(vπ∗

t−1)

t = 1 . . . H

(1)

t, δj

The solution of these equations can be reduced to a vector-
weighted optimal path problem in a particular graph, with up-
date functions (the f i’s) on the arcs allowing the propagation
of policy values over nodes. Indeed, consider a graph where
t corresponds to decision rule δi at step t, and
each node δi
each arc of the form (δi
t−1) corresponds to a transition be-
tween decision rules. Moreover, nodes δj
1, j = 1 . . . N are
connected to a sink denoted 0, and a source denoted H is
connected to nodes δj
H, j = 1 . . . N. Hence, any path from
t to node 0 corresponds to a t-step policy where deci-
node δi
sion δi is applied ﬁrst. We name that graph the decision rule
graph. Note that the Bellman update via f i’s is nicely sepa-
rable (f i
j’s can be computed independently) and therefore the
vector value of a path can be obtained componentwise (state
by state) as usual in classic MDP algorithms. This property
will be exploited later on for Algebraic MDPs. Coming back
to Example 1 and assuming that H = 2, there are 16 available

H

δ1
2



*

δ2
2

j

δ3
2

^

δ4
2

-
37


δ1
1

s-
3
7

δ2
1

-
s
w
3

δ3
1

w-
s
U

δ4
1

^j*

0

Figure 1: A decision rule graph.

policies (all possible combinations of two successive decision
rules). The corresponding graph is pictured on Figure 1.

For all i = 1 . . . n, function f i is associated to every arc
issued from δi
t, t = 1 . . . H. Moreover, the identity function
is assigned to every arc issued from H. Thus, the optimal
policy can be computed thanks to backward induction on the
decision rule graph, by propagating the value (0, . . . , 0) ∈
Rn from the sink 0 corresponding to the empty policy.
In
Example 1, backward induction leads to the labels indicated
in Table 2. The optimal policy value can be recovered on
node H. The optimal vector value is (17, 23) and the optimal
policy (recovered from bolded values in Table 2) is (δ4
1).

2, δ1

t
1
2

δ1
t

(8,12)
(16,22)

δ2
t

(8,11)
(16,23)

δ3
t

(7,12)
(17,22)

δ4
t

(7,11)
(17,23)

Table 2: Labels obtained during backward induction.

In the next section, we show how to generalize this ap-
proach to a wide range of MDPs. In this concern, we intro-
duce the notion of algebraic Markov decision process.

3 Algebraic Markov Decision Process
We now deﬁne a more general setting to model rewards and
uncertainty in MDPs. Our approach relies on previous works
aiming at generalizing uncertainty measurement and expec-
tation calculus. Our rewards take values in a set V and we
use plausibility measures1 [Friedman and Halpern, 1995] to
model uncertainty. A plausibility measure Pl is here a func-
tion from 2W (the set of events) to P , where W is the set
of worlds, P is a set endowed with two internal operators ⊕P
and ⊗P (the analogs of + and × in probability theory), a (pos-
sibly partial) order relation (cid:23)P , and two special elements 0P
and 1P such that 1P (cid:23)P p (cid:23)P 0P for all p ∈ P . Further-
more, Pl veriﬁes Pl(∅) = 0P , Pl(W ) = 1P and Pl(X) (cid:23)P
Pl(Y ) for all X, Y such that Y ⊆ X ⊆ W . We assume here
that Pl is decomposable, i.e. Pl(X ∪ Y ) = Pl(X) ⊕P Pl(Y )
for any pair of disjoint events X and Y . To combine plausi-
bilities and rewards, we use the generalized expectation pro-
posed by [Chu and Halpern, 2003a]. This generalized ex-
pectation is deﬁned on an expectation domain (V, P, ⊞, ⊠) 2

1This notion must not be confused with the Dempster-Shafer no-

tion of plausibility function.

2In [Chu and Halpern, 2003a], an expectation domain is written
(U, P, V, ⊞, ⊠). This structure can be simpliﬁed here since V = U.

where ⊞ : V × V → V and ⊠ : P × V → V are the coun-
terparts of + and × in probabilistic expectation, and the three
following requirements are satisﬁed: (x⊞y)⊞z = x⊞(y⊞z),
x ⊞ y = y ⊞ x, 1P ⊠ x = x. For any plausibility distribution
Pl on V having its support in X, the generalized expectation

writes: P⊞

x∈X Pl(x) ⊠ x.

An Algebraic MDP (AMDP) is described as a tuple

(S, A, T, R), where T and R are redeﬁned as follows:
• T : S × A → Pl(S) is a transition function, where Pl(S)
is the set of plausibility measures over S valued in P ,
• R : S × A → V is a reward function giving the immediate
reward in V .
Consistently with the standard Markov hypothesis, the next
state and the expected reward depend only on the current state
and the action taken. In particular, plausibility distributions
of type T (s, a) are (plausibilistically) independent of the past
states and actions. This plausibilistic independence refers to
the notion introduced by [Friedman and Halpern, 1995] and
leads to the following algebraic counterpart of the probabilis-
tic independence property: Pl(X ∩ Y ) = Pl(X) ⊗P Pl(Y )
for any pair X, Y of independent events.

In this setting, rewards and plausibilities take values in two
semirings. Roughly speaking, a semiring is a set endowed
with two operators allowing the combination of elements (re-
wards or plausibilities) together. We now recall some deﬁni-
tions about semirings.

Deﬁnition 1 A semiring (X, ⊕, ⊗, 0, 1) is a set X with two
binary operations ⊕ and ⊗, such that:
(A1) (X, ⊕, 0) is a commutative semigroup with 0 as neutral
element (i.e., a⊕b = b⊕a, (a⊕b)⊕c = a⊕(b⊕c), a⊕0 = a).
(A2) (X, ⊗, 1) is a semigroup with 1 as neutral element, and
for which 0 is an absorbing element (i.e., (a ⊗ b) ⊗ c = a ⊗
(b ⊗ c), a ⊗ 1 = 1 ⊗ a = a, a ⊗ 0 = 0 ⊗ a = 0).
(A3) ⊗ is distributive with respect to ⊕ (i.e., (a ⊕ b) ⊗ c =
(a ⊗ c) ⊕ (b ⊗ c), a ⊗ (b ⊕ c) = (a ⊗ b) ⊕ (a ⊗ c)).

A semiring is said to be idempotent when (X, ⊕) is
an idempotent commutative semigroup (i.e., a commutative
semigroup such that a ⊕ a = a). The idempotence of ⊕ en-
ables to deﬁne the following canonical order relation (cid:23)X:

a (cid:23)X b ⇐⇒ a ⊕ b = a

∀a, b ∈ X

From now on, we will assume that the rewards are elements
of an idempotent semiring (V, ⊕V , ⊗V , 0V , 1V ). Operator
⊕V is used to select the optimal values in V , whereas operator
⊗V is used to combine rewards. In classic MDPs with the
total reward criterion, ⊕V = max and ⊗V = +.

Moreover the structure (P, ⊕P , ⊗P , 0P , 1P ) is also sup-
posed to be a semiring. Operator ⊕P allows to combine the
plausibilities of disjoint events and operator ⊗P allows to
combine the plausibilities of independent events. Note that
the assumption that (P, ⊕P , ⊗P , 0P , 1P ) is a semiring is not
very restrictive since [Darwiche and Ginsberg, 1992], who
use similar properties to deﬁne symbolic probability, have
shown that it subsumes many representations of uncertainty,
such as probability theory, possibility theory and other impor-
tant calculi used in AI.

Now that the general framework has been deﬁned, we
can follow the usual approach in MDPs and deﬁne a value
function for policies. The accumulated reward for a history
i=1 R(si, ai). For
an initial state s, a t-step policy π = (δt, . . . , δ1) induces a
plausibility measure Plπ
t (s, ·) over histories. Such a policy
will be evaluated with respect to the generalized expectation
of accumulated reward, which writes:

γ = (st, at, st−1, . . . , a1, s0) is R(γ) =Nt

vπ

t (s) = P⊞

γ∈Γt(s) Plπ

t (s, γ) ⊠ R(γ)

The policies can be compared with respect to the componen-
twise dominance relation (cid:23)V n between vectors in V n:

x (cid:23)V n y ⇐⇒ (∀i = 1, . . . , n, xi (cid:23)V yi)

for all x = (x1, . . . , xn), y = (y1, . . . , yn) ∈ V n.

Most of the MDPs introduced previously in the literature

are instances of our algebraic MDP:
- In standard MDPs,
the underlying algebraic structure
on P is (P, ⊕P , ⊗P , 0P , 1P ) = ([0, 1], +, ×, 0, 1), and
operators ⊞ = + and ⊠ = × are used to deﬁne
the classic expectation operation. When rewards are de-
ﬁned on (V, ⊕V , ⊗V , 0V , 1V ) = (R, max, +, −∞, 0) where
R = R ∪ {−∞}, we recognize the total reward crite-
rion. With (R, max, +γ , −∞, 0) (where x +γ y = x + γy),
we recognize the weighted total reward criterion. With
(R, max, +H , −∞, 0) where a+H b = 1
H a+b, we recognize
the average reward criterion assuming that there is an initial
dummy step with zero reward.
- Qualitative MDPs, introduced in [Bonet and Pearl, 2002],
are AMDPs where the rewards and the plausibility measures
are deﬁned on the semiring of two-sided inﬁnite formal se-
ries, which is a subset of the extended reals [Wilson, 1995].
In order to assign functions to the arcs in the decision rule
j : V n → V (the update function after

graph, we ﬁrst deﬁne f i
applying decision rule δi in state sj), for all x ∈ V n, by:

j (x) = R(sj, δi(sj)) ⊗V (cid:16)P⊞

f i
Then, for any decision rule δi, we deﬁne the update function
f i : V n → V n which associates to any vector x ∈ V n the
vector (f i

i=1..n T (sj, δi(sj), si) ⊠ xi(cid:17).

1(x), . . . , f i

n(x)).

Dynamic consistency in AMDPs is guaranteed by speciﬁc
properties on functions f i. The fulﬁllment of these properties
strongly relies on the following conditions:
(C1) p ⊠ (x ⊕V y) = (p ⊠ x) ⊕V (p ⊠ y)
(C2) x ⊞ (y ⊕V z) = (x ⊞ y) ⊕V (x ⊞ z)
(C3) p ⊠ (q ⊠ x) = (p ⊗P q) ⊠ x

(C4) P⊞

i pi ⊠ (x ⊗V yi) = x ⊗V (P⊞

(C5) p ⊠ (x ⊞ y) = (p ⊠ x) ⊞ (p ⊠ y)
for all p, q, pi ∈ P, x, y, z, yi ∈ V .

i pi ⊠ yi)

Conditions (C1) and (C2) are two distributivity properties
entailing a kind of additivity of (cid:23)V w.r.t. ⊠ and ⊞ (i.e.,
x (cid:23)V y ⇒ (z ∗ x (cid:23)V z ∗ y) for ∗ ∈ {⊠, ⊞}). Condition
(C3) enables the reduction of lotteries. Condition (C4) en-
ables to isolate a sure reward in a lottery and is similar to the
distributivity axiom used in [Luce, 2003]. Condition (C5) is
a distributivity condition as in classic expectation. We now
establish a monotonocity result that will be used later (Prop.

3) to justify a dynamic programming approach.
Proposition 1 If (C1) and (C2) hold,
decreasing for all δi ∈ ∆, i.e

then f i

is non-

∀x, y ∈ V n, (cid:0)x (cid:23)V n y ⇒ f i(x) (cid:23)V n f i(y)(cid:1)

Proof. Let x, y in V n s.t. x (cid:23)V n y. For sj ∈ S, we have
k T (sj, δi(sj), sk) ⊠yk
by (C1) and (C2). Thanks to distributivity of ⊗V over ⊕V ,
we have f i

P⊞
k T (sj, δi(sj), sk) ⊠xk (cid:23)V P⊞

j (y). Therefore, f i(x) (cid:23)V n f i(y).

j (x) (cid:23)V f i

Moreover, the value of a policy π can be computed recur-

0 = (1V , . . . , 1V ).

sively thanks to the following result:
Proposition 2 Let π = (δi, π′) a (t + 1)-step policy, and
If (C3), (C4) and (C5)
assume that vπ
t+1 = f i(vπ′
hold, then vπ
Proof.
Let s be a state and a denote δi(s). We note
γt = (st, at, . . . , a1, s0) and γt+1 = (s, a, st, at, . . . , a1, s0).
We have:

t ) for all t ≥ 0.

vπ
t+1(s) =

Plπ

t+1(s, γt+1) ⊠ R(γt+1)

γt+1∈Γt+1(s)



 

⊞X

 



T (s, a, s′)⊗P Plπ′

t (s′, γt)

⊠

R(s, a)⊗V R(γt)

T (s, a, s′) ⊗P Plπ′

t (s′, γt)

⊠ R(γt)

s′ ∈S

γt∈Γt(s′)

T (s, a, s′) ⊠

Plπ′

t (s′, γt) ⊠ R(γt)





⊞X

⊞X

=

s′∈S

γt ∈Γt(s′)

= R(s, a) ⊗V

by (C4)

= R(s, a) ⊗V

⊞X
 
⊞X
⊞X
⊞X

s′∈S

 

⊞X

by (C3) and (C5)

= R(s, a) ⊗V

s′∈S

γt∈Γt(s′)

T (s, a, s′) ⊠ vπ′

t (s′)

Therefore, for all sj ∈ S, vπ

t+1(sj) = f i

j (vπ′

t ).

In order to establish the algebraic version of Bellman equa-
tions, we deﬁne the subset of maximal elements of a set with
respect to an order relation (cid:23) as:

∀Y ⊆ X, M (Y, (cid:23)) = {y ∈ Y : ∀z ∈ Y, not(z ≻ y)}

Furthermore, we denote P ∗(X, (cid:23)) the set {Y ⊆ X : Y =
M (Y, (cid:23))}. When there is no ambiguity, these sets will be
denoted respectively M (Y ) and P ∗(X). Besides, for any
function f : V n → V n, for all X ∈ P ∗(V n), f (X) denote
{f (x) : x ∈ X}.

We deﬁne the semiring (P ∗(V n), ⊕, ⊗, 0, 1) where 0 =
{(0V , . . . , 0V )}, 1 = {(1V , . . . , 1V )}, and for all X, Y ∈
P ∗(V n): X ⊕ Y = M (X ∪ Y )

X ⊗ Y = M ({x ⊗V y : x ∈ X, y ∈ Y })

with x ⊗V y = (x1 ⊗V y1, . . . , xn ⊗V yn). Hence, the alge-
braic generalization of Bellman equations (1) writes:

V π∗
0 = 1
V π∗

t = LN

t = 1 . . . H
M : P ∗(V n) → P ∗(V n) is deﬁned by f i

M (V π∗
t−1)

i=1 f i

where f i
M (f i(X)) for all X ∈ P ∗(V n).

M (X) =

4 Generalized path algebra for AMDPs
Following the construction proposed in Section 2, a decision
rule graph can be associated to an AMDP. Clearly, solving al-
gebraic Bellman equations amounts to searching for optimal
paths with respect to the canonical order associated to ⊕. We
now show how to solve this problem.

Consider the set F of functions from P ∗(V n) to P ∗(V n)

satisfying for all f ∈ F, X ∈ P ∗(V n), Y ∈ P ∗(V n):

f (X ⊕ Y ) = f (X) ⊕ f (Y )
f (0)

= 0

The ⊕ operation on P ∗(V n) induces a ⊕ operation on F de-
ﬁned, for all h, g ∈ F and X ∈ P ∗(V n), by:

(h ⊕ g)(X) = h(X) ⊕ g(X)

i pi ⊠ (0V ⊗V

Proof. For any δi, we have f i

0V ) = 0V ⊗V (P⊞

Proposition 3 (C1) and (C2) imply (cid:0)∀δi ∈ ∆, f i
M (0) = 0 sinceP⊞
P⊞

Let ◦ denote the usual composition operation between func-
tions, id the identity function, and (for simplicity) 0 the con-
stant function everywhere 0. It has been shown by [Minoux,
1977] that the algebraic structure (F , ⊕, ◦, 0, id) is a semi-
M ’s belong to F.
ring. We now prove that update functions f i
M ∈ F(cid:1).
i pi ⊠ 0V =
i pi ⊠ 0V ) = 0V by
(C4) and absorption. Now, we show that f i
M (X ⊕ Y ) =
M (Y ), ∀X, Y ∈ P ∗(V n). Consider X, Y in
f i
M (X) ⊕ f i
P ∗(V n). First, we have f i(M (X ∪ Y )) ⊆ f i(X ∪ Y ) (∗).
Second, we prove that M (f i(X ∪ Y )) ⊆ f i(M (X ∪ Y ))
(∗∗). Let z ∈ M (f i(X ∪ Y )). Then it exists w ∈ X ∪ Y
such that f i(w) = z.
If w ∈ M (X ∪ Y ) then
z ∈ f i(M (X ∪ Y )).
If w 6∈ M (X ∪ Y ) then it exists
w∗ ∈ M (X ∪ Y ) such that w∗ ≻V w. As f i is non-
decreasing, we have f i(w∗) (cid:23)V f i(w). By assumption,
f i(w) ∈ M (f i(X ∪ Y )), therefore f i(w∗) = f i(w). Finally,
z ∈ f i(M (X ∪ Y )) since z = f i(w∗). By (∗) and (∗∗),
we have M (f i(X ∪ Y )) ⊆ f i(M (X ∪ Y )) ⊆ f i(X ∪ Y ).
Therefore M (f i(X ∪ Y )) = M (f i(M (X ∪ Y ))), which
means, by deﬁnition, that f i

M (X ⊕Y ) = f i

M (X)⊕f i

M (Y ).

Propositions 2 and 3 prove that the algebraic generaliza-
tion of Jacobi algorithm solves algebraic Bellman Equations
(3) and (4) [Minoux, 1977]. Thanks to the particular struc-
ture3 of the decision rule graph, the Jacobi algorithm takes
the following simple form:
ALGEBRAIC JACOBI ALGORITHM

1.
2.
3.
4.
5.
6.
7.

V0 ← 1; t ← 0
Qi
repeat

t = 0, ∀t = 1 . . . H, i = 1 . . . N

t ← t + 1
for i = 1 to N do Qi

t ← f i

M (Vt−1)

Vt ← LN

until t = H

i=1 Qi

t

We recognize a standard optimal path algorithm on the de-
cision rule graph valued with functions f i
M . When the it-
eration has ﬁnished, Vt is the set of generalized expected

3The graph is layered; update functions labelling arcs issued
from a same node are identical and invariant from a layer to another.

accumulated rewards at step t associated to optimal paths.
The above algorithm is not efﬁcient since lines 5 and 6
require to consider N = mn decision rules in the com-
the complex-
putation of the Bellman update. Actually,
In-
ity of the algorithm can be signiﬁcantly improved.
j }, where gk
deed, remark that ∪N
j (x) =
i=1...n T (sj, ak, si) ⊠ xi, and that ∀v ∈ V n,
n(v)}. Since the
k=1{gk
∪N
maxima over a Cartesian product equals the Cartesian prod-
M (Vt−1) =

R(sj, ak) ⊗V P⊞
k=1{gk
uct of maxima over components, we haveLN
Lv∈Vt−1 (cid:0)Lm

n(v)(cid:1). Hence, lines 5

1 (v) × .. ×Lm

and 6 can be replaced by:

i=1{f i(v)} = ∪m

1 (v)} × .. × ∪m

j } = ∪m

k=1{gk

i=1{f i

k=1 gk

k=1 gk

i=1 f i

Vt ← ∅
for v ∈ Vt−1 do

for j = 1 to n do

for k = 1 to m do qk

tj ← gk

j (v)

k=1 qk

tj

Vtj ← Lm

endfor
Vt ← Vt ⊕ (Vt1 × .. × Vtn)

5.1.
5.2.
5.3.
5.4.
6.1.
6.2.
6.3.
6.4.

endfor

This algebraic counterpart of backward induction runs in
polynomial time when the reward scale is completely ordered
and algebraic operators can be computed in O(1).

5 Examples
To illustrate the potential of the algebraic approach for MDPs,
we now consider decision models that have not yet been in-
vestigated in a dynamic setting.
• Qualitative MDPs. In decision under possibilistic uncer-
tainty, [Giang and Shenoy, 2001] have recently studied a new
qualitative decision model (binary possibilistic utility), allow-
ing to handle weak information about uncertainty while im-
proving the discrimination power of qualitative utility mod-
els introduced by [Dubois and Prade, 1995]. The latter have
been investigated in sequential decision problems by [Sab-
badin, 1999]. We show here that the former can be exploited
in sequential decision problems as well.

Possibilistic uncertainty is measured on a ﬁnite qualita-
tive totally ordered set P , endowed with two operators ∨
and ∧ (max and min respectively). We denote 0P (resp.
1P ) the least (resp. greatest) element in P . The structure
(P, ∨, ∧, 0P , 1P ) is a semiring.

In Giang and Shenoy’s model, rewards are valued in an
ordered scale (UP , (cid:23)) where UP = {hλ, µi : λ ∈ P, µ ∈ P,
λ ∨ µ = 1P } and hλ, µi (cid:23) hλ′, µ′i ⇔ (λ ≥ λ′ and µ ≤ µ′).
The relevant semiring is here (V, ⊕V , ⊗V , 0V , 1V ) where:

V = {hλ, µi : λ ∈ P, µ ∈ P }
hα, βi ⊕V hλ, µi = hα ∨ λ, β ∧ µi
hα, βi ⊗V hλ, µi = hα ∧ λ, β ∨ µi
0V = h0P , 1P i, 1V = h1P , 0P i

Note that ⊕V and ⊗V are operators max and min on UP .

The binary possibilistic utility model

is a generalized
expectation with operators ⊞ and ⊠ taken as componentwise
∨ and ∧ respectively. Note that this criterion takes values in
UP . Thanks to distributivity of ∨ (resp. ∧) over ∧ (resp. ∨),
conditions (C1) to (C5) hold, which proves that algebraic

backward induction yields the value of optimal policies.
In planning problems, different
• Multicriteria MDPs.
aspects (time, energy, distance...) enter into the assessment
of the utility of an action, and often these aspects cannot
be reduced to a single scalar reward.
This shows the
practical interest of investigating MDPs with multicriteria
additive rewards and multicriteria comparison models. As
an example, consider the following multicriteria decision
model. Let Q denote the set of criteria and (cid:23)Q a (possibly
partial) order relation over Q (reﬂecting the importance of
criteria). Following [Grosof, 1991] and [Junker, 2002], we
denote ≻G the following strict order relation between vectors:

x ≻G y ⇔ (cid:26)∃i = 1 . . . |Q|, xi 6= yi

∀i : xi 6= yi,(cid:0)(xi > yi) or (∃j ≻Q i, xj > yj)(cid:1)

The interest of such a deﬁnition is to unify in a single model
both the lexicographic order (when (cid:23)Q is a linear order) and
the Pareto dominance order (when (cid:23)Q= ∅).

In this case, the relevant semiring structure on rewards is

deﬁned by (neutral elements are omitted here):

|Q|

)

V = P ∗(R
X ⊕V Y = M (X ∪ Y, ≻G)
X ⊗V Y = M ({xi + yi : x ∈ X, y ∈ Y }, ≻G)

Hence, such a reward system can easily be inserted in the
classical probabilistic setting. When the generalized expecta-
tion is chosen as the componentwise (usual) expectation, con-
ditions (C1) to (C5) hold, which proves that algebraic back-
ward induction yields the value of optimal policies.

6 Conclusion
We have introduced a general approach for deﬁning solvable
MDPs in various contexts. The interest of this approach is to
factorize many different positive results concerning various
rewards systems, uncertainty and decision models. Once the
structure on rewards, the representation of uncertainty and the
decision criteria have been chosen, it is sufﬁcient to check that
we have two semirings on V and P and that conditions (C1)
through (C5) are fulﬁlled to justify the use of an algorithm
“à la Jacobi” to solve the problem. It is likely that this result
generalizes to the inﬁnite horizon case, provided a suitable
topology is deﬁned on the policy valuation space.

Remark that, despite its generality, our framework does
not include all interesting decision theories under uncertainty.
For instance, in a probabilistic setting, the RDEU model (as
well as other Choquet integrals) cannot be expressed under
the form of the generalized expectation used in the paper. Ac-
tually, RDEU is known as a dynamically inconsistent model
[Machina, 1989; Sarin and Wakker, 1998] and it is unlikely
that constructive algorithms like backward induction are ap-
propriate. A further speciﬁc study for this class of decision
models might be of interest.

References
[Bistarelli et al., 1995] S. Bistarelli, U. Montanari, and F. Rossi.

Constraint solving over semirings. In Proc. of IJCAI, 1995.

[Bonet and Pearl, 2002] B. Bonet and J. Pearl. Qualitative MDPs
and POMDPs: An order-of-magnitude approximation. In Proc.
of the 18th UAI, pages 61–68, 2002.

[Chu and Halpern, 2003a] F.C. Chu and J.Y. Halpern. Great expec-
tations. part I: On the customizability of generalized expected
utility. In Proc. of the 18th IJCAI, pages 291–296, 2003.

[Chu and Halpern, 2003b] F.C. Chu and J.Y. Halpern. Great expec-
tations. part II: Generalized expected utility as a universal deci-
sion rule. In Proc. of the 18th IJCAI, pages 297–302, 2003.

[Darwiche and Ginsberg, 1992] A. Darwiche and M.L. Ginsberg.
A symbolic generalization of probability theory. In Proc. of the
10th AAAI, pages 622–627, 1992.

[Dean et al., 1993] T. Dean, L.P. Kaelbling,

J. Kirman, and
A. Nicholson. Planning with deadlines in stochastic domains.
In Proc. of the 11th AAAI, pages 574–579, 1993.

[Dubois and Prade, 1995] D. Dubois and H. Prade. Possibility the-
ory as a basis of qualitative decision theory. In Proc. of the 14th
IJCAI, pages 1925–1930, 1995.

[Friedman and Halpern, 1995] N. Friedman and J. Halpern. Plausi-
bility measures: A user’s guide. In Proc. of the 11th UAI, pages
175–184, 1995.

[Giang and Shenoy, 2001] P.H. Giang and P.P. Shenoy. A compari-
son of axiomatic approaches to qualitative decision making using
possibility theory. In Proc. of the 17th UAI, pages 162–170, 2001.
[Grosof, 1991] B. Grosof. Generalizing prioritization. In Proc. of

the 2nd KR, pages 289–300, 1991.

[Junker, 2002] U. Junker.

Preference-based search and multi-

criteria optimization. In Proc. of AAAI, pages 34–40, 2002.

[Kaebling et al., 1999] L.P. Kaebling, M. Littman, and A. Cassan-
dra. Planning and acting in partially observable stochastic do-
mains. Artiﬁcial Intelligence, 101:99–134, 1999.

[Littman and Szepesvári, 1996] M.L. Littman and C. Szepesvári. A
generalized reinforcement-learning model: Convergence and ap-
plications. In Proc. of the 13th ICML, pages 310–318, 1996.

[Luce, 2003] R.D. Luce.

Increasing increment generalizations of
rank-dependent theories. Theory and Decision, 55:87–146, 2003.
[Machina, 1989] M.J. Machina. Dynamic consistency and non-
expected utility models of choice under uncertainty. Journal of
Economic Literature, 27:1622–1668, 1989.

[Minoux, 1977] M. Minoux. Generalized path algebra.

In Sur-
veys of Mathematical Programming, pages 359–364. Publishing
House of the Hungarian Academy of Sciences, 1977.

[von Neumann and Morgenstern, 1947] J.

and
O. Morgenstern. Theory of games and economic behavior. 2nd
Ed. Princeton University Press, 1947.

von Neumann

[Puterman, 1994] M.L. Puterman. Markov Decision Processes -
Discrete Stochastic Dynamic Programming. Wiley & Sons, 1994.
[Quiggin, 1993] J. Quiggin. Generalized expected utility theory:
the rank-dependant model. Kluwer Academic Publishers, 1993.
[Sabbadin, 1999] R. Sabbadin. A possibilistic model for qualita-
tive sequential decision problems under uncertainty in partially
observable environments. In Proc. of UAI, pages 567–574, 1999.
[Sarin and Wakker, 1998] R. Sarin and P. Wakker. Dynamic choice

and non-EU. J. of Risk and Uncertainty, 17:87–119, 1998.

[Savage, 1954] L.J. Savage. The Foundations of Statistics. J. Wiley

& Sons, 1954.

[Wakuta, 1995] K. Wakuta.

processes and the systems of linear inequalities.
Processes and their Applications, 56:159–169, 1995.

Vector-valued markov decision
Stochastic

[Wilson, 1995] N. Wilson. An order of magnitude calculus.

Proc. of the 11th UAI, pages 548–555, 1995.

In


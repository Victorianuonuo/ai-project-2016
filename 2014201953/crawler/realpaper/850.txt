Sequence Labelling in Structured Domains

with Hierarchical Recurrent Neural Networks

Santiago Fern´andez1 and Alex Graves1 and J¨urgen Schmidhuber1,2

1 IDSIA, Galleria 2, 6928 Manno-Lugano, Switzerland

2 TU Munich, Boltzmannstr. 3, 85748 Garching, Munich, Germany

{santiago,alex,juergen}@idsia.ch

Abstract

Modelling data in structured domains requires es-
tablishing the relations among patterns at multiple
scales. When these patterns arise from sequential
data, the multiscale structure also contains a dy-
namic component that must be modelled, partic-
ularly, as is often the case, if the data is unseg-
mented. Probabilistic graphical models are the pre-
dominant framework for labelling unsegmented se-
quential data in structured domains. Their use re-
quires a certain degree of a priori knowledge about
the relations among patterns and about the patterns
themselves. This paper presents a hierarchical sys-
tem, based on the connectionist temporal classi-
ﬁcation algorithm, for labelling unsegmented se-
quential data at multiple scales with recurrent neu-
ral networks only. Experiments on the recognition
of sequences of spoken digits show that the system
outperforms hidden Markov models, while making
fewer assumptions about the domain.

Introduction

1
Assigning a sequence of labels to an unsegmented stream of
data is the goal of a number of practical tasks such as speech
recognition and handwriting recognition. In these domains,
the structure at multiple scales is often captured with hierar-
chical models to assist with the process of sequence labelling.
Probabilistic graphical models, such as hidden Markov
models [Rabiner, 1989, HMMs] and conditional random
ﬁelds [Lafferty et al., 2001, CRFs], are the predominant
framework for sequence labelling. Recently, a novel algo-
rithm called connectionist temporal classiﬁcation [Graves et
al., 2006, CTC] has been developed to label unsegmented
sequential data with recurrent neural networks (RNNs) only.
Like CRFs, and in contrast with HMMs, CTC is a discrimi-
nant algorithm. In contrast with both CRFs and HMMs, CTC
is a general algorithm for sequence labelling, in the sense that
CTC does not require explicit assumptions about the statisti-
cal properties of the data or explicit models of the patterns
and the relations among them.

Hierarchical architectures are often used with HMMs.
There exist efﬁcient algorithms for estimating the parameters
in such hierarchies in a global way, i.e. a way that maximises

the performance at the top of the hierarchy. Nonetheless,
HMMs do not efﬁciently represent information at different
scales, indeed they do not attempt to abstract information in
hierarchical form [Nevill-Manning and Witten, 1997].

The only paper known to us that describes a hierar-
chy of CRFs has been presented recently by Kumar and
Hebert [2005] for classifying objects in images. The hier-
archy is not trained globally, but sequentially: i.e. estimates
of the parameters in the ﬁrst layer are found and, then, with
these parameters ﬁxed, those in the second layer (and transi-
tion matrices) are estimated.

This paper uses a hierarchical approach to extend the appli-
cability of CTC to sequence labelling in structured domains.
Hierarchical CTC (HCTC) consists of successive levels of
CTC networks. Every level predicts a sequence of labels and
feeds it forward to the next level. Labels at the lower lev-
els represent the structure of the data at a lower scale. The
error signal at every level is back-propagated through all the
lower levels, and the network is trained with gradient descent.
The relative weight of the back-propagated error and the pre-
diction error at every level can be adjusted if necessary, e.g.
depending on the degree of uncertainty about the target label
sequence at that level, which can depend on the variability
in the data. In the extreme case in which only the error at the
top level is used for training, the network can, potentially, dis-
cover structure in the data at intermediate levels that results
in accurate ﬁnal predictions.

The next section brieﬂy introduces the CTC algorithm.
Section 3 describes the architecture of HCTC. Section 4 com-
pares the performance of hierarchical HMMs and HCTC on a
speech recognition task. Section 5 offers a discussion on sev-
eral aspects of the algorithm and guidelines for future work.
Final conclusions are given in section 6.

2 Connectionist Temporal Classiﬁcation
CTC is an algorithm for labelling unsegmented sequential
data with RNNs only [Graves et al., 2006]. The basic idea
behind CTC is to interpret the network outputs as a prob-
ability distribution over all possible label sequences, condi-
tioned on the input data sequence. Given this distribution,
an objective function can be derived that directly maximises
the probabilities of the correct labellings. Since the objec-
tive function is differentiable, the network can then be trained
with standard backpropagation through time [Werbos, 1990].

IJCAI-07

774

The algorithm requires that the network outputs at different
times are conditionally independent given the internal state
of the network. This requirement is met as long as there are
no feedback connections from the output layer to itself or the
network.

A CTC network has a softmax output layer [Bridle, 1990]
with one more unit than the number of labels required for the
task. The activation of the extra unit is interpreted as the prob-
ability of observing a “blank”, or no label at a given time step.
The activations of the other units are interpreted as the prob-
abilities of observing the corresponding labels. The blank
unit allows the same label to appear more than once consec-
utively in the output label sequence. A trained CTC network
produces, typically, a series of spikes separated by periods of
blanks. The location of the spikes usually corresponds to the
position of the patterns in the input data; however, the algo-
rithm is not guaranteed to ﬁnd a precise alignment.
2.1 Classiﬁcation
For an input sequence x of length T we require a label se-
quence l ∈ L
≤T is the set of sequences of length
≤ T on the alphabet L of labels. We begin by choosing a
label (or blank) at every timestep according to the probabil-
ity given by the network outputs. This deﬁnes a probability
(cid:3)T of length T sequences on the
distribution over the set L
(cid:3) = L ∪ {blank} of labels with blank
extended alphabet L
(cid:3)T from label
included. To disambiguate the elements of L
sequences, we refer to them as paths.
(cid:3)T is

The conditional probability of a particular path π ∈ L

≤T , where L

given by:

T(cid:2)

t=1

2.2 Training
The objective function for CTC is derived from the principle
of maximum likelihood. That is, it attempts to maximise the
log probability of correctly labelling the entire training set.
Let S be such a training set, consisting of pairs of input and
target sequences (x, z), where the length of sequence z is less
than or equal to the length of the input sequence x. We can
express the objective function to be minimised as:

OM L(S) = −

.

(5)

(cid:3)

ln

(x,z)∈S

(cid:4)

(cid:5)
p(z|x)

The network can be trained with gradient descent by dif-
ferentiating equation (5) with respect to the network outputs.
This can be achieved by calculating the conditional probabil-
ities p(z|x) with a dynamic programming algorithm similar
to the forward-backward recursions used for HMMs [Graves
et al., 2006].

We deﬁne αt(s) as the probability of having passed

through sequence l1...s and being at symbol s at time t:
αt(s) = P (π1...t : B(π1...t) = l1...s, πt = s|x)

(cid:2)
yt
πt(cid:2) ,

(6)

and βt(s) as the probability of passing through the rest of the
label sequence (ls...|l|) given that symbol s has been reached
at time t:

βt(s) = P (πt+1...T : B(πt...T ) = ls...|l||πt = s, x)

(cid:3)

t(cid:2)

=

B(π1...t)=l1...s

π:

(cid:2)=1
t

(cid:3)

T(cid:2)

π:

B(πt:T )=ls:|l|

(cid:2)=t+1
t

p(π|x) =

πt, ∀π ∈ L
(cid:3)T
yt

(1)

=

(cid:2)
yt
πt(cid:2) .

(7)

where yt

k is the activation of output unit k at time t.

Paths can be mapped into label sequences by ﬁrst remov-
ing the repeated labels, and then removing the blanks. For ex-
ample, the path (a,-,a,b,-), and the path (-,a,a,-,-,a,b,b) would
both map onto the labelling (a,a,b). The conditional probabil-
ity of a given labelling l ∈ L
≤T is the sum of the probabilities
(cid:3)
of all the paths corresponding to it:
p(l|x) =

p(π|x)

(2)

π∈B−1(l)

≤T is the many-to-one map implied by

where B : L
the above process.

(cid:3)T (cid:6)→ L

Finally, the output of the classiﬁer h(x) is the most proba-

ble labelling for the input sequence:
h(x) = arg max

p(l|x).

l

(3)

In general, ﬁnding this maximum is intractable, but there are
several effective approximate methods [Graves et al., 2006].
The one used in this paper assumes that the most probable
path will correspond to the most probable labelling:

h(x) ≈ B(π

∗

)

∗

where π

= arg max

π

p(π|x)

(4)

is just the concatenation of the most active outputs at

∗

and π
every time-step.

|l(cid:2)|(cid:3)

p(l|x) =

To allow for blanks in the output paths, for every label se-
quence l ∈ L
(cid:3)
≤T we consider a modiﬁed label sequence l
,
with blanks added to the beginning and the end and inserted
between every pair of labels. The length of l
is therefore
(cid:3)| = 2|l| + 1.
|l
In calculating αt(s) and βt(s), we allow
all transitions between blank and non-blank labels, and also
those between any pair of distinct non-blank labels. The se-
quences can start with either blank or the ﬁrst symbol in l,
and can end with either blank or the last symbol in l.

(cid:3)

From equations (1), (2), (6) and (7), we ﬁnd that for any t:

αt(s)βt(s).

(8)

s=1

Noting that the same label (or blank) may be repeated several
times in a single labelling l, we deﬁne the set of positions
s = k}, which
where label k occurs as lab(l, k) = {s : l
(cid:3)
may be empty, and differentiate equation (8) with respect to
the network outputs:
∂p(l|x)
∂yt
k

αt(s)βt(s).

(cid:3)

1
yt
k

(9)

=

s∈lab(l,k)

Setting l = z and differentiating the objective function,
we obtain the error signal received by the network during

IJCAI-07

775

seven

seven

level I

level 2

level 1

level I

level 2

level 1

se ven

s e v e n

se ven

s e v e n

unsegmented speech data

unsegmented speech data

Figure 1: Schematic representation of an HCTC network la-
belling an unsegmented input data stream. Outputs at each
level correspond to the probability of having detected a par-
ticular label (or emitting the blank label) at a particular time.

Figure 2: Schematic representation of the error signal ﬂow on
an HCTC network. The network is trained globally with gra-
dient descent. External error signals injected at intermediate
levels are considered optional. If applied, their contribution
to the total error signal can be adjusted.

Zt =

αt(s)βt(s)

(11)

Δtarget

i

:= ∂OM L

i

is a normalization factor.

s=1

training:

∂OM L({(x, z)},Nw)

∂ut
k

= yt
k

− 1
Zt

(cid:3)

αt(s)βt(s)

s∈lab(z,k)

where ut
puts of the softmax layer, respectively, and

k and yt

(10)
k are the unnormalised and normalised out-

|l(cid:2)|(cid:3)

3 Hierarchical Connectionist Temporal

Classiﬁcation

A hierarchy of CTC networks is a series g = (g1, g2, . . . , gI)
of CTC networks, with network g1 receiving as input the ex-
ternal signal and all other networks gi receiving as input the
output of network gi−1. This architecture is illustrated in Fig-
ure 1.

Note that every network gi in the hierarchy has a softmax
output layer. This forces the hierarchical system to make
decisions at every level and use them in the upper levels to
achieve a higher degree of abstraction. The analysis of the
structure of the data is facilitated by having output probabili-
ties at every level.

Because of the modular design, the ﬂexibility of the system
allows using other architectures (such as feeding the external
inputs to several levels in the hierarchy) as long as the mathe-
matical requirements of the CTC algorithm are met (see sec-
tion 2).

3.1 Training
In general, HCTC requires target sequences for every level in
the hierarchy, given a particular input sequence in the train-
ing set. Each example in the training set S consists of a

pair (x, Z), where x = (x1, x2, . . . , xT ) is the external in-
put sequence, and Z = (z1, z2, . . . , zI) is the set of target
sequences, where |zi| ≤ T ∀i.
The system is trained globally with gradient descent. Fig-
ure 2 illustrates the error signal ﬂow for an HCTC network.
The error signal due to the target sequence zi, received by
network gi, is given by equation (10). To simplify the nota-
tion, we deﬁne:

({(vi, zi)})
∂ut

i,k

(12)

where the input sequence vi is the external input sequence
for network g1 and the output of network gi−1 for all levels
i (cid:9)= 1.

The error signal Δtarget

is back-propagated into the net-
work gi, and then into all lower levels j : j < i. The con-
tribution of this term to the error signal at the unnormalised
i,k of the kth output unit of network gi : i (cid:9)= I is:
activation ut

i

(cid:7)

(cid:6)
wmk −

(cid:3)

(cid:3)

δm

m∈M

wmk

(cid:2) yt
k

(cid:2)

(13)

(cid:2)∈K

k

Δbackprop

i

= yt
k

where M is the set of units in level i + 1 which are connected
to the set of softmax output units, K, in level i; δm is the
error signal back-propagated from unit m ∈ M; wmk is the
weight associated with the connection between units m ∈ M
and k ∈ K; and yt
k are the output activations of the softmax
layer at level i.

Finally, the total error signal Δi received by network gi is
the sum of the contributions in equations (12) and (13). In
general, the two contributions can be weighted depending on
the problem at hand. This is important if, for example, the
target sequences at some intermediate levels are uncertain or
not known at all.

(cid:8)

Δi =

i

Δtarget
λiΔtarget

i

+ Δbackprop

i

if i = I,
otherwise,

(14)

IJCAI-07

776

Phonemes
Z - II - R - OW
W - AX - N
T - OO
TH - R - II
F - OW - R
F - AY - V
S - I - K - S

Digit
ZERO
ONE
TWO
THREE
FOUR
FIVE
SIX
SEVEN S - EH - V - E - N
EIGHT
NINE
OH

EY - T
N - AY - N
OW

Table 1: Phonetic labels used to model the digits in the exper-
iments.
with 0 ≤ λi ≤ 1. In the extreme case where λi = 0, no
target sequence is provided for the network gi, which is free
to make any decision that minimises the error Δbackprop
re-
ceived from levels higher up in the hierarchy. Because train-
ing is done globally, the network can, potentially, discover
structure in the data at intermediate levels that results in ac-
curate predictions at higher levels of the hierarchy.

i

4 Experiments
In order to validate the HCTC algorithm we chose a speech
recognition task because this is a problem known to contain
structure at multiple scales. HMMs remain state-of-the-art
in speech recognition, and therefore HCTC performance is
compared with that of HMMs.

The task was to ﬁnd the sequence of digits spoken in an
standard set of utterances using, at an intermediate level, the
sequence of phonemes associated to every word.

4.1 Materials
The speaker-independent connected-digit database, TIDIG-
ITS [Leonard and Doddington, 1993], consists of more than
25 thousand digit sequences spoken by over 300 men, women
and children. The database was recorded in the U.S. and it is
dialectically balanced. The utterances were distributed into
a test and a training set. We randomly selected ﬁve percent
of the training set to use as a validation set. This left 11922
utterances in the training set, 627 in the validation, and 12547
in the test set.

The following eleven digits are present in the database:
“zero”, “one”, “two”, “three”, . . . , “nine” and “oh”. Utter-
ances consist of a sequence of one to seven digits. The repre-
sentation of the digits at the phonetic level used in the exper-
iments can be seen in Table 1. Nineteen phonetic categories
were used, nine of which are common to two or more digits.
Samples were digitized at 20 kHz with a quantization
range of 16 bits. The acoustic signal was transformed into
mel frequency cepstral coefﬁcients (MFCC) with the HTK
Toolkit [Young et al., 2005]. Spectral analysis was carried
out with a 40 channel Mel ﬁlter bank from 130 Hz to 6.8 kHz.
A pre-emphasis coefﬁcient of 0.97 was used to correct spec-
tral tilt. Twelve MFCC plus the 0th order coefﬁcient were
computed on Hamming windows 25.6 ms long, every 10 ms.

Delta and Acceleration coefﬁcients were added giving a vec-
tor of 39 coefﬁcients in total. For the network, the coefﬁcients
were normalised to have mean zero and standard deviation
one over the training set.

4.2 Setup
implemented with the HTK
The HMM system was
Toolkit [Young et al., 2005]. Three-states left-to-right models
were used for each one of the nineteen phonetic categories.
A silence model and a “short pause” model (allowed at the
end of a digit) were also estimated. Observation probabili-
ties were modelled by a mixture of Gaussians. The grammar
model allowed any sequence, preceded and followed by si-
lence, of one or more digits. Neither linguistic information
nor probabilities of partial phone sequences were included in
the system.

The number of Gaussians and the insertion penalty that op-
timised performance on the validation set was selected. Us-
ing the training set, the number of Gaussians was increased in
steps of two until performance on the validation set stabilised
or, as in our case, decreased slightly (96 Gaussians). Every
time the number of Gaussians was increased, the parameter
estimation algorithm (HERest) was applied twice and results
were collected on the validation set with insertion penalties
varying from 0 to -100 in steps of -5. The best performance
on the validation set was obtained with 80 Gaussians and an
insertion penalty of -85. The total number of parameters for
the HMM is 384,126.

A 2-level HCTC was used. The top, second level, had
as target the sequence of digits corresponding to the input
stream. The bottom, ﬁrst level, used as target the sequence
of phonemes corresponding to the target sequence of digits
for the top level. Each CTC network uses the bi-directional
LSTM recurrent neural network [Graves and Schmidhuber,
2005; Graves et al., 2006], primarily because on a phoneme
recognition task better results than for other RNNs have been
reported [Graves et al., 2005]. Also, the CTC formalism
is best realised with a bi-directional recurrent neural net-
work [Schuster and Paliwal, 1997] because the network out-
put at a particular time depends on both past and future events
in the input sequence.

Within each level, the input layer was fully connected to
the hidden layer and the hidden layer was fully connected to
itself and the output layer. In the ﬁrst level, the input layer
was size 39, the forward and backward layers had 128 blocks
each, and the output layer was size 20 (19 phonetic categories
plus blank). In the second level, the input layer was size 20
also, the forward and backward layers had 50 blocks each and
the output layer was size 12 (eleven digits plus the blank la-
bel). The input and output cell activation functions were a
hyperbolic tangent. The gates used a logistic sigmoid func-
tion in the range [0, 1]. The total number of weights in the
HCTC network is 207,852.

Training of the HCTC network was done by gradient de-
scent with weight updates after every training example. In
all cases, the learning rate was 10−4, momentum was 0.9,
weights were initialized randomly in the range [−0.1, 0.1]
and, during training, Gaussian noise with a standard devia-
tion of 1.0 was added to the inputs to improve generalisation.

IJCAI-07

777

System
HMM
HCTC (λ1 = 1)
HCTC (λ1 = 0)

LER
0.89 %
0.61 ± 0.04 %
0.51 %

Table 2: Label Error Rate (LER) on TIDIGITS. Results for
HCTC (λ1 = 1) are means over 5 runs with different random
weights, ± standard error. This gives a 95 % conﬁdence in-
terval of (0.50; 0.72), with a t-value of 2.8 for 4 degrees of
freedom and a two-tailed test. For HCTC (λ1 = 0) the best
result obtained is shown; this coincides with the best result
obtained with λ1 = 1.

Performance was measured as the normalised edit distance
(label error rate; LER) between the target label sequence and
the output label sequence given by the system.

4.3 Results
Performance rates for the systems tested can be seen in Ta-
ble 2. The best HMM-based system achieved an error rate of
0.89% in continuous digit recognition. The label error rate
for HCTC was on average 0.61 % (95 % conﬁdence interval
of (0.50; 0.72)). This is an improvement of more than 30 %
with respect to the HMM and with approximately half the
number of parameters.

The best results without injecting the error signal associ-
ated to the phoneme labels (λ1 = 0) was 0.51 %, which is
as good as the best performance achieved with λ1 = 1. In
this case, however, the pattern of activations of seven output
units (instead of twenty) was enough to encode the dynamic
structure of the data at the lower level in order to make ac-
curate predictions at the top level of the sequence of digits
in the utterance (see Figure 3). Some of these seven output
units become active/inactive at speciﬁc points in time for each
digit pattern. And some of them are active for different digits.
Nonetheless, they cannot be associated directly to phonetic
categories and might encode another type of information in
the signal.

5 Discussion and Future Work
Inserting levels in the hierarchy without a corresponding tar-
get label sequence is interesting if the target labels are not
known, or, for instance, if the phonetic labels used are sus-
pected to be invalid due to, e.g., the presence of dialectal vari-
ations in the dataset. If the sources of variability in the data
cannot be speciﬁed accurately, a system with less constraints
might be capable of making more effective decisions.

Experimentally, this means that the system will be more
difﬁcult to train for a particular goal. HCTC with and without
target label sequences at the phonetic level achieved similar
performance, albeit by different means. Nevertheless, HCTC
with λ1 = 0 suffered to a larger extent from the problem of
local minima.

Assuming that reasonable target label sequences can be
speciﬁed a priori for intermediate levels, a possible solution
is to train HCTC until the error has decreased signiﬁcantly,
and then remove the contribution to the error signal of the
target label sequences for intermediate levels. This will free

a partially trained network to explore alternatives that max-
imise performance at the top level of the hierarchy.

In the future, we would also like to explore ways of im-
proving system scalability. For example, large vocabulary
speech recognition requires systems that work with many
thousands of words. Using output layers of that size for
HCTC is not currently practical. Instead of assigning a unique
output unit to every possible label, other methods can be ex-
plored such as assigning labels to speciﬁc activation patterns
in a group of output units. This will require modifying CTC’s
training algorithm.

Another aspect we would like to investigate is the poten-
tial of HCTC for word spotting tasks. As a discriminant
method, HCTC may improve detection rates due to its capa-
bility of discriminating between keywords and non-speech or
other speech events. Besides this, HCTC provides estimates
of a posteriori probabilities that can help to directly assess
the level of conﬁdence in the predictions. This is in contrast
to generative approaches, such as HMMs, which use unnor-
malized likelihoods.

6 Conclusion
This paper has presented the HCTC algorithm, which uses a
hierarchy of recurrent neural networks to label sequences of
unsegmented data in structured domains. HCTC is capable
of ﬁnding structure at multiple levels and using it to achieve
accurate predictions further up in the hierarchy. The use of
neural networks offers a ﬂexible way of modelling the domain
while at the same time allowing the system to be trained glob-
ally. Experimental results demonstrated that this approach
outperformed hidden Markov models on a speech recognition
task.

Acknowledgments
This research was funded by SNF grant 200021-111968/1.

References
[Bridle, 1990] John S. Bridle. Neurocomputing: Algorithms,
architectures and applications, chapter Probabilistic in-
terpretation of feedforward classiﬁcation network outputs,
pages 227–236. Springer-Verlag, 1990.

[Graves and Schmidhuber, 2005] Alex Graves and J¨urgen
Schmidhuber.
Framewise phoneme classiﬁcation with
bidirectional LSTM and other neural network architec-
tures. Neural Networks, 18(5–6):602–610, June/July 2005.
[Graves et al., 2005] Alex Graves, Santiago Fern´andez, and
J¨urgen Schmidhuber. Bidirectional LSTM networks for
improved phoneme classiﬁcation and recognition. In Pro-
ceedings of the 2005 International Conference on Artiﬁ-
cial Neural Networks, Warsaw, Poland, 2005.

[Graves et al., 2006] Alex Graves, Santiago Fern´andez,
Faustino Gomez, and J¨urgen Schmidhuber. Connectionist
temporal classiﬁcation: Labelling unsegmented sequence
data with recurrent neural networks.
In Proceedings of
the 23rd International Conference on Machine Learning,
Pittsburgh, PA, USA, 2006.

IJCAI-07

778

HCTC on TIDIGITS with word and phoneme targets (λ

1=1)

HCTC on TIDIGITS with word targets only (λ

1=0)

ZERO 

 ZERO 

 FIVE 

 THREE THREE

Z II R OW Z II R OW  F AY V  TH R II  TH R II

 1

y
t
i
l
i
b
a
b
o
r
p

 
l
e
b
a
L

 0

 1

y
t
i
l
i
b
a
b
o
r
p

 
l
e
b
a
L

 0

 1

y
t
i
l
i
b
a
b
o
r
p

 
l
e
b
a
L

 0

 1

y
t
i
l
i
b
a
b
o
r
p

 
l
e
b
a
L

 0

ZERO 

 ZERO 

 FIVE 

 THREE THREE

Figure 3: HCTC output on TIDIGITS. The network on the left was trained with target label sequences at the phoneme and word
levels (i.e. λ1 = 1), whereas the network on the right received an error signal only at the word level (λ1 = 0). Activations are
shown for the output layers at the word level (top) and phoneme level (bottom). For λ1 = 0, the activations at the lower level
do not represent phonetic labels but some other kind of information which requires only seven (instead of twenty) output units.
Both networks achieved the same word error rate (0.51 %).

[Kumar and Hebert, 2005] Sanjiv Kumar

and Martial
A hierarchical ﬁeld framework for uniﬁed
Hebert.
In Proceedings of the 10th
context-based classiﬁcation.
IEEE International Conference on Computer Vision,
Beijing, China, 2005.

[Lafferty et al., 2001] John Lafferty, Andrew McCallum,
and Fernando Pereira. Conditional random ﬁelds: Proba-
bilistic models for segmenting and labeling sequence data.
In Proceedings of the 18th International Conference on
Machine Learning, 2001.

Phil Woodland. The HTK Book version 3.3. Cambridge
University Engineering Department, 2005.

[Leonard and Doddington, 1993] R. Gary Leonard and
Linguistic Data

TIDIGITS.

George Doddington.
Consortium, Philadelphia, 1993.

[Nevill-Manning and Witten, 1997] Craig

Nevill-
Manning and Ian H. Witten.
Identifying hierarchical
structure in sequences: A linear-time algorithm. Journal
of Artiﬁcial Intelligence Research, 7:67–82, 1997.

G.

[Rabiner, 1989] Lawrence R. Rabiner. A tutorial on hidden
markov models and selected applications in speech recog-
nition. Proceedings of the IEEE, 77(2):257–286, February
1989.

[Schuster and Paliwal, 1997] M. Schuster and K. K. Paliwal.
IEEE Transac-

Bidirectional recurrent neural networks.
tions on Signal Processing, 45:2673–2681, 1997.

[Werbos, 1990] Paul J. Werbos. Backpropagation through
time: What it does and how to do it. Proceedings of the
IEEE, 78(10):1550–1560, 1990.

[Young et al., 2005] Steve Young, Gunnar Evermann, Mark
Gales, Thomas Hain, Dan Kershaw, Gareth Moore, Julian
Odell, Dave Ollason, Dan Povey, Valtcho Valtchev, and

IJCAI-07

779


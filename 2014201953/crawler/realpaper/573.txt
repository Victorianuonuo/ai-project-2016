Dynamic Probabilistic Relational Models 

Sumit Sanghai  Pedro Domingos  Daniel Weld 
Department of Computer Science and Engineeering 

University of Washington 
Seattle, WA 98195-2350 

{sanghai,pedrod,weld}@cs.washington.edu 

Abstract 

Intelligent agents must function in an uncertain world, 
containing  multiple  objects  and  relations that  change 
over time. Unfortunately, no representation is currently 
available that can handle all these issues, while allowing 
for principled and efficient inference.  This paper ad(cid:173)
dresses this need by introducing dynamic probabilistic 
relational models (DPRMs).  DPRMs are an extension 
of dynamic Bayesian networks (DBNs) where each time 
slice (and its dependences on previous slices) is repre(cid:173)
sented by a probabilistic relational model (PRM). Parti(cid:173)
cle filtering, the standard method for inference in DBNs, 
has severe limitations when applied to DPRMs, but we 
are able to greatly improve its performance through a 
form of relational Rao-Blackwellisation.  Further gains 
in  efficiency arc obtained  through the  use  of abstrac(cid:173)
tion trees, a novel data structure. We successfully apply 
DPRMs to execution monitoring and fault diagnosis of 
an assembly plan, in which a complex product is gradu(cid:173)
ally constructed from subparts. 

Introduction 

1 
Sequential phenomena abound in the world, and uncertainty 
is a common  feature of them.  Currently  the most power(cid:173)
ful representation available for such phenomena is dynamic 
Bayesian networks, or DBNs [Dean and Kanazawa,  1989]. 
DBNs represent the state of the world as a set of variables, and 
model the probabilistic dependencies of the variables within 
and between time steps.  While a major advance over previ(cid:173)
ous approaches, DBNs are still unable to compactly represent 
many real-world domains. In particular, domains can contain 
multiple objects and classes of objects, as well as multiple 
kinds of relations among them; and objects and relations can 
appear and disappear over time.  For example, manufactur(cid:173)
ing plants assemble complex artifacts (e.g., cars, computers, 
aircraft) from large numbers of component parts, using mul(cid:173)
tiple kinds of machines and operations. Capturing such a do(cid:173)
main in a DBN would require exhaustively representing all 
possible objects and relations among them.  This raises two 
problems.  The first one is that the computational cost of us(cid:173)
ing such a DBN would likely be prohibitive.  The second is 
that reducing the rich structure of the domain to a very large, 
"flat" DBN  would render it essentially incomprehensible to 

human beings.  This paper addresses these two problems by 
introducing an extension of DBNs that exposes the domain's 
relational structure, and by developing methods for efficient 
inference in this representation. 

Formalisms that can represent objects and relations, as op(cid:173)
posed to just variables, have a long history in AI. Recently, 
significant progress has been made in combining them with a 
principled treatment of uncertainty.  In particular, probabilis(cid:173)
tic relational models or PRMs [Friedman et al,  1999] are an 
extension of Bayesian networks that allows reasoning with 
classes, objects and relations.  The representation we intro(cid:173)
duce in this paper extends PRMs to sequential problems in 
the same way that DBNs extend Bayesian networks. We thus 
call it dynamic probabilistic relational models, or DPRMs. 
We develop an efficient inference procedure for DPRMs by 
adapting Rao-Blackwcllised particle filtering, a state-of-the-
art inference method for DBNs [Murphy and Russell, 2001]. 
We introduce abstraction trees as a data structure to reduce 
the computational cost of inference in DPRMs. 

Early fault detection in complex manufacturing processes 
can greatly reduce their cost. In this paper we apply DPRMs 
to monitoring the execution of assembly plans, and show that 
our inference methods scale to problems with over a thou(cid:173)
sand objects and thousands of steps.  Other domains where 
we envisage DPRMs being useful include robot control, vi(cid:173)
sion in motion, language processing, computational modeling 
of markets, battlefield management, cell biology, ecosystem 
modeling, and the Web. 

The rest of the paper is structured as follows. The next two 
sections briefly review DBNs and PRMs. We then introduce 
DPRMs and methods for inference in them.  The following 
section reports on our experimental study in assembly plan 
monitoring. The paper concludes with a discussion of related 
and future work. 

2  Dynamic Bayesian Networks 
A Bayesian network encodes the joint probability distribu(cid:173)
tion of a set of variables, 
as a directed acyclic 
graph and a set of conditional probability models. Each node 
corresponds to a variable, and the model associated with it 
allows us to compute the probability of a state of the vari(cid:173)
able  given  the  state  of its  parents.  The  set  of parents  of 

denoted 

is the set of nodes with an arc to 

in the graph.  The structure of the network encodes the as-

992 

PROBABILISTIC  INFERENCE 

and 

this weighted distribution. The particles will thus tend to stay 
clustered in the more probable regions of the state space, ac(cid:173)
cording to the observations. 

such that 

Although particle filtering has scored impressive successes 
in many practical  applications,  it also has some significant 
limitations. One that is of particular concern to us here is that 
it tends to perform poorly in high-dimensional state spaces. 
This  is  because  the  number of particles  required  to  main(cid:173)
tain a good approximation to the state distribution grows very 
rapidly with the dimensionality. This problem can be greatly 
attenuated by analytically marginalizing out some of the vari(cid:173)
ables, a technique known as Rao-Blackwellisation [Murphy 
and Russell, 2001 ]. Suppose the state space can be divided 
into two subspaces 
can be computed analytically and efficiently.  Then we only 
need to sample from the smaller space 
, requiring far fewer 
particles to obtain the same degree of approximation.  Each 
particle is now composed of a sample from 
plus a parametric representation of P\ 
For 
example, if the variables in 
are discrete and independent of 
each  other  given  we can store for each variable the vector 
of parameters of the corresponding multinomial  distribution 
(i.e., the probability of each value). 
3  Probabilistic Relational Models 
A relational schema is a set of classes 
where each class C is associated with a set of propositional 
attributes , a nd a set of relational attributes or refer(cid:173)
ence 
.  The propositional  attribute A of class C 
is denoted C.A, and its domain (assumed  finite)  is denoted 
V(C.A).  The  relational  attribute  R  of C  is  denoted  C.R, 
and its domain is the power set 
In other words,  C.R is a set of objects belonging to some 
For example, the Aircraft schema might be used 
class 
to represent partially or completely assembled aircraft, with 
classes  corresponding  to  different types  of parts  like metal 
sheets, nuts and bolts.  The propositional attributes of a bolt 
might include its color, weight, and dimensions, and its rela(cid:173)
tional attributes might include the nut it is attached to and the 
two metal sheets it is bolting. An instantiation of a schema is 
a set of objects, each object belonging to some class 
with all propositional and relational attributes of each object 
specified. For example, an instantiation of the aircraft schema 
might be a particular airplane, with all parts, their properties 
and their arrangement specified. 

of a target class 

slots 

sertion  that  each  node  is  conditionally  independent  of its 
non-descendants given its parents.  The probability of an ar(cid:173)
bitrary event 
can then be computed as 

Dynamic Bayesian Networks (DBNs) are an extension of 
Bayesian networks for modeling dynamic systems. In a DBN, 
the state at time t is represented by a set of random variables 
The state at time t is dependent on 
the states at previous time steps.  Typically, we assume that 
each state only depends on the immediately preceding state 
(i.e., the system is first-order Markovian), and thus we need 
to represent the transition distribution . 
. This can 
be done using a two-time-slice Bayesian network fragment 
(2TBN) . 
whose 
and variables from 
parents are variables from 
without any parents.  Typically, we also assume that the 
process is stationary, i.e., the transition models for all time 
slices are identical: 
Thus a 
DBN is defined to be a pair of Bayesian networks I 
where 
is 
a two-time-slice Bayesian network, which as discussed above 
defines the transition distribution 

represents the initial distribution  P{ZO), and 

which  contains  variables from 

and/or 

The set 

is commonly divided into two sets:  the unob(cid:173)
. The 
are assumed to depend only on the cur(cid:173)
. The joint diwStribution represented by 

served state variables 
observed variables 
rent state variables 
a DBN can then be obtained by unrolling the 2TBN: 

and the observed variables 

Various types of inference in DBNs are possible.  One of 
the most useful is state monitoring (also known as filtering or 
tracking), where the goal is to estimate the current state of the 
world given the observations made up to the present, i.e., to 
compute the distribution 
Proper state 
monitoring is a necessary precondition tor rational decision(cid:173)
making  in  dynamic  domains. 
Inference  in  DBNs  is  NP-
complete,  and  thus  we  must  resort  to  approximate  meth(cid:173)
ods,  of which  the  most  widely  used  one  is particle filter-
ing  [Doucet  et 0/.,  2001].  Particle  filtering  is  a  stochas(cid:173)
tic  algorithm  which  maintains  a  set  of particles  (samples) 
to approximately represent the distribution of 
possible states at time  given the observations.  Each parti(cid:173)
cle 
contains a complete instance of the current state, i.e., a 
sampled value for each state variable. The current distribution 
is then approximated by 

where 

is 1 if the state represented b y is  same as 
and 0 otherwise. The particle filter starts by generating TV 
particles according to the initial distribution P 
Then, at 
for each parti(cid:173)
each step, it first generates the next state 
It then weights these 
cle  by sampling from 
samples according to the likelihood they assign to the ob(cid:173)
and resamples N particles from 
servations, 

A probabilistic relational model (PRM) encodes a proba(cid:173)
bility distribution over the set of all possible instantiations / 
of a schema [Friedman et al., 1999]. The object skeleton of 
an instantiation is the set of objects in it, with all attributes 
unspecified. The relational skeleton of an instantiation is the 
set of objects in it, with all relational attributes specified, and 
all propositional attributes unspecified.  In the simplest case, 
the relational skeleton is assumed known, and the PRM spec(cid:173)
ifies a probability  distribution  for each  attribute  A  of each 
class  C.  The  parents  of each  attribute  (i.e.,  the  variables 
it depends on) can be other attributes of C,  or attributes of 

lC.R can also be defined as a function from 

choose the simpler convention here. 

but we 

PROBABILISTIC  INFERENCE 

993 

classes that are related to C by some slot chain. A slot chain 
is a composition of relational attributes.  In general, it must 
be used together with an aggregation function that reduces a 
variable number of values to a single value.  For example, a 
parent of an attribute of a bolt in the aircraft schema might be 
avg(bolt.plate.nut.weight), the average weight of all the nuts 
on the metal plates that the bolt is attached to. 
Definition 1 A probabilistic relational model (PRM) II for a 
relational schema S is defined as follows.  For each class C 
and each propositional attribute A € -4(C), we have: 

DPRMs are extended to the case  where only the object 
skeleton for each time slice is known in the same way that 
PRMs are, by adding to Definition 2 a set of parents and con(cid:173)
ditional probability model for each relational attribute, where 
the parents can be in the same or the previous time slice. 
When the object skeleton is not known (e.g., if objects can 
appear and disappear over time), the 2TPRM includes in ad(cid:173)
dition a Boolean existence variable for each possible object, 
again with parents from the same or the previous time slice.3 
As with DBNs, we may wish to distinguish between observed 
and unobserved attributes of objects. In addition, wc can con(cid:173)
sider an Action class with a single attribute whose domain is 
the set of actions that can be performed by some agent (e.g., 
painting a metal plate, or bolting two plates together).  The 
distribution over instantiations in a time slice can then de(cid:173)
pend on the action performed in that time slice. For example, 
may with high probability pro(cid:173)
the action 
duce 
and with lower probability set 
Part J.mate to some other object of Part2's class (i.e., be im(cid:173)
properly performed, resulting in a fault). 

Just as a PRM can be unrolled into a Bayesian network, so 
can a DPRM be unrolled into a DBN. (Note, however, that 
this DBN may in general contain different variables in dif(cid:173)
ferent time slices.)  In principle,  we can perform inference 
on this DBN using particle  filtering.  However,  the  filter  is 
likely to perform poorly, because for non-trivial DPRMs its 
state space will be huge. Not only will it contain one variable 
for each attribute of each object of each class, but relational 
attributes will in general have very large domains.  We over(cid:173)
come this by adapting Rao-Blackwellisation to the relational 
setting. We make the following (strong) assumptions: 
1. Relational attributes with unknown values do not appear 
anywhere  in  the  DPRM  as  parents  of  unobserved  at(cid:173)
tributes, or in their slot chains. 

2.  Each reference slot can be occupied by at most one object. 
Proposition 1 Assumptions 1 and 2 imply that, given the 
propositional attributes and known relational attributes at 
times t and t — 1, the joint distribution of the unobserved 
relational attributes at time t is a product of multinomials, 
one for each attribute. 

Notice also that, by Assumption  1,  unobserved proposi(cid:173)
tional attributes can be sampled without regard to unobserved 
relational ones.  Rao-Blackwellisation can now be applied 
3Notice that the attributes of nonexistent objects need not be 
specified, because by definition no attributes of any other objects 
can depend on them [Getoor et al., 2001 ]. 

A PRM and relational skeleton can thus be unrolled into a 
large Bayesian network with one variable for each attribute 
of each object in the skeleton.2  Only PRMs that correspond 
to Bayesian networks without cycles are valid. 

More generally, only the object skeleton might be known, 
in which case the PRM also needs to specify a distribution 
over the relational attributes [Getoor et al., 2001]. In the air(cid:173)
craft domain,  a PRM might specify a distribution over the 
state of assembly of an airplane, with probabilities for differ(cid:173)
ent faults (e.g., a bolt is loose, the wrong plates have been 
bolted, etc.). 
4  Dynamic Probabilistic Relational Models 
In this section we extend PRMs to modeling dynamic sys(cid:173)
tems, the same way that DBNs extend Bayesian networks. 
We begin with the observation that a DBN can be viewed as a 
special case of a PRM, whose schema contains only one class 
Z with propositional attributes  Z\,..., Za and a single rela(cid:173)
tional attribute previous. There is one object Zt for each time 
slice, and the previous attribute connects it to the object in 
the previous time slice. Given a relational schema S, we first 
extend each class C with the relational attribute C.previous, 
with domain C.  As before, we initially assume that the re(cid:173)
lational skeleton at each time slice is known.  We can then 
define two-time-slice PRMs and dynamic PRMs as follows. 

Plus auxiliary (deterministic) variables for the required aggre(cid:173)

gations, which we omit from the formula for simplicity. 

994 

PROBABILISTIC  INFERENCE 

as the propositional attributes of all objects and 

with 
as their relational attributes. A Rao-Blackwellised particle is 
composed of sampled values for all propositional  attributes 
of all objects, plus a probability vector for each relational at(cid:173)
tribute of each object.  The vector element corresponding to 
is the probability that relation R holds between obj 
and the zth object of the target class, conditioned on the values 
of the propositional attributes in the particle, etc. 

Rao-Blackwellising the relational attributes can vastly re(cid:173)
duce the size of the state space which particle filtering needs 
to  sample.  However,  if the  relational  skeleton  contains  a 
large number of objects and relations, storing and updating 
all the requisite probabilities can still become quite expen(cid:173)
sive. This can be ameliorated if context-specific independen(cid:173)
cies exist, i.e., if a relational attribute is independent of some 
propositional  attributes given assignments of values to oth(cid:173)
ers [Boutilier et aly  1996].  We can then replace the vector 
of probabilities with a tree structure whose leaves represent 
probabilities for entire sets of objects. More precisely, we de(cid:173)
fine the abstraction tree data structure for a relational attribute 
obj.R with target class  _  as follows. A node  of the tree is 
composed of a probability p and a logical expression  over 
the propositional attributes of the schema. Let 
be the 
that satisfy the  's  of  and all of  's an-
set of objects in 

5  Experiments 
In this section we study the application of DPRMs to fault 
detection in complex assembly plans. We use a modified ver(cid:173)
sion of the Schedule World domain from the AIPS-2000 Plan(cid:173)
ning Competition.4 The problem consists of generating a plan 
for assembly of objects with operations such as painting, pol(cid:173)
ishing, etc.  Each object has attributes such as surface type, 
color, hole size, etc. We add two relational operations to the 
domain:  bolting and welding.  We assume that actions may 
be faulty, with fault model described below.  In our experi(cid:173)
ments, we first generate a plan using the FF planner [Hoff(cid:173)
mann and Nebel, 2001],  We then monitor the plan's execu(cid:173)
tion using particle filtering (PF), Rao-Blackwellised particle 
filtering (RBPF) and RBPF with abstraction trees. 

We consider three classes of objects:  Plate, Bracket and 
Bolt. Plate and Bracket have propositional attributes such as 
weight, shape, color, surface type, hole size and hole type, 
and relational attributes for the parts they are welded to and 
the bolts bolting them to other parts (e.g., Plate73.bolt4 corre(cid:173)
sponds to the fourth bolt hole on plate 73). The Bolt class has 
propositional attributes such as size, type and weight. Propo(cid:173)
sitional actions include painting, drilling and polishing, and 
change the propositional  attributes of an object.  The rela(cid:173)
tional action Bolt sets a bolt attribute of a Plate or Bracket 
object to a Bolt object. The Weld action sets a welded-to at(cid:173)
tribute of a Plate or Bracket object to another Plate or Bracket 
object. 

The fault model has a global parameter, the fault proba(cid:173)
bility 
,  With probability 1  - 
an action produces the 
intended effect.  With probability 
, one of several possible 
faults occurs.  Propositional faults include a painting oper(cid:173)
ation not being completed, the wrong color being used, the 
polish of an object being ruined, etc.  The probability of dif(cid:173)
ferent propositional  faults depends on the properties of the 
object being acted on.  Relational faults include bolting the 
wrong objects and welding the wrong objects.  The proba(cid:173)
bility of choosing a particular wrong object depends on its 
similarity to the intended object.  Similarity depends on dif(cid:173)
ferent propositional attributes for different actions and differ(cid:173)
ent classes of objects.  Thus the probability  of a particular 
wrong object being chosen is uniform across all objects with 
the same relevant attribute values. 

The DPRM also includes the following observation model. 
There are two instances of each attribute: the true one, which 
is never observed, and the observed one, which is observed 
at selected time steps.  Specifically, when an action is per(cid:173)
formed,  all  attributes  of the  objects  involved  in  it  are  ob(cid:173)
served, and no others.  Observations are noisy:  with proba(cid:173)
bility 
the true value of the attribute is observed, and 
with probability 
an incorrect value is observed.  Incorrect 
values for propositional observations are chosen uniformly. 
Incorrect values for relational observations are chosen with 
a probability that depends on the similarity of the incorrect 
object to the intended one. 

Notice  that,  if the  domain  consisted  exclusively  of the 
propositional attributes and actions on them, exact inference 
might be possible; however, the dependence of relational at(cid:173)
tributes and their observations on the propositional attributes 

4URL: http://www.cs.toronto.edu/aips2000 

PROBABILISTIC  INFERENCE 

995 

creates  complex  dependencies  between these,  making  ap(cid:173)
proximate inference necessary. 

A natural measure of the accuracy of an approximate infer(cid:173)
ence procedure is the K-L divergence between the distribu(cid:173)
tion it predicts and the actual one [Cover and Thomas, 2001]. 
However, computing it requires performing exact inference, 
which for non-trivial DPRMs is infeasible. Thus we estimate 
the K-L divergence by sampling, as follows.  Let D 
be 
the K-L divergence between the true distribution p and its 
approximation p, and let 
be the domain over which the 
distribution is defined. Then 

The first term  is  simply the entropy of X,  H(X),  and  is  a 
constant independent of the approximation  method.  Since 
we are mainly interested in measuring differences in perfor(cid:173)
mance between approximation methods, this term can be ne(cid:173)
glected. The K-L divergence can now be approximated in the 
usual way by taking S samples from the true distribution: 

where 
is the probability of the  zth sample according 
to the approximation procedure,  and the H subscript indi(cid:173)
is  offset  by  H(X).  We 
cates that the estimate  of 
thus evaluate the accuracy of PF and RBPF on a DPRM by 
generating S =  10,000 sequences of states and observations 
from the DPRM, passing the observations to the particle fil(cid:173)
ter,  inferring the marginal probability of the sampled value 
of each state variable at each step, plugging these values into 
the above formula, and averaging over all variables.  Notice 
that 
whenever a sampled value is not rep(cid:173)
resented in any particle.  The empirical estimates of the K-L 
divergence we obtain will be optimistic in the sense that the 
true K-L divergence may be infinity, but the estimated one 
will still be finite unless one of the values with zero predicted 
probability is sampled. This does not preclude a meaningful 
comparison between approximation methods, however, since 
on average the worse method should produce 
earlier in the time sequence. We thus report both the average 
K-L divergence before it becomes infinity and the time step 
at which it becomes infinity, if any. 

Figures 1 and 2 show the results of the experiments per(cid:173)
formed.  The observation noise parameter  was set to the 
same value as the fault probability 
throughout. One action 
is performed in each time step; thus the number of time steps 
is the length of the plan. The graphs show the K-L divergence 
of PF and RBPF at every 100th step (it is the same for RBPF 
with and without abstraction trees).  Graphs are interrupted 
at the first point where the K-L divergence became infinite 
in any of the runs (once infinite, the K-L divergence never 
went back to being finite in any of the runs), and that point is 
labeled with the average time step at which the blow-up oc(cid:173)
curred. As can be seen, PF tends to diverge rapidly, while the 

Figure  1:  Comparison  of RBPF  (5000  particles)  and  PF 
(200,000 particles) for 1000 objects and varying fault prob(cid:173)
ability. 

Figure  2:  Comparison  of RBPF  (5000  particles)  and  PF 
(200,000 particles) for fault probability of 1% and varying 
number of objects. 

K-L divergence of RBPF increases only very slowly, for all 
combinations of parameters tried.  Abstraction trees reduced 
RBPF's time and memory by a factor of 30 to 70, and took 
on average six times longer and 11 times the memory of PF, 
per particle.  However, note that we ran PF with 40 times 
more particles than RBPF. Thus, RBPF is using less time and 
memory than PF, and performing far better in accuracy. 

We also ran all the experiments while measuring the K-L 
divergence of the  full joint distribution of the state (as op(cid:173)
posed to just the marginals).  RBPF performed even better 
compared to PF in this case; the latter tends to blow up much 
sooner (e.g.,  from  around step 4000 to less than  1000 for 
1% and 1000 objects), while RBPF continues to de(cid:173)

grade only very slowly. 

6  Related Work 
Dynamic  object-oriented  Bayesian  networks  (DOOBNs) 
[Friedman et al, 1998] combine DBNs with OOBNs, a pre(cid:173)
decessor  of PRMs.  Unfortunately,  no  efficient  inference 

PROBABILISTIC  INFERENCE 

methods were proposed for DOOBNs, and they have not been 
evaluated experimentally. DPRMs can also be viewed as ex(cid:173)
tending relational Markov models (RMMs) [Anderson et al, 
2002] and logical hidden Markov models (LOHMMs) [Ker-
sting et al, 2003] in the same way that DBNs extend HMMs. 
Downstream, DPRMs should be relevant to research on re(cid:173)
lational  Markov  decision  processes  (e.g.,  [Boutilier  et  al, 
2001]). 

Particle filtering is currently a very active area of research 
[Doucet et al., 2001]. In particular, the FastSLAM algorithm 
uses a tree structure to speed up RBPF with Gaussian vari(cid:173)
ables [Montemerlo et al., 2002].  Abstraction trees are also 
related to the abstraction hierarchies in RMMs [Anderson et 
al., 2002] and to AD-trees [Moore and Lee, 1997]. An alter(cid:173)
nate method for efficient inference in DBNs that may also be 
useful in DPRMs was proposed by Boyen and Roller [1998] 
and combined with particle filtering by Ng et al. [2002]. Ef(cid:173)
ficient inference in relational probabilistic models has been 
studied by Pasula and Russell [2001]. 
7  Conclusions and Future Work 
This paper introduces dynamic probabilistic relational mod(cid:173)
els  (DPRMs),  a representation  that handles  time-changing 
phenomena,  relational  structure  and uncertainty  in  a prin(cid:173)
cipled manner.  We develop efficient approximate inference 
methods for DPRMs, based on Rao-Blackwellisation of rela(cid:173)
tional attributes and abstraction trees.  The power of DPRMs 
and the scalability of these inference methods are illustrated 
by  their  application  to  monitoring  assembly  processes  for 
fault detection. 

Directions  for future work  include  relaxing the assump(cid:173)
tions made, further scaling up inference,  formally studying 
the properties of abstraction trees, handling continuous vari(cid:173)
ables, learning DPRMs, using them as a basis for relational 
MDPs, and applying them to increasingly complex real-world 
problems. 

8  Acknowledgements 
This work was partly supported by an NSF CAREER Award 
to the second author, by ONR grant N00014-02-1-0932, and 
by NASA grant NAG 2-1538. We are grateful to Mausam for 
helpful discussions. 
References 
[Anderson et al., 2002]  C.  Anderson,  P.  Domingos,  and 
D. Weld.  Relational Markov models and their application 
to adaptive Web navigation.  In Proc. 8th ACM SIGKDD 
Intl.  Conf on Knowledge Discovery and Data Mining, 
pages 143-152, Edmonton, Canada, 2002. 

[Boutilier et al, 1996]  C. Boutilier, N. Friedman, M. Gold-
szmidt, and D. Koller.  Context-specific independence in 
Bayesian networks.  In Proc. 12th Conf. on Uncertainty 
in Artificial Intelligence, pages  115-123,  Portland, OR, 
1996. 

[Boutilier etal, 2001]  C. Boutilier, R. Reiter, and B. Price. 
Symbolic dynamic programming for first-order MDPs.  In 
Proc. 17th Intl. Joint Conf. on Artificial Intelligence, pages 
690-697, Seattle, WA, 2001. 

[Boyen and Koller, 1998]  X. Boyen and D. Koller. Tractable 
inference for complex stochastic processes. In Proc. 14th 
Conf on Uncertainty in Artificial Intelligence, pages 33-
42, Madison, WI, 1998. 

[Cover and Thomas, 2001]  T.  Cover and J.  Thomas.  Ele(cid:173)

ments of Information Theory. Wiley, New York, 2001. 

[Dean and Kanazawa, 1989]  T. Dean and K. Kanazawa.  A 
model  for  reasoning  about  persistence  and  causation. 
Computational Intelligence, 5(3): 142-150, 1989. 

[Doucetetal, 2001]  A. Doucet, N. de Freitas, and N. Gor(cid:173)

don, editors. Sequential Monte Carlo Methods in Practice. 
Springer, New York, 2001. 

[Friedman et al, 1998]  N. Friedman, D. Koller, and A. Pfef-
fer.  Structured representation of complex stochastic sys(cid:173)
tems.  In Proc.  15th National Conf on Artificial Intelli(cid:173)
gence, pages 157-164, Madison, WI, 1998. 

[Friedman et al, 1999]  N.  Friedman,  L.  Getoor,  D.  Koller, 
and A. PfefTer.  Learning probabilistic relational models. 
In Proc.  16th Intl. Joint Conf on Artificial Intelligence, 
pages 1300-1309, Stockholm, Sweden, 1999. 

[Getoor et al, 2001]  L. Getoor, N. Friedman, D. Koller, and 
B.  Taskar.  Learning  probabilistic  models  of relational 
structure. In Proc. 18th Intl. Conf on Machine Learning, 
pages 170-177, Williamstown, MA, 2001. 

[Hoffmann and Nebel, 2001] J. Hoffmann and B. Nebel. The 
FF planning system: Fast plan generation through heuris(cid:173)
tic search.  Journal of Artificial Intelligence Research, 
14:253-302,2001. 

[Kersting et al, 2003]  K. Kersting, T. Raiko, S. Kramer, and 
L. De Raedt. Towards discovering structural signatures of 
protein folds based on logical hidden Markov models.  In 
Proc. 8th Pacific Symposium on Biocomputing, Kauai, HI, 
2003. 

[Montemerlo etal, 2002]  M.  Montemerlo,  S.  Thrun, 
D.  Koller,  and  B.  Wegbreit.  FastSLAM:  A  factored 
solution  to  the  simultaneous  localization  and  mapping 
In  Proc.  18th  National  Conf  on  Artificial 
problem. 
Intelligence, pages 593-598, Edmonton, Canada, 2002. 

[Moore and Lee, 1997]  A. W. Moore and M. S. Lee. Cached 
sufficient  statistics  for  efficient  machine  learning  with 
large datasets. Journal of Artificial Intelligence Research, 
8:67-91, 1997. 

[Murphy and Russell, 2001]  K. Murphy and S. Russell. Rao-
Blackwcllised particle filtering for dynamic Bayesian net(cid:173)
works.  In A. Doucet, N. de Freitas, and N. Gordon, edi(cid:173)
tors, Sequential Monte Carlo Methods in Practice, pages 
499-516. Springer, New York, 2001. 

[Ng et al, 2002]  B. Ng, L. Peshkin, and A. Pfeffer. Factored 
particles for scalable monitoring.  In Proc. 18th Conf on 
Uncertainty in Artificial Intelligence, Edmonton, Canada, 
2002. Morgan Kaufmann. 

[Pasula and Russell, 2001]  H. Pasula and S. J. Russell.  Ap(cid:173)
proximate inference for first-order probabilistic languages. 
In Proc.  17th Intl. Joint Conf on Artificial Intelligence, 
pages 741-748, Seattle, WA, 2001. 

PROBABILISTIC  INFERENCE 

997 


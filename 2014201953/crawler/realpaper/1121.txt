The Value of Observation for Monitoring Dynamic Systems

Eyal Even-Dar

Sham M. Kakade

Yishay Mansour∗

Computer and Information Science

Toyota Technological Institute

School of Computer Science

University of Pennsylvania
Philadelphia, PA 19104 USA

evendar@seas.upenn.edu

Chicago, IL, USA

sham@tti-c.org

Tel Aviv University

Tel Aviv, 69978, Israel
mansour@cs.tau.ac.il

Abstract

We consider the fundamental problem of monitor-
ing (i.e. tracking) the belief state in a dynamic sys-
tem, when the model is only approximately correct
and when the initial belief state might be unknown.
In this general setting where the model is (perhaps
only slightly) mis-speciﬁed, monitoring (and con-
sequently planning) may be impossible as errors
might accumulate over time. We provide a new
characterization, the value of observation, which
allows us to bound the error accumulation.

The value of observation is a parameter that gov-
erns how much information the observation pro-
vides. For instance, in Partially Observable MDPs
when it is 1 the POMDP is an MDP while for an
unobservable Markov Decision Process the param-
eter is 0. Thus, the new parameter characterizes a
spectrum from MDPs to unobservable MDPs de-
pending on the amount of information conveyed in
the observations.

1 Introduction

Many real world applications require estimation of the un-
known state given the past observations. The goal is to main-
tain (i.e.
track) a belief state, a distribution over the states;
in many applications this is the ﬁrst step towards even more
challenging tasks such as learning and planning. Often the
dynamics of the system is not perfectly known but an approx-
imate model is available. When the model and initial state are
perfectly known then state monitoring reduces to Bayesian
inference. However, if there is modelling error (e.g. the tran-
sition model is slightly incorrect), then the belief states in the
approximate model may diverge from the true (Bayesian) be-
lief state. The implications of such a divergence might be
dire.

The most popular dynamic models for monitoring some
unknown state are the Hidden Markov Model (HMM) [Ra-
biner & Juang, 1986] and extensions such as Partially Observ-
able Markov Decision Process [Puterman, 1994], Kalman Fil-
ters [Kalman, 1960] and Dynamic Bayesian Networks [Dean

∗Supported in part by a grant from ISF and BSF.

& Kanazawa, 1989] — all of which share the Markov as-
sumption. Naturally, one would like to provide conditions
as to when monitoring is possible when modelling errors are
present. Such conditions can be made on either the dynam-
ics of the system (i.e. the transition between the states) or on
the observability of the system. While the true model of the
transition dynamics usually depends on the application itself,
the observations often depend on the user, e.g. one might be
able to obtain better observations by adding more sensors or
just more accurate sensors. In this paper our main interest is
in quantifying when the observations become useful and how
it effects the monitoring problem.

Before we deﬁne our proposed measure, we give an illus-
trative example of an HMM, where the value of information
can vary in a parametric manner. Consider an HMM in which
at every state the observation reveals the true state with prob-

ability 1−  and with probability  gives a random state. This
can be thought of as having a noisy sensor.
Intuitively, as
the parameter  varies from zero to one, the state monitoring
become harder.

We introduce a parameter which characterizes how infor-
mative the observations are in helping to disambiguate what
the underlying hidden state is. We coin this parameter the
value of observation. Our value of observation criterion tries
to quantify that different belief states should have different
observation distributions. More formally, the L1 distance be-
tween any two belief states and their related observation dis-
tributions is maintained up to a multiplicative factor (which
is at least the value of observation parameter).

In this paper we use as an update rule a variant of the
Bayesian update. First we perform a Bayesian update given
our (inaccurate) model, and then we add some noise (in par-
ticular, we mix the resulting belief state with the uniform dis-
tribution). Adding noise is crucial to our algorithm as it en-
sures the beliefs do not become incorrectly overly conﬁdent,
thus preventing the belief state from adapting fast enough to
new informative information.

Our main results show that if the model is only approxi-
mate then our modiﬁed Bayesian updates guarantee that the
true belief state and our belief state will not diverge — assum-
ing the value of observation is not negligible. More speciﬁ-
cally, we show that if the initial state is approximately ac-
curate, then the expected KL-divergence between our belief
state and the true belief state remains small. We also show

IJCAI-07

2474

that if we have an uninformative initial state (e.g., arbitrary
initial belief state) we will converge to a belief state whose
expected KL-divergence from the true belief state is small
and will remain as such from then on. Finally, we extend our
results to the setting considered in [Boyen & Koller, 1998],
where the goal is to compactly represent the belief state. The
precision and rate of convergence depends on the value of
observation.

One natural setting with an inaccurate model is when the
underlying environment is not precisely Markovian. For ex-
ample, it might be that the transition model is slightly inﬂu-
enced by some other extrinsic random variables. Given these
extrinsic variables, the true transition model of the environ-
ment is only a slightly different model each time step. This
is a case where we might like to model the environment as
Markovian, even at the cost of introducing some error, due to
the fact that transition model is not entirely Markovian. Our
results apply also to this setting. This is an encouraging re-
sult, since in many cases the Markovian assumption is more
of an abstraction of the environment, then the a precise de-
scription.
Related Work. The work most closely related to ours is that
of Boyen and Koller (1998), where they considered monitor-
ing in a Hidden Markov Model. In their setting, the environ-
ment is (exactly) known and the agent wants to keep a com-
pact factored representation of the belief state (which may
not exactly have a factored form). Their main assumption is
that the environment is mixing rapidly, i.e the error contract
by geometric factor after each time we apply the transition
matrix operator. In contrast, we are interested in monitoring
when we have only an approximate environment model. Both
our work and theirs assume some form of contraction where
beliefs tend to move closer to the truth under the Bayesian
updates — ours is through an assumption about the value of
observation while their is through assumption about the tran-
sition matrix. The main advantage of our method is that in
many applications one can improve the quality of its observa-
tions, by adding more and better sensors. However, the mix-
ing assumption used by Boyan and Koller may not be alter-
able. Furthermore, in the ﬁnal Section, we explicitly consider
their assumption in our setting and show how a belief state
can be compactly maintained when both the model is approx-
imate and when additional error accumulates from maintain-
ing a compact factored representation.

Particle Filtering [Doucet, 1998] is a different monitoring
approach, in which one estimates the current belief state by
making a clever sampling, where in the limit one observes the
true belief state. The major drawback with this method is in
the case of a large variance where it requires many samples.
A combination of the former two methods was considered by
[Ng et al., 2002].

Building on the work of [Boyen & Koller, 1998] and the
trajectory tree of [Kearns et al., 2002], McAllester and Singh
(1999) provides an approximate planning algorithm. Similar
extensions using our algorithm may be possible.

Outline. The outline of the paper is as follows. In Sec-
tion 2 we provide notation and deﬁnitions. Section 3 is the
main section of the paper and deals with monitoring and is
composed from several subsections; Subsection 3.1 describes

the algorithm; Subsection 3.2 provides the main monitoring
theorem; Subsection 3.3 proves the Theorem. In Section 4,
we show how to extend the results into the dynamic Bayesian
networks.

2 Preliminaries
An Hidden Markov Model (HMM) is 4-tuple, (S, P, Ob, O),
where S is the set of states such that |S| = N , P is the
transition probability form every state to every state, Ob is
the observations set and O is the observation distribution in
every state. A belief state b is a distribution over the states
S such that b(i) is the probability of being at state si. The
transition probabilities of the belief states are deﬁned accord-
ing to HMM transition and observation probability, using a
Bayesian update.

For a belief state b(·), the probability of observing o is

O(o|b), where

(cid:2)

O(o|b) =

O(o|s)b(s).

After observing an observation o in belief state b(·), the up-

s

dated belief state is:

(U O

o b)(s) =

O(o|s)b(s)
O(o|b)
(cid:2)

(T P b)(s) =

P (s(cid:2), s)b(s(cid:2)

).

where U O
Also, we deﬁne the transition update operator T as,

o is deﬁned to be the observation update operator.

s(cid:2)

We denote by bt the belief state at time t, where at time 0 it
is b0. (We will discuss both the case that the initial belief state
is known and the case where it is unknown.) After observing
observation ot ∈ Ob, the inductive computation of the belief
state for time t + 1 is:

bt+1 = T P U O
ot

bt,

where we ﬁrst update the belief state by the observation up-
date operator according to the observation ot and then by the
transition update operator. It is straightforward to consider
a different update order. Therefore, bt+1 is the distribution
over states conditioned on observing {o0, o1, . . . ot} and on
the initial belief state being b0.

3 Approximate Monitoring

We are interested in monitoring the belief state in the case
where either our model is inaccurate or we do not have the
correct initial belief state (or both). Let us assume that an
algorithm has access to a transition matrix
tion distribution
models. The algorithm’s goal is to accurately estimate the
belief state at time t, which we denote by ˆbt.

(cid:3)P and an observa-
(cid:3)O, which have error with respect to the true
(cid:3)P are clear from the context, we deﬁne T to
(cid:3)O are clear from the
(cid:3)T to be T bP
(cid:3)Uo to be U bO

When P and
be T P
context, we deﬁne Uo to be U O

For notational simplicity, we deﬁne Eo∼b = Eo∼O(·|b).

. When O and

o and

and

o .

IJCAI-07

2475

Our main interest is the behavior of

(cid:5)
(cid:4)
KL(bt||ˆbt)

E

where the expectation is taken with respect to observation se-

quences {o0, o1, . . . ot−1} drawn according to the true model,
and bt and ˆbt are the belief states at time t, with respect to
these observation sequences.

In order to quantify the accuracy of our state monitoring,
we must assume some accuracy conditions on our approxi-
mate model. The KL-distance is the natural error measure.
The assumptions that we make now on the accuracy of the
model will later be reﬂected in the quality of the monitoring.

Assumption 3.1 (Accuracy) For a given HMM model
(S, P, Ob, O), an (T , O) accurate model
is an HMM
(S, P, Ob, O), such that for all states s ∈ S,

KL(P (·|s)||(cid:3)P (·|s)) ≤ T
KL(O(·|s)||(cid:3)O(·|s)) ≤ O .

Next we deﬁne the value of observation parameter.

Deﬁnition 3.1 Given an observation distribution O, let M O
be the matrix such that its (o, s) entry is O(o|s). The Value
(cid:4)M x(cid:4)1 and it is
of Observation, γ, is deﬁned as inf x:(cid:4)x(cid:4)1=1
in [0, 1].

Note that if the value of observation is γ, then for any two

belief states b1 and b2,

(cid:4)b1 − b2(cid:4)1 ≥ (cid:4)O(·|b1) − O(·|b2)(cid:4)1 ≥ γ(cid:4)b1 − b2(cid:4)1 .

where the ﬁrst inequality follows from simple algebra.

The parameter γ plays a critical rule in our analysis. At
the extreme, when γ = 1 we have (cid:4)b1 − b2(cid:4)1 = (cid:4)O(·|b1) −
O(·|b2)(cid:4)1. Note that this deﬁnition is very similar to def-
(cid:4)P (b1,·) −
inition of the Dobrushin coefﬁcient, supb1,b2
P (b2,·)(cid:4)1 and it is widely used in the ﬁltering literature
Let γ be 1 and consider b1 having support on one state and
b2 on another state. In this case (cid:4)b1 − b2(cid:4)1 = 2 and there-
fore (cid:4)O(·|b1) − O(·|b2)(cid:4)1 = 2, which implies that we have

[Moral, 2004]. We now consider some examples.

a different observations from the two states. Since this holds
for any two states, it implies that given an observation we can
uniquely recover the state. To illustrate the value observation
characterization, in POMDP terminology for γ = 1 we have
a fully observable MDP as no observation can appear with
positive probability in two states. At the other extreme, for an
unobservable MDP, we can not have a value of γ > 0 since
(cid:4)O(·|b1) − O(·|b2)(cid:4)1 = 0 for any two belief states b1 and b2.
the observation reveals the true state with probability 1− and
with probability  gives a random state. Here, it is straight-
forward to show that γ is 1− . Hence, as  approaches 0, the
value of observation approaches 1.

Recall the example in the Introduction where at every state

We now show that having a value of γ bounded away from
zero is sufﬁcient to ensure some guarantee on the monitor-
ing quality, which improves as γ increases. Throughout, the
paper we assume that γ > 0.

3.1 The Belief State Update
Now we present the belief state update. The naive approach
is to just use the approximate transition matrix ˆP and the ap-
proximate observation distribution ˆO. The problem with this
approach is that the approximate belief state might place neg-
ligible probability on a possible state and thus a mistake may
be irreversible.

Consider the following update operator ˜T . For each states

s ∈ S,

( ˜T )(s) = (1 − U )( ˆT )(s) + U Uni(s),

where Uni is the uniform distribution. Intuitively, this update
operator ˜U mixes with the uniform distribution, with weight
U , and thus always keeps the probability of being in any state
bounded away from zero. Unfortunately, the mixture with
the uniform distribution is an additional source of inaccuracy
in the belief state, which our analysis would latter have to
account for.

The belief state update is as follows.
ˆbt,

ˆbt+1 = ˜T ˆUot
where ˆbt is our previous belief state.

(1)

3.2 Monitoring the belief state
In this subsection we present our main theorem, which relates
the accuracy of the belief state to our main parameters: the
quality of the approximate model, the value of observation,
and the weight on the uniform distribution.
Theorem 3.2 At time t let ˆbt be the belief state updated ac-
cording to equation (1), bt be the true belief state, Zt =
E

, and γ be the value of observation. Then

(cid:5)
(cid:4)
KL(bt||ˆbt)
Furthermore, if (cid:4)ˆb0 − b0(cid:4)1 ≤(cid:6)

Zt+1 ≤ Zt +  − αZ 2
t ,

where  = T + U log N + 3γ

√
O and α =

α then for all times t:

γ 2

2 log2 N
U

.



(cid:7)

Also, for any initial belief states b0 and ˆb0, and δ > 0, there
exists a time τ (δ) ≥ 1, such that for any t ≥ τ (δ) we have

Zt ≤


α + δ =

log N
U

γ

√

2 + δ

The following corollary now completely speciﬁes the algo-
rithm by providing a choice for U , the weight of the uniform
distribution.

Corollary 3.3 Assume that (cid:4)ˆb0 − b0(cid:4)1 ≤ (cid:6)



α and U =

T

log N . Then for all times t,

(cid:8)

T

Zt ≤ 6 log N
(cid:8)

γ

T + γ

(cid:8)

√
O ≤ 3

2 =

4T + 6γ

Proof: With the choice of U , we have:

√

√
O

√
O.

T + γ

And,

log

N
U

= log

N log N

T

≤ 2 log

N
T

,

which completes the proof.

(cid:2)


α =

log N
U

γ

√

2

Zt ≤
(cid:7)

IJCAI-07

2476

3.3 The Analysis
We start by presenting two propositions useful in proving the
theorem. These are proved later. The ﬁrst provides a bound
on the error accumulation.

Proposition 3.4 (Error Accumulation) For every belief states
bt and ˆbt and updates bt+1 = T Uot
ˆbt,
we have:
Eot

(cid:5)
(cid:4)
KL(bt+1||ˆbt+1)

≤ KL(bt||ˆbt) + U log N + O

bt and ˆbt+1 = ˜T(cid:3)Uot
−KL(O(·|bt)||(cid:3)O(·|ˆbt))

The next proposition lower bounds the last term in Propo-
sition 3.4. This term, which depends in the value of obser-
vation, enables us to ensure that the two belief states will not
diverge.
Proposition 3.5 (Value of Observation) Let γ be the value of
observation, b1 and b2 be belief states such that b2(s) ≥ μ
for all s. Then

KL(O(·|b1)||(cid:3)O(·|b2)) ≥ 1

(cid:10)

2

(cid:9)

γKL(b1||b2)
log 1
√
μ
O + O

2
−3γ

Using these two propositions we can prove our main theo-

rem, Theorem 3.2.

Eot

have:

2

2

(cid:14)

(cid:11)
(cid:13)(cid:11)

By taking expectation with respect to {o0, o1, . . . ot−1}, we

Zt+1 ≤ Zt +  − αE
≤ Zt +  − αZ 2
(cid:13)(cid:11)
t ,
(cid:11)

Proof of Theorem 3.2: Due to the fact that ˜U mixes with
the uniform distribution, we can take μ = U /N . Combining
Propositions 3.4 and 3.5, and recalling the deﬁnition of , we
obtain that:

(cid:4)
KL(bt+1||ˆbt+1)|ot−1, ..., o0
KL(bt||ˆbt) +  − α

(cid:5)
(cid:12)
≤
KL(bt||ˆbt)
(cid:12)
KL(bt||ˆbt)
(cid:4)
(cid:5)(cid:12)
KL(bt||ˆbt)
good in the sense that (cid:4)ˆb0 − b0(cid:4)1 ≤ (cid:6)
has derivative 1 − 2Ztα, which is positive when Zt ≤(cid:6)
α , then every Zt ≤(cid:6)
(cid:6)

We proceed with the case where the initial belief state is

α . Then we have
t + 

α .

Since Zt at
α is
mapped to a smaller value. Hence the Zt will always remain
below

(cid:6)
α . The function Zt − αZ 2

(cid:12)
KL(bt||ˆbt)

We conclude with the subtle case of unknown initial belief

state, and deﬁne the following random variable

where the last line follows since, by convexity,

which proves the ﬁrst claim in the theorem.

that Zt is always less than


α is mapped to

(cid:6)

(cid:6)

(cid:6)

2

≥

E

(cid:15)

(cid:14)





2

,

E


α .

(cid:6)

Z (cid:2)

t =

Zt, Zt >


α
, Otherwise


α

(cid:6)

(cid:6)

Note that Z (cid:2)

cannot be larger than

t is a positive super-martingale and therefore
converges with probability 1 to a random variable Z (cid:2)
. The
expectation of Z (cid:2)

α , since whenever
Z (cid:2)

α its expectation in the next timestep is
is larger than
t ≥
strictly less than its expected value. Since by deﬁnition Z (cid:2)
Zt then, regardless the our initial knowledge on the belief
state, the monitoring will be accurate and results in error less
(cid:2)
than

(cid:6)


α .

Error Accumulation Analysis
In this subsection we present a series of lemmas which prove
Proposition 3.4. The lemmas bound the difference between
the updates in the approximate and true model.

We start by proving the Lemma 3.8 provided at the begin-

ning of the Subsection
Lemma 3.6 For every belief states b1 and b2,

KL(T b1||(cid:3)T b2) ≤ KL(b1||b2) + T

Proof: Let us deﬁne the joint distributions p1(s(cid:2), s) =
P (s, s(cid:2))b1(s) and p2(s(cid:2), s) = ˆP (s, s(cid:2))b2(s). Throughout the
proof we speciﬁcally denote s to be the (random) ’ﬁrst’ state,
and s(cid:2)
to be the (random) ’next’ state. By the chain rule for
relative entropy, we can write:

KL(p1||p2) = KL(b1||b2) + Es∼b[KL(T (·|s)||(cid:3)T (·|s)]

≤ KL(b1||b2) + T

where the last line follows by Assumption 3.1.

Let p1(s|s(cid:2)) and p2(s|s(cid:2)) denote the distributions of the
ﬁrst state given the next state, under p1 and p2 respectively.
Again, by the chain rule of conditional probabilities we have,

KL(p1||p2) = KL(T b1||(cid:3)Tab2)
≥ KL(T b1||(cid:3)T b2),

+Es(cid:2)∼T b[KL(p1(s|s(cid:2)

)||p2(s|s(cid:2)

)]

where the last line follows from the positivity of the relative
entropy. Putting these two results together leads to the claim.
(cid:2)

The next lemma bounds the effect of mixing with uniform

distribution.
Lemma 3.7 For every belief states b1 and b2
KL(T b1|| ˜T b2) ≤ (1 − U )KL(T b1|| ˆT b2) + U log N

Proof: By convexity,

KL(T b1|| ˜T b2) ≤ (1 − U )KL(T b1|| ˆT b2)
+U KL(T b1||Uni(·))
≤ (1 − U )KL(T b1|| ˆT b2)

+U log N,

where the last line uses the fact that the relative entropy be-
tween any distribution and the uniform one is bounded by
log N .
(cid:2)

Combining these two lemmas we obtain the following

lemma on the transition model.

IJCAI-07

2477

Lemma 3.8 For every belief states b1 and b2,

KL(T b1|| ˜T b2) ≤ KL(b1||b2) + T + U log N

After dealing with transition model, we are left to deal with
the observation model. We provide an analog lemma with
regards to the observation model.
Lemma 3.9 For every belief states b1 and b2,

(cid:5)
(cid:4)
KL(Uob1||(cid:3)Uob2)

Eo∼O(·|b1)

≤ KL(b1||b2) + O − KL(O(·|b1)||(cid:3)O(·|b2))

Proof: First let us ﬁx an observation o. We have:

KL(Uob1||(cid:3)Uob2) =

s

(cid:2)
(cid:2)
(cid:2)
(cid:2)

s

s

+

s

Uob1(s) log

(Uob1)(s) log

(Uob1)(s) log

(Uob1)(s) log

Uob1(s)

O(o|s)b1(s)/O(o|b1)

(cid:3)Uob2(s)
(cid:3)O(o|s)b2(s)/(cid:3)O(o|b2)
(cid:3)O(o|b2)
(cid:3)O(o|s)
(cid:3)O(o|b2) are

b1(s)
b2(s)
O(o|s)

O(o|b1)

− log

=

=

where the last line uses the fact that O(o|b1) and
constants (with respect to s).

Now let us take expectations. For the ﬁrst term, we have:

(cid:17)

Eo∼b1

(cid:16)(cid:2)
(cid:2)
(cid:2)

o

s

=

=

s,o

Eo∼b1

O(o|b1)

(Uob1)(s) log

O(o|s)b1(s) log

s

(cid:17)

(cid:16)(cid:2)

b1(s) log

b1(s)
b2(s)
O(o|s)
b1(s)
O(o|b1)
b2(s)
= KL(b1||b2)
b1(s)
(cid:18)
b2(s)
o O(o|s) = 1. Simi-
(cid:17)
(cid:3)O(o|s)
O(o|s)b1(s)
(cid:4)
(cid:5)
O(o|b1)
KL(O(·|s)||(cid:3)O(·|s))
O(o|b1)
(cid:3)O(o|b2)

= KL(O(·|b1)||(cid:3)O(·|b2))

(cid:3)O(o|s)

(Uob1)(s) log

O(o|b1)

O(o|s)

O(o|s)

(cid:2)

(cid:17)

log

s

(cid:16)(cid:2)
(cid:2)

s

=

o

= Es∼b

(cid:16)
− log

For the second term,

Eo∼b1

where the last step uses the fact that
larly, for the third term, it straightforward to show that:

directly from the deﬁnition of the relative entropy. The lemma
(cid:2)
follows from Assumption 3.1.

Now we are ready to prove Proposition 3.4.

Proof of Proposition 3.4: Using the deﬁnitions of updates

and the previous lemmas:

Eot∼bt

(cid:5)
(cid:4)
bt|| ˜T(cid:3)Uot
KL(bt+1||ˆbt+1)
(cid:5)
bt||(cid:3)Uot
KL(T Uot

(cid:4)
(cid:4)

ˆbt)

= Eot∼bt
≤ Eot∼bt

(cid:5)

ˆbt)

≤ KL(bt||ˆbt) + O − KL(O(·|bt)||(cid:3)O(·|ˆbt))

KL(Uot

+ T + U log N

+T + U log N

where the ﬁrst inequality is by Lemma 3.8 and the second is
by Lemma 3.9 This completes the proof of Proposition 3.4.

(cid:2)

Value of Observation Proposition - The Analysis
The following technical lemma is useful in the proof and it
relates the L1 norm to the KL divergence.
Lemma 3.10 Assume that ˆb(s) > μ for all s and that μ < 1
2 .
Then

Proof: Let A be the set of states where b is greater than ˆb,

i.e. A = {s|b(s) ≥ ˆb(s)}. So
KL(b||ˆb) =

b(s) log

1
μ

KL(b||ˆb) ≤ (cid:4)b − ˆb(cid:4)1 log
(cid:2)
(cid:2)
(cid:2)
(b(s) − ˆb(s)) log
(cid:2)
(cid:9)
≤ log
(cid:2)

s∈A
b(s)
ˆb(s)
(b(s) − ˆb(s))

b(s)
ˆb(s)

≤

1
μ

s∈A

s∈A

=

s

+

ˆb(s) log

1 +

(cid:2)

s∈A

s∈A
1
μ

(cid:19)
≤ log

=

1
2

log

1
μ + 1

(cid:20)
(b(s) − ˆb(s)) +

(cid:4)b − ˆb(cid:4)1

s∈A

b(s) − ˆb(s)
(cid:2)

ˆb(s)

b(s) log

(cid:2)

+

s∈A

b(s)
ˆb(s)

ˆb(s) log

b(s)
ˆb(s)

(cid:10)

(b(s) − ˆb(s))

s∈A(b(s)− ˆb(s)) = 1

(cid:4)b− ˆb(cid:4)1. The claim now follows

where we have used the concavity of the log function and
that
using the fact that μ < 1
2 .

(cid:2)
We are now ready to complete the proof. We will make
use of Pinsker’s inequality, which relates the KL divergence
to the L1 norm. It states that for any two distributions p and q

2

Proof of Proposition 3.5: By Pinsker’s inequality and As-

sumption 3.1, we have

((cid:4)p − q(cid:4)1)2 .

KL(p||q) ≥ 1
2
(cid:8)
2KL(O(·|s)||(cid:3)O(·|s)) ≤ √

(2)

(cid:4)O(·|s) − O(·|s)(cid:4)1 ≤
for all states s ∈ S. Using the triangle inequality,

(cid:4)O(·|b)−O(·|ˆb)(cid:4)1 ≤ (cid:4)O(·|b)−(cid:3)O(·|ˆb)(cid:4)1+(cid:4)(cid:3)O(·|ˆb)−O(·|ˆb)(cid:4)1 .

2O

(cid:18)

IJCAI-07

2478

Therefore,

(cid:4)O(·|b) − (cid:3)O(·|ˆb)(cid:4)1 ≥ (cid:4)O(·|b) − O(·|ˆb)(cid:4)1
−(cid:4)(cid:3)O(·|ˆb) − O(·|ˆb)(cid:4)1
≥ γ(cid:4)b − ˆb(cid:4)1 − (cid:4)(cid:3)O(·|ˆb) − O(·|ˆb)(cid:4)1
≥ γ(cid:4)b − ˆb(cid:4)1 − √
where we used (cid:4)(cid:3)O(·|ˆb) − O(·|ˆb)(cid:4)1 ≤ maxs (cid:4)O(·|s) −
O(·|s)(cid:4)1 and Pinsker’s inequality to bound (cid:4)(cid:3)O(·|ˆb) −
KL(O(·|b)||(cid:3)O(·|ˆb)) ≥ 1

(cid:8)
2KL((cid:3)O(·|ˆb)||O(·|ˆb)) .

Combing with Pinsker’s inequality, we have

O(·|ˆb)(cid:4)1 ≤

2O

√
2O

2O)2

(|O(·|b) − (cid:3)O(·|ˆb)|1)2
(γ(cid:4)b − ˆb(cid:4)1 − √
2
≥ 1
2
(γ(cid:4)b − ˆb(cid:4)1)2 − γ(cid:4)b − ˆb(cid:4)1
≥ 1
2
+O
(cid:10)
(cid:9)
(γ(cid:4)b − ˆb(cid:4)1)2 − 2γ
2 − 3γ

γKL(b||ˆb)

≥ 1
2
≥ 1
2

log 1
μ

√
2O + O
√
O + O

where the before last inequality follows from (cid:4)b − ˆb(cid:4)1 ≤ 2

and the last inequality from Lemma 3.10.

(cid:2)

4 Extension to DBNs

In many application one of the main limitations is the rep-
resentation of the state which can grow exponentially in
the number of state variables. Dynamic Bayesian Networks
[Dean & Kanazawa, 1989] have compact representation of
the environment. However, the compact representation of the
network does not guarantee that one can represent compactly
the belief state.

The factored representation can be thought as having an

update of the following form

˜bt+1 = H ˆT ˆUo˜bt,

(3)

where H projects the belief state into the closest point in the
factored representation space. Next we adopt the following
deﬁnition from [Boyen & Koller, 1998] regarding the quality
of the factored representation.

Deﬁnition 4.1 An approximation ˜b of ˆb, incurs error  if rel-
ative to true belief state b we have

KL(b||ˆb)) − KL(b||˜b) ≤ p

Armed with these deﬁnitions one can prove an equivalent

Theorem to Theorem 3.2 with respect to KL(˜bt||bt). Due

to lack of space and the similarity to the previous results we
omit them.

5 Conclusions and Open Problems

In this paper we presented a new parameter in HMM, which
governs how much information the observation convey. We
showed how one can do fairly good monitoring in absence of
an accurate model/unknown starting state/compcat represen-
tation as long as the HMM’ observations are valuable. An
open question that remains is whether the characterization
can be made weaker and still an agent would be able to track,
for instance if in most states the observations are valuable but
there are few in which they are not, can we still monitor the
state? Another very important research direction is that of
planning in POMDPs. Our results show that one can monitor
the belief state of the agent, however this is not enough for
the more challenging problem of planning, where one should
also decide which actions to take. It is not clear whether our
characterization can yield approximate planning as well. The
major problem with planning is that of taking the best ac-
tion w.r.t to a distribution over states can lead to disastrous
state and one should look into the long term implications of
her actions due to the uncertainty. We leave these interesting
problems to future work.

References

Boyen, X., & Koller, D. (1998). Tractable inference for com-
plex stochastic processes. Proceedings of the Fourteenth
Conference on Uncertainty in Artiﬁcial Intelligence(UAI)
(pp. 33–42).

Dean, T., & Kanazawa, K. (1989). A model for reasoning
about persistence and causation. Computational Intelli-
gence, 5(3), 142–150.

Doucet, A. (1998). On sequential simulation-based meth-
ods for bayesian ﬁltering (Technical Report CUED/F-
INFENG/TR. 310). Cambridge University Department of
Engineering.

Kalman, R. (1960). A new approach to linear ﬁltering and
prediction problems. Transactions of the ASME - Journal
of Basic Engineering, 82, 35–45.

Kearns, M., Mansour, Y., & Ng, A. (2002). A sparse sam-
pling algorithm for near-optimal planning in large markov
decision. Machine Learning, 49(2-3), 193–208.

McAllester, D., & Singh, S. (1999). Approximate planning
for factored pomdps using belief state simpliﬁcation.
In
Proceedings of the Fifteenth Annual Conference on Uncer-
tainty in Artiﬁcial Intelligence (UAI) (pp. 409–416).

Moral, P. D. (2004). Feynman-kac formula. genealogical and

interacting particle systems. Springer.

Ng, B., Peshkin, L., & Pfeffer, A. (2002). Factored parti-
cles for scalable monitoring. Proceedings of the Eighteenth
Conf. on Uncertainty in Artiﬁcial Intelligence (UAI).

Puterman, M. (1994). Markov decision processes. Wiley-

Interscience.

Rabiner, L. R., & Juang, B. H. (1986). An introduction to

hidden Markov models. IEEE ASSP Magazine, 4–15.

IJCAI-07

2479


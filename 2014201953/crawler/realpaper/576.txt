Covariant Policy Search 

J. Andrew Bagnell and Jeff Schneider 

Robotics Institute 

Carnegie-Mellon University 

Pittsburgh, PA 15213 

{ dbagnell, schneide } @ ri. emu. edu 

Abstract 

We investigate the problem of non-covariant behav(cid:173)
ior of policy gradient reinforcement learning algo(cid:173)
rithms.  The policy gradient approach is amenable 
to analysis by information geometric methods. This 
leads us to propose a natural metric on controller 
parameterization that results from considering the 
manifold of probability distributions over paths in(cid:173)
duced  by  a  stochastic  controller. 
Investigation 
of this approach leads to a covariant gradient as(cid:173)
cent  rule. 
Interesting  properties  of this  rule  are 
discussed,  including  its  relation  with  actor-critic 
style reinforcement learning algorithms.  The al(cid:173)
gorithms discussed here are computationally quite 
efficient  and  on  some  interesting  problems  lead 
to  dramatic  performance  improvement over non-
covariant rules. 

Introduction 

1 
Much recent work in reinforcement learning and stochastic 
optimal control has focused on algorithms that search directly 
through a space of policies rather than building approximate 
value functions. Policy search has numerous advantages: do(cid:173)
main knowledge may be easily encoded in a policy, the policy 
may require less representational power than a value-function 
approximation, there are simple extensions to the multi-agent 
domain, and convergent algorithms are known. Furthermore, 
policy search approaches have recently scored some encour(cid:173)
aging successes [Bagnell and Schneider, 2001 ] [Baxter et al, 
19991 including helicopter control and game-playing. 

One  interesting  aspect  of existing  gradient  search  algo(cid:173)
rithms, however, is that they are non-covariant; that is,  a 
simple re-parameterization of the policy typically leads to a 
different gradient direction.  (Not that found by applying the 
Jacobian  of the  re-parameterization to the  gradient).  This 
is a odd result;  it is intuitively difficult to justify the gradi(cid:173)
ent computation as actually indicating the direction of steep(cid:173)
est descent.  This problem is well recognized in the pattern-
recognition and statistics communities and is a very active 
area of research.  [Kakade,  2002]  was,  to the best of our 
knowledge the first to identify this problem in reinforcement 
learning and to suggest that techniques from information ge(cid:173)
ometry may prove valuable in its solution. 

Inspired  by  the  work  of  [Amari  and  Nagaoka,  2000], 
Kakade proposed a "natural gradient" algorithm. In particu(cid:173)
lar, [Kakade, 2002] proposed a scheme for generating a met(cid:173)
ric on parameter space that has interesting theoretical prop(cid:173)
erties.  Most  convincingly,  Kakade  showed  strong empiri(cid:173)
cal  evidence  for the  algorithm's  usefulness. 
In  particular, 
[Kakade, 2002] applies the algorithm to the Tetris problem 
of [Bertsekas and Tsitsiklis,  1996].  This problem is partic(cid:173)
ularly interesting as value function methods (as for example 
described in [Bertsekas and Tsitsiklis,  1996] ) demonstrate 
non-monotone performance;  first  policy  iteration  improves 
the policy dramatically, but after a small number of iterations 
the policy gets much worse.  Normal gradient methods, in(cid:173)
cluding second-order and conjugate methods also prove very 
ineffective on this problem; even after a tremendous number 
of rounds they only mildly increase the performance of the 
game-player. The method presented in [Kakade, 2002] shows 
rapid performance improvement and achieves a significantly 
better policy than value-function methods (at their peak) in 
comparable time. 

However, despite recognizing an interesting defect in the 
general approach and intuiting an apparently powerful algo(cid:173)
rithm, Kakade concludes that his method also fails to be co-
variant leaving the problem open.  We present here what we 
believe to be an appropriate solution. 

In Kakade's work, there is no proper probability manifold, 
but rather a collection of such (one for each state) based on 
the policy. As such, Kakade must rely on an ad-hoc method 
for generating a metric. Here instead we take a proper mani(cid:173)
fold, the distribution over paths induced by the controller, and 
compute a metric based on that. In the special case appropri(cid:173)
ate to the average reward RL formulation, Kakade's scheme 
is shown to give rise to a bona-fide natural metric, despite the 
observation in the paper that the learning was non-covariant. 
We believe this is an artifact- perhaps of step-length.  Fur(cid:173)
ther,  we  note  that  parametric  invariance does  not  require 
this metric- there are numerous covariant metrics.  Rather, 
a stronger probabilistically natural  invariance demands the 
metric used. We describe this result to motivate our method. 
Importantly,  the  notion  of  the  metric  on  the  path-
distribution allows us to provide very simple and natural ex(cid:173)
tensions to Kakade's algorithm that cover the finite horizon, 
discounted start-state, and partially-observed reinforcement 
learning problems as well as the average reward one.  Fi-

PROBABILISTIC  PLANNING 

1019 

with respect to 

measurable with respect to 
usually parameterized by 

nally, for completeness, we describe the result discovered by 
Kakade relating the invariant metric and compatible actor-
critic methods [Sutton et aL, 1999]. 
1.1  Problem Setup and Notation 
A stochastic control problem consists of paths 
system trajectories) in a space 
space, 

(also called 
a distribution over path-
that is a function of a sequence of (finite) controls 
indexed by time, from a space A. Throughout 
this paper we will be considering Partially Observed Markov 
Decision Problems, in which there are state-variables 
of 
the system that compose a path  and render the past and fu(cid:173)
ture independent. In a POMDP, 
is defined by an initial 
state and next state transition probabilities 
We also typically have a sequence of outputs (observations) 
. A controller (or 
policy), 
is a feedback-type 
strategy that maps the history of observations 
to 
a distribution over controls 
Most of the derivations will 
be given in terms of memoryless stochastic controllers that 
map the current state to a distribution over actions, although 
they can be easily extended to finite window or recurrent rep(cid:173)
resentations that operate on only the observations. The goal 
in a control problem is to maximize the expected reinforce(cid:173)
ment 
The sum is 
always assumed to exist for all controllers. The reward func(cid:173)
tion on paths in a POMDP is additive in time (or in the infi(cid:173)
nite time case, discounted or averaged), and a function R 
of state, although the algorithms discussed here are not neces(cid:173)
sarily predicated on that assumption. Reinforcement learning 
is the adaptive version of the control problem where we at-
tept to maximize the expected reinforcment by sampling tra(cid:173)
jectories and then improving our policy. We use the notation 
throughout to denote  where the parameter  should be 
clear from context. For vectors  and  we use the notation 
to denote the inner product with respect to metric 
indicates the expected value of 

subject to 

M.  The notation 
the function / with respect to the distribution 
2  Covariance and Riemannian manifolds 
2.1  Meaning of steepest ascent 
For direct policy search methods, we are interested in finding 
the direction of steepest ascent with respect to our current pa(cid:173)
rameters. That is, we wish to maximize the reward function 
an infinitesimal. If we 
reparameterize our controller in terms of, e.g. 
and express 
the  same effective  infinitesimal  policy change  in  terms of 
these  parameters,  will of course remain the same. How(cid:173)
ever, if we measure lengths using the naive dot product, the 
same effective change will not have the same size.  It is this 
problem that leads to the odd behavior of "steepest descent" 
rules. 
2.2  Non-covariant learning rule 
To demonstrate the problem we first consider a simple two 
state, two action MDP described in [Kakade02j.  (Figure 1) 
In state 0 executing action 0 returns to the same state and gets 
a reward of 1. Executing action 1 get no reward but transits to 

Figure 1: A two-state MDP. Each state has two controls one which 
self transitions (and earns the reward that labels the state) and an(cid:173)
other that simply transitions to the other state. Rewards occur only 
on the transitions between different states. 

Figure 2: Log odds of actions for policies logp 
1  on the two-state MDP (horizontal axis corresponds to state 0). 
Different curves correspond to varying  Notice the non-covariant 
policy behavior. 

state 1. In state 1, action 0 returns to state 1 and gets reward 
2. Action 1 transits to state 0 and achieves no reward. 

Consider parameterized probabilistic policies of the form 
is an arbitrary scale 
parameter that we use to demonstrate that even mildly differ(cid:173)
ent parameterization lead to dramatically different behavior. 
Below we plot the resulting track through the space of poli(cid:173)
cies using the log ratio of probabilities of the actions for each 
of the possible states.  We start from a policy that makes it 
somewhat more likely to choose action 0 in state 0 and action 
1 in state 1. We then scale 
to 1, .5, and 2 and plot the log 
odds of the policy from state 1 (Figure 2). 

The non-covariant behavior is  quite clear in  this graph. 
Further, we note that it is very difficult to achieve a good 
policy using this algorithm from this starting point as the 
wrong action becomes overwhelmingly likely to be chosen 
from state 0. If sampling were used to compute the gradient, 
we would nearly never get samples from state 1- which we 
need to improve the policy. 
2.3  Path distribution manifolds 
The control problem is essentially a coupled one of optimiza(cid:173)
tion and integration over the space of possible paths, 
This 
motivates the idea that instead of considering the (arbitrary) 
distance in terms of parameterization of the policy, we may 
consider distance in terms of the change in distribution over 
paths resulting from the policy change.  We may view the 
as a parameterized manifold 
distribution over paths 
(nominally embedded in 
of dimension 
This takes 
some work to visualize; as an example consider a space of 
three possible paths.  All  possible distributions over these 
three paths can be smoothly represented with two parameters. 
In Figure 3 (left) we see one visualization with the embedding 

1020 

PROBABILISTIC PLANNING 

Figure 3: An example probability manifold over paths. Each 
axis represents the log probability of one of the three possible 
paths.  The manifold is formed as we vary the parameters 
defining it throughout their domain.  On the right we attach 
the tangent space at a particular point in parameter space. 

With  parameters, assuming no redundancy, we generate 
an  dimensional manifold. In the case pictured, it is the set 
of all distributions on 3 paths, but in general, the dimension(cid:173)
ality of the path probability manifold will  be tremendously 
less than the number of possible paths. It is important to un(cid:173)
derstand the manifold under consideration is that of the prob(cid:173)
ability distribution over paths and not of paths themselves. 

The  study  of parameterized  manifolds  like  that  pictured 
above is the domain of differential geometry.  Here we are 
interested in establishing a Riemannian structure on the man(cid:173)
ifold of paths. By that we mean we wish to establish a metric 
on the tangent space (the local  linear approximation to the 
so we can measure small parameter 
manifold at a point 
changes. We do this with the metric 
on the tan(cid:173)
gent space (which is spanned by the partials with respect to 
where G is a positive 
each parameter) as 
definite matrix.  This is a very natural thing to do- instead 
of just the standard dot product, we have a dot product that 
allows us to represent rotations and scalings (exactly what 
a positive definite matrix can represent) and that can vary 
throughout the manifold.  In Figure 3 (right) we depict the 
tangent space at a point in our parameterization as the local 
linear approximation at that point. 

2.4  Steepest ascent on Riemannian manifold 
There are  two questions that  then  naturally arise- what  is 
steepest descent on a function defined over a Riemannian 
space, and is there a Riemannian metric on the manifold of 
the paths that is in some sense natural.  We quickly answer 
the first question, and pursue the second in the next two sec(cid:173)
tions.  A Lagrange multiplier argument (given schematically 
here) makes it easy to see the form of the steepest descent 
direction. 

(1) 
(2) 

Form the Lagrangian: 

PROBABILISTIC  PLANNING 

and take derivatives with respect to each 

to solve for the optimal direction: 

(3) 
, then set to zero 

This implies that (since G is positive definite and hence 

invertible), giving us: 

(4) 
That is, the direction of steepest descent is simply the nor(cid:173)
mal gradient times the inverse of the metric evaluated at the 
point of tangency. We call this the "natural gradient" for the 
Reimannian manifold. [Amari and Nagaoka, 20001 
3 
Invariance and Chentsov's Theorem 
Some confusion seems to surround the choice of metric on 
probability  manifolds.  There is 
unique answer for this 
metric, even under the supposition of parametric invariance 
as suggested by some authors. The issue is rather more sub(cid:173)
tle.  Any metric that transforms under parameter change ac(cid:173)
cording to the Jacobian of the function connecting parameters 
meets this requirement. This type of parametric covariance is 
a minimum requirement for us. It is natural to suggest a met(cid:173)
ric that preserves essential probabilistic aspects of the mani(cid:173)
fold. Consider, for example, functions q (Markov mappings, 
congruent cmbeddings, or sufficient statistics, depending on 
your viewpoint) that carry one distribution over paths to an(cid:173)
other in a natural and recoverable way.  For example, con(cid:173)
sider the mapping between the two distributions P(paths) and 
P(paths')given by congruent embedding q depicted below: 

Paths' 

The mapping q above interchanges the role of two paths 
and splits one path into two (with probability  1/2 for each 
split). 
In  a  probabilistically  fundamental  way,  the  mani(cid:173)
folds  P(path)  and  P(path')  (where we  imagine P(path)  is 
a  smoothly  parameterized  set  of distributions)  are  similar 
to each other.  For each path' we can uniquely recover the 
original probability.  In this way, a parameterized distribu(cid:173)
tion over paths can be embedded into a different path space. 
(This equivalence can actually be phrased in a category the(cid:173)
oretic way where the morphisms are these congruent embed-
dings.) iChentsov, 1980] General congruent embeddings can 

1021 

be thought of as simply generalizations of the example de(cid:173)
picted  above to  allow  arbitrary  permutations and  arbitrary 
probabilities for different paths stemming from a single path, 
as well as compositions of these two. In the control problem 
this could arise by simple permutation of the state variables 
or change of co-ordinates to ones with redundant information. 
It is natural to require that with our metric the congruent em(cid:173)
bedding is an isometry on the tangent space,  (i.e.  preserves 
the length of the vectors).  That is, if we make a change of 
size  (under the metric 
(paths)then 
carrying that change through  we should measure the same 
change c using 
. Adding the requirement of invariance 
with respect to congruent embeddings leads to a unique (up to 
scale) metric on the manifold.  [Chentsov, 1980] This metric 
is well-known in statistical inference as the Fisher-Rao metric 
and it can be written as the Fisher information matrix [DeG-
root, 19701: 

to the distribution 

-  Compute zj  = i—^z +  jdjlog7r(a\x;9) 

Return G 

We use the Markov property and the invariance of the tran(cid:173)
sition probabilities (given the actions) in the algorithm above 
to compute  9jogp(£;0)  simply.  These details can be ex(cid:173)
tracted from the proofs below, as well as other, potentially 
better ways to sample. 

To compute the natural gradient, we simply invert this ma(cid:173)
trix and multiply it with the gradient computed using standard 
policy search methods. 

4.2  Limiting metric for infinite horizon problems 
A well-known result in statistics [DeGroot, 1970] gives a dif(cid:173)
ferent form for the Fisher metric under appropriate regularity 
conditions. We quickly derive that result, and then show how 
this gives us a simple form for the path probability metric as 

Another way to derive the  metric is to think about "dis(cid:173)
tances" on probability spaces.  The KL-divergencc (or rel(cid:173)
ative entropy) between two distributions is a natural diver(cid:173)
gence on changes in a distribution.  It is also manifestly in(cid:173)
variant to re-parameterization.  If we think about derivatives 
as small changes in parameters (differentials) then we dis(cid:173)
cover that we also get a unique metric as the second-order 
Taylor expansion of the KL-divergence. This too agrees with 
the Fisher information (up to scale).  We note that the di(cid:173)
rection of the KL-divergence is irrelevant as to second-order 

fAmari and Nagaoka, 2000]. 
4  Fisher-Rao Metric on the Path-space 

Manifold 

information  metric 

The issue now becomes how to derive the Fisher metric on 
the space of path distributions.  It turns out in the case of 
processes with underlying Markovian state to be rather easy, 
and involves only computations we already make in the likeli(cid:173)
hood ratio approach standard in gradient-based reinforcement 
learning. 
4.1  Derivation of finite-time path metric 
The  Fisher 

involves  computing 
.  Fortunately, this is easy to 
do.  The essential  algorithm within  gradient methods  like 
REINFORCE and  GPOMDP is a clever method of computing 
the expected score function. Thus, while 
the  expected  score  is  the  gradient,  the  correlation  of the 
score is the Fisher matrix.  The following simple algorithm 
is  unbiased  and  converges almost  surely  (under the  same 
regularity conditions as in [Baxter et al, 1999] as the number 
of sample paths 
Algorithm 1 (Finite-horizon metric computation)  For i in 
1 to m: 

goes to infinity: 

(5) 
The third line follows from the second by integrating by 
parts and the fifth follows by observing that the total proba(cid:173)
bility is constant. 

Now we show that in the limit this leads to a simple metric 
for the infinite horizon case. To get a convergent metric, we 
must normalize the metric, in particular by the total length 
of the path denoted t.  Since the metric is defined only up to 
scale, this is perfectly justified. 
Theorem 1 (Infinite-Horizon Metric)  For  an  ergodic 
Markov process the Fisher information matrix limits to the 
expected Fisher information of the policy for each state and 
control under stationary distribution of states and actions. 
Proof:  We  use  to indicate the t-step finite-horizon metric. 

For a Markov process we can write the likelihood ratio 

(6) 

1022 

PROBABILISTIC PLANNING 

Using the chain rule we continue the derivation with 
indicating the stationary distribution: 

where the second equality follows by noting that the like(cid:173)
lihood ratio is independent of transition probability matrix-
given the action probability, and by application of the ergodic 
theorem. The last line follows (with i the Fisher informa(cid:173)
tion for the policy at a given state) as the second term in line 
3 vanishes since the total probability is constant with respect 
to 

It is also interesting to consider what happens in the start-
state, discounted formulation of the problem. In this case we 
would like our metric, using the general definition over path 
distributions given above, to naturally weigh the start start 
more than it necessarily is in the infinite horizon average case. 
It is well-known that a discount factor 
is equivalent to an 
undiscounted problem where each trajectory terminates with 
probability 1  -  at each step. We can use this fact to derive 
a metric more appropriate for the discounted formalism. 
Theorem 2 (Start-State Metric) For a discounted Markov 
process the Fisher information matrix equals the Fisher in(cid:173)
formation of the policy for each state and control under the 
limiting  distribution  of  states  and  actions.  Proof: The 
proof is very similiar to the infinite horizon case and so we 
simply sketch it: 

where 

is the limiting distri(cid:173)
bution of states. Thus the infinite horizon (undiscounted) and 
start-state metrics give essentially the same metric, with only 
the effective weighting by states differing. 
4.3  Metrics for Partially Observed Problems 
For policies that map the observation space  of a  partially-
observed markov decision process into distributions over ac(cid:173)
tions, it is just as easy to derive appropriate metric using our 
approach. The tuple 
is also a markov chain, and 
with only subtle changes to the arguments above we end up 
with the same metric except using the limiting distribution of 
observations instead of states. 
5  Relation to Compatible Value Function 

Actor Critic 

Kakade noticed a fascinating connection between the limit(cid:173)
ing metric given in Theorem 1 and the improvement direction 
computed by a class of actor-critic methods that use a special 
compatible function approximation technique.[Sutton et al., 
1999; Konda and Tsitsiklis, 20021 Following Kakade we let 
and let the compatible function ap(cid:173)

proximator be linear in 

This type of value function approximator was initially sug(cid:173)
gested as it may be used to compute the true gradient. In prac(cid:173)
tice, it has not been clear what advantage this brings over the 
gradient estimation routines of [Baxter et al,  1999].  How(cid:173)
ever, "folk-wisdom" has it that performing an that infinitesi(cid:173)
mal policy iteration (moving in the direction of the best policy 
according to 
using this approximation has very 
good properties and often significantly outperforms standard 
gradient methods.  The natural gradient provides insight into 
this behavior. Let  minimize the squared value-function er(cid:173)
ror: 

1 

is the exact advantage function.  [Sutton et 
where 
al,  1999J  It  is  easy to check that  ([Kakade,  2002],  Theo(cid:173)
rem 
That is, the direction of maximum 
policy improvement is exactly the natural gradient direction. 
This can be seen by simply differentiating 
to minimize 
and  noting  the  result of [Sutton  et al, 
it with respect to 
1999] that 

6  Demonstration 
As a consequence of the results of section 5  and iKakade, 
2002], experimental results already exist demonstrating the 
effectiveness of the natural gradient method.  That is, all re(cid:173)
sults using policy improvement with compatible function ap(cid:173)
proximators are implicitly computing this result. As a demon(cid:173)
stration, we computed analytically the natural gradient for the 

PROBABILISTIC  PLANNING 

1023 

Figure 4:  Performance comparison of covariant (nat and scaled) 
and non-covariant (grad) on the 2-state MDP. The covariant learning 
algorithm dramatically outperforms the standard gradient algorithm. 
The standard method achieves J=2 after more than 1000 iterations. 

ing.  Unfortunately, working in the space of policies, it was 
difficult to generate such an algorithm.  Here we proposed 
considering the induced path-distribution manifold and used 
notions from information geometry to propose a natural co-
variant algorithm. This leads to interesting insight and a prac(cid:173)
tical algorithm. Fortunately, it agrees with the heuristic sug(cid:173)
gested by Kakade (despite the suggestion in the paper that 
the algorithm there was actually not covariant) in the infinite 
horizon case and extends to cover new problems.  [Peters et 
ai, 2002] independently developed theory related to ours (in 
particular the theorems in Section 4.2) and presented results 
in the context of robot dynamic control. 

Further work will provide more investigation into the ex(cid:173)
perimental behavior of the algorithms presented.  Future ef(cid:173)
fort may also yield deeper insight into the relationship be(cid:173)
tween the method presented here and value-function approx(cid:173)
imations. 
References 
[Amari and Nagaoka, 2000J  S.  Amari 
Methods of Information Geometry. 
Press, 2000. 

and  H.  Nagaoka. 
Oxford  University 

[Bagnell and Schneider, 2001]  J. Bagnell and J. Schneider. 
Autonomous helicopter control by policy-search based re(cid:173)
inforcement learning. In Proceedings of the 2001 IEEE 
Int. Conference on Robotics and Automation. IEEE, 2001. 

[Baxter etal, 1999]  J.  Baxter,  L.  Weaver,  and  P.  Bartlett. 
Direct-gradient-based reinforcement learning I:  Gradient 
estimation algorithms.  Technical report, Computer Sci(cid:173)
ences Lab, Australian National University, 1999. 

[Bertsekas and Tsitsiklis, 1996]  D. Bertsekas and J. Tsitsik-
lis.  Neuro-Dynamic Programming.  Athena Scientific, 
1996. 

[Chentsov, 1980] N.N. Chentsov. Statistical Decision Rules 
and Optimal Inference. American Mathematical Society, 
1980. 

LDeGroot, 1970] M. DeGroot. Optimal Statistical Decisions. 

McGraw-Hill, 1970. 

[Kakade, 2002]  S. Kakade. A natural policy gradient. In Ad(cid:173)

vances in Neural Information Processing Systems (NIPS-
14), 2002. 

[Konda and Tsitsiklis, 2002]  V.  Konda  and  J.  Tsitsiklis. 
Actor-critic algorithms, to appear in the SI AM Journal 
on Control and Optimization, 2002. 

[Peters etal, 2002]  Jan Peters, Sethu Vijaykumar, and Ste(cid:173)
fan  Schaal.  Policy gradient methods for robot control. 
Technical Report 00-737, USC, 2002. 

[Sutton et ai, 1999]  R. Sutton, D. McAllester, S. Singh, and 
Y.  Mansour.  Policy gradient methods for reinforcement 
learning with function approximation. In Neural Informa(cid:173)
tion Processing Systems 12, 1999. 

Figure 5: Path through policy space (showing log-odds of the ac(cid:173)
tions with state 0 along the horizontal axis) of covariant (nat and 
scaled) and non-covariant (grad) algorithms on the 2-state MDP. 
Note that, as required, the path is independent of scaling for the 
covariant algorithm. 

problem given in Section 2.2. This problem is interesting as it 
reveals some of the properties of the natural gradient method. 
First it is easily checked that for a complete Boltzmann policy 
in an MDP the natural gradient computes: 

This means that it is very similiar to the standard gradient 
except that it removes the weighting of states by the stationary 
distribution. Rather, each state is treated equally. This leads 
to much more reasonable results in the problem discussed as 
the partial derivative component for state 1 does not shrink as 
the policy changes initially. 

In Figure 4 we plot the performance of the natural gradi(cid:173)
ent (using two different scalings) and the standard gradient 
methods in optimizing the policy for this problem. 

It is interesting to note that in this graph the behavior of 
the natural gradient descent algorithm appears to be non-
covariant.  This is simply due to the step size heuristic not 
computing equivalent steps in the policy space. The direction 
however is  constant as illustrated in Figure 5. 
7  Conclusions 
[Kakade, 2002] suggested that covariant behavior of gradient 
algorithms is an important desiderata in reinforcement learn-

1024 

PROBABILISTIC  PLANNING 


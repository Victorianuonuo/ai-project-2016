Using Ontologies and the Web to Learn Lexical Semantics

Aarti Gupta

Amazon.com Inc.

1200 12th Avenue South

Seattle, WA 98144
aartig@amazon.com

Tim Oates

University of Maryland Baltimore County

1000 Hilltop Circle

Baltimore, MD 21784
oates@cs.umbc.edu

Abstract

A variety of text processing tasks require or bene-
ﬁt from semantic resources such as ontologies and
lexicons. Creating these resources manually is te-
dious, time consuming, and prone to error. We
present a new algorithm for using the web to de-
termine the correct concept in an existing ontology
to lexicalize previously unknown words, such as
might be discovered while processing texts. A de-
tailed empirical comparison of our algorithm with
two existing algorithms (Cilibrasi & Vitanyi 2004,
Maedche et al. 2002) is described, leading to in-
sights into the sources of the algorithms’ strengths
and weaknesses.

1 Introduction
Semantic resources, such as ontologies and lexicons, are
widely used in Natural Language Processing (NLP). Auto-
mated text processing and understanding, word sense dis-
ambiguation, information extraction, speech recognition, and
machine translation all require lexical resources. Ontologies
are used in information retrieval for automatic query expan-
sion, and the word sense disambiguation task can be made
easier by domain-speciﬁc ontologies that reﬂect the distribu-
tion of word senses in a particular domain. Outside the realm
of NLP, ontologies are widely used in the Semantic Web to
allow computer programs to “understand” web content.

Acquiring knowledge by hand is extremely costly and time
consuming. Construction of ontologies such as Word Net
(Fellbaum 1998), Cyc (Lenat 1995), and EDR (Yokoi 1995)
required enormous investments in both time and money. In
addition, hand-crafted lexical resources are prone to human
errors, biases, and inconsistencies in judgment.

We propose a novel method for learning lexical semantics.
The ontological concept that best captures the lexical seman-
tics of a new word is determined using statistical analysis of
web pages containing the word. All experiments reported in
this paper use the OntoSem ontology (Nirenburg & Raskin
2004), a large ontology and knowledge representation system
for language understanding tasks developed by the Institute
for Language and Information Technologies (ILIT). We per-
form a detailed comparison of the proposed method with two
existing methods, and in the process identify the individual

components that most signiﬁcantly impact performance on
the task of statistics-based extension of symbolic knowledge
for language processing tasks.

Lexical acquisition methods use different sources of data
for learning. Historically, a static corpus has been used for
training purposes (Widdows & Dorow 2002). Recently, the
web has become a popular alternative. We compare our al-
gorithm to that of Cilibrasi and Vitanyi (Cilibrasi & Vitanyi
2004), which uses the web to extract the meaning of words
and phrases.

Our work is related to work on learning ontologies. Lin
(Lin 1998) constructs an ontology from scratch by identify-
ing similar words in a parsed corpus. Agirre et al. (Agirre et
al. 2000) use the web to enrich existing concepts in ontolo-
gies. Each concept in WordNet (Fellbaum 1998) is associ-
ated with a topic signature (lists of topically related words).
To identify the sense of a new word, they build a binary hi-
erarchical clustering of all possible senses, compare the new
word’s context with the topic signatures for each branch and,
similar to a decision tree, choose the highest ranking branch
at each step. Maedche et al. (Maedche et al. 2002) deter-
mine the class of a new word by employing tree-descending
and tree-ascending classiﬁcation algorithms. Our algorithm
determines which concept in an existing ontology is the best
candidate for the meaning of a new word via tree descending,
and is compared to the algorithm of Maedche et al.

2 Methods

This section describes the proposed approach for learning lex-
ical semantics and summarizes two existing methods that are
used for comparison.

2.1 Proposed Approach

The OntoSem ontology consists of concepts and taxonomic
relations between concepts. To enrich existing concepts in the
ontology with new words, we need to ﬁnd in the hierarchy of
concepts the concept that best approximates the meaning of
a new word. This is accomplished using the notion of word
similarity, for which we use Latent Semantic Analysis (LSA)
(Deerwester et al. 1990).

To decide the most appropriate concept that a new word
should belong to, we need to determine the similarity of the
new word with words already lexicalized with the ontology.

IJCAI-07

1618

To do this, we obtain a training set T of words from the ontol-
ogy. For each word W , where W is in T or is the new word,
document W.txt is created by tokenising the list of deﬁnitions
retrieved after querying Google with “deﬁne:W ”. The result-
ing document collection is then passed as input to the LSA
algorithm. Using the World Wide Web to dynamically build
a document collection relevant to a given set of words allows
ad hoc word learning.

Words are stopped using Smart’s stop list, stemmed us-
ing Porter’s stemming algorithm, and ﬁltered based on the
logarithm of their term frequencies. A word i with term fre-
quency f reqi is included in the term-document matrix only if
1 + log (f reqi) ≥ t. Rather than represent raw term frequen-
cies of the remaining content words in the term-document
matrix, we use TFIDF.

We treat the OntoSem ontology similar to a decision tree.
Starting at the root, we attempt to descend the ontology tree
until we reach a leaf concept. At every tree node, the decision
of which path to follow is made by choosing the child concept
that is most similar to the new word. To compute similarity,
for each child of the ontology node we currently are at, we
randomly pick a representative set of words that belong to the
subtree rooted at the child.

We then obtain deﬁnitions for the words in the representa-
tive sets and the word we are trying to learn. For every word,
we create a new document that contains a list of deﬁnitions
for the word. So, at the end, we have a document collection
that contains a document for every word that we extracted
from the various ontological sub-trees and a document for the
new word. We then invoke the LSA algorithm which gives us
word and document vectors in the reduced space. We com-
pute cosine similarity scores for the word vectors. Finally, the
1-nearest neighbor classiﬁer assigns the new word to the child
concept that subsumes the word that was most similar to the
new word. The process is repeated until the search reaches a
leaf node.

2.2 Automatic Meaning Discovery using Google
This subsection describes a method proposed by Cilibrasi and
Vitanyi (Cilibrasi & Vitanyi 2004) to automatically learn the
meaning of words and phrases. Intuitively, the approach is
as follows. Words that are semantically similar will co-occur
more than words that are semantically unrelated. Conditional
probabilities can be used as a measure of the co-occurrence
of terms. p(x|y) gives the probability of term x accompany-
ing term y while p(y|x) gives the probability of term y ac-
companying term x. These two probabilities are asymmetric.
The semantic distance between terms x and y can be obtained
by taking the negative logarithm of conditional probabilities
p(x|y) and p(y|x), selecting the one that is maximum and
normalizing it by the maximum of log 1/p(x), log 1/p(y).

D(x, y) =

max{log

1

max{log 1

1

p(x|y) , log
p(x) , log 1

p(y|x) }
p(y) }

(1)

On the Web, probabilities of term co-occurrence can be ex-
pressed by Google page counts. The probability of term x
is given by the number of web pages returned when Google
is presented with search term x divided by the overall num-
ber of web pages M possibly returned by Google. The joint

probability p(x, y) is the number of web pages returned by
Google, containing both the search term x and the search
term y divided by the overall number of web pages returned
by Google. The conditional probability p(x|y) is deﬁned as
p(x, y)/p(y).

We attempt to use the Google method to extend the On-
toSem ontology. Once again, we use the tree descending al-
gorithm to assign a new word to a concept. For each child
of the ontology node we currently are at, we randomly pick a
representative set of words. We then compute the distance be-
tween the new word and every word extracted from the onto-
logical subtrees by submitting queries to Google via the Java
URL interface and scraping the page counts from the web
pages returned. The decision of which path to follow down
the tree is made by choosing the child concept that subsumes
a word which is closest to the new word by the distance mea-
sure.

2.3 Category Based Tree Descending Algorithm

Maedche et al. (Maedche et al. 2002) use a category-based
method to determine the child concept that is most similar to
a new word. For the ontology node that they are currently
at, they gather all hyponyms at most 3 levels below. Then
then build a generalized class vector for each child concept
by adding up all the vector representations of hyponyms gath-
ered that belong to the subtree rooted at the child and normal-
ize the resulting vector to unit length. Similarity between the
unit vector of a new word and the class vectors is measured
by means of three different similarity metrics: Jaccard’s coef-
ﬁcient, L1 distance, and skew divergence. The child concept
whose class vector is most similar to the new word’s vector is
chosen as the next node in the path down the ontology tree.

The utility of using the category-based method is that be-
sides reducing computational cost, it summarizes the charac-
teristics of each class by combining multiple prevalent fea-
tures together, even if these features are not simultaneously
present in a single vector.

We use the Maedche method to extend the OntoSem ontol-
ogy. At each ontology node, we gather all hyponyms at most
three levels below, build a term-document matrix for the rel-
evant document collection obtained from Google Deﬁne, ob-
tain the normalized class vectors and use the L1 similarity
metric to choose the child concept that is closest to the new
word.

3 Experiments

This section describes the experiments performed in the pro-
cess of identifying the components that work best for the
statistics-based reﬁnement of taxonomic relations.
It also
describes the experiments conducted to evaluate the perfor-
mance of the proposed LSA-based tree descending method
against the Google method and the category-based tree de-
scending method.

Since document W.txt is created by obtaining context for
word W , similarity between two words W1 and W2 can
be measured by computing similarity between documents
W1.txt and W2.txt.
In all experiments, we compute simi-
larity between document vectors and not word vectors be-

IJCAI-07

1619

cause we empirically found that using document vectors gave
marginally better results than word vectors.

For all experiments, we performed leave-one-out cross-
validation, that is, one word was held back as the test word
and the remaining words constituted the training set. Per-
formance was measured in terms of the number of words cor-
rectly classiﬁed. A word is correctly classiﬁed if it is assigned
to the correct child concept of an ontology node.

The value of t, that is, the threshold for ﬁltering out low
frequency words was ﬁxed at t=1.6, so words that occurred
fewer than two times in the corpus were ﬁltered out.

3.1

Impact of corpus on the learning task

We wanted to gauge the impact of the corpus on the LSA-
based tree descending algorithm. To do this, we randomly
picked at most ﬁfty words from each of the subtrees rooted at
the 18 child concepts of the SOCIAL-ROLE node in the On-
toSem ontology, giving a total of 269 words (after removing
polysemous words).

We used the Wall Street Journal corpus to obtain contexts.
For each word W , document W.txt was created by extract-
ing a window of width seven words centered on each of the
ﬁrst 40 occurences of W in the corpus. We then replaced
the WSJ document collection with the Google Deﬁne docu-
ment collection, while keeping everything else the same. For
each word W , document W.txt was created by submitting to
Google the query “deﬁne:W ”.

The results of using the Google Deﬁne corpus vs. the WSJ
corpus are shown in Figure 1. Accuracies for both corpora are
calculated as the number of words correctly classiﬁed over
the total number of words N that could possibly be classiﬁed
correctly. For the WSJ corpus, N =202 and for the Google
deﬁne corpus, N =254. The discrepancy between N and the
number of words originally extracted from the ontology is
due to the fact that not all words occur in the WSJ corpus or
have deﬁnitions on the Web, or if they do, they occur fewer
times than the threshold frequency.

It can be seen from Figure 5 that over all values of k,
the number of singular values retained when running LSA,
the system performs signiﬁcantly better when fed with the
Google Deﬁne corpus than when it is fed with the WSJ cor-
pus. The Google Deﬁne corpus has a best accuracy of 64.9%
for k=65 and k=85 while the WSJ corpus has a best accuracy
of 22.2% at k=65.

Thus, the Google Deﬁne corpus dramatically improves the
performance of the system because it contains fewer extra-
neous and irrelevant words that could potentially distract the
LSA algorithm.

3.2

Impact of Similarity metric

Maedche et al. make use of three different similarity met-
rics: Jaccard’s coefﬁcient, L1 distance, and skew divergence
in their category-based tree descending algorithm. We show
that the performance of the category-based tree descending
algorithm can be signiﬁcantly improved by using the cosine
similarity metric.

Initially, we implemented the category-based method using
the L1 similarity metric because Maedche et al. empirically

Figure 1: Performance of the LSA-based method using the
WSJ corpus and Google Deﬁne corpus

Node

Words Classiﬁed Correctly

SOCIAL-ROLE

MAMMAL

ANIMATE

OBJECT

2/254

1/347

103/208

17/282

Table 1: Results obtained for the category-based method us-
ing the L1 metric

show that it gives better results than Jaccard’s coefﬁcient and
skew divergence.

We carried out experiments at four nodes in the Ontology
tree. For each node, we created a relevant document col-
lection by picking all words at most three levels below the
node and Googling for the words’ deﬁnitions. We measured
similarity between the normalized category-based vector rep-
resentations obtained by summing individual document vec-
tors. Table 1 shows the results obtained. Each cell in the table
shows the number of words classiﬁed correctly out of the total
number of words that could possibly be classiﬁed correctly.

Analysis of the results shows that in almost all cases, a
new word was found to be similar to the child concept that in
comparison to the other child concepts subsumed the fewest
number of words. For example, at the SOCIAL-ROLE node,
a vast majority of words were found to be similar to the
CELEBRITY concept which had just one word under it,
while at the MAMMAL node, most words were found simi-
lar to the concept FRESHWATER-MAMMAL which had just
one word under it. This observation corroborates the ﬁndings
of Maedche et al.
in which they observed that most words
were assigned to concepts that subsumed few words.

Given two words and their length n vectors, x and y, the

L1 distance between them is given by:

n(cid:2)

Dist(x, y) =

i=0

|xi − yi|

From the deﬁnition, we can see that the distance between x
and y will be small if the difference between xi and yi is small
for all i. This means that, if x was a document vector for a

IJCAI-07

1620

Node

Words Classiﬁed Correctly

SOCIAL-ROLE

MAMMAL

ANIMATE

OBJECT

177/254

322/347

186/208

222/282

Table 2: Results obtained for the category-based method us-
ing the cosine similarity metric

new word and y was a generalized class document vector,
the distance between them would be small if two conditions
were satisﬁed. First, the documents for x and y contained
the same words and second, the documents for y as a whole
contained few other words. Child concepts that subsume few
words in the ontology tree are represented by few documents
which means fewer words in y as a whole and thus for such
concepts, by default, the second condition is satisﬁed.

We replaced the L1 metric with the cosine similarity met-
ric, and keeping everything else the same, carried out the ex-
periments at the four ontology nodes again. Table 2 shows
the results of using the cosine similarity metric.
It can be
seen that the cosine similarity metric does signiﬁcantly better
than the L1 metric at all four nodes.

3.3 Evaluation of the Three Methods

This section describes the experiments carried out to evaluate
the performance of the LSA-based tree descending algorithm,
the Google method and the category-based tree descending
algorithm.

We picked a path in the ontology from the root

to
a leaf node and carried out experiments at every node
on this path. Thus, we carried out experiments at the
ALL, OBJECT, PHYSICAL-OBJECT, ANIMATE, ANI-
MAL, VERTEBRATE, MAMMAL, PRIMATE, HUMAN,
and SOCIAL-ROLE nodes. The goal was to evaluate the per-
formance of the three methods based on the number of words
that were classiﬁed correctly at each node on the path. A word
is classiﬁed correctly if it is assigned to the correct child con-
cept.

LSA-based tree descending method
For every ontology node on the path, we randomly picked at
most 50 words from each of the subtrees rooted at the child
concepts of the node. So, at the ALL node, we would have
words picked from the OBJECT, EVENT and PROPERTY
subtrees, at the OBJECT node we would have words picked
from the PHYSICAL-OBJECT, INTANGIBLE-OBJECT,
MENTAL-OBJECT, SOCIAL-OBJECT and TEMPORAL-
OBJECT subtrees. Thus, each node on the path had a differ-
ent set of words on which the experiments were carried out.
For each set of words, we used cross-validation testing to de-
termine the number of words that were classiﬁed correctly.

To generalize the results over the random element involved
in picking words, at each node, we carried out the experi-
ments 10 times, each time picking a different set of words
and performing cross-validation testing. We used values of k
when running LSA that were empirically good.

Google Category

87.07

78.72

93.64

88.30

93.30

92.38

94.96

100.00

100.00

69.60

Node

ALL

OBJECT

PHYS-OBJ

ANIMATE

ANIMAL

VERTEBRATE

MAMMAL

PRIMATE

HUMAN

Baseline

33.33

20.00

50.00

33.33

50.00

16.66

12.50

50.00

LSA

53.72

59.74

74.64

85.19

83.30

81.71

78.65

93.68

49.72

48.48

68.54

80.42

74.60

61.22

58.00

94.76

100.00

100.00

100.00

SOCIAL-ROLE

5.55

64.16

37.50

Table 3: Average accuracy (%) of the LSA-based tree de-
scending method, the Google method and the category-based
tree descending method

Google method
As with the LSA-based tree descending method, for every
node on the path, we randomly picked at most 50 words from
each of the subtrees rooted at the child concepts of the node.
Thus, at each node we had a different set of words for which
we measured the performance of the Google method. The
experiments were carried out 10 times at each node, each time
with a different set of words.

Category-based tree descending method
We carried out the experiments for the category-based tree
descending method proposed by Maedche et al., except that
we replaced the metrics used by them with the cosine similar-
ity metric. For every node on the path, we gathered all words
at most three levels below from each of the subtrees rooted
at the child concepts of the node. Since there was no random
element involved, we carried out the experiments just once at
each node.

Results
To have a benchmark for the performance of the three meth-
ods, a baseline was calculated, which was the number of
words classiﬁed correctly when a new word was randomly
assigned to a child concept. Since the probability of getting a
word correct by chance depends on the number of child con-
cepts of a given node, the baseline varies with the number of
child concepts of a node.

Table 3 shows the results for the three methods. Each cell
shows the percentage accuracy for a given method, at a given
node. For the LSA-based and Google methods, this value is
the average of the accuracies obtained for each of the 10 runs
of the experiment at a given node. Since Google’s estimates
of page counts can vary quite a bit for a given search term,
the results for the Google method are as per the page counts
returned at the time of performing the experiments.

From the table we can see that at all nodes, the category-
based method outperforms the LSA-based and Google meth-
ods. The LSA-based method does better than the Google
method at all nodes except at the PRIMATE node.

At every node, we performed a t-test on the accuracy distri-
butions of the LSA-based and Google methods using the pub-

IJCAI-07

1621

y
a

c

y
b

d

x
x

Table 4: Contingency table for the co-occurrence of x and y

licly available Simple Interactive Statistical Analysis (SISA)
package. The t-test results show that the difference in perfor-
mance of the LSA-based and Google methods is statistically
signiﬁcant at all nodes other than at the ALL and PRIMATE
nodes.

Similarly, the z-scores for the category-based method show
that the difference in performance of the LSA-based and
category-based methods is statistically signiﬁcant at all nodes
other than at the ANIMATE and ANIMAL nodes.

4 Analysis
The experimental results show that the category-based tree
descending method outperforms the other two methods while
the LSA-based tree descending method outperforms the Goo-
gle method. In this section, we attempt to identify the reasons
for the superior performance of the category-based method as
well as the ﬂaws in the Google method.

4.1 Analysis of the Google method
In this subsection, we argue that the Google method performs
poorly because the NGD measure does not capture the true
semantic distance between polysemous terms.

Given two words x and y, the Normalized Google Distance

(NGD) between them is given by:
max{log
max{log 1

N GD(x, y) =

1

1

p(y|x

}

p(x|y , log
p(x) , log 1

p(y) } (1)

Words x and y will be perceived to be semantically similar if
the distance between them is small. That is, the numerator of
(1) should be small while the denominator should be large.
Minimizing the numerator means maximizing both p(x|y)
and p(y|x) while maximizing the denominator means mini-
mizing either p(x) or p(y).

Table 4 shows a contingency table for the co-occurrence of

terms x and y. From the table:

p(x|y) =

p(y|x) =

p(x ∩ y)

p(y)

p(x ∩ y)

p(x)

=

=

a

a + c

a

a + b

In order to maximize p(x|y) and p(y|x), c and b respectively
should be minimized. Also, from the table,

p(x) =

p(y) =

a + b

a + b + c + d

a + c

a + b + c + d

To maximize the denominator, we minimize either p(x) or
p(y). To minimize p(x), we need to maximize c and d. How-
ever, maximizing c is not possible because c needs to be mini-
mized to minimize the numerator. To minimize p(y), we need

to maximize b and d. Maximizing b is not possible because
b should be small to minimize the numerator. Thus, in either
case, d needs to be maximized.

It is clear that the Google method will say that x and y
are similar if three conditions are satisﬁed. First, x and y co-
occur in a certain number of web pages (cell 1), second, the
number of web pages in which either of x or y occurred with-
out the other is minimal (cells 2 and 3) and, ﬁnally, there are
inﬁnitely many web pages in which neither x nor y occurred
(cell 4).

The Web, by virtue of its unrestricted size satisﬁes the last
condition. However, at the same time, the size of the Web
can be a problem too. Due to the sheer mass of web pages
on almost every conceivable topic, it is likely that a word on
the Web will be used in more than one sense. This means
that, even if two words x and y are semantically related, there
are likely to be a signiﬁcant number of web pages in which x
is used in a sense not related to y and vice-versa. Thus, for
almost all pairs of words, the second condition would not be
satisﬁed. For example, the words monk and pope are both
RELIGIOUS-ROLES that a HUMAN plays. Since the two
words are semantically related, they co-occur in a signiﬁcant
number of pages. A Google query with the words “monk
and pope” retrieves 569,000 web pages. However, the query
“monk and not pope” retrieves 5,740,000 web pages while
the query “pope and not monk” retrieves 17,600,000 pages.

4.2 Analysis of the Category-based method

Rather surprisingly, the category-based method, which com-
putes word similarities in the original vector space performs
better than the more sophisticated LSA-based method. To
identify the reasons for the category-based method perform-
ing better than the LSA-based method, we carried out a set of
controlled experiments.

We carried out experiments with the LSA-based method
and the category-based method at the PHYSICAL-OBJECT,
ANIMATE, ANIMAL, MAMMAL and SOCIAL-ROLE
nodes. The category-based method was modiﬁed slightly in
that, at a given node, at most 50 words were randomly picked
from each of the subtrees rooted at the child concepts. This
modiﬁcation was made so that, other than the use of LSA in
the ﬁrst method and category-based vector representations in
the second method, the two methods were identical in all re-
spects.

For the experiments at each node,

two control groups
were used. The ﬁrst “control group” is a method that com-
putes word similarity using document vector representations
in the original vector space, that is, it neither uses LSA
nor category-based vector representations. The second “con-
trol group” is a method that computes word similarity using
category-based vector representations in the reduced vector
space, that is, it uses both LSA and category-based vector
representations.

At each node, we evaluated the performance of the four
methods 10 times each, using a different set of words for each
evaluation.

Tables 6 through 10 show the results obtained at each node.
For each table, the ﬁrst cell shows the average percentage
accuracy for the method that uses both LSA and category-

IJCAI-07

1622

Category ¬Category

LSA
¬LSA

60.94

79.86

74.64

80.49

Category ¬Category

LSA
¬LSA

67.08

87.06

78.65

80.10

Table 8: Results obtained at the MAMMAL node, k= 30

Category ¬Category

LSA
¬LSA

53.93

69.60

66.14

64.17

Table 9: Results obtained at the SOCIAL-ROLE node, k= 75

6 Acknowledgments

This research was supported in part by NSF CAREER award
0447435. We would like to thank the anonymous reviewers
for their many suggestions on ways to improve the paper.

References
[1] E. H. Eneko Agirre, Olaz Ansa and D. Martinez. En-
riching very large ontologies using the www. In EJCAI
2000 WOrkshop on Ontology Learning.

[2] R. Cilibrasi

and
discovery

meaning
arxiv.org/abs/cs.CL/0412098, 2004.

P. Vitanyi.
using

google.

Automatic
preprint,

[3] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. Indexing by latent seman-
tic analysis. Journal of the American Society of Infor-
mation Science, 41(6):391–407, 1990.

[4] C. D. Fellbaum. WordNet:

an electronic lexical
database. The MIT press, Cambridge, MA, USA, 1998.

[5] D. B. Lenat. Cyc: a large-scale investment in knowledge

infrastructure. Commun. ACM, 38(11):33–38, 1995.

[6] D. Lin. Automatic retrieval and clustering of similar
words. In 17th International Conference on Computa-
tional Linguistics and of the 36th Annual Meeting of the
Association for Computational Linguistics.

[7] A. Maedche, V. Pekar, and S. Staab. Ontology Learning
Part One - On Discovering Taxonomic Relations from
the Web, pages 301–322. Springer Verlag, 2002.

[8] S. Nirneburg and V. Raskin. Ontological Semantics. The

MIT press, Cambridge, MA, USA, 1998.

[9] D. Widdows and B. Dorow. A graph model for unsuper-

vised lexical acquisition. In COLING, 2002.

[10] T. Yokoi. The edr electronic dictionary. Commun. ACM,

38(11):42–44, 1995.

Table 5: Results obtained at the PHYSICAL-OBJECT node,
k= 15

Category ¬Category

LSA
¬LSA

84.44

89.95

85.19

85.88

Table 6: Results obtained at the ANIMATE node, k= 20

based vector representations, the second cell shows the aver-
age accuracy for the LSA-based method, the third cell shows
the average accuracy for the category-based method and the
fourth cell shows the average accuracy for the non-LSA, non
category-based method. The values of k at which the exper-
iments were carried out were the “good” values which were
empirically chosen.

By summing the accuracies in the tables row-wise, we
can see that, at each node, the non-LSA method does bet-
ter than the LSA method. Summing the accuracies column-
wise. we see that, at the PHYSICAL-OBJECT, MAMMAL
and SOCIAL-ROLE nodes, the non-category method seems
to be better, while at the ANIMATE and ANIMAL nodes,
the use of category-based vectors seems to give better perfor-
mance. Looking at individual cells, we see that the non-LSA,
category-based method does best at all nodes except at the
PHYSICAL-OBJECT node. In fact, even the simplistic non-
LSA, non category-based method does better than the LSA-
based method at all nodes except at the SOCIAL-ROLE node.

5 Conclusions

We presented a novel method that used both symbolic knowl-
edge and statistical techniques to learn lexical semantics. We
found that using good context dramatically improved the per-
formance of the system. We then used the Google method
and the category-based tree descending method to perform
the task of ontology learning. We discovered that the perfor-
mance of the category-based tree descending method could be
signiﬁcantly improved using the cosine similarity metric. The
modiﬁed category-based tree descending method was found
to be the best approach for extending an existing ontology
with new words. We showed that the Google method per-
formed poorly because the NGD measure did not capture the
true semantic distance between polysemous terms. We also
showed that our proposed method performed poorly because
of the LSA algorithm.

Category ¬Category

LSA
¬LSA

86.70

87.59

83.30

86.55

Table 7: Results obtained at the ANIMAL node, k= 20

IJCAI-07

1623


Information-Based Agency

Carles Sierra

John Debenham

Institut d’Investigacio en Intel.ligencia Artiﬁcial

Spanish Scientiﬁc Research Council, UAB

Faculty of Information Technology
University of Technology, Sydney

08193 Bellaterra, Catalonia, Spain

sierra@iiia.csic.es

NSW, Australia

debenham@it.uts.edu.au

Abstract

Successful negotiators look beyond a purely util-
itarian view. We propose a new agent architec-
ture that integrates the utilitarian, information, and
semantic views allowing the deﬁnition of strate-
gies that take these three dimensions into account.
Information-based agency values the information
in dialogues in the context of a communication lan-
guage based on a structured ontology and on the no-
tion of commitment. This abstraction uniﬁes mea-
sures such as trust, reputation, and reliability in a
single framework.

needs into goals and these into plans composed of actions.
Second, a reactive machinery, that uses the received messages
to obtain a new world model by updating the probability dis-
tributions in it. Agents summarise their world models using
a number of measures (e.g. trust, reputation, and reliability
[Sierra and Debenham, 2006]) that can then be used to de-
ﬁne strategies for “exchanging information” — in the sense
developed here, this is the only thing that an agent can do.

We introduce the communication language in Section 2,
the agent architecture in Section 3, some summary measures
based on the architecture in Section 4, and some associated
interaction strategies in Section 5.

Introduction

1
In this paper we introduce an agency framework grounded
on information-based concepts [MacKay, 2003]. It presents
an agent architecture that admits a game-theoretical reading
and an information-theoretical reading. And, what is more
interesting, permits a connection between these two worlds
by considering negotiation as both cooperative, in the sense
that all interaction involves the exchange of information, and
competitive, in the sense that agents aim at getting the best
they can. This approach contrasts with previous work on the
design of negotiation strategies that did not take information
exchange into account, but focused on the similarity of of-
fers [Jennings et al., 2001; Faratin et al., 2003], game theory
[Rosenschein and Zlotkin, 1994], or ﬁrst-order logic [Kraus,
1997].
system
We
{α, β1, . . . , βo, ξ, θ1, . . . , θt}, contains an agent α that
interacts with negotiating agents, βi, information providing
agents, θj, and an institutional agent, ξ,
that represents
the institution where we assume the interactions happen
[Arcos et al., 2005]. Institutions give a normative context to
interactions that simplify matters (e.g an agent can’t make
an offer, have it accepted, and then renege on it). We will
describe a communication language C based on an ontology
that will permit us both to structure the dialogues and to
structure the processing of the information gathered by
agents. Agents have an internal language L used to build a
probabilistic world model.

multiagent

We understand agents as being built on top of two basic
functionalities. First, a proactive machinery, that transforms

assume

that

a

2 The Multiagent System
Our agent α has two languages: C is an illocutionary-based
language for communication, and L is a language for internal
representation. C is described following, and L in Section 3.
2.1 Communication Language C
The shape of the language that α uses to represent the infor-
mation received and the content of its dialogues depends on
two fundamental notions. First, when agents interact within
an overarching institution they explicitly or implicitly accept
the norms that will constrain their behaviour, and accept the
established sanctions and penalties whenever norms are vi-
olated. Second, the dialogues in which α engages are built
around two fundamental actions: (i) passing information, and
(ii) exchanging proposals and contracts. A contract δ = (a, b)
between agents α and β is a pair where a and b represent the
activities that agents α and β are respectively responsible for.
Contracts signed by agents and information passed by agents,
are similar to norms in the sense that they oblige agents to be-
have in a particular way, so as to satisfy the conditions of the
contract, or to make the world consistent with the information
passed. Contracts and Information can then be thought of as
normative statements that restrict an agent’s behaviour.

Norms, contracts, and information have an obvious tempo-
ral dimension. Thus, an agent has to abide by a norm while
it is inside an institution, a contract has a validity period, and
a piece of information is true only during an interval in time.
The set of norms affecting the behaviour of an agent deﬁne
the context that the agent has to take into account.

The communication language that α needs requires two
fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ,

IJCAI-07

1513

what is the world α aims at bringing about and that β has
the right to verify, complain about or claim compensation
for any deviations from, and Done(a) to represent the event
that a certain action a1 has taken place. In this way, norms,
contracts, and information chunks will be represented as in-
stances of Commit(·) where α and β can be individual agents
or institutions, C is:

a ::= illoc(α, β, ϕ, t) | a; a |
Let context In a End
ϕ ::= term | Done(a) | Commit(α, ϕ) | ϕ ∧ ϕ |
ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv
context ::= ϕ | id = ϕ | prolog clause | context; context
where ϕv is a formula with free variable v, illoc is any ap-
propriate set of illocutionary particles, ‘;’ means sequencing,
and context represents either previous agreements, previous
illocutions, or code that aligns the ontological differences be-
tween the speakers needed to interpret an action a.

For example, we can represent the following offer: “If you
spend a total of more than e100 in my shop during October
then I will give you a 10% discount on all goods in Novem-
ber”, as:
Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 →

∀ y. Done(Inform(ξ, α, pay(β, α, y), November)) →

Commit(α, β, discount(y,10%)))

Note the use of the institution agent ξ to report the payment.
In order to deﬁne the language introduced above that
structures agent dialogues we need an ontology that in-
cludes a (minimum) repertoire of elements: a set of con-
cepts (e.g. quantity, quality, material) organised in a is-a
hierarchy (e.g. platypus is a mammal, australian-dollar is
a currency), and a set of relations over these concepts (e.g.
price(beer,AUD)).2 We model ontologies following an alge-
braic approach [Kalfoglou and Schorlemmer, 2003] as:
An ontology is a tuple O = (C, R,≤, σ) where C is a ﬁ-
nite set of concept symbols (including basic data types), R
is a ﬁnite set of relation symbols, ≤ is a reﬂexive, transi-
tive and anti-symmetric relation on C (a partial order), and
σ : R → C + is the function assigning to each relation sym-
bol its arity. ≤ is a traditional is-a hierarchy, and R contains
relations between the concepts in the hierarchy.

The concepts within an ontology are closer, semantically
speaking, depending on how far away are they in the structure
deﬁned by the ≤ relation. Semantic distance plays a funda-
mental role in strategies for information-based agency. How
signed contracts, Commit(·) about objects in a particular se-
mantic region, and their execution Done(·), affect our deci-
sion making process about signing future contracts on nearby
semantic regions is crucial to model the common sense that
human beings apply in managing trading relationships. A
measure [Li et al., 2003] bases the semantic similarity be-
tween two concepts on the path length induced by ≤ (more
1Without loss of generality we will assume that all actions are

dialogical.

2Usually, a set of axioms deﬁned over the concepts and relations

is also required. We will omit this here.

distance in the ≤ graph means less semantic similarity), and
the depth of the subsumer concept (common ancestor) in the
shortest path between the two concepts (the deeper in the hi-
erarchy, the closer the meaning of the concepts). Semantic
similarity could then be deﬁned as:

(cid:2)
Sim(c, c

−κ1l · eκ2h − e
eκ2h + e

−κ2h
−κ2h

) = e

where l is the length (i.e. number of hops) of the shortest
path between the concepts, h is the depth of the deepest con-
cept subsuming both concepts, and κ1 and κ2 are parameters
scaling the contribution of shortest path length and depth re-
spectively.
Given a formula ϕ ∈ C in the communication language we
deﬁne the vocabulary or ontological context of the formula,
O(ϕ), as the set of concepts in the ontology used in it. Thus,
we extend the previous deﬁnition of similarity to sets of con-
cepts in the following way:

Sim(ϕ, ψ) = max
ci∈O(ϕ)

min

cj∈O(ψ)

{Sim(ci, cj)}

The following does not depend on this particular deﬁnition.

3 Agent Architecture
Agent α receives all messages expressed in C in an in-box X
where they are time-stamped and sourced-stamped. A mes-
sage μ from agent β (or θ or ξ) is then moved from X to a
percept repository Y t where it is appended with a subjective
t(α, β, μ) that normally decays with time.
belief function R
α acts in response to a message that expresses a need. A
need may be exogenous such as a need to trade proﬁtably and
may be triggered by another agent offering to trade, or en-
dogenous such as α deciding that it owns more wine than it
requires. Needs trigger α’s goal/plan proactive reasoning de-
scribed in Section 3.1, other messages are dealt with by α’s
reactive reasoning described in Section 3.2.
3.1 Proactive Reasoning
α’s goal/plan machinery operates in a climate of changing un-
certainty and so typically will pursue multiple sub-goals con-
currently. This applies both to negotiations with a potential
trading partner, and to interaction with information sources
that either may be unreliable or may take an unpredictable
time to respond. Each plan contains constructors for a world
model Mt that consists of probability distributions, (Xi), in
ﬁrst-order probabilistic logic L. Mt is then maintained from
percepts received using update functions that transform per-
cepts into constraints on Mt — described in Section 3.2.
The distributions in Mt are determined by α’s plans that
are determined by its needs. If α is negotiating some con-
tract δ in satisfaction of need χ then it may require the dis-
t(eval(α, β, χ, δ) = ei) where for a particular δ,
tribution P
eval(α, β, χ, δ) is an evaluation over some complete and dis-
joint evaluation space E = (e1, . . . , en) that may contain
hard (possibly utilitarian) values, or fuzzy values such as “re-
ject” and “accept”. This distribution assists α’s strategies to
decide whether to accept a proposed contract leading to a
t(acc(α, β, χ, δ)). For example,
probability of acceptance P

IJCAI-07

1514

t(eval(α, β, χ, δ) = ei) could be derived from the subjec-
P
t(satisfy(α, β, χ, δ) = fj) of the expected ex-
tive estimate P
tent to which the execution of δ by β will satisfy χ, and an
t(val(α, β, δ) = gk) of the expected val-
objective estimate P
uation of the execution of δ possibly in utilitarian terms. This
second estimate could be derived by proactive reference to
the {θi} for market data. In a negotiation α’s plans may also
t(acc(β, α, δ)) that estimates the
construct the distribution P
probability that β would accept δ — we show in Section 3.2
how α may derive this estimate from the information in β’s
proposals.

α’s plans may construct various other distributions such
t(trade(α, β, o) = ei) that β is a good person to sign
as: P
t(conﬁde(α, β, o) = fj)
contracts with in context o, and P
that α can trust β with conﬁdential information in context o.
The integrity of percepts decreases in time. α may have
background knowledge concerning the expected integrity of
a percept as t → ∞. Such background knowledge is repre-
sented as a decay limit distribution. If the background knowl-
edge is incomplete then one possibility is for α to assume that
the decay limit distribution has maximum entropy whilst be-
ing consistent with the data. Given a distribution, P(Xi), and
a decay limit distribution D(Xi), P(Xi) decays by:

P

t(Xi).

t(Xi))

i and D

t+1(Xi) = Δi(D(Xi), P

t+1(Xi) = (1−νi)× D(Xi)+νi× P

(1)
where Δi is the decay function for the Xi satisfying the prop-
erty that limt→∞ P
t(Xi) = D(Xi). For example, Δi could
t(Xi), where
be linear: P
νi < 1 is the decay rate for the i’th distribution. Either the
decay function or the decay limit distribution could also be a
function of time: Δt
3.2 Reactive Reasoning
In the absence of in-coming messages the integrity of Mt
decays by Eqn. 1. The following procedure updates Mt for
all percepts expressed in C. Suppose that α receives a mes-
sage μ from agent β at time t. Suppose that this message
states that something is so with probability z, and suppose
t(α, β, μ) to μ — this
that α attaches an epistemic belief R
probability reﬂects α’s level of personal caution. Each of α’s
active plans, s, contains constructors for a set of distributions
{Xi} ∈ Mt together with associated update functions, Js(·),
such that J Xi
s (μ) is a set of linear constraints on the posterior
distribution for Xi. Examples of these update functions are
t(Xi) by
given in Section 3.4. Denote the prior distribution P
(cid:13)p, and let (cid:13)p(μ) be the distribution with minimum relative en-
tropy3 with respect to (cid:13)p: (cid:13)p(μ) = arg min(cid:5)r
pj that

(cid:2)
j rj log rj

P

P

i pi − 1 = 0) is: (cid:2)p = arg min(cid:2)r
qj + (cid:2)λ · (cid:2)g. Minimising L, { ∂L

3Given a probability distribution (cid:2)q, the minimum relative entropy
distribution (cid:2)p = (p1, . . . , pI ) subject to a set of J linear constraints
(cid:2)g = {gj((cid:2)p) = (cid:2)aj · (cid:2)p − cj = 0}, j = 1, . . . , J (that must include
rj
qj .
the constraint
This may be calculated by introducing Lagrange multipliers (cid:2)λ:
L((cid:2)p, (cid:2)λ) =
∂λj = gj((cid:2)p) =
0}, j = 1, . . . , J is the set of given constraints (cid:2)g, and a solution to
∂pi = 0, i = 1, . . . , I leads eventually to (cid:2)p. Entropy-based infer-
ence is a form of Bayesian inference that is convenient when the data
is sparse [Cheeseman and Stutz, 2004] and encapsulates common-
sense reasoning [Paris, 1999].

j pj log

j rj log

P

∂L

pj

s (μ). Then let (cid:13)q(μ) be the distribu-

satisﬁes the constraints J Xi
tion:

(cid:13)q(μ) = R
and then let:

t(α, β, μ) × (cid:13)p(μ) + (1 − R
(cid:3)

t(α, β, μ)) × (cid:13)p

(2)

(3)

P

t(Xi(μ)) =

(cid:13)q(μ)
(cid:13)p

if (cid:13)q(μ) is more interesting than (cid:13)p
otherwise

A general measure of whether (cid:13)q(μ) is more interesting than
(cid:2)
(cid:13)p is: K((cid:13)q(μ)(cid:11)D(Xi)) > K((cid:13)p(cid:11)D(Xi)), where K((cid:13)x(cid:11)(cid:13)y) =
j xj ln xj
is the Kullback-Leibler distance between two
yj
probability distributions (cid:13)x and (cid:13)y.

Finally merging Eqn. 3 and Eqn. 1 we obtain the method

for updating a distribution Xi on receipt of a message μ:

P

t+1(Xi) = Δi(D(Xi), P

(4)
This procedure deals with integrity decay, and with two prob-
abilities: ﬁrst, the probability z in the percept μ, and second
the belief R

t(α, β, μ) that α attached to μ.

t(Xi(μ)))

In a simple multi-issue contract negotiation α may esti-
t(acc(β, α, δ)), the probability that β would accept
mate P
δ, by observing β’s responses. Using shorthand notation, if
β sends the message Oﬀer(δ1) then α may derive the con-
straint: J acc(β,α,δ)(Oﬀer(δ1)) = {P
t(acc(β, α, δ1)) = 1},
and if this is a counter offer to a former offer of α’s, δ0,
t(acc(β, α, δ0)) = 0}. In
then: J acc(β,α,δ)(Oﬀer(δ1)) = {P
the not-atypical special case of multi-issue bargaining where
the agents’ preferences over the individual issues only are
known and are complementary to each other’s, maximum en-
tropy reasoning can be applied to estimate the probability
that any multi-issue δ will be acceptable to β by enumerating
the possible worlds that represent β’s “limit of acceptability”
[Debenham, 2004].
3.3 Reliability

The idea of Eqn. 2, is that R

t(α, β, μ) is an epistemic probability that takes account of
R
t(α, β, μ)
α’s personal caution. An empirical estimate of R
may be obtained by measuring the ‘difference’ between com-
mitment and enactment. Suppose that μ is received from
(cid:2) at some later time t.
agent β at time u and is veriﬁed by ξ as μ
u(Xi) by (cid:13)p. Let (cid:13)p(μ) be the posterior min-
Denote the prior P
imum relative entropy distribution subject to the constraints
(cid:2)).
J Xi
s (μ), and let (cid:13)p(μ(cid:2)) be that distribution subject to J Xi
s (μ
u(α, β, μ) should have been in the
We now estimate what R
(cid:2).
light of knowing now, at time t, that μ should have been μ
t(α, β, μ) should be such that,
on average across Mt, (cid:13)q(μ) will predict (cid:13)p(μ(cid:2)) — no matter
whether or not μ was used to update the distribution for Xi, as
determined by the condition in Eqn. 3 at time u. The observed
Xi(α, β, μ)|μ
(cid:2), on the
reliability for μ and distribution Xi, R
(cid:2), is the value of k that
basis of the veriﬁcation of μ with μ
minimises the Kullback-Leibler distance:
K(k · (cid:13)p(μ) + (1 − k) · (cid:13)p (cid:11) (cid:13)p(μ(cid:2)))
Xi(α, β, μ)|μ
(cid:2)
The predicted information in the enactment of μ with respect
to Xi is:

= arg min

R

k

t

t

t
Xi(α, β, μ) = H
I

t(Xi) − H

t(Xi(μ))

(5)

IJCAI-07

1515

that is the reduction in uncertainty in Xi where H(·) is Shan-
t(α, β, μ).
non entropy. Eqn. 5 takes account of the value of R
If X(μ) is the set of distributions that μ affects, then the
observed reliability of β on the basis of the veriﬁcation of μ
with μ

(cid:2) is:

t(α, β, μ)|μ
(cid:2)

R

=

t

Xi(α, β, μ)|μ
(cid:2)

R

(6)

If X(μ) are independent the predicted information in μ is:

t(α, β, μ) =
I

t
Xi(α, β, μ)
I

(7)

(cid:4)

i

1|X(μ)|
(cid:4)
Xi∈X(μ)

Suppose α sends message μ to β where μ is α’s private infor-
mation, then assuming that β’s reasoning apparatus mirrors
α’s, α can estimate I
For each formula ϕ at time t when μ has been veriﬁed with
(cid:2), the observed reliability that α has for agent β in ϕ is:
μ

t(β, α, μ).

t+1(α, β, ϕ) =(1 − ν) × R

R

t(α, β, ϕ)+

ν × R

t(α, β, μ)|μ

(cid:2) × Sim(ϕ, μ)

where Sim measures the semantic distance between two sec-
tions of the ontology as introduced in Section 2.1, and ν is the
learning rate. Over time, α notes the context of the various μ
received from β, and over the various contexts calculates the
t(μ). This leads to an overall expectation
relative frequency, P
of the reliability that agent α has for agent β:

R

t(α, β) =

t(μ) × R

t(α, β, μ)

(cid:4)

P

μ

3.4 Commitment and Enactment
The interaction between agents α and β will involve β mak-
ing contractual commitments and (perhaps implicitly) com-
mitting to the truth of information exchanged. No matter
what these commitments are, α will be interested in any
variation between β’s commitment, ϕ, and what is actually
observed (as advised by the institution agent ξ), as the en-
(cid:2). We denote the relationship between commit-
actment, ϕ
(cid:2))|Commit(ϕ)) simply
ment and enactment, P
as P
In the absence of in-coming messages the conditional prob-
(cid:2)|ϕ), should tend to ignorance as represented
abilities, P
by the decay limit distribution and Eqn. 1. We now show
(cid:2)|ϕ) as observa-
how Eqn. 4 may be used to revise P
tions are made. Let the set of possible enactments be Φ =
(cid:2)|ϕ). Sup-
{ϕ1, ϕ2, . . . , ϕm} with prior distribution (cid:13)p = P
pose that message μ is received, we estimate the posterior
(cid:13)p(μ) = (p(μ)i)m

(cid:2)|ϕ) ∈ Mt.

t(Observe(ϕ

i=1 = P

(cid:2)|ϕ).

t+1(ϕ

t(ϕ

t(ϕ

t(ϕ

t(ϕ

First, if μ = (ϕk, ϕ) is observed then α may use this ob-
servation to estimate p(ϕk)k as some value d at time t + 1.
We estimate the distribution (cid:13)p(ϕk) by applying the principle
of minimum relative entropy as in Eqn. 4 with prior (cid:13)p, and
the posterior (cid:13)p(ϕk) = (p(ϕk)j)m
j=1 satisfying the single con-
straint: J (ϕ(cid:2)|ϕ)(ϕk) = {p(ϕk)k = d}.
(cid:2) of
Second, we consider the effect that the enactment φ
another commitment φ, also by agent β, has on (cid:13)p. This is
achieved in two ways, ﬁrst by appealing to the structure of
the ontology using the Sim(·) function, and second by intro-
ducing a valuation function.

, φ),

t(ϕi|ϕ)+(1− | Sim(φ
(cid:2)

The Sim(·) method. Given the observation μ = (φ
(cid:2)
deﬁne the vector (cid:13)t by
, φ)−Sim(ϕi, ϕ) |)·Sim(ϕ
ti = P
, φ)
for i = 1, . . . , m. (cid:13)t is not a probability distribution. The mul-
, φ) limits the variation of probability
tiplying factor Sim(ϕ
to those formulae whose ontological context is not too far
away from the observation. The posterior (cid:13)p(φ(cid:2),φ) is deﬁned to
be the normalisation of (cid:13)t.

(cid:2)

(cid:2)

The valuation method. α may wish to value ϕ in some
sense. This value will depend on the future use that α makes
of it. So α estimates the value of ϕ using a probability
distribution (p1, . . . , pn) over some evaluation space E =
(e1, . . . , en). pi = wi(ϕ) is the probability that ei is the cor-
rect evaluation of the enactment ϕ, and (cid:13)w : L × L → [0, 1]n
is the evaluation function.
t(ϕm|ϕk)) is the prior
distribution of α’s estimate of what will be observed if β com-
mitted to ϕk, and (cid:13)w(ϕk) = (w1(ϕk), . . . , wn(ϕk)) is α’s
evaluation over E of β’s commitment ϕk. α’s expected eval-
uation of what will be observed that β has committed to ϕk
is (cid:13)wexp(ϕk):

t(ϕ1|ϕk), . . . , P

For a given ϕk, (P

exp(ϕk)i =

w

t(ϕj|ϕk) · wi(ϕj)

P

m(cid:4)

j=1

rev(ϕk) | (φ
(cid:13)g( (cid:13)w

(cid:2)|φ)) =
(cid:2)
exp(ϕk), Sim(φ

for i = 1, . . . , n. Now suppose that α observes the enactment
(cid:2) of another commitment φ also by agent β. Eg: α may buy
φ
wine and cheese from the same supplier. α may wish to revise
the prior estimate of the expected valuation (cid:13)wexp(ϕk) in the
(cid:2)
light of the observation (φ
( (cid:13)w

, φ) to:

, φ), Sim(ϕ, φ), (cid:13)w(ϕ), (cid:13)w(φ), (cid:13)w(φ

))
for some function (cid:13)g — the idea being, for example, that if the
commitment, φ, a contract for cheese was not kept then α’s
expectation that the commitment, ϕ, a contract for wine will
not be kept should increase. We estimate the posterior (cid:13)p(φ(cid:2),φ)
by applying the principle of minimum relative entropy as in
Eqn. 4 with prior (cid:13)p and (cid:13)p(φ(cid:2),φ) = (p(φ(cid:2),φ)j)m
j=1 satisfying the
n constraints:
(cid:2)
(ϕ(cid:2)|ϕ)((φ

p(φ(cid:2),φ)j · wi(ϕj) =

(cid:5) m(cid:4)

, φ)) =

J

(cid:2)

j=1

(cid:2)
exp(ϕk), Sim(φ

gi( (cid:13)w

, φ), Sim(ϕ, φ), (cid:13)w(ϕ), (cid:13)w(φ), (cid:13)w(φ

))

i=1
This is a set of n linear equations in m unknowns, and so
the calculation of the minimum relative entropy distribution
may be impossible if n > m.
In this case, we take only
the m equations for which the change from the prior to the
posterior value is greatest. That is, we attempt to select the
most signiﬁcant factors.
4 Summary Measures
The measures here generalise what are commonly called
trust, reliability and reputation measures into a single com-
putational framework. It they are applied to the execution of

(cid:6)n

(cid:2)

IJCAI-07

1516

contracts they become trust measures, to the validation of in-
formation they become reliability measures, and to socially
transmitted behaviour they become reputation measures.

Ideal enactments. Consider a distribution of enactments
that represent α’s “ideal” in the sense that it is the best that
α could reasonably expect to happen. This distribution will
be a function of α’s context with β denoted by e, and is
(cid:2)|ϕ, e). Here we measure the relative entropy between
t
I(ϕ
P
(cid:2)|ϕ, e), and the distribution of ex-
t
this ideal distribution, P
I(ϕ
(cid:2)|ϕ). That is:
t(ϕ
(cid:4)
pected enactments, P
M(α, β, ϕ) = 1 −

(cid:2)|ϕ, e) log

t
I(ϕ

(8)

P

t
I(ϕ
P
Pt(ϕ

(cid:2)|ϕ, e)
(cid:2)|ϕ)

ϕ(cid:2)

where the “1” is an arbitrarily chosen constant being the max-
imum value that this measure may have. This equation mea-
sures one, single commitment ϕ. It makes sense to aggregate
these values over a class of commitments, say over those ϕ
that are in the context o, that is ϕ ≤ o:

(cid:2)
β(ϕ) [1 − M(α, β, ϕ)]
(cid:2)
ϕ:ϕ≤o P
ϕ:ϕ≤o P

t
β(ϕ)

t

M(α, β, o) = 1 −

t
β(ϕ) is a probability distribution over the space of
where P
commitments that the next commitment β will make to α is
ϕ. Similarly, for an overall estimate of β’s reputation to α:

M(α, β) = 1 −

t

β(ϕ) [1 − M(α, β, ϕ)]

(cid:4)

P

ϕ

t
I(ϕ

Preferred enactments. The previous measure requires that
(cid:2)|ϕ, e), has to be speciﬁed for each
an ideal distribution, P
ϕ. Here we measure the extent to which the enactment
(cid:2) is preferable to the commitment ϕ. Given a predicate
ϕ
Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environ-
t(Prefer(c1, c2, e)) may be de-
ment e. An evaluation of P
ﬁned using Sim(·) and the evaluation function (cid:13)w(·) — but
we do not detail it here. Then if ϕ ≤ o:
(cid:2)
t(Prefer(ϕ
(cid:2)
(cid:2)
ϕ:ϕ≤o P
ϕ:ϕ≤o P

t
β(ϕ)M(α, β, ϕ)

M(α, β, ϕ) =

M(α, β, o) =

(cid:2) | ϕ)

, ϕ, o))P

(cid:4)

t
β(ϕ)

and:

t(ϕ

ϕ(cid:2)

P

Certainty in enactment. Here we measure the consistency
in expected acceptable enactment of commitments, or “the
lack of expected uncertainty in those possible enactments that
If ϕ ≤ o
are better than the commitment as speciﬁed”.
, ϕ, o)) > κ} for some
let: Φ+(ϕ, o, κ) = {ϕ
constant κ, and:

(cid:2) | P

(cid:2)

t(Prefer(ϕ
(cid:4)

M(α, β, ϕ) = 1 +

t
where P
+(ϕ
Φ+(ϕ, o, κ),

∗

B

=

(cid:2)|ϕ) log P

t
+(ϕ
(cid:2)|ϕ) for ϕ

(cid:2)|ϕ)
(cid:2) ∈

t(ϕ

∗ ·
1
B

P

ϕ(cid:2)∈Φ+(ϕ,o,κ)

t
+(ϕ
(cid:2)|ϕ) is the normalisation of P
(cid:3)
1
log |Φ+(ϕ, o, κ)| otherwise

if |Φ+(ϕ, o, κ)| = 1

As above we aggregate this measure for those commitments
in a particular context o, and measure reputation as before.
Computational Note. The various measures given above in-
(cid:2)
volve extensive calculations. For example, Eqn. 8 contains
(cid:2). We obtain a
ϕ(cid:2) that sums over all possible enactments ϕ
more computationally friendly measure by appealling to the
structure of the ontology described in Section 2.1, and the
right-hand side of Eqn. 8 may be approximated to:

ϕ(cid:2):Sim(ϕ(cid:2),ϕ)≥η

(cid:2)|ϕ, e) log

t
η,I(ϕ
Pt
η(ϕ
(cid:2)|ϕ, e) is the normalisation of P

(cid:2)|ϕ, e)
(cid:2)|ϕ)
(cid:2)|ϕ, e) for
where P
(cid:2)|ϕ). The extent
(cid:2)
Sim(ϕ
of this calculation is controlled by the parameter η. An
, ϕ) ≥
even tighter restriction may be obtained with: Sim(ϕ
η and ϕ

t
η,I(ϕ
, ϕ) ≥ η, and similarly for P

(cid:2) ≤ ψ for some ψ.

(cid:4)

1 −

t
η,I(ϕ

t
I(ϕ

t
η(ϕ

P

P

(cid:2)

δ

{ P

5 Strategies
An approach to issue-tradeoffs is described in [Faratin et al.,
2003]. That strategy attempts to make an acceptable offer
by “walking round” the iso-curve of α’s previous offer (that
has, say, an acceptability of c) towards β’s subsequent counter
offer. In terms of the machinery described here:
t(acc(α, β, χ, δ)) ≈ c } (9)
In multi-issue negotiation the number of potential contracts
that satisfy, or nearly satisfy, a utilitarian strategy such as this
will be large — there is considerable scope for reﬁning the
choice of response.

t(acc(β, α, δ)) | P

arg max

t(β, α, μ

Everything that an agent communicates gives away infor-
mation. So even for purely self-interested agents interac-
tion is a semi-cooperative process. α’s machinery includes
measures of expected information gain in messages either
sent to, or received from, β. For example, α can reﬁne a
strategy such as Eqn. 9 by replying to a message μ from β
(cid:2) that attempts to give β equitable information gain:
with μ
t−1(α, β, μ) ≈ I
(cid:2)). This reﬁnement aims to be
I
fair in sharing α’s private information. Unlike the quasi-
utilitarian measures, both the measure of information gain
and the semantic measure (see below) apply to all messages.
The structure of the ontology is provided by the Sim(·)
(cid:2) containing con-
function. Given two contracts δ and δ
j} respectively, the (non-
cepts {o1, . . . , oi} and {o
(cid:2)
(cid:2)
1, . . . , o
(cid:2)) = (dk :
(cid:2) from δ is the vector (cid:13)Γ(δ, δ
symmetric) distance of δ
x) | x = 1, . . . , j},
k=1 where dk = minx{Sim(ok, o
(cid:2)
(cid:2)(cid:2)
k)i
o
j}, ok) and
k = sup(arg minx{Sim(ok, x) | x = o
(cid:2)
(cid:2)(cid:2)
(cid:2)
1, . . . , o
o
the function sup(·,·) is the supremum of two concepts in the
(cid:2) is to δ and en-
ontology. (cid:13)Γ(δ, δ
ables α to “work around” or “move away from” a contract
under consideration.

(cid:2)) quantiﬁes how different δ

More generally, suppose that α has selected a plan s that
is attempting to select the ‘best’ action in some sense. Then
s will construct a distribution D = (di) ∈ Mt where di is
the probability that zi is the best action that α can take, and
{z1, . . . , zr} are the available actions. s reduces H(D) to an
acceptable level before committing itself to act — here α is an

IJCAI-07

1517

uncertainty minimising agent. s reduces the entropy of Mt
by acquiring information as we now describe.

Distribution D will typically be deﬁned in terms of other
distributions. For example, the ‘best’ action may be deﬁned in
terms of the trustworthiness of β (Section 4), quasi-utilitarian
distributions such as those described in Section 3.1, measures
of information gain in Section 3.3, and measures of semantic
context in Section 2.1. Suppose that D is deﬁned in terms
of {Xi}. The integrity of each Xi will develop in time by
Eqn. 4. The update functions for Xi, J Xi, identify potential
“entropy reducing” information that may perhaps be obtained
from the information sources {θi}t
i=1. If α requests informa-
(cid:2) it is managed as de-
tion ϕ from θj that is delivered as ϕ
scribed in Section 3.4. That section also showed how nearby
(cid:2)
, φ), may be used to update distributions, so
observations, (φ
identifying additional information that may reduce entropy.

Let M t

αβ be the set of time-stamped messages that α has
sent to β, and M t
βα likewise both at time t. The next action
that α takes in this dialogue will be to send β a message μ
that is determined by its strategy. α’s strategy is a function
f(·) of the following form:
t(β, α, z),
f(I
μ = arg max
s ∈ M
s) | x
s ∈ M
t(acc(β, α, z), P

αβ ∪ M
t(α, β, χ, z)) | z is a proposal))

{I
s(β, α, x
s) | x
{(cid:13)Γ(z, x
(P

αβ},{I
βα},

s ∈ M

s(α, β, x

t

t

s) | x

t

βα},

z

t

It takes account of the acceptability (expected utility) of pro-
posals, the expected information gain in each communication
and the semantic relationship between proposals, both as seen
by itself and by β.

6 Conclusion
Information-based agency integrates in an homogeneous
agent architecture the exchange of information and the ne-
gotiation of norms and contracts using a rich communication
language. These agents’ world models are manipulated by the
agent’s proactive and reactive machinery using ideas from in-
formation theory. These agents are aware of the value of the
information in every utterance, the semantic relationship be-
tween utterances as well as their utilitarian value. Summary
measures that generalise trust, reliability and reputation illus-
trate the strength of the architecture. These agents’ negotia-
tion strategies are based on three dimensions: informational,
rational and semantic.
Acknowledgments Carles Sierra has been supported by the
EU funded STREP project OpenKnowledge and the Spanish
MEC project Webi-2. Both authors are supported by the Aus-
tralian ARC Discovery Project ‘The Curious Negotiator’.

References
[Arcos et al., 2005] Josep Lluis Arcos, Marc Esteva, Pablo
Noriega, Juan Antonio Rodr´ıguez, and Carles Sierra. En-
vironment engineering for multiagent systems. Journal
on Engineering Applications of Artiﬁcial Intelligence, 18,
2005.

[Cheeseman and Stutz, 2004] P. Cheeseman and J. Stutz.
Bayesian Inference and Maximum Entropy Methods in Sci-
ence and Engineering, chapter On The Relationship be-
tween Bayesian and Maximum Entropy Inference, pages
445 – 461. American Institute of Physics, Melville, NY,
USA, 2004.

[Debenham, 2004] J. Debenham. Bargaining with informa-
tion.
In N.R. Jennings, C. Sierra, L. Sonenberg, and
M. Tambe, editors, Proceedings Third International Con-
ference on Autonomous Agents and Multi Agent Systems
AAMAS-2004, pages 664 – 671. ACM Press, New York,
July 2004.

[Faratin et al., 2003] P. Faratin, C. Sierra, and N. Jennings.
Using similarity criteria to make issue trade-offs in au-
tomated negotiation.
Journal of Artiﬁcial Intelligence,
142(2):205–237, 2003.

[Jennings et al., 2001] Nick Jennings,

Peyman Faratin,
Alessandro Lomuscio, Simon Parsons, Carles Sierra, and
Mike Wooldridge. Automated negotiation: Prospects,
methods and challenges. International Journal of Group
Decision and Negotiation, 10(2):199–215, 2001.

[Kalfoglou and Schorlemmer, 2003] Yannis Kalfoglou and
IF-Map: An ontology-mapping
Marco Schorlemmer.
In Stefano
method based on information-ﬂow theory.
Spaccapietra, Sal March, and Karl Aberer, editors, Jour-
nal on Data Semantics I, volume 2800 of Lecture Notes in
Computer Science, pages 98–127. Springer-Verlag: Hei-
delberg, Germany, 2003.

[Kraus, 1997] S. Kraus. Negotiation and cooperation in
multi-agent environments. Artiﬁcial Intelligence, 94(1–
2):79–97, 1997.

[Li et al., 2003] Yuhua Li, Zuhair A. Bandar, and David
McLean. An approach for measuring semantic similar-
ity between words using multiple information sources.
IEEE Transactions on Knowledge and Data Engineering,
15(4):871 – 882, July / August 2003.

[MacKay, 2003] D. MacKay. Information Theory, Inference
and Learning Algorithms. Cambridge University Press,
2003.

[Paris, 1999] J. Paris. Common sense and maximum entropy.

Synthese, 117(1):75 – 93, 1999.

[Rosenschein and Zlotkin, 1994] J. S. Rosenschein and
The MIT Press,

Rules of Encounter.

G. Zlotkin.
Cambridge, USA, 1994.

[Sierra and Debenham, 2006] C. Sierra and J. Debenham.
Trust and honour in information-based agency.
In Pro-
ceedings Fifth International Conference on Autonomous
Agents and Multi Agent Systems AAMAS-2006, Hakodate,
Japan, May 2006. ACM Press, New York.

IJCAI-07

1518


A Subspace Kernel for Nonlinear Feature Extraction

Mingrui Wu, Jason Farquhar

Max Planck Institute for Biological Cybernetics, 72076 T¨ubingen, Germany

{ﬁrstname.lastname}@tuebingen.mpg.de

Abstract

Kernel based nonlinear Feature Extraction (KFE)
or dimensionality reduction is a widely used pre-
processing step in pattern classiﬁcation and data
mining tasks. Given a positive deﬁnite kernel func-
tion, it is well known that the input data are implic-
itly mapped to a feature space with usually very
high dimensionality. The goal of KFE is to ﬁnd
a low dimensional subspace of this feature space,
which retains most of the information needed for
classiﬁcation or data analysis. In this paper, we pro-
pose a subspace kernel based on which the feature
extraction problem is transformed to a kernel pa-
rameter learning problem. The key observation is
that when projecting data into a low dimensional
subspace of the feature space, the parameters that
are used for describing this subspace can be re-
garded as the parameters of the kernel function be-
tween the projected data. Therefore current kernel
parameter learning methods can be adapted to op-
timize this parameterized kernel function. Exper-
imental results are provided to validate the effec-
tiveness of the proposed approach.

Introduction

Suppose that we are given a set of n data points, {xi}n

1
Feature extraction or dimensionality reduction is a widely
used pre-processing step for classiﬁcation and data mining
tasks, since extracting proper features can reduce the effect
of noise and remove redundant information in the data that is
irrelevant to the classiﬁcation or data analysis tasks.
i=1,
d is the input data, X is the input space.
where xi ∈ X ⊂ R
Traditional feature extraction approaches, such as the Prin-
ciple Component Analysis (PCA) and Linear Discriminant
Analysis (LDA) are linear methods and they project the input
data xi into a low dimensional subspace of the input space X .
Recently, constructing nonlinear algorithms based on the
kernel methods [Sch¨olkopf and Smola, 2002] have proved
successful. For a given positive deﬁnite kernel function
K : X × X → R, the input data xi, 1 ≤ i ≤ n are im-
plicitly mapped to a feature space F with usually very high
dimensionality. Let φ(·) denote the map from X to F, then

K(xi, xj) = (cid:6)φ(xi), φ(xj)(cid:7),

1 ≤ i, j ≤ n

A kernel based algorithm essentially applies linear methods
in F for the mapped data {(φ(xi)}n
i=1. For example, in
the Kernel Principal Component Analysis (KPCA) algorithm
[Sch¨olkopf and Smola, 2002], PCA is used to extract a repre-
sentative subspace of F. Compared with the traditional linear
approaches, kernel methods are more powerful since they can
explore nonlinear structures of the data, and more ﬂexible as
we can recover the linear algorithms by simply using the lin-
ear kernel in the kernel based methods.
Usually the dimensionality of F is very high or even inﬁ-
nite, which is helpful for separating different classes of data.
However, such a high dimensional space F may contain some
redundancy that is irrelevant or even noisy for the given clas-
siﬁcation or data mining tasks. Hence, as is the case for fea-
ture extraction in the input space, it may be also helpful for
classiﬁcation or data mining tasks to ﬁnd a lower dimensional
subspace S of F.
Many Kernel based Feature Extraction (KFE) approaches
have been proposed to ﬁnd a lower dimensional subspace S
of the feature space F. For example, KPCA [Sch¨olkopf and
Smola, 2002] is widely used for this task. As mentioned
above, it essentially performs linear PCA in the feature space
F. The goal is to ﬁnd directions along which the data vari-
ance is the largest.

In this paper, we discuss feature extraction methods with
the focus on improving the classiﬁcation accuracy.
In the
c-class classiﬁcation problem, each data point xi is associ-
ated with a label yi ∈ R
c, where yi = [yi1, . . . , yic](cid:2), and
yik = 1 (1 ≤ k ≤ c) if xi belongs to class k, and 0 oth-
erwise.1 It can be seen that KPCA may be not effective for
classiﬁcation problems since it is an unsupervised feature ex-
traction method, which ignores the labels of the given data.

Hence several supervised KFE algorithms have been pro-
posed, which make use of both the input data and the corre-
sponding labels. Like KPCA, they also perform linear feature
extraction or linear dimensionality reduction in the feature
space F.

The Kernel Fisher Discriminant Analysis (KFDA) [Mika
et al., 2001] aims to ﬁnd a data projection by minimizing
the within-class variance and maximizing the between-class
variance simultaneously, thus achieving good discrimination
1Other strategies for constructing the label yi (1 ≤ i ≤ n) are

also possible.

IJCAI-07

1125

between different classes. An efﬁcient variant of KFDA
based on QR decomposition, called AKDA/QR, is proposed
in [Xiong et al., 2005]. A distinct property of AKDA/QR is
that it scales as O(ndc). And in AKDA/QR, the number of
features extracted is ﬁxed to the number of classes.

The Partial Least Squares (PLS) algorithm [Wold, 1975]
has been widely applied in the domain of chemometrics. Un-
like the PCA algorithm, which extracts features only based
on the variance of the input data, the PLS algorithm uses the
covariance between the inputs and the labels to guide the ex-
traction of features. The Kernel PLS (KPLS) algorithm is
proposed in [Rosipal and Trejo, 2001].

The Orthogonal Centroid (OC) [Park and Park, 2004] al-
gorithm is a linear dimensionality reduction method that pre-
serves the cluster structure in the data. In this algorithm, the
given data are ﬁrstly clustered, and then projected into a space
spanned by the centroids of these clusters. An orthogonal
basis of this subspace is computed by applying QR decom-
position to the matrix whose columns consist of the cluster
centroids. In [Kim et al., 2005], this method is applied for
dimensionality reduction in text classiﬁcation tasks and ex-
hibits good results. Its kernel based nonlinear extension, i.e.
the Kernel Orthogonal Centroid (KOC) algorithm is also pre-
sented in [Park and Park, 2004]. To incorporate the label in-
formation, the KOC (and OC) algorithm treats input data in
the same class as one single cluster, therefore the number of
extracted features equals the number of classes. However this
method can be easily extended by allowing more clusters in
each class.

In this paper, we propose a subspace kernel, based on
which the nonlinear feature extraction problem can be trans-
formed into a kernel parameter learning problem.

The rest of this paper is organized as follows. In section 2,
we propose the basic idea of our approach and formulate the
subspace kernel. Some connections to the related methods are
described in section 3. In section 4, we present one possible
way to optimize the proposed subspace kernel. Experimental
results are provided in section 5 and we conclude the paper in
the last section.

2 Nonlinear Feature Extraction via Kernel

Parameter Learning

2.1 Basic Idea
As mentioned before, a given positive deﬁnite kernel K im-
plicitly introduces a mapping of the given data φ(xi), 1 ≤
i ≤ n, to a usually high dimensional feature space F. When
projecting φ(xi) (1 ≤ i ≤ n) into a subspace S of F, the
kernel function has to be modiﬁed correspondingly since the
feature space has changed from F to S. For convenience,
we call this modiﬁed kernel function the subspace kernel. As
will be shown later, the parameters that are used for describ-
ing S are also the parameters of the corresponding subspace
kernel. Therefore current kernel parameter learning methods
can be adapted to optimize this kernel function. This way we
can ﬁnd a discriminating subspace S where different classes
of data are well separated. In the following, we will explain
the above idea in detail by formulating the aforementioned
subspace kernel.

2.2 The Subspace Kernel
Suppose S is an nf dimensional subspace of F and O =
] is a matrix whose columns constitute an orthog-
[o1, . . . , onf
onal basis of S. Let T denote the subspace spanned by the
mapped data φ(xi) (1 ≤ i ≤ n) in F, then each oi can be
uniquely decomposed into two parts, one is contained in T
and the other one is in the orthogonal complement of T ,

ok = o(cid:3)
k ∈ T and (cid:6)o⊥

k + o⊥
k ,
k , φ(xi)(cid:7) = 0 for 1 ≤ i ≤ n. Therefore

where o(cid:3)
for any φ(xi), its projection into S can be computed as

1 ≤ k ≤ nf

(1)

O(cid:2)φ(xi) = (O(cid:3))(cid:2)φ(xi)
1, . . . , o(cid:3)

nf ].2

where O(cid:3) = [o(cid:3)
Equation (1) indicates that to compute the projection of
φ(xi) in S, it is enough to only consider the case where S
is a subspace of T , which implies that any vector in S can
be expressed as a linear combination of φ(xi), 1 ≤ i ≤ n.
∈ S, let Z denote
Therefore, for any nf vectors z1, . . . , znf
[z1, . . . , znf
], and X denote [φ(x1), . . . , φ(xn)], then Z can
be written as
(2)
where W = [wik] ∈ R
n×nf is a matrix of combination coef-
ﬁcients.
Moreover, if z1, . . . , znf are linearly independent, then the
nf dimensional subspace S can be spanned by these nf vec-
tors. Thus the elements of W introduce a subspace S of F,
for which we have the following lemma.
Lemma 1. When projecting the data φ(xi) into S, the kernel
matrix of the projected data in S can be computed as,3

Z = XW

Kw = (X(cid:2)Z)(Z(cid:2)Z)−1(X(cid:2)Z)(cid:2)

= (KW)(W(cid:2)KW)−1(KW)(cid:2)

(3)
(4)
where K = [kij] ∈ R
n×n is the kernel matrix of the input
data, i.e. kij = K(xi, xj).
Proof. For any φ(xi), 1 ≤ i ≤ n, in order to calculate its
projection into the subspace S, spanned by the columns of
Z = XW, we need an orthogonal basis U of S. We build U
as follows:

(5)
In the above equation, T is computed as follows: Assume
Kz = Z(cid:2)Z then
where Λ ∈ R
matrix Kz, and V ∈ R
eigenvectors of Kz. Equation (6) leads to

(6)
nf×nf is a diagonal matrix of eigenvalues of
nf×nf is a matrix whose columns are

T = VΛ− 1

U = ZT

2

K−1
z = TT(cid:2)

(7)

2More precisely, the result of equation (1) is the coordinate of
the projection of φ(xi) in S. As is widely done in the literature of
feature extraction and dimensionality reduction, this coordinate will
be used as the extracted features for classiﬁcation.

3Here the “kernel matrix of the projected data” refers to the ma-
trix whose elements equal the inner product of the projected data in
S.

IJCAI-07

1126

(10)

(11)

and

T(cid:2)KzT = I

(8)
where I is the unit matrix. The following equation follows
from (5) and (8),

U(cid:2)U = T(cid:2)KzT = I

So the columns of U form an orthogonal basis of the subspace
S.
Thus, for φ(xi) ∈ F, 1 ≤ i ≤ n, their projections into the
subspace S can be computed as

Xw = U(cid:2)X = T(cid:2)Z(cid:2)X

(9)
where Xw is the matrix whose columns are the projections of
φ(xi) in S, 1 ≤ i ≤ n.

Having obtained the projected data Xw, we can now com-
pute the inner product between points in the subspace as the
following:

Kw = X(cid:2)

wXw = X(cid:2)UU(cid:2)X

z (X(cid:2)Z)(cid:2)

= X(cid:2)ZTT(cid:2)Z(cid:2)X
= (X(cid:2)Z)K−1
= (X(cid:2)Z)(Z(cid:2)Z)−1(X(cid:2)Z)(cid:2)
= (X(cid:2)XW)(W(cid:2)X(cid:2)XW)−1(X(cid:2)XW)(cid:2)
= (KW)(W(cid:2)KW)−1(KW)(cid:2)

(12)
where we used equation (5) in the second line, equation (7)
in the third line and equation (2) in the ﬁfth line. The equa-
tions (11) and (12) are identical to (3) and (4) respectively,
therefore the lemma is proven.
The proof also tells that for a given W, the projection of the
data into the subspace S introduced by W can be computed
as equation (9).
Let Kw(·,·) denote corresponding subspace kernel func-
tion. Then according to (3) and (4), for any x, x(cid:5) ∈ X , the
subspace kernel Kw(·,·) between them can be computed as
(13)
= ψ(x)(cid:2)W(W(cid:2)KW)−1W(cid:2)ψ(x(cid:5))(14)

Kw(x, x(cid:5)) = φ(x)(cid:2)Z(Z(cid:2)Z)−1Z(cid:2)φ(x(cid:5))

where

ψ(x) = [K(x, x1), . . . , K(x, xn)](cid:2)

is the empirical kernel map [Sch¨olkopf and Smola, 2002].
Equation (14) illustrates that the elements of W, by which
the subspace S is described, also serve as the kernel parame-
ters of Kw(·,·). So in order to ﬁnd a discriminating subspace
S where different classes of data are well separated, we can
turn to optimize the corresponding subspace kernel Kw.

3 Connections to Related Work
3.1 Feature Selection via Kernel Parameter

Learning

In [Weston et al., 2000; Chapelle et al., 2002], kernel pa-
rameter learning approaches are adopted for feature selection
problem. The kernel of the following form is considered

Kθ(u, v) = K(θ. ∗ u, θ. ∗ v)

(15)

Namely,

where .∗ denotes the component-wise product between two
for θ = [θ1, . . . , θd](cid:2) and u =
vectors.
[u1, . . . , ud](cid:2), θ. ∗ u = [θ1u1, . . . , θdud](cid:2). By optimizing
the kernel parameter θ with margin maximization or Radius-
Margin bound [Chapelle et al., 2002] minimization, and with
a 1-norm or 0-norm penalizer on θ, feature selection can be
done by by choosing the features corresponding to the large
elements of the optimized θ.
Feature selection locates a discriminating subspace of the
input space X . Similarly as the above approaches, we also
use kernel parameter learning algorithms to ﬁnd a discrim-
inating subspace. However, in this paper, we address the
problem of feature extraction but not feature selection, and
the subspace we want to ﬁnd is contained in the feature space
F but not the input space X .
3.2 Sparse Kernel Learning Algorithms
The subspace kernel function given by (14) is in a general
form. As described before, each column in the matrix Z =
] (c.f (2)) is a vector in the feature space F. Now
[z1, . . . , znf
we show that this kernel relates to the work of [Wu et al.,
2005] in the special case where each column of Z has a pre-
image [Sch¨olkopf and Smola, 2002] in the input space X .
That is, for each zi ∈ F, there exists a vector ˆzi ∈ X , such
that zi = φ(ˆzi). So now the subspace S can be spanned by
φ(ˆz1), . . . , φ(ˆznf

For convenience, let ˆZ = [φ(ˆz1), . . . , φ(ˆznf

)] (note that
ˆZ = Z). Then in this case, according to (13), the subspace
kernel function now becomes:

).

Kw(x, x(cid:5)) = φ(x)(cid:2) ˆZ(ˆZ(cid:2) ˆZ)−1 ˆZ(cid:2)φ(x(cid:5))

= ψˆz(x)K−1

ˆz ψˆz(x(cid:5))

where ψˆz(x) = φ(x)(cid:2) ˆZ = [K(x, ˆz1), . . . , K(x, ˆznf
and Kˆz = ˆZ(cid:2) ˆZ.

In [Wu et al., 2005], an algorithm for building Sparse
Large Margin Classiﬁers (SLMC) is proposed, which builds
a sparse Support Vector Machine (SVM) [Vapnik, 1995] with
nf expansion vectors, where nf is an given integer. In [Wu
et al., 2005], it is pointed out that building an SLMC is
equivalent to building a standard SVM with the kernel func-
tion computed as (16). And the SLMC algorithm essentially
ﬁnds an nf dimensional subspace of F, which is spanned by
φ(ˆz1), . . . , φ(ˆznf
), and where the different classes of data are
linearly well separated.

In [Wu et al., 2005], the kernel function (16) is obtained
with the Lagrange method, which is different from the one
adopted in the above. And the kernel function (16) is a special
case of the subspace kernel (14). Therefore it can be seen
that based on the general subspace kernel (14), useful special
cases can be derived for some applications.

(16)
)](cid:2),

4 Optimizing Kw
We optimize Kw based on the Kernel-Target Alignment
(KTA) [Cristianini et al., 2002], which is a quantity to mea-
sure the degree of ﬁtness of a kernel for a given learning task.
In particular, we compute W by solving the following KTA

IJCAI-07

1127

maximization problem:

(cid:6)Kw, Ky(cid:7)F

Ky = Y(cid:2)Y

n×nf

max

W∈R

A(W) =

(cid:2)(cid:6)Kw, Kw(cid:7)F(cid:6)Ky, Ky(cid:7)F

(17)
where (cid:6)·,·(cid:7)F denotes the Frobenius product between two ma-
trices that are of the same size, i.e. for any two equally sized
matrices M and N, (cid:6)M, N(cid:7)F =
In (17),
Ky ∈ R
n×n is the gram matrix between the labels, deﬁned
by

ij MijNij.

(cid:3)

(18)
where Y = [y1, . . . , yn] ∈ R
c×n, and yi is the label of xi,
1 ≤ i ≤ n.
The elements in Ky reﬂect the similarities between labels,
ij equals 1 if xi and xj belong to the same class, and 0
as Ky
otherwise. Therefore ’aligning’ Kw with Ky will make the
similarities between the data points in the same class higher
than the similarities between the points in different classes.
Thus by maximizing A(W), we can ﬁnd a subspace of F,
where points in the same class are closer to each other than
those in different classes. Hence a good classiﬁcation perfor-
mance can be expected for the data projected into this sub-
space.

Note that the subspace kernel allows us to apply many ker-
nel parameter learning algorithms to the feature extraction
problem. Therefore apart from KTA, we can also choose
other approaches to compute W, such as the one based on
the Radius-Margin Bound [Chapelle et al., 2002]. For sim-
plicity, we use KTA in this paper.

Gradient based algorithms can be used to maximize
A(W). In our implementation, we use the conjugate gradient
algorithm to solve problem (17). To compute A(W), we uti-
lize the fact that Kw = X(cid:2)
wXw (see (10)) and Ky = Y(cid:2)Y
(see (18)). Thus, we can decompose Kw and Ky as follows

Kw =

Ky =

nf(cid:4)

i=1

c(cid:4)

j=1

ˆxiˆx(cid:2)

i

ˆyj ˆy(cid:2)

j

(19)

(20)

where ˆxi ∈ R
and ˆyj ∈ R

n (1 ≤ i ≤ nf ) denotes the i-th column of X(cid:2)
n (1 ≤ j ≤ c) denotes the j-th column of Y(cid:2).

w

Based on the above two equations, we have

(cid:6)Kw, Ky(cid:7)F =

(cid:6)Kw, Kw(cid:7)F =

nf(cid:4)

c(cid:4)

i=1

nf(cid:4)

j=1

nf(cid:4)

i=1

j=1

(ˆx(cid:2)

i ˆyj)2

(ˆx(cid:2)

i ˆxj)2

(21)

(22)

Equation (21) and (22) can be computed with time com-
plexity O(ncnf ) and O(nn2
f ) respectively. When both nf
and c are small, they are more efﬁcient than computing the
Frobenius product directly, which requires time complexity
of O(n2).

Similarly, to compute ∇A(W), we can use the following

equations:

(cid:6) ∂Kw
∂wuv
(cid:6) ∂Kw
∂wuv

, Ky(cid:7)F =

, Kw(cid:7)F =

c(cid:4)

i=1

nf(cid:4)

j=1

i ( ∂Kw
ˆy(cid:2)
∂wuv
j ( ∂Kw
ˆx(cid:2)
∂wuv

)ˆyi

)ˆxj

(23)

(24)

where wuv (1 ≤ u ≤ n, 1 ≤ v ≤ nf ) is the element of
W. Inspired by (23) and (24), we investigate how to compute
α(cid:2)( ∂Kw
n is an arbitrary vector. Actually,
∂wuv
by performing linear algebra straightforwardly, we have

)α, where α ∈ R

α(cid:2) ∂Kw
∂wuv

α = 2tuβv

where βv is the v-th element of a vector β, computed as

β = (W(cid:2)KW)−1(W(cid:2)K)α

(25)

(26)

and in (25), tu is the u-th element of a vector t, deﬁned as:

t = Kα − KWβ

(27)
Note that for any given α, the vectors β and t need to be
computed only once, according to (26) and (27) respectively,
α can be calculated as (25) for 1 ≤ u ≤ n and
then α(cid:2) ∂Kw
1 ≤ v ≤ nf . Now we can apply (25) to (23) and (24), and
∇A(W) can be calculated.

∂wuv

5 Experimental Results
5.1 Experimental Settings
We empirically investigate the performance of the follow-
ing KFE algorithms on classiﬁcation tasks: KPLS, KOC,
AKDA/QR and the proposed Subspace Kernel based Feature
Extraction (SKFE) method. Following the same scheme in
[Xiong et al., 2005], the features extracted by each KFE al-
gorithm are input to a 1-Nearest Neighbor (1-NN) classiﬁer,
and the classiﬁcation performance on the test data is used to
evaluate the extracted features. As a reference, we also re-
port the classiﬁcation results of the 1-NN algorithm using the
input data directly without KFE.

As mentioned before, in a c-class classiﬁcation problem,
the number of features nf extracted by both AKDA/QR and
KOC is ﬁxed at c. To compare with these two algorithms,
the value of nf for SKFE is also set to c in the experiments,
although the number of features extracted by SKFE can be
varied. For KPLS, three different values of nf are tried: c/4,
c/2 and c. The best results are reported for KPLS.4

For our proposed SKFE algorithm, the function A(W) in
(17) is not convex, so the optimization result depends on the
initial choice of W. To get a good initial guess, we can use
the subspaces found by other KFE algorithms for initializa-
tion. In the experiments, for efﬁciency we use the KOC algo-
rithm to compute the initial W.

4When c = 2, only two values of nf are tried for KPLS: 1 and

2.

IJCAI-07

1128

5.2 Experiments on Microarray Gene Expression

Data

In this subsection, we take seven microarray gene datasets
to test various KFE methods: Brain Tumor1, Brain Tu-
mor2, Leukemia1, Leukemia2, Prostate Tumor, DLBCL and
11 Tumors.5 Descriptions of these datasets are presented in
Table 1. As shown in Table 1, a typical characteristic of these
datasets is that the number of data n is much smaller than the
data dimensionality d.

Table 1: Datasets adopted in the experiments. The ﬁrst seven
are microarray gene datasets, while the last seven are text
datasets. For each of them, the number of data n, the di-
mensionality d and the number of classes c are provided.

Dataset
B.Tumor1
B.Tumor2
Leukemia1
Leukemia2
P.Tumor
DLBCL
11 Tumors

tr11
tr23
tr41
tr45
la1
la2

hitech

type
GENE
GENE
GENE
GENE
GENE
GENE
GENE
TEXT
TEXT
TEXT
TEXT
TEXT
TEXT
TEXT

n
90
50
72
72
102
77
174
414
204
878
690
3204
3075
2301

d

5920
10367
5327
11225
10509
5469
12534
6424
5832
7454
8261
31472
31472
10080

c
5
4
3
3
2
2
11
9
6
10
10
6
6
6

A Gaussian kernel is used in the experiments:
K(x, x(cid:5)) = exp(−γ (cid:10) x − x(cid:5) (cid:10)2)

(28)

Five fold cross validation is conducted for parameter selec-
tion, and the best cross validation error rate is used to mea-
sure the performance of different algorithms. The experiment
is repeated 20 times independently. And the results in Ta-
ble 2 show the mean cross validation error and the standard
deviation over these 20 runs.

From Table 2, we can observe that SKFE and KPLS com-
pare favorably to the other KFE algorithms.
In particular,
SKFE improves the results of KOC algorithm in all cases,
although KOC is used to initialize SKFE. It can also be
seen that SKFE and KPLS are competitive with each other.
They are are not signiﬁcantly different (judged by t-test) on
Leukemia1, Leukemia2, DLBCL and 11 Tumors, and KPLS
is better than SKFE on Brain Tumor2, while SKFE outper-
forms KPLS on Brain Tumor1 and Prostate Tumor.

5.3 Experiments on Text Classiﬁcation
In this subsection, we investigate different KFE methods on
the text classiﬁcation task.
It has been observed that there
usually exist cluster structures in the text data. The OC al-
gorithm (or equivalently the KOC algorithm with the linear
kernel), which can keep these structures, is used for dimen-
sionality reduction in text classiﬁcation tasks in [Kim et al.,
2005] and exhibits good results.

5They are available at http://www.gems-system.org.

Seven text datasets from the TREC collections are adopted:
tr11, tr23, tr41, tr45, la1, la2 and hitech. More information
about these seven datasets are available at Table 1.

Similar to the microarray gene data, the data used in text
classiﬁcation tasks are also of very high dimensionality. An-
other characteristic of these seven datasets is that they are
highly unbalanced, which means that the number of data con-
tained in different classes are quite different. For example, in
the tr11 dataset, there are 132 data points contained in the
seventh class, while just 6 data points in the ninth class, only
4.6% of the former.

On each dataset, we randomly select half of the data from
each class to form the training set and use the remaining data
for test. As is done in the OC algorithm, the linear kernel is
used in this set of experiments. Similarly as before, for each
dataset, the experiment is repeated independently 20 times.
The average test error and the standard deviation over these
20 runs are reported in Table 3.

Table 3 illustrates that SKFE outperforms other KFE meth-
ods on most datasets. Also it can be seen from both Table 2
and 3 that in most cases, all the KFE algorithms obtain bet-
ter performances than the 1-NN algorithm with the raw data,
whilst reducing the data dimensionality dramatically from d
to nf , where nf << d. (c.f. section 5.1 for the choice of nf .)
Although SKFE compares favorably to the other KFE
methods in terms of the classiﬁcation accuracy, its compu-
tational cost is higher than the others. For the problems re-
ported in Table 1, on a 2.2 GHz Pentium-4 PC, KPLS requires
from 0.15 to 39 seconds, AKDA/QR takes between 0.35 and
3 seconds, KOC requires between 0.11 and 5 seconds, while
SKFE takes between 0.38 to 69 seconds. The optimization
step of SKFE is implemented in C++, and the others are im-
plemented in Matlab.

6 Conclusion
We have presented a subspace kernel based on which nonlin-
ear feature extraction can be conducted by kernel parameter
learning. Connections to related work have been explained.
In particular, the comparison with the Spare Large Margin
Classiﬁer (SLMC) [Wu et al., 2005] illustrates that useful
special cases can be derived from the proposed subspace ker-
nel for some applications. We have also described a method
to optimize the subspace kernel by Kernel-Target Alignment
(KTA) [Cristianini et al., 2002] maximization. But other ker-
nel parameter learning approaches can also be applied. Fi-
nally, experimental results have been provided to validate the
effectiveness of our approach.

References
[Chapelle et al., 2002] O. Chapelle, V. Vapnik, O. Bousquet,
and S. Mukherjee. Choosing multiple parameters for sup-
port vector machines. Machine Learning, 46(1-3):131–
159, 2002.

[Cristianini et al., 2002] N. Cristianini,

J. Shawe-Taylor,
A. Elisseeff, and J. Kandola. On kernel-target alignment.
In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,
Advances in Neural Information Processing Systems 14,
Cambridge, MA, 2002. MIT Press.

IJCAI-07

1129

Table 2: Average cross validation error rates (%) and the standard derivations (%) on the seven microarray gene datasets. For
each dataset, the results shown in boldface are signiﬁcantly better than the others, judged by t-test, with a signiﬁcance level of
0.01.

Dataset
B.Tumor1
B.Tumor2
Leukemia1
Leukemia2
P.Tumor
DLBCL
11 Tumors

1-NN

14.33±1.65
28.90±2.79
11.94±2.13
6.74±1.64
24.17±1.94
14.03±1.72
16.52±1.09

KPLS

14.28±1.26
24.70±3.57
5.21±1.49
3.96±2.27
19.80±1.87
4.09±1.21
11.44±0.85

AKDA/QR
13.89±1.42
26.70±3.63
7.50±1.65
7.01±1.77
28.82±2.59
14.74±1.80
12.70±0.89

KOC

14.83±1.58
28.30±2.99
7.57±1.88
7.36±1.81
20.88±2.38
9.22±1.93
15.11±1.01

SKFE

12.00±1.42
27.30±2.27
4.86±1.46
3.68±0.82
14.85±1.63
3.57±0.93
11.84±0.96

Table 3: Average test error rates (%) and the standard deviations (%) on the seven text datasets. For each dataset, the results
shown in boldface are signiﬁcantly better than the others, judged by t-test, with a signiﬁcance level of 0.01.

Dataset

tr11
tr23
tr41
tr45
la1
la2

hitech

1-NN

26.66±2.64
28.25±4.76
20.17±3.02
28.48±2.68
40.82±1.68
38.82±1.59
57.83±1.69

KPLS

21.71±2.61
30.05±6.35
9.56±1.55
23.45±2.24
19.23±0.69
16.94±0.76
33.14±1.57

AKDA/QR
23.63±4.32
27.30±4.88
9.59±1.40
19.33±3.84
18.67±0.77
16.25±1.02
31.88±1.08

KOC

15.29±3.14
23.20±4.40
7.66±0.96
15.23±2.15
18.28±0.82
16.36±0.94
31.71±1.41

SKFE

15.78±2.47
18.85±3.86
6.14±1.08
9.85±1.80
14.51±0.96
13.23±0.74
29.71±1.12

[Xiong et al., 2005] T. Xiong, J. Ye, Q. Li, R. Janardan, and
V. Cherkassky. Efﬁcient kernel discriminant analysis via
QR decomposition. In L. K. Saul, Y. Weiss, and L. Bottou,
editors, Advances in Neural Information Processing Sys-
tems 17, pages 1529–1536. MIT Press, Cambridge, MA,
2005.

[Kim et al., 2005] H. Kim, P. Howland, and H. Park. Dimen-
sion reduction in text classiﬁcation with support vector
machines. Journal of Machine Learning Research, 6:37–
53, 2005.

[Mika et al., 2001] S. Mika, G. R¨atsch, and K. R. M¨uller. A
mathematical programming approach to the kernel ﬁsher
algorithm. In T. K. Leen, T. G. Dietterich, and V. Tresp,
editors, Advances in Neural Information Processing Sys-
tems 13, Cambridge, MA, 2001. The MIT Press.

[Park and Park, 2004] C. H. Park and H. Park. Nonlinear
feature extraction based on centroids and kernel functions.
Pattern Recognition, 37:801–810, 2004.

[Rosipal and Trejo, 2001] R. Rosipal and L. J. Trejo. Ker-
nel partial least squares regression in reproducing kernel
hilbert space.
Journal of Machine Learning Research,
2:97–123, 2001.

[Sch¨olkopf and Smola, 2002] B. Sch¨olkopf and A. J. Smola.
Learning with Kernels. The MIT Press, Cambridge, MA,
2002.

[Vapnik, 1995] V. Vapnik. The Nature of Statistical Learning

Theory. Springer Verlag, New York, 1995.

[Weston et al., 2000] J. Weston, S. Mukherjee, O. Chapelle,
M. Pontil, T. Poggio, and V. Vapnik. Feature selection
for svms. In Advances in Neural Information Processing
Systems 12, Cambridge, MA, 2000. MIT Press.

[Wold, 1975] H. Wold. Soft modeling by latent variables;
In
the nonlinear iterative partial least squares approach.
J. Gani, editor, Perspectives in Probability and Statistics,
pages 520–540, London, 1975. Academic Press.

[Wu et al., 2005] M. Wu, B. Sch¨olkopf, and G. Bakir. Build-
In L. D. Raedt and
ing sparse large margin classiﬁers.
S. Wrobel, editors, Proc. 22th International Conference
on Machine Learning, pages 1001–1008. ACM, 2005.

IJCAI-07

1130


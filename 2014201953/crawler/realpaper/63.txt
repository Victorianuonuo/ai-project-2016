Probabilistic Reasoning with Hierarchically Structured Variables

Rita Sharma

Department of Computer Science
University of British Columbia

http://www.cs.ubc.ca/spider/rsharma/

rsharma@cs.ubc.ca

David Poole

Department of Computer Science
University of British Columbia

http://www.cs.ubc.ca/spider/poole/

poole@cs.ubc.ca

Abstract

Many practical problems have random variables
with a large number of values that can be hierarchi-
cally structured into an abstraction tree of classes.
This paper considers how to represent and exploit
hierarchical structure in probabilistic reasoning. We
represent the distribution for such variables by spec-
ifying, for each class, the probability distribution
over its immediate subclasses. We represent the
conditional probability distribution of any variable
conditioned on hierarchical variables using inher-
itance. We present an approach for reasoning in
Bayesian networks with hierarchically structured
variables that dynamically constructs a ﬂat Bayesian
network, given some evidence and a query, by col-
lapsing the hierarchies to include only those values
necessary to answer the query. This can be done
with a single pass over the network. We can answer
the query from the ﬂat Bayesian network using any
standard probabilistic inference algorithm such as
variable elimination or stochastic simulation. The
domain size of the variables in the ﬂat Bayesian net-
work is independent of the size of the hierarchies;
it depends on how many of the classes in the hierar-
chies are directly associated with the evidence and
query. Thus, the representation is applicable even
when the hierarchy is conceptually inﬁnite.

1 Introduction
Many problem domains have discrete variables with a large
number of values that can be represented a priori as an abstrac-
tion hierarchy (or an is-a hierarchy or taxonomic hierarchy)
[Pearl, 1986; Mackworth et al., 1985]. We call these vari-
ables hierarchical variables. Taxonomic hierarchies allow us
to manage effectively the large state space of a variable be-
cause they allow facts and regularities to be revealed both at
high and low levels of abstraction. In taxonomic hierarchies
the information from high-level abstractions are automatically
inherited by more speciﬁc concepts. Abstraction hierarchies
also allow us to answer more abstract queries. As an example
of a hierarchical variable, consider a random variable LT that
describes the species of living things. The large number of

values of LT (i.e., millions of species) can be classiﬁed ac-
cording to Linnaean taxonomy hierarchy.1 Living things are
divided into kingdoms (e.g., plantae, animalia), classes (e.g.,
mammals, birds, ﬁsh), all the way down to species.

In this paper, we look at two related problems with hierar-
chical variables in Bayesian networks. The ﬁrst is the compact
representation of conditional probability distributions. The
second is how to exploit that representation in probabilistic
inference for computational gain. We assume that the tree hi-
erarchy of the values is ﬁxed and does not change with the
context.

To compute a posterior probability, given some evidence
in a Bayesian network that has both simple and hierarchi-
cal variables, we construct a ﬂat Bayesian network by col-
lapsing the hierarchies and including only those values neces-
sary to answer the query. We can answer the query from the
ﬂat Bayesian network using any standard inference algorithm.
The domain size of the variables in the ﬂat Bayesian network is
independent of the size of the hierarchies; it depends on how
many of the classes in the hierarchy are directly associated
with the evidence and query.

2 Hierarchical Variables
We divide the discrete random variables into two cate-
gories: simple variables and hierarchical variables. We
call Bayesian networks that have both simple and hierarchical
variables hierarchical Bayesian networks.2 We use upper case
letters to denote simple random variables (e.g., X1, X2, X) and
the actual value of these variables by small letters (e.g., a, b,
x1). The domain of X, written Val (X), is the set of values that
X can take on.

A hierarchical variable is a variable in which subsets of the
values of the variable are represented hierarchically in a tree
[Pearl, 1986]. We refer to a node in the tree as a class. The
nodes in the tree represent subsets of the values of the variable.
The root of the tree represents the set of all the values of the
variable. The children of a node correspond to a partitioning
of the set of values of the node. Thus, the subsets represented
by the children of a class must be mutually disjoint and any
class represents the union of its children in the tree.
1http : //www.wikipedia.org/wiki/Linnaean_taxonomy
2The term hierarchical Bayesian network used in this paper is
different than the term hierarchical Bayesian model used in statistics.

livingthing

animal

plant

reptile

mammal

insect

fish

bird

platypus

cat

bat

sparrow

penguin

Figure 1: Part of a taxonomic hierarchy of living things

If a class Cj is a child of Ci, we say that Cj is an immediate
subclass of Ci. The subclass relation is the symmetric transi-
tive closure of the immediate subclass relation. That is, Cj is
a subclass of Ci, written Cj ≤ Ci, if Cj is a descendant of Ci in
the abstraction hierarchy (or is Ci). The inverse of subclass is
superclass. The conjunction of two classes Cj and Ck, written
as Cj ∧ Ck, denotes the set intersection of Cj and Ck. This is
empty unless one is a superclass of the other.
We denote the hierarchical variables using boldface upper-
case letters (e.g., LT). For example, a part of the tree hierarchy
of values of variable LT is shown in Figure 1. tree(LT) denotes
the tree hierarchy of LT.

3 Conditional Probabilities of Hierarchical

Bayesian Networks

There are two main issues to be considered to represent hier-
archical Bayesian networks:
C1: specifying the conditional probability for hierarchical

variables;

C2: specifying how variables are conditioned on hierarchical

parents.

The simplest case of C1 is how to specify the prior prob-
ability of a hierarchical variable. The simplest case of C2 is
how to specify the conditional probability of a simple variable
conditioned on a hierarchical variable. The more complicated
cases are built from these simpler cases.

3.1 Prior Probability for Hierarchical Variables
The prior probability of a hierarchical variable can be speci-
ﬁed in a top-down manner. For each class that has subclasses,
we specify a probability distribution over its immediate sub-
classes. For example, suppose we want to specify the prior
over the hierarchical variable LT as shown in Figure 1. The
root of the hierarchy is the class livingthing. We specify the
probability distribution over its immediate subclasses:

P(animal|livingthing) = 0.4
P(plant|livingthing) = 0.6

We can specify the probability distribution over the immedi-
ate subclasses of animal. Suppose we have as part of this
distribution:

P(mammal|animal) = 0.3
P(bird|animal) = 0.2, etc.

We can specify the probability of an immediate subclass of
mammal given mammal, with probabilities such as:

P(bat|mammal) = 0.1

We can compute the prior probability of any class in a recur-
sive manner by multiplying the probability of class given its
immediate superclass and the probability of its immediate su-
perclass. The root has probability 1 (as it represents the set
of all values). For example, given the probabilities as above,
P(bat) can be computed as follows:
P(bat) = P(bat|mammal) × P(mammal|animal) ×

P(animal|livingthing)

= 0.012

In this representation, computing the probability of any class
is linear in the depth of the class in the hierarchy and otherwise
is not a function of the hierarchy’s size.

3.2 Hierarchical Variable Is the Parent of a Simple

Variable

Suppose that H is a hierarchical variable, F is a simple vari-
able, and H is the only parent of F. We can exploit any hier-
archical structure by assuming that the inﬂuence on F is local
in the hierarchy of H. To specify the conditional probabil-
ity distribution (CPD) P(F|H) we use the notion of a default
probability distribution. The default probability distribution
of F for class Cj, written Pd (F|Cj), is assumed to be inherited
by all subclasses of Cj, unless overridden by a more speciﬁc
default probability distribution. We can represent P(F|H) by
specifying Pd (F|Cj) for some classes Cj in the hierarchy of H.
We call such classes Cj the exceptional classes of H. The idea
of using “inheritance” with abstraction hierarchies is similar
to the use of “context speciﬁc independence” for the compact
representation of the CPTs in a Bayesian network [Boutilier
et al., 1996].

To make this properly deﬁned, we need to ensure that every
terminal node in the tree hierarchy of H is covered by some
default probability distribution. The union of all of the sets
of values associated with the exceptional classes must equal
Val(H). This condition holds trivially if the root class is an
exceptional class for F. If the root class is not an exceptional
class, there must be an exceptional class along every path from
the root.
The conditional probability of F for any class Ck of H,
P(F|Ck), can be computed as follows:
• if Ck does not have any exceptional subclasses, P(F|Ck)
is inherited from the default probability distribution of F
for the lowest exceptional superclass Cj of Ck. Thus,

P(F|Ck) = Pd (F|Cj)

• otherwise, P(F|Ck) can be derived from its children
P(F|Ci) × P(Ci|Ck)

P(F|Ck) = (cid:1)

∀Ci∈children(Ck )

Example: Consider a Bayesian network in which a hierarchi-
cal variable LT is the only parent of a simple variable FLYING
with domain {ﬂying,¬ﬂying}. To represent P(FLYING|LT),

we need to deﬁne the default probability distributions over
FLYING for the exceptional classes of LT. For example, we
can state that livingthings have a low probability of ﬂying by
default, but bird and insect are exceptional because they have
a high probability of ﬂying. From the children of class mam-
mal, a bat is exceptional because it has a high probability of
ﬂying. From the children of class bird, a penguin is excep-
tional because it has a very low probability of ﬂying. Thus, to
represent P(FLYING|LT) we could have:

Pd (ﬂying|livingthing) = 0.00001
Pd (ﬂying|bird) = 0.5
Pd (ﬂying|bat) = 0.3
Pd (ﬂying|penguin) = 0.000001
Pd (ﬂying|insect) = 0.4

Note that P(ﬂying|livingthing) (cid:5)= 0.00001 as the class liv-
ingthing contains bird and bat that each have a much higher
probability of ﬂying.

3.3 The General Case
The general case is when a random variable (hierarchical or
simple) can have any number and any kind of variables as
parents. To represent the CPD of a hierarchical variable H
conditioned on its parents, we assume that each class in the
hierarchy of H has a probability distribution over its immediate
subclasses that is conditioned on (some of) the parents of H.
We can treat each of these classes as a simple variable. Thus,
the problem of representing a (conditional) probability dis-
tribution over a hierarchical variable reduces to the problem
of representing a (conditional) probability distribution over
simple variables.

Let F be a simple variable that has both simple and hier-
archical parents. Suppose pas(F) denotes the simple parents
of F and suppose pah(F) denotes the hierarchical parents of
parent of F; we call X a parent context. X = (cid:2)
(cid:3)
F. Let X denote an assignment of a class to each hierarchical
, . . . , cn
,
x
where ci
x is a class in the tree hierarchy of the ith hierarchical
parent of F.

c1
x

To represent P

we specify the default
probability distribution over F for different combinations of
values for simple parents, pas(F), and some parent contexts.
The given parent contexts (parent contexts for which the de-
fault distribution over F has been deﬁned) can be represented
in a concept hierarchy Cp of partial ordering Cp = (Zp,≤),
where Zp is the set of given parent contexts. A parent context
X ∈ Zp is a super parent context of parent context Y, written
Y ≤ X, if ∀i, ci
x. The conditional distribution over F
for a parent context X is inherited from the default probability
distribution of F for the3 super parent context Z of X in Zp
that is not overridden by a more speciﬁc default probability
distribution of F for parent context Y such that X ≤ Y < Z.
Example: Consider the Bayesian network shown in Figure 2

≤ ci

y

(cid:2)

F|pas (F) ∧ pah (F)

(cid:3)

3We assume that there is only one most-speciﬁc super parent con-
text. If there are more than one most-speciﬁc compatible parent con-
texts, with different default probability distributions, there must be a
default distribution that disambiguates the two default distributions.
We are thus forcing the user to disambiguate cases in which there
would otherwise be a problem of multiple inheritance.

S

LOC

location

land

ocean

LT

northern

hemisphere

equatorial

southern

hemisphere

FLYING

LAYS_EGGS

N−temperate

N−polar

S−temperate

S−polar

(a)

Part of tree hierarchy of location on the earth

(b)

Figure 2: (a) A Bayesian network with simple and hierarchical
variables (b) A part of the tree hierarchy for LOC

(a). The hierarchical variable LOC represents the location on
earth. The simple variable S represents the season. Figure 2
(b) shows the part of the hierarchy of the location. We consider
that S has only two values: (northern hemisphere) summer
(May – October) and winter (November – April). The prob-
ability distribution of class animal of LT over its immediate
subclasses can be conditioned on some of its parents. Suppose
the distribution of class animal in the equatorial region is in-
dependent of season, so for parent context X = (equatorial)
we could have a default distribution such as:

Pd (reptile|animal ∧ X) = 0.1
Pd (insect|animal ∧ X) = 0.4
Pd (mammal|animal ∧ X) = 0.2, etc.
In the polar regions the distribution of class animal over its
children depends on the season, so for parent context X =
(N − polar) we could have a default distribution such as:

Pd (reptile|animal ∧ S = summer ∧ X) = 0.01
Pd (mammal|animal ∧ S = summer ∧ X) = 0.04
Pd (insect|animal ∧ S = summer ∧ X) = 0.7, etc.

4 Construction of a Flat Bayesian Network
An observation about a hierarchical variable H can be posi-
tive or negative. An observation about H is positive when we
observe that H is Cobs, where Cobs is a class in the hierarchy
of H. This means that H has a value vh such that vh ∈ Cobs.
An observation about H is negative when we observe that H is
¬Cobs. This means that H has a value vh such that vh (cid:5)∈ Cobs.
Without loss of generality, we can assume that there is always
one positive observation about H, denoted by Cpos. For ex-
ample, suppose we have two positive observations C1 and C2
about H. If classes C1 and C2 are disjoint then observations
about H are not consistent. If C1 is a subclass of C2, C1∧ C2
equals C1. If there are only negative observations about H,
we assume root class is the positive observation. There can be
multiple negative observations about H that are descendants
of Cpos in the hierarchy of H.

We assume a query asks either for a distribution over a
simple variable or for the probability of a particular class in a
hierarchical variable.

Given some evidence (a set of observations) and a query,
we construct a ﬂat Bayesian network, Bf , from a hierarchical
Bayesian network Bh by replacing each hierarchical variable
with a simple variable whose domain includes only those val-
ues necessary to answer the query. The state space of a hi-

erarchical variable H can be abstracted because, for a given
problem, only certain classes of H are supported directly by
the evidence or relevant to the query. To construct Bf from
Bh given some evidence and a query, we traverse Bh from
the leaves upwards, prune those variables irrelevant to an-
swer the query, and abstract the hierarchical variables. We
abstract the hierarchical variables as part of a pass to remove
variables that are irrelevant to the query. From the bottom-
up, you can recursively prune any variable that has no chil-
dren and is not observed or queried [Geiger et al., 1990;
Pearl, 1988] as well as abstract hierarchical variables when
their children have been abstracted. We can then answer the
query from the ﬂat Bayesian network using any standard prob-
abilistic inference algorithm.

4.1 Abstraction of Hierarchical Variables
Let H be a hierarchical variable with values Val(H). The ab-
straction of the domain of H, denoted by part(H), is a partition
of Val(H). That is, part(H) is a set of subsets of Val(H) such
that all sets in part(H) are mutually disjoint and the union
of all the sets equals Val(H). We refer to a set in part(H)
as an abstract value of H. An abstraction of hierarchical
variable H, denoted by Ha, is a simple variable with domain
Val(Ha) = part(H).
4.2 Finding a Safe Abstraction
In this section we describe a simple algorithm Flat_Bayes
for constructing a ﬂat Bayesian network Bf from a hierarchi-
cal Bayesian network Bh given some evidence and a query.
A safe abstraction has the same posterior distribution over the
query variable in Bf as in Bh. To develop a safe abstraction for
efﬁcient inference, the following constraints are followed in
Flat_Bayes: 1. For every evidence or query, there must be an
abstract value that directly associates with that evidence/query
and conveys its effect. 2. All abstract values must be mutually
disjoint and exclusive. The algorithm Flat_Bayes consists of
three phases:
Phase0:
If a query asks for the probability of a particular class in
hierarchical variable H, we create an extra binary child Q
of H, which is true exactly when the query is true (i.e.,
Pd (Q = true|Croot) = 0 and Pd (Q = true|Cq) = 1, where
Croot is the root class and Cq is the query class in the tree hi-
erarchy of H). The variable Q has the same probability as the
query class. We thus reduce the problem to one of computing
the probability of a simple variable.
Phase1: Abstract
In this phase Flat_Bayes traverses Bh from the leaves up-
wards.
It prunes a variable that is not queried or ob-
served and does not have any children [Geiger et al., 1990;
Pearl, 1988]. For each unpruned hierarchical variable H, the
algorithm computes its abstraction Ha as follows:
Case1: H is not observed
The hierarchical variable H can be a parent of several simple
variables.4 We need to ﬁnd the exceptional classes Ck of H
4Note that because of the bottom-up nature of the algorithm, H

can only have simple children at this stage of the algorithm.

R1

Croot

(true)

C1

C2

(false)

R3
C3

C4

Cpos

R2

C5

C6

(????)

C1neg

C2neg

Figure 3: The abstraction hierarchy of H is divided into three
regions R1, R2, and R3 by positive (Cpos) and negative (C1neg,
C2neg) observations about H

with respect to each of these children (i.e., classes that are
associated with the evidence or the query). The domain of
Ha is the set of non-empty abstract values for each excep-
tional class. The algorithm computes the abstract value, va,
for every exceptional class Ck as follows:
• va = Ck, if Ck does not have any exceptional strict sub-
classes.
• Otherwise, va = Ck − C1 − . . .− Cm, where C1, . . . , Cm
are the highest exceptional strict subclasses of Ck. This
set difference represents the set of all of the values that
are in Ck and are not covered by other abstract values.

The abstract values that are empty are removed from the do-
main of Ha. To recursively abstract the parents of H, the
algorithm ﬁnds those strict super classes of the exceptional
classes Ck, which are affected by H’s parents.
Case2: H is observed
The observations about H divide its tree hierarchy into three
regions R1, R2, and R3. R1 includes the classes that are true for
the observation (superclasses of Cpos), R2 includes the classes
that we know nothing about, and R3 includes the classes that
we know are false. For example, suppose we have one positive
(Cpos) and two negative (C1neg, C2neg) observations about H
as shown in Figure 3. The regions R1, R2, and R3 are shown
by the bold lines in Figure 3. We do not need to distinguish
between the values of H that we know are false. The values of
H can be partitioned into two disjoint and exclusive subsets:
one (vfalse) corresponds to all the values that are false and an-
other (vnotknow) corresponds to all the values we know nothing
about. Thus,

vfalse = (cid:4)(cid:2)
vnotknow = (cid:4)
Val(Ha) = (cid:4)

(cid:3) ∪ (negative observations)

Croot − Cpos
Cpos − (negative observations)
vnotknow, vfalse

If H also has children, we need to partition its abstract value
vnotknow into subsets based on the evidence from its children.
We need to ﬁnd the exceptional classes Ck of H with respect
to its children that are in region R2 in the hierarchy of H. We
can compute the abstract value voa that corresponds to each
exceptional class Ck in the same way as described in Case1,
but we need to remove all the false values. Thus,
voa = va − {negative observations that are subclasses ofCk}

(cid:5)

(cid:5)

(cid:5)

1

(cid:5)

voa
1

Let voa
, . . . , voa
respond to exceptional classes Ck. Let vr = voa
1
Then, Val(Ha) = (cid:4)

k be the non-empty abstract values that cor-
∪ . . . ∪ voa
k.

, vnotknow − vr , vfalse

, . . . , voa
k

To recursively abstract the parents of H, the algorithm ﬁnds
those strict super classes of exceptional classes Ck and Cpos
that are affected by H’s parents.
Phase2: Construct tables
The algorithm constructs the CPT for variables Xa of a ﬂat
network. Let paa(Xa) denote the abstracted parents of Xa.
We compute P(Xa = va|pas(Xa) = Vs ∧ paa(Xa) = V) for
each value va of Xa and for each assignment Vs of pas(Xa)
and V of paa(Xa) for the following two cases:
Case1: Xa is not abstracted but it has abstracted parents
As discussed in Section 3.3, P(Xa = va|pas(Xa) = Vs ∧
paa(Xa) = V) is inherited from the default distribution of Xa
for the parent context C, C ∈ Zp, V ≤ C and (cid:5) ∃Y ∈ Zp such
that V ≤ Y < C, where Zp is the set of parent contexts for
which the default distribution of Xa has been deﬁned.
Case2: Xa is an abstracted variable
Let  denote the assignment pas(Xa) = Vs ∧ paa(Xa) = V.
As discussed in Phase1, va = Ck − C1 − . . . − Cm. Then,
P (Xa = va|) = P(Ck|) − . . . − P(Cm|)
As discussed in Section 3.1, to compute P(Ck|) we need
the probability distribution of all the super classes of Ck over
their immediate subclasses for parent context V. As discussed
in Section 3.3, we can treat each of these classes as a simple
variable. Thus, the probability distribution of a class over its
immediate subclasses for parent context V can be computed
in the same way as described in Case1.

The evidence for observed hierarchical variable H is trans-
lated into the corresponding abstract variable Ha of Bf . The
evidence Eh for Ha in Bf is the disjunction of all those abstract
values of H that are not false for the observation.

The domain size of the variables in Bf is independent of
the size of the hierarchies; it depends on how many classes in
their hierarchy are exceptional with respect to their relevant5
children in Bh. The running time to construct Bf depends on
both the depth and number of the exceptional classes. Af-
ter constructing Bf we can answer the query from Bf using
any standard probabilistic inference algorithm, for example,
variable elimination algorithm [Zhang and Poole, 1994] or
stochastic simulation.

4.3 Correctness of Flat Bayesian Network
Let Bh be a hierarchical Bayesian network that has n discrete
random variables, X = {X1, . . . , Xn}, and we want to an-
swer some probabilistic query, P (Q|E), where Q, E ⊆ X.
Q denotes the query variables. We observed that E is in
the set e, e ⊆ domain(E), E ∈ e is the evidence. From
Bh we can recursively prune any variable that has no chil-
dren, and is not observed or queried [Geiger et al., 1990;
Pearl, 1988].

P(Q|E ∈ e) = P (Q ∧ E ∈ e)
P (E ∈ e)

5Variables irrelevant to the query are pruned by Flat_Bayes.

Let X1, . . . , Xs be the non-query non-observed random vari-
ables of Bh. pa(Xi) denotes the parents of variable Xi. The
marginal P(Q ∧ E) can be computed as follows:

P (Q ∧ E ∈ e) = (cid:1)

. . .

(cid:1)

P (Xi|pa(Xi)){E∈e}

n(cid:6)
i=1

Xs

X1

Lemma 4.1 The posterior probability P(Q|E) computed
from a Bayesian network after replacing its variables Xk by
their abstractions Xa
k is the same as if it is computed with-
out replacing Xk, if the Xa
k are constructed from the partitions
part(Xk) that have the following property :
∀Xk, ∀Yj ∈ children(Xk), ∀V ∈ part(pa(Yj)),
∀v1, v2 ∈ V , P(Yj|pa(Yj) = v1) = P(Yj|pa(Yj) = v2)
(1)
Proof Sketch Summing over all the values of Xk is equivalent
to summing over the partition of the values of Xk. Thus,

(cid:1)

P(Xk = v|pa(Xk)) × j(cid:6)
l=1

P (Yl|pa (Yl))
P(Xk = v|pa(Xk)) × j(cid:6)
l=1

(cid:1)
v∈V

v∈Val(Xk )
= (cid:1)
V∈part(Xk )

(2)
P (Yl|pa (Yl))

If (1) is true, we can distribute the product out of the inner-
most sum from the RHS of the above equality, leaving the
term
Thus (2) is equal to

(cid:7)
v∈V P(Xk = v|pa(Xk)), which is equal to P(V|pa(Xk)).
(cid:1)

P(Xa
k

= V|pa(Xk)) × j(cid:6)
l=1

P (Yl|pa (Yl))

V∈part(Xk )

It is straightforward to incorporate evidence E and query Q
into the proof.
Lemma 4.2 The algorithm Flat_Bayes partitions the values
of the hierarchical variables Xk of Bh such that all its children
satisﬁes the Equation (1).
Proof Sketch Let part(Xk) denote the abstraction of the do-
main of hierarchical variable Xk computed by Flat_Bayes. Xa
k
denotes the abstraction of Xk. If Xk is observed, Flat_Bayes
removes a set V, V ∈ part(Xk), such that all values in V are
false. Flat_Bayes constructs the conditional probability tables
for variables Xa
k of a ﬂat Bayesian network using “inheritance”
for the following two cases:
Case1: Xa
As discussed in Phase2, P(Xa
k
paa(Xa
k
fault distribution of Xa
and (cid:5) ∃Y ∈ Zp such that V ≤ Y < C. Thus,
∀v ∈ V, P(Xa
Pd (Xk|pas(Xk) = Vs ∧ C). This implies Equation (1).
Case2: Xa
) =
As discussed in Phase2, to compute P(Xa
) = V) we need the conditional distribution of
Vs ∧ paa(Xa
k
classes Cj, Cj ∈ tree(Xk), over their immediate subclasses.
We can treat a class Cj, Cj ∈ tree(Xk), as a simple variable.
Thus, the proof follows from Case1.

k is not abstracted but some of its parents are
) = Vs ∧
) = V), V ∈ part(pah(Xk)), is inherited from the de-
k for the parent context C ∈ Zp, V ≤ C
) = v) =

) = Vs ∧ paa(Xa

= va|pas(Xa

= va|pas(Xa

k is abstracted

|pas(Xa

k

k

k

k

k

k

conf_loc

acam_loc
1

spou_loc
1

acam_loc
k

spou_loc
k

(a)

world

europe

north america

uk

scotland

england

edinburgh

glassgow

canada

usa

(b)

Figure 4: (a) Bayesian network representation of conference
academics domain (b) A part of the tree hierarchy for location
in the world
Theorem 4.1 The posterior probability P(Q|E) computed
from a ﬂat Bayesian network as constructed by Flat_Bayes
is equal to the posterior probability computed from a hierar-
chical Bayesian network.
Proof Sketch The theorem follows from Lemmas 4.1 and 4.2.

5 Experiments
We implement our Flat_Bayes algorithm on top of the variable
elimination (VE) algorithm, we call it Flat_Bayes + VE. To
get an idea of the performance of Flat_Bayes+ VE compared
to VE, we investigate how the performance of both algorithms
vary with the number of hierarchical variables in the Bayesian
network, with the depth of the tree hierarchies of the hierar-
chical variables, and with the depth of the exceptional classes.
To test the algorithms, we consider a simple conference
domain. In this domain, the location of an academic is inﬂu-
enced by the location of the conference and the location of
the spouse of the academic is inﬂuenced by the location of
the academic. Let there be k academics. We assume that the
locations of the different academics are not independent; they
depend on the conference location. We consider that all loca-
tion variables (conference location or person locations) have
the same domain and are hierarchical variables. The Bayesian
network representation of this domain is shown in Figure 4
(a). The part of the tree hierarchy for the location is shown
in Figure 4 (b). Here we have 2k + 1 hierarchical random
variables. The hierarchical random variable conf_loc denotes
the conference location. The hierarchical random variables
acam_loc1, . . . , acam_lock denote the locations of the aca-
demics, and spou_loc1, . . . , spou_lock denote the locations
of their spouses respectively.
To
probability
P(acam_lock|conf_loc), we need to specify the default
distribution of each class in the hierarchy of acam_lock
over its immediate subclasses for some parent contexts (for
some classes in the hierarchy of conf_loc). We assume the
conference location inﬂuences each academic’s location very
locally; we need to specify the default distribution of the
classes over their immediate subclasses for very few parent
contexts.
Example: Suppose we know that the academic is in the UK
but don’t know whether he is in Scotland, Wales or England.
Now, knowing that the conference is taking place in the UK
but not where in the UK, doesn’t provide us any information

conditional

specify

the

VE when obs. are at depth = d 
VE when obs. are at depth = 1
Flat_Bayes+VE when obs. are at depth = 1
Flat_Bayes+VE when obs. are at depth = d

800

700

600

500

400

300

200

100

 
.
s
m
 
n
i
 
e
m

i
t
 
.
c
o
r
P

0

1

2

3

4

5

6

depth of abstraction hierarchies (d)

7

8

9

10

Figure 5: The average inference time of applying VE and
Flat_Bayes + VE for k = 2 as a function of the depth of the
hierarchy, and depth of the observations

about the location of the academic in the UK. However, if
we know that the conference is taking place somewhere in
Scotland, we can state that there is a high probability that
the academic is in Scotland. We assume that knowing the
conference location in Scotland (e.g., in Edinburgh), doesn’t
provide us any extra information about where they may be if
not in Scotland. Thus, we could have a default probability
distribution of the class uk in the hierarchy of acam_lock
over its immediate subclasses conditioned on conf_loc such
as:

Pd (scotland|uk ∧ conf_loc = scotland) = 0.8
Pd (scotland|uk ∧ conf_loc = england) = 0.3
Pd (scotland|uk ∧ conf_loc = world) = 0.4
For the experiments, we observe the locations of the spouses
and we want to compute where the conference is taking place.
To determine how the inference time depends on the number of
hierarchical variables, we vary k from 1 to 8. We represent the
abstraction hierarchy of all location variables by a binary tree
of depth d where d is a parameter. To determine how inference
time depends on the depth of the hierarchy (d) we vary d from
1 to 10 with a step of 1. To determine how the inference
time depends on the depth of the exceptional classes, for each
value of d we vary the depth of the observation for the observed
variables from 1 to d. We compute the posterior distribution
of conf_loc by applying only VE and Flat_Bayes + VE.
Figure 5 shows the average inference time of both VE and
Flat_Bayes + VE for the belief network that has only 2 aca-
demics as a function of the depth of the tree, and depth of the
observations. Figure 6 shows the average inference time of
both VE and Flat_Bayes + VE as a function of the number of
academics and the depth of observations; here we consider the
depth of the hierarchy is 8. Figure 5 shows that the inference
time for Flat_Bayes + VE doesn’t depend on the depth of the
hierarchy but increases linearly as the depth of the observa-
tions increases. This increase is because the depth of the ob-
servations for spou_lock variables changes the depth and the
number of exceptional classes for acam_lock and conf_loc.

.
s
m
 
n
i
 
e
m

i
t
 
.
c
o
r
P

300

250

200

150

100

50

1

VE when obs. are at depth = 1 
VE when obs. are at depth = 8
Flat_Bayes+VE  when obs. are at depth = 1
Flat_Bayes+VE when obs. are at depth = 8

2

3

4

5

6

7

8

Number of academics (k)

Figure 6: The average inference time of applying VE and
Flat_Bayes + VE for d = 8 as a function of the number of
academics and the depth of the observations

Note that VE takes less time as the depth of observation in-
creases. This is because the domain of spou_lock variable
reduces as the depth of the observation increases. The re-
sults shown in Figures 5 and 6 give us some indication of the
overhead of constructing a ﬂat Bayesian network and cases in
which it may be more effective to use it rather than VE alone.

6 Related Work
There is very limited work on combining taxonomic hierar-
chies and probabilistic reasoning. The problem of evidential
reasoning in a taxonomic hierarchy of hypotheses was ﬁrst
studied by Gordon and Shortliffe [1985] using the Dempster-
Shafer (D-S) theory. Shenoy and Shafer [1986] improved the
algorithm proposed by Gordon and Shortliffe and showed that
belief functions can be propagated using local computations.
Pearl [1986] provides a method for the same problem using
Bayesian formalism. In all of these, a very restricted naive
Bayes classiﬁer network form was considered.
In this pa-
per we concentrate on general Bayesian networks that have
both hierarchical and simple variables. Pearl’s method in-
volves traversing the whole hierarchy for each observation.
We show in this paper that we do not need to do this. In more
recent work, Koller and Pfeffer [1998] describe a represen-
tation language for combining frame-representation systems
and Bayesian networks for representing complex structured
domains. They are looking at different aspects of the prob-
lem, concentrating on multiple objects, combining ﬁrst-order
logic representation with probabilities.

7 Conclusion
Having the values of a variable hierarchically structured can
make reasoning more efﬁcient than reasoning over the set of
all values. We show how, given a query and observations,
we can dynamically construct a ﬂat Bayesian network from
the given hierarchical Bayesian network that can be used in
any standard probabilistic inference algorithm. The size of

the ﬂat network is independent of the size of the hierarchies;
it depends on how many of the classes in the hierarchy are
exceptional with respect to children that are observed or have
observed descendants in a Bayesian network. Thus it is pos-
sible to have hierarchical variables with unbounded or inﬁnite
domains. For example, with a spatial hierarchy, as long as
we have a way to compute the probability distribution over
subclasses, there is no reason not to have a hierarchy that is
inﬁnitely detailed. It is only when we ask a query of a class
or make observations that we need to restrict the size of the
hierarchy.

Acknowledgments
This work was supported by NSERC Discovery Grant OG-
POO44121. Thanks to Valerie McRae, Mark Crowley and
Ben D’Andrea for proofreading.

References
[Boutilier et al., 1996] C. Boutilier, N. Friedman, M. Gold-
szmidt, and D. Koller. Context-speciﬁc independence in
Bayesian networks. In Proceeding of Twelfth Conf. on Un-
certainity in Artiﬁcial Intelligence (UAI-96), pages 115–
123, 1996.

[Geiger et al., 1990] D. Geiger, T. Verma, and J. Pearl. d-
separation: From theorems to algorithms. In Proceeding of
Fifth Conf. on Uncertainity in Artiﬁcial Intelligence (UAI-
90), pages 139–148, 1990.

[Gordon and Shortliffe, 1985] J. Gordon and E.H. Shortliffe.
A method of managing evidential reasoning in a hierarchi-
cal hypothesis space. Artiﬁcial Intelligence, 26:323–57,
1985.

[Koller and Pfeffer, 1998] D. Koller and A. Pfeffer. Prob-
abilistic frame-based systems.
In Proceedings of the
15th National Conference on Artiﬁcial Intelligence (AAAI),
pages 580–587, 1998.

[Mackworth et al., 1985] A.K. Mackworth, J.A. Mulder, and
W.S. Havens. Hierarchical arc consistency: exploiting
structured domains in constraint satisfaction problems. In
Computational Intelligence, volume 1(3), 1985.

[Pearl, 1986] J. Pearl. On evidential reasoning in a hierarchy

of hypotheses. Artiﬁcial Intelligence, 28:9–15, 1986.

[Pearl, 1988] J. Pearl. Probabilistic reasoning in intelligent
systems: networks of plausible inference. Morgan Kauf-
mann Publishers Inc., 1988.

[Shenoy and Shafer, 1986] P.P. Shenoy and G. Shafer. Prop-
IEEE

agating belief functions with local computations.
Expert, 1:43–52, 1986.

[Zhang and Poole, 1994] N.L. Zhang and D. Poole. A simple
approach to Bayesian network computation. In Proc. of the
10th Candian Conference on Artiﬁcial Intelligence, 1994.


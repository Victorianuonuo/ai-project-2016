Characterizing Solution Concepts in Games Using Knowledge-Based Programs

Joseph Y. Halpern∗

Yoram Moses

Computer Science Department

Department of Electrical Engineering

Cornell University, U.S.A.

Technion—Israel Institute of Technology

e-mail: halpern@cs.cornell.edu

32000 Haifa, Israel

email: moses@ee.technion.ac.il

Abstract

We show how solution concepts in games such as
Nash equilibrium, correlated equilibrium, rational-
izability, and sequential equilibrium can be given
a uniform deﬁnition in terms of knowledge-based
programs. Intuitively, all solution concepts are im-
plementations of two knowledge-based programs,
one appropriate for games represented in normal
form, the other for games represented in extensive
form. These knowledge-based programs can be
viewed as embodying rationality. The representa-
tion works even if (a) information sets do not cap-
ture an agent’s knowledge, (b) uncertainty is not
represented by probability, or (c) the underlying
game is not common knowledge.

x

0

.5

.5

S

x

1

0

z

2

x

2

S

z

1

2

BB

x

3

B

x

4

L

3

z

2

R

6

z

3

L

2

z

4

R

4

z

5

Figure 1: A game of imperfect recall.

1 Introduction

in
Game theorists represent games in two standard ways:
normal form, where each agent simply chooses a strategy,
and in extensive form, using game trees, where the agents
make choices over time. An extensive-form representation
has the advantage that it describes the dynamic structure of
the game—it explicitly represents the sequence of decision
problems encountered by agents. However, the extensive-
form representation purports to do more than just describe
the structure of the game; it also attempts to represent the
information that players have in the game, by the use of in-
formation sets. Intuitively, an information set consists of a
set of nodes in the game tree where a player has the same
information. However, as Halpern [1997] has pointed out,
information sets may not adequately represent a player’s in-
formation.

Halpern makes this point by considering the following
single-agent game of imperfect recall, originally presented by
Piccione and Rubinstein [1997]: The game starts with nature
moving either left or right, each with probability 1/2. The
agent can then either stop the game (playing move S) and get

∗Supported in part by NSF under grants CTC-0208535 and ITR-
0325453, by ONR under grant N00014-02-1-0455, by the DoD
Multidisciplinary University Research Initiative (MURI) program
administered by the ONR under grants N00014-01-1-0795 and
N00014-04-1-0725, and by AFOSR under grant F49620-02-1-0101.

a payoff of 2, or continue, by playing move B. If he contin-
ues, he gets a high payoff if he matches nature’s move, and a
low payoff otherwise. Although he originally knows nature’s
move, the information set that includes the nodes labeled x3
and x4 is intended to indicate that the player forgets whether
nature moved left or right after moving B. Intuitively, when
he is at the information set X, the agent is not supposed to
know whether he is at x3 or at x4.

It is not hard to show that the strategy that maximizes ex-
pected utility chooses action S at node x1, action B at node
x2, and action R at the information set X consisting of x3
and x4. Call this strategy f . Let f (cid:2)
be the strategy of choos-
ing action B at x1, action S at x2, and L at X. Piccione and
Rubinstein argue that if node x1 is reached, the player should
reconsider, and decide to switch from f to f (cid:2)
. As Halpern
points out, this is indeed true, provided that the player knows
at each stage of the game what strategy he is currently using.
However, in that case, if the player is using f at the infor-
mation set, then he knows that he is at node x4; if he has
switched and is using f (cid:2)
, then he knows that he is at x3. So,
in this setting, it is no longer the case that the player does not
know whether he is at x3 or x4 in the information set; he can
infer which state he is at from the strategy he is using.

In game theory, a strategy is taken to be a function from in-
formation sets to actions. The intuition behind this is that,
since an agent cannot tell the nodes in an information set

IJCAI-07

1300

apart, he must do the same thing at all these nodes. But this
example shows that if the agent has imperfect recall but can
switch strategies, then he can arrange to do different things
at different nodes in the same information set. As Halpern
[1997] observes, ‘ “situations that [an agent] cannot distin-
guish” and “nodes in the same information set” may be two
quite different notions.’ He suggests using the game tree to
describe the structure of the game, and using the runs and sys-
tems framework [Fagin et al., 1995] to describe the agent’s
information. The idea is that an agent has an internal local
state that describes all the information that he has. A strat-
egy (or protocol in the language of [Fagin et al., 1995]) is a
function from local states to actions. Protocols capture the
intuition that what an agent does can depend only what he
knows. But now an agent’s knowledge is represented by its
local state, not by an information set. Different assumptions
about what agents know (for example, whether they know
their current strategies) are captured by running the same pro-
tocol in different contexts. If the information sets appropri-
ately represent an agent’s knowledge in a game, then we can
identify local states with information sets. But, as the exam-
ple above shows, we cannot do this in general.

A number of solution concepts have been considered in the
game-theory literature, ranging from Nash equilibrium and
correlated equilibrium to reﬁnements of Nash equilibrium
such as sequential equilibrium and weaker notions such as
rationalizability (see [Osborne and Rubinstein, 1994] for an
overview). The fact that game trees represent both the game
and the players’ information has proved critical in deﬁning
solution concepts in extensive-form games. Can we still rep-
resent solution concepts in a useful way using runs and sys-
tems to represent a player’s information? As we show here,
not only can we do this, but we can do it in a way that gives
deeper insight into solution concepts. Indeed, all the standard
solution concepts in the literature can be understood as in-
stances of a single knowledge-based (kb) program [Fagin et
al., 1995; 1997], which captures the underlying intuition that
a player should make a best response, given her beliefs. The
differences between solution concepts arise from running the
kb program in different contexts.

In a kb program, a player’s actions depend explicitly on
the player’s knowledge. For example, a kb program could
have a test that says “If you don’t know that Ann received the
information, then send her a message”, which can be written
if ¬Bi(Ann received info) then send Ann a message.

This kb program has the form of a standard if . . . then state-
ment, except that the test in the if clause is a test on i’s knowl-
edge (expressed using the modal operator Bi for belief; see
Section 2 for a discussion of the use of knowledge vs. belief).
Using such tests for knowledge allows us to abstract away
from low-level details of how the knowledge is obtained. Kb
programs have been applied to a number of problems in the
computer science literature (see [Fagin et al., 1995] and the
references therein). To see how they can be applied to under-
stand equilibrium, given a game Γ in normal form, let Si(Γ)
consist of all the pure strategies for player i in Γ. Roughly
speaking, we want a kb program that says that if player i
believes that she is about to perform strategy S (which we

express with the formula doi(S)), and she believes that she
would not do any better with another strategy, then she should
indeed go ahead and run S. This test can be viewed as em-
bodying rationality. There is a subtlety in expressing the
statement “she would not do any better with another strat-
egy”. We express this by saying “if her expected utility, given
that she will use strategy S, is x, then her expected utility if
she were to use strategy S(cid:2)
is at most x.” The “if she were
to use S(cid:2)
” is a counterfactual statement. She is planning
to use strategy S, but is contemplating what would happen
if she were to do something counter to fact, namely, to use
S(cid:2)
. Counterfactuals have been the subject of intense study
in the philosophy literature (see, for example, [Lewis, 1973;
Stalnaker, 1968]) and, more recently, in the game theory lit-
erature (see, for example, [Aumann, 1995; Halpern, 2001;
Samet, 1996]). We write the counterfactual “If A were the
case then B would be true” as “A (cid:2) B”. Although this state-
ment involves an “if . . . then”, the semantics of the counter-
factual implication A (cid:2) B is quite different from the material
implication A ⇒ B. In particular, while A ⇒ B is true if A
is false, A (cid:2) B might not be.

With this background, consider the following kb program

for player i:

for each strategy S ∈ Si(Γ) do
if Bi(doi(S) ∧ ∀x(EUi = x ⇒

(cid:2)

S (cid:2)∈Si(Γ)(doi(S(cid:2)) (cid:2) (EUi ≤ x)))) then S.

This kb program is meant to capture the intuition above. In-
tuitively, it says that if player i believes that she is about to
perform strategy S and, if her expected utility is x, then if
she were to perform another strategy S(cid:2)
, then her expected
utility would be no greater than x, then she should perform
strategy S. Call this kb program EQNFΓ
(with the indi-
vidual instance for player i denoted by EQNFΓ
i ). As we
show, if all players follow EQNFΓ
, then they end up play-
ing some type of equilibrium. Which type of equilibrium they
play depends on the context. Due to space considerations,
we focus on three examples in this abstract. If the players
have a common prior on the joint strategies being used, and
this common prior is such that players’ beliefs are indepen-
dent of the strategies they use, then they play a Nash equilib-
rium. Without this independence assumption, we get a cor-
related equilibrium. On the other hand, if players have pos-
sibly different priors on the space of strategies, then this kb
program deﬁnes rationalizable strategies [Bernheim, 1984;
Pearce, 1984].

To deal with extensive-form games, we need a slightly dif-
ferent kb program, since agents choose moves, not strategies.
Let EQEFΓ
i be the following program, where a ∈ PM de-
notes that a is a move that is currently possible.

for each move a ∈ PM do

if Bi(doi(a) ∧ ∀x((EUi = x) ⇒

(cid:2)

(cid:2)∈PM (doi(a

a

(cid:2)) (cid:2) (EUi ≤ x)))) then a.

Just as EQNFΓ
characterizes equilibria of a game Γ repre-
sented in normal form, EQEFΓ
characterizes equilibria of
a game represented in extensive form. We give one example
here: sequential equilibrium. To capture sequential equilib-
rium, we need to assume that information sets do correctly

IJCAI-07

1301

describe an agent’s knowledge. If we drop this assumption,
however, we can distinguish between the two equilibria for
the game described in Figure 1.

All these solution concepts are based on expected utility.
But we can also consider solution concepts based on other
decision rules. For example, Boutilier and Hyaﬁl [2004]
consider minimax-regret equilibria, where each player uses
a strategy that is a best-response in a minimax-regret sense to
the choices of the other players. Similarly, we can use max-
imin equilibria [Aghassi and Bertsimas, 2006]. As pointed
out by Chu and Halpern [2003], all these decision rules can
be viewed as instances of a generalized notion of expected
utility, where uncertainty is represented by a plausibility mea-
sure, a generalization of a probability measure, utilities are
elements of an arbitrary partially ordered space, and plausi-
bilities and utilities are combined using ⊕ and ⊗, generaliza-
tions of + and ×. We show in the full paper that, just by inter-
preting “EUi = u” appropriately, we can capture these more
exotic solution concepts as well. Moreover, we can capture
solution concepts in games where the game itself is not com-
mon knowledge, or where agents are not aware of all moves
available, as discussed by Halpern and Rˆego [2006].

Our approach thus provides a powerful tool for represent-
ing solution concepts, which works even if (a) information
sets do not capture an agent’s knowledge, (b) uncertainty is
not represented by probability, or (c) the underlying game is
not common knowledge.

and EQEFΓ

The rest of this paper is organized as follows.

In Sec-
tion 2, we review the relevant background on game theory
and knowledge-based programs. In Section 3, we show that
EQNFΓ
characterize Nash equilibrium, cor-
related equilibrium, rationalizability, and sequential equilib-
rium in a game Γ in the appropriate contexts. We conclude
in Section 4 with a discussion of how our results compare to
other characterizations of solution concepts.

2 Background

In this section, we review the relevant background on games
and knowledge-based programs. We describe only what we
need for proving our results. The reader is encouraged to con-
sult [Osborne and Rubinstein, 1994] for more on game the-
ory, [Fagin et al., 1995; 1997] for more on knowledge-based
programs without counterfactuals, and [Halpern and Moses,
2004] for more on adding counterfactuals to knowledge-
based programs.

2.1 Games and Strategies

A game in extensive form is described by a game tree. Asso-
ciated with each non-leaf node or history is either a player—
the player whose move it is at that node—or nature (which
can make a randomized move). The nodes where a player i
moves are further partitioned into information sets. With each
run or maximal history h in the game tree and player i we can
associate i’s utility, denoted ui(h), if that run is played. A
strategy for player i is a (possibly randomized) function from
i’s information sets to actions. Thus a strategy for player i
tells player i what to do at each node in the game tree where
i is supposed to move. Intuitively, at all the nodes that player

i cannot tell apart, player i must do the same thing. A joint
(cid:2)S = (S1, . . . , Sn) for the players determines a distri-
strategy
bution over paths in the game tree. A normal-form game can
be viewed as a special case of an extensive-form game where
each player makes only one move, and all players move si-
multaneously.

2.2 Protocols, Systems, and Contexts

To explain kb programs, we must ﬁrst describe standard pro-
tocols. We assume that, at any given point in time, a player in
a game is in some local state. The local state could include
the history of the game up to this point, the strategy being
used by the player, and perhaps some other features of the
player’s type, such as beliefs about the strategies being used
by other players. A global state is a tuple consisting of a local
state for each player.

A protocol for player i is a function from player i’s lo-
cal states to actions. For ease of exposition, we consider
only deterministic protocols, although it is relatively straight-
forward to model randomized protocols—corresponding to
mixed strategies—as functions from local states to distribu-
tions over actions. Although we restrict to deterministic pro-
tocols, we deal with mixed strategies by considering distrib-
utions over pure strategies.

A run is a sequence of global states; formally, a run is
a function from times to global states. Thus, r(m) is the
global state in run r at time m. A point is a pair (r, m)
consisting of a run r and time m. Let ri(m) be i’s local
state at the point (r, m); that is, if r(m) = (s1, . . . , sn), then
ri(m) = si. A joint protocol is an assignment of a protocol
for each player; essentially, a joint protocol is a joint strat-
(cid:2)P performs a joint ac-
egy. At each point, a joint protocol
tion (P1(r1(m)), . . . , Pn(rn(m))), which changes the global
(cid:2)P
state. Thus, given an initial global state, a joint protocol
generates a (unique) run, which can be thought of as an ex-
(cid:2)P . The runs in a normal-form game involve only
ecution of
one round and two time steps: time 0 (the initial state) and
time 1, after the joint strategy has been executed. (We as-
sume that the payoff is then represented in the player’s local
state at time 1.) In an extensive-form game, a run is again
characterized by the strategies used, but now the length of the
run depends on the path of play.

A probabilistic system is a tuple PS = (R, (cid:2)μ), where R is
a set of runs and (cid:2)μ = (μ1, . . . , μn) associates a probablity μi
on the runs of R with each player i. Intuitively, μi represents
player i’s prior beliefs. In the special case where μ1 = · · · =
μn = μ, the players have a common prior μ on R. In this
case, we write just (R, μ).

We are interested in the system corresponding to a joint
(cid:2)P . To determine this system, we need to describe the
protocol
(cid:2)P is being executed. For our purposes, this
setting in which
setting can be modeled by a set G of global states, a subset G0
of G that describes the possible initial global states, a set As
of possible joint actions at each global state s, and n probabil-
ity measures on G0, one for each player. Thus, a probabilistic
context is a tuple γ = (G, G0, {As : s ∈ G}, (cid:2)μ).1 A joint

1We are implicitly assuming that the global state that results from

IJCAI-07

1302

(cid:2)P is appropriate for such a context γ if, for every
protocol
(cid:2)P can generate are in As.
global state s, the joint actions that
(cid:2)P is appropriate for γ, we abuse notation slightly and
When
(cid:2)P
refer to γ by specifying only the pair (G0, (cid:2)μ). A protocol
(cid:2)P is appropriate generate a sys-
and a context γ for which
tem; the system depends on the initial states and probability
measures in γ. Since these are all that matter, we typically
simplify the description of a context by omitting the set G of
global states and the sets As of global actions. Let R( (cid:2)P , γ)
(cid:2)P in context γ.
denote the system generated by joint protocol
If γ = (G0, (cid:2)μ), then R( (cid:2)P , γ) = (R, (cid:2)μ(cid:2)), where R consists of
a the run r(cid:2)s for each initial state (cid:2)s ∈ G0, where r(cid:2)s is the run
i(r(cid:2)s) = μi((cid:2)s),
generated by
for i = 1, . . . , n.

(cid:2)P when started in state (cid:2)s, and μ(cid:2)

A probabilistic system (R, (cid:2)μ(cid:2)) is compatible with a con-
text γ = (G0, (cid:2)μ) if (a) every initial state in G0 is the initial
state of some run in R, (b) every run is the run of some pro-
tocol appropriate for γ, and (c) if R((cid:2)s) is the set of runs in
R with initial global state (cid:2)s, then μ(cid:2)
j(R((cid:2)s)) = μj((cid:2)s), for
j = 1, . . . , n. Clearly R( (cid:2)P , γ) is compatible with γ.

We can think of the context as describing background in-
formation.
In distributed-systems applications, the context
also typically includes information about message delivery.
For example, it may determine whether all messages sent are
received in one round, or whether they may take up to, say,
ﬁve rounds. Moreover, when this is not obvious, the context
speciﬁes how actions transform the global state; for exam-
ple, it describes what happens if in the same joint action two
players attempt to modify the same memory cell. Since such
issues do not arise in the games we consider, we ignore these
facets of contexts here. For simplicity, we consider only con-
texts where each initial state corresponds to a particular joint
strategy of Γ. That is, ΣΓ
i is a set of local states for player i
indexed by (pure) strategies. The set ΣΓ
i can be viewed as de-
scribing i’s types; the state sS can the thought of as the initial
state where player i’s type is such that he plays S (although
we stress that this is only intuition; player i does not have to
play S at the state sS). Let GΓ
n. We will be
interested in contexts where the set of initial global states is a
subset G0 of GΓ
0 . In a normal-form game, the only actions pos-
sible for player i at an initial global state amount to choosing
a pure strategy, so the joint actions are joint strategies; no ac-
tions are possible at later times. For an extensive-form game,
the possible moves are described by the game tree. We say
that a context for an extensive-form game is standard if the
local states have the form (s, I), where s is the initial state
and I is the current information set. In a standard context,
an agent’s knowledge is indeed described by the information
set. However, we do not require a context to be standard.
For example, if an agent is allowed to switch strategies, then
the local state could include the history of strategies used. In
such a context, the agent in the game of Figure 1 would know
more than just what is in the information set, and would want
to switch strategies.

1 × . . . × ΣΓ

0 = ΣΓ

performing a joint action in As at the global state s is unique and ob-
vious; otherwise, such information would also appear in the context,
as in the general framework of [Fagin et al., 1995].

2.3 Knowledge-Based Programs
A knowledge-based program is a syntactic object. For our
purposes, we can take a knowledge-based program for player
i to have the form

if κ1 then a1
if κ2 then a2
. . . ,

where each κj is a Boolean combination of formulas of the
form Biϕ, in which the ϕ’s can have nested occurrences of B(cid:3)
operators and counterfactual implications. We assume that
the tests κ1, κ2, . . . are mutually exclusive and exhaustive, so
that exactly one will evaluate to true in any given instance.
The program EQNFΓ
i can be written in this form by simply
replacing the for . . . do statement by one line for each pure
strategy in Si(Γ); similarly for EQEFΓ
i .

We want to associate a protocol with a kb program. Unfor-
tunately, we cannot “execute” a kb program as we can a pro-
tocol. How the kb program executes depends on the outcome
of tests κj. Since the tests involve beliefs and counterfactuals,
we need to interpret them with respect to a system. The idea
is that a kb program Pgi for player i and a probabilistic sys-
tem PS together determine a protocol P for player i. Rather
than giving the general deﬁnitions (which can be found in
[Halpern and Moses, 2004]), we just show how they work in
the two kb programs we consider in this paper: EQNF and
EQEF.

Given a system PS = (R, (cid:2)μ), we associate with each for-
mula ϕ a set [[ϕ]]PS of points in PS. Intuitively, [[ϕ]]PS is the
set of points of PS where the formula ϕ is true. We need a
little notation:

• If E is a set of points in PS, let R(E) denote the set
of runs going through points in E; that is R(E) = {r :
∃m((r, m) ∈ E)}.

• Let Ki(r, m) denote the set of points that i cannot dis-
tinguish from (r, m): Ki(r, m) = {(r(cid:2), m(cid:2)) : (r(cid:2)
i(m(cid:2)) =
ri(m)}. Roughly speaking, Ki(r, m) corresponds to i’s
information set at the point (r, m).

• Given a point (r, m) and a player i, let μ(i,r,m) be the
probability measure that results from conditioning μi
on
Ki(r, m), i’s information at (r, m). We cannot condi-
tion on Ki(r, m) directly: μi
is a probability measure
on runs, and Ki(r, m) is a set of points. So we actu-
ally condition, not on Ki(r, m), but on R(Ki(r, m)), the
set of runs going through the points in Ki(r, m). Thus,
μi,r,m = μi | R(Ki(r, m)). (For the purposes of this ab-
stract, we do not specify μi,r,m if μi(R(Ki(r, m))) = 0.
It turns out not to be relevant to our discussion.)

The kb programs we consider in this paper use a limited
collection of formulas. We now can deﬁne [[ϕ]]PS for the
formulas we consider that do not involve counterfactuals.

• In a system PS corresponding to a normal-form game
Γ, if S ∈ Si(Γ), then [[doi(S)]]PS is the set of initial
points (r, 0) such that player i uses strategy S in run r.
• Similarly, if PS corresponds to an extensive-form game,
then [[doi(a)]]PS is the set of points (r, m) of PS at
which i performs action a.

IJCAI-07

1303

• Player i believes a formula ϕ at a point (r, m) if the
event corresponding to formula ϕ has probability 1 ac-
cording to μi,r,m. That is, (r, m) ∈ [[Biϕ]]PS if
μi(R(Ki(r, m)) (cid:11)= 0 (so that conditioning on Ki(r, m)
is deﬁned) and μi,r,m([[ϕ]]PS ∩ Ki(r, m)) = 1.

• With every run r in the systems we consider, we can as-
(cid:2)S used in r.2 This pure
sociate the joint (pure) strategy
strategy determines the history in the game, and thus de-
termines player i’s utility. Thus, we can associate with
every point (r, m) player i’s expected utility at (r, m),
where the expectation is taken with respect to the prob-
ability μi,r,m. If u is a real number, then [[EUi = u]]PS
is the set of points where player i’s expected utility is u;
[[EUi ≤ u]]PS is deﬁned similarly.

• Assume that ϕ(x) has no occurrences of ∀. Then
[[∀xϕ(x))]]PS = ∩
a∈IR[[ϕ[x/a]]]PS , where ϕ[x/a] is
the result of replacing all occurrences of x in ϕ by a.
That is, ∀x is just universal quantiﬁcation over x, where
x ranges over the reals. This quantiﬁcation arises for us
when x represents a utility, so that ∀xϕ(x) is saying that
ϕ holds for all choices of utility.

We now give the semantics of formulas involving coun-
terfactuals. Here we consider only a restricted class of such
formulas, those where the counterfactual only occurs in the
form doi(S) (cid:2) ϕ, which should be read as “if i were to use
strategy S, then ϕ would be true. Intuitively, doi(S) (cid:2) ϕ is
true at a point (r, m) if ϕ holds in a world that differs from
(r, m) only in that i uses the strategy S. That is, doi(S) (cid:2) ϕ
is true at (r, m) if ϕ is true at the point (r(cid:2), m) where, in
run r(cid:2)
, player i uses strategy S and all the other players
use the same strategy that they do at (r, m).
(This can be
viewed as an instance of the general semantics for coun-
terfactuals used in the philosophy literature [Lewis, 1973;
Stalnaker, 1968] where ψ (cid:2) ϕ is taken to be true at a world
w if ϕ is true at all the worlds w(cid:2)
closest to w where ψ is
true.) Of course, if i actually uses strategy S in run r, then
r(cid:2) = r. Similarly, in an extensive-form game Γ, the closest
point to (r, m) where doi(a
is an
action that i can perform in the local state ri(m)) is the point
(r(cid:2), m) where all players other than player i use the same pro-
tocol in r(cid:2)
agrees with i’s protocol
in r except at the local state ri(m), where i performs action
(cid:2)
is the run that results from player i making a sin-
a
at time m) from the protocol she uses in
gle deviation (to a
r, and all other players use the same protocol as in r.

(cid:2)) is true (assuming that a

and r, and i’s protocol in r(cid:2)

. Thus, r(cid:2)

(cid:2)

(cid:2)

There is a problem with this approach. There is no guar-
antee that, in general, such a closest point (r(cid:2), m) exists in
the system PS. To deal with this problem, we restrict atten-
tion to a class of systems where this point is guaranteed to
exist. A system (R, (cid:2)μ) is complete with respect to context γ
if R includes every run generated by a protocol appropriate
for context γ. In complete systems, the closest point (r(cid:2), m)
is guaranteed to exist. For the remainder of the paper, we

2If we allow players to change strategies during a run, then we
will in general have different joint strategies at each point in a run.
For our theorems in the next section, we restrict to contexts where
players do not change strategies.

evaluate formulas only with respect to complete systems. In
a complete system PS, we deﬁne [[doi(S) (cid:2) ϕ]]PS to consist
of all the points (r, m) such that the closest point (r(cid:2), m) to
(r, m) where i uses strategy S is in [[ϕ]]PS . The deﬁnition
of [[doi(a) (cid:2) ϕ]]PS is similar. We say that a complete sys-
tem (R(cid:2), (cid:2)μ(cid:2)) extends (R, (cid:2)μ) if μj and μ(cid:2)
j agree on R (so that
μ(cid:2)
j(A) = μj(A)) for all A ⊆ R) for j = 1, . . . , n.
Since each formula κ that appears as a test in a kb program
Pgi for player i is a Boolean combination of formulas of the
form Biϕ, it is easy to check that if (r, m) ∈ [[κ]]PS , then
Ki(r, m) ⊆ [[κ]]PS . In other words, the truth of κ depends
only on i’s local state. Moreover, since the tests are mutually
exclusive and exhaustive, exactly one of them holds in each
PS
local state. Given a system PS, we take the protocol Pg
i
((cid:7)) = aj if, for some point (r, m) in PS
to be such that Pg
with ri(m) = (cid:7), we have (r, m) ∈ [[κj]]PS . Since κ1, κ2, . . .
are mutually exclusive and exhaustive, there is exactly one
action aj with this property.

PS
i

Intuitively, a joint protocol

(cid:2)
Pg in context γ if

We are mainly interested in protocols that implement a kb
(cid:2)P implements a kb
program.
(cid:2)P performs the same actions as
program
(cid:2)
(cid:2)P that have positive probability, assuming
Pg in all runs of
(cid:2)
that the knowledge tests in
Pg are interpreted with respect
to the complete system PS extending R( (cid:2)P , γ). Formally,
(cid:2)P (de facto) implements a joint kb program
a joint protocol
(cid:2)
Pg [Halpern and Moses, 2004] in a context γ = (G0, (cid:2)μ) if
Pi((cid:7)) = Pg
((cid:7)) for every local state (cid:7) = ri(m) such that
r ∈ R( (cid:2)P , γ) and μi(r) (cid:11)= 0, where PS is the complete sys-
tem extending R( (cid:2)P , γ). We remark that, in general, there
may not be any joint protocols that implement a kb program
in a given context, there may be exactly one, or there may be
more than one (see [Fagin et al., 1995] for examples). This
is somewhat analogous to the fact that there may not be any
equilibrium of a game for some notions of equilibrium, there
may be one, or there may be more than one.

PS
i

i

1 , . . . , P nf

i , chooses strategy S; let

3 The Main Results
Fix a game Γ in normal form. Let P nf
be the protocol that,
(cid:2)P nf =
in initial state sS ∈ ΣΓ
(P nf
n ). Let STRATi be the random variable on
initial global states that associates with an initial global state s
player i’s strategy in r. As we said, Nash equilibrium arises
in contexts with a common prior. Suppose that γ = (G0, μ) is
a context with a common prior. We say that μ is compatible
(cid:2)S if μ is the probability on pure
with the mixed joint strategy
(cid:2)S (under the obvious identiﬁcation
joint strategies induced by
of initial global states with joint strategies).

(cid:2)S is a Nash equilibrium of
Theorem 3.1: The joint strategy
the game Γ iff there is a common prior probability measure
μ on GΓ
0 such that STRAT1, . . . , STRATn are independent
with respect to μ, μ is compatible with
imple-
ments EQNFΓ

in the context (GΓ

(cid:2)S, and

0 , μ).

(cid:2)P nf

(cid:2)S is a (possibly mixed strategy) Nash
Proof: Suppose that
equilibrium of the game Γ. Let μ (cid:2)S be the unique probability

IJCAI-07

1304

If

.

i

(cid:2)S.

(cid:2)P nf

0 compatible with

implements EQNFΓ

(cid:2)S is played, then the proba-
on GΓ
bility of a run where the pure joint strategy (T1, . . . , Tn) is
played is just the product of the probabilities assigned to Ti
by Si, so STRAT1, . . . , STRATn are independent with re-
spect to μ (cid:2)S. To see that
in the
context γ = (GΓ
0 , μ (cid:2)S), let (cid:7) = ri(0) be a local state such that
r = R( (cid:2)P nf , γ) and μ(r) (cid:11)= 0. If (cid:7) = sT , then P nf
((cid:7)) = T ,
so T must be in the support of Si. Thus, T must be a best
(cid:2)S−i, the joint strategy where each player j (cid:11)= i
response to
(cid:2)S. Since i uses strategy T in r, the for-
plays its component of
mula Bi(doi(T (cid:2))) holds at (r, 0) iff T (cid:2) = T . Moreover, since
T is a best response, if u is i’s expected utility with the joint
, the formula doi(T (cid:2)) (cid:2) (EUi ≤ u)
strategy
holds at (r, 0). Thus, (EQNFΓ
i )PS ((cid:7)) = T , where PS is
the complete system extending R( (cid:2)P nf , γ).
It follows that
(cid:2)P nf

(cid:2)S, then for all T (cid:2)

implements EQNFΓ

(cid:2)S, and

For the converse, suppose that μ is a common prior proba-
bility measure on GΓ
0 , STRAT1, . . . , STRATn are indepen-
dent with respect to μ, μ is compatible with
im-
plements EQNFΓ
in the context γ = (GΓ
0 , μ). We want to
(cid:2)S is a Nash equilibrium. It sufﬁces to show that
show that
each pure strategy T in the support of Si is a best response
(cid:2)S, there must be a run r
to
such that μ(r) > 0 and ri(0) = sT (i.e., player i chooses T in
run r). It since
, and in the context
γ, EQNFΓ
ensures that no deviation from T can improve
(cid:2)S−i, it follows that T is
i’s expected utility with respect to
indeed a best response.

(cid:2)S−i. Since μ is compatible with

implements EQNFΓ

(cid:2)P nf

(cid:2)P nf

As is well known, players can sometimes achieve better
outcomes than a Nash equilibrium if they have access to a
helpful mediator. Consider the simple 2-player game de-
scribed in Figure 2, where Alice, the row player, must choose
between top and bottom (T and B), while Bob, the column
player, must choose between left and right (L and R):

L

T
(3, 3)
B (4, 1)

R

(1, 4)
(0, 0)

Figure 2: A simple 2-player game.

It is not hard to check that the best Nash equilibrium for
this game has Alice randomizing between T and B, and Bob
randomizing between L and R; this gives each of them ex-
pected utility 2. They can do better with a trusted mediator,
who makes a recommendation by choosing at random be-
tween (T, L), (T, R), and (B, L). This gives each of them
expected utility 8/3. This is a correlated equilibrium since,
for example, if the mediator chooses (T, L), and thus sends
recommendation T to Alice and L to Bob, then Alice con-
siders it equally likely that Bob was told L and R, and thus
has no incentive to deviate; similarly, Bob has no incentive to
deviate. In general, a distribution μ over pure joint strategies
is a correlated equilibrium if players cannot do better than
following a mediator’s recommendation if a mediator makes

recommendations according to μ. (Note that, as in our ex-
ample, if a mediator chooses a joint strategy (S1, . . . , Sn) ac-
cording to μ, the mediator recommends Si to player i; player
i is not told the joint strategy.) We omit the formal deﬁnition
of correlated equilibrium (due to Aumman [1974]) here; how-
ever, we stress that a correlated equilibrium is a distribution
over (pure) joint strategies. We can easily capture correlated
equilibrium using EQNF.
Theorem 3.2: The distribution μ on joint strategies is a corre-
implements EQNFΓ
lated equilibrium of the game Γ iff
in the context (GΓ

0 , μ).

(cid:2)P nf

Both Nash equilibrium and correlated equilibrium require
a common prior on runs. By dropping this assumption, we get
another standard solution concept: rationalizability [Bern-
heim, 1984; Pearce, 1984]. Intuitively, a strategy for player
i is rationalizable if it is a best response to some beliefs that
player i may have about the strategies that other players are
following, assuming that these strategies are themselves best
responses to beliefs that the other players have about strate-
gies that other players are following, and so on. To make
this precise, we need a little notation. Let S−i = Πj(cid:4)=iSj.
Let ui((cid:2)S) denote player i’s utility if the strategy tuple
(cid:2)S is
played. We describe player i’s beliefs about what strategies
the other players are using by a probability μi on S−i. A
strategy S for player i is a best response to beliefs described
ui(S, (cid:2)T )μi( (cid:2)T ) ≥
by a probability μi on S−i(Γ) if
(cid:3)
ui(S(cid:2), (cid:2)T )μi( (cid:2)T ) for all S(cid:2) ∈ Si. Following Osborne
and Rubinstein [1994], we say that a strategy S for player i
in game Γ is rationalizable if, for each player j, there is a
set Zj ⊆ Sj(Γ) and, for each strategy T ∈ Zj, a probability
measure μj,T on S−j(Γ) whose support is Z−j such that

(cid:2)T ∈S−i

(cid:2)T ∈S−i

(cid:3)

• S ∈ Zi; and
• for each player j and strategy T ∈ Zj, T is a best re-

sponse to the beliefs μj,T .

For ease of exposition, we consider only pure rationaliz-
able strategies. This is essentially without loss of generality.
It is easy to see that a mixed strategy S for player i is a best
response to some beliefs μi of player i iff each pure strategy
in the support of S is a best response to μi. Moreover, we
can assume without loss of generality that the support of μi
consists of only pure joint strategies.
Theorem 3.3: A pure strategy S for player i is rationalizable
iff there exist probability measures μ1, . . . , μn, a set G0 ⊆
(cid:2)P nf
0 , and a state (cid:2)s ∈ G0 such that P nf
GΓ
implements EQNFΓ

in the context (G0, (cid:2)μ).

i (si) = S and

(cid:2)P nf

i

implements EQNFΓ

Proof: First, suppose that
in con-
text (G0, (cid:2)μ). We show that for each state (cid:2)s ∈ G0 and player
i, the strategy S(cid:2)s,i = (cid:2)P nf
(si) is rationalizable. Let Zi =
{S(cid:2)s,i : (cid:2)s ∈ G0}. For S ∈ Zi, let E(S) = {(cid:2)s ∈ G0 : si = sS};
that is, E(S) consists consists of all initial global states where
player i’s local state is sS; let μi,S = μi(· | E(S)) (under the
obvious identiﬁcation of global states in G0 with joint strate-
gies). Since
, it easily follows that
S best response to μi,S . Hence, all the strategies in Zi are
rationalizable, as desired.

implements EQNFΓ

(cid:2)P nf

IJCAI-07

1305

For the converse, let Zi consist of all the pure rationaliz-
able strategies for player i. It follows from the deﬁnition of
rationalizability that, for each strategy S ∈ Zi, there exists
a probability measure μi,S on Z−i such that S is a best re-
sponse to μi,S . For a set Z of strategies, we denote by ˜Z the
set {sT : T ∈ Z}. Set G0 = ˜Z1 × . . . × ˜Zn, and choose some
measure μi on G0 such that μi(· | E(S)) = μi,S for all S ∈
Zi. (We can take μi =
αSμi,S, where αS ∈ (0, 1)
(sS) = S for all states
and
sS. It immediately follows that, for every rationalizable joint
, . . . , sSn) ∈ G0,
strategy
(cid:2)S = (cid:2)P nf ((cid:2)s). Since the states in G0 all correspond to
and
rationalizable strategies, and by deﬁnition of rationalizability
each (individual) strategy Si is a best response to μi,S, it is
easy to check that
in the context
(GΓ
0 , (cid:2)μ), as desired.

(cid:2)S = (S1, . . . , Sn), both (cid:2)s = (sS1

αS = 1.) Recall that P nf

implements EQNFΓ

(cid:2)P nf

S∈Zi

(cid:3)

(cid:3)

S∈Zi

i

We remark that Osborne and Rubinstein’s deﬁnition of ra-
tionalizability allows μj,T to be such that j believes that other
players’ strategy choices are correlated. In most of the lit-
erature, players are assumed to believe that other players’
choices are made independently. If we add that requirement,
then we must impose the same requirement on the probability
measures μ1, . . . , μn in Theorem 3.3.

Up to now we have considered solution concepts for games
in normal form. Perhaps the best-known solution concept for
games in extensive form is sequential equilibrium [Kreps and
(cid:2)S is a se-
Wilson, 1982]. Roughly speaking, a joint strategy
(cid:2)S−i at all infor-
quential equilibrium if Si is a best response to
mation sets, not just the information sets that are reached with
(cid:2)S. To understand how se-
positive probability when playing
quential equilibrium differs from Nash equilibrium, consider
the game shown in Figure 3.

shown in Figure 3, the unique joint strategy in a sequential
equilibrium has A choosing acrossA and B choosing downB.
The main difﬁculty in deﬁning sequential equilibrium lies
in capturing the intuition of best response in information sets
that are reached with probability 0. To deal with this, a se-
quential equilibrium is deﬁned to be a pair ((cid:2)S, β), consist-
(cid:2)S and a belief system β, which as-
ing of a joint strategy
sociates with every information set I a probability β(I) on
the histories in I. There are a number of somewhat subtle
consistency conditions on these pairs pairs; we omit them
here due to lack of space (see [Kreps and Wilson, 1982;
Osborne and Rubinstein, 1994] for details). Our result de-
pends on a recent characterization of sequential equilibrium
[Halpern, 2006] that uses nonstandard probabilities, which
can assign inﬁnitesimal probabilities to histories. By assum-
ing that every history gets positive (although possibly inﬁni-
tesimal) probability, we can avoid the problem of dealing with
information sets that are reached with probaility 0.

To every nonstandard real number r, there is a closest stan-
dard real number denoted st (r), and read “the standard part
of r”: |r − st (r) | is an inﬁnitesimal. Given a nonstandard
probability measure ν, we can deﬁne the standard probabil-
ity measure st (ν) by taking st (ν) (w) = st (ν(w)). A non-
standard probability ν on G0 is compatible with joint strategy
(cid:2)S if st (ν) is the probability on pure strategies induced by
(cid:2)S. When dealing with nonstandard probabilities, we general-
(cid:2)P
ize the deﬁnition of implementation by requiring only that
(cid:2)P such that
performs the same actions as
st (ν) (r) > 0. Moreover, the expression “EUi = x” in
EQEFΓ
is interpreted as “the standard part of i’s expected
utility is x” (since x ranges over the standard real numbers).
Theorem 3.4: If Γ is a game with perfect recall3 there is a be-
lief system β such that ((cid:2)S, β) is a sequential equilibrium of Γ
iff there is a common prior nonstandard probability measure
ν on GΓ
0 that gives positive measure to all states such that
STRAT1, . . . , STRATn are independent with respect to ν,
ν is compatible with
in the
standard context (GΓ

implements EQEFΓ

(cid:2)
Pg in runs r of

(cid:2)S, and
0 , ν).

(cid:2)P ef

Figure 3: A game with an unreasonable Nash equilibrium.

One Nash equilibrium of this game has A playing downA
and B playing acrossB. However, this is not a sequential
equilibrium, since playing across is not a best response for
B if B is called on to play. This is not a problem in a
Nash equilibrium because the node where B plays is not
reached in the equilibrium. Sequential equilibrium reﬁnes
Nash equilibrium (in the sense that every sequential equilib-
rium is a Nash equilibrium) and does not allow solutions such
as (downA, acrossB). Intuitively, in a sequential equilibrium,
every player must make a best response at every information
set (even if it is reached with probability 0).
In the game

This is very similar in spirit to Theorem 3.1. The key dif-
ference is the use of a nonstandard probability measure. Intu-
(cid:2)S to be a best response even at information
itively, this forces
sets that are reached with (standard) probability 0.

The effect of interpreting “EUi = x” as “the standard part
of i’s expected utility is x” is that we ignore inﬁnitesimal dif-
(cid:2)P ef
i ( (cid:2)s0) might not
ferences. Thus, for example, the strategy
(cid:2)S−i; it might just be an -best response
be a best response to
for some inﬁnitesimal . As we show in the full paper, it fol-
lows from Halpern’s [2006] results that we can also obtain a
characterization of (trembling hand) perfect equilibrium [Sel-
ten, 1975], another standard reﬁnement of Nash equilibrium,
if we interpret “EUi = x” as “the expected utility for agent i
is x” and allow x to range over the nonstandard reals instead
of just the standard reals.

3These are games where players remember all actions made and
the states they have gone through; we give a formal deﬁnition in the
full paper. See also [Osborne and Rubinstein, 1994].

IJCAI-07

1306

4 Conclusions

We have shown how a number of different solution con-
cepts from game theory can be captured by essentially one
knowledge-based program, which comes in two variants: one
appropriate for normal-form games and one for extensive-
form games. The differences between these solution concepts
is captured by changes in the context in which the games are
played: whether players have a common prior (for Nash equi-
librium, correlated equilibrium, and sequential equilibrium)
or not (for rationalizability), whether strategies are chosen
independently (for Nash equilibrium, sequential equilibrium,
and rationalizability) or not (for correlated equilibrium); and
whether uncertainty is represented using a standard or non-
standard probability measure.

Our results can be viewed as showing that each of these so-
lution concepts sc can be characterized in terms of common
knowledge of rationality (since the kb programs EQNFΓ
and EQEFΓ
embody rationality, and we are interested in
systems “generated” by these program, so that rationality
holds at all states), and common knowledge of some other
features Xsc captured by the context appropriate for sc (e.g.,
that strategies are chosen independently or that the prior).
Roughly speaking, our results say that if Xsc is common
knowledge in a system, then common knowledge of rational-
ity implies that the strategies used must satisfy solution con-
(cid:2)S satisﬁes sc, then there
cept sc; conversely, if a joint strategy
is a system where Xsc is common knowledge, rationality is
(cid:2)S is being played at some state. Re-
common knowledge, and
sults similar in spirit have been proved for rationalizability
[Brandenburger and Dekel, 187] and correlated equilibrium
[Aumann, 1987]. Our approach allows us to unify and ex-
tend these results and, as suggested in the introduction, ap-
plies even to settings where the game is not common knowl-
edge and in settings where uncertainty is not represented by
probability. We believe that the approach captures the essence
of the intuition that a solution concept should embody com-
mon knowledge of rationality.

References

[Aghassi and Bertsimas, 2006] M. Aghassi and D. Bertsi-
mas. Robust game theory. Mathematical Programming,
Series B, 107(1–2):231–273, 2006.

[Aumann, 1974] R. J. Aumann. Subjectivity and correlation
in randomized strategies. Journal of Mathematical Eco-
nomics, 1:67–96, 1974.

[Aumann, 1987] R. J. Aumann. Correlated equilibrium as an
expression of Bayesian rationality. Econometrica, 55:1–
18, 1987.

[Aumann, 1995] R. J. Aumann. Backwards induction and
common knowledge of rationality. Games and Economic
Behavior, 8:6–19, 1995.

[Bernheim, 1984] B. D. Bernheim. Rationalizable strategic

behavior. Econometrica, 52(4):1007–1028, 1984.

[Boutilier and Hyaﬁl, 2004] C. Boutilier and N. Hyaﬁl. Re-
gret minimizing equilibria and mechanisms for games with

strict type uncertainty. In Proc. Twentieth Conf. on Uncer-
tainty in AI (UAI 2004), pages 268–277, 2004.

[Brandenburger and Dekel, 187] A. Brandenburger

and
E. Dekel. Rationalizability and correlated equilibria.
Econometrica, 55:1391–1402, 187.

[Chu and Halpern, 2003] F. Chu and J. Y. Halpern. Great ex-
pectations. Part I: On the customizability of generalized
expected utility.
In Proc. Eighteenth International Joint
Conf. on AI (IJCAI ’03), pages 291–296, 2003.

[Fagin et al., 1995] R. Fagin, J. Y. Halpern, Y. Moses, and
M. Y. Vardi. Reasoning About Knowledge. MIT Press,
1995. (Slightly revised paperback version, 2003.)

[Fagin et al., 1997] R. Fagin, J. Y. Halpern, Y. Moses, and
M. Y. Vardi. Knowledge-based programs. Distributed
Computing, 10(4):199–225, 1997.

[Halpern and Moses, 2004] J. Y. Halpern and Y. Moses. Us-
ing counterfactuals in knowledge-based programming.
Distributed Computing, 17(2):91–106, 2004.

[Halpern and Rˆego, 2006] J. Y. Halpern and L. C. Rˆego.
Extensive games with possibly unaware players.
In
Proc. Fifth International Joint Conf. on Autonomous
Agents and Multiagent Systems, pages 744–751, 2006.

[Halpern, 1997] J. Y. Halpern. On ambiguities in the inter-
pretation of game trees. Games and Economic Behavior,
20:66–96, 1997.

[Halpern, 2001] J. Y. Halpern. Substantive rationality and
backward induction. Games and Economic Behavior,
37:425–435, 2001.

[Halpern, 2006] J. Y. Halpern. A nonstandard characteri-
zation of sequential equilibriu, perfect equilibrium, and
proper equilibrium. Unpublished manuscript, 2006.

[Kreps and Wilson, 1982] D. M. Kreps and R. B. Wilson.

Sequential equilibria. Econometrica, 50:863–894, 1982.

[Lewis, 1973] D. K. Lewis. Counterfactuals. Harvard Uni-

versity Press, 1973.

[Osborne and Rubinstein, 1994] M. J. Osborne and A. Ru-

binstein. A Course in Game Theory. MIT Press, 1994.

[Pearce, 1984] D. G. Pearce. Rationalizable strategic be-
havior and the problem of perfection. Econometrica,
52(4):1029–1050, 1984.

[Piccione and Rubinstein, 1997] M. Piccione and A. Rubin-
stein. On the interpretation of decision problems with im-
perfect recall. Games and Economic Behavior, 20(1):3–
24, 1997.

[Samet, 1996] D. Samet.

Hypothetical knowledge and
games with perfect information. Games and Economic Be-
havior, 17:230–251, 1996.

[Selten, 1975] R. Selten. Reexamination of the perfectness
concept for equilibrium points in extensive games. Inter-
national Journal of Game Theory, 4:25–55, 1975.

[Stalnaker, 1968] R. C. Stalnaker. A semantic analysis of
conditional logic. In N. Rescher, editor, Studies in Logical
Theory, pages 98–112. Oxford University Press, 1968.

IJCAI-07

1307


Efﬁcient Bayesian Task-Level Transfer Learning

Daniel M. Roy and Leslie P. Kaelbling

Massachusetts Institute of Technology

Computer Science and Artiﬁcial Intelligence Laboratory

{droy, lpk}@csail.mit.edu

Abstract

In this paper, we show how using the Dirichlet Pro-
cess mixture model as a generative model of data
sets provides a simple and effective method for
transfer learning. In particular, we present a hierar-
chical extension of the classic Naive Bayes classi-
ﬁer that couples multiple Naive Bayes classiﬁers by
placing a Dirichlet Process prior over their parame-
ters and show how recent advances in approximate
inference in the Dirichlet Process mixture model
enable efﬁcient inference. We evaluate the result-
ing model in a meeting domain, in which the sys-
tem decides, based on a learned model of the user’s
behavior, whether to accept or reject the request on
his or her behalf. The extended model outperforms
the standard Naive Bayes model by using data from
other users to inﬂuence its predictions.

1 Introduction
In machine learning, we are often confronted with multiple,
related data sets and asked to make predictions. For example,
in spam ﬁltering, a typical data set consists of thousands of la-
beled emails belonging to a collection of users. In this sense,
we have multiple data sets—one for each user. Should we
combine the data sets and ignore the prior knowledge that dif-
ferent users labeled each email? If we combine the data from
a group of users who roughly agree on the deﬁnition of spam,
we will have increased the available training data from which
to make predictions. However, if the preferences within a
population of users are heterogeneous, then we should expect
that simply collapsing the data into an undifferentiated col-
lection will make our predictions worse.

The process of using data from unrelated or partially re-
lated tasks is known as transfer learning or multi-task learn-
ing and has a growing literature (Baxter, 2000; Guestrin et al.,
2003; Thrun, 1996; Xue et al., 2005). While humans effort-
lessly use experience from related tasks to improve their per-
formance at novel tasks, machines must be given precise in-
structions on how to make such connections. In this paper,
we introduce such a set of instructions, based on the statis-
tical assumption that there exists some partition of the tasks
into clusters such that the data for all tasks in a cluster are
identically distributed. Ultimately, any such model of sharing

must be evaluated on real data, and, to that end, we evaluate
the resulting model in a meeting domain. The learned system
decides, based on training data for a user, whether to accept
or reject the request on his or her behalf. The model that
shares data outperforms its no-sharing counterpart by using
data from other users to inﬂuence its predictions.

When faced with a classiﬁcation task on a single data set,
well-studied techniques abound (Boser et al., 1992; Lafferty
et al., 2001). A popular classiﬁer that works well in practice,
despite its simplicity, is the Naive Bayes classiﬁer (Maron,
1961). We can extend this classiﬁer to the multi-task setting
by training one classiﬁer for each cluster in the latent par-
tition. To handle uncertainty in the number of clusters and
their membership, we deﬁne a generative process for data
sets that induces clustering. At the heart of this process is
a non-parametric prior known as the Dirichlet Process. This
prior couples the parameters of the Naive Bayes classiﬁers
attached to each data set. This approach extends the applica-
bility of the Naive Bayes classiﬁer to the domain of multi-task
learning when the tasks are deﬁned over the same input space.
Bayesian inference under this clustered Naive Bayes model
combines the contribution of every partition of the data sets,
weighing each by the partition’s posterior probability. How-
ever, the sum over partitions is intractable and, therefore, we
employ recent work by Heller and Ghahramani (2005a) to
implement an approximate inference algorithm. The result is
efﬁcient, task-level transfer learning.

2 Models

In this paper, we concentrate on classiﬁcation settings where
the features Xf , f = 1, . . . , F and labels Y are drawn from
ﬁnite sets Vf , L, respectively. Our goal is to learn the rela-
tionship between the input features and output labels in order
to predict a label given an unseen combination of features.

Consider U tasks, each with Nu training examples com-
posed of a label Y and a feature vector X. To make the dis-
cussion more concrete, assume each task is associated with
a different user performing some common task. Therefore,
we will write D(u)
and
corresponding labels Y (u,i)
, i = 1, . . . , Nu, associated with
the u-th user. Then D is the entire collection of data across
all users, and D(u,j) = (X (u,j), Y (u,j)) is the j-th data point
for the u-th user.

to represent the feature vectors X (u,i)

IJCAI-07

2599

Let mu,y denote the number of data points labeled y ∈ L
in the data set associated with the u-th user and let nu,y,f,x
denote the number of instances in the u-th data set where the
f -th feature takes the value x ∈ Vf when its parent label takes
value y ∈ L. Let θu,y,f,x (cid:2) P(X (u)
f = x | Y (u) = y), and
φu,y (cid:2) P(Y (u) = y). We can now write the general form of
the probability mass function of the data conditioned on the
parameters θ and φ of the Naive Bayes model:

p(D(u,n)|θu, φu)

(cid:2)

1≤u≤U
1≤n≤Nu

U(cid:2)

u=1

(a)
=

p(D|θ, φ)
Nu(cid:2)
U(cid:2)

(c)
=

u=1

U(cid:2)

n=1

(cid:2)

(d)
=

(b)
=

p(D(u)|θu, φu)
F(cid:2)

p(Y (u,n)|φu,y)
(cid:2)

F(cid:2)

φmu,y

u,y

f =1

θnu,y,f,x
u,y,f,x ,

u=1

y∈L

f =1

x∈Vf

p(X (u,n)

f

|Y (u), θu,y,f )

In (a), p(D|θ, φ) is expanded into a product of terms, one for
each data set, reﬂecting that the data sets are independent con-
ditioned on the parameterization. Step (b) assumes the data
points are exchangeable; in particular, the label/feature pairs
are independent of one another given the parameterization.
In step (c), we have made use of the Naive Bayes assump-
tion that the features are conditionally independent given the
label. Finally, in step (d), we have used the fact that the distri-
butions are multinomials. The maximum likelihood parame-
terizations are

φu,y =

,

θu,y,f,x =

mu,y(cid:3)
y∈L mu,y

(cid:3)

nu,y,f,x

nu,y,f,x

x∈Vf

Because each data set is parameterized separately, it is no
surprise that the maximum likelihood parameterization for
each data set depends only on the data in that data set. In
order to induce sharing, we must somehow constrain the pa-
rameterization across users. In the Bayesian setting, the prior
distribution F (θ, φ) can be used to enforce such constraints.
Given a prior, the resulting joint distribution on the data is

(cid:4)

p(D) =

Θ×Φ

p(D|θ, φ) dF (θ, φ).

Both models introduced in this section are completely spec-
iﬁed by a prior distribution over the parameters of the Naive
Bayes model. As we will see, different priors result in differ-
ent types of sharing.

2.1 No-Sharing Baseline Model
We have already seen that a ML parameterization of the Naive
Bayes model ignores related data sets. In the Bayesian set-
ting, any prior density f over the entire set of parameters that
factors into densities for each user’s parameters will result in
no sharing. In particular,

(cid:2)U

f (θ, φ) =

f (θu, φu),

u=1

is equivalent to the statement that the parameters for each user
are independent of the parameters for other users. Under this

DP

φu

φu

Yuj

θuyf

Yuj

θuyf

|L|F

|L|F

Xujf

F
Nu

(a)

Xujf

F
Nu

U

U

(b)

Figure 1: Graphical models for the (a) No-Sharing and (b) Clus-
tered Naive Bayes (right) models. Each user has its own parame-
terization in the no-sharing model. The parameters of the Clustered
Naive Bayes model are drawn from a Dirichlet Process. Here, the
intermediate measure G has been integrated out.

assumption of independence, training the entire collection of
models is identical to training each model separately on its
own data set. We therefore call this model the no-sharing
model.

Having speciﬁed that the prior factors into parameter dis-
tributions for each user, we must specify the actual parameter
distributions for each user. A reasonable (and tractable) class
of distributions over multinomial parameters are the Dirichlet
distributions which are conjugate to the multinomials. There-
fore, the distribution over φu, which takes values in the |L|-
simplex, is

.

f (φu) =

(cid:3)
(cid:5)
y∈L αu,y)
y∈L Γ(αu,y)

Γ(

(cid:2)

y∈L

φαu,y −1

u,y

.

Similarly, the distribution over θu,y,f , which takes values in
|Vf |-simplex, is

(cid:3)

Γ(

(cid:5)
βu,y,f,x)
x∈Vf Γ(βu,y,f,x)

x∈Vf

(cid:2)

x∈Vf

θβu,y,f,x−1
u,y,f,x

.

f (θuy,f ) =

We can write the resulting model compactly as a generative
process:1

φu ∼ Dir([αu,y : y ∈ L])

(1)

Y (u,n) | φu ∼ Discrete(φu)

θu,y,f ∼ Dir([βu,y,f,x : x ∈ Vf ])

X (u,n)

f

| Y (u,n), {θu,y,f : ∀y ∈ L} ∼ Discrete(θu,(Y (u,n)),f )

The No-Sharing model will function as a baseline against
which we can compare alternative models that induce shar-
ing. We turn now to specifying a model that induces sharing.

1The ∼ symbol denotes that the variable to the left is distributed
according to the distribution speciﬁed on the right. It should be noted
that the Dirichlet distribution requires an ordered set of parameters.
We therefore deﬁne an arbitrary ordering of the elements of L, Vf .

IJCAI-07

2600

2.2 The Clustered Naive Bayes Model
A plausible assumption about a collection of related data sets
is that they can be clustered into closely-related groups. To
make this more precise, we will consider two tasks t1 and t2
over some space X × Y to be related if the data associated
with both tasks are identically distributed. While this is a very
coarse model of relatedness, it leads to improved predictive
performance with limited training data.

As a ﬁrst step we must specify a distribution over parti-
tions of tasks. There are several properties we would like
this distribution to have: ﬁrst, we want exchangeability of
tasks (users); e.g., the probability should not depend on the
ordering (i.e. identity) of the tasks (users); second, we want
exchangeability of clusters; e.g., the probability should not
depend on the ordering/naming of the clusters; ﬁnally, we
want consistency; e.g., a priori, the (hypothetical) existence
of an unobserved task should not affect the probability that
any group of tasks are clustered together.

The Chinese Restaurant Process (CRP) is a stochastic pro-
cess that induces a distribution over partitions that satisﬁes all
these requirements (Aldous, 1985). The following metaphor
was used to deﬁne the process: imagine a restaurant with a
countably inﬁnite number of indistinguishable tables. The
ﬁrst customer sits at an arbitrary empty table. Subsequent
customers sit at an occupied table with probability propor-
tional to the number of customers already seated at that table
and sit at an arbitrary, new table with probability proportional
to a parameter α > 0. The resulting “seating chart” parti-
tions the customers. It can be shown that, in expectation, the
number of occupied tables after n customers is Θ(log n) (An-
toniak, 1974; Navarro et al., 2006).

The tasks within each cluster of the partition will share
the same parameterization. Extending our generative model,
imagine that when a new user enters the restaurant and sits at
a new table, they draw a complete parameterization of their
Naive Bayes model from some base distribution. This param-
eterization is then associated with their table. If a user sits at
an occupied table, they adopt the parameterization already as-
sociated with the table. Therefore, everyone at the same table
uses the same rules for predicting.

This generative process corresponds with the well known
Dirichlet Process mixture model (DPM) and has been used
very successfully to model latent groups (Ferguson, 1973).
The underlying Dirichlet process has two parameters, a mix-
ing parameter α, which corresponds to the same parameter of
the CRP, and a base distribution, from which the parameters
are drawn at each new table. It is important to specify that we
draw a complete parameterization of all the feature distribu-
tions, θy,f , at each table. We have decided not to share the
marginal distributions, φ, because we are most interested in
knowledge relating the features and labels.

Again, we can represent the model compactly by specify-

ing the generative process:

φu ∼ Dir([αu,y : y ∈ L])
Y (u,n) | φ ∼ Discrete(φu)

F(cid:2)

(cid:2)

f =1

y∈L

G ∼ DP(α,

Dir([βy,f,x : x ∈ Vf ])

θu = (θu,y,f )

y∈L
f =1,2,...,F ∼ G

f

X (u,n)
| Y (u,n), {θu,y,f : ∀y ∈ L} ∼ Discrete(θ(u,Y (u,n)),f )
The discrete measure G is a draw from the Dirichlet Process;
in practice, this random variable is marginalized over. Be-
cause the tasks are being clustered, we have named this model
the Clustered Naive Bayes model (and denote its distribution
function over the parameters as F
CNB). We now explain how
to use the model to make predictions given training data.

3 Approximate Inference
Given labelled training data D(u) = {Y (u,i), X (u,i)}1≤i≤Nu
for all tasks u ∈ {1, 2, . . . , U } and an unlabeled feature vec-
tor X ∗ (cid:2) X (v,Nv+1)
for some task v, we would like to com-
pute the posterior distribution of its label Y ∗ (cid:2) Y (v,Nv+1)
.
Using Bayes rule, and ignoring normalization constants,

p(Y ∗

= y|X ∗, D) ∝ p(X ∗|Y ∗

= y, D) p(Y ∗

= y, D)

(cid:4)

∝

Θ×Φ

p(D(cid:5)|θ, φ) dF

CNB(θ, φ),

where D(cid:5)
is the data set where we imagine that the (v, Nv +
1)-th data point has label y. Therefore, Bayesian inference
requires that we marginalize over the parameters, including
the latent partitions of the Dirichlet process. Having cho-
sen conjugate priors, the base distribution can be analytically
marginalized. However, the sum over all partitions makes
exact inference under the Dirichlet Process mixture model
intractable. While Markov Chain Monte Carlo and varia-
tional techniques are the most popular approaches, this paper
uses a simple, recently-proposed approximation to the DPM
known as Bayesian Hierarchical Clustering (BHC) (Heller
and Ghahramani, 2005a), which approximates the sum over
all partitions by ﬁrst greedily generating a hierarchical clus-
tering of the tasks and then efﬁciently summing over the ex-
ponential number of partitions “consistent” with this hierar-
chy. This approach leads to a simple, yet efﬁcient, algorithm
for achieving task-level transfer.

Consider a rooted binary tree T where each task is associ-
ated with a leaf. It will be convenient to identify each internal
node, Tn, with the set of leaves descending from that node. A
tree-consistent partition of the tasks is any partition such that
each subset corresponds exactly with a node in the graph (Fig-
ure 2). It can seen that, given any rooted tree over more than
two objects, the set of all tree-consistent partitions is a strict
subset of the set of all partitions. Exact inference under the
DPM requires that we marginalize over the latent partition,
requiring a sum over the super-exponential number of parti-
tions. The BHC approximation works by efﬁciently comput-
ing the sum over the exponential number of tree-consistent
partitions, using a divide-and-conquer approach to combine
the results from each subtree. Intuitively, if the tree is chosen
carefully, then the set of tree-consistent partitions will cap-
ture most of the mass in the posterior. BHC tries to ﬁnd such
a tree by combining Bayesian model selection with a greedy
heuristic.

Just as in classic, agglomerative clustering (Duda et al.,
2001), BHC starts with all objects assigned to their own clus-
ter and then merges these clusters one by one, implicitly

IJCAI-07

2601

9

8

7

6

1

2 3 4 5

{9}
{6,8}
{3,6,7}
{3,4,5,6}
{1,2,8}
{1,2,3,7}
{1,2,3,4,5}
Inconsistent

[12345]
[12][345]
[12][3][45]
[12][3][4][5]
[1][2][345]
[1][2][3][45]
[1][2][3][4][5]
[123][45]

Figure 2: All tree-consistent partitions represented both as sets of
nodes (left) and collection of leaves (right), and one partition that is
not tree-consistent (the sets of leaves [123] is not representable by
an internal node).

forming a tree that records which merges were performed.
However, instead of using a distance metric and merging the
nearest clusters, BHC merges the two clusters that maximize
a statistical hypothesis test. At each step, the algorithm must
determine which pair in the set of clusters T1, T2, . . . , Tm to
merge next. Consider two particular clusters Ti and Tj and
let Di and Dj be the set of tasks in each respectively. The
BHC algorithm calculates the posterior probability that these
two clusters are in fact a single cluster Tk = Ti + Tj. Specif-
ically, the hypothesis Hk is that the data in Dk = Di ∪ Dj
are identically distributed with respect to the base model (in
this case, some Naive Bayes model). The probability of the
data in this new cluster under Hk, p(Dk|Hk) is simply the
marginal likelihood of the data.

The alternative hypothesis, ¯Hk is that the data Di and Dj
are, in fact, split into two or more clusters. Computing the
probability associated with this hypothesis would normally
require a sum over the super-exponential number of partitions
associated with the tasks in Di and Dj. However, the clever
trick of the BHC algorithm is to restrict its attention to tree-
consistent partitions. Therefore, the probability of the data
Dk = Di ∪ Dj under ¯Hk, p(Dk| ¯Hk) = p(Di|Ti) p(Dj|Tj),
where p(Di|Ti) is the probability of the data associated with
the tree Ti. Let πk = p(Hk) be the prior probability of the
cluster Tk. Then, we can write p(Dk|Tk) recursively
p(Dk|Tk) = πkp(Dk|Hk)+(1−πk)p(Di|Ti)p(Dj|Tj). (2)
Then the posterior probability of Hk is

p(Hk|Dk) =

πkp(Dk|Hk)

p(Dk|Tk)

.

(3)

We now present the BHC algorithm, whose output is sufﬁ-
cient for approximate Bayesian predictions under our model.
input data D = {D(1), . . . , D(n)}

model p(X|Y, θ) and prior density f (θ)

initialize number of clusters c=n, and
Di = {D(i)} for i = 1, . . . , n

while c > 1 do

Find the pair Di and Dj with the highest posterior

probability of Hk where Tk = Ti + Tj.

Merge Dk ← Dk ∪ Dj, Tk ← (Ti, Tj)
Delete Di and Dj, c ← c − 1

end while

Heller and Ghahramani (2005a) show that a speciﬁc choice
of the prior πk = p(Hk) leads to an approximate inference
scheme for the DPM. Let α be the corresponding parameter
from the DPM. Then, we can calculate the prior probability
for each cluster Tj in the tree built by BHC.
initialize each leaf i to have di = α, πi = 1
for each internal node k do

dk = αΓ(nk) + dleftk
πk =
end for

αΓ(nk)

dk

drightk

Having built a tree that approximates the posterior distribu-
tion over partitions, we can use the tree to compute the pos-
terior probability of an unseen label. Assume we have an
unlabeled example Xk associated with the k-th task. Let Ak
be the set of nodes along the path from the node k to the root
in the tree generated by the BHC algorithm (e.g. in Figure 2,
A5 = {5, 7, 8, 9}). Note that the elements Ti ∈ Ak corre-
spond to all clusters that task k participates in across all tree-
consistent partitions. Our predictive distribution for Y will
then be the weighted average of the predictive distributions
for each partition:

(cid:6)

wi(cid:3)

p(Yk|Xk, D, T ) =

p(Yk|Xk, Di),

(4)

wj

j∈Ak

Ti∈Ak

(cid:5)
i∈Ak/{k}(1 − ri) and p(Yk|Xk, Dk) is the
where wk = rk
predictive distribution under the base model after combining
the data from all the tasks in cluster k.

While the computational complexity of posterior computa-
tion is quadratic in the number of tasks, Heller and Ghahra-
mani (2005b) have proposed O(n log n) and O(n) random-
ized variants.

4 Results

The utility of the type of sharing that the Clustered Naive
Bayes supports can only be assessed on real data sets. To that
end, we evaluated the model’s predictions on a meeting clas-
siﬁcation data set collected by Rosenstein et al. (2005). The
data set is split across 21 users from multiple universities, an
industry lab and a military training exercise. In total, there
are 3966 labeled meeting requests, with 100-400 meeting re-
quests per user. In the meeting acceptance task, we aim to
predict whether a user would accept or reject an unseen
meeting request based on a small set of features that describe
various aspects of the meeting.

To evaluate the clustered model, we assessed its predictive
performance in a transfer learning setting, predicting labels
for a user with sparse data, having observed all the labeled
data for the remaining users. In particular, we calculated the
receiver-operator characteristic (ROC) curve having trained
on 1,2,4,8,16,32,64, and 128 training examples for each user
(conditioned on knowledge of all labels for the remaining
users). Each curve was generated according to results from
twenty random partitions of the users’ data into training and
testing sets. Figure 3 plots the area under the ROC curve as
a measure of classiﬁcation performance versus the number of
training examples.

IJCAI-07

2602

1

0.9

0.8

0.7

0.6

0.5

C
O
R

 
r
e
d
n
u

 

a
e
r
a

User #1: AB (Mil)

]

]

7

 

4

 

]

2

 

[
 

1
←

]

8

 

5

 

]

3

 

[
 

1
←

[
 

1
←

[
 

1
←

3

 

2

 

]

3

 

[
 

1
←

1
←

[
 

Clus.
NS

User #3: ED (Mil)

User #20: TD (Oregon S.)
1

]

1

User #17: TLP (MIT)

]

0
2

]

 

[
 

9
←

0
2
←

[
 

]

0
2

 

[
 

9
←

]

3

 

[
 

1
←

0.9

0.8

0.7

0.6

0.5

]

7
1

 

2
1
←

[
 

]

7
1
←

[
 

0
2
←

[
 

0.9

0.8

0.7

0.6

0.5

]

7
1
←

[
 

]

7
1

 

[
 

9
←

1

0.9

0.8

0.7

0.6

0.5

User #7: NH (Mil)

]

7

 

[
 

4
]←
7
5

 

]

7

 

[
 

1
←

 

[
 

1
←

1

0.9

0.8

0.7

0.6

0.5

0 1 2 3 4 5 6 7

2n samples

0 1 2 3 4 5 6 7

2n samples

0 1 2 3 4 5 6 7

2n samples

0 1 2 3 4 5 6 7

2n samples

0 1 2 3 4 5 6 7

2n samples

Figure 3: Area under the curve (AUC) vs. training size for ﬁve representative users. The AUC varies between 1 (always correct), 0 (always
wrong), and 0.5 (chance). For each experiment we label the (MAP) cluster of users to which the user belongs. If the cluster remains the same
for several experiments, we omit all but the ﬁrst mention. The ﬁrst three examples illustrate improved predictive performance. The last two
examples demonstrate that it is possible for performance to drop below that of the baseline model.

16 examples

32 examples

64 examples

128 examples

B
N

C
O
R
u
A
−

 

 

B
N
C

C
O
R
u
A
e
c
n
e
r
e

 

0.1

0.08

0.06

0.04

0.02

0

MMMMMMMMPPPPSSPSSPPPP

MMMMMMMMPPPPPPPPSPSSS

MMMMMMMMPPPPPPSPPSPSS

MMMMMMMMPPPPPPPSSPSPS

Figure 4: Progression of trees found by BHC for 16, 32, 64 and
128 examples per user. Short vertical edges indicate that two tasks
are strongly related. Long vertical edges indicate that the tasks are
unrelated. Key: (M)ilitary, (P)rofessor, (S)RI researcher.

From the 21 users, we have selected ﬁve representative
samples. The ﬁrst three examples (users 1, 3 and 20) show
how the model performs when it is able to use related user’s
data to make predictions. With a single labeled data point,
the model groups user 1 with two other military personnel
(users 5 and 8). While at each step the model makes pre-
dictions by averaging over all tree-consistent partitions, the
MAP partition listed in the ﬁgure has the largest contribution.
For user 1, the MAP partition changes at each step, provid-
ing superior predictive performance. However, for the third
user in the second ﬁgure, the model chooses and sticks with
the MAP partition that groups the ﬁrst and third user. In the
third example, User 20 is grouped with user 9 initially, and
then again later on. Roughly one third of the users witnessed
improved initial performance that tapered off as the number
of examples grew.

The fourth example (user 17) illustrates that, in some cases,
the initial performance for a user with very few samples is not
improved because there are no good candidate related users
with which to cluster. Finally, the last example shows one
of the four cases where predictions using the clustered model
leads to worse performance. In this speciﬁc case, the model
groups the user 7 with user 1.
It is not until 128 samples
that the model recovers from this mistake and achieves equal
performance.

Figure 4 shows the trees and corresponding partitions re-
covered by the BHC algorithm as the number of training ex-
amples for each user is increased. Inspecting the partitions,
they fall along understandable lines; military personnel are

f
f
i

 

D
n
a
e
M

−0.02

−0.04

1

2

3

4
5
2n Training Samples

6

7

8

Figure 5: The clustered model has more area under the ROC curve
than the standard model when there is less data available. After 32
training examples, the standard model has enough data to match the
performance of the clustered model. Dotted lines are standard error.

most often grouped with other military personnel, and pro-
fessors and SRI researchers are grouped together until there
is enough data to warrant splitting them apart.

Figure 5 shows the relative performance of the clustered
versus standard Naive Bayes model. The clustered variant
outperforms the standard model when faced with very few
examples. After 32 examples, the models perform roughly
equivalently, although the standard model enjoys a slight ad-
vantage that does not grow with more examples.

5 Related Work

Some of the earliest work related to transfer learning focused
on sequential transfer in neural networks, using weights from
networks trained on related data to bias the learning of net-
works on novel tasks (Caruana, 1997; Pratt, 1993). More re-
cently, these ideas have been applied to modern supervised
learning algorithms, like support vector machines (Wu and
Dietterich, 2004). More work must be done to understand the
connection between these approaches and the kind of sharing
one can expect from the Clustered Naive Bayes model.

This work is related to a large body of transfer learning
research conducted in the hierarchical Bayesian framework,
in which common prior distributions are used to tie together
model components across multiple data sets. The clustered
model can be seen as an extension of the model ﬁrst pre-
sented by Rosenstein et al. (2005) for achieving transfer with
the Naive Bayes model. In that work, they ﬁt a Dirichlet dis-
tribution for each shared parameter across all users. Unfortu-

IJCAI-07

2603

nately, because the Dirichlet distribution cannot ﬁt arbitrary
bimodal distributions, the model cannot handle more than one
cluster, i.e. each parameter is shared completely on not at
all. The model presented in this paper can handle any num-
ber of users by modelling the density over parameters using
a Dirichlet Process prior. It is possible to loosely interpret
the resulting Clustered Naive Bayes model as grouping tasks
based on a marginal likelihood metric. From this viewpoint,
this work is related to transfer-learning research which aims
to ﬁrst determine which tasks are relevant before attempting
transfer (Thrun and O’Sullivan, 1996).

Ferguson (1973) was the ﬁrst to study the Dirichlet Process
and show that it can, simply speaking, model any other dis-
tribution arbitrarily closely. The Dirichlet Process has been
successfully applied to generative models of documents (Blei
et al., 2004), genes (Dahl, 2003), and visual scenes (Sudderth
et al., 2005). Teh et al. (2006) introduced the Hierarchical
Dirichlet Process, which achieves transfer in document mod-
eling across multiple corpora. The work closest in spirit to
this paper was presented recently by Xue et al. (2005). They
investigate coupling the parameters of multiple logistic re-
gression models together using the Dirichlet Process prior
and derive a variational method for performing inference. In
the same way, the Clustered Naive Bayes model we introduce
uses a Dirichlet Process prior to tie the parameters of sev-
eral Naive Bayes models together for the purpose of transfer
learning. There are several important differences: ﬁrst, the
logistic regression model is discriminative, meaning that it
does not model the distribution of the inputs. Instead, it only
models the distribution of the output labels conditioned on
the inputs. As a result, it cannot take advantage of unlabeled
data. Second, in the Clustered Naive Bayes model, the data
sets are clustered with respect to a generative model which
deﬁnes a probability distribution over both the inputs. As a
result, the Clustered Naive Bayes model could be used in a
semi-supervised setting. Implicit in this choice is the assump-
tion that similar feature distributions are associated with sim-
ilar predictive distributions. This assumption must be judged
for each task: for the meeting acceptance task, the generative
model of sharing is appropriate and leads to improved results.

6 Conclusion

The central goal of this paper was to evaluate the Clustered
Naive Bayes model in a transfer-learning setting. To evalu-
ate the model, we measured its performance on a real-world
meeting acceptance task, and showed that the clustered model
can use related users’ data to provide better prediction even
with very few examples.

The Clustered Naive Bayes model uses a Dirichlet Process
prior to couple the parameters of several models applied to
separate tasks. This approach is immediately applicable to
any collection of tasks whose data are modelled by the same
parameterized family of distributions, whether those models
are generative or discriminative. This paper suggests that
clustering parameters with the Dirichlet Process is worth-
while and can improve prediction performance in situations
where we are presented with multiple, related tasks. A theo-
retical question that deserves attention is whether we can get

improved generalization bounds using this technique. A log-
ical next step is to investigate this model of sharing on more
sophisticated base models and to relax the assumption that
users are exactly identical.

References
D. Aldous. Exchangeability and related topics. Springer Lecture

Notes on Math, 1117, 1985.

C. Antoniak. Mixtures of Dirichlet processes with applications to

Bayesian nonparametric problems. Annals of Stat., 2, 1974.

J. Baxter. A model of inductive bias learning. JAIR, 12, 2000.

D. Blei, T. Grifﬁths, M. Jordan, and J. Tenenbaum. Hierarchical
topic models and the nested Chinese restaurant process. NIPS,
2004.

B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm

for optimal margin classiﬁers. COLT, 1992.

R. Caruana. Multitask Learning. Machine Learning, 28, 1997.

D. B. Dahl. Modeling differential gene expression using a Dirichlet

process mixture model. JASA, 2003.

R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation. John

Wiley and Sons, 2001.

T. Ferguson. A Bayesian Analysis of Non-parametric Problems. An-

nals of Stat., 1, 1973.

C. Guestrin, D. Koller, C. Gearhart, and N. Kanodia. Generalizing

Plans to New Environments in Relational MDPs. IJCAI, 2003.

K. Heller and Z. Ghahramani. Bayesian Hierarchical Clustering.

ICML, 2005a.

K. Heller and Z. Ghahramani. Randomized Algorithms for Fast
In PASCAL Workshop on

Bayesian Hierarchical Clustering.
Statistics and Optimization of Clustering, 2005b.

J. Lafferty, A. McCallum, and F. Pereira. Conditional Random
Fields: Probabilistic Models for Segmenting and Labeling Se-
quence Data. ICML, 2001.

M. E. Maron. Automatic indexing: An experimental inquiry. JACM,

8(3), 1961.

D. Navarro, T. Grifﬁths, M. Steyvers, and M. Lee. Modeling individ-
ual differences using Dirichlet processes. J. of Math. Psychology,
50, 2006.

L. Y. Pratt. Non-literal Transfer Among Neural Network Learners.
Artiﬁcial Neural Networks for Speech and Vision. Chapman and
Hall, 1993.

M. Rosenstein, Z. Marx, L. P. Kaelbling, and T. G. Dietterich. Trans-
fer Learning with an Ensemble of Background Tasks. NIPS Work-
shop on Inductive Transfer: 10 Years Later, 2005.

E. Sudderth, A. Torralba, W. Freeman, and A. Willsky. Describ-
ing Visual Scenes using Transformed Dirichlet Processes. NIPS,
2005.

Y. W. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical Dirichlet

Processes. JASM, 2006.

S. Thrun. Is learning the n-th thing any easier than learning the ﬁrst?

NIPS, 1996.

S. Thrun and J. O’Sullivan. Discovering Structure in Multiple Learn-

ing Tasks: The TC algorithm. ICML, 1996.

P. Wu and T. G. Dietterich. Improving SVM accuracy by training on

auxiliary data sources. ICML, 2004.

Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. Learning multiple

classiﬁers with Dirichlet process mixture priors. NIPS, 2005.

IJCAI-07

2604


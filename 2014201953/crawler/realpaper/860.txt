Real Boosting a la Carte with an Application to Boosting Oblique Decision Trees

Claudia Henry

Richard Nock

U. Antilles-Guyane / CEREGMIA

97275 Schoelcher, Martinique, France
{chenry,rnock}@martinique.univ-ag.fr

Frank Nielsen

SONY CS Labs Inc.

Tokyo 141-0022, Japan
Nielsen@csl.sony.co.jp

Abstract

In the past ten years, boosting has become a major
ﬁeld of machine learning and classiﬁcation. This
paper brings contributions to its theory and algo-
rithms. We ﬁrst unify a well-known top-down de-
cision tree induction algorithm due to [Kearns and
Mansour, 1999], and discrete AdaBoost [Freund
and Schapire, 1997], as two versions of a same
higher-level boosting algorithm.
It may be used
as the basic building block to devise simple prov-
able boosting algorithms for complex classiﬁers.
We provide one example:
the ﬁrst boosting al-
gorithm for Oblique Decision Trees, an algorithm
which turns out to be simpler, faster and signiﬁ-
cantly more accurate than previous approaches.

1 Introduction
Loosely speaking, a boosting algorithm repeatedly trains
moderately accurate learners from which it gets its weak hy-
potheses, and combines them to output a strong classiﬁer
which boosts the accuracy to arbitrary high levels [Kearns
and Valiant, 1989]. A pioneering paper [Schapire, 1990]
proved the existence of such boosting algorithms, and another
one drew the roots of their most popular representative: Ad-
aBoost, that builds a linear combination of the predictions of
the weak hypotheses [Freund and Schapire, 1997]. Another
paper due to [Kearns and Mansour, 1999] proved that some of
the most popular top-down decision tree induction algorithms
are also boosting algorithms in disguise [Breiman et al., 1984;
Quinlan, 1993]. These two kinds of algorithms are outwardly
different from each other: while AdaBoost repeatedly modi-
ﬁes weights on the training examples and directly minimizes
a convex exponential loss, top-down decision tree induction
algorithms do not modify weights on the examples, and they
minimize the expectation of a concave, so-called permissible
function [Kearns and Mansour, 1999].

This explains why the starting point of this paper is a quite
surprising result, as we prove that these two kinds of algo-
rithms are in fact the same algorithm, whose induction is per-
formed on two different classiﬁer “graphs”. The apparent per-
ceptual differences in the algorithms stem only from different
structural properties of the classiﬁers. This suggests a generic
induction scheme which gives, for many classiﬁers that meet

a simple assumption, a corresponding boosting algorithm.
Using a recent result due to [Nock and Nielsen, 2006], this
algorithm uses real-valued weak hypotheses, but it does not
face the repetitive minimization of the exponential loss faced
by previous Real boosting algorithms [Friedman et al., 2000;
Schapire and Singer, 1999]. When the classiﬁer is an oblique
decision tree, the algorithm obtained has three key features:
it is fast, simple, and the ﬁrst provable boosting algorithm
that fully exploits this class. This is important, as the prob-
lem of inducing oblique decision trees has a longstanding his-
tory, and the algorithms available so far are either time con-
suming or complex to handle formally [Breiman et al., 1984;
Cant´u-Paz and Kamath, 2003; Heath et al., 1993; Murthy et
al., 1994; Shawe-Taylor and Cristianini, 1998]. The follow-
ing Section presents deﬁnitions; it is followed by a Section on
our theoretical results, and a Section that discusses them and
gives our boosting algorithm for oblique decision trees; two
last Sections discuss experiments, and conclude. For the sake
of clarity, proofs have been postponed to an appendix.

2 Deﬁnitions and problem statement

Bold-faced variables such as w and x, represent vectors.
Unless otherwise stated, sets are represented by calligraphic
upper-case alphabets, e.g. X , and (unless explicitely stated)
enumerated following their lower-case, such as {xi
: i =
1, 2, ...} for vector sets, and {xi : i = 1, 2, ...} for other sets.
Consider the following supervised learning setting. Let X
denote a domain of observations of dimension n, such as Rn,
{0, 1}n, etc. . We suppose a set S of |S| = m examples, with
|.| the cardinal notation, deﬁned as S = {si = (cid:2)xi, yi(cid:3) ∈
X × {−1, +1} : i = 1, 2, ..., m}, onto which a discrete dis-
tribution w1 is known. “+1” is called the positive class, and
“-1” the negative class. Our primary objective is related to
building an accurate classiﬁer (or hypothesis) H : X → R.
The goodness of ﬁt of H on S may be evaluated by two quan-
tities:

ε(H, w1) = Ew1(1[sign(H(x))(cid:2)=y]) ,
εexp(H, w1) = Ew1(exp(−yH(x))) .

(2)
Here, sign(a) = +1 iff a ≥ 0 and −1 otherwise, 1[.] is the 0/1
indicator variable, and E.() is the expectation. (1) is the con-
ventional empirical risk, and (2) is an upperbound, the expo-
nential loss [Schapire and Singer, 1999]. We are interested in

(1)

IJCAI-07

842

Input: S, w1;
for t = 1, 2, ..., T

Input: S, w1;
for t = 1, 2, ..., T

Input: S, w1;
for t = 1, 2, ..., T

ht ← Tree(S, wt);
αt ← arg minα∈R Ewt (exp(−yαht(x)));
for i = 1, 2, ..., m

wt+1,i ← wt,i exp(−αtyiht(xi))/Zt;

let (cid:3) ∈ leaves(Ht−1) containing
examples of both classes;

let (cid:3) ∈ leaves(Ht−1) containing
examples of both classes;

ht ← Stump(S(cid:2), w1);
(cid:3) ← ht;

ht ← StumpLS(S(cid:2), w1);
(cid:3) ← ht;

AdaBoost + Trees

TopDown DT (CART, C4.5)

TopDown ODT (OC1, SADT)

Figure 1: An abstraction of 3 core greedy procedures of some popular induction algorithms (see text for references and details).

the stagewise, greedy ﬁtting of H to S, from scratch. A large
number of classes of classiﬁers have popular induction algo-
rithms whose core procedure follows this scheme, including
Decision Trees (DT) [Breiman et al., 1984; Quinlan, 1993],
Branching Programs (BP) [Mansour and McAllester, 2002]
and Linear Separators (LS) [Freund and Schapire, 1997;
Nock and Nielsen, 2006]. Virtually all these procedures share
another common-point: the components that are used to grow
H are also classiﬁers. These are the DTs for AdaBoost with
trees, the internal node splits for the DTs and BPs (akin to
decision stumps), etc.

(cid:4)

;

on S (cid:4)

and (discrete) distribution w

Now, given some ﬁxed class of classiﬁers in which we ﬁt
H, here is the problem statement. A weak learner (WL) is
assumed to be available, which can be queried with a sam-
ple S (cid:4)
the assump-
tion of its existence is called a Weak Learning Assump-
tion (WLA). It returns in polynomial time1 a classiﬁer h
(cid:4)) ≤
on which only the following can be assumed: ε(h, w
1/2 − γ for some γ > 0 [Schapire and Singer, 1999].
Given 0 <  < 1, is it possible to build in polynomial
time some H with ε(H, w1) ≤ , after having queried T
times WL for classiﬁers h1, h2, ..., hT , for some T > 0 ?
Provided additional assumptions are made for its generaliza-
tion abilities (see Section “Discussion” below), this algorithm
is called a boosting algorithm [Freund and Schapire, 1997;
Kearns and Valiant, 1989].

T

(cid:2)

3 Unifying boosting properties
For the sake of clarity, we now plug T in subscript on
H; An element of LS is a classiﬁer HT with HT (x) =
t=1 αtht(x), where αt ∈ R is a leveraging coefﬁcient that
may be interpreted as a conﬁdence in ht. An element of DT is
a rooted directed tree in which each internal node supports a
single boolean test over some observation variable, and each
leaf is labeled by a real. The classiﬁcation of some obser-
vation proceeds by following, from the root, the path whose
tests it satisﬁes, until it reaches a leaf which gives its class.
Figure 2 (left) presents an example of DT (n = 2), where
v1, v2 are Boolean description variables. Finally, Oblique De-
cision Trees (ODT) generalizes DT, as linear combinations of
variables are authorized for the internal nodes, which allows
splits that are oblique instead of just axis-parallel [Breiman et
al., 1984].

1For the sake of simplicity, polynomial time means polynomial

in the relevant parameters throughout the paper.

Figure 1 gives an abstract view of some of the most pop-
ular induction algorithms, or at least of their core procedure
which induces a large classiﬁer: AdaBoost with trees [Freund
and Schapire, 1997; Schapire and Singer, 1999], C4.5 [Quin-
lan, 1993], CART [Breiman et al., 1984], OC1 [Murthy et
al., 1994], SADT [Heath et al., 1993]; the ﬁrst algorithm for
the induction of ODT dates back to the mid-eighties; it was a
descendant of CART [Breiman et al., 1984]. Most of the in-
duction algorithms for DT or ODT integrate a pruning stage;
here, we are only interested in the greedy induction scheme.
WL has various forms: it induces a decision tree on the whole
sample S and on a distribution which is repeatedly modiﬁed
for AdaBoost; it induces a decision tree with a single internal
node (a stump) on the subset of S that reaches leaf (cid:3) (noted
S(cid:2)) and on a distribution which is not modiﬁed for CART and
C4.5; it replaces this ordinary, axis-parallel stump, by a linear
separator stump (also called stump for the sake of simplicity)
for OC1 and SADT. The choice of a split for DT and ODT re-
lies on the repetitive minimization of an “impurity” criterion
which is the expectation, over the leaves of the current tree
H, of a so-called permissible function [Kearns and Mansour,
1999]:

(cid:3)

(cid:3)

(cid:4)(cid:4)

εimp(H, w1, f ) = E(cid:2)∼leaves(H)

f

; (3)

w+
1,S(cid:2)
w1,S(cid:2)

(cid:5)

in w1 and w+
.

here, w1,S (cid:2) is the total weight of S (cid:4)
is its
weight further restricted to the positive class. In the expec-
tation, the weight of a leaf (cid:3) is w1,S(cid:2) . The permissible func-
tion f : [0, 1] → [0, 1] has to be concave, symmetric around
1/2, and with f (0) = f (1) = 0 and f (1/2) = 1. Ex-
amples of permissible functions are f (z) = 4z(1 − z) for
Gini index [Breiman et al., 1984], f (z) = −z log z − (1 −
z) log(1 − z) for the entropy [Quinlan, 1993] (log base-2), or
z(1 − z) for the optimal choice of [Kearns
even f (z) = 2
and Mansour, 1999]. Remark that we have εimp(H, w1, f ) ≥
εimp(H, w1, 2 min{z, 1 − z}) = 2ε(H, w1), for any per-
missible f , and so minimizing (3) amount to minimizing
the empirical risk of H as well. The reason why this is
a better choice than focusing directly on the empirical risk
comes from the concavity of the permissible function [Kearns
and Mansour, 1999]. Though seemingly very different from
each other, AdaBoost and DT induction algorithms from the
CART-family were independently proven to be boosting al-
gorithms [Freund and Schapire, 1997; Kearns and Mansour,
1999; Schapire and Singer, 1999]. So far, no such formal

IJCAI-07

843

v2 = 0

v2 = 1

h1

v2 = 0

v2 = 1

v1 = 0

v1 = 1

v1 = 0

v1 = 1

+1

h2

h3

+1 −1

h4

h5

(α1, h1) = (−2, +1)
(α2, h2) = (3, +1)
(α3, h3) = (3, +1)
(α4, h4) = (0, +1)(α5, h5) = (2, −1)

Input: S, w1;
for t = 1, 2, ..., T

compute St;
ht ← WL(St, wt);
for i = 1, 2, ..., m

(cid:7)

wt+1,i←wt,i×
for t = 1, 2, ..., T
αt ← (1/(2h(cid:4)

1−(μtyiht(xi)/h(cid:3)
t )

1−μ2
t

1

if si ∈ St
if si ∈ S\St

;

t )) ln((1 + μt)/(1 − μt));

Figure 2: A DT with 3 leaves and 2 internal nodes (left) and
an equivalent representation which ﬁts to (A) (right).

GenericGreedy (G2)

boosting algorithm or proof exist for ODT that would take
full advantage of their structure combining oblique stumps
and decision trees. For this objective, let us shift to a more
general standpoint on our problem, and address the following
question: what kind of classiﬁers may be used to solve it ?
Consider the following assumption, that HT represents some
kind of local linear classiﬁer:

(assumption A) ∀T > 0, denote HT = {h1, h2, ..., hT }
the set of outputs of WL; then, there exists a function
which maps any observation to a non-empty subset of
HT , gHT : X → 2HT \{∅}. ∀x ∈ X , gHT (x) is re-
quired to be computable polynomially in n and the size
of HT ; it describes the classiﬁcation function of HT , in
the following way:

(cid:6)

HT (x) =

ht∈gHT (x)

αtht(x) , ∀x ∈ X .

By means of words, the classiﬁcation of HT is obtained by
summing the outputs of some elements of HT . Many classi-
ﬁers satisfy (A), such as decisition trees, decision lists, linear
separators, etc.
Deﬁnition 1 The decision graph of HT (on X ) is an oriented
graph G = (HT , E); an arc (ht, ht(cid:2) ) ∈ E iff t < t(cid:4)
and there
exists some x ∈ X such that ht, ht(cid:2) ∈ gHT (x) and no ht(cid:2)(cid:2)
with t < t(cid:4)(cid:4) < t(cid:4)
Remark that G is acyclic, and gHT maps each observation of
X to some path of G. We also deﬁne P as representing the
set of paths of G that is mapped from X by gHT :

is in gHT (x).

P = {p ⊆ HT : ∃x ∈ X , p = gHT (x)} .

The simplest case of G is obtained when gHT (x) =
HT , ∀x ∈ X : HT is a linear separator, and G a single path
from h1 to hT . Examples of classes of classiﬁers different
from linear separators and that ﬁt to (A) include DT and ODT.
In this case, G is a tree and weak hypotheses are in fact con-
stant predictions on the tree nodes. Figure 2 (right) displays
the way we can represent a DT so as to meet (A). Remark
that there exists many possible equivalent transformations of
a DT; the way we induce HT shall favor a single one out of
all, as follows. We deﬁne two types of subsets of S (with
p ∈ P and 1 ≤ t ≤ T + 1):

Sp = {(cid:2)xi, yi(cid:3) ∈ S : p = gHT (xi)} ,
St = ∪p∈P:ht∈pSp .

(4)

(5)

Figure 3: The generic greedy induction of HT . The compu-
tation of St depends on the decision graph of Ht.

(cid:2)

(cid:2)

si∈Sp wt(cid:2),i and wt(cid:2),St =

We also extend the weight notation, and let wt(cid:2),Sp =
si∈St wt(cid:2),i (with p ∈ P and
1 ≤ t(cid:4) ≤ T + 1). We also deﬁne h(cid:4)
|ht(xi)| ∈
R, the maximal absolute value of ht over St. After [Nock and
Nielsen, 2006], we deﬁne two notions of margins: the ﬁrst is
the normalized margin ht over S:

t = maxsi∈St

(cid:6)

μt =

1

h(cid:4)
t wt,St

si∈St

wt,iyiht(xi) ∈ [−1, +1] .

(6)

With the help of this deﬁnition, Figure 3 presents the generic
greedy induction of HT . Remark that when HT is a linear
separator, G2 is the AdaBoostR boosting algorithm of [Nock
and Nielsen, 2006]. The second margin deﬁnition is the mar-
gin of HT on example (cid:2)x, y(cid:3) [Nock and Nielsen, 2006]:
νT ((cid:2)x, y(cid:3)) = tanh(yHT (x)/2) ∈ [−1, +1] .

(7)

This margin comes to mind from the relationships between
boosting and the logistic prediction HT (x) = log(Pr[y =
+1|x]/Pr[y = −1|x]) [Friedman et al., 2000]. In this case,
(7) becomes νT ((cid:2)x, y(cid:3)) = y(Pr[y = +1|x] − Pr[y =
−1|x]), whose expectation is the normalized margin (6) of
the Bayesian prediction in [−1, +1] (also called gentle lo-
gistic approximation [Friedman et al., 2000]). Following
[Nock and Nielsen, 2006], we deﬁne the margin error of HT
(∀θ ∈ [−1, +1]):

νw1,HT ,θ = Ew1 (1[νT (s)≤θ]) ,

(8)
and we have ε(HT , w1) ≤ νw1,HT ,0. Thus, minimizing
νw1,HT ,0 would amount to minimize ε(HT , w1) as well. The
following Theorem shows much more, as under some weak
conditions, we can show that νw1,HT ,θ is minimized for all
values of θ ∈ [−1, +1) (and not only for θ = 0).
Theorem 1 After T ≥ 1 queries of WL, the classiﬁer ob-
tained, HT , satisﬁes:

(cid:8)

(cid:9)

(cid:6)

(cid:10)

(cid:11)

νw1,HT ,θ ≤

1 + θ
1 − θ

×

wT +1,Sp

p∈P

ht∈p

1 − μ2

t , (9)

for any θ ∈ [−1, +1].
(proof in appendix). To read Theorem 1, consider the follow-
ing WLA for real-valued weak hypotheses, borrowed from
[Nock and Nielsen, 2006]:

IJCAI-07

844

(WLA) |μt| ≥ γ, for some γ > 0 (∀t ≥ 1).

the WLA, Theorem 1 states that νw1,HT ,θ ≤
Under
Kθ exp(− minp∈P |p|γ2/2), with Kθ constant whenever θ
is a constant ∈ [−1, +1). In other words, provided the in-
duction in G2 is performed so as to keep paths with roughly
equal size, such as by a breadth-ﬁrst induction of the decision
graph, the margin error is guaranteed to decrease exponen-
tially fast. To see how G2 ﬁts to tree-shaped decision graphs,
consider the following assumption:

(assumption B) (i) each ht ∈ HT is a constant, and (ii)
G is a rooted tree.

Assumption (B) basically restricts HT to be a tree of any arity,
still in which any kind of classiﬁer may be used to split the
internal nodes. As in (3), we use notation w+/−
as the index’
weight for class “+1” or “-1”, respectively.

.

Theorem 2 Suppose that (A) and (B) hold. Then:

HT (x) =

1
2

ln

w+
1,St
−
w
1,St

,

(10)

∀x ∈ X and ht is the leaf of gHT (x). Furthermore, (2)
simpliﬁes as:

(cid:3)

(cid:12)(cid:13)(cid:13)(cid:14) w+
(cid:5)

1,St
w1,St

1 −

w+
1,St
w1,St

z(1 − z)) .

(cid:4)

.(11)

(12)

(cid:6)

εexp(HT , w1) =

w1,St

× 2

ht leaf of G

= εimp(HT , w1, 2

(proof in appendix).

4 Discussion

(cid:5)

4.1 Kearns and Mansour’s algorithm, AdaBoost

and G2

z(1 − z)
The similarity between (12) and (3) with f (z) = 2
is immediate, and quite surprising as it shows the identity be-
tween a convex loss and the expectation of a concave loss.
However, this is not a coincidence. Indeed, Theorem 2 shows
a rather surprising result: the choice of the weak hypotheses
does not impact at all on HT (see (10)). When (A) and (B)
hold, the only way to modify HT is thus through its deci-
sion graph, i.e. on the choice of the splits of the tree. There
is a simple way to choose them, which is to do the same
thing as the most popular LS boosting algorithms [Friedman
et al., 2000; Schapire and Singer, 1999]: repeatedly mini-
mize the exponential loss in (2). Because of Theorem 2, this
amounts to the minimization of the impurity criterion in (3)
z(1 − z). This is exactly the DT induc-
with f (z) = 2
tion algorithm proposed by [Kearns and Mansour, 1999] that
meets the representation optimal bound.

(cid:5)

On the other hand, when HT is a linear separator,
there is no inﬂuence of the decision graph on the induc-
tion of HT as it is a single path from h1 to hT . The
only way to modify HT is thus through the choice of
the weak hypotheses. Suppose that each weak hypothe-
sis has output restricted to the set of classes, {−1, +1}.
In this case, αt = (1/2) ln((1 − ε(ht, wt))/ε(ht, wt)) =

arg minR Ew1 (exp(−yht(x))) (Figure 1), and G2 matches
exactly discrete AdaBoost [Freund and Schapire, 1997].

Thus, decision trees and linear separators are somehow ex-
tremal classiﬁers with respect to G2. Finally, when HT is a
linear separator without restriction on the weak hypotheses,
G2 specializes to AdaBoostR [Nock and Nielsen, 2006].

4.2 All boosting algorithms

In the original boosting setting, the examples are drawn in-
dependently according to some unknown but ﬁxed distribu-
tion D over X , and the goal is to minimize the true risk
ε(HT , D) with high probability, i.e. we basically wish that
ε(HT , D) ≤  with probability ≥ 1 − δ over the sampling of
S [Freund and Schapire, 1997; Kearns and Valiant, 1989].
Two sufﬁcient conditions for a polynomial time induction
algorithm to satisfy this constraint are (i) return HT with
ε(HT , w1) = 0, and (ii) prove that structural parameters of
the class of classiﬁers to which HT belongs satisfy particular
bounds [Freund and Schapire, 1997; Shawe-Taylor and Cris-
tianini, 1998]. Theorem 1 is enough to prove that (i) holds
under fairly general conditions for algorithm G2 in Figure 3
provided WLA holds. For example, T = (2/γ2) log(1/) it-
erations for LS and T = (1/)2/γ 2
for DT are enough to have
ε(HT , w1) ≤  from Theorem 1. Fixing  < mini w1,i easily
yields (i). The bound for LS is the same as AdaBoost (dis-
crete or real) [Schapire and Singer, 1999], while that for DT
improves upon the exponent constant of [Kearns and Man-
sour, 1999]. Finally, (ii) is either immediate, or follows from
mild assumptions on G and WL. As a simple matter of fact,
(i) and (ii) also hold when inducing ODT with G2.

4.3 Recursive Boosting and Oblique Decision

Trees

The preceeding Subsections suggest that G2 could be used
not only to build HT , but also as the core procedure for WL.
For example, it comes from Theorem 2 that AdaBoost + trees
without pruning (Figure 1) is equivalent to G2 growing lin-
ear separators, in which WL is G2 growing a decision tree,
in which WL returns any constant weak hypothesis. In the
general case, we would obtain a recursive/composite design
of the “master” G2, via G2 itself, and the recursion would
end until we reach a WL that can afford exhaustive search for
simple classiﬁers (e.g. axis-parallel stumps, constant classi-
ﬁers, etc.), instead of calling again G2. However, G2 can
also be used to build the decision graph of HT , in the same
recursive fashion. Consider for example the class ODT. The
internal nodes’ splits are local classiﬁers from LS that decide
the path based on the sign of their output, or equivalently, on
the class they would give. This suggest to build the tree with
G2 on both the linear separators in the internal nodes of HT
(use S(cid:2) to split leaf (cid:3), where the linear separator uses ordi-
nary decision stumps), and on the tree shape as well. Call
BoostODT this ODT induction algorithm. It turns out that it
brings a boosting algorithm, that takes full advantage of the
ODT structure. However, this time, it is enough to assume the
WLA one level deeper, i.e. only for the stumps of the linear
separators, and not for the splits of the oblique decision tree.

Theorem 3 BoostODT is a boosting algorithm.

IJCAI-07

845

Xd6

Domain

BoostODT

OC1

 100

 80

 60

 40

 20

 0

Xd6

 100

 80

 60

 40

 20

Xd6

 100

 80

 60

 40

 20

-1

-0.5

 0

 0.5

 0

-1

 1

-0.5

 0

 0.5

 0

-1

 1

-0.5

 0

 0.5

 1

Figure 4: Margin distributions for BoostODT (T1 = 50) on
domain XD6, with 10% (left), 20% (center) and 30% class
noise (right). Stair-shaped (bold plain) curves are the theoret-
ical margins for the logistic model (see text).

The proof, omitted due to the lack of space, builds upon The-
orem (1) plus Lemma 2.1 in [Mansour and McAllester, 2002],
and structural arguments in [Freund and Schapire, 1997;
Shawe-Taylor and Cristianini, 1998]. The proof emphasizes
the relative importance of sizes: suppose that each linear sep-
arator contains the same number of weak hypotheses (T1),
and the tree is complete with T2 internal nodes; then, having

T1 log T2 = Ω((1/γ2) log(1/))

is enough to have ε(HT , w1) ≤ . From an experimental
standpoint, this suggests to build trees with T1 (cid:14) T2.

5 Experiments
We have performed comparisons on a testbed of 25 domains
with two classes, most of which can be found on the UCI
repository of ML databases [Blake et al., 1998]. Comparisons
are performed via ten-fold stratiﬁed cross-validation, against
OC1 and AdaBoost in which the weak learner is C4.5 un-
pruned. BoostODT was ran for T1 ∈ {50, 200} (the weak
hypotheses of the linear separators are decision stumps) and
T2 = 4. To make fair comparisons, we ran AdaBoost for
a total number of T = 5 boosting iterations. This brings
fair comparisons, as an observation is classiﬁed by 5 nodes
(including leaves) in BoostODT, and 5 unpruned trees in Ad-
aBoost. Before looking at the results, BoostODT has proven
to be much faster than OC1 in practice (ten to hundred times
faster). OC1’s time complexity is O(nm2 log m) [Murthy
et al., 1994], without guarantee on its result, while Boos-
tODT’s is O(nm) under the WLA, with the guarantee to
reach empirical consistency in this case. Complexity reduc-
tions have almost the same order of magnitude with respect to
SVM based induction of ODT [Shawe-Taylor and Cristianini,
1998]. Table 1 summarizes the results obtained. With rejec-
tion probabilities p ranging from less than .05 to less than
.005 for the hypothesis H0 that BoostODT does not perform
better, the four sign tests comparing both runs of BoostODT
to its opponents are enough to display its better performances,
and this is conﬁrmed by student paired t-tests. There is more:
we can tell from simulated domains that BoostODT performs
as better as the domain gets harder.
It is the case for the
Monks domains, and the LEDeven domains. BoostODT is
indeed beaten by both opponents on LEDeven, while it beats
both on LEDeven+17 (=LEDeven +17 irrelevant variables).
Looking at these simulated domains, we have drilled down
the results of BoostODT. Using (8), we have plotted on Figure

Adult-strech
Breast-cancer
Breast-cancer-w.
Bupa
Colic
Colic.ORIG
Credit-a
Credit-g
Diabetes
Hepatitis
Ionosphere
Kr-vs-kp
Labor
LEDeven
LEDeven+17
Monks1
Monks2
Monks3
Mushroom
Parity
Sick
Sonar
Vote
XD6
Yellow-small
#Wins (T1 = 50)
#Wins (T1 = 200)

T1 = 50
0.00
28.67
3.72
28.12
17.66
13.86
14.64
26.50
25.91
20.65
7.41
3.38
10.53
15.25
22.75
25.36
1.00
1.44
0.00
47.66
2.15
13.94
3.91
20.57
0.00

T1 = 200
0.00
27.27
3.86
28.12
18.21
15.76
14.93
25.40
23.44
18.71
6.27
3.35
10.53
14.75
21.50
25.36
0.67
2.17
0.00
47.27
1.91
12.50
3.45
18.77
0.00

17(10/4)

18(10/7)

0.15
37.76
6.01
37.97
23.91
16.03
20.43
31.10
34.64
20.65
9.12
3.69
15.79
9.75
38.25
0.00
0.33
2.71
0.05
46.88
2.41
33.17
4.37
5.33
0.00

5(3)
5(3)

AdaBoost
+C4.5
0.00
33.92
4.43
31.59
18.21
32.07
15.51
28.40
29.82
18.71
7.41
0.69
12.28
10.50
30.75
2.16
26.96
2.53
0.00
44.14
1.09
18.27
5.29
5.40
0.00

8(3)
8(3)

Table 1: Results on 25 domains. For each domain, bold
In each row “#Wins
faces indicate the lowest errors.
(T1 = z)”, bold faces denote the number of times the
corresponding algorithm in column is the best over three
columns: BoostODT(T1 = z), OC1 and AdaBoost+C4.5
(z ∈ {50, 200}). Furthermore, the four numbers in parenthe-
ses in each row are the number of signiﬁcant wins (student
paired t-test, p = .05), for BoostODT vs OC1, BoostODT vs
AdaBoost+C4.5, and OC1 vs BoostODT, AdaBoost+C4.5 vs
BoostODT (from left to right).

4 its margin error curves on domain XD6 with variable class
noise (see e.g. [Nock and Nielsen, 2006] for a description of
the domain), averaged over the test folds [Nock and Nielsen,
2006]. The margin curve obtained is compared to that of the
logistic prediction of [Friedman et al., 2000], which can be
computed exactly. The approximation of the logistic model
by BoostODT is quite remarkable. Indeed, its margin curves
display the single stair-shape of a theoretical logistic model
for a domain XD6 with 8-13% additional class noise, uni-
formly distributed among the ODT leaves.

6 Conclusion
Perhaps one main contribution of this paper is to show that
formal boosting is within reach using the same uniﬁed algo-
rithm, for a wide variety of formalisms not restricted to the
most popular included in this paper (such as decision lists
[Rivest, 1987], simple rules [Nock, 2002], etc. ). Another
contribution, quite surprising, is to show that a boosting al-
gorithm follows immediately even for complex combinations

IJCAI-07

846

of these formalisms, such as linear combinations of oblique
decision trees, decision trees in which splits are decided by
decision lists, etc. This is crucial, as our last contribution, the
ﬁrst boosting algorithm for the class of oblique decision trees,
contrasts in simplicity with respect to previous approaches on
inducing oblique decision trees. In future works, we plan to
evaluate the experimental and theoretical potentials of these
boosting algorithms for these other formalisms.

Acknowledgments and code availability

R. Nock gratefully thanks Sony CS Labs Tokyo for a vis-
iting grant during which this work was started. Resources
related to BoostODT, including related Java classes for the
Weka platform, can be obtained from the authors.

References
[Blake et al., 1998] C. L. Blake, E. Keogh, and C.J. Merz.
UCI repository of machine learning databases, 1998.
http://www.ics.uci.edu/∼mlearn/MLRepository.html.

[Breiman et al., 1984] L. Breiman, J. H. Freidman, R. A. Ol-
shen, and C. J. Stone. Classiﬁcation and regression trees.
Wadsworth, 1984.

[Cant´u-Paz and Kamath, 2003] E. Cant´u-Paz and C. Ka-
math. Inducing oblique decision trees with evolutionary
algorithms. IEEE Trans. on Evo. Comp., 7:54–68, 2003.

[Freund and Schapire, 1997] Y. Freund and R. E. Schapire.
A Decision-Theoretic generalization of on-line learning
and an application to Boosting. JCSS, 55:119–139, 1997.

[Friedman et al., 2000] J. Friedman, T. Hastie, and R. Tib-
shirani. Additive Logistic Regression : a Statistical View
of Boosting. Annals of Statistics, 28:337–374, 2000.

[Heath et al., 1993] D. Heath, S. Kasif, and S. Salzberg.
Learning oblique decision trees. In Proc. of the 13 th IJ-
CAI, pages 1002–1007, 1993.

[Kearns and Mansour, 1999] M.J. Kearns and Y. Mansour.
On the boosting ability of top-down decision tree learning
algorithms. JCSS, 58:109–128, 1999.

[Kearns and Valiant, 1989] M.J. Kearns and L. Valiant.
Cryptographic limitations on learning boolean formulae
and ﬁnite automata. Proc. of the 21 th ACM STOC, pages
433–444, 1989.

[Mansour and McAllester, 2002] Y.

and
Boosting using branching programs.

Mansour

D. McAllester.
JCSS, 64:103–112, 2002.

[Murthy et al., 1994] S. K. Murthy,

and
S. Salzberg. A system for induction of oblique deci-
sion trees. JAIR, 2:1–32, 1994.

S. Kasif,

[Nock and Nielsen, 2006] R. Nock and F. Nielsen. A Real
Generalization of discrete AdaBoost. In Proc. of the 17 th
ECAI, pages 509–515, 2006.

[Nock, 2002] R. Nock. Inducing interpretable Voting clas-
siﬁers without trading accuracy for simplicity: theoretical
results, approximation algorithms, and experiments. JAIR,
17:137–170, 2002.

[Quinlan, 1993] J. R. Quinlan. C4.5 : programs for machine

learning. Morgan Kaufmann, 1993.

[Rivest, 1987] R.L. Rivest. Learning decision lists. Machine

Learning, 2:229–246, 1987.

[Schapire and Singer, 1999] R. E. Schapire and Y. Singer.
Improved boosting algorithms using conﬁdence-rated pre-
dictions. Machine Learning, 37:297–336, 1999.

[Schapire, 1990] R. E. Schapire. The strength of weak learn-

ability. Machine Learning, pages 197–227, 1990.

[Shawe-Taylor and Cristianini, 1998] J. Shawe-Taylor and
N. Cristianini. Data dependent structural risk minimiza-
tion for perceptron decision trees.
In NIPS* 10, pages
336–342, 1998.

7 Appendix
Proof of Theorem 1. Let us focus on a single p ∈ P, and
consider an example si ∈ Sp. Using the proof of Theorem 1
in [Nock and Nielsen, 2006], we obtain:

w1,i1[νT (si)≤θ]

≤ wT +1,i

1 − μ2

t ,

(cid:8)

(cid:9)

(cid:11)

(cid:10)

1 + θ
1 − θ

×

ht∈p

Computing νw1,HT ,θ = Ew1(1[νT (si)≤θ]) and simplifying
for each p ∈ P yields the statement of the Theorem.

Proof of Theorem 2. We need the following simple Lemma
(proof straightforward).

t+1,St , ∀1 ≤ t ≤ T .

Lemma 1 Whenever (A) and (i) of (B) are satisﬁed, we have
w−
t+1,St = w+
Consider a leaf ht of G, and let p = gHT (x), for an
observation x that reaches this leaf. We ﬁrst suppose that
St contains both positive and negative examples.
This
implies that (αt(cid:2) ht(cid:2) ) is ﬁnite, ∀ht(cid:2) ∈ p. All the positive
examples (resp. negative examples) of St have seen their
weights modiﬁed in the same way through induction.
Let σb,t = 1 + bμt, with b ∈ {+, −}. When HT is
the total weight, according to wT +1, of the posi-
built,
tive (resp. negative) examples of St, satisfy w+
T +1,St =
w+
1,St /
(resp.
w−
T +1,St = w−
ht(cid:2) ∈p:ht(cid:2) >0 σ−,t(cid:2))).
From Lemma 1, we have w+
T +1,St , and we
×
obtain (w+

T +1,St = w−
ht(cid:2) ∈p:ht(cid:2) >0 (σ+,t(cid:2) /σ−,t(cid:2))

ht(cid:2) ∈p:ht(cid:2) >0 σ+,t(cid:2)

ht(cid:2) ∈p:ht(cid:2) <0 σ−,t(cid:2)

ht(cid:2) ∈p:ht(cid:2) <0 σ+,t(cid:2)

1,St /w

−
1,St ) =

(cid:16)
(cid:15)(cid:16)
(cid:17)

(cid:15)(cid:16)

1,St/(

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(cid:15)(cid:16)

ht(cid:2) ∈p:ht(cid:2) <0 (σ−,t(cid:2) /σ+,t(cid:2) )

. Taking half the logarithms of

both sides and rearranging, we obtain:

(cid:6)

(cid:6)

1
2

ln

w+
1,St
−
w
1,St

=

ht(cid:2) ∈p

ht(cid:2)
2h(cid:4)
t(cid:2)

ln

1 + μt(cid:2)
1 − μt(cid:2)

αt(cid:2) ht(cid:2)(x)=HT (x) ,

=
ht(cid:2) ∈p

as claimed. The case where St does not contain examples of
both classes is immediate. Indeed, that means that at least one
of w+
1,St is zero as G is a tree, and we obtain that
1,St) = HT (x) = ±∞. Finally, (11) is the
(1/2) ln(w+
simpliﬁcation of (2) with the new expressions for HT (.).

1,St and w−
1,St /w

−

IJCAI-07

847


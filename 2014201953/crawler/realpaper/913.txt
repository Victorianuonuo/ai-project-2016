An Empirical Study of the Noise Impact on Cost-Sensitive Learning

Xingquan Zhu1, 3, Xindong Wu2, Taghi M. Khoshgoftaar1, Yong Shi3

1Dept. of Computer Science & Eng., Florida Atlantic University, Boca Raton, FL 33431, USA 

2Department of Computer Science, University of Vermont, Burlington, VT 05405, USA 

3Graduate University, Chinese Academy of Sciences, Beijing, 100080, China 

{xqzhu, taghi}@cse.fau.edu; xwu@cems.uvm.edu; yshi@gucas.ac.cn

Abstract

In this paper, we perform an empirical study of the
impact  of  noise  on  cost-sensitive  (CS)  learning,
through observations on how a CS learner reacts to 
the  mislabeled  training  examples  in  terms of  mis-
classification cost and classification accuracy. Our 
empirical  results  and  theoretical  analysis  indicate
that mislabeled training examples can raise serious
concerns for cost-sensitive classification, especially
when  misclassifying  some classes  becomes  ex-
tremely expensive. Compared to general inductive
learning,  the  problem  of  noise  handling  and data
cleansing  is more  crucial,  and  should  be  carefully
investigated to ensure the success of CS learning. 

1  Introduction
Recently,  a  body  of  work  has  been  attempted  to  address
cost-related  inductive  learning  issues,  with  techniques
known  as  cost-sensitive  learning  (Tan 93, Turney  95,  Paz-
zani et al. 94), where the “cost” could be interpreted as mis-
classification  cost,  training  cost,  and  test  cost  (Turney).
Among all different types of costs, the misclassification cost
is the most popular one. In general, misclassification cost is
described by a cost matrix C, with C(i, j) indicating the cost 
of predicting that an example belongs to class i when in fact
it belongs to class j. With this type of cost, the objective of a 
CS learner is to form a generalization such that the average
cost on previously unobserved instances is minimized. Ob-
viously, this minimal cost is determined by two most impor-
tant  factors:  (1)  the  inductive bias  of  the  underlying CS
learner;  and (2)  the quality  of  the  training  data.  Existing
research has made significant progress in exploring efficient
CS learning algorithms (Bradford et al. 98, Zuberk & Diet-
terich 02, Tan 93, Geibel & Wysotzki 03, Brefeld et. al. 03,
Domingos  99,  Chan  &  Stolfo  98,  Zadrozny  03,  Abe  & 
Zadrozny  04),  with  assumptions  that  the  input data  are 
noise-free or noise in the datasets is not significant. In data-
driven application domains, many potential problems, such
as  unreliable data  acquisition  sources, faulty  sensors,  and
data collection errors, will make data vulnerable to errors. It
is therefore important to understand how a CS learning algo-

 The support of the National Science Foundation of China un-

der Grant No.60674109 is acknowledged.

rithm  behaves  in noisy  environments  and  how  to  handle
data errors in supporting effective CS learning. In this paper,
we report our empirical study on the impact of noise on CS
learning. It is hoped that our observations will be beneficial
to any real-world applications with cost concerns.

2  Related Work
The problem of learning from noisy data has been a focus of
much attention in data mining (Quinlan 86b), and most al-
gorithms have a mechanism to handle noise in the training
data. For example, pruning on a decision tree is designed to
reduce the chance that the tree is overfitting to noise in the
training data (Quinlan 86). In real-world applications, many
learning  algorithms  rely  on  a  data  cleaning model  for  data
enhancement (Brodley & Friedl 99). Although the problem
of error handling in supervised learning has been well stud-
ied, research in the area has been mainly focused on general
inductive learning for the minimization of zero-one loss or
error rate. In reality, many applications are not only charac-
terized by error rate, but also by various types of costs (Tur-
ney), such as misclassification cost and test cost.

Among  all different  types  of  costs,  the  misclassification
cost  is  the  most  popular  one.  Assume  that  examples  of  a 
dataset are drawn independently from a distribution, d, with
domain X Y C, where X is the input space to a classifier, Y
is an output space, and C  [0,
) is the importance (mis-
classification  cost)  associated with  misclassifying  that  ex-
ample.  The goal  of  cost-sensitive  (CS)  learning,  from  the
misclassification cost perspective, is to learn a classifier h: X

Y which minimizes the expected cost (Zadrozny 03)

E

[),

,(

 

y

)]

)((
xhIc

cyxd

(1)
Two  important  issues  determine  the  minimization  of  the
expected cost: the total number of misclassifications and the
cost  of  each  single  misclassification.  To  minimize  overall
costs, a compromise is often made by sacrificing cheap ex-
amples  and  enhancing  the  accuracy on  classes  containing
expensive instances. This is distinct from general inductive
learning, because the latter is biased towards larger classes
(likely the cheap examples in a CS scenario). To enhance a 
CS  learner  trained  from  noisy  data  environments,  Zhu  and
Wu  (04) have  proposed  a  Classification  Filter  for  data
cleansing.  But  the  study  of  CS  learners  in  noisy  environ-
ments still lacks in-depth empirical and theoretical analysis.

IJCAI-07

1168

3  Experiment Setting
An empirical study needs a full control over noise levels in
the data, so that we can observe the behaviors of the under-
lying  CS  learners.  For  this  purpose,  we  implement  two
manual  corruption  mechanisms, Total  Random  Corruption
(TRC)  and Proportional  Random  Corruption  (PRC).  With
TRC, when  the  users  specify  an  intended  corruption  level
x 100%, we will randomly introduce noise to all classes. I.e., 
an instance with its label i has an x 100% chance to be mis-
labeled  as  another  random  class  (excluding  class i). TRC,
however,  changes  the  class  distribution  in  the dataset  after
noise corruption, which raises a big concern in CS Learning,
because  modifying  the  class  distribution may  change  the
average  cost  considerably. We  then  propose  PRC,  which
keeps the class distribution constant during the noise corrup-
tion.  Given a  dataset with Y  classes,  assume  the  original 
class distribution is  1,
i =1, where  1 and
Y are the percentage of the most and least common classes 
respectively.  For  any  user  specified  corruption  level,  say 
x 100%, we proportionally introduce random noise to differ-
ent classes with instances in class i having a ( Y / i)  x 100%
chance of being corrupted as another random class. That is, 
we use the least common class as the baseline, and propor-
tionally  introduce  noise  to  different  classes.  It  is  obvious
that the actual noise level in PRC is less (or even much less) 
than the intended corruption level.

2,.., Y, with 

Y
i

For  each  experiment,  we  perform  10  times  3-fold  cross-
validation. In each run, the dataset is divided into a training 
set E and a test set T. We introduce a certain level of noise
to E and generate a noisy dataset E . Meanwhile, assuming a
noise  cleansing  technique  is  able  to  remove  all  noisy  in-
stances  from E , we  can  therefore  build  a  cleansed  dataset
E .  We  observe CS  learners trained  from E, E ,  and E   to
study noise impact (assessed by using T as the test set). The
empirical study is made by the observation on three bench-
mark  two-class  datasets,  Credit  Screen  Dataset  (Credit),
Wisconsin  Breast  Cancer  Dataset  (WDBC),  and  Thyroid
disease dataset  (Sick)  (Blake  &  Merz  98), with  their  class
distributions varying from almost even to extremely biased 
(as shown in Table 1). We use two-class datasets as our test
bed, because  a  two-class  problem  can  explicitly  reveal  the 
impacts of noise and cost-ratio on CS learning, whereas for
multiple-class problems, issues such as costs among classes,
class  distributions,  and  errors  in  each  class  often  make  it 
difficult  to  draw  comprehensive  conclusions.  We use  the 
C5.0 CS classification tree (Quinlan 97) in all experiments.

To assign misclassification cost matrix values, C(i, j), i j,
we adopt a Proportional Cost (PC) mechanism: for any two
classes i and j (i j), we first check their class distribution i
and j. If 
j, which means that class j is relatively rarer
than i, then C(j, i)=100 and C(i, j) equals to 100 r; otherwise
C(i, j)=100 and C(j, i)= 100 r, where r is the cost-ratio, as
defined by Eq. (2).
r

,(
iC

(2)

)

jC

(

,

j

)

i

 

i

Table 1. Class distributions of the benchmark datasets 

Dataset

Class Distribution Separability(c4.5)

Credit Screening (Credit) 

Wisconsin Diagnostic
Breast Cancer (WDBC)
Thyroid disease (Sick)

0.56:0.44
0.63:0.37
0.94:0.06

Table 2. Major symbols used in the paper
Description

Symbol

82.8%
93.3%
98.7%

E, T, E , E

CS_X

Ac_Cs_X
Ac_Nm_X
h(t) vs H(t)

E: Training set, T: Test set, E : Noise corrupted

training set, E : Noise cleansed training set

The average misclassification cost from dataset X
The average classification accuracy of a CS and a

normal classifier on dataset X respectively

A CS classifier vs a non-CS classifier

x, y, c

r

C (i, j)

1,

2

,

1,

2

CSmin
CSAvg

CSUpper_bound

d(x,y,c)
d (x,y,c)
Ed(x,y,c)

[c I(h(x) y)]

x: an input instance, y: class label of x, c: the cost

associated to mislabeling x

The cost ratio between the minor class and the major

class

The number of instances in the test set T

The cost of predicting that an example belongs to

class i when in fact it belongs to class j.

The distribution of the major class vs the minor class
The classification error rate on the whole test set, the
major class examples and the minor class examples

respectively

the minor class

The cost of classifying a major class example into

The average misclassification cost on the test set T

The upper bound of a CS classifier

The distribution of the noisy training set E

The distribution of a new set constructed by sam-

pling E

The expected average misclassification cost of the

instances drawn from the distribution d

4 Noise Impact
4.1 Misclassification Costs
In Figs. 1 to 2, we report the noise impact on the cost of CS
learners  trained from  the  benchmark datasets,  where  Figs.
1(a) and  1(b),  and Figs.  2(a)  and 2(b)  represent the results
from  different  noise  corruption  models  of the WDBC  and
Sick dataset  respectively.  Fig.  2(c) reports  the  results from
the  Credit  dataset  (because the class  distributions  of  the
Credit dataset are almost even, we only report its results on 
the TRC model). In all figures, the first and the second rows
of the x-axis represent the intended and actual noise levels
in  the dataset.  The y-axis  indicates  the  average  cost of CS
classifiers. CS_E  and CS_E  represent the average cost of a 
CS classifier trained from E  and E  (before and after noise
cleaning).

As we can see from Figs. 1 to 2, for both noise corruption
models,  the  average  cost of  a  CS  classifier  proportionally
increases  with  the  value of r,  even  if  the dataset  is  noise-
free.  This does  not  surprise us,  because  we  fix  the  cost of
C(i, j) to 100 and set the cost of C(j, i) to 100 r. So raising
the  value  of  r  will  surely  increase  the  average  cost. When
noise is introduced to the dataset, the average cost will in-
evitably  increase,  regardless of  the noise  corruption  model
and  the  value of r. On the  other  hand,  removing  noisy in-

IJCAI-07

1169

stances  can  keep  average  costs almost  the  same  as  that  of 
the noise-free dataset (E). This indicates that data cleansing 
is  an  effective  way  to  reduce  the  impact of  noise  on  CS 
learning.

Although noise  increases  the  cost  of  a  CS  classifier  re-
gardless  of  the  cost-ratio (r),  when  comparing  the  results
from  three  r  values,  we  can  find  that  their  patterns of  in-
crease  are different.  As  shown in  Fig.  1(a),  Fig. 2(a), and
Fig.  2(c),  when r  is  small,  increasing noise  levels  will
gradually raise the average cost, and the impacts of noise are
less significant for low noise level data. On the other hand,
for large r values, introducing a small portion of noise will
elevate  the  average  cost  greatly,  and  increasing  the noise
level further does not show significant extra impacts. How-
ever, TRC in Fig. 1(a), Fig. 2(a), and Fig. 2(c) have already
changed  the  class  distribution  of  the dataset,  it  is  not  clear 
that  given  the  same  amount  of  noise  in  the  dataset which
one is responsible for the increase of the cost: the change of
the class distribution or the large cost-ratio r. We then turn 
to Fig. 1(b) and Fig. 2(b) for answers, where the class distri-
bution is constant during noise corruption.

As shown in Figs. 1(b) and 2(b), depending on the bias of
the class distribution of the dataset, the actual noise level in
the dataset is lower (or much lower) than the intended cor-
ruption level. For example, in Fig. 2(b), when the intended
noise  level  is  0.1,  the  actual  noise  level  in  the  database  is
just about 0.012 (please refer to Section 3 and Tab. 1 for the
reason). At this noise level, the cost increase for r=2 is 0.56
(from  1.91  to 2.47),  but for r=10  the  increase  is  about 3.5
(from 5.8 to 9.31). One may argue that this increase may be
incurred  by  increasing  the  class-ratio r, because  raising  r
will make the minority class more expensive. Nevertheless,
even  if  we  assume  that  the  classification  accuracy  remains
the  same,  we  can  still  see  that  the  average  increase  from 

r=10  (3.5/10)  is  significantly larger  than  the  increase  from 
r=2 (0.56/2). It  is  obvious  that  given  the same  amount of
noise in the dataset, even if noise does not change the class
distribution,  a  dataset  with  a  large  cost-ratio  tends  to  be 
more error prone and therefore may receive a large misclas-
sification  cost.  These  observations  indicate  that  when  the 
dataset  has  a  large  cost-ratio  value,  the  existence  of  noise
becomes fatal, and a small portion of noise can corrupt the 
results significantly. Because of its sensitivity to minor data
errors, a dataset with a large cost-ratio will experience more
difficulties in data cleansing, if the overall goal is to mini-
mize the misclassification cost.

Another  interesting  finding  in  Figs. 1  to  2  is  that  when
applying CS  learners  to  a  noisy  dataset,  it  seems  that  the
trained CS classifiers have a saturation point in reacting to 
the actual noise in the dataset, regardless of whether errors
change the class distribution or not. When the noise level is
below this point, it will continuously impact the CS classi-
fier. But once the noise level is beyond this point, the classi-
fier  becomes  insensitive  to  noise.  The  following  analysis
will  reveal  that  this saturation  point  is  determined  by  two
most important  factors:  the  class  distribution  and  the  cost-
ratio r. Given a two-class dataset, assuming its class distri-
bution is 
2, with 1 and 2 denoting the distribution of
the major class and the minor class respectively. The cost of 
predicting  a  minor  class  instance  as  the  major  class  is 
CSmin r,  with CSmin  indicating  the  opposite  cost (from  the 
major class to the minor class). Assuming that we have built
a CS classifier from the dataset and, for a test set T with
examples,  the  classification  error rates of  this  classifier  on 
the  whole  test  set  and  the major  class  are 
respectively. The  error  rate  for 
2
is the number of incorrectly classified minor class examples

and
the  minor  class 

1:

1

s
t
s
o
C

 
.

g
v
A

65

55

45

35

25

15

5

0
  0 

0.1

0.1

0.2

0.2

0.3

0.3

0.4

  0.4

Noise Level

CS_E' (r=2)

CS_E'' (r=2)

CS_E' (r=5)

CS_E'' (r=5)

CS_E' (r=10)

CS_E'' (r=10)

s
t
s
o
C

 
.

g
v
A

65

55

45

35

25

15

5

0
0

0.1
0.07

0.2
0.15

0.3
0.22

0.4
  0.29

Noise Level

CS_E' (r=2)

CS_E'' (r=2)

CS_E' (r=5)

CS_E'' (r=5)

CS_E' (r=10)

CS_E'' (r=10)

(a) WDBC dataset, total random noise (TRC)

 (b) WDBC dataset, proportional random noise (PRC)

Fig. 1. Impacts of class noise on the average cost of a CS classification algorithm

100

80

60

40

20

s
t
s
o
C

 
.

g
v
A

0

0
  0 

0.1

0.1

0.2

 0.2

0.3

0.3

0.4
0.4

Noise Level

CS_E' (r=2)

CS_E'' (r=2)

CS_E' (r=5)

CS_E'' (r=5)

CS_E' (r=10)

CS_E'' (r=10)

s
t
s
o
C

 
.

g
v
A

17

15

13
11

9

7

5

3

1

0
  0 

0.1
  0.012

0.2
 0.025

0.3
  0.036

0.4
0.047

Noise Level

CS_E' (r=2)

CS_E'' (r=2)

CS_E' (r=5)

CS_E'' (r=5)

CS_E' (r=10)

CS_E'' (r=10)

s
t
s
o
C

 
.

g
v
A

60

50

40

30

20

10

0
  0 

0.1

0.1

0.2

 0.2

0.3

0.3

0.4
0.4

Noise Level

CS_E' (r=2)

CS_E'' (r=2)

CS_E' (r=5)

CS_E'' (r=5)

CS_E' (r=10)

CS_E'' (r=10)

  (a) Sick dataset, total random noise (TRC)  (b) Sick dataset, proportional random noise (PRC)  (c) Credit dataset, total random noise (TRC)

Fig. 2. Impacts of class noise on the average cost of a CS classification algorithm

IJCAI-07

1170

divided by the total number of minor class instances, as s-
hown in Eq. (3). Then, the average misclassification cost in 
T, CSAvg, is denoted by Eq. (4).

(

1

)

2

1

1

 (3)

(

2

CS

Avg

1

(

1

1

1

1

)

(

2

1

CS

inm

2

)

1

2

2

2

r

CS

)

inm

1
CS

min

2

(

1

1

1

1

2

2

1

)

1

r

CS

min

(4)

1

2

When the average error rate   increases, it normally raises 
the  average misclassification  cost CSAvg,  with  its  value
bounded by an upper bound given by Eq. (5).

CS

Upper

_

bound

1

2

2

2

1

1

CS

min

if

r

1

2

(5)

r

CS

min

Otherwise

Actually, the upper bound in Eq. (5) is the average cost of
predicting all instances belonging to the minor class when r
1/ 2,  or  predicting  all  instances  belonging  to  the  major
class when r < 
1/ 2 (which is the optimal solution for a CS
classifier with low confidence, because any cost higher than
this upper bound is not necessary). It is understandable that
CSAvg CSUpper_bound holds most of the time, which eventu-
ally produces the inequality denoted by Eq. (6). Since
r
2
Otherwis

r
rr

_
Upper
bound

CS

CS

(
(

if

Avg

1

1

1

2

1

)
1
)
1

2

1

2

1

1

1

1

1

so

1(

(

2

)

1
r

(

1

1

1

)

1

)

1

2
r

)
For two class problems

r

(

2

1

r

1

if

r

1

2

(6)

1

1

Otherwise

1+ 2=1, so Eq. (6) becomes

*

1{(
1
)11(
r

)

1

1

1

2

1

1

}

r

if

r

1

Otherwise

2

2

 (7)

With the inequality in (7), we can finally analyze the rela-
2, the cost ratio

tionship between the class distribution 1,
r, and the error rate , and reveal the following facts: 
1. Given  a  test  set T,  the  average  misclassification  error
rate  of a CS classifier, which is trained from a training 
set equivalent to T, is bounded by a critical value * de-
fined by Eq. (7). When r
1/ 2, the higher the value of
the cost-ratio r, the lower the *, and the more likely the
classifier tends to approach the saturation point.
,
In theory, when r < 
11
2. This means that the
which can be approximated as
2),
larger the number of minor class instances (a larger 
the higher the *, and the less likely the classifier tends
to approach the saturation point. However, this conclu-
sion  crucially  relies  on  the fact  that  the  underlying CS
learner  is  “optimal”  and  indeed  knows  that  sticking  to
the major class will lead to the lowest cost, which is not
always the case in reality (as we will analyze next).

1/ 2, * becomes

)11(
r

2.

2

1:

that

From  Section  3  and  Tab.  1,  we know 
2=0.627:0.373  (r  > 

1:
1/ 2  for  any r=2,  5,  10) and
CSmin=100  for the WDBC dataset.  So  according  to  Eq.  (5) 
CSUpper_bound is  62.7,  which  is  consistent  with  the  results  in
Fig. 1(a). Now the interesting results come from the results
in  Fig. 2(a),  which  also  clearly  poses  a  saturation  point of
the misclassification cost. According to Table 1 and Eq. (5),
2=0.94:0.06, so the
we know that for the Sick dataset 
1/ 2 is  far  larger  than  the value of r.  As a
ratio  between
result,  the CSUpper_bound  should be
r
(corresponding  to  the  second  item  in  Eq.  (5)). Unfortu-
nately,  the  results  in  Fig. 2  (a)  indicate  that  the  real  CSUp-
per_bound we got is about 94, which actually equals the value
of  having  all  instances  classified  into  the  minor  class  (the 
first item in Eq. (5)). In reality, it is understandable that the
underlying CS  learners  are  reluctant  to  stick  to  the  major
class  for  minimal misclassification  cost,  especially  when
noise  has  changed  the apriori  class  distributions  signifi-
cantly. Instead, the CS learners will tend to stick to the mi-
nor  classes  in  high  uncertainty  environments.  As  a  result,
the  results  from this  seriously  biased dataset  are  consistent
with the conclusion drawn from the first item in Eq. (5). 

6)

CS

min

r

(

2

1

2

Based on  the above observations,  we  know  that given
datasets with the same noise level but a different cost-ratio 
value r, the larger the cost ratio r, the smaller the value of
*, then the dataset is more likely to approach to a satura-
tion point, where the misclassification cost appears to reach
the  maximum.  Meanwhile,  class  noise  does  not  continu-
ously make an impact on the CS classifier all the time, and
after  noise  reaches  a  certain  level,  the  CS  classifier  may
simply stick to the minor class. As a result, the impact of the 
noise in the system becomes constant. Given a dataset with 
1/ 2 (which is pretty common in reality), the higher the
r
cost-ratio, the more likely the classifier tends to do so. For
situations r <  1/ 2, the conclusions will also likely hold.

4.2  Classification Accuracies
The  above observations  conclude  that  a  dataset  with a
higher cost-ratio is sensitive to data errors, and learners built
from such data are most likely cost intensive. This conclu-
sion does not, however, solve the concerns like: (1) why is a 
learner built from noisy data with large cost-ratio cost inten-
sive? and (2) given any noisy dataset D, if users were able to
build both a normal classifier or a CS classifier, which one 
of them is more trustworthy in identifying noisy examples?
Now, let’s refer to the theoretical proof and empirical com-
parisons  of  the  classification  accuracies  between  a  normal
classifier and a CS classifier for answers.
The following analyses are based on an existing Folk Theo-
rem* (Zadrozny et.  al.  03), which  states  that  if  we have
examples drawn from the distribution:

*  It  was  called  “folk  theorem”  in  the  literature,  because  the  authors  con-
cluded that the result appears to be known, although it has not been pub-
lished yet. 

IJCAI-07

1171

),
cyxd

,('

E

c

cyxd

,(

,

),
cyxd

,(

(8)

][
c

)

where d(x,y,c)  represents  the  distribution of  the  original
dataset E, then  the optimal  error rate  classifiers  built  for
newly constructed distribution d are the optimal cost mini-
mizers for data drawn from d. This theorem states that if we 
sample  from the  original distribution, d,  of dataset E  and
constructing another distribution d , the efforts which try to
learn an optimal error rate classifier on d  actually leads to a
classifier which optimizes the misclassification cost for d.

THEOREM 1: For a dataset E with a distribution d, assum-
ing we are able to use an optimal learning theory to build a
CS  (h(t)),  and a  normal  classifier  (H(t))  respectively.  Then
h(t) is inferior to H(t) in terms of the average classification 
accuracy. That is, Errd(H)
 Errd(h), where ErrX( ) represents
the classification error rate of the classifier on X.
Proof:
Assume a CS classifier h(t) was built from d. Then with the
CS learning formula in Eq. (1), the expected cost of h(t) on 
d can be denoted by Eq. (9).

E

),
cyxd

,(

[
)((
xhIc

y

))]

)((
xhIccyxd

),

,(

y

)

(9)

With the folk theorem in Eq. (8), we know that

,
,
cyx

d

(

cyx

,

,

)

c

E

xd

(

,

cy

,

)

[

c

]

cyxd

(

,

,

, where d   is the sam-

)

pled distribution from d. Then Eq. (9) becomes.

E

),
cyxd

(

,

[

)((
xhIc

y

))]

E

),
cyxd

(

,

][
c

)((),
xhIcyxd

,(

y

)

(10)

,
,
cyx
][
Ec

E

),
cyxd

(

,

),
cyxd

(

,

)(([
xhI

y

)]

We can transform Eq. (10) as follows.

E

),
cyxd

,(

)(([
xhI

y

)]

1

E

),
cyxd

,(
c

E

),
cyxd

,(

)((
[
xhIc

y

)]

][
c

(11)

E

),
cyxd

,(

)(([
xhI

y

)]

E

),
cyxd

,(

][
c

(

(

))

))

))

(
tH

((
th

Err
d

((
th

Err
d

Err
d

Because h(t) was built  on  a  biased  distribution d   sampled
from d,  therefore Err
;  otherwise,  there
d
is no need for H(t) to exist (we can directly use h(t) on d for
normal classification). Therefore, we have Eq. (14) and the
statement of Theorem 1 is true.
))(
tH

(14)
Now  let’s  turn  to  the  experimental  results  in  Fig. 4  to
evaluate whether  the  empirical  results  indeed  support our
theoretical analysis (the meaning of each curve in Fig. 4 is
denoted in Fig. 3). In Fig. 4, Ac_Cs_E and Ac_Nm_E  indi-
cate the classification accuracies of a normal and a CS clas-
sifier trained from E  respectively. As shown in Fig. 4, when 
the  cost-ratio is  small  (r=2),  the differences  between  the
accuracies of the normal and CS classifiers are trivial, with
the accuracy of the CS classifier slightly worse. This is un-
derstandable,  because  a CS  classifier  likely  sacrifices  the 
accuracy for minimal costs. When the cost-ratio gets larger
(r=5  or  higher),  the  accuracy  of  the  CS  classifier becomes
significantly worse than the normal classifier. If we compare
Fig. 4(a) with Fig. 1(a) and Fig. 4(c) with Fig. 2(a), we can
find  that  this  is  actually  the reason  a  dataset  with  a  large
cost-ratio  is  more  willing  to  approach  to  the saturation
point.  In  this  situation,  a  small  portion  of  noise  will  make
the CS classifier ignore the classification accuracy and stick
to the “optimal” class.

Theorem 1 together with the results in Fig. 4 clearly indi-
cates that in general situations, a CS classifier is inferior to a
normal  classifier,  in  terms of  classification  accuracy.  Be-
cause removing mislabeled training examples was shown to
lead to a better CS learner (as shown in Figs. 1 to 2), class 
noise  handing  for  effective CS  learning  will  crucially  de-
pend on accurate noise identification mechanisms. As a re-
sult,  in  noisy  environments,  if  we  want  to  adopt  a  noise
identification mechanism for data cleansing, we shall trust a 
normal  classifier  rather  than  a  CS  classifier,  because  the
former  always  has  a  higher accuracy  in  determining  the
right class of an instance. 

E

Because
becomes

][),
c

cyxd

,(

arg

max

c

,{(

),
cyx

d

}

, Eq. (11)

Ac_Cs_E' (r=2)

95

Ac_Nm_E' (r=2)

Ac_Cs_E' (r=5)

y
c
a
r
u
c
c
A

75

55

35

0

0.1

0.2

0.3

0.4

Ac_Nm_E' (r=5)

Noise Level

Ac_Cs_E' (r=10)

Ac_Nm_E' (r=10)

E

),
cyxd

,(

That is 

)(([
xhI
Err
d

)]
y
((
th

))

E

)(([
xhI
))
((
th

,(
),
cyxd
Err
d

y

)]

(12)

 

 

(13)

Fig. 3. The meaning of each curve in Fig. 4 

y
c
a
r
u
c
c
A

95

85

75

65

55

45

35

y
c
a
r
u
c
c
A

95

85

75

65

55

45

35

y
c
a
r
u
c
c
A

100

80

60

40

20

0

y
c
a
r
u
c
c
A

99
98
97
96
95
94
93
92
91

0
   0

0.1
0.1

0.2

0.2

0.3
 0.3

0.4
  0.4

0
   0

0.1
  0.07

0.2

 0.15

0.3

0.22

0.4

  0.29

0
  0 

0.1
 0.1

0.2
 0.2

0.3

  0.3

0.4

 0.4

0
  0 

0.1
0.012

0.2
 0.025

0.3
0.036

0.4
  0.047

Noise Level

Noise Level

Noise Level

Noise Level

(a) WDBC Dataset (TRC)

  (b) WDBC Dataset (PRC)

 

(c) Sick Dataset (TRC)

 (d) Sick Dataset (PRC)

Fig. 4. Impacts of class noise on the classification accuracy of normal and CS classification algorithms

IJCAI-07

1172

4.3 Misclassification Costs and Accuracies
For a given example Ik, assume we know the probability of
each class j, P(j|Ik), the Bayes optimal prediction for Ik is the 
class i  that  minimizes  the  conditional  risk (Domingos 99,
Duda  &  Hart 73),  as  defined  by  Eq.  (15).  The  conditional
risk P(i|Ik)  is  the  conditional  probability  of  predicting  in-
stance Ik belongs to class i. According to the Bayesian for-
mula,  we  can  transform  Eq.  (15)  to  Eq.  (17).  The  Bayes 
optimal prediction is guaranteed to achieve the lowest pos-
sible overall cost (i.e., the lowest expected cost over all pos-
sible examples Ik, weighted by their probabilities P(Ik)).

With Eq. (17), it is clear that C(i, j), P(Ik | j), and j, to-
gether with the rule above imply a partition of the example
space with j regions, such that class j is the optimal (least-
cost) prediction in region j. The goal of cost-sensitive classi-
fication  is  to  find  the  frontiers  between  these  regions,
explicitly or implicitly. However, this is complicated by two
factors: (1) their dependence on the cost matrix C; and (2) 
the changes of both likelihood P(Ik | j) and prior probability
j of different classes j, caused by the existence of noise. In 
general, as misclassifying examples of class j becomes more
expensive relative to misclassifying others, the region where
j should be predicted will expand at the expense of the re-
gions of other classes, even if the class probabilities P(j|Ik)
remain unchanged. In noisy environments, the probability of
each class j, P(j|Ik), becomes less reliable, due to the reason 
that noise modifies the likelihood P(Ik|j) and the prior prob-
ability j. With Eq. (15), it is understandable that any error
introduced to the likelihood P(Ik|j) is going to be magnified
by  the  cost matrix C(i, j)  (the  CS  classifier  tends  to  favor
“expensive”  classes),  even if  noise does  not  change  the
apriori class distribution j. As a result, the higher the cost-
ratio in the dataset, the more sensitive the CS classifier be-
haves to the data errors.
)

),(
j

(15)

|(
iP

min

arg

min

iC

(
jP

|

I

arg

i

I

k

i

)

k

According to Bayesian formula, we have

j

IjP

(

|

)

k

(
|
IP
k
(
IP

k

)(
)
jPj
)(
)|
iPi

(
IP

k

|

)
)(
jPj
P

(16)

Yi

where P(Ik | j)  is  the  likelihood  of  instance Ik, given a par-
ticular  class  j,  and  P  is  the sum  of  all  the  likelihoods.  As-
j=P(j)  is  the prior probability  of  class  j.  Then,  Eq.
sume
(15) can be transformed as follows:

arg

min

|(
iP

I

)

k

i

arg

min

i

(
IP

k

|

,(
)
iCj

j

)

j

P

(17)

j

5  Conclusions 
This paper has empirically and theoretically studied the im-
pacts  of  mislabeled  training examples  (commonly  referred
to  as  class  noise)  on  cost-sensitive  learning. We  observed
behaviors of  cost-sensitive  learners  trained  from  different
noisy  environments  and  assessed  their performances  in
terms  of  average  misclassification  cost  and  classification 

accuracy.  Our  quantitative  study  concludes  that  the  exis-
tence of class noise can bring serious troubles (higher mis-
classification costs  and  lower  classification  accuracy)  for 
cost-sensitive learning, especially when misclassifying some
classes  becomes  extremely  expensive.  Removing  noisy  in-
stances will significantly improve the performance of a cost-
sensitive  classifier  learned  from  noisy  environments.  Our
observations  suggest  that  cost-sensitive  learning  should  be 
carefully  conducted  in noisy  environments.  Comparing  to
general inductive learning, the success of the cost-sensitive
learning crucially depends on the quality of the underlying
data,  where  system  performances  can  deteriorate  dramati-
cally in the presence of a small portion of data errors.

References

Abe  N.  &  Zadrozny  B.  (04),  An  iterative  method  for  multi-class
cost-sensitive learning. Proc. of the 10th KDD Conference, 2004. 
Blake C. & Merz C. (98), UCI Data Repository, 1998.
Bradford,  J.  Kunz  C.,  Kohavi  R.,  Brunk  C.,  &  Brodley  C.  (98),
Pruning  decision  trees  with  misclassification costs. Proc.  of  the
ECML Conf., 1998.
Brodley C. & Friedl M. (99), Identifying mislabeled training data. 
Journal of AI Research, 11:131-167, 1999.
Brefeld U., Geibel P. & Wysotzki F. (03), Support vector machines
with example dependent costs. Proc. of the ECML Conf., 2003. 
Chan  P.  &  Stolfo  S.  (98), Toward  scalable  learning  with  non-
uniform class and cost distributions. Proc. of the KDD Conf., 1998.
Domingos P. (99), MetaCost: a general method for making classi-
fiers costsensitive. Proc. of the 5th KDD Conference, 1999. 
Duda R. & Hart P. (73), Pattern classification and scene analysis,
Wiley, New York, 1973.
Gamberger  D.,  Lavrac  N.,  &  Groselj  C.  (99), Experiments  with 
noise filtering in a medical domain. Proc. of the ICML Conf., 1999.
Geibel P. & Wysotzki F. (03), Perceptron based learning with ex-
ample dependent and noisy costs. Proc. of the ICML Conf., 2003. 
Pazzani  M.,  Merz  C.,  Murphy P.,  Ali  K.,  Hume  T.  &  Brunk  C.
(94),  Reducing  misclassification  costs.  Proc.  of  the  11th  ICML 
Conference, 1994.
Quinlan  J.  (86),  Induction  of  decision trees. Machine  Learning,
1(1): 81-106, 1986.
Quinlan, J. (86b), The effect of noise on concept learning, Machine
Learning, Morgan Kaufmann, 1986.
Quinlan J. (97), http://rulequest.com/see5-info.html, 1997.
Tan  M.  (93),  Cost-sensitive  learning  of  classification  knowledge
and its applications in robotics. Machine Learning, 13:7-33, 1993.
Turney 
bibliography,
http://purl.org/peter.turney/bibliographies/cost-sensitive.html.
Zadrozny  B.,  Langford  J.  &  Abe  N.  (03),  Cost-sensitive  learning
by  cost-proportionate  example  weighting. Proc.  of  the  3rd  ICDM
Conference, 2003.
Zhu,  X.&  Wu,  X.,  (04),  Cost-guided  Class  Noise  Handling  for
Effective Cost-sensitive Learning. Proc. of the ICDM Conf., 2004.
Zubek V. & Dietterich T. (02), Pruning improves heuristic search 
for cost-sensitive learning. Proc. of the ICML Conference, 2000. 

Cost-sensitive 

learning 

P., 

IJCAI-07

1173


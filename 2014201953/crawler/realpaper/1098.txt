Adaptive Genetic Algorithm with Mutation and Crossover Matrices

Nga Lam Law and K.Y. Szeto

Hong Kong University of Science and Technology

Department of Physics

Clear Water Bay, Hong Kong SAR, China

Corresponding author: phszeto@ust.hk

Abstract

A matrix formulation for an adaptive genetic
algorithm is developed using mutation matrix
and crossover matrix. Selection, mutation, and
crossover are all parameter-free in the sense that
the problem at a particular stage of evolution will
choose the parameters automatically. This time de-
pendent selection process was ﬁrst developed in
MOGA (mutation only genetic algorithm) [Szeto
and Zhang, 2005] and now is extended to include
crossover. The remaining parameters needed are
population size and chromosome length. The adap-
tive behavior is based on locus statistics and ﬁt-
ness ranking of chromosomes. In crossover, two
methods are introduced: Long Hamming Distance
Crossover (LHDC) and Short Hamming Distance
Crossover (SHDC). LHDC emphasizes exploration
of solution space. SHDC emphasizes exploitation
of local search process. The one-dimensional ran-
dom coupling Ising Spin Glass problem, which is
similar to a knapsack problem, is used as a bench-
mark test for the comparison of various realiza-
tions of the adaptive genetic algorithms. Our results
show that LHDC is better than SHDC, but both are
superior to MOGA, which has been shown to be
better than many traditional methods.

1 Introduction
The notion of mutation matrix is used in the development of
adaptive genetic algorithm so that the selection process does
not require any external input parameter. Numerical exam-
ples on knapsack problems indicate that this new adaptive ge-
netic algorithm, called MOGA (mutation only genetic algo-
rithm) is superior to many traditional methods [Ma and Szeto,
2004; Szeto and Zhang, 2005; Zhang and Szeto, 2005]. The
idea behind MOGA is that mutation probability is a function
of time, ﬁtness ranking of the chromosome, and locus. The
dependence is automatically determined by the statistics of
the locus and the ﬁtness ranking of the chromosome at the
given time. The traditional genetic algorithm is a special case
of MOGA in which all the mutation probabilities are ﬁxed.
In this paper, we extend the basic idea behind MOGA to in-
clude the crossover process. Section 2 reviews the MOGA

formalism without crossover. Section 3 focuses on the imple-
mentation of the crossover matrix. A brief explanation of the
test problem - Ising spin glass is given in section 4, and the
test results on Ising spin glass are discussed in section 5.

2 Mutation Matrix

Traditional genetic algorithm can be considered as a special
case of the mutation only genetic algorithm (MOGA). For
a population of NP chromosomes, each encoded by L locus,
can be arranged into a population matrix A with NP rows and
L columns. Each element of the matrix A has a probability p
(0 < p < 1) to undergo mutation. These mutation probabili-
ties Mij(i = 1, ..., NP ; j = 1, ..., L) form a mutation matrix
M: Mij is the mutation probability of Aij . In matrix A, each
row is one chromosome. The rows are listed in decreasing
order of the ﬁtness: fi ≥ fj if i < j .

In traditional genetic algorithm, the best N1 = c1 × NP
chromosomes survive (0 < c1 < 1). These survivors produce
N2 = c2 × NP children to replace the less ﬁt ones through
crossover and/or mutation (0 < c2 < 1 − c1). The remaining
N3 = (1 − c1 − c2) × NP unﬁt chromosomes are replaced by
the same number of randomly generated candidates in order
to ensure population diversity. Thus the traditional genetic
algorithm can be described by a mutation matrix which has
Mij = 0 for the ﬁrst N1 rows; Mij = m(0 < m < 1) for the
next N2 rows; Mij = 1 for all remaining rows. In traditional
genetic algorithm, we must input three time independent pa-
rameters: c1, c2 and m.

MOGA makes use of the information of the ﬁtness rank-
ing and loci statistics to determine Mij dynamically. It out-
performs traditional genetic algorithm in terms of both speed
and quality of solution. In this formalism, the mutation ma-
trix is nearly the same as the population matrix A, but with
the loci arranged according to the standard deviation σ(j) for
the allele distribution. If there are V allele values possible for
the locus, (in ordinary binary string representation of genetic
algorithm, the possible value of an allele is either 0 or 1, so V
= 2), then we deﬁne the standard deviation σ(j) for the allele
distribution as:

NP

i=1(Aij − h(j))2 × C(f (i))

(cid:5)

NP

i=1 C(f (i))

(1)

(cid:5)

(cid:2)(cid:3)(cid:3)(cid:4)

σ(j) =

IJCAI-07

2330

where

h(j) =

NP(cid:6)

i=1

1

NP

Aij .

(2)

Here, C(f(i)) is the ﬁtness cumulative probability of chromo-
some i, which, as well as ﬁtness, can be considered as an in-
formative measure of chromosome i. High ﬁtness cumulative
probability indicates high informative measure. The allele
standard deviation σ(j) for each loci can also be regarded as
an informative measure. A locus which has a smaller allele
standard deviation contains more useful information than the
other loci.

At the beginning of the algorithm, chromosomes are ran-
domly generated. Alleles are randomly distributed. Thus the
allele standard deviation is not so informative. After genera-
tions of evolution, chromosomes acquire higher ﬁtness, good
chromosome structures emerge. Then more chromosomes
evolve to acquire the good structures. Therefore, a locus with
high structural information has a small allele standard devia-
tion.

i=1

The presence of the factor of

C(f (i)) in the allele
standard deviation is to take account of the informative use-
fulness of each chromosome. Loci in chromosomes with high
ﬁtness generally are more informative than those in chromo-
somes with low ﬁtness. With this factor, the allele standard
deviation more appropriately reﬂect the amount of informa-
tion contained in a particular locus. Our cumulative probabil-
ity C(f (i)) is deﬁned by

(cid:5)

C(f (i))
NP

C(f (i)) =

1

NP

(cid:6)

g≤f

N (g).

(3)

N(g) is the number of chromosomes with ﬁtness value g. Note
that C(f) is a non-decreasing function of f where C(fmax) =
1 and ( 1

≤ C(f ) ≤ 1).

NP

Therefore, the mutation matrix can be viewed as an ar-
rangement of genes based on the informative measures: the
ﬁtness cumulative probability C(f (i)) and the standard de-
viation σ(j) for the allele distribution. The higher the row
is, the more useful information the loci in this row contains.
Similarly, the closer to the right side the locus is, the more
information it contains. Our mutation is to make use of these
two informative measures through the mutation matrix.

In each generation, rows (i.e. chromosomes) in the muta-
tion matrix were ﬁrst selected for mutation based on the ﬁt-
ness cumulative probability. The probability α(i) to choose
the ith row for mutation is deﬁned as,

α(i) = 1 − C(f (i)).

(4)

Therefore, a chromosome with higher ﬁtness has a lower
probability to participate in mutation, or in other words, it
has higher ability to survive. As the chromosomes are kept
in a decreasing order of the ﬁtness: fi ≥ fj if i < j, in the
matrix A; α(i) is a non-decreasing function of the row index
(chromosome index) i. Thus, α(i) is higher for less ﬁt rows
(chromosome). The ﬁttest chromosome, i.e. the ﬁrst row of
the population matrix A, α(i) = 0. It is, hence, never se-
lected for mutation. The worst chromosome, i.e. the last row
of A, will mutate with a probability close to unity for large

enough NP : α(NP ) = 1 − 1
NP = 20.

NP

≈ 1, e.g. α(NP ) = 0.95 for

Next, we employ the ﬁtness cumulative probability again
to determine the number Nmg of loci to undergo mutation
for the selected chromosomes, each with L loci. Stating the
formula for Nmg,

Nmg = α(i) × L.

(5)

It is obvious that Nmg is also a function of the row index
i, or in GA language, the ﬁtness. If a selected chromosome
has a high ﬁtness, it has fewer loci to undergo mutation. If
a selected chromosome has a low ﬁtness, it has more loci to
mutate. Finally, we mutate the Nmg leftmost loci located in
the selected rows in the mutation matrix, i.e. the Nmg least
informative loci of each chromosome. In this way, most of
the informative loci remain and guide the evolution process
to the next generation, while most of the less informative loci
mutate until they contain more information.

3 Crossover Based on Hamming Distance

the mutation matrix,

With the implementation of
the
crossover is operated implicitly on the population. The im-
plicit crossover deploys the chromosome ﬁtness ranking and
the loci statistics in search of solution. Besides these two
kinds of information, the Hamming distance can also be
used to search the solution. Through the introduction of a
crossover matrix, two different explicit crossover methods
which employ the information of the Hamming distance are
implemented and compared.

The distance matrix H is a square NP × NP matrix with
matrix element Hii(cid:2) deﬁned by a distance measure between
−→
R i which is the i-th row
the i-th chromosome, represented by
vector of the population matrix A, and the i’-th chromosome
R i(cid:2) : Hii(cid:2) = D(−→
−→
−→
R i(cid:2) ). In this paper, we have two differ-
R i,
ent measures for H. The ﬁrst one is called long Hamming dis-
tance crossover (LHDC) in which Hii(cid:2) = H L
ii(cid:2) = Dii(cid:2) , where
Dii(cid:2) is equal to the number of different alleles in these two
rows. The second measure is called short Hamming distance
crossover (SHDC), with Hii(cid:2) = H S
ii(cid:2) = L − Dii(cid:2) . We see that
in LHDC, the smaller the distance between two chromosomes
i and i’ is, the more similar they are. While in SHDC, it is just
the opposite: smaller distance indicates low similarity. Note
that the matrix H is a symmetric matrix. The diagonal entries
of H are zero in LHDC, but equal L in SHDC.

After deﬁning the matrix H, we have to select two chro-
mosomes for crossover. The probabilities to choose the two
chromosomes are different, but they are both related to H.
One probability is just the ﬁtness cumulative probability, and
we call it the ﬁrst crossover probability PCI (i).

PCI (i) = C(f (i)).

(6)

A chromosome, which we call the ﬁrst chromosome, is ﬁrst
chosen with this ﬁrst crossover probability. As the row index
in H is also the row index in A, chromosome represented by
smaller H row index has higher probability to be selected for
crossover. Then, the second chromosome is selected with an-
other probability called the second crossover probability. The

IJCAI-07

2331

second crossover probability depends on the chromosome al-
ready chosen. If chromosome i (i.e. chromosome with H row
index i), is the ﬁrst chromosome, then the second crossover
probability PCII (i(cid:3)) to choose chromosome i’ as the second
chromosome is:

PCII (i

(cid:3)) =

Hii(cid:2)(cid:5)

NP
k=1 Hik

.

(7)

If the ﬁrst and the second chromosome are the same, the
second one is chosen again until they are both different. Both
LHDC and SHDC are single-point crossovers with the cross-
ing points chosen randomly once in a generation.
In each
generation, a total of NP chromosomes are selected to partic-
ipate in the crossover. There is no limitation on the number of
times a chromosome can take part in crossover in one gener-
ation. Later, the ﬁttest NP chromosomes are selected to form
a new generation from the pool of NP original chromosomes
and the NP children chromosomes produced by crossover.

In each generation, we ﬁrst carry out crossover and then
mutation. At the end of crossover, i.e. when the ﬁttest NP
chromosomes are selected, as well as at the end of the muta-
tion, we update the ﬁtness and cumulative ﬁtness probability
for all the chromosomes in the population.

4 Application: One-Dimensional Ising Spin

Glass

The algorithm is applied to ﬁnd the minimum energy of a 1D
periodic Ising model consisting of Nloci spins (Nloci = 10, 11,
15, 20, 25 and 30). Each spin can be either up or down, rep-
resented by ’1’, or ’-1’ respectively. The interaction energy
between a pair of neighbouring Ising spin is randomly gener-
ated in the range [-1,1]. The problem is to ﬁnd the minimum
of the energy

Nloci−1(cid:6)

E =

Jj,j+1sjsj+1

(8)

j=1

for a given set of interaction energy . For each value of
Nloci, two different sets of interaction energy were gener-
ated. For each set of interaction energy, the exact minimum
energy Emin was ﬁrst computed by exhaustive search over
2Nloci spin conﬁgurations. The program is then executed 400
times with a ﬁxed population size of NP = 20 chromosomes.
Each execution is regarded as one sample. An execution stops
when an acceptable solution is found or the maximum al-
lowed generations NM A = 250000 is reached. An acceptable
solution is a candidate solution which is within 1% tolerance
of the exact minimum energy Emin. When no solution can be
found within NM A generations in an execution, the execution
is considered as a failure sample.

5 Results

Nloci

10

15

20

25

30

sample MOGA LHDC SHDC

1

2

1

2

1

2

1

2

1

2

20
(0)
20
(0)
146
(0)
318
(0)
2456
(0)
1483
(0)

41274

(1)

31144

(0)

123621

(0)

123460
(280)

6
(0)
7
(0)
14
(0)
49
(0)
52
(0)
159
(0)
171
(0)
586
(0)
1724
(0)
7610
(0)

6
(0)
7
(0)
14
(0)
65
(0)
47
(0)
197
(0)
194
(0)
669
(0)
1956
(0)

11863

(0)

Table 1: Average generation and number of failure in 400
samples to reach solution using the MOGA, LHDC and
SHDC. The number of failure is written in the round brackets.

generation and number of successful searches. Moreover
LHDC gives the best performance.

The two GAs with crossover is better due to the informa-
tion exchange between chromosomes. In LHDC, ﬁt chromo-
some is selected and crossover with a very different chro-
mosome to generate two new chromosomes. This expands
the exploration space and minimizes the chance of prema-
ture convergence. In SHDC, ﬁt chromosome is selected and
crossover with a similar one. This exploits the local search
process and speeds up convergence. The results suggest that
a second crossover probability emphasizing the exploration
of search space is better for the Ising-spin-glass-like problem
(i.e. knapsack-type problem)[Kellerer et al., 2004].

6 Conclusion

Three different ways to implement the adaptive genetic al-
gorithm are tested and compared using the one-dimensional
Ising model for spin glass as the testing problem. The re-
sults show that LHDC is best and that all three methods
are parameter-free. Since MOGA outperforms other al-
gorithms, including dynamic programming [Kellerer et al.,
2004; Simoes and Costa, 2001; 1999], for solving the knap-
sack problem, our test shows the superiority of LHDC in this
kind of search problem. This work also provides a matrix for-
mulation of parameter free adaptive genetic algorithm with
both mutation and crossover.

We have compared the efﬁciency of three adaptive genetic al-
gorithms (MOGA, LHDC and SHDC) on the spin glass prob-
lem. Table 1 summarizes the average generation to reach so-
lution and the number of failure. Both GAs with crossover,
LHDC and SHDC, outperform MOGA in terms of average

Acknowledgments

N.L.Law and K.Y. Szeto acknowledge interesting discussion
from C.W. Ma. K.Y. Szeto acknowledges the support of RGC
grant 603203.

IJCAI-07

2332

References
[Kellerer et al., 2004] Hans Kellerer, Ulrich Pferschy, and
Knapsack Problems, pages 22–26.

David Pisinger.
Springer, 2004.

[Ma and Szeto, 2004] Chun Wai Ma and Kwok Yip Szeto.
Locus oriented adaptive genetic algorithm: Application
to the zero/one knapsack problem. In Proceeding of The
5th International Conference on Recent Advances in Soft
Computing, RASC2004, pages 410–415, Nottingham, UK,
December 2004.

[Simoes and Costa, 1999] A. B. Simoes and E. Costa. Trans-
position versus crossover: An empirical study. In Proceed-
ing of the Genetic and Evolutionary Computation Confer-
ence. Morgan Kaufmann, 1999.

[Simoes and Costa, 2001] A. B. Simoes and E. Costa. An
evolutionary approach to the zero/one knapsack problem:
Testing ideas from biology. In Proceeding of Fifth Inter-
national Conference on Aritical Neurakl Network and Ge-
netic Algorithms, 2001.

[Szeto and Zhang, 2005] Kwok Yip Szeto and J. Zhang.
Adaptive genetic algorithm and quasi-parallel genetic al-
gorithm: Application to low-dimensional physics. In Lec-
ture Notes in Computer Science 3743, pages 186–196, So-
zopol, June 2005. Springer-Verlag.

[Zhang and Szeto, 2005] Jian Zhang and Kwok Yip Szeto.
Mutation matrix in evolutionary computation: An appli-
cation to resource allocation problem.
In Lecture Notes
in Computer Science 3612, pages 112–119, Changsha,
China, August 2005. Springer-Verlag.

IJCAI-07

2333


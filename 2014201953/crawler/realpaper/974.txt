Formal Trust Model for Multiagent Systems

∗

Yonghong Wang and Munindar P. Singh

Department of Computer Science
North Carolina State University
Raleigh, NC 27695-8206, USA

Abstract

Trust should be substantially based on evidence.
Further, a key challenge for multiagent systems
is how to determine trust based on reports from
multiple sources, who might themselves be trusted
to varying degrees. Hence an ability to combine
evidence-based trust reports in a manner that dis-
counts for imperfect trust in the reporting agents is
crucial for multiagent systems.
This paper understands trust in terms of belief and
certainty: A’s trust in B is reﬂected in the strength
of A’s belief that B is trustworthy. This paper for-
mulates certainty in terms of evidence based on a
statistical measure deﬁned over a probability dis-
tribution of the probability of positive outcomes.
This novel deﬁnition supports important mathemat-
ical properties, including (1) certainty increases as
conﬂict increases provided the amount of evidence
is unchanged, and (2) certainty increases as the
amount of evidence increases provided conﬂict is
unchanged. Moreover, despite a more subtle deﬁ-
nition than previous approaches, this paper (3) es-
tablishes a bijection between evidence and trust
spaces, enabling robust combination of trust reports
and (4) provides an efﬁcient algorithm for comput-
ing this bijection.

1 Introduction

In simple terms, an agent’s trust in another can be understood
as a belief that the latter’s behavior will support the agent’s
plans. Subtle relationships underlie trust in social and organi-
zational settings [Castelfranchi and Falcone, 1998]. Without
detracting from such principles, this paper takes a narrower
view of trust: here an agent seeks to establish a belief or dis-
belief that another agent’s behavior is good (thus abstracting
out details of the agent’s own plans and the social and orga-
nizational relationships between the two agents). The model
proposed here can, however, be used to capture as many di-
mensions of trust as needed, e.g., timeliness, quality of ser-
vice, and so on.

∗This research was partially supported by the National Science

Foundation under grant ITR-0081742.

For rational agents, trust in a party should be based sub-
stantially on evidence consisting of positive and negative ex-
periences with it. This evidence can be collected by an agent
locally or via a reputation agency or by following a referral
protocol. In such cases, the evidence may be implicit in the
trust reports obtained that somehow summarize the evidence.
This paper develops a principled evidence-based approach for
trust that supports two crucial requirements of multiagent sys-
tems:

Dynamism. Practical agent systems face the challenge that
trust evolves over time, both as additional information is
obtained and as the parties being considered alter their
behavior.

Composition. It is clear that trust cannot be trivially propa-
gated. For example, A may trust B who trusts C, but
A may not trust C. However, we need to combine trust
reports that cannot themselves be perfectly trusted, pos-
sibly because of their provenance or the way in which
they are obtained.

Traditionally, principled approaches to trust have been difﬁ-
cult to come by because of the above requirements. With few
exceptions, current approaches for combining trust reports
tend to involve ad hoc formulas, which might be simple to im-
plement but are extremely difﬁcult to understand from a con-
ceptual basis. The common idea underlying a solution that
satisﬁes the above requirements is the notion of discounting.
Dynamism can be accommodated by discounting over time
and composition by discounting over the space of sources
(i.e., agents). Others have used discounting before, but with-
out adequate mathematical justiﬁcation. For instance, Yu and
Singh [2002] develop such a discounting approach layered on
their (principled) Dempster-Shafer account.

The best multiagent application of the present approach is
in the work of Wang and Singh [2006a], who develop an al-
gebra for aggregating trust over graphs understood as webs
of trust. Wang and Singh concentrate on their algebra and as-
sume a separate, underlying trust model, which is the one de-
veloped here. By contrast, the present paper is neutral about
the discounting and aggregation mechanisms, and instead de-
velops a principled evidential trust model that would underlie
any such agent system where trust reports are gathered from
multiple sources.

Following Jøsang [2001], we understand trust based on the

IJCAI-07

1551

probability of the probability of outcomes, and adopt his idea
of a trust space of triples of belief (in a good outcome), dis-
belief (or belief in a bad outcome), and uncertainty. Trust in
this sense is neutral as to the outcome and is reﬂected in the
certainty (i.e., one minus the uncertainty). Thus the following
three situations are distinguished:

• Trust in a party (i.e., regarding its being good): belief is

high, disbelief is low, and uncertainty is low.

• Distrust in a party: belief is low, disbelief is high, and

uncertainty is low.

• Lack of trust in a party (pro or con): uncertainty is high.
However, whereas Jøsang deﬁnes certainty in an ad hoc
manner, we deﬁne certainty based on a well-known statistical
measure. Despite the increased subtlety of our deﬁnition, it
preserves a bijection between trust and evidence spaces, en-
abling combination of trust reports (via mapping them to ev-
idence). Our deﬁnition captures the following key intuitions.

Effect of evidence. Certainty increases as evidence in-
creases (for a ﬁxed ratio of positive and negative obser-
vations).

Effect of conﬂict. Certainty decreases as the extent of con-

ﬂict increases in the evidence.

Jøsang’s approach satisﬁes the intuition about evidence
but fails the intuition about conﬂict.
It falsely predicts
that mounting conﬂicting evidence increases certainty—and
equally as much as mounting conﬁrmatory evidence. Say Al-
ice deals with Bob four times or obtains (fully trustworthy)
reports about Bob from four witnesses:
in either case, her
evidence would be between 0 and 4 positive experiences. It
seems uncontroversial that Alice’s certainty is greatest when
the evidence is all in favor or all against and least when the
evidence is equally split. Section 3.2 shows that Jøsang as-
signs the same certainty in each case.

Yu and Singh [2002] model positive, negative, or neu-
tral evidence, and apply Dempster-Shafer theory to compute
trust. Neutral experiences yield uncertainty, but conﬂicting
positive or negative evidence doesn’t increase uncertainty.
Further, for conﬂicting evidence, Dempster-Shafer theory can
yield unintuitive results [Sentz and Ferson, 2002]. Say Pete
sees two physicians, Dawn and Ed, for a headache. Dawn
says Pete has meningitis, a brain tumor, or neither with prob-
abilities 0.79, 0.2, and 0.01, respectively. Ed says Pete has a
concussion, a brain tumor, or neither with probabilities 0.79,
0.2, and 0.01, respectively. Dempster-Shafer theory yields
that the probability of a brain tumor is 0.725, which is highly
counterintuitive, because neither Dawn nor Ed thought that
was likely.

This paper contributes (1) a rigorous, probabilistic deﬁni-
tion of certainty that satisﬁes the above key intuitions, (2)
the establishment of a bijection between trust reports and ev-
idence, which enables the principled combination of trust re-
ports, and (3) an efﬁcient algorithm for computing this bijec-
tion.

abilistic terms. Speciﬁcally, an agent can represent the prob-
ability of a positive experience with, i.e., good behavior by,
another agent. This probability must lie in the real interval
[0, 1]. The agent’s trust corresponds to how strongly the agent
believes that this probability is a speciﬁc value (whether large
or small, we don’t care). This strength of belief is also cap-
tured in probabilistic terms. To this end, we formulate a prob-
ability density function of the probability of a positive expe-
rience. Following [Jøsang, 1998], we term this a probability-
certainty density function (PCDF). In our approach, unlike
Jøsang’s, certainty is a statistical measure deﬁned on a PCDF.

2.1 Certainty from a PCDF
Because the cumulative probability of a probability lying
within [0, 1] must equal 1, all PCDFs must have the mean
density of 1 over [0, 1], and 0 elsewhere. Lacking additional
knowledge, a PCDF would be a uniform distribution over
[0, 1]. However, with additional knowledge, the PCDF would
deviate from the uniform distribution. For example, know-
ing that the probability of good behavior is at least 0.5, we
would obtain a distribution that is 0 over [0, 0.5) and 2 over
[0.5, 1]. Similarly, knowing that the probability of good be-
havior lies in [0.5, 0.6], we would obtain a distribution that is
0 over [0, 0.5) and (0.6, 1], and 10 over [0.5, 0.6].

In formal terms, let p ∈ [0, 1] represent the probability of
a positive outcome. Let the distribution of p be given as a
function f : [0, 1] (cid:4)→ [0, ∞) such that
0 f (p)dp = 1. The
probability that the probability of a positive outcome lies in
[p1, p2] can be calculated by
f (p)dp. The mean value of

(cid:2) 1

(cid:2)

p2
p1

R
0 f (p)dp

1

1−0

= 1. When we know nothing else, f is a uni-
f is
form distribution over probabilities p. That is, f (p) = 1 for
p ∈ [0, 1] and 0 elsewhere. This reﬂects the Bayesian intu-
ition of assuming an equiprobable prior. The uniform distri-
bution has a certainty of 0. As more knowledge is acquired,
the probability mass shifts so that f (p) is above 1 for some
values of p and below 1 for other values of p.

Our key intuition is that the agent’s trust corresponds to in-
creasing deviation from the uniform distribution. Two of the
most established measures for deviation are standard devia-
tion and mean absolute deviation (MAD). MAD is more ro-
bust, because it does not involve squaring (which can increase
standard deviation because of outliers or “heavy tail” distri-
butions such as the notorious Cauchy distribution). Abso-
lute values can sometimes complicate the mathematics. But,
in the present setting, MAD turns out to yield straightfor-
ward mathematics. In a discrete setting involving data points
i=1|xi − ˆx|. In
x1. . . xn with mean ˆx, MAD is given by
the present case, instead of n we divide by the size of the do-
main, i.e., 1. Because a PCDF has a mean value of 1, increase
in some parts must match reduction elsewhere. Both increase
and reduction from 1 are counted by |f (p) − 1|. Deﬁnition 1
1
2 to remove this double counting.
scales the MAD for f by

1
n Σn

Deﬁnition 1 The certainty based on f , cf , is given by cf =
1
2

|f (p) − 1|dp

0

(cid:2) 1

2 Modeling Certainty
The proposed approach is based on the fundamental intuition
that an agent can model the behavior of another agent in prob-

Certainty captures the fraction of the knowledge that we do
have. For motivation, consider randomly picking a ball from
a bin that contains N balls colored white or black. Suppose

IJCAI-07

1552

p is the probability that the ball randomly picked is white. If
we have no knowledge about how many white balls there are
in the bin, we can’t estimate p with any conﬁdence That is,
certainty c = 0. If we know that exactly m balls are white,
then we have perfect knowledge about the distribution. We
can estimate p = m
N with c = 1. However, if all we know is
that at least m balls are white and at least n balls are black
(thus m + n ≤ N ), then we have partial knowledge. Here
c = m+n
N . The probability of drawing a white ball ranges
N to 1 − n
from m

N . We have

⎧⎨
⎩ 0,

f (p) =

N

N −m−n
0

[0 m
N )
p ∈ [ m
(1 − n

N , 1 − n
N ]
N , 1].

Using Deﬁnition 1, we can conﬁrm that cf = m+n
N :
(cid:2) 1

(cid:2) 1− n

(cid:2) 1
(cid:2) m
|f (p) − 1|dp
0
0 1 dp +
N + N −m−n

m
N

N

(

N

cf = 1
2
= 1
2 (
= 1
2 ( m
= m+n

N

N

N −m−n

− 1)dp +

1 dp

1− n

N

N

(
N

N −m−n

− 1) + n
N )

2.2 Evidence and Trust Spaces Conceptually

For simplicity, we model a (rating) agent’s experience with
a (rated) agent as a binary event: positive or negative. Evi-
dence is conceptualized in terms of the numbers of positive
and negative experiences.
In terms of direct observations,
these numbers would obviously be whole numbers. However,
our motivation is to combine evidence in the context of trust.
As Section 1 motivates, for reasons of dynamism or compo-
sition, the evidence may need to be discounted to reﬂect the
aging of or the imperfect trust placed in the evidence source.
Intuitively, because of such discounting, the evidence is best
understood as if there were real (i.e., not necessarily natural)
numbers of experiences. Accordingly, we model the evidence
space as E = R × R, a two-dimensional space of reals. The
members of E are pairs (cid:8)r, s(cid:9) corresponding to the numbers
of positive and negative experiences, respectively. Combin-
ing evidence is trivial: simply perform vector sum.
Deﬁnition 2 Deﬁne evidence space E = {(r, s)|r ≥ 0, s ≥
0, t = r + s > 0}

Let x be the probability of a positive outcome. The poste-
rior probability of evidence (cid:8)r, s(cid:9) is the conditional probabil-
ity of x given (cid:8)r, s(cid:9) [Casella and Berger, 1990, p. 298].
Deﬁnition 3 The conditional probability of x given (cid:8)r, s(cid:9) is

(cid:6)

where g((cid:8)r, s(cid:9)|x) =

f (x|(cid:8)r, s(cid:9)) = g((cid:2)r,s(cid:3)|x)f (x)

1

1

r(1−x)s

R
0 g((cid:2)r,s(cid:3)|x)f (x)dx
R
0 xr(1−x)sdx

= x
(cid:7)
xr(1 − x)s

r + s
r

Traditional probability theory models the event (cid:8)r, s(cid:9) by
(α, 1 − α), the expected probabilities of positive and negative
outcomes, respectively, where α = r+1
r+s+2 . The traditional
probability model ignores uncertainty.

A trust space consists of trust reports modeled in a three-
dimensional space of reals in (0, 1). Each point in this space
is a triple (cid:8)b, d, u(cid:9), where b + d + u = 1, representing the
weights assigned to belief, disbelief, and uncertainty, respec-
tively. Certainty c is simply 1 − u. Thus c = 1 and c = 0
indicate perfect knowledge and ignorance, respectively.

Combining trust reports is nontrivial. Our proposed deﬁni-
tion of certainty is key in accomplishing a bijection between
evidence and trust reports. The problem of combining inde-
pendent trust reports is reduced to the problem of combining
the evidence underlying them. (Deﬁnitions 2 and 4 are based
on [Jøsang, 2001].)
Deﬁnition 4 Deﬁne trust space as T = {(b, d, u)|b > 0, d >
0, u > 0, b + d + u = 1}.

2.3 From Evidence to Trust Reports
Using Deﬁnition 3, deﬁne certainty based on evidence (cid:8)r, s(cid:9):
Deﬁnition 5 c(r, s) = 1
2

− 1|dx

(cid:2) 1

r(1−x)s

(x
R
0 xr(1−x)sdx

0

|

1

Throughout, r, s, and t = r + s refer to positive, negative,
and total evidence, respectively. Importantly, α = r+1
t+2 , the
expected value of the probability of a positive outcome, also
characterizes conﬂict in the evidence. Clearly, α ∈ (0, 1):
α approaching 0 or 1 indicates unanimity, whereas α = 0.5
means r = s, i.e., maximal conﬂict in the body of evidence.
We can write c(r, s) as c((t + 2)α − 1, (t + 2)(1 − α) − 1).
When α is ﬁxed, certainty is a function of t, written c(t).
(cid:4)(t)
When t is ﬁxed, it is a function of α, written c(α). And, c
and c

(cid:4)(α) are the corresponding derivatives.

The following is our transformation from evidence to trust
spaces. This transformation relates positive and negative evi-
dence to belief and disbelief, respectively, but discounted by
the certainty. The idea of adding 1 each to r and s (and thus
2 to r + s) follows Laplace’s famous rule of succession for
applying probability to inductive reasoning [Sunrise, 2006].
This reduces the impact of sparse evidence, and is sometimes
termed Laplace smoothing. If you only made one observa-
tion and it was positive, you would not want to conclude that
there would never be a negative observation. As the body of
evidence increases, the increment of 1 becomes negligible.
More sophisticated formulations of rules of succession exist
[Ristad, 1995], but Laplace’s rule is simple and reasonably
effective for our present purposes. Laplace’s rule is insen-
sitive to the number of outcomes in that 1 is always added.
The effect of this statistical “correction” (the added 1) de-
creases inversely as the number of outcomes being considered
increases. More sophisticated approaches may be thought of
as decreasing the effects of their corrections more rapidly.

Deﬁnition 6 Let Z(r, s) = (b, d, u) be a transformation
from E to T such that Z = (b(r, s), d(r, s), u(r, s)), where
b(r, s) = αc(r, s), d(r, s) = (1 − α)c(r, s), and u(r, s) =
1 − c(r, s).

One can easily verify that c(0, 1) > 0. In general, because
t = r + s > 0, c(r, s) > 0. Moreover, c(r, s) < 1: thus,
1 − c(r, s) > 0. This coupled with the rule of succession
ensures that b > 0, d > 0, and u > 0. Notice that α = b
b+d .

IJCAI-07

1553

s
t+1 ,

Jøsang et al. [1998] map evidence (cid:8)r, s(cid:9) to a trust triple
1
( r
t+1 ). Two main differences with our approach
t+1 ,
are: (1) they ignore the rule of succession and (2) in essence,
t
they deﬁne certainty as
t+1 . They offer no mathematical jus-
tiﬁcation for doing so. Section 3.2 shows an unintuitive con-
sequence of their deﬁnition.

3 Important Properties and Computation

We now show that the above deﬁnition yields important for-
mal properties and how to compute with it.

3.1

Increasing Experiences with Fixed Conﬂict

Consider the scenario where the total number of experiences
increases for ﬁxed α = 0.70. For example, compare observ-
ing 6 good episodes out of 8 with observing 69 good episodes
out of 98. The expected value, α, is the same in both cases,
but the certainty is clearly greater in the second. In general,
we would expect certainty to increase as the amount of evi-
dence increases. Deﬁnition 5 yields a certainty of 0.46 from
(cid:8)r, s(cid:9) = (cid:8)6, 2(cid:9), but a certainty of 0.70 for (cid:8)r, s(cid:9) = (cid:8)69, 29(cid:9).

Figure 1 plots how certainty varies with t when α = 0.5.

Theorem 1 captures this property in general.

i

y
t
n
a
t
r
e
C

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

5

10

15

20

25

30

35

40

# of total outcomes

Figure 1: Certainty increases with t when conﬂict (α = 0.5)
is ﬁxed; X-axis: t; Y-axis: c(t)

Theorem 1 Fix α. Then c(t) increases with t for t > 0.

Proof idea: Show that c

(cid:4)(t) > 0 for t > 0.

The full proofs of this and other theorems of this paper are

included in a technical report [Wang and Singh, 2006b].

3.2

Increasing Conﬂict with Fixed Experience

Another important scenario is when the total number of ex-
periences is ﬁxed, but the evidence varies to reﬂect different
levels of conﬂict by using different values of α. Clearly, cer-
tainty should increase as r or s dominates the other (i.e., α
approaches 0 or 1) but should reduce as r and s are balanced
(i.e., α approaches 0.5). Figure 2 plots certainty for ﬁxed t
and varying conﬂict.

More speciﬁcally, consider Alice’s example from Sec-

tion 1. Table 1 shows the effect of conﬂict where t = 4.

Theorem 2 captures the property that certainty increases

with increasing unanimity.

i

y
t
n
a
t
r
e
C

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0  

0.2

0.4

0.6

0.8

1.0

Conflict ratio

Figure 2: Certainty is concave when t is ﬁxed at 10; X-axis:
r + 1; Y-axis: c(α); minimum occurs at r = s = 5

Table 1: Certainty computed by different approaches for
varying conﬂict

Our approach
Jøsang et al.
Yu & Singh

(cid:8)0, 4(cid:9)
0.54
0.80
0

(cid:8)1, 3(cid:9)
0.35
0.80
0

(cid:8)2, 2(cid:9)
0.29
0.80
0

(cid:8)3, 1(cid:9)
0.35
0.80
0

(cid:8)4, 0(cid:9)
0.54
0.80
0

Theorem 2 c(α) is decreasing when 0 < α ≤ 1
when

2 ≤ α < 1 and c(α), and minimum at α = 1
2 .

1

2 , increasing

Proof idea: Show that c
(cid:4)(α) > 0 when α ∈ [0.5, 1.0).
c

(cid:4)(α) < 0 when α ∈ [0, 0.5) and

i

y
t
n
a
t
r
e
C

0.8

0.6

0.4

0.2

0
5

4

3

2

1

# of negative outcomes

0

0

5

4

3

2

1

# of positive outcomes

Figure 3: X-axis: r; Y-axis: s; Z-axis: certainty

Putting the above results together suggests that the rela-
tionship between certainty on the one hand and positive and
negative evidence on the other is nontrivial. Figure 3 con-
ﬁrms this intuition by plotting certainty against r and s as a
surface.

3.3 Bijection Between Evidence and Trust Reports

The ability to combine trust reports effectively relies on be-
ing able to map between the evidence and the trust spaces.
With such a mapping in hand, to combine two trust reports,
we would simply perform the following steps: (1) map trust
reports to evidence; (2) combine the evidence; (3) transform

IJCAI-07

1554

the combined evidence to a trust report. The following theo-
rem establishes that Z has a unique inverse Z

−1

.

Theorem 3 The transformation Z is a bijection.
Proof sketch: Given (b, d, u) ∈ T , we need (r, s) ∈ E such
that Z(r, s) = (b, d, u). As explained in Section 2.3, α =
b+d . Thus, we only need to ﬁnd t such that c(t) = 1 − u. The
existence and uniqueness of t is proved by showing that

b

1. c(t) is increasing when t > 0 (Theorem 1)

2. limt→∞ c(t) = 1
3. limt→0 c(t) = 0

Brieﬂy, Yu and Singh [2002] base uncertainty not on con-
ﬂict, but on intermediate (neither positive not negative) out-
comes. Let’s revisit Pete’s example of Section 1. In our ap-
proach, Dawn and Ed’s diagnoses correspond to two b, d, u
triples (where b means “tumor” and d means “not a tumor”):
(0.2, 0.79, 0.01) and (0.2, 0.79, 0.01), respectively. Combin-
ing these we obtain the b, d, u triple of (0.21, 0.78, 0.01).
That is, the weight assigned to a tumor is 0.21 as opposed
to 0.725 by Dempster-Shafer theory, which is unintuitive, be-
cause a tumor is Dawn and Ed’s least likely prediction.

3.4 Algorithm and Complexity

−1

. Algorithm 1 calculates
No closed form is known for Z
(via binary search on c(t)) to any necessary precision,
Z
 > 0. Here tmax > 0 is the maximum evidence considered.

−1

1

2

3

4

5

6

7

b+d ;

α = b
t1 = 0;
t2 = tmax;
while t2 − t1 ≥  do

;

2

t = t1+t2
if c(t) < c then t1 = t else t2 = t
return r = ((t + 2)α − 1), s = t − r
Algorithm 1: Calculating (r, s) = Z

−1(b, d, u)

Theorem 4 The complexity of Algorithm 1 is Ω(− lg ).
Proof: After the while loop iterates i times, t2 − t1 =
tmax2−i. Eventually, t2 − t1 falls below , thus terminat-
ing the while loop. Assume it terminates in n iterations.
Then, t2 − t1 = tmax2−n <  ≤ tmax2−n+1
. This implies
. That is, n > (lg tmax − lg ) ≥ n − 1.
2n > tmax

≥ 2n−1



4 Discussion

This paper is meant to offer a theoretical development of trust
that would underlie a variety of situations where trust reports
based on evidence are combined. In particular, it contributes
to a mathematical understanding of trust, especially as it un-
derlies a variety of multiagent applications. These include
referral systems and webs of trust in particular, in studying
which we identiﬁed the need for this research. Such applica-
tions require a natural treatment of composition and discount-
ing in an evidence-based framework.

Further, an evidence-based notion of trust must support im-
portant properties regarding the effects of increasing evidence
(for constant conﬂict) and of increasing conﬂict (for constant
evidence). The theoretical validation provided here is highly
valuable in a general-purpose conceptually driven mathemat-
ical approach. The main technical insight of this paper is how
to manage the duality between trust and evidence spaces in
a manner that provides a rigorous basis for combining trust
reports.

Let’s brieﬂy revisit the topic of trust dynamics from Sec-
tion 1. The foregoing showed how trust evolves with respect
to increasing outcomes under different conditions. The same
properties apply to the evolution of trust over time, that is, as
time passes and more evidence is obtained. A crucial obser-
vation is that because of the bijection we established, the his-
torical evidence at any point can be summarized in a belief-
disbelief-uncertainty triple. New evidence can then be added
as explained above. Moreover, we can discount the value of
evidence over time if necessary, e.g., at every time step (cho-
sen based on the domain: every hour or day, or after every
transaction). Thus new evidence would have a greater impact
than older evidence.

A payoff of this approach is that an agent who wishes to
achieve a speciﬁc level of certainty can compute how much
evidence would be needed at different levels of conﬂict. Or,
the agent can iteratively compute certainty to see if it has
reached an acceptable level.

4.1 Directions
This work has opened up some important directions for fu-
ture work. An important technical challenge is to extend the
above work from binary to multivalued events. Such an ex-
tension will enable us to handle a larger variety of interactions
among people and services. A current direction is to experi-
mentally validate this work, doing which is made difﬁcult by
the lack of established datasets and testbeds, but the situation
is improving in this regard [Fullam et al., 2005].

4.2 Literature on Trust
A huge amount of research has been conducted on trust, even
if we limit our attention to evidential approaches. Abdul-
Rahman and Hailes [2000] present an early model for com-
puting trust. However, their approach is highly ad hoc and
limited. Speciﬁcally, various weights are simply added up
without any mathematical justiﬁcation. Likewise, the term
uncertainty is described but without any foundation.

The Regret system combines several aspects of trust, no-
tably the social aspects [Sabater and Sierra, 2002]. It involves
a number of formulas, which are given intuitive, but not math-
ematical, justiﬁcation. A lot of other work, e.g., [Huynh et al.,
2006], involves heuristics that combine multiple information
sources to judge trust. It would be an interesting direction
to combine a rigorous approach such as ours with the above
heuristic approaches to capture a rich variety of practical cri-
teria well.

Teacy et al.

[2005] develop a probabilistic treatment of
trust. They model trust in terms of conﬁdence that the ex-
pected value lies within a speciﬁed error tolerance. An
agent’s conﬁdence increases with the error tolerance. Teacy et

IJCAI-07

1555

[Jøsang, 1998] Audun Jøsang. A subjective metric of authen-
tication. In Proceedings of the 5th European Symposium
on Research in Computer Security (ESORICS), volume
1485 of LNCS, pages 329–344. Springer-Verlag, 1998.

[Jøsang, 2001] Audun Jøsang. A logic for uncertain proba-
bilities. Journal of Uncertainty, Fuzziness and Knowledge-
Based Systems, 9:279–311, 2001.

[Ristad, 1995] Eric Sven Ristad. A natural law of succession.
TR 495-95, Department of Computer Science, Princeton
University, July 1995.

[Sabater and Sierra, 2002] Jordi Sabater and Carles Sierra.
Reputation and social network analysis in multi-agent sys-
tems. In Proceedings of the 1st International Joint Confer-
ence on Autonomous Agents and Multiagent Systems (AA-
MAS), pages 475–482. ACM Press, July 2002.

[Sentz and Ferson, 2002] Karl Sentz and Scott Ferson. Com-
bination of evidence in Dempster Shafer theory. TR 0835,
Sandia National Laboratories, Albuquerque, New Mexico,
2002.

[Shannon, 1948] Claude E. Shannon. The mathematical the-
ory of communication. Bell System Technical Journal,
27(3):379–423, 1948.

[Smith, 2001] Jonathan D. H. Smith. Some observations on
the concepts of information-theoretic entropy and random-
ness. Entropy, 3:1–11, 2001.

[Sunrise, 2006] The

problem,
http://en.wikipedia.org/wiki/Sunrise problem.

sunrise

2006.

[Teacy et al., 2005] Luke Teacy, Jigar Patel, Nicholas Jen-
nings, and Michael Luck. Coping with inaccurate rep-
utation sources: Experimental analysis of a probabilistic
trust model. In Proceedings of the 4th International Joint
Conference on Autonomous Agents and MultiAgent Sys-
tems (AAMAS), pages 997–1004. ACM Press, July 2005.

[Wang and Singh, 2006a] Yonghong Wang and Munindar P.
Singh. Trust representation and aggregation in a dis-
tributed agent system. In Proceedings of the 21st National
Conference on Artiﬁcial Intelligence (AAAI), pages 1425–
1430, 2006a.

[Wang and Singh, 2006b] Yonghong Wang and Munindar P.
Singh. Trust via evidence combination: A mathematical
approach based on certainty. TR 2006-11, North Carolina
State University, Raleigh, 2006b.

[Yu and Singh, 2002] Bin Yu and Munindar P. Singh. Dis-
tributed reputation management for electronic commerce.
Computational Intelligence, 18(4):535–549, November
2002.

al. study combinations of probability distributions to which
the evaluations given by different agents might correspond.
They do not formally study certainty. And their approach
doesn’t yield a probabilistically valid method for combining
trust reports.

4.3 Literature on Information Theory

Shannon entropy [1948] is the best known information-
theoretic measure of uncertainty.
It is based on a discrete
probability distribution p = (cid:8)p(x)|x ∈ X(cid:9) over a ﬁnite
(cid:8)
set X of alternatives (elementary events). Shannon’s for-
mula encodes the number of bits required to obtain certainty:
S(p) = −
x∈X p(x) log2 p(x). Here S(p) can be viewed
as the weighted average of the conﬂict among the eviden-
tial claims expressed by p. More complex, but less well-
established, deﬁnitions of entropy have been proposed for
continuous distributions as well, e.g., [Smith, 2001].

Entropy, however, is not suitable for the present purposes
of modeling evidential trust. Entropy models bits of missing
information which ranges from 0 to ∞. At one level, this
disagrees with our intuition that, for the purposes of trust,
we need to model the conﬁdence placed in a probability es-
timation. Moreover, the above deﬁnitions cannot be used in
measuring the uncertainty of the probability estimation based
on past positive and negative experiences.

Acknowledgments

We thank the anonymous reviewers for their helpful com-
ments and Chung-Wei Hang for useful discussions.

References
[Abdul-Rahman and Hailes, 2000] Alfarez Abdul-Rahman
and Stephen Hailes. Supporting trust in virtual commu-
nities.
In Proceedings of the 33rd Hawaii International
Conference on Systems Science, 2000.

[Casella and Berger, 1990] George Casella and Roger L.

Berger. Statistical Inference. Duxbury Press, 1990.

[Castelfranchi and Falcone, 1998] Cristiano Castelfranchi
and Rino Falcone. Principles of trust for MAS: cog-
nitive anatomy, social
importance, and quantiﬁcation.
In Proceedings of the 3rd International Conference on
Multiagent Systems, pages 72–79, 1998.

[Fullam et al., 2005] Karen Fullam, Tomas B. Klos, Guil-
laume Muller, Jordi Sabater, Andreas Schlosser, Zvi
Topol, K. Suzanne Barber, Jeffrey S. Rosenschein, Laurent
Vercouter, and Marco Voss. A speciﬁcation of the Agent
Reputation and Trust (ART) testbed: experimentation and
competition for trust in agent societies.
In Proceedings
of the 4th International Joint Conference on Autonomous
Agents and MultiAgent Systems (AAMAS), pages 512–518.
ACM Press, July 2005.

[Huynh et al., 2006] Trung Dong Huynh, Nicholas R. Jen-
nings, and Nigel R. Shadbolt. An integrated trust and rep-
utation model for open multi-agent systems. Journal of
Autonomous Agents and MultiAgent Systems, 13(2):119–
154, September 2006.

IJCAI-07

1556


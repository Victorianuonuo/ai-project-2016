Learning Subjective Representations for Planning

Dana Wilkinson

Michael Bowling

Ali Ghodsi

School of Computer Science

Department of Computing Science

School of Computer Science

University of Waterloo
Waterloo, ON, Canada

d3wilkinson@uwaterloo.ca

University of Alberta
Edmonton, AB, Canada
bowling@cs.ualberta.ca

University of Waterloo
Waterloo, ON, Canada
aghodsib@uwaterloo.ca

Abstract

Planning involves using a model of an agent’s ac-
tions to ﬁnd a sequence of decisions which achieve
a desired goal. It is usually assumed that the mod-
els are given, and such models often require ex-
pert knowledge of the domain. This paper ex-
plores subjective representations for planning that
are learned directly from agent observations and ac-
tions (requiring no initial domain knowledge). A
non-linear embedding technique called Action Re-
specting Embedding is used to construct such a rep-
resentation.
It is then shown how to extract the
effects of the agent’s actions as operators in this
learned representation. Finally, the learned repre-
sentation and operators are combined with search to
ﬁnd sequences of actions that achieve given goals.
The efﬁcacy of this technique is demonstrated in a
challenging robot-vision-inspired image domain.

Introduction

1
Planning, at its essence, involves searching an appropriately
deﬁned state space. This state space is a consequence of the
agent’s model of the effects of its actions (e.g., STRIPS [Fikes
and Nilsson, 1971], Markov Decision Processes [Puterman,
1994]). It is assumed that these models can be deﬁned from
domain experts and planning uses these models to ﬁnd se-
quences of actions to achieve given goals. The problem is
that models are not always known or easily built for a do-
main of interest. Others have studied methods for learning or
agumenting models, such as STRIPS operators [Wang, 1995]
or MDP transitions [Peng and Williams, 1993]. All of these
techniques still require expert intuition about the domain to
provide, at the least, an appropriate state representation.

This paper focuses on learning an appropriate representa-
tion for planning, using only an agent’s observations and ac-
tions. We call this a subjective representation as the learned
representation is extracted based only on the agent’s experi-
ence, and requires no expert knowledge of the domain. The
approach solves two important problems: (i) learning an ap-
propriate state-space representation, and (ii) learning the ef-
fects of the agent’s actions in this representation.1 The re-

1The approach of learning a subjective representation more

quired input is a sequence of actions and observations from
the agent’s subjective experience, but no semantic meaning
(domain-speciﬁc or otherwise) is required. This input is used
in a three step process.

First, Section 2 reviews Action Respecting Embedding
(ARE). ARE is a technique for dimensionality reduction that
speciﬁcally makes use of a temporal sequence of observations
and actions. ARE learns manifolds that capture the important
underlying dynamics of the high-dimensional data in much
fewer dimensions (addressing the ﬁrst problem). Next, Sec-
tion 3 describes a method for learning operators for each ac-
tion that can be applied to any point in the learned represen-
tation (addressing the second problem). Examples of learned
operators are provided along with a discussion of how the se-
mantic meaning of the associated actions can sometimes be
extracted. Finally, Section 4 shows the results of planning in
the resulting representation using the learned operators. Both
the resulting plan in the learned representation, and the result
of applying the plans in the original high-dimensional domain
are compared.

2 Action Respecting Embedding
High-dimensional data sets, such as sequences of images, can
often be characterized by a low-dimensional representation
that is related to the process generating the data. For example,
a low-dimensional representation for image data may corre-
spond to the degrees of freedom of a platform moving a cam-
era. Such a representation is ideal for planning as it directly
captures the actions’ effects on the world. The goal here is
to take a temporal sequence of data points, z1, . . . , zn, and
associated actions, a1, . . . , an−1, and ﬁnd a low-dimensional
representation for zi that is appropriate for planning.

Recently, nonlinear manifold learning techniques have
been used to map a high-dimensional dataset into a smaller
dimensional space. Semideﬁnite Embedding (SDE) [Wein-
berger and Saul, 2004] is one such technique. SDE learns
a kernel matrix, which represents a non-linear projection of

closely resembles recent work on learning predictive representa-
tions [James and Singh, 2004; Rosencrantz et al., 2004; Jaeger,
2000] than previous work on augmenting operators or transition
probabilities. This approach, though, is speciﬁcally designed for
very high-dimensional observation spaces as it implicitly involves a
dimensionality reduction component.

Algorithm: SDE(|| · ||, (z1, . . . , zn))

Construct neighbors, N, using k-NN with || · ||.

Maximize Tr(K) subject to K (cid:23) 0,P

∀ij Nij > 0 ∨ [N T N ]ij > 0 ⇒

Kii − 2Kij + Kjj = ||zi − zj||2
Run Kernel PCA with learned kernel, K.

ij Kij = 0, and

Table 1: Algorithm: Semideﬁnite Embedding (SDE).

the input data into a more linear representation. It then uses
Kernel PCA [Scholkopf and Smola, 2002], a generalization
of principle components analysis using feature spaces rep-
resented by kernels, to extract out a low-dimensional rep-
resentation of the data. The kernel matrix K is learned in
SDE by solving a semideﬁnite program with a simple set of
constraints. The most important constraints encode the com-
mon requirement in dimensionality reduction that the non-
linear embedding should preserve local distances.
In other
words, nearby points in the original input space should re-
main nearby in the resulting feature representation. There-
fore SDE requires a distance metric ||·|| on the original input
space, and uses this metric to construct a k-nearest neighbors
graph. It then adds constraints into the semideﬁnite program
to ensure that the distance between neighbors is preserved.
The optimization maximizes Tr(K), i.e., the variance of the
learned feature representation, which should minimize its di-
mensionality. The SDE Algorithm is shown in Table 1.

SDE does not take into account two important pieces of
knowledge about the data: the temporal ordering of the in-
put vectors, zi, and the action labels, ai. Therefore, SDE
doesn’t guarantee that temporally-nearby input points will
be spatially nearby in the feature representation. Also, SDE
won’t necessarily result in a space where actions have a sim-
ple interpretation. The recent Action Respecting Embedding
(ARE) algorithm [Bowling et al., 2005] extends SDE to make
use of exactly this type of knowledge about the data.

Formally, ARE takes a set of D-dimensional input vectors,
z1, . . . , zn (e.g., images), in temporal order, along with asso-
ciated discrete actions, a1, . . . , an−1, where action ai was ex-
ecuted between input zi and input zi+1. ARE then computes
a set of d-dimensional output vectors x1, . . . , xn in one-to-
one correspondence with the input vectors. This provides a
meaningful embedding in d < D dimensions. ARE modiﬁes
SDE in two key ways. First, it exploits the knowledge that the
images are given in a temporal sequence. It uses this knowl-
edge to build an improved neighborhood graph based on each
input’s distances to its temporal neighbors using the provided
local distance metric2. Second, it constrains the embedding
to respect the action labels that are associated with adjacent
pairs of observations. This ensures that the actions have a
simple interpretation in the resulting feature space.

It is this second enhancement of ARE that is the critical

Algorithm: ARE(|| · ||, (z1, . . . , zn), (a1, . . . , an−1))

Construct neighbors, N, as in [Bowling et al., 2005].

Maximize Tr(K) subject to K (cid:23) 0,P

∀ij Nij > 0 ∨ [N T N ]ij > 0 ⇒
∀ij

ij Kij = 0,
Kii − 2Kij + Kjj ≤ ||zi − zj||2 , and
ai = aj ⇒
K(i+1)(i+1) − 2K(i+1)(j+1) + K(j+1)(j+1) =
Kii − 2Kij + Kjj

Run Kernel PCA with learned kernel, K.

Table 2: Algorithm: Action Respecting Embedding (ARE).

feature for subjective planning. ARE constrains the learned
manifold to be in a space where the labeled actions corre-
spond to distance-preserving transformations—those consist-
ing only of rotation and translation3. Therefore, for any two
inputs, zi and zj, the same action from these inputs must
preserve their distance in the learned feature space. Letting
Φ(zi) denote input zi’s representation in the feature space,
action a’s transformation, fa, must satisfy:

∀i, j

||fa(Φ(zi)) − fa(Φ(zj))|| =
||Φ(zi) − Φ(zj)||.

(1)
Now, let a = ai and consider the case where aj = ai. Then,
fa(Φ(zi)) = Φ(zi+1) and fa(Φ(zj)) = Φ(zj+1), and Con-
straint 1 becomes:

||Φ(zi+1) − Φ(zj+1)|| = ||Φ(zi) − Φ(zj)||.
In terms of the kernel matrix, this can be written as:

(2)

(3)

∀i, j ai = aj ⇒

K(i+1)(i+1) − 2K(i+1)(j+1) + K(j+1)(j+1) =
Kii − 2Kij + Kjj

ARE simply adds Constraint 3 into SDE’s usual constraints
to arrive at the optimization and algorithm shown in Table 2.
Experiments. Here we deﬁne IMAGEBOT, a synthetic im-
age interaction domain used for all experiments. Given an
image, imagine a virtual robot that can observe a small patch
on that image and also take actions to move the patch around
the larger image. This “image robot” provides an excellent
domain in which subjective planning can be demonstrated.

For these experiments, IMAGEBOT will always be view-
ing a 200 by 200 patch of a 2048 by 1536 image displayed
Figure 1. IMAGEBOT has eight distinct actions: four trans-
lations, two zoom actions, and two rotation actions. The al-
lowed translations are forward (F ), back (B), left (L) and
right (R) by 25 pixels The zoom changes the scale of the
underlying image by a factor of 21/8 (i) or 2−1/8 (o). The
rotation rotates the square left (l) or right (r) by π

There are three distinct experimental data sets that are

8 radians.

looked at in this paper.

2We have found that ARE is fairly robust to the choice of distance
metrics, and use simple Euclidean distance for all of the experiments
in this paper.

3Notice this is not requiring the actions in the objective space
to be rotations and translations, since ARE is learning a non-linear
feature representation.

Figure 2: IMAGEBOT’s output (and ARE’s input) for the AT data set.

Figure 1: IMAGEBOT’s world.

Figure 3: The path IMAGEBOT follows to generate AT .

AT : IMAGEBOT follows a path which looks like an “A” us-

ing only the translation actions:
F × 10, L × 5, R × 5, B × 5, L × 5, F × 5, B × 10

AZ: IMAGEBOT follows the same A pattern but substituting
zoom in for left actions and zoom out for right actions:
F × 10, i × 8, o × 8, B × 5, i × 8, F × 10, B × 20

F r: IMAGEBOT moves back and forth in a line, but only us-

ing F and r actions:
F × 10, r × 8, F × 10, r × 8, F × 5, r × 16, F × 5

Note that in AZ the F and B actions only move half as
much when zoomed in as when zoomed out. Note that in F r
there are no opposites for the two actions used. An example
of the output of IMAGEBOT, and correspondingly an input
for ARE, is shown in Figure 2. These are the images seen in
the AT data set. Note that while we know that, for example,
action label 3 corresponds to action F , ARE gets no such
semantic information—it gets as input only the images and
the labels associated with them.

The effectiveness of ARE has been demonstrated previ-
ously [Bowling et al., 2005]. Here evidence is shown of
its power in capturing useful representations for planning.
Figure 3 shows the actual manifold underlying the AT test
set (i.e., IMAGEBOT’s path). Figure 4 shows the manifold
learned by ARE on that test set. Clearly the structure has
been captured.

Figure 4: The representation learned by ARE for AT .

Figure 5 shows a “top” view and a “side” view of the 3-
dimensional manifold learned for the F r data set. here the
portion of the manifold corresponding to rotating to the right
(r) is a black line while the portion corresponding to mov-
ing forward (F ) is a light-gray line. The point corresponding
to the ﬁrst image is circled. Although not as clear as in the
previous case, this manifold is clearly and distinctly captur-
ing the structure of the original path, with two dimensions
capturing the r action and a third capturing the F action. In
Figure 3, the original domain was one in which the actions
were distance-preserving and this structure was, indeed, ex-
tracted. In Figure 5 it is not immediately obvious what man-

33333333335555566666444445555533333444444444159131721252933374145159131721252933374145Figure 5: Two different views of the three-dimensional cylindrical manifold learned for the F r data set.

ifold will be learned which makes the resulting one all the
more impressive—not just as a representation appropriate for
planning but as an aid to intuitive understanding of the origi-
nal underlying structure.

3 Distance Preserving Operators
ARE learns a representation with explicit constraints that the
actions correspond to distance-preserving transformations in
that representation. Before one can plan, though, one needs
to discover these transformations. For each unique action a
there is a collection of data point pairs (xt, xt+1) which are
connected by that action. Another way of thinking of this is
that there is a function fa where fa(xt) = xt+1, and such a
function needs to be learned for each action. Because of the
distance-preserving constraints, fa can be represented as:

fa(xt) = Aaxt + ba = xt+1

Recall that transformations of the above form encode trans-
lation in the ba vector, and rotation and scaling in the Aa ma-
trix. Aa and ba could be learned using simple linear regres-
sion but scaling is not distance preserving so there is the ad-
a Aa = I. It
ditional constraint that Aa does not scale, i.e., AT
turns out that this is similar to the extended orthonormal Pro-
crustes problem [Schoenemann and Carroll, 1970], but with-
out allowing for a global scaling constant. Here the solution
to the regression problem is derived.

Let Xa be the d by n matrix whose columns are xt for all
t such that at = a, and let Ya be the d by n matrix whose
columns are xt+1 for the same t. The goal is to learn a
rotation matrix Aa and a translation ba which maps Xa to
Ya. Formally, the following optimization problem needs to
be solved.

||AaXa + baeT − Ya||

minimize:
subject to: AT

a Aa = I

where e is a column vector with n ones.

In order to obtain the least squares estimation of Aa and

ba, write the Lagrangian function L:
L(Aa, ba, Λ) =

Tr((AaXa + baeT − Ya)T (AaXa + baeT − Ya)) +
Tr(Λ(AT

a Aa − I))

where Λ is a matrix of lagrangian multipliers, and Tr(.) stands
for the trace of a matrix.
L(Aa, ba, Λ) =

Tr(Y T
−2Tr(Y T
+2Tr(ebT

a Ya) + Tr(X T

a AT

a AaXa) + nbT

a ba

a AaXa) − 2Tr(ebT
a AaXa) + Tr(Λ(AT

a Ya)
a Aa − I))

Now take the derivative of the Lagrangian function with re-
spect to the unknowns and set to zero:

∂L
∂Aa

∂L
∂ba

= 2AaX T

a Xa − 2YaX T

a +

+2baeT X T

a + Aa(Λ + ΛT ) = 0

= 2nba − 2Yae + 2AaXae = 0

The translation vector from Equation 5 gives:

(Ya − AaXa)e

ba =

n
a /2 on the right:
Multiplying Equation 4 by AT
A + Aa(Λ + ΛT )AT
a XaAT
a − baeT X T
YaX T
a AT

AaX T

a AT
a

a /2 =

(4)

(5)

(6)

Since the left hand side is symmetric, the right hand side must
also be symmetric. Substituting Equation 6, the right hand
side can be written as:

YaX T

a AT

X T

a AT

a + AaXa

X T

a AT
a

where the last term is symmetric. Thus the rest of the expres-
sion must also be symmetric. This can be simpliﬁed as:

(cid:18) eeT

(cid:19)

n

(cid:19)

n

(cid:18) eeT
a − Ya
(cid:18)
(cid:18)
(cid:19)

Ya

I − eeT
n

(cid:19)

(7)

(cid:19)T

(cid:18)

(cid:18)

Ya

I − eeT
n

Since 7 is symmetric, it should be equivalent to its transpose:

X T
a

AT

a = Aa

Ya

X T
a

(8)

One can easily verify that the following satisﬁes Equation 8:

(cid:19)

X T
a

(cid:19)
(cid:18)
(cid:18)

(cid:18)

AT
a

(cid:18)

I − eeT
n

(cid:19)

I − eeT
n

X T
a

(cid:19)
(cid:19)

V SW T = svd

Ya

Aa = V W T

(9)

Figure 6: Demonstrating distance-preserving operators in the
representation learned for AT .

Figure 7: Demonstrating distance-preserving operators in the
representation learned for AZ.

where svd(·) is the singular value decomposition, so,

V SW T W V T = V W T W ST V T

because W T W = I (since W is orthonormal) and S = ST
(since S is diagonal). Thus Equations 6 and 9 are a solution to
the dual function, D = minΛ L(Aa, ba, Λ). Since this solu-
tion is also feasible with regards to the primal problem, strong
duality holds even though the original problem is non-convex.
Results. Figure 6 shows the two-dimensional representa-
tion that ARE generated for AT . The solid arrows show a
path which consists of new points resulting from the applica-
tion of operators learned for each action (F , B, L and R) as
described above. Clearly, the operators are intuitively captur-
ing the essence of the actions used to generate the data.

Note that while there is no semantic meaning given with
the input actions, such meaning can now be derived. It can
easily be tested whether a pair of actions are opposites, such
as (F, B) or (R, L). Also two actions can be tested for or-
thogonality or independence, such as F and R. If the learned
representation captures some underlying structure within the
data, the learned operators will maintain that structure and,
from them, relationships can be successfully hypothesized.

Figure 7 is similar to Figure 6, except the underlying rep-
resentation is from the AZ data set. Here, note that while
some actions are again opposite to each other, no actions are
orthoganal—the F and B actions are not independent of the
i and o actions. This critical facet of the original data set has
been successfully captured in the representation learned by
ARE and consequently in the action functions learned in that
manifold. Note, in particular, that when zoomed in all the
way (i × 8) 10 F actions are equivalent to 5 F actions when
zoomed out all the way. Although F action when zoomed
in half way was never observed, the learned operators cap-
ture the fact that between 7 and 8 F actions at this scale are
equivalent to 5 and 10 actions at the other scales.

4 Planning
Now that low-dimensional representations and operators in
those representations can be learned, all the pieces are in
place to perform planning. The points in the learned represen-
tation are states, and the operators learned in Section 3 deﬁne
transitions between the states. This new domain has two ad-
vantages over the original data set. First, the dimensionality
has been reduced drastically (from 40,000 to 2 or 3). Second,

from the action labels functions have been learned for each
unique action that explicitly give the resulting state when that
action is applied from any state. Learning such a function in
the orignal space is not only intractable, but would require
application of knowledge speciﬁc to IMAGEBOT (or even the
underlying image) in order to attain any success.

Given any two images, one can ﬁnd a shortest path be-
tween them, even if it traverses unobserved parts of the space.
First, ﬁnd the corresponding points in the low-dimensional
representation, then ﬁnd the shortest path between them us-
ing traditional search methods and the set of learned opera-
tors. Since each operator in the low-dimensional space corre-
sponds to an action label in the orginal space the list of action
labels that indicate the desired path can be returned. For the
following results, iterative-deepening depth-ﬁrst search was
used and the path whose ﬁnal point was closest to the desired
goal was returned. The quality of a path is demonstrated by
starting IMAGEBOT at the initial state, applying the sequence
of actions and showing the resulting image.
Results. For each of the three test sets, two sub-ﬁgures will
be shown. The ﬁrst shows the representation learned for that
data set and the shortest path between a chosen initial state
(labelled with a triangle pointing right) and a chosen goal
state (labelled with a triangle pointing left). The second ﬁgure
contains two images. The left image shows the image at the
initial state, the right image contains two highlighted boxes.
The light-gray dotted box shows the image at the goal state,
the darker-gray solid box highlights the image obtained after
executing the resulting sequence of actions.

Figures 4(a) and 4(b) show the results for AT —the goal
state image and the image corresponding to the ﬁnal state in
our path in Figure 4(b) are the same. Note that the shortest
path was successfully found, even though it involves moving
through portions of the space that we have never seen.

Figures 4(c) and 4(d) show the results for AZ—the goal
state image and the image corresponding to the ﬁnal state in
the path in Figure 4(d) are the same. Note, the path found
successfully identiﬁes that action B must occur before action
o—if taken after then the B action would jump over the de-
sired end state to a state halfway between it and the next state.
Figures 4(e) and 4(f) show the results for F r—the goal
state image and the image corresponding to the ﬁnal state in
the path in Figure 4(f) are very close to each other. Recall
that for this data set, unlike the others, the only actions were

RRRRRRFFFLLLBBiiiiFFFFFFFFoooBBBBBB(a)

(b)

(c)

(d)

(e)

(f)

Figure 8: Results of planning in three examples. (a), (c), and (e) show the paths in the learned representation. (b), (d), and (f)
show the starting image, the sequence of actions, and outlines the resulting and goal images.

F and r with no corresponding opposites. This means that
such a path must rotate all the way around, step forward, then
rotate all the way around again—a fairly complex path.

5 Conclusion
ARE can be used to learn a subjective representation appro-
priate for planning. The only input required is a sequence
of observations and actions—no additional domain-speciﬁc
knowledge is necessary. The output from ARE is a new low-
dimensional space which captures the critical dynamics of the
environment. Operators which reﬂect these dynamics can be
recovered in the new space. Simple search procedures can
then can be used to ﬁnd sequences of operators which achieve
goals in the learned representation. Since operators corre-
spond to original actions, this sequence provides a plan in the
original space. These plans are accurate, even though they
can involve actions in unobserved parts of the space.

Acknowledgments
We thank Finnegan Southey, Dale Schuurmans, and Pascal
Poupart for discussions and insight. We thank Wesley Loh for
helping with implementation details. We acknowledge Al-
berta Ingenuity Fund for their support of this research through
the Alberta Ingenuity Centre for Machine Learning.

References
[Bowling et al., 2005] Michael Bowling, Ali Ghodsi, and
Dana Wilkinson. Action respecting embedding. Technical
Report TR05-09, University of Alberta, 2005.

[Fikes and Nilsson, 1971] Richard Fikes and Nils J. Nilsson.
Strips: A new approach to the application of theorem prov-
ing to problem solving. In IJCAI, pages 608–620, 1971.

[Jaeger, 2000] Herbert Jaeger. Observable operator models
for discrete stochastic time series. Neural Computation,
12(6):1371–1398, 2000.

[James and Singh, 2004] Michael R. James and Satinder
Singh. Learning and discovery of predictive state represen-
tations in dynamical systems with reset. In ICML, 2004.

[Peng and Williams, 1993] J. Peng and R. J. Williams. Ef-
ﬁcient learning and planning within the Dyna framework.
Adaptive Behavior, 2:437–454, 1993.

[Puterman, 1994] Martin L. Puterman. Markov Decision
Processes: Discrete Stochastic Dynamic Programming.
John Wiley & Sons, Inc., 1994.

[Rosencrantz et al., 2004] Matthew Rosencrantz, Geoff Gor-
don, and Sebastian Thrun. Learning low dimensional pre-
dictive representations. In ICML, 2004.

[Schoenemann and Carroll, 1970] P. H. Schoenemann and
R. Carroll. Fitting one matrix to another choice of a central
dilation and a rigid motion. Psychometrika, 35(2), 1970.

[Scholkopf and Smola, 2002] B. Scholkopf and A. Smola.

Learning with Kernels. MIT Press, 2002.

[Wang, 1995] Xuemei Wang. Learning by observation and
practice: an incremental approach for planning operator
acquisition. In ICML, pages 549–557, 1995.

[Weinberger and Saul, 2004] K. Weinberger and L. Saul.
Unsupervised learning of image manifolds by semideﬁnite
programing. In CVPR, pages 988–995, 2004.

RFFRRRRBoooooooorrFrrrrrrrrrrrrrrR,F×2,R×4R,F×2,R×4r×7,F,r×9
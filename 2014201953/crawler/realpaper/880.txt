Learning from Partial Observations

∗

Loizos Michael

Division of Engineering and Applied Sciences

Harvard University, Cambridge, MA 02138, U.S.A.

loizos@eecs.harvard.edu

Abstract

We present a general machine learning framework
for modelling the phenomenon of missing informa-
tion in data. We propose a masking process model
to capture the stochastic nature of information loss.
Learning in this context is employed as a means to
recover as much of the missing information as is re-
coverable. We extend the Probably Approximately
Correct semantics to the case of learning from par-
tial observations with arbitrarily hidden attributes.
We establish that simply requiring learned hypothe-
ses to be consistent with observed values sufﬁces to
guarantee that hidden values are recoverable to a
certain accuracy; we also show that, in some sense,
this is an optimal strategy for achieving accurate re-
covery. We then establish that a number of natural
concept classes, including all the classes of mono-
tone formulas that are PAC learnable by monotone
formulas, and the classes of conjunctions, disjunc-
tions, k-CNF, k-DNF, and linear thresholds, are
consistently learnable from partial observations.
We ﬁnally show that the concept classes of parities
and monotone term 1-decision lists are not properly
consistently learnable from partial observations, if
RP (cid:3)= NP. This implies a separation of what is con-
sistently learnable from partial observations versus
what is learnable in the complete or noisy setting.

1 Introduction

Consider the task of predicting missing entries in a medical
database, given the information that is already available. How
does one go about making such predictions, and what kind of
guarantees might one provide on the accuracy of these pre-
dictions? The problem with which one is faced here is that of
missing information in data, an arguably universal and multi-
disciplinary problem. Standard statistical techniques [Schafer
and Graham, 2002] fail to provide a formal treatment for the
general case of this problem, where the fact that information
is missing might be arbitrarily correlated with the actual value
of the missing information (e.g., patients exhibiting a certain
symptom might be less inclined to disclose this fact).

∗This work was supported by grant NSF-CCF-04-27129.

In this work we employ learning as a means of identifying
structure in a domain of interest, given access to certain ob-
servations. We subsequently utilize such identiﬁed structure
to recover missing information in new observations coming
from the same domain. Note that the manner in which ac-
quired knowledge may be utilized to draw conclusions is not
necessarily a single step process (see, e.g., [Valiant, 2000]).
Nonetheless, we focus here on examining whether even in-
dividual learned rules can be meaningfully applied on partial
observations, given that such rules are learned from obser-
vations that are partial themselves. Studying how multiple
learned rules can be chained and reasoned with to draw richer
conclusions presents further challenges, and necessitates a so-
lution to the more fundamental problem examined herein.

We present a general machine learning framework within
which the problem of dealing with missing information can
be understood. We formulate the notion of masked attributes,
whose values in learning examples (e.g., patient records) are
not made known to an agent. Such masked attributes account
for missing information both in the given target that the agent
attempts to learn (e.g., the presence of a particular disease),
as well as in the learning features over which the agent’s hy-
potheses are formed (e.g., various pieces of information from
a patient’s medical history). Masked attributes are determined
by an arbitrary stochastic process that induces for each ex-
ample a possibly different but ﬁxed distribution over partial
observations to which the example is mapped (see also [Schu-
urmans and Greiner, 1994]); this is intended to capture situa-
tions such as the probabilistic failure or inability of an agent’s
sensors to provide readings. We extend the Probably Approx-
imately Correct learning semantics [Valiant, 1984] to apply
to the described situation. A salient feature of the extension
we propose is the lack of need for specially prepared learn-
ing materials; the agent simply utilizes whatever information
is made available through the masking process. We call this
type of learning autodidactic to emphasize that although the
agent might still employ supervised learning techniques, this
is done without the presence of a teacher explicitly providing
the agent with “labelled instances” during the learning phase.
We propose consistency as an intuitive measure of success
of the learning process. An agent faced with partial observa-
tions needs to produce hypotheses that do not contradict what
is actually observed; the values of masked attributes need not
be predicted correctly. In addition, hypotheses that do not as-

IJCAI-07

968

sume a deﬁnite value due to masked attributes need not make
a prediction. We allow, thus, the possibility of “don’t know”
predictions, but restrict such predictions in a natural manner,
providing a notion of completeness of the prediction process.
Following the presentation of our framework, we discuss
accuracy as an alternative measure of success, whereby an
agent is expected to correctly predict the values of masked at-
tributes. We show that the success of an agent in this stricter
setting might be completely impaired, depending on how con-
cealing the masking process is (i.e., how adversarially infor-
mation is hidden from the agent). On the positive side, we
show that to the degree allowed by the masking process, an
agent can perform optimally in making accurate predictions,
by simply making consistent predictions. This surprising re-
lation between the two measures of success allows an agent to
focus on the more natural task of learning consistently, while
not losing anything with respect to predicting accurately.

We then examine consistent learnability more closely. We
deﬁne a notion of reduction between learning tasks, and es-
tablish that any concept class of monotone formulas that is
PAC learnable by some hypothesis class of monotone formu-
las is also consistently learnable from partial observations;
the result is obtained by reducing the learning task to one over
complete observations. Through a second reduction we show
that the concept classes of conjunctions, disjunctions, k-CNF,
k-DNF, and linear thresholds over literals, are all properly
consistently learnable from partial observations.

On the negative side, we show that the set of consistently
learnable concept classes is a subset of the PAC learnable con-
cept classes. We continue to prove that the concept classes of
parities and monotone term 1-decision lists are not properly
consistently learnable from partial observations, given that
the widely held complexity assumption RP (cid:3)= NP is true. The
intractability of properly learning monotone term 1-decision
lists from partial observations provides a partial answer to a
question posed by Rivest [1987]. Our intractability results es-
tablish separations between our model of consistent learnabil-
ity from partial observations, and the existing models of PAC
learnability [Valiant, 1984] and learnability in the presence of
random classiﬁcation noise [Angluin and Laird, 1987].

We assume the reader is familiar with basic PAC learning
terminology (see, e.g., [Kearns and Vazirani, 1994]). Proofs
are only brieﬂy discussed in this paper due to lack of space.

2 The Learning Framework

In the PAC learning model [Valiant, 1984], a set of boolean
variables {x1, x2, . . . , xn} represents the attributes of the en-
vironment. A concept c is a boolean formula over the boolean
variables. An example for the concept c is a truth-assignment
to the boolean variables, drawn from an underlying probabil-
ity distribution D, paired with the induced truth-value of c.

Such a treatment distinguishes the target attribute from the
attributes acting as learning features for that target. As a more
natural and better suited approach for autodidactic learning,
where target attributes are not externally “labelled”, we con-
sider examples that treat all attributes equally as properties of
the environment. The attribute acting as a learning target need
only be deﬁned as part of the learning task one undertakes.

Deﬁnition 2.1 (Examples and Observations) Consider any
non-empty ﬁnite set A of attributes. An example over A is a
vector exm ∈ {0, 1}|A|
. An observation over A is a vector
obs ∈ {0, 1, ∗}|A|
. An observation obs masks an exam-
ple exm if obs[i] ∈ {exm[i], ∗} for every attribute xi ∈ A.
An attribute xi ∈ A is masked in an observation obs if
obs[i] = ∗. A masking process is a (stochastic) function
mask : {0, 1}|A| → {0, 1, ∗}|A|
that maps each example
exm to some observation obs that masks exm.

Examples deﬁne the “truth” about the environment. Such
examples are drawn from some underlying ﬁxed probability
distribution D that is unknown to the agent. Unlike standard
PAC learning, the agent does not directly observe such exam-
ples, but only masked versions of the examples. We denote by
mask(D) the induced distribution over these observations.

The stochastic masking process can be understood in two
ways. If attributes correspond to an agent’s sensors, masking
corresponds to the stochastic failure of these sensors to pro-
vide readings. If attributes correspond to properties of the en-
vironment, masking corresponds to an agent’s inability to si-
multaneously sense all properties. In either case, masking in-
duces for each example exm a possibly different but ﬁxed dis-
tribution mask(exm) over observations; the induced distribu-
tions remain unknown to the agent, and so does mask(D).

Masked attributes have their values hidden, without any
connotations. In particular, a masked attribute is not to be
understood as “non-deducible” from the rest of the attributes.
The goal of an agent is not to deduce that a masked attribute
is assigned the value ∗, but rather to deduce the truth-value of
the masked attribute according to the underlying masked ex-
ample. This is a rather non-trivial, and sometimes impossible,
task, depending on the masking process being considered.
) over
Deﬁnition 2.2 (Formulas) A formula f (xi1 , . . . , xik
A is a function f : {0, 1}k → {0, 1} whose arguments
∈ A. The
are associated with the attributes xi1 , . . . , xik
) given an example exm is deﬁned to
value of f (xi1 , . . . , xik
be val(f (xi1 , . . . , xik
) | exm) (cid:2) f (exm[i1], . . . , exm[ik]).
) given an observation obs, de-
The value of f (xi1 , . . . , xik
) | obs), is deﬁned to be the
noted by val(f (xi1 , . . . , xik
common value of the formula given all examples masked by
obs, in case such a common value exists, or ∗ otherwise.

An agent’s task of identifying structure in its environment
can be made precise as the problem of learning how a certain
target attribute in A can be expressed as a formula over other
attributes in A, as these are perceived through the agent’s sen-
sors.1 To study learnability, one usually assumes that the tar-
get attribute is indeed expressible as such a formula, called
the target concept, and that the target attribute always as-
sumes a truth-value according to the target concept. The de-
scribed setting is captured by the following deﬁnitions.

Deﬁnition 2.3 (Formula Equivalence) Formulas ϕ1 and ϕ2
over A are equivalent w.r.t. a probability distribution D if
P r (val(ϕ1 | exm) = val(ϕ2 | exm) | exm ← D) = 1.

1More generally, one can consider how a formula over attributes
can be expressed as a formula over other attributes. The approach is
similar, and our deﬁnitions and results apply largely unchanged.

IJCAI-07

969

Deﬁnition 2.4 (Supported Concepts) A concept class over
A is a set C of formulas over A. A probability distribution
D supports C for an attribute xt if xt is equivalent to some
formula c ∈ C w.r.t. D; c is the target concept for xt under D.

Supported concept classes essentially encode a known or
assumed bias on the probability distribution from which ex-
amples are drawn. This imposes constraints on the examples,
in what is perhaps the simplest possible manner that still facil-
itates learnability. Assuming such a bias, the goal of an agent
is then to identify a formula from some hypothesis class, that
is consistent with the target attribute with high probability.
Deﬁnition 2.5 (Learning Tasks) A learning task over A is a
triple (cid:7)xt, C, H(cid:8), where xt is an attribute in A, C is a concept
class over A, and H a hypothesis class of formulas over A.

We omit writing the set of attributes A over which a learn-
ing task is deﬁned, when this does not introduce ambiguities.
Deﬁnition 2.6 ((1−ε)-consistency) A hypothesis h conﬂicts
with a target attribute xt ∈ A w.r.t. an observation obs if
{val(h | obs), val(xt | obs)} = {0, 1}. A hypothesis h
is (1 − ε)-consistent with a target attribute xt ∈ A under a
probability distribution D and a masking process mask if
P r ({val(h | obs), val(xt | obs)} = {0, 1} |

exm ← D; obs ← mask(exm)) ≤ ε.
Recall that formulas might evaluate to ∗ given an observa-
tion. We interpret this value as a “don’t know” prediction, and
such a prediction is always consistent with a target attribute.
Similarly, a value of ∗ for an attribute is interpreted as a “don’t
know” sensor reading, and every prediction is consistent with
such a sensor reading. That is to say, as long as the prediction
coming through a hypothesis and the sensor reading do not
directly conﬂict by producing different {0, 1} values, there is
no inconsistency at the observational level.

It is important to note that the ability to make “don’t know”
predictions cannot be abused by an agent. Every hypothesis is
necessarily a formula, which assumes a deﬁnite {0, 1} value
whenever sufﬁciently many of its arguments are speciﬁed. It
only evaluates to ∗ given an observation, when its value on
the actual underlying example that was masked to obtain the
observation cannot be determined. Thus, our framework ac-
counts for an implicit notion of completeness, by imposing a
natural restriction on the “don’t know” predictions.
Deﬁnition 2.7 (Consistent Learnability) An algorithm L is
a consistent learner for a learning task (cid:7)xt, C, H(cid:8) over A if
for every probability distribution D supporting C for xt, every
masking process mask, every real number δ : 0 < δ ≤ 1, and
every real number ε : 0 < ε ≤ 1, the algorithm runs in time
polynomial in 1/δ, 1/ε, |A|, and the size of the target concept
for xt under D, and with probability 1−δ returns a hypothesis
h ∈ H that is (1−ε)-consistent with xt under mask(D). The
concept class C over A is consistently learnable on xt by H
if there exists a consistent learner for (cid:7)xt, C, H(cid:8) over A.

3 Consistent Learners vs. Accurate Predictors
We have taken the approach that learned hypotheses are ex-
pected to be consistent with observations, a natural general-
ization of the respective requirements of PAC learning. Such

hypotheses will correctly predict the values of non-masked at-
tributes that are artiﬁcially (for the purposes of analysis) “ob-
scured” in an observation, after the observation is drawn. In
some sense this is the best one can hope for. If an agent never
gets to observe parts of its environment, then it can only form
hypotheses that in the best case are consistent with its obser-
vations, although they might not agree with the underlying
masked examples. This is reminiscent of developing physical
theories by ﬁnding laws that are consistent with what we ob-
serve, without this implying that our current, past, or future
physical theories are actually the “correct” ones. Hypotheses
developed in this manner are, of course, used to make predic-
tions on masked attributes of the world. Humans go into great
lengths to subsequently obtain the values of such masked at-
tributes, so as to experimentally validate a physical theory.

In the context of this work we study whether developed the-
ories, or hypotheses, that are consistent with the partial obser-
vations of an agent, would actually make accurate predictions
on a hypothetical validation experiment. That is, given an ob-
servation obs masking an example exm, and an attribute xt
that is masked in obs, we wish to examine whether it is pos-
sible to predict exm[t], and thus accurately (and not simply
consistently) “ﬁll-in” the missing information in obs.
Deﬁnition 3.1 ((1 − ε)-accuracy) A hypothesis h is (1 − ε)-
accurate w.r.t. a target attribute xt ∈ A under a probability
distribution D and a masking process mask if
P r ({val(h | obs), val(xt | exm)} = {0, 1} |

exm ← D; obs ← mask(exm)) ≤ ε.
Hypotheses might still evaluate to ∗ given an observation.
Thus, the accuracy requirement amounts to asking that when-
ever a hypothesis predicts a {0, 1} value, the value should
be in accordance with the actual (rather than the observed)
value of the target attribute. Identifying the conditions under
which one can form accurate hypotheses is essential, in that
an agent’s actions yield utility based not on what the agent
observes, but based on what actually holds in the agent’s en-
vironment. The more informed the agent is about the actual
state of its environment (either through observations or accu-
rate predictions), the better decisions the agent might reach.

Clearly, predictions that are accurate are necessarily con-
sistent (since it holds that obs[t] ∈ {exm[t], ∗}). The other
direction, however, does not hold in general. Indeed, predic-
tions on masked target attributes are always consistent, while
there is no evident reason why they should also be accurate.

Theorem 3.1 (Indistinguishability in Adversarial Settings)
Consider a target attribute xt, and a concept class C over
A \ {xt}, and let ϕ1, ϕ2 ∈ C be such that ϕ1 (cid:3)= ϕ2. There
exist probability distributions D1, D2 such that: (i) ϕ1, ϕ2
are equivalent w.r.t. neither D1 nor D2, (ii) ϕ1, xt are
equivalent w.r.t. D1, and (iii) ϕ2, xt are equivalent w.r.t.
D2. There also exists a masking process mask such that
mask(D1) = mask(D2), and no attribute in A \ {xt} is
masked in any drawn observation.

Theorem 3.1 shows that examples might be masked in such
a way so that two non-equivalent concepts are indistinguish-
able given a set of observations. In fact, it sufﬁces to only
mask the target attribute in a few (but adversarially selected)

IJCAI-07

970

cases for the result to go through. The non-masked attributes
are also adversarially selected so that observations will imply
a {0, 1} value for all formulas over A \ {xt}, excluding the
possibility of a “don’t know” prediction. Clearly, an agent has
no means of identifying which of the probability distributions
D1, D2 examples are drawn from, or equivalently, which of
the formulas ϕ1, ϕ2 is the target concept for xt. Thus, it is im-
possible for the agent to conﬁdently return a hypothesis that
is highly accurate w.r.t. xt under mask(D1) = mask(D2);
either conﬁdence or accuracy is necessarily compromised.

We note that the indistinguishability result imposes very
mild restrictions on the probability distributions D1, D2, and
the concept class C, which implies that an adversarial choice
of the masking process mask can “almost always” prove dis-
astrous for an algorithm attempting to make accurate predic-
tions, even if the algorithm is computationally unbounded,
the known bias on the probability distribution is as strong as
possible (i.e., the concept class is of cardinality two), and the
hypothesis class comprises of all formulas over A \ {xt}.

The established impossibility result suggests that having an
infrequently masked target attribute does not sufﬁce to learn
accurately; it is important to have an infrequently masked tar-
get attribute in the right context. We formalize this next.
Deﬁnition 3.2 ((1 − η)-concealment) A masking process
mask is (1 − η)-concealing for a learning task (cid:7)xt, C, H(cid:8)
if η is the maximum value such that for every example exm,
and every hypothesis h ∈ H
P r (val(xt | obs) (cid:3)= ∗ | obs ← mask(exm);

{val(h | obs), val(xt | exm)} = {0, 1}) ≥ η.

Roughly speaking, Deﬁnition 3.2 asks that whenever a hy-
pothesis is inaccurate, the agent will observe evidence of this
fact with some probability. This generalizes the case of PAC
learning, where an inaccurate hypothesis is always observed
to conﬂict with the target attribute (which is never masked).
We note that the masking process mask whose existence is
guaranteed by Theorem 3.1 is necessarily 1-concealing for
every learning task (cid:7)xt, C, H(cid:8) with a non-trivial concept class.

Theorem 3.2 (The Relation of Consistency and Accuracy)
Consider a learning task (cid:7)xt, C, H(cid:8), and a masking process
mask that is (1 − η)-concealing for (cid:7)xt, C, H(cid:8). Then, (i)
for every probability distribution D and hypothesis h ∈ H,
h is (1 − ε/η)-accurate w.r.t. xt under mask(D) if h is
(1 − ε)-consistent with xt under mask(D), and (ii) there
exists a probability distribution D0 and a hypothesis h0 ∈ H
such that h0 is (1 − ε/η)-accurate w.r.t. xt under mask(D0)
only if h0 is (1 − ε)-consistent with xt under mask(D0).

Assuming that our physical world does not adversarially
hide information from us, one can interpret the above result
as a partial explanation of how it is possible for humans to
learn rules, and construct physical theories, that make accu-
rate predictions in situations where nothing is known, despite
the fact that learning takes place and is evaluated mostly on
observations with inherently missing information.

Similarly to the case of constructing learners for noisy ex-
amples [Kearns, 1998], we assume that an algorithm is given
a bound on the concealment degree of the masking process
and allowed time that depends on this bound during learning.

Deﬁnition 3.3 (Accurate Predictability) An algorithm L is
an accurate predictor for a learning task (cid:7)xt, C, H(cid:8) over A if
for every probability distribution D supporting C for xt, every
real number η : 0 < η ≤ 1, every masking process mask
that is (1 − η)-concealing for (cid:7)xt, C, H(cid:8), every real number
δ : 0 < δ ≤ 1, and every real number ε : 0 < ε ≤ 1, the
algorithm runs in time polynomial in 1/η, 1/δ, 1/ε, |A|, and
the size of the target concept for xt under D, and with proba-
bility 1−δ returns a hypothesis h ∈ H that is (1−ε)-accurate
w.r.t. xt under mask(D). The concept class C over A is ac-
curately predictable on xt by H if there exists an accurate
predictor for (cid:7)xt, C, H(cid:8) over A.

It is now straightforward to show the following.

Theorem 3.3 (Consistent Learners / Accurate Predictors)
Consider a learning task (cid:7)xt, C, H(cid:8), and a masking process
mask that is (1 − η)-concealing for (cid:7)xt, C, H(cid:8). If algorithm
L is a consistent learner for (cid:7)xt, C, H(cid:8), then algorithm L
given η as extra input and allowed running time that grows
polynomially in 1/η, is an accurate predictor for (cid:7)xt, C, H(cid:8).

We have thus established not only that consistent learning
implies accurate predicting, but that the same algorithm can
be used, with the only provision that the algorithm will be
allowed more running time to achieve the same precision as
determined by ε. The running time dependence on η can be
eliminated if the following are true: (i) the consistent learner
is such that it only uses the observations that do not mask the
target attribute, and (ii) the induced predictor has access to an
oracle that returns observations from distribution mask(D),
conditioned, however, on the observations not masking the
target attribute. The use of such an oracle exempliﬁes the fact
that a predictor does not require more computation to produce
an accurate hypothesis, but rather more observations in order
to obtain enough “labelled instances” of the target concept.

A rather intriguing implication of our results is that a con-
sistent learner is, without any knowledge of the concealment
degree of the masking process, also able to predict accurately,
albeit with a “discounted” accuracy factor. In fact, as condi-
tion (ii) of Theorem 3.2 suggests, a consistent learner is, in
some sense, as accurate a predictor as possible. Given this
result, it sufﬁces to restrict our attention to consistent learning
for the rest of our study on learning from partial observations.

4 Consistently Learnable Concept Classes
The stronger learnability requirements we impose compared
to PAC learning do not render learnability impossible. It is
an easy exercise to show that the typical algorithm for PAC
learning conjunctions [Valiant, 1984] and its analysis can be
applied essentially unmodiﬁed on partial observations.
Theorem 4.1 The concept class C of conjunctions of literals
over A \ {xt} is properly consistently learnable on xt.

4.1 One-To-Many Reductions
Reductions between learning tasks are often used to establish
that certain concept classes are or are not learnable. Standard
reductions map examples from one learning task to examples
of a different learning task. In our case such reductions map,
in general, partial observations to partial observations.

IJCAI-07

971

t , C j, Hj(cid:8) over Aj }r−1

Deﬁnition 4.1 (Reductions) The learning task (cid:7)xt, C, H(cid:8)
j
over A is reducible to the set {(cid:7)x
j=0 of
learning tasks, where r ∈ N is polynomially-bounded by |A|,
if there exists an efﬁciently computable hypothesis mapping
g : H0 × . . . × Hr−1 → H, and an efﬁciently computable in-
stance mapping f j : {0, 1, ∗}|A| → {0, 1, ∗}|Aj|
for every
j ∈ {0, . . . , r − 1}, such that the following conditions hold:
(i) for every tuple h ∈ H0 × . . . × Hr−1

and every obser-
vation obs ∈ {0, 1, ∗}|A|
, it holds that g(h) conﬂicts
with xt w.r.t. obs only if there exists j ∈ {0, . . . , r − 1}
j
such that hj = h[j] conﬂicts with x
t w.r.t. f j(obs);

(ii) the probability distribution mask(D) from which obs is
drawn is such that D supports C for xt only if for every
j ∈ {0, . . . , r − 1} there exists an induced probability
j(Dj ) from which f j(obs) is drawn
distribution mask
j
supports C j
such that Dj
for x
t , and the size of the target
j
is polynomially-bounded by |A|
t under Dj
concept for x
and the size of the target concept for xt under D.

Roughly, the two conditions guarantee that (i) learned hy-
potheses can be meaningfully employed in the original task,
and that (ii) observations in the resulting tasks can be obtained
by masking examples drawn from appropriate distributions.

Theorem 4.2 (Learning through Reductions) Consider a
learning task (cid:7)xt, C, H(cid:8) over A that is reducible to the set
j
of learning tasks {(cid:7)x
j=0. The concept
class C is consistently learnable on xt by H if for every
j ∈ {0, . . . , r − 1}, the concept class C j
is consistently learn-
j
t by Hj
able on x

t , C j, Hj(cid:8) over Aj }r−1

.

The following special case is of particular interest, in that

observations in the resulting learning tasks are complete.

Deﬁnition 4.2 (Total Reductions) A reduction is total if for
every j ∈ {0, . . . , r − 1}, f j : {0, 1, ∗}|A| → {0, 1}|Aj|

.

4.2 Shallow-Monotone Formulas

We establish a reduction between certain classes of formulas.

Deﬁnition 4.3 (Shallow-Monotonicity) A formula ϕ is
shallow-monotone w.r.t. a set M of substitutions if the pro-
cess of substituting an attribute x(cid:2)
i(ψ) for every sub-formula ψ
of ϕ such that x(cid:2)
i(ψ)/ψ ∈ M, produces a monotone formula;
denote by basis(ϕ | M) the resulting (monotone) formula. A
set F of formulas is shallow-monotone w.r.t. a set M of sub-
stitutions if every formula ϕ ∈ F is shallow-monotone w.r.t.
M; we deﬁne basis(F | M) (cid:2) {basis(ϕ | M) | ϕ ∈ F }.

We implicitly assume that the substitution process replaces
distinct new attributes for distinct sub-formulas of ϕ, and that
the resulting formula is entirely over these new attributes.

Clearly, every set F of formulas is shallow-monotone w.r.t.
some set M of substitutions. The emphasis of Deﬁnition 4.3
is on the choice of M, and the corresponding basis of F w.r.t.
M. Note, for instance, that the class of k-CNF formulas for
some constant k ∈ N has a basis that comprises of conjunc-
tions, and this basis is w.r.t. a set of substitutions that is only
polynomially large in the number of attributes over which the

k-CNF formulas are deﬁned (since exactly one substitution is
required for each of the polynomially many possible clauses).

t, C(cid:2), H(cid:2)(cid:8) over A(cid:2)

Theorem 4.3 (Reduction to Monotone Classes) The learn-
ing task (cid:7)xt, C, H(cid:8) over A is reducible to the learning task
(cid:7)x(cid:2)
if there exists a set M of substitutions
such that: (i) M is computable in time polynomial in |A|,
(ii) every sub-formula substituted under M can be evaluated
given an observation in time polynomial in |A|, and (iii) x(cid:2)
t=
basis(xt | M), C(cid:2)=basis(C | M), and H(cid:2)=basis(H | M).
An immediate corollary of Theorems 4.1 and 4.3 is that
the concept class of k-CNF formulas is properly consistently
learnable for every constant k ∈ N.

4.3 Learning Monotone Formulas
We employ reductions to establish certain learnability results.

learning

t, C(cid:2), H(cid:2)(cid:8) over A(cid:2)

such that A(cid:2) = A, x(cid:2)

Theorem 4.4 (Total Self-Reduction) The
task
(cid:7)xt, C, H(cid:8) over A is total reducible to the learning task
(cid:7)x(cid:2)
t = xt, C(cid:2) = C,
H(cid:2) = H, and g(·) is the identity mapping, if C and H are
classes of monotone formulas over A \ {xt}, and (cid:11) (cid:3)∈ C.2
Proof Idea: By monotonicity, any formula in {xt} ∪ C ∪ H
that assumes a {0, 1} value given obs retains its value when
masked attributes are mapped to val(xt | obs) ∈ {0, 1}. (cid:3)

Theorem 4.4 establishes a rather surprising fact for mono-
tone formulas: consistently learning from partial observations
reduces to consistently learning the same concept class from
complete observations. Equally intriguing is the fact that hy-
potheses learned (from complete observations) for the result-
ing task, apply unmodiﬁed for making predictions (on partial
observations) in the original task. The preceding facts nicely
complement Theorem 3.2, which establishes that consistently
learned hypotheses are also as accurate as possible. Together,
Theorems 3.2 and 4.4 imply that a concrete strategy to predict
accurately on partial observations is to simply assign appro-
priate default truth-values to masked attributes, consistently
learn from the resulting complete observations, and then em-
ploy the learned hypothesis unchanged to make predictions.

A technical point worth discussing here is the encoding of
the value of the target attribute in certain attributes of the re-
sulting task. Observe that an agent learning in the resulting
task, although agnostic to this fact, uses the “label” of the ex-
ample in a much more involved manner than its standard use
as a means to test the predictions of a hypothesis. What makes
the result established by the reduction non-trivial, is the fact
that the hypothesis does not depend on the target attribute in
the context of the original task. In some sense, we allow an
agent to use an example’s “label” in an involved manner when
learning, but require that the hypothesis eventually employed
for making predictions does not depend on the target attribute.
The next corollary follows from Theorems 4.3 and 4.4, and

the PAC learnability of certain monotone concept classes.
Corollary 4.5 Each concept class C ∈ {conjunctions, dis-
junctions, k-CNF, k-DNF, linear thresholds} of literals over
A \ {xt} is properly consistently learnable on xt.

2Assuming (cid:2) (cid:3)∈ C is without loss of generality, since sampling
can be used to determine w.h.p. whether the target concept is a tau-
tology, and the reduction can be used only when this is not the case.

IJCAI-07

972

The result holds, more generally, for any class of monotone
formulas PAC learnable by some class of monotone formulas.

5 Negative Results in Consistent Learning

Consistent learnability is at least as strong as PAC learnabil-
ity, as it requires learning under arbitrary masking processes,
including the trivial identity masking process. This observa-
tion provides a basic upper bound on consistent learnability.
Theorem 5.1 A concept class C over A \ {xt} is consistently
learnable on the target attribute xt by a hypothesis class H
over A \ {xt} only if C is PAC learnable by H.

Theorem 5.1 implies that any concept class known not to
be PAC learnable (possibly under some assumptions), is also
not consistently learnable (under the same assumptions).

We continue to present negative results on the consistent
learnability of certain speciﬁc concept classes. For the rest of
this section we only require that partial observations have at
most three masked attributes. This suggests that the property
of masking that compromises consistent learnability is not the
frequency of the masked attributes, but rather the context in
which they appear. Recall that Theorem 3.1 establishes that a
similar property also compromises accurate predictability.

Intractability of Learning Parities

5.1
Our results so far leave open the possibility that every concept
class PAC learnable by some hypothesis class is also consis-
tently learnable from partial observations by the same hypoth-
esis class. We dismiss this possibility by showing the concept
class of parities, known to be properly PAC learnable [Helm-
bold et al., 1992], not to be properly consistently learnable
from partial observations, unless RP = NP.
Theorem 5.2 The concept class C of parities over A \ {xt}
is not properly consistently learnable on xt, unless RP = NP.

The proof of Theorem 5.2 follows similar proofs from the
literature (see, e.g., [Pitt and Valiant, 1988]). The reduction
is from 3-SAT, and it relies on constructing observations that
in order to be explained consistently, require the learned hy-
pothesis to depend on any non-empty subset of the masked at-
tributes, without, however, the observations specifying which
such subset is to be chosen. It is the case that with complete
observations one can still force the learned hypothesis to de-
pend on certain attributes, but the possible dependencies are
necessarily restricted in a subtle, yet critical, manner.

Intractability of Learning Decision Lists

5.2
Rivest [1987] showed that the concept class of k-decision lists
for a constant k ∈ N is properly PAC learnable, by showing
how to identify a hypothesis that agrees with a given set of
examples, and employing an Occam’s Razor type argument
[Blumer et al., 1987]. He asked whether the same can be done
when instead of examples one considers partial observations.
In our notation, he deﬁned agreement3 of a formula ϕ with an
observation obs to mean val(ϕ | obs) = val(xt | obs),

3Rivest [1987] used the term “consistency” rather than “agree-
ment”. We avoid, however, using the term “consistency” in this con-
text, as it has a different meaning in our framework.

where xt is the target attribute, which he assumed to be non-
masked. As posed, the question almost always admits a trivial
negative answer: an observation obs generally masks a set of
examples such that the value of ϕ varies across examples, im-
plying val(ϕ | obs) = ∗, and making ϕ disagree with obs.
We recast the notion of “agreement” to what, we believe,
is a more appropriate (and possibly the intended) form: a for-
mula ϕ agrees with an observation obs if ϕ does not con-
ﬂict with the target attribute xt under obs. This weaker
notion of “agreement” only requires that val(ϕ | obs) =
val(xt | obs) when val(ϕ | obs), val(xt | obs) ∈ {0, 1}.
We partially answer this new question in the negative, by
showing the concept class of monotone term 1-decision lists
not to be properly consistently learnable from partial observa-
tions, unless RP = NP. The negative answer carries to Rivest’s
original question, due to his stronger notion of “agreement”.
Theorem 5.3 The concept class C of monotone term 1-
decision lists over A \ {xt} is not properly consistently learn-
able on xt, unless RP = NP.

Theorem 5.3 establishes a separation from the framework
of learning in the presence of random classiﬁcation noise, in
which the concept class of k-decision lists is known to be
properly learnable for every constant k ∈ N [Kearns, 1998].

6 Related Work
Valiant [1984] recognizes early on the problem of missing
information in observations, and proposes a model in which
the target attribute is positive exactly when the target con-
cept is positive in all examples masked by the observation.
Some subsequent work follows a similarly-ﬂavored approach
in making certain assumptions regarding the value of the tar-
get attribute. Schuurmans and Greiner [1994] employ a mask-
ing process closely related to ours, although they assume that
the target attribute is never masked. Their goal is also dif-
ferent, in that they focus on learning default concepts from
partial observations, and not on the problem of recovering
missing information. Goldman et al. [1997] consider a model
in which the target attribute is positive/negative exactly when
the target concept is correspondingly so in all the examples
masked by the observation; the target attribute is masked only
when the value of the target concept cannot be deduced from
the non-masked attributes. Thus, they effectively treat “don’t
know” as a third distinguished value, and the problem reduces
to that of learning ternary functions in what is essentially a
complete information setting. In contrast, we focus on learn-
ing boolean formulas from partial observations without mak-
ing such assumptions: the target attribute is masked arbitrar-
ily, and when non-masked it simply indicates the value of the
target concept for some example masked by the observation.
Decatur and Gennaro [1995] assume that attributes are
masked independently, a crucial prerequisite for their sta-
tistical learning approach to work. We, on the other hand,
consider the general setting where attributes are masked arbi-
trarily, staying thus closer to the spirit of the PAC semantics;
arbitrary probability distributions model the unknown depen-
dencies between properties of the environment, while arbi-
trary masking processes model the unknown dependencies on
what is observable within the environment.

IJCAI-07

973

Multiple Instance Learning bears some resemblance to our
work (see, e.g., [Dietterich et al., 1997]). In that setting, a
partial observation is an arbitrary bag of examples (usually
assumed to be drawn independently), and an observation is
positive exactly when at least one example is positive. Our
partial observations can be seen as structured bags of (not
independently drawn) examples. The structure restricts the
possible combinations of examples, while the bag “labels”
are much less informative, since they can assume either truth-
value when bags contain both positive and negative examples.
Our masking process resembles the models of random at-
tribute [Shackelford and Volper, 1988] and classiﬁcation [An-
gluin and Laird, 1987] noise, where the values of attributes
are affected, although independently across different exam-
ples, before being observed by an agent. Our model is per-
haps closer to that of malicious noise [Valiant, 1985], in that
no assumption is made as to how attributes are affected across
examples. Malicious noise is known to render learnability
almost impossible [Kearns and Li, 1993], while our more
benign model, where affected attributes are explicitly made
known to an agent, does not severely impair learnability.

7 Conclusions and Open Problems

We have presented an autodidactic learning framework as a
means to recover missing information in data. Our frame-
work builds on two natural generalizations of Valiant’s PAC
model [Valiant, 1984] to address the problem of learning from
partial observations: learning consistently and predicting ac-
curately. Producing accurate hypotheses was shown to be im-
possible under certain masking processes, even for computa-
tionally unbounded learners. On the positive side, producing
consistent hypotheses was shown to be a concrete strategy for
predicting as accurately as permitted by the masking process.
Within our framework we have presented a reduction tech-
nique, through which a number of natural boolean concept
classes were shown to be consistently learnable. On the other
hand, we have shown that properly consistently learning cer-
tain other concept classes is intractable, establishing, thus,
a separation from the models of learning from complete or
noisy observations. It remains open whether the intractability
results are only representation-speciﬁc, and whether consis-
tently learning from partial observations is (strictly) harder
than learning under the random classiﬁcation noise model.

A more general question concerns the consistent learnabil-
ity of concept classes that are not shallow-monotone w.r.t.
some efﬁciently computable set of substitutions. It is unclear
whether such learnability results can be obtained through to-
tal reductions, which work for monotone concept classes.

Acknowledgments

The author is grateful to Leslie Valiant for his advice, and for
valuable suggestions and remarks on this research.

References

[Angluin and Laird, 1987] Dana Angluin and Philip D.
Laird. Learning from noisy examples. Machine Learn-
ing, 2(4):343–370, 1987.

[Blumer et al., 1987] Anselm Blumer, Andrzej Ehrenfeucht,
David Haussler, and Manfred K. Warmuth. Occam’s razor.
Information Processing Letters, 24:377–380, 1987.

[Decatur and Gennaro, 1995] Scott E. Decatur and Rosario
Gennaro. On learning from noisy and incomplete ex-
amples. In Eighth Annual Conference on Computational
Learning Theory, pages 353–360, 1995.

[Dietterich et al., 1997] Thomas G. Dietterich, Richard H.
Lathrop, and Tom´as Lozano-P´erez. Solving the multiple
instance problem with axis-parallel rectangles. Artiﬁcial
Intelligence, 89(1–2):31–71, 1997.

[Goldman et al., 1997] Sally A. Goldman, Stephen Kwek,
and Stephen D. Scott. Learning from examples with un-
speciﬁed attribute values (extended abstract).
In Tenth
Annual Conference on Computational Learning Theory,
pages 231–242, 1997.

[Helmbold et al., 1992] David Helmbold, Robert Sloan, and
Manfred K. Warmuth. Learning integer lattices. SIAM
Journal on Computing, 21(2):240–266, 1992.

[Kearns and Li, 1993] Michael J. Kearns and Ming Li.
Learning in the presence of malicious errors. SIAM Jour-
nal on Computing, 22(4):807–837, 1993.

[Kearns and Vazirani, 1994] Michael

and
Umesh V. Vazirani.
An Introduction to Computa-
tional Learning Theory. The MIT Press, Cambridge,
Massachusetts, U.S.A., 1994.

J.

Kearns

[Kearns, 1998] Michael J. Kearns. Efﬁcient noise-tolerant
Journal of the ACM,

learning from statistical queries.
45(6):983–1006, 1998.

[Pitt and Valiant, 1988] Leonard Pitt and Leslie G. Valiant.
Computational limitations on learning from examples.
Journal of the ACM, 35(4):965–984, 1988.

[Rivest, 1987] Ronald L. Rivest. Learning decision lists. Ma-

chine Learning, 2(3):229–246, 1987.

[Schafer and Graham, 2002] Joseph L. Schafer and John W.
Graham. Missing data: Our view of the state of the art.
Journal of Psychological Methods, 7(2):147–177, 2002.

[Schuurmans and Greiner, 1994] Dale Schuurmans and Rus-
sell Greiner. Learning default concepts. In Tenth Canadian
Conference on Artiﬁcial Intelligence, pages 99–106, 1994.

[Shackelford and Volper, 1988] George Shackelford

and
Learning k-DNF with noise in the
In First Annual Workshop on Computational

Dennis Volper.
attributes.
Learning Theory, pages 97–103, 1988.

[Valiant, 1984] Leslie G. Valiant. A theory of the learnable.

Communications of the ACM, 27:1134–1142, 1984.

[Valiant, 1985] Leslie G. Valiant. Learning disjunctions of
conjunctions. In Ninth International Joint Conference on
Artiﬁcial Intelligence, Vol. 1, pages 560–566, 1985.

[Valiant, 2000] Leslie G. Valiant. Robust logics. Artiﬁcial

Intelligence, 117(2):231–253, 2000.

IJCAI-07

974


Solving POMDPs Using Quadratically Constrained Linear Programs

Christopher Amato and Daniel S. Bernstein and Shlomo Zilberstein

Department of Computer Science

University of Massachusetts

Amherst, MA 01003

{camato,bern,shlomo}@cs.umass.edu

Abstract

Developing scalable algorithms for solving par-
tially observable Markov decision processes
(POMDPs) is an important challenge. One ap-
proach that effectively addresses the intractable
memory requirements of POMDP algorithms is
based on representing POMDP policies as ﬁnite-
state controllers. In this paper, we illustrate some
fundamental disadvantages of existing techniques
that use controllers. We then propose a new
approach that formulates the problem as a quadrat-
ically constrained linear program (QCLP), which
deﬁnes an optimal controller of a desired size. This
representation allows a wide range of powerful
nonlinear programming algorithms to be used to
solve POMDPs. Although QCLP optimization
techniques guarantee only local optimality,
the
results we obtain using an existing optimization
method show signiﬁcant solution improvement
over the state-of-the-art techniques. The results
open up promising research directions for solving
large POMDPs using nonlinear programming
methods.

Introduction

1
Since the early 1990s, Markov decision processes (MDPs)
and their partially observable counterparts (POMDPs) have
been widely used by the AI community for planning under
uncertainty. POMDPs offer a rich language to describe sit-
uations involving uncertainty about the domain, stochastic
actions, noisy observations, and a variety of possible objec-
tive functions. POMDP applications include robot control
[Simmons and Koenig, 1995], medical diagnosis [Hauskrecht
and Fraser, 1998] and machine maintenance [Eckles, 1968].
Robots typically have sensors that provide uncertain and in-
complete information about the state of the environment,
which must be factored into the planning process. In a medi-
cal setting, the internal state of the patient is often not known
with certainty. The machine maintenance problem, one of the
earliest application areas of POMDPs, seeks to ﬁnd a cost-
effective strategy for inspection and replacement of parts in
a domain where partial information about the internal state is

obtained by inspecting the manufactured products. Numer-
ous other POMDP applications are surveyed in [Cassandra,
1998b].

Developing effective algorithms for MDPs and POMDPs
has become a thriving AI research area [Cassandra, 1998a;
Feng and Hansen, 2002; Feng and Zilberstein, 2005; Hansen,
1998; Littman et al., 1995; Meuleau et al., 1999; Poupart and
Boutilier, 2003]. Thanks to these new algorithms and im-
provements in computing power, it is now possible to solve
very large and realistic MDPs. In contrast, current POMDP
exact techniques are limited by high memory requirements to
toy problems.

POMDP exact and approximate solution techniques cannot
usually ﬁnd near-optimal solutions with a limited amount of
memory. Even though an optimal solution may be concise,
current exact algorithms that use dynamic programming of-
ten require an intractable amount of space. While work has
been done to make this process more efﬁcient [Feng and Zil-
berstein, 2005], the time and space complexities of POMDP
algorithms remain serious challenges. POMDP approxima-
tion algorithms can operate with a limited amount of mem-
ory, but as a consequence they provide very weak theoretical
guarantees, if any. In contrast, we describe a new approach
that bounds space usage while permitting a principled search
based on the optimal solution.

Current techniques used to ﬁnd optimal ﬁxed-size con-
trollers rely solely on local information [Meuleau et al., 1999;
Poupart and Boutilier, 2003]. We present a formulation of
an optimal ﬁxed-size controller which represents an optimal
POMDP policy of a given controller size. To illustrate some
of its beneﬁts, we employ a standard nonlinearly constrained
optimization technique. Nonlinearly constrained optimiza-
tion is an active ﬁeld of research that has produced a wide
range of techniques that can quickly solve a variety of large
problems [Bertsekas, 2004]. The quadratic constraints and
linear objective function of our formulation belong to a spe-
cial class of optimization problems for which many robust
and efﬁcient algorithms have been developed. While these
techniques only guarantee locally optimal solutions, our new
formulation may facilitate a more efﬁcient search of the so-
lution space and produces high-quality results. Moreover,
the optimization algorithms employ more advanced tech-
niques than those previously used [Meuleau et al., 1999;
Poupart and Boutilier, 2003] to avoid convergence at certain

IJCAI-07

2418

suboptimal points.

The rest of the paper is organized as follows. We ﬁrst give
an overview of the POMDP model and explain how a solu-
tion can be represented as a stochastic controller. We brieﬂy
discuss some of the previous work on solving POMDPs using
these controllers and then show how to represent an optimal
controller using a quadratically constrained linear program
(QCLP). We conclude by demonstrating for a set of large
POMDPs that our formulation permits higher valued ﬁxed-
size controllers to be found than those generated by previ-
ous approaches while maintaining similar running times. Our
results suggest that by using our QCLP formulation, small,
high-valued controllers can be efﬁciently found for a large
assortment of POMDPs.

2 Background
A POMDP can be deﬁned with the following tuple:

tribution b0

M = (cid:2)S, A, P, R, Ω, O(cid:3), with
• S, a ﬁnite set of states with designated initial state dis-
• A, a ﬁnite set of actions
• P , the state transition model: P (s(cid:2)|s, a) is the transition
• R, the reward model: R(s, a) is the expected immediate
• Ω, a ﬁnite set of observations
• O, the observation model: O(o|s(cid:2), a) is the probability
of observing o if action a is taken, resulting in state s(cid:2)

probability of state s(cid:2) if action a is taken in state s

reward for taking action a in state s

We consider the case in which the decision making process
unfolds over an inﬁnite sequence of stages. At each stage the
agent selects an action, which yields an immediate reward,
and receives an observation. The agent must choose an action
based on the history of observations seen. Note that because
the state is not directly observed, it may be beneﬁcial for the
agent to remember the observation history. The objective of
the agent is to maximize the expected discounted sum of re-
wards received. Because we consider the inﬁnite sequence
problem, we use a discount, 0 ≤ γ < 1, to maintain ﬁnite
sums.

Finite-state controllers can be used as an elegant way of
representing POMDP policies using a ﬁnite amount of mem-
ory. The state of the controller is based on the observation se-
quence, and in turn the agent’s actions are based on the state
of its controller. These controllers address one of the main
reasons for intractability in POMDP exact algorithms by not
remembering whole observation histories. We also allow for
stochastic transitions and action selection, as this can help to
make up for limited memory [Singh et al., 1994]. The ﬁnite-
state controller can formally be deﬁned by the tuple (cid:2)Q, ψ, η(cid:3),
where Q is the ﬁnite set of controller nodes, ψ : Q → ΔA is
the action selection model for each node, mapping nodes to
distributions over actions, and η : Q × A × O → ΔQ repre-
sents the node transition model, mapping nodes, actions and
observations to distributions over the resulting nodes. The
value of a node q at state s, given action selection and node
transition probabilities P (a|q) and P (q(cid:2)|q, a, o), is given by:

(cid:2)

For a given node q and variables xa and xq(cid:2),a,o
Maximize , for
(cid:3)
∀s V (q, s) +  ≤ (cid:2)
Improvement constraints:
xaR(s, a)+
(cid:2)
(cid:2)
s(cid:2) P (s(cid:2)|s, a)
o O(o|s(cid:2), a)
(cid:2)
(cid:2)
Probability constraints:
a xa = 1 and ∀a
∀a xa ≥ 0 and ∀q(cid:2), a, o xq(cid:2),a,o ≥ 0

q(cid:2) xq(cid:2),a,o = xa

γ

a

(cid:4)
q(cid:2) xq(cid:2),a,oV (q(cid:2), s(cid:2))

Table 1: The linear program for BPI. Variables xa and xq(cid:2),a,o
represent P (a|q) and P (q(cid:2), a|q, o) for a given node, q.

V (q, s) =

(cid:2)

γ

s(cid:2) P (s(cid:2)|s, a)

(cid:2)

(cid:3)
a P (a|q)
R(s, a)+
(cid:2)
o O(o|s(cid:2), a)

(cid:2)

(cid:4)
q(cid:2) P (q(cid:2)|q, a, o)V (q(cid:2), s(cid:2))

This equation is referred to as the Bellman equation.

2.1 Previous work
While many POMDP approximation algorithms have been
developed, we will only discuss controller-based approaches,
as that is the focus of our work. These techniques seek to
determine the best action selection and node transition pa-
rameters for a ﬁxed-size controller.

Poupart and Boutilier [2003] have developed a method
called bounded policy iteration (BPI) that uses a one step
dynamic programming lookahead to attempt to improve a
POMDP controller without increasing its size. This approach
alternates between policy improvement and evaluation. It it-
erates through the nodes in the controller and uses a linear
program, shown in Table 1, to examine the value of proba-
bilistically taking an action and then transitioning into the old
controller. If an improvement can be found for all states, the
action selection and node transition probabilities are updated
accordingly. The controller is then evaluated and the cycle
continues until no further improvement can be found. BPI
guarantees to at least maintain the value of a provided con-
troller, but it is not likely to ﬁnd a concise optimal controller.
A heuristic has also been proposed for incorporating start
state knowledge and increasing the performance of BPI in
practice [Poupart and Boutilier, 2004].
In this extension,
termed biased BPI, improvement is concentrated in certain
node and state pairs by weighing each pair by the (unnormal-
ized) occupancy distribution, which can be found by solving
the set of linear equations:
o(q(cid:2), s(cid:2)) = bp0(q(cid:2), s(cid:2))+

q,s,a,o o(q, s)P (s(cid:2)|s, a)P (a|q)O(o|s(cid:2), a)P (q(cid:2)|q, a, o)
for all states and nodes. The variables used are those previ-
ously deﬁned and the probability, bp0, of beginning in a node
state pair. A factor, δ, can also be included, which allows
value to decrease by that amount in each node and state pair.
This makes changes to the parameters more likely, as a small
amount of value can now be lost in note state pairs. As a re-
sult, value may be higher for the start state and node, but it

(cid:2)

γ

IJCAI-07

2419

is taken, rather than calculating the true value of updating
action and transition probabilities. Since BPI requires that
there is a distribution over nodes that increases value for all
states, it will not make any improvements. Biased BPI sets
equal weights for each state, thus preventing improvement.
Allowing value to be lost by using a predetermined δ does
not guarantee controller improvement, and for many chosen
values quality may instead decrease.

Likewise, the gradient calculation in GA will have dif-
ﬁculty ﬁnding an optimal controller. Because Meuleau et
al. formulate the problem as unconstrained, some heuristic
must adjust the gradient to ensure proper probabilities are
maintained. For the example problem, some heuristics will
improve the controller, while others remain stuck. In general,
no heuristic can guarantee ﬁnding the globally optimal solu-
tion. Essentially, this controller represents a local maximum
for both of these methods, causing suboptimal behavior. One
premise of our work is that a more general formulation of the
problem, which deﬁnes an optimal controller, facilitates the
design of solution techniques that can overcome the above
limitation and produce better stochastic controllers. While no
existing technique guarantees global optimality, experimental
results show that our new formulation is advantageous.

In general, the linear program used by BPI may allow for
controller improvement, but can easily get stuck in local max-
ima. While the authors suggest heuristics for becoming un-
stuck, our nonlinear approach offers some advantages. Using
a single step or even a multiple step backup to improve a con-
troller will generally not allow an optimal controller to be
found. While one set of parameters may appear better in the
short term, only the inﬁnite lookahead deﬁned in the QCLP
representation can predict the true change in value.

GA also gets stuck often in local maxima. Meuleau et al.
must construct a cross-product MDP from the controller and
the underlying POMDP in a complex procedure to calculate
the gradient. Also, their representation does not take into ac-
count the probability constraints and thus does not calculate
the true gradient of the problem. Techniques more advanced
than gradient ascent may be used to traverse the gradient, but
these shortcomings remain.

3 Optimal ﬁxed-size controllers
Unlike BPI and GA, our formulation deﬁnes an optimal con-
troller for a given size. This is done by creating a set of vari-
ables that represents the values of each node and state pair.
Intuitively, this allows changes in the controller probabilities
to be reﬂected in the values of the nodes of the controller. To
ensure that these values are correct given the action selection
and node transition probabilities, quadratic constraints (the
Bellman equations for each node and state) must be added.
This results in a quadratically constrained linear program. Al-
though it is often difﬁcult to solve a QCLP exactly, many
robust and efﬁcient algorithms can be applied. Our QCLP
has a simple gradient calculation and an intuitive represen-
tation that matches well with common optimization models.
The more sophisticated optimization techniques used to solve
QCLPs may require more resources, but commonly produce
much better results than simpler methods.

Figure 1: Simple POMDP for which BPI and GA fail to ﬁnd
an optimal controller

could instead decrease value for that pair. While these heuris-
tics may increase performance, they are difﬁcult to adjust and
not applicable in all domains.

Meuleau et al. [1999] have proposed another approach to
improve a ﬁxed-size controller. The authors use gradient as-
cent (GA) to change the action selection and node transition
probabilities and increase value. A cross-product MDP is cre-
ated from the controller and the POMDP by considering the
states of the MDP to be all combinations of the states of the
POMDP and the nodes of the controller and actions of the
MDP to be based on actions of the POMDP and determin-
istic transitions in the controller after an observation is seen.
The value of the resulting MDP can be determined and ma-
trix operations allow the gradient to be calculated. Unfortu-
nately, this calculation does not preserve the parameters as
probability distributions. This further complicates the search
space and is less likely to result in a globally optimal solu-
tion. The gradient can be followed in an attempt to improve
the controller, but due to the complex and incomplete gradi-
ent calculation, this method can be time consuming and error
prone.

2.2 Disadvantages of BPI and GA
Both BPI and GA fail to ﬁnd an optimal controller for
very simple POMDPs. This can be seen with the two state
POMDP with two actions and one observation in Figure 1.
The transitions are deterministic, with the state alternating
when action A1 is taken in state 1 or action A2 is taken in
state 2. When the state changes, a positive reward is given.
Otherwise, a negative reward is given. Since there are no in-
formative observations, given only a single node and an initial
state distribution of being in either state with equal likelihood,
the best policy is to choose either action with equal probabil-
ity. This can be modeled by a one node stochastic controller
with value equal to 0. Notice that this example also shows
that with limited memory (one node), a stochastic controller
can provide arbitrarily greater total discounted reward than
any deterministic controller of that same size.

If the initial controller is deterministic and chooses either
action, say A1, BPI will not converge to an optimal controller.
The value of the initial controller in state 1 is R− γR/(1− γ)
and −R/(1 − γ) in state 2. For γ > 0.5, which is common,
value is negative in each state. Based on a one step lookahead,
assigning any probability to the other action, A2, will raise
the value for state 2, but lower it for state 1. This is because
the node is assumed to have the same value after a new action

IJCAI-07

2420

For variables: x(q(cid:2), a, q, o) and y(q, s)
Maximize

(cid:5)

b0(s)y(q0, s)

Given the Bellman constraints:
⎡
⎛
⎝(cid:5)
⎣

∀q, s y(q, s) =

(cid:5)

s

⎞
⎠ R(s, a) + γ
x(q(cid:2), a, q, o)

a

q(cid:2)

And probability constraints:

∀q, o, a

(cid:5)

q(cid:2),a

∀q, o
(cid:5)

(cid:5)

s(cid:2)

P (s(cid:2)|s, a)

(cid:5)

o

O(o|s(cid:2), a)

(cid:5)

q(cid:2)

⎤
⎦
x(q(cid:2), a, q, o)y(q(cid:2), s(cid:2))

x(q(cid:2), a, q, o) = 1

(cid:5)

x(q(cid:2), a, q, o) =

x(q(cid:2), a, q, ok)

q(cid:2)
∀q(cid:2), a, q, o x(q(cid:2), a, q, o) ≥ 0

q(cid:2)

Table 2: The QCLP deﬁning an optimal ﬁxed-size controller. Variable x(q(cid:2), a, q, o) represents P (q(cid:2), a|q, o), variable y(q, s)
represents V (q, s), q0 is the initial controller node and ok is an arbitrary ﬁxed observation.

The experimental results suggest that many POMDPs have
small optimal controllers or can be approximated concisely.
Because of this, it may be unnecessary to use a large amount
of memory in order to ﬁnd a good approximation. As our
approach optimizes ﬁxed-size controllers, it allows the al-
gorithm to scale up to large problems without using an in-
tractable amount of space. In the rest of this section, we give
a formal description of the QCLP and prove that its optimal
solution deﬁnes an optimal controller of a ﬁxed size.

3.1 QCLP formulation
Unlike BPI, which alternates between policy improvement
and evaluation, our quadratically constrained linear program
improves and evaluates the controller in one phase. The value
of an initial node is maximized at an initial state distribution
using parameters for the action selection probabilities at each
node P (a|q), the node transition probabilities P (q(cid:2)|q, a, o),
and the values of each node in each state V (q, s). To ensure
that the value variables are correct given the action and node
transition probabilities, nonlinear constraints must be added
to the optimization. These constraints are the Bellman equa-
tions given the policy determined by the action selection and
node transition probabilities. Linear constraints are used to
maintain the proper probabilities.

(cid:2)

To reduce the representation complexity, the action selec-
tion and node transition probabilities are merged into one,
with

q(cid:2) P (q(cid:2), a|q, o) = P (a|q)

P (q(cid:2), a|q, o) = P (a|q)P (q(cid:2),|q, a, o)
and
This results in a quadratically constrained linear program.
QCLPs may contain quadratic terms in the constraints, but
have a linear objective function. They are a subclass of gen-
eral nonlinear programs that has structure which algorithms
can exploit. This produces a problem that is often more dif-
ﬁcult than a linear program, but possibly simpler than a gen-
eral nonlinear program. The QCLP formulation also permits
a large number of algorithms to be applied.

Table 2 describes the QCLP which deﬁnes an optimal
ﬁxed-size controller. The value of a designated initial node is
maximized given the initial state distribution and the neces-
sary constraints. The ﬁrst constraint represents the Bellman
equation for each node and state. The second and last con-
straints ensure that the variables represent proper probabili-
ties, and the third constraint guarantees that action selection
does not depend on the resulting observation which has not
yet been seen.
Theorem 1 An optimal solution of the QCLP results in an
optimal stochastic controller for the given size and initial
state distribution.
Proof The optimality of the controller follows from the Bell-
man equation constraints and maximization of a given node
at the initial state distribution. The Bellman equation con-
straints restrict the value variables to valid amounts based on
the chosen probabilities, while the maximum value is found
for the initial node and state. Hence, this represents an opti-
mal controller.

4 Methods for solving the QCLP
Constrained optimization seeks to minimize or maximize
an objective function based on equality and inequality con-
straints. When the objective and all constraints are linear,
this is called a linear program (LP). As our formulation has
a linear objective, but contains some quadratic constraints, it
is a quadratically constrained linear program. Unfortunately,
our problem is nonconvex. Essentially, this means that there
may be multiple local maxima as well as global maxima, thus
ﬁnding globally optimal solutions cannot be guaranteed.

Nevertheless, a wide range of nonlinear programming al-
gorithms have been developed that are able to efﬁciently ﬁnd
solutions for nonconvex problems with many variables and
constraints. Locally optimal solutions can be guaranteed, but
at times, globally optimal solutions can also be found. For
example, merit functions, which evaluate a current solution
based on ﬁtness criteria, can be used to improve convergence

IJCAI-07

2421

Figure 2: Hallway domain with goal state designated by a star

and the problem space can be made convex by approximation
or domain information. These methods are much more ro-
bust than gradient ascent, while retaining modest efﬁciency in
many cases. Also, the quadratic constraints and linear objec-
tive of our problem often permits better approximations and
the representation is more likely to be convex than problems
with a higher degree objective and constraints.

For this paper, we used a freely available nonlinearly con-
strained optimization solver called snopt [Gill et al., 2005]
on the NEOS server (www-neos.mcs.anl.gov). The algo-
rithm ﬁnds solutions by a method of successive approxima-
tions called sequential quadratic programming (SQP). SQP
uses quadratic approximations which are then solved with
quadratic programming (QP) until a solution to the more gen-
eral problem is found. A QP is typically easier to solve,
but must have a quadratic objective function and linear con-
straints. In snopt, the objective and constraints are combined
and approximated to produce the QP. A merit function is also
used to guarantee convergence from any initial point.

5 Experiments
In this section, we compare the results obtained using our new
formulation and the snopt solver with those of BPI and biased
BPI. GA was also implemented, but produced signiﬁcantly
worse results and required substantially more time than the
other techniques.
In the interest of saving space, we omit
the details of GA and focus on the more competitive tech-
niques, BPI and biased BPI, which we implemented accord-
ing to their original descriptions in [Poupart and Boutilier,
2003] and [Poupart and Boutilier, 2004]. Choices of the loss
parameter, δ, in biased BPI often decreased performance in
the second domain studied. Because of this, the best value
for δ was found experimentally after much trial and error.
This search centered around the value suggested by Poupart
and Boutilier, (maxs,aR(s, a)−mins,aR(s, a))/400(1−γ).
Note that our goal in these experiments is to demonstrate the
beneﬁts of our formulation when used in conjunction with an
“off the shelf” solver such as snopt. The formulation is very
general and many other solvers may be applied. In fact, we
are currently developing a customized solver that would take
further advantage of the inherent structure of the QCLP and
increase scalability.

Two domains are used to compare the performance of the
QCLP and the two versions of BPI. Each method was ini-
tialized with ten random deterministic controllers and we re-
port mean values and times after convergence. To slightly
increase the quality of the QCLP produced controllers, up-
per and lower bounds were added. These represent the value
of taking the highest and lowest valued action respectively
for an inﬁnite number of steps. While the experiments were

Figure 3: Hallway domain: the mean values using BPI and
the QCLP for increasing controller size

conducted on different computers, we expect that this affects
solution times by only a small constant factor in our experi-
ments. Mean time for biased BPI with or without a delta is
not reported as the time for each was only slightly higher than
that for BPI.
5.1 Hallway benchmark
The hallway domain, shown in Figure 2 was introduced by
Littman, Cassandra and Kaelbling [1995] and is a frequently
used benchmark for POMDP algorithms. It consists of a grid
world with 57 states, 21 observations and 5 actions. There
are 14 squares in which the robot may face north, south, east
or west, and a goal square. The robot begins in a random
location and orientation and must make its way to a goal state
by turning, going forward or staying in place. The start state
is never the same as the goal state and the observations consist
of the different views of the walls in the domain. Both the
observations and transitions are extremely noisy. Only the
goal has a reward of 1 and the discount factor used was 0.95.
The results for this domain are shown in Table 3 and Figure
3. We see that for all controller sizes the mean value produced
by the QCLP is greater than those produced by each version
of BPI. Biased BPI, and in particular biased BPI with a δ of
0.05, improves upon BPI, but the quality remains limited. The
value of an optimal POMDP policy is not known. However,
the value of an optimal policy for the underlying MDP, which
represents an upper bound on an optimal POMDP policy, was

size QCLPhal
1
2
4
6
8
10

BPIhal
< 1 min < 1 min
< 1min
< 1 min
< 1 min < 1 min
1.5 mins
1.5 mins
2.5mins
6.9 mins
9.1 mins
3.7mins

QCLPmac
< 1 min
< 1 min
7.3 mins
38.2 mins
72.2 mins
122.5 mins

BPImac
1.2 mins
4.3 mins
13.9 mins
25.2 mins
40.2 mins
59.5 mins

Table 3: Mean running times for the QCLP and BPI on the
hallway and machine problems

IJCAI-07

2422

found to be 1.519. The QCLP provides a very good solution
to the POMDP that is signiﬁcantly closer to this bound than
that found by BPI.

The time taken to produce these controllers remains mostly
similar, but the difference increases as controller size grows.
While in this case the signiﬁcantly higher quality results pro-
duced using QCLP required more computation time, it is not
generally the case that the QCLP is less efﬁcient. Also, it can
be noted that a four node controller improved by the QCLP
produces a higher quality solution than any BPI method for
any controller size while using very little time.

Poupart and Boutilier also report results for BPI on a 1500
node controller. In their implementation, an escape technique
which attempts to avoid local maxima, was also used. After
69 hours, BPI produced a controller with a value of 0.51. Our
QCLP formulation generates a 10 node controller with 40%
higher value in 0.21% of the time with 0.67% of the size.
5.2 Machine maintenance
In order to test performance on a larger problem, we consider
a machine maintenance domain with 256 states, 4 actions and
16 observations [Cassandra, 1998a]. There are four indepen-
dent components in a machine used to manufacture a part.
Each component may be in good, fair or bad condition as
well as broken and in need of a replacement. Each day, four
actions are possible. The machine can be used to manufac-
ture parts or we can inspect, repair, or replace the machine.
The manufacture action produces good parts based on the
condition of the components. Good components always pro-
duce good parts, and broken components always produce bad
parts. Components in fair or bad condition raise the proba-
bility of producing bad parts. The condition of the resulting
part is fully observed. Inspecting the machine causes a noisy
observation of either good or bad for each component. Com-
ponents in good or fair condition are more likely to be seen as
good,and those in fair or broken are more likely to be seen as
bad. Repairing the machine causes parts that are not broken
to improve one condition with high probability. The replace
action transitions all components to good condition. Com-
ponents may degrade each day unless repaired or replaced.
Rewards for each action are: 1 for manufacturing good parts
for the day, -1 for inspecting, -3 for repairing and -15 for pro-
ducing bad parts. A discount factor of 0.99 was used.

Figure 4: Machine domain: the mean values using BPI and
the QCLP for increasing controller size

sentation not only permits controllers to be found that outper-
form those provided by BPI, but ﬁnding solutions can also be
more efﬁcient for small controller sizes. A real strength of
the QCLP approach seems to be ﬁnding very compact high
valued controllers.

6 Conclusions
We introduced a new approach for solving POMDPs using
a nonlinear programming formulation of the problem which
deﬁnes an optimal ﬁxed-size stochastic controller. This al-
lows a wide range of powerful nonlinear optimization algo-
rithms to be applied. We showed that by using a standard
nonlinear optimization algorithm, we can produce higher val-
ued controllers than those found by the existing state-of-the-
art approach, BPI. Our approach provided consistently higher
values over a range of controller sizes even when the best pos-
sible heuristic settings were used for BPI in each domain. We
also showed that better solutions can be found with signif-
icantly smaller controllers by using the QCLP. These results
suggest that concise optimal or near-optimal controllers could
be found for large POMDPs, making our approach very use-
ful in a range of practical application domains. We are partic-
ularly encouraged by the ability of our approach to generate a
very small controller with 40% higher value in 0.21% of the
time used by BPI on a large (1500 node) controller.

This work opens up new promising research directions
that could produce further improvement in both quality and
efﬁciency. A better understanding of the relationship be-
tween the nonlinear formulation and various optimization al-
gorithms may improve performance. Different representa-
tions may better match current nonlinearly constrained opti-
mization methods and may thus produce better solutions. One
goal is to identify subclasses of POMDPs for which globally
optimal solutions can be efﬁciently found. Also, we are cur-
rently working on a customized QCLP solver that can take
advantage of the speciﬁc structure inherent in POMDPs. This
new approach shows promise for scaling up the size of con-
trollers that are efﬁciently solvable with our formulation.

For this problem, again the value of an optimal POMDP
policy is not known, but the value of an optimal policy for the
underlying MDP is known to be 66.236. We see in Figure 4
that very small controllers produced by the QCLP provide
solutions with values which are exceptionally close (92%)
to this upper bound. The QCLP generates higher quality
controllers for all sizes and we are encouraged to see par-
ticularly signiﬁcant performance improvement for small con-
troller sizes. Again, a four node controller produced with the
QCLP representation has a higher mean value than any BPI
approach.

Running times, as seen in Table 3, are more favorable for
our fomulation in this problem. The QCLP required less time
than BPI for small controller sizes and remains reasonable for
larger controller sizes. While more time is required for larger
controller sizes, these results suggest that the QCLP repre-

IJCAI-07

2423

The QCLP technique is competitive with the state-of-the-
art and in the future, we will conduct a more comprehensive
evaluation of our approach with respect to a wide range of ap-
proximation techniques and domains. BPI has already been
shown to be competitive with other leading POMDP approxi-
mation techniques such as Perseus [Spaan and Vlassis, 2005]
and PBVI [Pineau et al., 2005]. This suggests that the QCLP
approach compares favorably with these techniques as well.
We are conﬁdent that our experiments will verify this as well
as provide insight into the strengths and weaknesses of the
QCLP approach in relation to other techniques.

Finally, we have begun to study the applicability of the
QCLP approach to problems involving multiple agents mod-
eled as decentralized POMDPs [Amato et al., 2006]. The
BPI technique has already been generalized successfully to
the multi-agent case [Bernstein et al., 2005]. Based on our
recent experiments, we are conﬁdent that the QCLP approach
can also be used to ﬁnd concise near-optimal ﬁxed-size con-
trollers for a set of agents in large multi-agent domains.

7 Acknowledgments
We would like to thank the anonymous reviewers for their
helpful comments. Support for this work was provided in
part by the National Science Foundation under Grant No. IIS-
0535061 and by the Air Force Ofﬁce of Scientiﬁc Research
under Agreement No. FA9550-05-1-0254.

References
[Amato et al., 2006] Christopher Amato, Daniel S. Bern-
stein, and Shlomo Zilberstein. Optimal ﬁxed-size con-
trollers for decentralized POMDPs. In Proceedings of the
Workshop on Multi-Agent Sequential Decision Making in
Uncertain Domains (MSDM) at AAMAS, Hakodate, Japan,
2006.

[Bernstein et al., 2005] Daniel S. Bernstein, Eric Hansen,
and Shlomo Zilberstein. Bounded policy iteration for de-
centralized POMDPs.
In Proc. of the Nineteenth Inter-
national Joint Conference on Artiﬁcial Intelligence, Edin-
burgh, Scotland, 2005.

[Bertsekas, 2004] Dimitri P. Bertsekas. Nonlinear Program-

ming. Athena Scientiﬁc, 2004.

[Cassandra, 1998a] Anthony R. Cassandra. Exact and Ap-
proximate Algorithms for Partially Observable Markov
Decision Processes. PhD thesis, Brown University, Provi-
dence, RI, 1998.

[Cassandra, 1998b] Anthony R. Cassandra. A survey of
POMDP applications. In AAAI Fall Symposium: Planning
with POMDPs, Orlando, FL, 1998.

[Eckles, 1968] James E. Eckles. Optimum maintenance with
incomplete information. Operations Research, 16:1058–
1067, 1968.

[Feng and Hansen, 2002] Zhengzhu Feng and Eric A.
Hansen. Symbolic heuristic search for factored Markov
decision processes.
In Proc. of the Eighteenth National
Conference on Artiﬁcial Intelligence, pages 455–460,
2002.

[Feng and Zilberstein, 2005] Zhengzhu Feng and Shlomo
Zilberstein. Efﬁcient maximization in solving POMDPs.
In Proc. of the Twentieth National Conference on Artiﬁ-
cial Intelligence, Pittsburgh, PA, 2005.

[Gill et al., 2005] Philip E. Gill, Walter Murray, and Michael
Saunders. Snopt: An SQP algorithm for large-scale con-
strained optimization. SIAM Review, 47:99–131, 2005.

[Hansen, 1998] Eric A. Hansen.

Solving POMDPs by
searching in policy space.
In Proc. of the Fourteenth
Conference on Uncertainty in Artiﬁcial Intelligence, pages
211–219, Madison, WI, 1998.

[Hauskrecht and Fraser, 1998] Milos

and
Hamish Fraser. Modeling treatment of ischemic heart dis-
ease with partially observable Markov decision processes.
In Proc. of American Medical Informatics Association
annual symposium on Computer Applications in Health
Care, pages 538–542, 1998.

Hauskrecht

[Littman et al., 1995] Michael L. Littman, Anthony R. Cas-
sandra, and Leslie Pack Kaelbling. Learning policies for
partially observable environments: Scaling up. Technical
Report CS-95-11, Brown University, Department of Com-
puter Science, Providence, RI, 1995.

[Meuleau et al., 1999] Nicolas Meuleau, Kee-Eung Kim,
Leslie Pack Kaelbling, and Anthony R. Cassandra. Solv-
ing POMDPs by searching the space of ﬁnite policies. In
Proc. of the Fifteenth Conference on Uncertainty in Ar-
tiﬁcial Intelligence, pages 417–426, Stockholm, Sweden,
1999.

[Pineau et al., 2005] Jolle Pineau, Geoffrey Gordon, and
Sebastian Thrun.
Point-based approximations for fast
POMDP solving. Technical Report SOCS-TR-2005.4,
McGill University. School of Computer Science, Mon-
treal, QC, 2005.

[Poupart and Boutilier, 2003] Pascal Poupart

and Craig
Boutilier. Bounded ﬁnite state controllers. In Advances in
Neural Information Processing Systems, 16, Vancouver,
BC, 2003.

[Poupart and Boutilier, 2004] Pascal Poupart

and Craig
Boutilier. VDCBPI: an approximate scalable algorithm
for large scale POMDPs.
In Advances in Neural In-
formation Processing Systems, 17, pages 1081–1088,
Vancouver, BC, 2004.

[Simmons and Koenig, 1995] Reid Simmons

and Sven
Koenig. Probabilistic navigation in partially observable
environments.
In Proc. of the 14th International Joint
Conference on Artiﬁcial Intelligence, pages 1080–1087,
1995.

[Singh et al., 1994] Satinder Singh, Tommi Jaakkola, and
Michael Jordan. Learning without state-estimation in par-
tially observable Markovian decision processes.
In Ma-
chine Learning: Proceedings of the Eleventh International
Conference, pages 284–292, 1994.

[Spaan and Vlassis, 2005] Matthijs T. J. Spaan and Nikos
Vlassis. Perseus: Randomized point-based value iteration
for POMDPs. Journal of AI Research, 24:195–220, 2005.

IJCAI-07

2424


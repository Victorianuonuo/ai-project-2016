Towards a Computational Model of Melody IdentiÔ¨Åcation in Polyphonic Music

S√∏ren Tjagvad Madsen

Austrian Research Institute for
ArtiÔ¨Åcial Intelligence, Vienna

soren.madsen@ofai.at

Gerhard Widmer

Department of Computational Perception

Johannes Kepler University, Linz

gerhard.widmer@jku.at

Abstract

This paper presents Ô¨Årst steps towards a sim-
ple,
robust computational model of automatic
melody identiÔ¨Åcation. Based on results from mu-
sic psychology that indicate a relationship between
melodic complexity and a listener‚Äôs attention, we
postulate a relationship between musical complex-
ity and the probability of a musical line to be per-
ceived as the melody. We introduce a simple mea-
sure of melodic complexity, present an algorithm
for predicting the most likely melody note at any
point in a piece, and show experimentally that this
simple approach works surprisingly well in rather
complex music.

1 Introduction
Melody is a central dimension in almost all music. Human
listeners are very effective in (unconsciously) picking out
those notes in a ‚Äì possibly complex ‚Äì multi-voice piece that
constitute the melodic line. Melody is also an important as-
pect in music-related computer applications, for instance, in
Music Information Retrieval (e.g., in music databases that of-
fer retrieval by melodic motifs [Weyde and Datzko, 2005] or
Query by Humming [Birmingham et al., 2006]).

It is not easy to unequivocally deÔ¨Åne the concept of
‚Äòmelody‚Äô. In a sense, the melody is the most prominent line
in a polyphonic (multi-voice) piece of music. Although in
Western music, the melody is often found among the higher
notes, this is not always the case. Also, even in composi-
tions that are explicitly structured into individual lines by the
composer (such as, e.g., orchestral pieces consisting of mono-
phonic instrument voices), the melody is by no means always
represented by (or ‚Äòappearing in‚Äô) the same line throughout a
piece. In a way, which notes constitute the melody is deÔ¨Åned
by where the listeners perceive the most interesting things to
be going on in the music, or what they sense to be the most
coherent path through the complex interweaving of musical
lines. Thus, though our experience tells us that hearing the
melody is a rather simple and intuitive task for humans, it is
by no means a simple task to be formalised in a machine.

In popular music, it is sometimes assumed that the melody
does not change between the instruments present. Some work
has been done on predicting which one of the tracks in a

MIDI Ô¨Åle contains the ‚Äòmain melody‚Äô [Rizo et al., 2006;
Friberg and Ahlb¬®ack, 2006]. In both cases a statistical ap-
proach was taken ‚Äì learning properties of melodic and non-
melodic tracks. Ideas for converting polyphonic tracks into
melodically meaningful monophonic sequences have also
been proposed [Uitdenbogerd and Zobel, 1998].

This paper presents Ô¨Årst steps towards a simple, robust
computational model of automatic melody note identiÔ¨Åcation.
Based on results from musicology and music psychology that
indicate a relationship between melodic complexity and a lis-
tener‚Äôs attention, we postulate that the notes making up the
melody line may be identiÔ¨Åed by calculating and integrating
some measures of perceived complexity over time. We will
introduce a simple, straightforward measure of melodic com-
plexity based on entropy, present an algorithm for predicting
the most likely melody note at any point in a piece, and show
experimentally that this very simple approach works surpris-
ingly well in picking out the melody notes in quite complex
polyphonic music. Still, the results are still far below what
we can expect humans to achieve, and we will discuss a num-
ber of possible extensions of the approach towards a more
comprehensive and effective computational model.

2 Complexity and Melody Perception
The basic motivation for our model of melody identiÔ¨Åca-
tion is the observation, which has been made many times
in the literature on music cognition, that there seems to be
a connection between the complexity of a musical line, and
the amount of attention that will be devoted to it on the
part of a listener. A voice introducing new or surprising
musical material will potentially attract the listener‚Äôs atten-
tion. However, if the new material is constantly repeated,
we will pay less and less attention to it and become ha-
bituated or accustomed to the stimulus. Less attention is
required from the listener and the voice will fall into the
background [Snyder, 2000]. The notion of musical sur-
prise is also related to the concept of ‚Äòexpectation‚Äô as it
has been put forth in recent music theories [Narmour, 1990;
Huron, 2006]. If we assume that the melody is the musical
line that commands most attention and presents most new in-
formation, it seems natural to investigate melodic complexity
measures as a basis for melody detection algorithms.

Indeed, the idea of using information-theoretic complex-
ity measures to characterise aspects of musical development

IJCAI-07

459

is not at all new. For instance, to cite just two, in [Dubnov
et al., 2006], a measure of Information Rate computed over
a piece of music was shown to correlate in signiÔ¨Åcant ways
with familiarity ratings and emotional force response proÔ¨Åles
In [Li and Sleep, 2005] it was
by human human subjects.
shown that kernel-based machine learning methods using a
compression-based similarity measure on audio features per-
form very well in automatic musical genre classiÔ¨Åcation.

In the current paper, we intend to show that the complexity
or information content of a sequence of notes may be directly
related to the degree to which the note sequence is perceived
as being part of the melody. Our approach is to start with a
very simple (or simplistic) measure of complexity based only
on note-level entropies, in order to Ô¨Årst understand the in-
Ô¨Çuence of this simple parameter. In the next phases of the
project, more complex complexity measures based on pattern
compression and top-down heuristics derived from music the-
ory will be added, one by one.

3 A Computational Model
The basic idea of the model consists in calculating a series of
complexity values locally, over short-term musical segments,
and for each individual voice in a piece.1 Based on these
series of local complexity estimates, the melody is then re-
constructed note by note by a simple algorithm (section 3.4).
The information measures will be calculated from the
structural core of music alone: a digital representation of the
printed music score. Though the model works with arbitrary
MIDI Ô¨Åles which may represent actual performances, it will
not consider performance aspects like expressive dynamics,
articulation, timbre, etc. These may well contain a lot of use-
ful information for decoding the melody (in fact, expressive
music performance is a means used by performers to eluci-
date the musical structure of a piece [Gabrielsson, 1999]),
but we want to exclusively focus on music-structural aspects
Ô¨Årst, in order not to mix different factors in our investigation.

3.1 The Sliding Window
The algorithm operates by in turn examining a small subset of
the notes in the score. A Ô¨Åxed length window (with respect to
duration) is slid from left to right over the score. At each step,
the window is advanced to where the next ‚Äòchange‚Äô happens
so no window will contain exactly the same set of notes, but at
the same time all possibilities of placing the window resulting
in different content have been examined. The Ô¨Årst window
starts at time 0. It is then moved ahead in time until the end
of the piece. At each step, the next window will begin at
whatever of the following two situations occurs Ô¨Årst:

1. offset of Ô¨Årst ending note in current window

2. onset of next note after current window

pitch
6

a

b

c

time
-

 wi+2 -  wi+3 -

 wi+1 -

 wi -

Figure 1: Sliding the window

notes in the current window end at the same time, the next
window will thus begin right after, which makes it possible
for the window to be empty. Only then will the second of the
above cases apply (the change is the entry of the next note).

Figure 1 shows the positions of the window when sliding
over three notes a, b, and c. Window wi contains a and b,
wi+1 contains only b, wi+2 is empty and wi+3 contains c.

From the notes belonging to the same voice (instrument) in
the window, we calculate a complexity value. We do that for
each voice present in the window. The most complex voice
is expected to be the one that the listener will focus on in this
time period. The complexity measures and the melody note
prediction method are explained below.

3.2 Entropy Measures in Musical Dimensions
Shannon‚Äôs entropy [Shannon, 1948] is a measure of random-
ness or uncertainty in a signal. If the predictability is high,
the entropy is low, and vice versa.

Let X = {x1, x2, . . . , xn} and p(x) = P r(X = x) then

the entropy H(x) is deÔ¨Åned as:
(cid:2)

H(X) = ‚àí

p(x) log2 p(x)

(1)

x‚ààX

X could for example be the set of MIDI pitch numbers
and p(x) would then be the probability (estimated by the fre-
quency) of a certain pitch. In the case that only one type of
event (one pitch) is present in the window, that event is highly
predictable or not surprising at all, and the entropy is 0. En-
tropy is maximised when the probability distribution over the
present events is uniform.

We are going to calculate entropy of ‚Äòfeatures‚Äô extracted
from the notes in monophonic lines. We will use features re-
lated to pitch and duration of the notes. A lot of features are
possible: MIDI pitch number, MIDI interval, pitch contour,
pitch class, note duration, inter onset interval etc. (cf. [Con-
klin, 2006]). We have used the following three measures in
the model presented here:

In the Ô¨Årst case, the change occurs since a note ‚Äòleaves‚Äô the
current window; the next window will begin right after the
Ô¨Årst occurring offset of a note in the current window. If all

1We assume that a polyphonic piece has already been ‚Äòstreamed‚Äô
into individual voices. That is another non-trivial music analysis
problem, but recent work of ours [Madsen and Widmer, 2006] indi-
cates that it can be solved quite effectively, via heuristic search.

1. Pitch class (C): count the occurrences of different pitch
classes present (the term pitch class is used to refer the
‚Äòname‚Äô of a note, i.e., the pitch irrespective of the octave,
such as C, D, etc.);

2. MIDI Interval (I): count the occurrences of each melodic
interval present (e.g., minor second up, major third
down, . . . );

IJCAI-07

460

3. Note duration (D): count the number of note duration
classes present, where note classes are derived by dis-
cretisation (a duration is given its own class if it is not
within 10% of an existing class).

With each measure we extract events from a given se-
quence of notes, and calculate entropy from the frequencies
of these events (HC, HI ,HD).

HC and HI are thought to capture opposite cases. HC
will result in high entropy when calculated on notes that form
a scale, while HI will result in low entropy.
In situations
where there are more different intervals than pitch classes
(e.g. an ‚Äòalberti bass‚Äô, or a triad chord played in arpeggio
up and down), HI will produce a higher entropy than HC .

So far rhythm and pitch are treated separately. We have
also included a measure HCID weighting the above three
measures: HCID = 1

4 (HC + HI ) + 1

HD.

2

Entropy is also deÔ¨Åned for a pair of random variables with

joint distribution:

H(X, Y ) = ‚àí

(cid:2)

(cid:2)

x‚ààX

y‚ààY

p(x, y) log2[p(x, y)]

(2)

We will test two joint entropy measures: Pitch class in re-
lation to duration (HC,D) and interval plus duration (HI,D).
These are expected to be more speciÔ¨Åc discriminators.

As melodic intervals are meant to be calculated from suc-
cessive single musical events (rather than between chords),
we will apply these measures only to individual lines in the
music ‚Äì instrumental parts. In the few cases where it does
happen that a part contains simultaneous notes (e.g., a vio-
lin), only the top note is used in the entropy calculation.

3.3 An Alternative: Complexity via Compression

The entropy function is a purely statistical measure related to
the frequency of events. No relationships between events is
measured ‚Äì e.g.
the events abcabcabc and abcbcacab
will result in the same entropy value. Humans, however,
would probably describe the Ô¨Årst string as three occurrences
of the substring abc ‚Äì we infer structure. According to
Snyder, we perceive music in the most structured way pos-
sible [Snyder, 2000]. To take this into account, complexity
measures based on compression could be considered. Music
that can be compressed a great deal (in a lossless way) can
then be considered less complex than music that cannot be
compressed. Methods exist that substitute recurring patterns
with a new event, and store the description of that pattern only
once, e.g. run-length encoding or LZW compression [Ziv and
Lempel, 1977]. This idea has been discussed in several musi-
cal application contexts (e.g., in Music Information Retrieval
[Li and Sleep, 2005] or in automated music analysis and pat-
tern discovery [Lartillot, 2006]). However, these compres-
sion algorithms are not well suited for compressing the short
sequences that we are dealing with. Special algorithms for
compressing short sequences will be needed.

Shmulevich and Povel [Shmulevich and Povel, 2000] have
examined methods for measuring the complexity of short
rhythmic patterns which are supposed to repeat inÔ¨Ånitely.
Tanguiane‚Äôs measure [Tanguiane, 1993] is based on the idea
that a rhythmic pattern can be described as elaborations of

windows
6

 w1 -

 w2 -

 w4 -

 w5 -

 w3 -

p1 p2

p3

p4

p5

prediction period
-

(1) (1,2)

(2,3)

(4)

(4,5)

overlapping windows

Figure 2: Prediction periods and windows overlapping them.

simpler patterns. Before turning to these more complex mea-
sures and the assumptions they make, we would like to in-
vestigate the power of a simple and well-understood measure
like entropy.

3.4 Predicting Melody Notes

Given the set of windows described above and an entropy
value for each voice present in each window we will predict
the notes expected to belong to the melody. We consider in
turn the notes in the interval between the starting points of
two consecutive windows ‚Äì the prediction period. The pre-
diction period pi is thus the interval between the beginning of
window wi and the beginning of wi+1 (see Figure 2).

We want to mark, in each prediction period, the notes that
pertain to the most complex voice. Complexity measures
from each window a note appears in will inÔ¨Çuence the pre-
diction. In a prediction period pi, we will consider the set
o(pi) of all windows that overlap this period. For pi the set
will contain all windows that have not ended at the start time
of pi+1. Figure 2 gives an example of prediction periods and
overlapping windows ‚Äì e.g. o(p2) is the set {w1, w2}.

For each voice in each window, a complexity value was
calculated by the time the windows were calculated. The av-
erage complexity value for each voice present in the windows
in o(pi) is calculated. Based on these values, we can now
rank the voices according to their average complexity over
o(pi), and a ‚Äòwinning‚Äô voice is found. Every note in wi (the
window where pi begins) gets its melody attribute set to true
if it is part of the winning voice, and to false otherwise.

In each prediction period, exactly the windows overlapping
(or partly overlapping) this period are contributing to the Ô¨Ånal
score of each voice. The policy is to mark all notes in the
entire window wi starting at pi. When wi+1 occurs before wi
has ended, the contribution from this new window can change
the prediction of wi from that point in time and onwards. A
new prediction period pi+1 begins ‚Äì including all windows
now present at this time, and the notes‚Äô melody attribute is
reset accordingly. So if a note is only contained in a single
prediction period, its status will only be set once. A long note
contained in a number of prediction periods will Ô¨Ånally be
judged by the period it last occurs in.

Thus the predicted notes may belong to different voices
from period to period ‚Äì the role as the most prominent voice
may change quickly. We are not just predicting an entire
voice to be the most interesting, but every note is considered.
In case there are several voices that share the highest com-

IJCAI-07

461

plexity value for a given prediction period, the voice with the
highest pitch will be predicted (e.g., of two voices playing in
octaves, the higher one is chosen).

4 Experiments

4.1 The Musical Test Corpus

To perform experiments we need music which is composed
for different parts, and encoded in such a way that each part
is separately available. Furthermore, since our complexity
measure assumes monophonic music, each voice in the piece
should be close to monophonic. This places some restric-
tions on the experiments we are able to do. A typical piano
sonata, lacking the explicit voice annotation (or being one
non-monophonic part), will not be an appropriate choice.

Since the model presented here does not take into account
any performance aspects but only the musical surface (score),
we will use MIDI Ô¨Åles generated from the MuseData for-
mat (http://www.musedata.org). The durations of the notes
in these Ô¨Åles are nicely quantised. The model will run on all
types of MIDI Ô¨Åles, but performed music tends to bias the
rhythm complexity. Two pieces of music were chosen to be
annotated and used in the experiment:

1. Haydn, F.J.: String quartet No 58 op. 54, No. 2, in C

major, 1st movement

2. Mozart, W.A.: Symphony No 40 in G minor (KV 550),

1st movement

This is not an awful lot of data, but since the music will
have to be annotated manually, this will have to do for our
initial experiments.

4.2 Annotating Melody Notes

Melody annotation is non-trivial and rather tedious task. A
trained musicologist was asked to serve as expert annotator.
She was given a piano roll representation of the music with
‚Äòclickable‚Äô notes and simple playback options for listening to
the MIDI sound. The annotator, who was not aware of the
purpose of this task, was instructed to mark notes she would
consider as ‚Äòmelody‚Äô.

The test pieces turned out to have quite distinguishable
melodic lines. The annotator solved the task by marking the
notes that she was humming during listening to the pieces.
Some immediate differences between the ‚Äòtrue‚Äô concept of
melody and some assumptions behind our complexity-based
melody prediction model became immediately clear after
talking to the annotator:

Our model assumes that there is always one most complex
voice present, but a melody was not found to be present at all
times. Furthermore, melodic lines were sometimes marked as
overlapping, which the model does not allow. The annotator
chose not to mark thematic lines that clearly were a (melodic)
response in another voice to the melody rather than a contin-
uation of the melody line. Our model might mark both the
melody and the response. Another source of error is situa-
tions where the melodic line consists of long sustained tones
while the accompanying notes are doing all the action. The
model will erroneously predict an accompanying voice. (In

Total number of notes
Number of melody notes
Melody note percentage
Number of voices
Duration

Haydn
5832
2555
43.8 %

4

Mozart
13428
2295
17.1 %

10

11:38 min

6:55 min

Table 1: The test data

these situations it is also difÔ¨Åcult to tell what a listener will
actually listen to).

In any case, the annotations made by the musicologists
were taken to be authoritative; they were adopted unchanged
and used as ground truth for the evaluation. Table 1 shows
some information about the test data.

4.3 Evaluation Method
We can now measure how well the predicted notes correspond
to the annotated melody in the score. We express this in terms
of recall (R) and precision (P ) values [van Rijsbergen, 1979].
Recall is the number of correctly predicted notes (true posi-
tives, TP) divided by the total number of notes in the melody.
Precision is TP divided by the total number of notes predicted
(TP + FP (false positives)). The F-measure combines recall
and precision into one value:

F (R, P ) = 1 ‚àí 2RP
R + P

(3)

A high rate of correctly predicted notes will result in high

values of recall, precision and F-measure (close to 1.0).

4.4 Results
We performed prediction experiments with four different
window sizes (1-4 seconds) and with the six different entropy
measures described above. Table 2 shows recall, precision
and F-measure values from all experiments with the two eval-
uation pieces. In addition, the columns marked P show the
value of the F-measure achieved by the simple strategy of al-
ways picking the highest note (see below). The highest values
in each row are printed in bold (ignoring the P columns).

The string quartet turned out to be the less complex of the
two pieces. This is not much of a surprise. It is easier to
discriminate between 4 than 10 voices, and also the compo-
sitional structures of the pieces are quite different. Overall
the joint pitch class and duration measure HC,D was found
to have the greatest predictive power, generally followed by
the weighted combination HCID. Pitch class seems to be the
single most important measure in the string quartet. In total,
the joint measures perform better than the measures based on
a single feature.

We can conclude that there is indeed a correlation between
melody and complexity in both pieces. The precision value
of 0.60 in the best symphony experiment with a resulting F-
measure of 0.51 (window size 3 seconds) tells us that 60 % of
the predicted notes in the symphony are truly melody notes.

In the string quartet, starting in bar 105 (see Figure 3) the
second violin is alternating between a single note and notes
from a descending scale, making the voice very attractive
(lots of different notes and intervals) while the ‚Äòreal melody‚Äô

IJCAI-07

462

Win

1 s

2 s

3 s

4 s

P

R 0.91
P
0.96
F
0.94
R 0.91
P
0.96
F
0.93
R 0.91
P
0.97
F
0.94
R 0.93
P
0.96
F
0.95

Vl. I

Vl. II

Vla.

Vlc.

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

Figure 3: The string quartet, measures 105-108

in the Ô¨Årst violin is playing fewer different notes, but has
a more varied rhythm. We took a closer look at this pas-
sage. Setting the window size to 2 seconds, the measures
HD, HCID, and HC,D recognise the upper voice as melody
whereas HC , HI , and HI,D suggest the lower. The measures
based on intervals are naturally led astray in this case, and the
measure based solely on pitch class is also.

The reader may also ask if it would not be more effective
to just always predict the highest note as the melody note ‚Äì
after all, that is where the melody is most often, at least in
relatively simple music. We checked against this baseline.
The results are shown in the columns labeled P in Table 2.
Indeed, it turns out that in Hadyn string quartet, the simple
strategy of always picking the highest note outperforms all
of our heuristics ‚Äì at least in terms of global precision and
recall, though not necessarily in terms of the local musical
coherence of the resulting melodic line.

In the more complex Mozart piece, however, we witness
that the highest note prediction strategy was outperformed by
most of our methods. In the symphony movement the melody
is not on top as often. Also, the melody role is taken by dif-
ferent instruments at different times. Figure 4 shows 6 bars
(bar 17-22) from the symphony. In the Ô¨Årst three, the Ô¨Çute
(top voice) was annotated to be the melody, but the melody
then moves to the Ô¨Årst violin in bar 19. In bar 21 the Ô¨Årst
oboe enters on a high-pitched long note ‚Äì above the melody.
Such high-pitched non-melodic notes (doubling chord tones)
occur frequently in the movement. The HC,D measure cor-

Haydn

Mozart

HC HI HD HCID HC,D HI,D
0.80
0.83
0.74
0.78
0.77
0.81
0.81
0.83
0.81
0.76
0.78
0.82
0.80
0.83
0.78
0.83
0.83
0.79
0.79
0.84
0.78
0.84
0.79
0.84

0.79
0.73
0.76
0.80
0.75
0.77
0.79
0.75
0.77
0.76
0.74
0.75

0.67
0.80
0.73
0.64
0.81
0.71
0.63
0.81
0.71
0.59
0.78
0.67

0.84
0.81
0.83
0.80
0.82
0.81
0.78
0.83
0.81
0.74
0.84
0.78

0.87
0.81
0.84
0.87
0.87
0.87
0.84
0.87
0.85
0.80
0.85
0.83

P

0.27
0.61
0.37

0.23
0.58
0.33

0.20
0.53
0.29

0.18
0.50
0.26

HC HI HD HCID HC,D HI,D
0.32
0.36
0.32
0.41
0.32
0.38
0.37
0.35
0.41
0.41
0.39
0.38
0.36
0.33
0.42
0.40
0.36
0.38
0.33
0.34
0.41
0.40
0.37
0.36

0.31
0.31
0.31
0.33
0.36
0.35
0.28
0.31
0.29
0.24
0.28
0.26

0.32
0.52
0.40
0.27
0.52
0.36
0.24
0.49
0.33
0.19
0.41
0.26

0.41
0.54
0.47
0.37
0.57
0.45
0.33
0.54
0.41
0.31
0.55
0.40

0.47
0.54
0.51
0.45
0.58
0.51
0.45
0.60
0.51
0.44
0.61
0.51

Table 2: Recall, precision, and F-measure for melody note predictions.

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

p

p

p

Fl.

Ob.

Cl.

Fg.

Cor.

Cor.

Vl. I

Vl. II

Vla.

Vc.
Cb.

f

f

f

f

f

f

f

f

f

f

p

p

p

Figure 4: The symphony, measures 17-22

rectly catches the entry of the theme in bar 19, but the bassoon
(Fg.) is falsely predicted in bar 17-19 ‚Äì the wind instruments
play very similar lines, but because the bassoon continues a
bit further than the others it becomes the most complex.

5 Discussion
In our opinion, the current results, though based on a rather
limited test corpus, indicate that it makes sense to consider
musical complexity as an important factor in computational
models of melody perception. The results of the experiments
show that a simple entropy based incremental algorithm can

IJCAI-07

463

ƒ¢
identify parts of melodies in quite complex music. We can
also turn the argument around and interpret this as empirical
evidence that melody in classical tonal music is indeed partly
deÔ¨Ånable by complexity ‚Äì a Ô¨Ånding that musicology may Ô¨Ånd
interesting.

Clearly, recall and precision values of around 50% (as in
the Mozart piece) are not sufÔ¨Åcient, neither for justifying our
approach as a full model of melody perception, nor for prac-
tical applications. The next steps towards improvement are
quite clearly mapped out. The notion of musical complex-
ity must be extended to encompass structural aspects (i.e.,
recognisable patterns and their recurrence). We will study
various compression-based complexity measures, in particu-
lar with respect to their suitability in the context of very short
sequences (windows).

A second research direction is dictated by the musicologi-
cal insight that melody, and generally the perception of what
constitutes a coherent musical line, has something to do with
continuity in certain parametric dimensions (e.g., in terms of
intervallic motion). Such Gestalt-like principles are usually
difÔ¨Åcult to formalise, due to their inherently top-down nature,
but local measures of continuity and melodic coherence over
short windows should be quite straightforward.

Generally, the more different aspects are integrated in a
model of music perception, the more conÔ¨Çicts may arise be-
tween them (for instance the obvious conÔ¨Çict between conti-
nuity and unpredictability). Good music, in fact, thrives on
these kinds of conÔ¨Çicts ‚Äì divergent cues make music interest-
ing to the listeners, maintaining their attention at a high level.
That is precisely why performers are so important in music.
Their task is, in part, to disambiguate different possible ‚Äòread-
ings‚Äô of a piece through their playing. Thus, eventually we
will also have to take into account performance information,
such as dynamics (relative loudness), articulation, etc. But
we plan to proceed in stages, evaluating contributions of indi-
vidual components step by step, in order to arrive at a model
that not only works, but tells us something interesting about
a human musical ability.

Acknowledgments
This research was supported by the Viennese Science and
Technology Fund (WWTF, project CI010) and by the Aus-
trian Federal Ministries of Education, Science and Culture
and of Transport, Innovation and Technology.

References
[Birmingham et al., 2006] William Birmingham, Roger
Dannenberg, and Bryan Pardo. Query By Humming with
the VocalSearch System. Communications of the ACM,
49(8):49‚Äì52, August 2006.

[Conklin, 2006] Darrell Conklin. Melodic analysis with seg-
ment classes. Machine Learning, Special Issue on Ma-
chine Learning in Music(in press), 2006.

[Dubnov et al., 2006] S. Dubnov,

S.McAdams,

and
Structural and affective aspects of mu-
Journal of
Information Science and

R. Reynolds.
sic from statistical audio signal analysis.
the American Society for
Technology, 57(11):1526‚Äì1536, September 2006.

[Friberg and Ahlb¬®ack, 2006] Anders Friberg

and Sven
Ahlb¬®ack. A method for recognising the melody in a
symbolic polyphonic score (Abstract). In Proceedings of
the 9th International Conference on Music Perception and
Cognition (ICMPC), Bologna, Italy, 2006.

[Gabrielsson, 1999] Alf Gabrielsson. The performance of
music. In Diana Deutsch, editor, The Psychology of Mu-
sic (2nd Ed.), pages 501‚Äì602, San Diego, CA, 1999. Aca-
demic Press.

[Huron, 2006] David Huron. Sweet Anticipation: Music and
the Psychology of Expectation. MIT Press, Cambridge,
Massachusetts, 2006.

[Lartillot, 2006] Olivier Lartillot. A musical pattern discov-
ery system founded on a modeling of listening strategies.
Computer Music Journal, 28(3):53‚Äì67, 2006.

[Li and Sleep, 2005] Ming Li and Ronan Sleep. Genre clas-
siÔ¨Åcation via an lz78-based string kernel. In Proceedings
of the 6th International Conference on Music Information
Retrieval (ISMIR 2005), London, U.K., 2005.

[Madsen and Widmer, 2006] S√∏ren Tjagvad Madsen and
Gerhard Widmer. Separating voices in MIDI. In Proceed-
ings of the 7th International Conference on Music Infor-
mation Retrieval (ISMIR 2006), Victoria, Canada, 2006.

[Narmour, 1990] Eugene Narmour. The Analysis and Cog-
nition of Basic Melodic Structures. University of Chicago
Press, Chicago, IL, 1990.

[Rizo et al., 2006] David Rizo, Pedro J. Ponce de Le¬¥on, An-
tonio Pertusa, Carlos P¬¥erez-Sachno, and Jos¬¥e M. I nesta.
Melodic track identiÔ¨Åcation in MIDI Ô¨Åles. In Proceedings
of the 19th International FLAIRS Conference, Melbourne
Beach, Florida, 2006.

[Shannon, 1948] C. E. Shannon. A mathematical theory
of communication. The Bell System Technical Journal,
27:379‚Äì423, 623‚Äì656, July, October 1948.

[Shmulevich and Povel, 2000] Ilya Shmulevich and Dirk-
Jan Povel. Measures of temporal pattern complexity. Jour-
nal of New Music Research, 29(1):61‚Äì69, 2000.

[Snyder, 2000] Bob Snyder. Music and Memory: An Intro-

duction. MIT Press, 2000.

[Tanguiane, 1993] A.S. Tanguiane. ArtiÔ¨Åcial Perception and

Music Recognition. Springer, Berlin, 1993.

[Uitdenbogerd and Zobel, 1998] Alexandra L. Uitdenbogerd
and Justin Zobel. Manipulation of music for melody
matching. In ACM Multimedia, pages 235‚Äì240, 1998.

[van Rijsbergen, 1979] C. J. van Rijsbergen. Information Re-

trieval. Butterworth, London, 1979.

[Weyde and Datzko, 2005] T. Weyde and C. Datzko. EfÔ¨Å-
cient melody retrieval with motif contour classes. In Pro-
ceedings of the 6th International Conference on Music In-
formation Retrieval (ISMIR 2005), London, U.K., 2005.

[Ziv and Lempel, 1977] J. Ziv and A. Lempel. A universal
algorithm for sequential data compression. IEEE Transac-
tions on Information Theory, 23(3):337‚Äì343, May 1977.

IJCAI-07

464


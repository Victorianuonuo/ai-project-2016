A Call Admission Control Scheme using NeuroEvolution Algorithm in Cellular 

Networks

Xu Yang 
MPI-QMUL

Information Systems Research Centre 
A313, Macao Polytechnic Institute 

Macao SAR, China 
xuy@mpi-qmul.org

John Bigham 

Department of Electronic Engineering 
Queen Mary University of London, 

London E1 4NS, U.K. 

john.bigham@elec.qmul.ac.uk

Abstract

This paper proposes an approach for learning call 
admission control (CAC) policies in a cellular net-
work  that  handles  several  classes  of  traffic  with 
different  resource  requirements.  The  performance 
measures in cellular networks are long term revenue, 
utility, call blocking rate (CBR) and handoff failure 
rate  (CDR).  Reinforcement  Learning  (RL)  can  be 
used to provide the optimal solution, however such 
method fails when the state space and action space 
are huge. We apply a form of NeuroEvolution (NE) 
algorithm  to  inductively  learn  the  CAC  policies, 
which is called CN (Call Admission Control scheme 
using NE). Comparing with the Q-Learning based 
CAC scheme in the constant traffic load shows that 
CN can not only approximate the optimal solution 
very well but also optimize the CBR and CDR in a 
more  flexibility  way.  Additionally  the  simulation 
results  demonstrate  that  the  proposed  scheme  is 
capable of keeping the handoff dropping rate below 
a pre-specified value while still maintaining an ac-
ceptable CBR in the presence of smoothly varying 
arrival rates of traffic, in which the state space is too 
large for practical deployment of the other learning 
scheme.

1  Introduction 
Next Generation Wireless Systems are expected to support 
multimedia services with diverse quality of services (QoS), 
such  as  voice,  video  and  data.  Due  to  the  rapid  growth  in 
mobile users and scarce radio resources, CAC has become 
vital to guarantee the QoS for the multiple services and util-
ize the network resources [Ahmed, 2005]. 

Generally  a  cellular  network  has  a  limited  number  of 
bandwidth  units  (BWU)  or  channels,  which  could  be  fre-
quencies, time slots or codes depending on the radio access 
technique used, viz, FDMA, TDMA, or CDMA respectively. 
Arriving calls are accepted to or rejected from access to the 
network by the CAC scheme based on the predefined policy. 

As a user moves from one cell to another, the call requires 
reallocation of channels in the destination cell. This proce-
dure is called handoff.  If there are no available channels in 
the destination cell, the call may be prematurely terminated 
due  to  handoff  failure,  which  is  highly  undesirable.  The 
potential performance measures in cellular networks are long 
term  revenue,  utility,  CBR  (which  is  calculated  by  the 
number of rejected setup calls divided by the number of setup 
requests) or handoff failure rate (CDR, which is calculated by 
the number of rejected handoff calls divided by the number 
of handoff requests). CDR can be reduced by reserving some 
bandwidth for future handoffs. However, the CBR may in-
crease due to such bandwidth reservation. Hence, reduction 
of CBR and CDR are conflicting requirements, and optimi-
zation of both is extremely complex.

Several  researchers  have  shown  that  the  guard  channel 
threshold policy, which a priori reserves a set of channels in 
each  cell  to  handle  handoff  requests,  is  optimal  for  mini-
mizing the CDR in a non-multimedia situation. However the 
computational complexity of these approaches becomes too 
high in multi-class services with diverse characteristics con-
text, and exact solutions become intractable [Soh and Kim, 
2001]. [Choi, 2002] suggest a bandwidth reservation scheme 
using  an effective mechanisms to predict a mobile terminal 
(MT)’s moving direction and reserve bandwidth dynamically 
based  on  the  accurate  handoff  prediction.  However  this 
scheme  incurs  high  communication  overheads  for  accurate 
prediction of a MT’s movement.

There are a variety of artificial intelligence techniques that 
have been applied to the CAC schemes, such as RL [Senouci, 
2004].  However  RL  can  be  difficult  to  scale  up  to  larger 
domains due to the exponential growth of the number of state 
variables. So in complex real world situations learning time 
is very long [Sutton, 1998].  

This paper proposes a novel approach to solve the CAC in 
multimedia cellular networks with multiple classes of traffic 
with different resource and QoS requirements, and the traffic 
loads can vary according to the time. A near optimal CAC 
policy  is  obtained  through  a  form  of  NE  algorithm  called 
NeuroEvolution  of  Augmenting  Topologies 
(NEAT) 
[Stanley,  2004].  By  prioritizing  the  handoff  calls  and  per-
ceiving the real time CBR and CDR, the proposed learning 

IJCAI-07

186

scheme called CN can be trained offline and the learned best 
policy can dynamically adjust the CAC policy to adapt to the 
varying  of  traffic  loads:  when  the  perceived  CDR  become 
higher, the system will reject more setup calls to decrease the 
CDR, and vice versa.

The  learned  policies  are  compared  to  a  greedy  scheme, 
which  always  accepts  a  call  if  there  is  enough  bandwidth 
capacity to accept this call, and a RL CAC scheme, which is 
similar as the scheme proposed in [Senouci et al., 2004].The 
simulation results shows that our proposed scheme can learn 
very  flexible  and  near  optimal  CAC  policies,  which  can 
maintain  the  QoS  constraints  in  the  presence  of  smoothly 
changing arrival rate of traffic, a scenario for which the state 
space  is  too  large  for  practical  deployment  of  the  other 
learning scheme evaluated.

The paper is organized as follows: Section 2 gives a brief 
introduction to NEAT, which is a form of NE Method, and 
describes how to apply the NEAT to the CAC application; 
section 3 describes the test system model, and formulates the 
fitness function, while section 4 compares the performance in 
constant and time-varying traffic loads.

2  Applying NEAT to CAC 
NEAT is a kind of NE method that has been shown to work 
very efficiently in complex RL problems. NE is a combina-
tion of neural networks and genetic algorithms where neural 
networks (NNs) are the phenotype being evaluated. [Stanley, 
2004]

The NEAT method for evolving artificial NNs is designed 
to  take  advantage  of  neural  network  structure  as  a  way  of 
minimizing  the  dimensionality  of  the  search  space.  The 
evolution starts with a randomly generated small set of NNs 
with  simple  topologies.  Each  of  these  NNs  is  assigned  a 
fitness  value  depending  on  how  well  it  suits  the  solution. 
Once all the members of the population are assigned fitness 
values, a selection process is carried out where better indi-
viduals (high fitness value) stand a greater chance to be se-
lected for the next operation. Selected individuals undergo 
recombination  and  mutation  to  result  in  new  individuals. 
Structural  mutations  add  new  connections  and  nodes  to 
networks in the population, leading to incremental growth. 
The whole process is repeated with this new population until 
some termination criterion is satisfied. [Stanley, 2004] 

This section specifies the main requirements of applying 
NEAT to the CAC application, and describes the work proc-
ess of setting up a connection. 
2.1  Main Requirements   
NEAT can be seen as a black box, which can provide a neural 
network for receiving the inputs and generating the outputs. 
There area several issues need to be considered: 

1. What are the inputs? 
Normally  the  inputs  are  the  perceived  state  of  the  envi-
ronment that is essential to make the action decision.  For the 
CAC problem the perceived state may include the new re-
quest call and all the currently carried connections in the cell.  

2. What are the outputs? 

Generally the outputs are the possible actions that can be 
performed in the real application. In CAC, there are only two 
possible actions: Accept and Reject, so one output is enough. 
We define the output is a real number, and its value is be-
tween 0 and 1, if it is larger than 0.5, then the action selected 
is Accept; otherwise, it is Reject. 

3. How to evaluate each policy and how to formulate the 

fitness function? 

The fitness function determines what is good or bad pol-
icy. A good policy has a high fitness score and so will have a 
higher  probability  to  create  offspring  policies.  The  fitness 
function is described in a later section. 
Internal and External Supervisor 

4.
During the learning period, many NNs are randomly gen-
erated and some of them can lead to very bad performance. 
To prevent damage or unacceptable performance, an Internal 
Supervisor  is  needed  to  filter  out  clearly  bad  performance 
policies. In CAC, NNs that always reject all kinds of calls are 
frequently  generated  and  so  would  be  evaluated.  However 
these  Always  Reject  policies  are  obviously  not  good  and 
evaluation is pointless. So the Internal Supervisor gives their 
fitness score the value of 0.

Additionally, the actions generated by NEAT are not al-
ways  feasible.  For  example,  during  evaluation  the  evolved 
NNs may try to accept a request call when the system is full, 
which is physically unrealizable. Therefore an External Su-
pervisor is added and uses prior knowledge in order to fil-
tering impossible actions due to the system constraints.
2.2  The Work Process of Setting up a connection   
Figure 1 illustrates the work process of setting up a connec-
tion using our scheme.

Input 0 and 1 
Real-time CBR and 
CDR

Input 2 
Ongoing setup calls 
in class I 

Input 3 
Ongoing handover 
calls in class I 
Input 4 
Ongoing setup calls 
in class II 

Input 5 
Ongoing handover 
calls in class II 

Input 6 
The new user request

Update real-time CBR or CDR 

NEAT

Selected NN 

Internal 
Supervisor 

External
Supervisor 

Valid 
Output

population

Cell

User Request 

Figure 1 the process of setting up a connection 

1. The network cell receives a user request event (in this 
case from a network traffic simulator) , perceives the 
network state (such as checking all the ongoing con-
nections in the cell, calculates the real time CBR and 
CDR),  and send the inputs to NEAT; 

IJCAI-07

187

3.

2. The NN currently being evaluated in NEAT generates 
the output according to the received inputs, and sends 
the output to the External Supervisor;
If  the  output  is  invalid  as  determined  by  the  prede-
fined rules in the External Supervisor, a Reject action 
will  be  sent  to  the  cell.  This  occurs  only  when  the 
capacity  of  cell  is  at  its  upper  limit  and  the  output 
decision of the CAC policy is still Accept. In this way 
the  policy  will  not  be  punished  by  negative  fitness 
value for creating impossible actions, because during 
exploration it is very difficult to create a policy that 
can always generate correct actions for any possible 
environment.

4. The cell performs the output action, which means to 

accept or reject the request event.
If the decision is to accept, the cell will allocate the 
requested bandwidth; if the decision is to reject, the 
cell will not take any action. 

5.

3  Fitness Function  
Our objective of CAC is to accept or reject request (setup or 
hand off) calls so as to maximize the expected revenue over 
an infinite planning horizon and meet the QoS constraints, 
and more specifically to trade off CBR and CDR so that the 
specified  CDR  will  not  exceed  a  predefined  upper  bound 
while retaining an acceptable CBR level.

In this paper we only consider non-adaptive services, e.g. 
the  bandwidth  of  a  connection  is  constant.  We  assume  m
classes  of  traffic: {1, 2,... }m .  A  connection  in  class i  con-
sumes ib  units of bandwidth or channels, and the network can 
obtain ir   rewards per logical second by carrying it. We also 
assume  that  traffic  from  each  class  arrives  according  to 
Poisson distribution and their holding time is exponentially 
distributed.  All  the  arrival  distributions  and  call  holding 
distributions  are  independent  of  each  other.  The  arrival 
events  include  new  call  arrival  events  and  handoff  arrival 
events. Since the call departures do not affect the CAC de-
cisions, we only consider the arrival events in the CAC state 
space. Additionally, we only consider a cell with fixed ca-
pacity. The fixed total number of bandwidth (channels) is C .
,i h  as the arrival rates of new setup 
 as the av-

and handover requests in class i ,
erage holding time of setup and handover calls in class  i .
3.1 First Goal: maximize revenue- 1F
If the goal is simply to maximize revenue, the fitness can be 
assessed  by 1F  ,  which  can  be  calculated  from  the  average 
reward obtained per request connection as equation (1). Let 
N  be  the  total  number  of  request  connections  (includes 
setup and handoff calls),  n is the total number of accepted 
connections.    

We denote 

,i s  and 

,i s and

,i h

1

1

  For large N, 

F
1

N

n

R
n

1
N

m

i

0

r k
i

i s
,

1

i s
,
N

k

i h
,

i h
,

1

  (1)

   Here  m  denotes  the  total  number  of  classes  of  traffic, i
denotes  the  class  of  traffic  for  each  individual  connection, 
   denotes  the  number  of  the  accepted  new  setup 
,i sk and ,i hk

calls  and  handover  calls  in  class i .The  relationship  below 
also holds. 

k

i s
,

N

m

(

p
i s
,

and

k

i h
,

N

i s
,

i s
,

)

i h
,

p
i h
,

,

i h
,

m

i

0

(

i s
,

)

i h
,

,i sp

  Where
setup request calls and handover calls in class  i .
the number of accepted setup calls in class i

denotes  the  acceptance  rate  of  new 

,i hp

i

0
and

p
i s
,

request setup calls in class i

p
i h
,

the number of accepted handover calls in class i

request handover calls in class i

To  simplify  the  above  formula,  define  the  service  demand 
parameter
r
i

 as 

and

i h
,

i h
,

i h
,

r
i

i s
,

i s
,

i s
,

1

1

m

m

i s
,

i h
,

i
0
Therefore

F
1

m

i

0

(

p
i s
,

i s
,

p
i h
,

)

i h
,

i s
,

i h
,

0

i
                   (2)   

It can be seen that the total revenue is determined by both 
 and the learned CAC 
,  which  can  be  calculated  after 

of the service demand parameters 
performance
evaluation of each individual policy.

,i sp

,i hp

and

2F .

3.2 Handle CBR and CDR trade-off--
Two  parameters  are  used  to  trade  off  between  CBR  and 
and  the  Penalty  Fitness f .
CDR:  the  Importance  Factor 
The Importance Factor
indicates the preference of the sys-
tem as shown in Equation (3). For a network it is commonly 
more desirable to reject new setup requests and accept more 
handoff  requests,  so  normally ,
i h i h  .  By  carefully 
i s
,
selecting
for each kind of traffic, the expected CAC policy 
can be evolved. The fitness function is shown as below: 

i s
,

,

F

2

m

(

p

i s
,

i s
,

i s
,

p

i h
,

)

i h
,

i h
,

m

(

f

i s
,

f

i h
,

)

       (3) 

i

0

i

0

and cbrT

The other parameters in Equation (3) are the Penalty Fit-
ness f .When the observed CBR or CDR exceeds their pre-
defined upper limit bound ( cdrT
), the evaluated policy 
needs to be punished by a negative fitness score f ,  which 
should be large enough to affect the total fitness 2F . Addi-
tionally,  although  dropping  handoff  calls  is  usually  more 
undesirable  than  blocking  a  new  connection,  designing  a 
system with zero CDR is practically impossible and will lose 
unacceptable  revenue.  In  practice,  an  acceptable  CDR  is 
around 1 or 2%. 
 give an opportunity to prevent 
the network from losing too much revenue by rejecting setup 
calls. Generally, we consider that CDRs are more important 
than CBRs, therefore

i h
,
These  two  parameters

i s
,
 and  f work  cooperatively  to 
trade off between CBRs and CDRs: 
 gives extra weight for 
each different kind of traffic to indicate the preference of the 
can increase the speed to find policies that can 
network,

,i sf and

,i hf

.

f

f

,i hf

IJCAI-07

188

, and 

to trade off between CBRs and CDRs.

decrease CDR under cdrT
cbrT
4  Training Design and Performance Com-

,i sf defines the upper limit of 

parison

This section presents two kinds of training processes: one for 
constant  traffic  load,  another  for  time-varying  traffic  load. 
We use computer simulations to evaluate the performance of 
the learning approach. A single cell within a cellular network 
C
with a limited number of channels
is considered. We 
20
define two classes of traffic labeled 
1C  and 
2C , which require 
bandwidth capacity 1 and 2 respectively. The reward rate is 
r

1/sec/connection
To  evaluate  the  traffic  load  in  each  cell,  we  choose  to 
normalized offered load with respect to the radio bandwidth 
capacity in a wireless cell, which is defined as “Normalized 
offered load” mentioned in [Soh, 2001] . 

for carrying each connection.

L

1
C

2

i

1

1
b
i s i
,

i s
,

1
b
i h i
,

i h
,

The simulation is divided as two periods: a Learning Pe-
riod and an Evaluation Period. During the Learning Period 
CN  is  trained  to  evolve  higher  fitness  policies.  During  the 
evaluation  period,  the  highest  fitness  policy  is  selected  to 
perform CAC.
4.1 Constant traffic load 
In this experiment the CN is compared with a Q Learning RL 
based scheme and a Greedy Policy, all for a constant nor-
malized  offered  load.  The  traffic  parameters  is  defined  in 
Table 1. 

Parameters

1

1
i

1C

2C

Setup  Handover 
0.2 
30 

0.1 
20 

Setup  Handover 
0.1 
20 

0.05 
15 

Table 1.Traffic parameters 

The proposed scheme--CN
Training  in  constant  traffic  loads,  the  service  demand  pa-
doesn’t change, so the fitness is only concerned 
rameter
 . CN requires five 
with the learned CAC policy
inputs. Input 0 indicates the setup utility of different kinds of 
request  event  (the  utility  is  calculated  as  equation(4)),  and 
input 1 to 4 indicates the number of ongoing calls for each 
kinds of traffic carrying in the cell. For better performance 
these inputs are normalized from 0 to 1. 

and ,i hp

,i sp

(reward rate) 

r

(importance factor)

b

Utility

 (bandwidth capacity)

                  (4) 
  Equation (3) is used as the fitness function, and Table 2 
shows  the  value  of  the  related  parameters.  Comparing  the 
2C has  lest  value, 
values  of 
therefore it has the most probability to be rejected. 

i ,  the  setup  calls  of  class 

i

Parameters

i

i

i

i

1C

Setup  Handover 
13.33 

4.44 
10 
44.44 

3 
40 

2C

Setup  Handover 
3.33 
5 

2.22 
80 

16.67 

177.88 

Table 2.CN simulation parameters 

Q s a
,
),( asr

During learning period, NEAT evolves 13 generations, each 
generation had 200 policies, and each policy was evaluated 
for 20,000 events.
RL based CAC scheme 
We  implemented  a  lookup  table  based  Q-Learning  CAC 
scheme (RL-CAC), which is similar to the one proposed in 
[Senouci, 2004]. The Q-function is learned through the fol-
lowing update equation: 
Q s a
,

Q s a
',

Q s a
,

r s a
( , )

'

max
a
'

  The reward 
assess the immediate payoff due to a 
action decision in time t that it is in state  s  and has taken 
action a . Since the network can obtains the rewards by car-
rying each connection in a while, and receive nothing when 
just accept or reject it, the immediate reward is calculated by 
the  total  rewards  obtains  during  the  time  between  two 
consecutive events: one is happened in time t , and another is 
the previous event. 
Learning rate 

1

,

t

1

visit s a
,
t

(

t

t

)

visit s a is  the  total  number  of  times  this 

t

t

t

,

)

(

where 
state-action pair has been visited. 
1.
Discount factor 0
Exploration probability
Important factor 
of traffic, and prioritize the handoff calls. 

1 .

 is used to differentiate each kind 

1,s
5 

1,h
10 

2,s
1 

2,h
80 

Table 3.Important Factor in RL-CAC 

Greedy Policy 
The  system  always  accepts  a  request  if  there  is  enough 
bandwidth capacity to accept the new call. 
Performance Comparison 
Three  CN  schemes  are  compared  with  RL-CAC  and  the 
Greedy  Policy.  The  total  CBR  and  CDR  used  in  Fig  2 
evaluate the total rejected rate for setup and handoff calls. 

As shown in Fig 2, the CN-C01, CN-C02 has similar per-
formance  with  RL-CAC,  which  is  evidence  that  the  CN 
scheme obtains near optimal policy.  

Table 4 and 5 shows that by defining different thresholds 
for  each  kinds  of  traffic,  CN  can  learn  very  flexible  CAC 
policies, and not only trade-off between setup and handoff 
calls but also trade-off between different classes of traffic, 

IJCAI-07

189

which is difficult implemented by RL based CAC Schemes. 
Because for such constraints semi-Markov decision problem 
(SMDP),  the  state  space  should  be  defined  by  QoS  con-
straints,  it  becomes  too  large,  and  some  form  of  function 
approximation is needed to solve the SMDP problem. 

Total CBR and CDR Comparison

%
0
0
1

20.00
18.00
16.00
14.00
12.00
10.00
8.00
6.00
4.00
2.00
0.00

Greedy 
RL-CAC
CN-C01
CN-C02
CN-C03

traffic 

the  changes  of 
loads.  The  simulation  results 
demonstrate that the CBR and CDR can be good candidates. 
Because using any non-adaptive CAC policy, when the traf-
fic load increases the CBR and CDR will become higher, the 
CBR and CDR can be a factor to reflect the traffic loads.

With time-varying traffic loads CN requires seven inputs 
(Figure 1): Input 0 indicates the real-time CBR calculated by 
the number of rejected setup calls divided by the latest 500 
setup requests.  If the number of setup requests is less than 
500, then the number of rejected setup calls is divided by the 
number of setup request; Input 1 indicates the real-time CDR 
calculated in a similar way as input 0; Input 2 to 6 are same as 
the inputs in the experiment with constant traffic loads. 

IF

3

p

1,

s

10

p

1,

h

5

p

2,

s

80

p

2,

h

f               (5) 

TotalCBR

TotalCDR

Figure 2. Total CBR and CDR Comparison

CN-C02 
CN-C03 

cbrT
10% 
20% 

Total CBR 

9.91% 
18.07% 

cdrT
1% 
0.50% 

Total CDR

0.87% 
0.37% 

Table 4.CN schemes comparison 

0.70

0.65

0.60

0.55

0.50

0.45

0.40

0.35

0.30

L
 
s
d
a
o
L

 

d
e
r
e

f
f

 

O
d
e
z

i
l

a
m
r
o
N

1

2

(a)

6

5

4

3
8
Training Intervals

7

L
 
s
d
a
o
L
 
d
e
r
e
f
f

O
 
d
e
z

i
l

a
m
r
o
N

0.70

0.65

0.60

0.55

0.50

0.45

0.40

0.35

0.30

0

9

10

(b)

400

200
800
Evaluation Intervals

600

1000

CAC

Parameters Total
CDR

Total
CBR

Schemes
RL-CAC  CBR or CDR  1.68%  8.33%  1.1%  2.88%  3.49% 18.54%
10% 
Upper limit 
CBR or CDR  1.15%  8.97%  0.79%  1.89%  8.53% 9.91%

CN-C01

10% 

1% 

2% 

No 

No 

1,hC

2,hC

1,sC

1,sC

Table 5. CN and RL-CAC comparison 

 In addition,  Simulation results show that by average CN 
schemes  lost  around  25%  of  revenue  during  learning  by 
filtering clear bad performance policies.
4.2 Time-varying traffic load 
In real world, the traffic load in a cellular system varies with 
time.  A  dynamic  CAC  policy  which  can  adapt  to  the 
time-varying  traffic  loads  is  required:  when  the  handoff 
failure rates grows higher, the system will reject more setup 
calls in order to carry more handoff requests, and vice versa. 
In this dynamic environment, most learning schemes trained 
in constant traffic loads cannot obtain the optimal perform-
ance and maintain the QoS constraints. 

To simplify the problem, only the arrival rates of each kind 
of traffic are changed, the averages of holding times are kept 
as constant during the whole simulation. 

In this dynamic environment, the state space is very large, 
and  until  now  there  is  no  technical  report  to  provide  any 
optimal solution by using RL algorithm. Therefore we only 
compare our schemes with Greedy Policy.

In the time-varying traffic loads, the service demand factor 
 varies according to the time and is difficult to be calculate, 
,  however  it  does  not 
We  use  equation  (5)  to  ignore  the
adapt to the changes of offered loads. In this case the CAC 
scheme requires   feedback from the environment to indicate 

Figure 3 Traffic loads during learning and evaluation 

The  experiment  is  still  divided  into  two  periods:  the 
Learning  Period  and  Evaluation  Period.  In  the  Learning 
Period, each policy is trained with ten different normalized 
offered loads, which is called one Training Session (Figure 
3.a).  The  load  was  generated  by  segmenting  the  Training 
Session  into  10  time  intervals  where  during  each  training 
interval the load was held at the constant level.  The changes 
 follow a sine function as shown in equation(6).  Each 
of
policy is evaluated interval by interval and obtains an interval 
fitness  score  calculated  by  interval  fitness  function  IF as
equation  (5).  After  finishing  the  evaluation  of  the  10th  in-
terval, the ten interval fitness scores are averaged as the final 
fitness of the policy.

 

1

i

1

learning

1

min

1

min

sin(

n
10

)

18

(6)

1

evaluation

1

min

1

min

sin(

n
1000

)

                  (7)

Parameters

1
min
1
i

1C

2C

Setup  Handover 
0.25 
25 

0.125 
20 

Setup  Handover 
0.2 
10 

0.1 
5 

Table 6.The traffic definition in time-varying traffic loads 

The Evaluation Period evaluates the learned best policy. It 
is  divided  into  1000  equal  lengths  of  time  intervals  with 

IJCAI-07

190

different values of arrival rates for each kind of traffic (Fig-
1  again follow a sine function as, 
ure 3.b). The changes of 
which is similar as the one used in the learning phase, but 
sampled  more  finely.  Within  each  evaluation  interval  the 
arrival rate is kept unchanged. Each evaluation interval is as 
long as one training interval. Table 4 shows the traffic defi-
nition.

The  simulation  runs  200,000,000  logical  seconds.  Each 
training or evaluation interval is 10,000 logical seconds with 
around 20,000 request events. NEAT evolves 20 generations, 
and each generation has 100 policies.

 

 

%
0
0
1
R
D
C
d
n
a
R
B
C

 

CN02-CDR
CN02-CBR
CN01-CDR
CN01-CBR
Greedy-CDR(CBR) 

40

35

30

25

20

15

10

5

0

0

200

400

600

800

1000

Evaluation Intervals

Figure  4.Comparison  of 
interval  CBR  and  CDR  with 
time-varying traffic loads. At the end of each evaluation interval, 
the total CBR and CDR is calculated and compared. 

Two CN schemes are compared: 

01CN only specified the 
02CN specified both of the 
threshold for CDR, which is 2%; 
CDR  (2%)  and  CBR  (varying  according  to  the  different 
training intervals).

Evaluation Period   Greedy 
1.39 
1.44 

Total CBR 
Total CDR 

CN01 
5.11 
0.33 

CN02 
7.61 
0.51 

Table 7.Comparison of CBR and CDR 

Figure 4 shows the comparison of CBR and CDR for dif-
ferent  CAC  schemes.  We  can  see  that  with  time-varying 
traffic loads CN can successfully maintains the CDR under a 
predefined level. Additionally from Table 7, it can be seen 
that the CN01 only defines the threshold for CDR therefore 
its CBR is higher, the system will lost more revenue by re-
jecting too many setup calls, however CN02 uses threshold 
for both of the CBR and CDR to force the system to decrease 
the CBR so as to successfully trade-off between the CBR and 
CDR.

In  this  experiment,  the  highest  fitness  NNs  evolved  by 
NEAT are very complex and non-standard, and it is impos-
sible to specify any particular hidden layers, there are sets of 
hidden  nodes  connected  with  each  other  by  negative  or 
positive weights.

5  Conclusion  
This paper proposes a novel learning approach to solve the 
CAC in multimedia cellular networks with multiple classes 
of traffic. The near optimal CAC policy is obtained through a 
form of NeuroEvolution (NE) algorithm.

Simulation  results  show  that  the  new  scheme  can  effec-
tively guarantee the specified CDRs remain under their pre-
defined upper bound while retaining acceptable CBRs. Ad-
ditionally our proposed scheme can learn very flexible CAC 
policies  by  defining  different  thresholds  for  each  kind  of 
traffic, whereas RL based CAC scheme can not.

Furthermore this scheme can learn dynamic CAC policies 
to adapt to changes of traffic loads. No matter what the traffic 
load is, the CAC scheme is trained to reject more setup calls 
to decrease the CDR when the CDR exceeds the prescribed 
upper limit, and vice versa. Therefore if the scheme is trained 
carefully according to the traffic instance during a day in a 
network element, the highest fitness policy can be used di-
rectly in the real world. 

Acknowledgments
Thanks Kenneth O. Stanley and Ugo Vieruccifor very much 
for  providing  free  software  of  NEAT  in  the  website 
http://www.cs.utexas.edu/~nn/index.php.

References
[Ahmed, 2005] Mohaned Hossam Ahmed. Call Admission 
Control in Wireless Networks: A Comprehensive Survey.
Memorial  University  of  Newfoundland,  IEEE  Commu-
nications Surveys&Tutorials,2005. 

[Choi  et  al.,  2001]  Sunghyun  Choi,  and  Kang  G.  Shin. 
Adaptive bandwidth reservation and admission control in 
QoS-sensitive  cellular  networks.  In  IEEE  Transactions 
on parallel and distributed systems, Vol. 13, September 
2002.

 [Ho et al.,  1999] Chi-Jui Ho and Chin-Tau Lea. Improving 
call  admission  policies  in  wireless  networks.  Wireless
Networks, 5(1999):257–265, 1999. 

 [Senouci et al., 2004] S.M. Senouci, A.Beylot, G. Pujolle. 
Call  Admission  Control  in  Cellular  Networks:  A  Rein-
forcement  Learning  Solution.  International  journal  of 
network management, page 89-103, 2004. 

[Soh  and  Kim,  2001]  Wee-Seng  Soh  and  Hyong  S.  Kim. 
Dynamic  guard  bandwidth  scheme  for  wireless  broad-
vol.1,  pages 
band  networks”. 
572-581,2001. 0-7803-7016-3/01. 

INFCOM 

IEEE 

[Stanley, 2004] Kenneth O. Stanley. Efficient Evolution of 
Neural Networks through Complexification. PhD thesis.
Artificial  Intelligence  Laboratory,  the  University  of 
Texas at Austin, TX 78712, 2004 

[Sutton  and  Barto,  1998]  Richard  S.  Sutton,  Andrew  G. 
Barto.  Reinforcement  Learning:  An  Introduction.  MIT 
Press, Cambridge, MA, 1998 

IJCAI-07

191


Taming Decentralized POMDPs:  Towards Efficient Policy Computation for 

Multiagent Settings 

R. Nair  and  M. Tambe 
Computer Science Dept. 

University of Southern  California 

Los Angeles CA 90089 
{nair,tambe}@usc.edu 

M.  Yokoo 

Coop. Computing Research Grp. 

D. Pynadath, S. Mar sella 

Information Sciences Institute 

NTT Comm. Sc. Labs 
Kyoto, Japan 619-0237 

yokoo@cslab.kecl.ntt.co.jp 

University of Southern California 

Marina del Rey CA 90292 

{pynadath, marsella}@isi.edu 

Abstract 

The problem of deriving joint policies  for a group 
of agents  that  maximize  some joint  reward  func(cid:173)
tion  can  be  modeled  as  a  decentralized partially 
observable  Markov  decision  process  (POMDP). 
Yet,  despite  the  growing  importance  and  applica(cid:173)
tions of decentralized POMDP models in the mul-
tiagents  arena,  few  algorithms  have  been  devel(cid:173)
oped for efficiently deriving joint policies for these 
models.  This  paper  presents  a  new  class  of lo(cid:173)
cally optimal algorithms called "Joint Equilibrium-
based search for policies (JESP)". We first describe 
an  exhaustive  version  of  JESP  and  subsequently 
a  novel  dynamic programming approach to  JESP. 
Our  complexity  analysis  reveals  the  potential  for 
exponential speedups due to the dynamic program(cid:173)
ming approach.  These theoretical  results  are  ver(cid:173)
ified  via  empirical  comparisons  of the  two  JESP 
versions with  each other and with a globally opti(cid:173)
mal brute-force search algorithm. Finally, we prove 
piece-wise  linear  and  convexity  (PWLC)  proper(cid:173)
ties,  thus  taking  steps  towards  developing  algo(cid:173)
rithms for continuous belief states. 

Introduction 

1 
As multiagent systems move out of the research lab into crit(cid:173)
ical  applications such  as  multi-satellite  control,  researchers 
need  to provide high-performing,  robust multiagent designs 
that are as nearly optimal as feasible. To this end, researchers 
have increasingly resorted to decision-theoretic models as a 
framework in which to formulate and evaluate multiagent de(cid:173)
signs.  Given a group of agents, the problem of deriving sep(cid:173)
arate policies for them that maximize some joint reward can 
be modeled as a decentralized POMDP (Partially Observable 
Markov Decision Process).  In particular, the DEC-POMDP 
(Decentralized POMDP)  [Bernstein et al., 2000]  and MTDP 
(Markov  Team  Decision  Problem  [Pynadath  and  Tambe, 
2002])  are  generalizations  of a  POMDP  to  the  case  where 
there are multiple, distributed agents basing their actions on 
their separate observations.  These frameworks allow a vari(cid:173)
ety  of multiagent  analysis.  Of particular  interest  here,  they 
allow us to formulate what constitutes an optimal policy for a 
multiagent system and in principle derive that policy. 

However,  with  a  few  exceptions,  effective  algorithms for 
deriving  policies  for  decentralized  POMDPs  have  not  been 
developed. Significant progress has been achieved in efficient 
single-agent  POMDP  policy  generation  algorithms  [Mona-
han,  1982;  Cassandra  et  al,  1997;  Kaelbling  et  al,  1998]. 
However,  it  is  unlikely  such  research  can  be  directly  car(cid:173)
ried  over  to  the  decentralized  case.  Finding  optimal  poli(cid:173)
cies  for  decentralized  POMDPs  is  NEXP-complete  [Bern(cid:173)
stein et al, 2000].  In contrast, solving a POMDP is PSPACE-
complete [Papadimitriou and Tsitsiklis,  1987].  As Bernstein 
et al.  [2000] note, this suggests a fundamental difference in 
the nature of the problems.  The decentralized problem can(cid:173)
not be treated as one of separate POMDPs in which individ(cid:173)
ual  policies  can  be  generated  for  individual  agents because 
of possible  cross-agent interactions  in  the  reward,  transition 
or observation functions.  (For any  one action of one agent, 
there may be many different rewards possible, based on the 
actions  that  other agents may  take.)  In  some  domains,  one 
possibility is to simplify the nature of the policies considered 
for  each  of the  agents.  For  example,  Chades  et  al.  [2002] 
restrict  the  agent policies  to  be  memoryless  (reactive) poli(cid:173)
cies.  Further,  as  an  approximation,  they  define  the  reward 
function and the transition function over observations instead 
of over  states  thereby  simplifying  the  problem  to  solving  a 
multi-agent MDP  [Boutilier,  1996].  Xuan et al.  [2001]  de(cid:173)
scribe how to derive decentralized MDP (not POMDP) poli(cid:173)
cies from a centralized MDP policy.  Their algorithm, which 
starts with an assumption of full communication that is grad(cid:173)
ually relaxed, relies on instantaneous and noise free commu(cid:173)
nication.  Such simplifications reduce the applicability of the 
approach and essentially side-step the question of solving de(cid:173)
centralized POMDPs.  Peshkin et al.  [2000]  take a different 
approach by  using gradient descent  search to  find  local  op(cid:173)
timum  finite-controllers  with  bounded memory.  Their algo(cid:173)
rithm finds locally optimal  policies  from  a  limited subset of 
policies,  with  an  infinite  planning horizon,  while  our algo(cid:173)
rithm finds locally  optimal policies  from an  unrestricted set 
of possible policies, with a finite planning horizon. 

Thus,  there  remains  a  critical  need  for  new  efficient 
algorithms  for  generating  optimal  policies  in  distributed 
POMDPs.  In this paper, we present a new class of algorithms 
for solving decentralized POMDPs, which we refer to as Joint 
Equilibrium-based Search for Policies (JESP).  JESP iterates 
through the agents,  finding  an optimal policy for each agent 

MULTIAGENT SYSTEMS 

705 

assuming the policies of the  other agents are  fixed.  The  it(cid:173)
eration continues until no improvements to the joint reward 
is  achieved.  Thus JESP  achieves  a  local  optimum  similar 
to a Nash Equilibrium.  We discuss Exhaustive-JESP which 
uses exhaustive search to find the best policy for each agent. 
Since this exhaustive search for even a single agent's policy 
can be very expensive, we also present DP-JESP which im(cid:173)
proves on Exhaustive-JESP by using dynamic programming 
to  incrementally derive  the policy.  We  conclude with  sev(cid:173)
eral  empirical  evaluation that contrast JESP against a glob(cid:173)
ally optimal algorithm that derives the globally optimal pol(cid:173)
icy via a full search of the space of policies. Finally, we prove 
piece-wise linear and convexity (PWLC) properties, thus tak(cid:173)
ing steps towards developing algorithms for continuous initial 
belief states. 

2  Model 
We  describe 
(MTDP) 
lem 
work 
in  detail  here 
tion  of  a  decentralized  POMDP  model. 
other  decentralized  POMDP  models 
tially  also  serve  as  a  basis 
Xuan et al.,2001]. 

the  Markov  Team  Decision  Prob(cid:173)
frame-
[Pynadath  and  Tambe,  2002] 
illustra(cid:173)
to  provide  a  concrete 
However, 
could  poten(cid:173)
[Bernstein  et  ai,  2000; 

Given  a  team  of  n  agents,  an  MTDP  [Pynadath  and 

Tambe,  2002]  is  defined  as  a  tuple: 
5  is  a  finite  set  of  world  states 

A  = 
,  where  A1,...,  An,  are  the  sets  of action  for 
agents  1  to n.  A joint action is represented as  (a1,...,  a„). 
P(s i,(a 1,...,a n)  , s f ),  the  transition  function,  represents 
the  probability  of  the  current  state  is  .sf,  if  the  previous 
state  is  Si  and  the  previous  joint  action  is  (o 1,...,a„). 
are  the  set  of observations  for  agents  1  to  n. 
0(s, (a 1,... ,a n) ,u)  ,  the  observation function,  represents 
the  probability  of joint  observation 
if the  current state  is 
$  and the previous joint action  is ( a 1 , . . ., a n).  The agents 
receive  a  single,  immediate  joint  reward  R(s,a 1,..  . ,a n) 
which is shared equally. 

Practical analysis using models like MTDP often assume 
that observations of each agent is independent of each other's 
observations. Thus the observation function can be expressed 
as 0(s, ( a i , . . ., a n ),  
• . . .• 

(au...  , a „ ),  

=  O1(s, 

lOpenRight',  'Listen'}.  The  transition 

tiger problem used in illustrating single agent POMDPs[Kael-
bling et ai,  1998] and create an MTDP ((S, A, P,  O,  R)) 
for this  example. 
In  our  modified  version,  two  agents  are 
in  a  corridor  facing  two  doors:"left"  and  "right".  Behind 
one  door  lies  a  hungry tiger  and  behind  the  other  lies  un(cid:173)
told  riches  but  the  agents  do  not  know  the  position  of ei(cid:173)
ther.  Thus,  5  =  {SL,SR},  indicating  behind  which  door 
the  tiger  is  present.  The  agents  can jointly  or  individually 
open either door.  In addition, the agents can independently 
listen  for  the  presence  of the  tiger.  Thus,  A1  =  A2  — 
{'OpenLeft', 
func(cid:173)
tion  P,  specifics  that  every  time  either agent  opens  one  of 
the doors,  the  state  is  reset to  SL  or SR with  equal  proba(cid:173)
bility,  regardless of the  action  of the  other agent,  as  shown 
in Table  1.  However, if both agents listen, the state remains 
unchanged.  After every action each agent receives an obser(cid:173)
vation about the new state.  The observation function,  01  or 
02  (shown in Table 2) will  return either HL or HR with dif(cid:173)
ferent probabilities depending on the joint action taken and 
the resulting world state.  For example,  if both agents listen 
and the tiger is behind the left door (state is SL), each agent 
receives  the  observation  HL  with  probability  0.85  and  HR 
with probability 0.15. 

Table 1: Transition function 

Table 2: Observation function for each agent 

Each agent i chooses its actions based on its local policy, 
7Tj, which is a mapping of its observation history to actions. 
Thus,  at  time  t,  agent  i  will  perform action 
where 
refers to the joint policy 
of the team of agents.  The important thing to note is that in 
this model,  execution is distributed but planning is central(cid:173)
ized.  Thus agents don't know each other's observations and 
actions at run-time but they know each other's policies. 

3  Example Scenario 
For  illustrative  purposes  it  is  useful  to  consider  a  familiar 
and simple example, yet one that is capable of bringing out 
key difficulties in  creating optimal policies for MTDPs.  To 
that  end,  we  consider  a  multiagent  version  of the  classic 

If either  of them  opens  the  door  behind  which  the  tiger 
is present, they are both attacked (equally) by the tiger (sec 
Table  3).  However,  the  injury  sustained  if they  opened  the 
door to the tiger is  less severe if they open that door jointly 
than  if they  open  the  door  alone.  Similarly,  they  receive 
wealth which they share equally when they open the door to 
the riches in proportion to the number of agents that opened 
that door.  The agents incur a small  cost for performing the 
'Listen'  action. 

Clearly,  acting jointly  is  beneficial  (e.g.,  A1  —  A2  = 
'OpenLeft') because the agents receive more riches and sus(cid:173)
tain less damage by acting together.  However,  because the 
agents receive  independent observations (they  do  not  share 
observations), they need to consider the observation histories 
of the other agent and what action they are likely to perform. 

706 

MULTIAGENT SYSTEMS 

Table 3:  Reward function A 

We  also  consider consider another case  of the  reward  func(cid:173)
tion, where we vary the penalty for jointly opening the door 
to the tiger (See Table 4). 

Table 4:  Reward function B 

4  Optimal Joint Policy 

When agents do not share all of their observations, they must 
instead coordinate by  selecting policies that are  sensitive  to 
their teammates'  possible beliefs,  of which each agent's en(cid:173)
tire  history  of observations provides some  information.  The 
problem  facing  the  team  is  to  find  the  optimal joint  policy, 
i.e.  a combination of individual agent policies that produces 
behavior that maximizes the team's expected reward. 

One  sure-fire  method  for  finding  the  optimal joint  policy 
is to simply search the entire space of possible joint policies, 
evaluate  the  expected reward  of each,  and  select  the  policy 
with the highest such value.  To perform such a search,  we 
must first be able to determine the expected reward of a joint 
policy. We compute this expectation by projecting the team's 
execution over all possible branches on different world states 
and different observations.  We present here the 2-agent ver(cid:173)
sion  of this  computation, but the results easily generalize to 
arbitrary team sizes.  At each time step, we can compute the 
expected  value  of a joint policy, 
for a  team 
starting in a given state, 
with a given set of past observa(cid:173)
tions, 

as follows: 

and 

At  each  time  step, 

the  computation  of 

performs 
a  summation  over  all  possible  world  states  and  agent 
observations,  so  the  time  complexity  of  this  algorithm 
is 
The  overall  search  performs 
this  computation  for  each  and  every  possible  joint  pol(cid:173)
icy.  Since  each  policy  specifies  different  actions  over pos(cid:173)
sible  histories  of observations,  the  number of possible  poli(cid:173)
The 
cies  for  an  individual  agent 
number  of  possible  joint  policies  for  n  agents 
thus 
O 
correspond  to 
the largest individual action and observation spaces, respec(cid:173)
tively,  among  the  agents.  The  time  complexity  for find(cid:173)
ing the  optimal joint  policy by  searching this  space  is  thus: 
O  

is  O 

,  where 

and 

is 

5  Joint Equilibrium-based Search for Policies 
Given  the  complexity  of exhaustively  searching  for  the  op(cid:173)
timal joint  policy,  it  is  clear that  such  methods  will  not  be 
successful  when  the  amount  of time  to  generate  the  policy 
is restricted.  In this section, we will present algorithms that 
are guaranteed to find a locally optimal joint policy. We refer 
to this category of algorithms as "JESP" (Joint Equilibrium-
Based Search for Policies). Just like the solution in Section 4, 
the solution obtained using JESP  is a Nash equilibrium.  In 
particular  it  is  a  locally  optimal  solution  to  a  partially  ob(cid:173)
servable  identical payoff stochastic game(POIPSG)  [Peshkin 
et  al.,  2000].  The  key  idea  is  to  find  the  policy  that maxi(cid:173)
mizes the joint expected reward for one agent at a time, keep(cid:173)
ing  the  policies  of all  the  other  agents  fixed.  This  process 
is repeated until an equilibrium is reached (local optimum is 
found). The problem of which optimum the agents should se(cid:173)
lect when there are multiple local optima is not encountered 
since planning is centralized. 

5.1  Exhaustive  approach(Exhaustive-JESP) 
The  algorithm  below  describes  an  exhaustive  approach  for 
JESP.  Here we consider that there are n cooperative agents. 
We modify the policy of one agent at a time keeping the poli(cid:173)
cies  of the  other  n  -  1  agents  fixed.  The  function  b e s t-
P o l i c y, returns the joint policy that maximizes the expected 
joint  reward,  obtained  by  keeping  n  —  1  agents'  policies 
fixed and  exhaustively  searching  in  the  entire  policy  space 
of the  agent  whose  policy  is  free.  Therefore  at  each  itera(cid:173)
tion, the value of the modified joint policy will  always either 

MULTIAGENT SYSTEMS 

707 

increase or remain unchanged. This is repeated until an equi(cid:173)
librium  is  reached,  i.e.  the policies  of all  n  agents  remains 
unchanged. This policy is guaranteed to be a local maximum 
since the value of the new joint policy at each iteration is non-
decreasing. 

Algorithm 1  EXHAUSTIVE-JESPQ 

1: 
2: 
3: 
4: 
5: 
6: 
7: 
8: 
9: 
10: 
11: 
12: 
13: 

random joint policy, conv 

0 

prev 
while conv 

n  -  1  do 

for  i 

1 to n do 

fix  policy of all agents except i 
policy  Space 
new 
if new.value  =  prev.value then 

list of all  policies  for i 
bestPolicy(i,policySpace,prev) 

conv  conv +  1 

else 

prev 

new, conv 
if conv  =  n  -  1  then 

0 

break 

return new 

The  best  policy  cannot  remain  unchanged  for  more  than 
n  —  1  iterations  without convergence being  reached and  in 
the worst case, each and every joint policy is the best policy 
for at least one iteration.  Hence, this algorithm has the same 
worst case complexity as the exhaustive search for a globally 
optimal policy.  However, it could do much better in practice 
as illustrated in Section 6. Although the solution found by this 
algorithm  is  a  local  optimum,  it may  be  adequate  for some 
applications.  Techniques  like  random  restarts  or simulated 
annealing can be applied to perturb the solution found to see 
if it settles on a different higher value. 

The  exhaustive  approach  to  Steps  5  and  6  of  the 
Exhaustive-JESP  algorithm  enumerates  and  searches  the 
entire  policy  space  of  a  single  agent, 
There  are 

i. 

Os

u

ch  policies,  and  evaluating  each  incurs 

a time  complexity  of  O 
Thus,  using the ex(cid:173)
haustive approach incurs an overall time complexity in Steps 
5 and 6 of:  O 
.  Since we incur this 
complexity  cost  in  each  and  every  pass  through  the  JESP 
algorithm,  a  faster  means  of performing  the  b e s t P o l i cy 
function call of Step 6 would produce a big payoff in overall 
efficiency.  We describe a dynamic programming alternative 
to this exhaustive approach for doing JESP next. 
5.2  Dynamic Programming (DP-JESP) 
If we  examine the  single-agent  POMDP  literature  for  inspi(cid:173)
ration, we find algorithms that exploit dynamic programming 
to incrementally construct the best policy, rather than simply 
search the entire policy space [Monahan, 1982; Cassandra et 
al.,  1997; Kaelbling et a/.,  1998].  These algorithms rely on 
a principle of optimality that states that each sub-policy of an 
overall optimal policy must also be optimal.  In other words, 
if we  have  a  T-step  optimal  policy,  then,  given  the  history 
over the first t steps, the portion of that policy that covers the 
last T — t steps must also be optimal over the remaining T -t 

steps.  In this section, we show how we can exploit an anal(cid:173)
ogous optimality property in the multiagent case to perform 
more  efficient  construction  of the  optimal  policy  within  our 
JESP algorithm. 

To  support  such  a  dynamic-programming  algorithm,  we 
must  define  belief states  that  summarize  an  agent's  history 
of past  observations,  so that they allow the agents to  ignore 
the actual history of past observations, while still  supporting 
construction  of the  optimal  policy  over  the  possible  future. 
In the  single-agent case,  a belief state that stores the  distri(cid:173)
=,  is a sufficient statistic, because 
bution, 
the  agent  can  compute  an  optimal  policy  based  on 
without having  to consider the  actual  observation sequence, 

[Sondik, 1971]. 
In the multiagent case, an agent faces a complex but nor(cid:173)
mal single-agent POMDP if the policies of all other agents are 
fixed.  However, 
is not sufficient, because the agent 
must also reason about the action selection of the other agents 
and  hence  on  the  observation  histories  of the  other  agents. 
Thus, at each time t, the agent i reasons about the tuple  — 
is 
the joint observation histories  of all  the  agents  except i.  By 
treating e\ to be the state of the agent i at time t, we can define 
the transition function and observation function for the single 
agent POMDP for agent i as follows: 

where 
for all agents except  . 

is the joint policy 

We  now  define  the  novel  multiagent  belief state  for  an 
agent  i  given  the  distribution  over  the  initial  state,  b(s)  = 
Pr(S1  =  s): 

(4) 
In other words, when reasoning about an agent's policy in 

the context of other agents, we maintain a distribution over 
rather than simply the current state.  Figure  1  shows different 
belief states  B1  ,  B2  and 
for agent  1  in the tiger domain. 
For instance,  B2  shows probability distributions  over 
.  In 
=  (SL,  (HR))9  (HR)  is  the  history  of agent  2's  observa(cid:173)
tions while SL is the current state.  Section 5.3 demonstrates 
how we can use this multiagent belief state to construct a dy(cid:173)
namic program that incrementally constructs the optimal pol(cid:173)
icy  for agent i. 

The  Dynamic  Programming A l g o r i t hm 

5.3 
Following  the  model  of the  single-agent  value-iteration  al(cid:173)
gorithm, our dynamic program centers around a value func(cid:173)
tion  over a T-step  finite  horizon.  For readability,  this  sec(cid:173)
tion  presents  the  derivation  for the  dynamic program in  the 

708 

MULTIAGENT  SYSTEMS 

For 
is 
obtained using Equations 2 and 3 and Bayes Rule and is given 
as follows: 

the updated 

Figure 1: Trace of Tiger Scenario 

two-agent case;  the  results  easily  generalize to  the  n-agent 
case.  Having fixed the policy of agent 2, our value function, 
Vt{Bt), 
represents the expected reward that the team will re(cid:173)
ceive when  agent  1  follows its optimal policy  from the t-th 
step  onwards when  starting  with  a  current belief state, 
We start at the end of the time horizon (i.e., t = T), and then 
work our way back to the beginning. Along the way, we con(cid:173)
struct  the  optimal policy  by  maximizing  our value  function 
over possible action choices: 

We can define the action value function, 

recursively: 

The first term in equation 6 refers to the expected immediate 
reward, while the second term refers to the expected future re(cid:173)
ward, 
is the belief state updated after performing action 
a1  and observing 
.  In the base case, t  —  T, the future 
reward is 0, leaving us with: 

(7) 
The  calculation  of expected  immediate  reward  breaks  down 
as follows: 

Thus, we can compute the immediate reward using only the 
agent's current belief state  and the primitive elements of our 
given MTDP model (See Section 2). 

Computation  of the  expected  future  reward  (the  second 
term  in  Equation 6)  depends  on  our ability to  update  agent 
1 's belief state from B1
in light of the new observa(cid:173)
For example,  in Figure  1,  the belief state  B1  is 
tion, 
updated to 
on performing action a1 and receiving obser(cid:173)
vation 
.  We now derive an algorithm for performing such 
an update,  as well  as computing the  remaining 
term  from  Equation 6.  The  initial belief state  based on the 
distribution over initial state, 6, is: 

t to 

(9) 

We 

treat 

as  a  normalizing  constant 

the  denominator  of  Equation  10 

(10) 
(i.e., 
to  bring 
Pr 
the  sum  of the  numerator  over  all 
to  be  1.  This  result 
also  enters  into  our  computation  of future  expected  reward 
in  the  second  term  of Equation  6.  Thus,  we  can  compute 
the agent's new belief state  (and the  future expected reward 
and the overall value function, in turn) using only the agent's 
current belief state  and  the  primitive  elements  of our given 
MTDP model.  Having computed the overall value function, 
Vt,  we  can  also  extract  a  form  of the  optimal  policy, 
that maps observation histories into  actions,  as  required by 
Equations 8 and 10. 

Algorithm 2  presents the pseudo-code  for our overall  dy(cid:173)
namic  programming  algorithm.  Lines  1-6  generate  all  of 
the  belief states  reachable  from  a  given  initial  belief state, 
.  Since there is a possibly unique belief 
state for every seauence of actions and observations by agent 
1, there are O 
reachable  belief states.  This 
reachability analysis uses our belief update procedure (Algo(cid:173)
rithm  3),  which  itself has  time  complexity 
when  invoked  on  a  belief state  at  time  t.  Thus,  the  over(cid:173)
all  reachability  analysis  phase  has  a  time  complexity  of 
O 
Lines 7-22 perform the heart 
of our  dynamic  programming  algorithm,  which  also  has  a 
time  complexity  of  0(  
.  Lines 23-
27 translate the resulting value function into an agent policy 
defined  over  observation  sequences,  as  required by  our  al(cid:173)
gorithm (i.e., the 
argument).  This last phase has a lower 
time  and space complexity,  O 
, than our 
other two phases,  since  it considers only optimal actions for 
agent  1.  Thus, the overall time complexity of our algorithm 
is O 
.  The space complexity of the 
resulting value function and policy is essentially the product 
of the  number  of reachable  belief states  and  the  size  of our 
belief state representation:  O   

5.4  Piecewise  Linearity  and  Convexity  of Value 

Function 

Algorithm 2  computes a value  function over only those  be(cid:173)
lief states that are reachable  from  a given initial belief state, 
which is a subset of all possible probability distributions over 
St  and 
To use dynamic programming over the entire set, 
we  must  show  that  our  chosen  value  function  is  piecewise 
linear and convex (PWLC). Each agent is faced with solving 
a  single  agent  POMDP  is  the  policies  of all  other agents  is 
fixed as shown in Section 5.2.  Sondik [1971] showed that the 
value function for a single  agent  POMDP  is PWLC.  Hence 
the value function in Equation 5 is PWLC. Thus, in addition 

MULTIAGENT SYSTEMS 

709 

to supporting the more efficient dynamic programming of Al(cid:173)
gorithm  2,  our  novel  choice  of belief state  space  and  value 
function can potentially support a dynamic programming al(cid:173)
gorithm  over  the  entire  continuous  space  of possible  belief 
states. 

6  Experimental  Results 
In  this  section,  we  perform  an  empirical  comparison  of the 
algorithms described in Sections 4 and 5 using the Tiger Sce(cid:173)
nario (Sec Section 3) in terms of time and performance.  Fig(cid:173)
ure  2,  shows the results of running the globally optimal al(cid:173)
gorithm and the Exhaustive JESP algorithm for two different 
reward  functions (Tables  3  and 4.  Finding the globally op(cid:173)
timal  policy  is  extremely slow  and  is  doubly  exponential  in 
the  finite  horizon, T and so we evaluate the algorithms only 
for  finite  horizons  of 2  and  3.  We  ran  the  JESP  algorithm 
for  3  different  randomly  selected  initial  policy  settings  and 
compared the performance of the  algorithms  in  terms  of the 
number of policy evaluations (on Y-axis using log scale) that 
were necessary.  As can be seen from this figure, for T  —  2 
the JESP algorithm requires much fewer evaluations to arrive 
at an equilibrium. The difference in the run times of the glob(cid:173)
ally optimal algorithm and the JESP algorithm is even more 
apparent when T  =  3.  Here  the  globally  optimal  algorithm 
performed 
4 million policy evaluations while the JESP al(cid:173)
gorithm did 
7000 evaluations.  For the reward function A, 
JESP  succeeded  in  finding  the  globally  optimal  policies  for 
both T  =  2 (expected reward =  - 4) and 3 (expected reward 
=  —6).  However, this is not always the case.  Using reward 
function B  for T  =  2,  the JESP algorithm sometimes settles 
on  a  locally  optimal  policy  (expected  reward  =  - 4)  that  is 
different  from  the  globally  optimal  policy  (expected  reward 
=  20).  However, when random restarts are used, the globally 
optimal reward can be obtained. 

Based  on  Figure  2,  we  can  conclude  that  the  exhaustive 
JESP algorithm performs better than an exhaustive search for 
the globally optimal policy but can some times settle on a pol(cid:173)
icy  that  is only  locally  optimal.  This  could be  sufficient for 
problems  where  the  difference  between  the  locally  optimal 
policy's value and the globally optimal policy's value is small 
and it is imperative that a policy be  found quickly.  Alterna(cid:173)
tively, the JESP algorithm could be altered so that it doesn't 
get stuck in a local optimum via random restarts. 

Table 5 compares presents experimental results from com(cid:173)
parison  of exhaustive  JESP  with  our dynamic  programming 
approach (DP-JESP).  These  results,  also  from  the tiger do(cid:173)
main,  show run-time in  milliseconds (ms)  for the two  algo(cid:173)
rithms  with  increasing horizon.  DP-JESP  is  seen  to  obtain 
significant speedups over exhaustive-JESP.  For time horizon 
of 2 and 3 DP-JESP run time is essentially 0 ms, compared to 
the significant run times of Exhaustive-JESP. As we increased 
the horizon to 
3, we could not run exhaustive-JESP at all; 
while DP-JESP could be easily run up to horizon of 7. 

7  Summary  and  Conclusion 
With  the  growing  importance  of decentralized  POMDPs  in 
the multiagents arena, for both design and analysis, it is crit-
ical  to develop efficient algorithms for generating joint poli-

710 

MULTIAGENT SYSTEMS 

[Chades et al., 2002]  I. Chades, B. Scherrer, and F. Charpil-
let. A heuristic approach for solving decentralized-pomdp: 
Assessment on the pursuit problem. In SAC, 2002. 

[Kaelbling et al., 1998]  L.  Kaelbling,  M.  Littman,  and 
A. Cassandra.  Planning and acting in partially observable 
stochastic domains. Artificial Intelligence, 101, 1998. 

[Monahan, 1982] G. Monahan. A survey of partially observ(cid:173)
able markov decision processes: Theory, models and algo(cid:173)
rithms. Management Science, 101(1): 1-16, January 1982. 
[Papadimitriou and Tsitsiklis, 1987]  C.  Papadimitriou  and 
Complexity  of  markov  decision  pro(cid:173)
J.  Tsitsiklis. 
cesses.  Mathematics ofOperatios Research, 12(3):441-
450, 1987. 

[Peshkinet  al.,2000]  L.  Peshkin,  N.  Meuleau,  K.  E.  Kim, 
and L. Kaelbling. Learning to cooperate via policy search. 
In UAI, 2000. 

[Pynadath and Tambe, 2002]  D.  Pynadath  and  M.  Tambe. 
The  communicative  multiagent  team  decision  problem: 
Analyzing teamwork theories and models. JAIR, 2002. 

[Sondik, 1971]  Edward J.  Sondik.  The  optimal  control  of 
partially observable markov processes. Ph.D. Thesis, Stan-
ford, 1971. 

[Xuan et al., 2001]  P.  Xuan,  V.  Lesser,  and  S.  Zilberstein. 
Communication decisions  in  multiagent cooperation.  In 
Agents-01, 2001. 

Figure 2: Evaluation Results 

Table 5: Run time(ms) for various T with Pentium 4, 2.0GHz, 
1GB memory, Linux Redhat 7.1, Allegro Common Lisp 6.0 

cies.  Yet,  there  is  a  significant  lack  of such efficient algo(cid:173)
rithms.  There are three novel contributions in this paper to 
address this shortcoming.  First, given the complexity of the 
exhaustive policy search algorithm — doubly exponential in 
the  number  of agents  and  time  —  we  describe  a  class  of 
algorithms called "Joint Equilibrium-based Search for Poli(cid:173)
cies"  (JESP)  that  search  for a  local  optimum  rather than  a 
global optimum. In particular, we provide detailed algorithms 
for  Exhaustive JESP  and dynamic programming JESP(DP-
JESP). Second, we provide complexity analysis for DP-JESP, 
which illustrates a potential for exponential speedups over ex(cid:173)
haustive JESP.  We have implemented all  of our algorithms, 
and  empirically  verified  the  significant  speedups  they  pro(cid:173)
vide.  Third, we provide a proof that the value function for 
individual agents is piece-wise linear and convex (PWLC) in 
their belief states.  This key result could pave the way to a 
new  family  of algorithms  that  operate  over continuous  be(cid:173)
lief states, increasing the range of applications that can be at(cid:173)
tacked via decentralized POMDPs, and is now a major issue 
for our future work. 
Acknowledgments 
We thank Piotr Gmytrasiewicz for discussions related to the 
paper.  This research was supported by NSF grant  0208580 
and DARPA award no. F30602-98-2-0108. 

References 
[Bernstein et al, 2000]  D.  Bernstein,  S.  Zilberstein,  and 
N.  Immerman.  The  complexity  of decentralized  control 
of MDPs.  InUAI,  2000. 

[Boutilier, 1996]  C.  Boutilier.  Planning,  learning  &  coor-
In  TARK-96, 

dination in multiagent decision processes. 
1996. 

[Cassandra et al.,  1997]  A.  Cassandra,  M.  Littman,  and 
N.  Zhang. 
Incremental  pruning:  A  simple,  fast,  ex(cid:173)
act method for partially observable markov decision pro(cid:173)
cesses.  In UAI, 1997. 

MULTIAGENT SYSTEMS 

711 


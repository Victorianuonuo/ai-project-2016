DP-SLAM: Fast, Robust Simultaneous Localization and Mapping Without 

Predetermined Landmarks 

Austin Eliazar and Ronald Parr 
Department of Computer Science 

Duke University 

{eliazar, parr} @cs.duke.edu 

Abstract 

We present a novel,  laser range finder based algorithm 
for simultaneous localization and mapping (SLAM) for 
mobile robots.  SLAM  addresses  the  problem  of con(cid:173)
structing an accurate map in real time despite imperfect 
information about the robot's trajectory through the en(cid:173)
vironment.  Unlike other approaches that assume prede(cid:173)
termined landmarks (and must deal with a resulting data-
association problem) our algorithm is purely laser based. 
Our algorithm uses a particle filter to represent both robot 
poses and possible map configurations.  By using a new 
map  representation,  which  we  call  distributed particle 
(DP) mapping, we are able to maintain and update hun(cid:173)
dreds of candidate maps and robot poses efficiently. The 
worst-case complexity of our algorithm per laser sweep 
is log-quadratic  in the number of particles we maintain 
and linear in the area swept out by the laser. However, in 
practice our run time is usually much less than that. Our 
technique contains essentially no assumptions about the 
environment yet it is accurate enough to close loops of 
60m in length with crisp, perpendicular edges on corri(cid:173)
dors and minimal or no misalignment errors. 

Introduction 

1 
The  availability  of relatively  inexpensive  laser  range  finders 
and the development of particle  filter  based algorithms have 
led  to  great strides  in  recent  years  on  the  problem  of robot 
localization - determining  a  robot's  position  given  a  known 
map  [Fox  et  al,  1999].  Initially,  the  maps  used  for  these 
methods were constructed by  hand.  However,  the  accuracy 
of the  laser suggests  its  use  for  map-making  as  well  as  lo(cid:173)
calization.  Potential  applications  for  accurate  map-making 
would include search and rescue operations, as well as space, 
underwater and subterranean exploration. 

Even  with  an  accurate  laser  range  finder,  map-making 
presents a difficult challenge:  A  precise position estimate  is 
required to make consistent updates to the the map, but a good 
map is required for reliable localization.  The challenge of si(cid:173)
multaneous localization and mapping (SLAM) is that of pro(cid:173)
ducing accurate maps in real time, based on a single pass over 
the sensor data, without an off line correction phase. Straight(cid:173)
forward approaches that localize the robot based upon a par(cid:173)
tial map and then update the map based upon the maximum 
likelihood position of the robot tend to produce maps with er(cid:173)
rors that accumulate over time. When the robot closes a phys(cid:173)

ical loop in the environment, serious misalignment errors can 
result. 

The EM  algorithm provides a very principled approach to 
the problem,  but it involves an  expensive off-line  alignment 
phase [Burgard et ah, 1999]. There exist heuristic approaches 
to this problem that fall  short of full  EM1, but they are not a 
complete solution and they require additional passes over the 
sensor data [Thrun, 2001]. Scan matching can produce good 
maps  fLu  and Milios,  1997; Gutmann  and Konolige,  2000] 
from  laser  range  finder  data,  but  such  approaches  typically 
must explicitly look for and require additional effort to close 
loops. In varying degrees, these approaches can be viewed as 
partially separating the localization and mapping components 
of SLAM. 

The  FastSLAM  algorithm  [Montemerlo  et  a/.,  2002J, 
which  does  not require  explicit  loop-closing  heuristics,  is  a 
recent  SLAM  approach  which  has  made  great  progress  in 
the  field.  FastSLAM  follows  a  proposal  by  Murphy  [Mur(cid:173)
phy,  19991 using a Rao-Blackwellized particle  filter  to sam(cid:173)
ple  robot  poses  and  track  the  position  of a  fixed  number of 
predetermined landmarks using  a  Kalman  filter.  (The  land(cid:173)
mark positions are conditionally independent given the robot 
pose.) This method mitigates some of the challenges in map(cid:173)
ping at the expense of some challenges in landmark selection 
and identification. The latter can involve a fairly complicated 
data association problem, although recent progress has been 
made in addressing this [Montemerlo and Thrun, 20021. 

We  present  a  novel,  laser  range  finder  based  algorithm 
called  DP-SLAM  that,  like  FastSLAM,  exploits  the  condi(cid:173)
tional  independences noted by  Murphy.  However,  our algo(cid:173)
rithm is purely laser based and  makes no landmark assump(cid:173)
tions. We avoid the data association problem by storing mul(cid:173)
tiple detailed maps instead of sparse landmarks, thereby sub(cid:173)
suming association  with  localization.  Our algorithm uses  a 
particle  filter  to represent both robot poses and possible map 
configurations.  Using  a new map representation,  which wc 
call distributed particle (DP) mapping, we are able to main(cid:173)
tain and update hundreds or thousands of candidate maps and 
robot poses  in  real  time as  the robot moves through the en(cid:173)
vironment.  The  worst-case  complexity  of our algorithm  per 
laser  sweep  is  log-quadratic  in  the  number  of particles  we 

lThis approach is heuristic because it does not maintain a joint 

probability distribution over maps and poses. 

ROBOTICS 

1135 

maintain and linear in the area swept out by the laser.  How(cid:173)
ever, in practice our run time is usually much less.  Our tech(cid:173)
nique  makes  essentially  no  assumptions  about  the  environ(cid:173)
ment yet it is accurate enough to close loops of 60m in length 
with crisp, perpendicular edges on corridors and minimal or 
no misalignment errors. This accuracy is achieved throughout 
the  mapping  process,  without a need  for explicit algorithms 
to correct the loop as temporally distant observations begin to 
overlap.  In  fact,  DP-SLAM  could  be  further complimented 
by  inclusion of existing algorithms for closing loops, though 
the addition may be unnecessary if a sufficient number of par(cid:173)
ticles is used. 

2  Particle Filters for Localization and 

Mapping 

A  particle  filter  is  a  simulation-based  method  of tracking  a 
system with partially observable state.  We briefly review par(cid:173)
ticle  filters  here,  but  refer the  reader  to  excellent  overviews 
of  this  topic  iDoucet  el  ai,  2001]  and  its  application  to 
robotics [Thrun, 2000J for a more complete discussion. 

A particle filter maintains a weighted (and normalized) set 
cahed particles.  At each 
of sampled states, 
step, upon observing an observation o (or vector of observa(cid:173)
tions), the particle filter: 

The a and b terms are linear correction to account for consis(cid:173)
tent errors in  motion.  The function. 
returns  random 
noise  from  a  normal  distribution  with  mean  0  and  standard 
deviation a, which is derived experimentally and may depend 
upon the magnitudes  of 

and 

After simulation, we need to weight the particles based on 
the robot's current observations of the environment.  For pure 
localization, the robot stores a map in memory.  The position 
described by each particle corresponds to a distinct point and 
orientation in the map. Therefore, it is relatively simple to de(cid:173)
termine what values the sensors should return, given the pose 
within the map. The standard assumption is that sensor errors 
are normally distributed.  Thus,  if the first obstruction in  the 
map along a line traced by a laser cast is at distance d and the 
the  probability  density  of observing 
reported  distance is 
discrepancy 
is normally distributed with mean 0. 
For our experiments we assumed a standard deviation in laser 
measurements of 5cm.  Given the model and pose, each sen(cid:173)
sor reading is correctly treated as an independent observation 
1 Murphy, 19991. The total posterior for particle i is then 

1. Samples 

new  s t a t e s f

replacement. 

r om  S  with 

where 
ceived distances for sensor (laser cast)  and particle i. 

is  the  difference  between  the  expected  and  per(cid:173)

2.  Propagates each new state through a Markovian transi(cid:173)
tion  (or simulation) model: 
.  This entails sam(cid:173)
pling a new state from the conditional distribution over 
next states given the sampled previous state. 

3.  Weighs each new state according to a Markovian obser(cid:173)

vation model: 

4.  Normalizes the weights for the new set of states 
Particle  filters  arc  easy  to  implement  have  been  used 
to  track  multimodal  distributions  for  many  practical  prob(cid:173)
lems  [Doucet et al,  2001]. 

2.1  Particle  Filters  for  Localization 
A particle filter is a natural approach to the localization prob(cid:173)
lem,  where the robot pose  is the hidden state to be tracked. 
The state transition is the robot's movement and the observa(cid:173)
tions are the robot's sensor readings, all of which are noisy. 
The  change  of  the  state  over  time  is  handled  by  a  mo(cid:173)
tion  model.  Usually,  the  motion  indicated by the odometry 
is taken as the basis for the motion model, as it is a reliable 
measure of the amount that the wheels have turned.  However, 
odometry is a notoriously inaccurate measure of actual robot 
motion, even in the best of environments.  The slip and shift 
of the robot's wheels, and unevenness in the terrain can com(cid:173)
bine to give significant errors which will quickly accumulate. 
A motion model differs across robots and types of terrain, but 
generally consists of a  linear shift,  to account for systematic 
errors and Gaussian noise.  Thus, for odometer changes of x, 
a particle  filter  applies the error model and obtains, 

and 

for particle 

2.2  Particle  Filters  for  S L AM 
Some  approaches  for  SLAM  using  particle  filters  attempt 
to maintain  a  single  map  with  multiple  robot poses  [Thrun, 
2001],  an  approach that  we avoid  because  it  leads  to errors 
that accumulate over time.  The basic problem is that the hid(cid:173)
den state is actually both the robot pose and the map itself.  An 
important consequence of this problem is that all observations 
arc no longer compared to a single map, which is presumed 
to be correct.  Instead, the observations are compared against 
an incomplete and possibly incorrect map, identified with the 
particle  in  question.  The  map  itself is  created  by  the  accu(cid:173)
mulation of observations of the environment and estimates of 
robot positions. 

In  principle,  this  "solves"  the  SLAM  problem. 

In  prac(cid:173)
tice, it replaces a conceptual problem with an algorithmic one. 
Particles with errors in robot position estimates will  make er(cid:173)
roneous  additions  to  their  maps.  An  error  in  the  map  will 
then,  in  turn,  cause  another error in  localization  in  the  next 
step,  and  these  inaccuracies  can  quickly  compound.  Thus, 
the number of particles required for SLAM  is typically more 
than that required for localization since the price of accumu(cid:173)
lated  errors  is  much  higher.  Note  that  in  pure  localization, 
small errors in position estimation can be absorbed as part of 
the noise in the motion model. 

The algorithmic problem becomes one of efficiently main(cid:173)
taining a large enough set of particles to obtain robust perfor(cid:173)
mance, where each particle is not merely a robot pose, but a 
pose  and  map.  Since  maps  are  not  light  weight data struc(cid:173)
tures,  maintaining  the  hundreds  or  thousands  of such  maps 
poses a serious challenge.  One reasonable approach to tam(cid:173)
ing this problem is to assume that the uncertainty in the map 

1136 

ROBOTICS 

can be represented in a simple parametric form. This is essen(cid:173)
tially the approach taken by FastSLAM, for which the map is 
a Kalman  filter  over a set of landmark positions.  This is cer(cid:173)
tainly  the right thing to do if one is given a set of landmarks 
that can  be quickly  and  unambiguously identified.  We  will 
show  that  this  strong  assumption  is  not  required:  By  using 
raw laser data, combined with an occupancy grid and efficient 
data  structures,  we  can  handle  a  large number of candidate 
maps and poses efficiently, achieving robust performance. 

3  DP-SLAM 
In  this  section  we  motivate  and  present  the  main  technical 
contribution of the DP-SLAM  algorithm.  DP-SLAM  imple(cid:173)
ments  what  is  essentially  a  simple  particle  filter  over  maps 
and  robot  poses.  However,  it  uses  a  technique  called  dis(cid:173)
tributed particle mapping (DP-Mapping), which enables it to 
maintain a large number of maps very efficiently. 

3.1  Naive  S L AM 
When  using  a  particle  filter  for SLAM,  each  particle corre(cid:173)
sponds to a specified trajectory through the environment and 
has a specific map associated with it.  When a particle is re-
sampled, the entire map itself is treated as part of the hidden 
state that is being tracked and is copied over to the new parti(cid:173)
cle.  If the map is an occupancy grid of size M and P particles 
are maintained by the particle filter, then ignoring the cost of 
localization,  O(MP)  operations  must  be  performed  merely 
copying maps.  For a number of particles sufficient to achieve 
precise  localization  in  a  reasonably  sized  environment,  the 
naive approach would require gigabytes worth of data move(cid:173)
ment per update2. 

3.2  Distributed  Particle  M a p p i ng 
By  now  the  astute  reader  has  most  likely  observed  that  the 
naive approach is doing too much work. To make this clearer, 
we  will  introduce the notion of a  particle ancestry.  When  a 
particle is sampled at iteration i to produce a successor parti(cid:173)
we call the generation i particle a parent 
cle at iteration 
particle a child.  Two children  with 
and the generation 
the same parent are siblings. From here, the concept of a par(cid:173)
ticle ancestry extends naturally. Suppose the laser sweeps out 
and consider two siblings, s1 and s2. 
an area of size 
Each sibling will correspond to a different robot pose and will 
make at most A updates to the map it inherits from its parent. 
Thus, s1 and s2 can differ in at most A map positions. 

When the problem is presented in this manner, the natural 
reaction  from most computer scientists is to propose record(cid:173)
ing the  "diff" between maps,  i.e,  recording a list of changes 
that each particle makes to its parent's map. While this would 
solve the problem of making efficient map updates,  it would 
create  a  bad  computational  problem  for  localization:  Trac(cid:173)
ing a line though the map to look for an obstacle would re(cid:173)
quire  working  through  the  current  particle's  entire  ancestry 
and consulting the stored  list of differences for each particle 
in  the  ancestry.  The  complexity  of this  operation  would  be 

2In  addition  to  the  theoretical  analysis,  anecdotal  comments 
made by researchers in this area reinforce the impracticality of this 
approach. 

linear  in  the  number  of iterations  of the  particle  filter.  The 
challenge is, therefore, to provide data structures that permit 
efficient updates to the map and efficient localization queries 
with  time  complexity  that  is  independent  of the  number  of 
iterations  of the  particle  filter.  We  call  our  solution  to  this 
problem  Distributed  Particle  Mapping  or  DP-Mapping,  and 
we explain it in terms of the two data structures that are main(cid:173)
tained: the ancestry tree and the map itself. 

Maintaining the particle ancestry tree 
The  basic  idea of the  particle  ancestry tree  is fairly straight(cid:173)
forward.  The  tree itself is rooted  with an  initial  particle,  of 
which all other particles are progeny. Each particle maintains 
a pointer to its parent and is assigned a unique numerical ID. 
Finally each particle maintains a list of grid squares that it has 
updated. 

The details of how we will  use the ancestry tree for local(cid:173)
ization are described in the subsequent section. In this section 
we focus on the maintenance of the ancestry tree, specifically 
on making certain that the tree has bounded size regardless of 
the number of iterations of the particle  filter. 

We maintain a bounded size tree by pruning away unnec(cid:173)
essary nodes.  First,  note that certain  particles may not have 
children and can simply be removed from the tree. Of course, 
the  removal  of such  a  particle  may  leave  its  parent  without 
children  as  well,  and  we  can  recursively  prune  away  dead 
branches  of  the  tree.  After  pruning,  it  is  obvious  that  the 
only particles which are stored in our ancestry tree are exactly 
those particles  which are ancestors of the  current generation 
of particles. 

This  is  still  somewhat more  information  than  we  need to 
remember. 
If a  particle  has  only  one  child  in  our  ancestry 
tree, we can essentially  remove it,  by collapsing that branch 
of the  tree.  This  has  the  effect  of merging  the  parent's  and 
child's updates to the map, a process described in the subse(cid:173)
quent section. By applying this process to the entire tree after 
pruning, we obtain a minimal ancestry tree, which has several 
desirable and easily provable properties: 
Proposition 3.1  Independent  of the  number  of iterations  of 
particle filtering,  a  minimal  ancestry  tree  of P  particles 

J.  has exactly P leaves, 
2.  has branching factor of at least 2, and 
3.  has depth no more than P. 

Map  representation 
The challenge for our map representation is to devise a data 
structure that permits efficient updates and efficient localiza(cid:173)
tion.  The naive approach of a complete map  for each parti(cid:173)
cle is inefficient, while the somewhat less naive approach of 
simply  maintaining  history  of each  particle's updates  is  also 
inefficient because it introduces a dependency on the number 
of iterations of the particle  filter. 

Our solution to the map representation problem is to asso(cid:173)
ciate  particles  with  maps,  instead  of associating  maps  with 
particles.  DP-mapping  maintains  just  a  single  occupancy 
grid.  (The particles are distributed over the map.)  Unlike a 
traditional occupancy grid, each grid square stores a balanced 
tree, such as a red-black tree. The tree is keyed on the IDs of 

ROBOTICS 

1137 

the particles that have made changes to the occupancy of the 
square. 

The grid is initialized as a matrix of empty trees.  When a 
particle makes an observation about a grid square it inserts its 
ID and the observation  into the associated  tree.  Notice that 
this  method of recording maps  actually  allows each  particle 
to behave as  if it  has  its  own  map.  To check  the  value of a 
grid  square,  the  particle checks each  of its  ancestors to find 
the most recent one that made an observation for that square. 
If no ancestor made an entry,  then  the particle can  treat this 
position as being unknown. 

We can now describe the effects of collapsing an ancestor 
with a single child in the ancestry tree more precisely:  First, 
the set of squares updated by the child is merged into the par(cid:173)
ent's  set.  Second,  for each  square visited  by  the child,  we 
change the ID key stored in the balanced tree to match that of 
the parent. (If both the child and parent have made an update 
to the same square,  the parent's update is replaced with  the 
child's.) The child is then removed from the tree and the par(cid:173)
ent's grandchildren become its direct children.  Note that this 
ensures that the  number of items  stored  in  the balanced tree 
at  each  grid  square  is  0{P). 

3.3  Computational  Complexity 
The  one  nice  thing  about  the  naive  approach  of keeping  a 
complete  map  for each  particle  is  the  simplicity:  If we  ig(cid:173)
nore  the  cost  of block  copying  maps,  lookups  and  changes 
to the map can all  be done in constant  time.  In these areas, 
distributed particle mapping may initially seem less efficient. 
However,  we can  show that DP maps are in  fact  asymptoti(cid:173)
cally superior to the naive approach. 

Lookup on a DP-map requires a comparison between the 
ancestry  of  a  particle  with  the  balanced  tree  at  that  grid 
square.  Let D  be the depth of the ancestry tree,  and thus is 
the maximum  length of a particle's ancestry.  Strictly speak(cid:173)
ing, as the ancestry tree is not guaranteed to be balanced, D 
can  be O(P).  However,  in  practice,  this  is  almost never the 
case, and we have found 
, as the nature of parti(cid:173)
cle resampling lends to very balanced ancestry trees.  (Please 
see the discussion in the following section for more detail on 
this point.)  Therefore, we can complete our lookup after just 
D accesses to the balanced tree.  Since the balanced tree itself 
can hold at most P entries, and a single search takes 0(lgP) 
time.  Accessing a specific grid square in the map can there(cid:173)
fore  be  done  in  O(DlgP)  time. 

For localization, each particle will need to make  0 ( A)  ac(cid:173)
cesses to the map.  As each particle needs to access the en(cid:173)
tire  observed  space  for  its  own  map,  we  need  O(AP)  ac(cid:173)
cesses,  giving  localization  with  DP-maps  a  complexity  of 
O(ADPlgP). 

To  complete  the  analysis  we  must  handle two  remaining 
details:  The cost of inserting new  information into the map, 
and the cost of maintaining the ancestry tree.  Since we use a 
balanced tree for each grid square, insertions and deletions on 
our map both take 0(lgP)  per entry.  Each particle can  make 
at most 0(A)  new entries, which in turn will only need to be 
removed once. Thus the procedure of adding new entries can 
be  accomplished  in  O(ADlgP)  per  particle,  or  O(ADPlgP) 

total and the cost of deleting childless particles will be amor(cid:173)
tized  as  O(ADPlgP). 

It remains  to be  shown  that the  housekeeping required to 
maintain the ancestry tree has  reasonable cost.  Specifically, 
we need to show that the cost of collapsing childless ancestry 
tree  nodes  does  not  exceed  O(ADPlgP).  This  may  not  be 
obvious  at  first,  since  successive  collapsing  operations  can 
make the set of updated squares for a node in the ancestry tree 
as large as the entire map.  We now argue that the amortized 
cost  of  these  changes  will  be  O(ADPlgP).  First,  consider 
the  cost  of merging  the  child's  list  of modified  squares  into 
the parent's list.  If the child has modified n squares, we must 
perform  0(nlgP)  operations  (n  balanced tree  queries  on  the 
parent's key) to check the child's entries against the parent's 
for duplicates. 

The  final  step  that  is  required  consists  of updating  the ID 
for  all  of the  child's  map  entries.  This  is  accomplished  by 
deleting the old ID, and inserting a new copy of it,  with  the 
parent's  ID.  The cost of this  is  again  0(nlgP).  Consider that 
each map entry stored in the particle ancestry tree has a po(cid:173)
tential of D steps that it can be collapsed, since D  is the total 
number of nodes between its initial position and the root, and 
no new nodes will ever be added in between.  At each  itera(cid:173)
tion, P particles each create A new map entries with potential 
D.  Thus the  total  potential  at  each  iteration  is  O(ADPlgP). 
The computational complexity of DP-SLAM can be sum(cid:173)

marized as follows: 
Proposition 3.2  For a particle  filter  that  maintains  P parti(cid:173)
cles,  laser that sweeps out A  grid squares,  and an ancestry 
tree of depth D, DP-SLAM requires: 

•  O(ADPlgP)  operations for 

localization  arising  from: 

-  P particles  checking  A  grid squares 
-  A  lookup  cost  of O(DlgP)  per  grid  square 

•  O(APlgP)  operations  to  insert  new  data  into  the  tree, 

arising  from: 

-  P particles  inserting  information  at  A  grid squares 
- 
Insertion  cost  of O(lgP)  per new piece  of informa(cid:173)
tion 
•  Ancestry 

tree  maintenance  with  amortized  cost 

O(ADPlgP) 

arising  from 

-  A cost of O(lgP) to remove an observation or move 

it up one level in the ancestry tree 

-  A  maximum  potential  of ADP  introduced  at  each 

iteration. 

that 

is  O(ADPlgP),  which  can  be  as 

3.4  Complexity  Comparison 
the  amortized  complexity  of 
Our  analysis  shows 
DP-SLAM 
large  as 
0(AP2lgP).  For  the  naive  approach,  each  map  is  repre(cid:173)
sented explicitly, so lookups can be done in constant time per 
grid  square.  The  localization  step  thus  takes  only  O(AP). 
Without the need for updates to an ancestry tree, map updates 
can likewise be done in O(AP) time.  However, the main bulk 
of computation lies in the need to copy over an entire map for 
each  particle,  which  will  require  0(MP)  time,  where  M  is 

1138 

ROBOTICS 

the size of the map.  Since typically 
is the dominant term in the computation. 

DP-SLAM  will  be  advantageous  when 

this  obviously 

Even in the worst case where D approaches P, there will still 
be a benefit. For a high resolution occupancy grid, M will be 
quite  large since  it  grows quadratically with  the  linear mea(cid:173)
sure  of the environment and  would grow  cubically  if we  we 
consider height.  Moreover,  A,  will  be  a  tiny  fraction  of M. 
The size  of 
P will,  of course, depend upon the environ(cid:173)
mental features and the sampling rate of the data. It is impor(cid:173)
tant  to  note  that  since  the  time  complexity  of our algorithm 
does  not depend  directly  on  the  size  of the  map,  there  will 
necessarily exist environments for which DP-SLAM is vastly 
superior to the naive approach since the naive approach will 
require block copying an amount of data will exceed physical 
memory. 

In  our  initial  experiments,  we  have  observed  that  P  was 
surprisingly small,  suggesting that  in  practice the advantage 
of  DP-SLAM  is  much  greater  than  the  worst-case  analysis 
suggests.  For some problems the point at which it is advan(cid:173)
tageous  to  use  DP-SLAM  may  be  closer  to . 
This  phenomenon  is  discussed  in  more  detail  in  the  subse(cid:173)
quent section. 

4  Empirical Results 
We  implemented and deployed the DP-SLAM algorithm on 
a real robot using data collected from the second floor of our 
computer science building.  In our initial  implementation our 
map representation is a binary occupancy grid.  When a par(cid:173)
ticle  detects  an  obstruction,  the  corresponding  point  in  the 
occupancy  grid  is  marked  as  fully  occupied  and  it  remains 
occupied  for  all  successors  of the  particle.  (A  probabilistic 
approach that updates the grid is  a natural avenue for future 
work.) 

On  a  fast  PC  (2.4  GHz  Pentium  4),  the  run  time  of DP-
SLAM is close to that of the data collection time, so the algo(cid:173)
rithm could have been  run in real time for our test domains. 
In practice, however, we collected our data in a log file using 
the relatively slow computer on our robot, and processed the 
data later using a faster machine. To speed things up, we also 
implemented a novel particle culling technique to avoid fully 
evaluating the  posterior for bad particles  that are  unlikely to 
be resampled. This works by dividing the sensor readings for 
each particle into  disjoint, evenly distributed subsets.  The 
posterior for the particles is computed in  passes, where pass 
i evaluates the contribution from subset  At the end of each 
pass,  particles  with  significantly  lower  (partially  evaluated) 
posterior in  comparison  to  the  others  are  assigned  0 proba(cid:173)
bility and removed from further consideration.  We also set a 
hard threshold on the number of particles we allowed the al(cid:173)
gorithm to keep after culling. Typically, this was set to the top 
10%  of the total  number of particles considered.  In practice, 
this cutoff was almost never used since culling effectively re(cid:173)
moved at  least  90%  of the  particles  that  were  proposed.  A 
value  if 

gave us a speedup of approximately 

For the results we present, it is important to emphasize that 
our  algorithm  knows  absolutely  nothing  about  the  environ(cid:173)
ment or the existence of loops.  No assumptions about the en(cid:173)

vironment are made, and no attempt is made to smooth over 
errors  in  the  map  when  loops  are  closed.  The  precision  in 
our maps results directly  from the  robustness of maintaining 
multiple maps. 
4.1  Robot and Robot Model 
The robot we used for testing DP-SLAM is an iRobot ATRV 
Jr.  equipped with  a  SICK  laser range  finder  attached  to the 
front  of  the  robot  at  a  height  of  7cm.  Readings  are  made 
across  180°, spaced one degree apart, with an effective dis(cid:173)
tance of up to 8m.  The error in distance readings is typically 
less than 5mm. 

Odometric readings from the ATRV Jr.'s shaft encoders are 
unreliable, particularly when turning.  Our motion model as(cid:173)
sumes  errors  are  distributed  in  a  Gaussian  manner,  with  a 
standard deviation of 25% in lateral motion and 50% in turns. 
Turns are quasi-holonomic, performed by skid steering.  We 
obtained  our  motion  model  using  an  automated  calibration 
procedure  that  worked  by  positioning  the  robot  against  a 
smooth  wall  and  comparing  odometry  data  with  the  move(cid:173)
ment inferred from the laser readings. 
4.2  Test Domain(s) and Map(s) 
We tested the algorithm on a loop of hallway approximately 
16m  by  14m.  A  new  set  of observations  was  recorded  af(cid:173)
ter each 20cm motion (approximately).  The maps were con(cid:173)
structed with a resolution of 3cm per grid square, providing a 
very high resolution cross section of the world at a height of 
7cm from the floor. 

Figure 1 shows the highest probability map generated after 
the  completion  of one  such  loop  using  9000  particles3. 
In 
this test, the robot began in the middle of the bottom hallway 
and continued counterclockwise through the rest of the map, 
returning to the starting point. 

This  example  demonstrates  the  accuracy  of  DP-SLAM 
when completing a long loop, one of the more difficult tasks 
for SLAM algorithms.  After traveling 60m, the robot is once 
again able to observe the same section of hallway in which it 
started.  At that point, any accumulated error will  readily be(cid:173)
come apparent, as it will lead to obvious misalignments in the 
corridor.  As the figure shows, the loop was closed perfectly, 
with no discernible misalignment. 

To  underscore  the  advantages  of  maintaining  multiple 
maps,  we  have  included  the  results  obtained  when  using  a 
single map and the same number of particles.  Figure 2 shows 
the result of processing the same sensor log file by generating 
9000 particles at each  time step,  keeping the  single particle 
with the highest posterior, and updating the map based upon 
the robot's pose in this particle.  There is a considerable mis(cid:173)
alignment error where the loop is closed at the top of the map. 
Larger, annotated versions of the maps shown here, as well 
as maps generated from different sensor logs,  will be avail(cid:173)
able 

http://www.es.duke.edu/~parr/dpslam/. 

at 

3In an earlier version of this paper, we presented a map produced 
with 1000 particles based upon a different sensor log file.  The run 
shown here covers the same area,  but has less reliable odometry, 
probably due to weaker battery, and is more illustrative of the bene(cid:173)
fits of DP-SLAM. 

ROBOTICS 

1139 

certain about its position. This is reflected in the higher peaks 
in the graph, and the less recent point of coalescence overall. 
In both cases,  strong correlation can be seen between the 
higher peaks in this graph and the turns that the robot took in 
the map.  (Note the iteration number annotations at the turns 
in Figure  1.)  This is because odometry is particularly unreli(cid:173)
able during skid turns and the robot has greater difficulty lo(cid:173)
calizing. The points at which our turns occur in our office en(cid:173)
vironment also tend to produce more ambiguous sensor data 
since turns are typically in open areas where the  laser scans 
will be more widely spaced.  Other peaks can be mapped to 
areas of clutter in the map, such as the recycling bins. 

Figure 4 was created from the same two experiments, and 
tracks  the  amount  of memory  used  over  time,  as  measured 
by the total  number of leaves  in  the balanced trees stored  in 
our  occupancy  grid.  Since  no  leaves  are  stored  for  empty 
space and our maps are essentially two one-dimensional sur(cid:173)
faces, the amount of memory required for leaves should grow 
roughly  linearly over time.  The peaks  corresponding to  the 
turns  in  the  map  are  even  more  pronounced  for  the  handi(cid:173)
capped version of DP-SLAM in this graph, indicating that un(cid:173)
der conditions of greater uncertainty, the ancestry tree grows 
in width as well as depth. 

making  our  O 

Of course,  shallow  coalescence  may  not  occur  and  may 
not even be desirable in all  environments, but it has  signifi(cid:173)
cant  implications  for the  efficiency of the  algorithm  when  it 
does  occur.  It  implies 
bound closer to 
in  practice.  Moreover, we  get 
maximum  benefit  from  the  compactness  of our  map  repre(cid:173)
sentation.  As the particles coalesce, irrelevant entries are re(cid:173)
moved.  Thus all areas in the map that were observed before 
the  point of coalescence  contain  only  one entry  in  their list. 
Similarly, those areas which have not been observed by any 
particle yet have no entries in their list.  If we can assume that 
the depth of coalescence is bounded by some reasonable con(cid:173)
stant C, then all map squares but those that have been altered 
prior the point of coalescence can  be thought of as requiring 
a constant amount of storage. This means that the total space 
required to store the map tends to be close to 
in  practice,  not the  theoretical  worst case  of O(MP). 

A sceptic may wonder if coalescence is indeed a desirable 
thing  since  it  may  imply  a  loss  of information.  If the  algo(cid:173)
rithm  maintains  only  a  limited  window  of uncertainty,  how 
can  it  be expected to  close  large  loops,  which  would  seem 
to require maintaining uncertainty over map entries for arbi(cid:173)
trarily long durations? The simple answer is that coalescence 
is desirable when it is warranted by the data and undesirable 
when  the  accumulation  of  sensor  data  is  insufficient  to  re(cid:173)
solve ambiguities.  In fact, coalescence is a direct product of 
the  amount  of  uncertainty  still  present  in  the  observations. 
Our  algorithm  is  both  able  to  maintain  ambiguity  for  arbi(cid:173)
trary  time  periods  and  benefit  from  coalescence  when  it  is 
warranted, so there is nothing negative about the fact that we 
have observed this phenomenon in our experiments. 

Of course, since we are representing a complicated distri(cid:173)
bution with a finite number of samples, there is always a dan(cid:173)
ger that the low probability "tails" of the distribution will  be 
lost  if too  few  particles  are  used.  This  would  result  in  pre(cid:173)
mature  coalescence due  to  failure  to  resample  a  potentially 

Figure 3:  Depth of coalescence as a function of time. 

4.3  Coalescence 
In our sample domain (the second  floor  of the Duke Univer(cid:173)
sity  Department  of Computer  Science),  we  have  found  that 
DP-SLAM produces very shallow ancestry trees due to a phe(cid:173)
nomenon we refer to as coalescence.  Over a number itera(cid:173)
tions of the particle filter, nearly all of the particles will have 
a common ancestor particle  in the not-too-distant past.  This 
point of coalescence,  or common ancestry,  varies over time, 
and  is  sensitive  to  the  environment.  Observations made  on 
our empirical  results  indicate that  while this  number peaked 
during  events  of  higher  uncertainty,  coalescence  was  often 
as  recent as  20 generations in  the past,  and  never exceeded 
90.  This  means that beyond about 20 steps  in  the past,  ev(cid:173)
ery particle has an identical map.  The fact that we can close 
loops so precisely  while  maintaining relatively shallow coa(cid:173)
lescence points suggests that we  are able to maintain distri(cid:173)
butions over map regions long enough to resolve ambiguities, 
but  not  longer  than  is  needed.  While  the  phenomenon de(cid:173)
serves more careful study, our initial impression is that by the 
time a region has passed outside of the robot's field of view, it 
has been scanned enough times by the laser that there is only 
a  single  hypothesis  that  is  consistent  with  the  accumulated 
observations. 

Additional experiments were run to show the relationship 
between coalescence and uncertainty, as well as the ability of 
DP-SLAM to automatically preserve more information when 
required.  We considered two different scenarios:  the  origi(cid:173)
nal  DP-SLAM  algorithm  with  the same  sensor data used to 
generate Figure  1, and  a handicapped version of DP-SLAM 
run on the same sensor log, but with three out of every four 
laser casts ignored at each iteration. The handicapped version 
required 30,000 particles to produce good maps. 

To study the effects of uncertainty on coalescence, we ran 
both algorithms with  the  same number of particles and then 
compared  the  coalescence  points.  The  results  in  Figure  3 
show  how  the  two  versions of DP-SLAM  performed on  the 
same sensor log with the same number of particles.  The de(cid:173)
crease in observational information for the handicapped ver(cid:173)
sion of DP- SLAM makes it more difficult for the robot to be 

ROBOTICS 

1141 

to one part of the  model  to compensation for unmodeled noise 
in  other parts.  Nevertheless,  in  future  work  we  would  like  to 
develop  a  more  comprehensive  error  model  and  a  principled 
scheme  for  updating  the  map  that  handles  partially  or  tran(cid:173)
siently  occupied  grid  squares.  Naturally,  we  would  like  to 
consider other  map  representation  methods  too. 

The  most  important  future  direction  for this  work  is  to  in(cid:173)
corporate  less  reliable  sensors  as  such  video  or  sonar to  con(cid:173)
struct  a  more  comprehensive,  three-dimensional  view  of  the 
environment. 

Acknowledgments 
We  are  very  grateful  to  Sebastian  Thrun  for critical  feedback 
and  encouragement  for  this  line  of  research.We  also  thank 
Tammy  Bailey,  Dieter  Fox,  Carlos  Guestrin,  Dirk  Haehnel, 
Mark  Paskin,  and  Carlo  Tomasi  for  helpful  feedback  and 
comments. 

References 
[Burgard et  al,  1999]  W. Burgard, D. Fox, H. Jans, C. Matenar, and 
S.  Thrun.  Sonar-based  mapping  with  mobile  robots  using  EM. 
In  Proc.  of the  International  Conference  on  Machine  Learning, 
1999. 

[Doucct et al., 2001]  Arnaud  Doucet,  Nando  de  Frcitas,  and  Neil 
Gordon.  Sequential Monte  Carlo Methods  in  Practice.  Springer-
Verlag, Berlin, 2001. 

[Fox  etal,  1999]  Dieter  Fox,  Wolfram  Burgard,  Frank  Dellaert, 
and  Sebastian  Thrun.  Monte  carlo  localization:  Efficient  posi 
tion estimation for mobile robots.  In AAA1-99,  1999. 

[Gutmann and Konolige, 2000]  J.  Gutmann  and  K.  Konoligc. 

cremental  mapping of large cyclic environments,  2000. 

In(cid:173)

[Lu  and Milios,  1997]  F.  Lu  and  E.  Milios.  Globally  consistent 

range scan alignment for environment mapping,  1997. 

[Montemerlo and Thrun, 2002]  M.  Montemerlo and S. Thrun.  Si(cid:173)
multaneous  localization  and  mapping  with  unknown  data  asso(cid:173)
In  IEEE International  Conference  on 
ciation  using  FastSLAM. 
Robotics  and Automation  (ICRA),  2002. 

[Montemerlo et al., 2002]  M. Montemerlo, S. Thrun, D. Roller, and 
B.  Wegbreit.  FastSLAM:  A  factored  solution  to  the  simultane(cid:173)
ous  localization and mapping problem.  In AAA1-02, Edmonton, 
Canada, 2002. AAAI. 

I Murphy,  1999]  K. Murphy.  Baycsian map learning in dynamic en(cid:173)
vironments.  In Advances  in  Neural Information  Processing  Sxs-
tems  II.  MIT Press,  1999. 

[Thrun, 2000]  S.  Thrun.  Probabilistic  algorithms  in  robotics.  AI 

Magazine, 21(4):93-109,  2000. 

[Thrun, 2001]  S. Thrun.  A probabilistic online mapping algorithm 
International  Journal  of Robotics 

for  teams  of  mobile  robots. 
Research, 20(5):335-363, 2001. 

Figure 4:  Total  map entries  used over time. 

useful  and  correct  map.  There  w i ll  certainly  be  some  envi(cid:173)
ronments,  typically those  with  very  sparse sensor data,  where 
it  will  be  difficult  to  avoid  premature  coalescence  without 
an  excessive  number  of  particles. 
In  situations  with  sparse 
data,  a  parametric  representation  of  uncertainty  over  land(cid:173)
marks  may  be  more tractable and  more  appropriate. 

5  Conclusion and Future Work 
We  have  presented  a  novel  particle-filter  based  algorithm  for 
simultaneous  localization  and  mapping.  Using  very  efficient 
data structures,  this algorithm is able to maintain thousands of 
maps  in  real  time,  providing robust  and highly accurate  map(cid:173)
ping.  The  algorithm  does  not  assume  the  presence  of  pre(cid:173)
determined  landmarks  or  assume  away  the  data  association 
problem  created  by  the  use  of landmarks.  Our  algorithm  has 
been  deployed  on  a  real  robot,  where  it  has  produced  highly 
detailed  maps  of an  office environment. 

We  believe  that  this  is  the  first  time  this  level  of accuracy 
has  been  achieved  for  the  type  of data  we  consider  using  an 
algorithm  that  does  not  explicitly  attempt  to  close  loops  and 
that  has  no domain  specific  knowledge.  Nevertheless,  our al(cid:173)
gorithm  does  have  some  limitations.  Most  reasonably  priced 
laser range  finders  scan  at a  fixed  height, giving us an incom(cid:173)
plete  view  of  the  environment.  Our  current  map  representa(cid:173)
tion  is  very simplistic  -  a grid  where each  cell  is  presumed  to 
be  completely  clear  or  completely  opaque  to  the  laser.  This 
obviously  creates  discretization  errors  at  the  edges  of objects 
and  confusion  for  very  small  objects.  Our  small  cell  size  re(cid:173)
duces  this  problem,  but  does  not  eliminate  it  entirely.  For 
example,  power  cords  hanging  off of  the  edge  of desks  tend 
to  introduce  localization  errors  and  increase  the  number  of 
particles  needed to  get robust performance. 

There  is  a  logical  inconsistency  in  the  way  we  treat  the 
laser  data.  When  we  add  new  information  to  the  map,  we 
treat the  laser  like  a deterministic  device,  but  when  we  local(cid:173)
ize,  we treat it  like a noisy device with a standard deviation in 
measurement that can  span  several grid squares.  Exaggerated 
error  models  like  this  are  common  in  many  real-world  appli(cid:173)
cations  of statistical  models,  where  additional  noise  is  added 

1142 

ROBOTICS 


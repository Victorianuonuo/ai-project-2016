Sensitivity Analysis in Markov Networks

Hei Chan and Adnan Darwiche
Computer Science Department

University of California, Los Angeles

Los Angeles, CA 90095

{hei,darwiche}@cs.ucla.edu

Abstract

This paper explores the topic of sensitivity analy-
sis in Markov networks, by tackling questions sim-
ilar to those arising in the context of Bayesian net-
works:
the tuning of parameters to satisfy query
constraints, and the bounding of query changes
when perturbing network parameters. Even though
the distribution induced by a Markov network
corresponds to ratios of multi-linear functions,
whereas the distribution induced by a Bayesian net-
work corresponds to multi-linear functions, the re-
sults we obtain for Markov networks are as effec-
tive computationally as those obtained for Bayesian
networks. This similarity is due to the fact that con-
ditional probabilities have the same functional form
in both Bayesian and Markov networks, which
turns out to be the more inﬂuential factor. The ma-
jor difference we found, however, is in how changes
in parameter values should be quantiﬁed, as such
parameters are interpreted differently in Bayesian
networks and Markov networks.

Introduction

1
In the domain of uncertainty reasoning, graphical models are
used to embed and represent probabilistic dependencies be-
tween random variables. There are two major types of graph-
ical models: Bayesian networks [Pearl, 1988; Jensen, 2001]
and Markov networks [Pearl, 1988]. In both models, there
are two components, a qualitative part and a quantitative part.
In the qualitative part, a graph is used to represent the interac-
tions between variables, such that variables with direct inter-
actions are connected by edges, and information of indepen-
dence relationships between variables can be inferred from
the network structure. In the quantitative part, a set of pa-
rameters are speciﬁed to quantify the strengths of inﬂuences
between related variables. The joint probability distribution
can then be induced from the two components.

Bayesian networks and Markov networks differ in both the
qualitative and quantitative parts of the models. For the qual-
itative part, directed edges are used in a Bayesian network to
represent inﬂuences from one variable to another, and no cy-
cles are allowed in the graph, while undirected edges are used

in a Markov network to represent symmetrical probabilistic
dependencies between variables.

For the quantitative part, the parameters in a Bayesian net-
work are speciﬁed in conditional probability tables (CPTs)
of the variables, where each CPT consists of the conditional
probability distributions of a variable given instantiations of
its parents, which are the variables that directly inﬂuence this
variable. Because these parameters are conditional probabili-
ties, they are more intuitive, but have to obey certain proper-
ties. For example, the probabilities in each conditional distri-
bution must be normalized and sum to 1. On the other hand,
the parameters in a Markov network are given in potentials,
deﬁned over instantiations of cliques, which are maximal sets
of variables such that every pair of variables in a clique are
connected by an edge. These parameters do not need to be
normalized and can be changed more freely.

The topic of sensitivity analysis is concerned with under-
standing and characterizing the relationship between the lo-
cal parameters quantifying a network and the global queries
computed based on the network. Sensitivity analysis has
been investigated quite comprehensively in Bayesian net-
works [Laskey, 1995; Castillo et al., 1997; Jensen, 1999;
Kjærulff and van der Gaag, 2000; Chan and Darwiche, 2002;
2004; 2005]. Earlier work on the subject has observed that
the distribution induced by a Bayesian network corresponds
to multi-linear functions, providing effective computational
methods for computing ∂P r(e)/∂θx|u: the partial derivative
of a joint probability with respect to a network parameter.
These earlier results were recently pursued further, providing
effective methods for solving the following problems:

• What is the necessary parameter change we need to ap-
ply such that a given query constraint is satisﬁed? For
example, we may want some query value P r(z | e)
to be at least p by changing network parameters. Efﬁ-
cient solutions were given to this problem for computing
minimum changes for single parameters [Chan and Dar-
wiche, 2002], and multiple parameters in a single CPT
[Chan and Darwiche, 2004].

• What is the bound on the change in some query value
if we apply an arbitrary parameter change? Here, we
are interested in ﬁnding a general bound for any query
and any parameter of any network. It has been shown
that the log-odds change in any conditional query in a

Bayesian network is bounded by the log-odds change in
any single parameter [Chan and Darwiche, 2002].

• What is the bound on the difference between some query
value induced by two networks that have the same struc-
ture but different parameter values? The solution to this
problem is based on a newly proposed distance measure
[Chan and Darwiche, 2005], which allows one to bound
the difference between the query values under two dis-
tributions. Based on this distance measure, and given
two Bayesian networks that differ by only a single CPT,
the global distance measure between the distributions in-
duced by the networks can be computed from the local
distance measure between the CPTs, thereby allowing
one to provide a bound on the difference between the
query values. Moreover, if we are given multiple CPTs
that do not intersect with each other, the global distance
measure can still be computed as the sum of the local
distance measures between the individual CPTs.

Figure 1: A sample Markov network structure in terms of an
undirected graph G.

to the deﬁnition and parameterization of Markov networks.
Section 3 presents a procedure which tunes parameters in a
Markov network in order to satisfy a query constraint. Sec-
tion 4 introduces a distance measure that can be used to bound
query change between distributions induced by two Markov
networks. Section 5 concludes the paper.

In this paper, we address these key questions of sensitiv-
ity analysis but with respect to Markov networks. The main
question of interest is the extent to which these promising
results hold in Markov networks as well. There is indeed a
key difference between Bayesian networks and Markov net-
works which appears to suggest a lack of parallels in this
case: whereas the joint distribution induced by a Bayesian
network corresponds to multi-linear functions, the joint dis-
tribution induced by a Markov network corresponds to ratios
of multi-linear functions. As it turns out, however, the con-
ditional probability function has the same functional form in
both Bayesian and Markov networks, as ratios of multi-linear
functions. This similarity turns out to be the key factor here,
allowing us to derive similarly effective results for Markov
networks. This is greatly beneﬁcial because we can answer
the previous three questions in the context of Markov net-
works as well, with the same computational complexity. For
example, we can go through each parameter in a Markov net-
work, compute the minimum single parameter changes nec-
essary to enforce a query constraint, and ﬁnd the one that per-
turbs the network the least. Alternatively, we can change all
parameters in a single clique table, and ﬁnd the change that
minimizes network perturbation. Afterwards, we can com-
pute a bound on any query change using the distance measure
incurred by the parameter change.

Our results, however, point to a main semantical difference
between Bayesian networks and Markov networks, relating
to how one should quantify and measure parameter changes.
That is, how should one quantify a parameter change from .3
to .4? In a Bayesian network, parameters are interpreted as
conditional probabilities, and the measure which quantiﬁes
the change is the relative odds change in the parameter value.
This means query values are much more sensitive to changes
in extreme probability values, whether close to 0 or 1. On the
other hand, in a Markov network, parameters are interpreted
as compatibility ratios, and the measure which quantiﬁes the
change is the relative change in the parameter value. This
difference stems from how the parameters in the two models
are interpreted and will be explained in more depth later.

This paper is structured as follows. Section 2 is dedicated

2 Markov networks
A Markov network M = (G, Θ) consists of two parts: the
network structure in terms of an undirected graph G, and the
parameterization Θ. Each node in the undirected graph G
represents a random variable, and two variables are connected
by an edge if there exists a direct interaction between them.
The absence of an edge between two variables means any po-
tential interaction between them is indirect and conditional
upon other variables.

As an example, consider four individuals, A1, A2, B1, B2,
and we wish to represent the possible transmission of a con-
tagious disease among them. A network structure which con-
sists of these variables is shown in Figure 1. Each pair of vari-
ables {Ai, Bj} is connected by an edge, meaning these pairs
of individuals are in direct contact of each other. However, A1
and A2 are not connected with an edge, meaning they do not
interact directly with each other, although diseases can still
be transmitted between them indirectly through B1 or B2.

To quantify the edges in a Markov network, we ﬁrst deﬁne
cliques in the network structure. A clique C is a maximal set
of variables where each pair of variables in the set C is con-
nected by an edge. In the network structure in Figure 1, there
are four cliques: {A1, B1}, {A1, B2}, {A2, B1}, {A2, B2}.
For each clique C, we deﬁne a table ΘC that assigns a
nonnegative number θc to each instantiation c of variables C,
which measures the relative degree of compatibility associ-
ated with each instantiation c. The numbers given indicate the
level of compatibility between the values of the variables in
the clique, where direct interactions exist between every pair
of variables. For example, in our example network, we deﬁne
the same clique table ΘAi,Bj shown in Table 1 for each clique
{Ai, Bj}. For example, it can be viewed that both Ai and Bj
not having the disease is four times more compatible than
both Ai and Bj having the disease, since θ ¯ai, ¯bj = 4θai,bj .1

1Parameters may be estimated on an ad hoc basis [Geman and
Geman, 1984] or by statistical models. It can be difﬁcult to assign
meanings and numbers to Markov network parameters intuitively
[Pearl, 1988, pages 107–108].

 A1 A2 B1 B2  Ai Bj ΘAi,Bj
ai
ai
¯ai
¯ai

bj
¯bj
bj
¯bj

1
2
3
4

Table 1: The clique table of {Ai, Bj} of the Markov network
whose structure is shown in Figure 1.

2.1 Joint and conditional probabilities induced by

Markov networks

We now proceed to ﬁnd the distribution induced by a Markov
network. If X is the set of all variables in the Markov network
M = (G, Θ), the joint potential ψ over X induced by M is
deﬁned as:

ψ(x) def=

θc.

(1)

(cid:89)

c∼x

That is, ψ(x) is the product of parameters θc in each clique
C, such that instantiation c is consistent with x. The joint dis-
tribution P rM induced by Markov network M is then deﬁned
as its normalized joint potential:

ψ(x)(cid:80)

x ψ(x)

(cid:80)

P rM (x) def=

= Kψ(x),

x ψ(x) is the normalizing constant. From
where K = 1/
this equation, we can easily verify that the speciﬁc parameter
values in a clique are not important, but their ratios are. This
is a major departure from Bayesian networks, where speciﬁc
parameter values in CPTs are important.

Given the network structure in Figure 1 with the parame-
terization in Table 1, we can compute the joint probability
of any full instantiation of variables induced by our example
network. For example, the joint probability of a1, ¯a2, b1, ¯b2 is
equal to Kθa1,b1θa1, ¯b2 θ ¯a2,b1 θ ¯a2, ¯b2 = K · 1· 2· 3· 4, where K
is the normalizing constant.
For an instantiation e of variables E ⊆ X, we can express

the joint probability P rM (e) as:

P rM (e) =

P rM (x) =

Kψ(x) = Kφ(e),

(cid:88)

x∼e

(cid:88)

x∼e

where the function φ(e) is deﬁned as the sum of the potentials
of x which are consistent with e:

φ(e) def=

ψ(x) =

θc,

(2)

(cid:88)

x∼e

(cid:88)

(cid:89)

x∼e

c∼x

and any conditional probability P rM (z | e) as:

P rM (z | e) = P rM (z, e)
P rM (e)

= Kφ(z, e)
Kφ(e)

= φ(z, e)
φ(e) .

(3)

or:

From Equation 2, we can see that both φ(z, e) and φ(e) are
sums of products of the network parameters. Therefore, the
conditional probability P rM (z | e) induced by a Markov net-
work can be expressed as the ratio of two multi-linear func-
tions of the network parameters.

As a comparison, the parameterization Θ of a directed
acyclic graph N for a Bayesian network consists of a set of

(cid:88)

x,u∼x

(cid:89)
(cid:89)
(cid:80)
(cid:80)

θx|u,

(cid:81)
(cid:81)

CPTs, one for each variable X, which are conditional proba-
bilities in the form of θx|u = P r(x | u), where U are the par-
ents of X. The joint distribution P rB induced by Bayesian
network B = (N, Θ) is:

P rB(x) def=

θx|u.

The joint probability P rB(e) can be expressed as:

P rB(e) =

x∼e

x,u∼x

and any conditional probability P rB(z | e) as:

P rB(z | e) = P rB(z, e)
P rB(e)

=

x∼z,e
x∼e

x,u∼x θx|u
x,u∼x θx|u

.

(4)

(cid:80)

We can easily see the similarities between the expressions of
P rM (z | e) and P rB(z | e) in Equations 3 and 4 lie in
the fact that both can be expressed as ratios of two multi-
linear functions of the network parameters. This is different
for joint probabilties, where in a Bayesian network they are
multi-linear functions of the network parameters, while in a
Markov network they are ratios of multi-linear functions of
the network parameters, since the normalizing constant can
be expressed as K = 1/
x ψ(x) = 1/φ(true). This simi-
larity in terms of conditional probabilities will be the basis of
our results in the following section.
3 Tuning of parameters in Markov networks
We now answer the following question in the context of
Markov networks: what is the necessary change we can ap-
ply to certain parameter(s) such that a query constraint is sat-
isﬁed? For example, if P r is the probability distribution in-
duced by a Markov network, we may want to satisfy the query
constraint P r(z | e) ≥ k, for some k ∈ [0, 1].2

Notice that given all the parameters θc in the clique C,
φ(e) can be expressed as a multi-linear function of these pa-
rameters, as shown in Equation 2. Since φ(e) is linear in each
parameter θc, and no two parameters in the same clique are
multiplied together, if we apply a change of ∆θc to each pa-
rameter θc in the clique C, the change in φ(e) is:

(cid:88)

c

∆φ(e) =

∂φ(e)
∂θc

∆θc.

To ensure the query constraint P r(z | e) ≥ k, from Equa-
tion 3, this is equivalent to ensuring the inequality φ(z, e) ≥
k · φ(e). If the current φ values of z, e and e are ϕ(z, e) and
ϕ(e) respectively, this inequality becomes:

ϕ(z, e) + ∆φ(z, e) ≥ k(ϕ(e) + ∆φ(e)),

(cid:88)

(cid:195)

(cid:88)

c

(cid:33)

ϕ(z, e) +

∂φ(z, e)

∆θc ≥ k

ϕ(e) +

∂θc

c

∂φ(e)
∂θc

∆θc

.

Rearranging the terms, we get the following theorem and
corollary.

2We can easily expand our results in this section into other forms
of constraints, such as P r(z | e) ≤ k, P r(z1 | e)−P r(z2 | e) ≥ k,
or P r(z1 | e)/P r(z2 | e) ≥ k.

Theorem 1 To ensure the probability distribution P r in-
duced by a Markov network satisﬁes the query constraint
P r(z | e) ≥ k, we must change each parameter θc in the
clique C by ∆θc such that:

α(θc)∆θc ≥ −(ϕ(z, e) − k · ϕ(e)),

(5)

(cid:88)

c

where ϕ(z, e) and ϕ(e) are the current φ values of z, e and
e respectively, and:

α(θc) = ∂φ(z, e)

∂θc

− k

∂φ(e)
∂θc

.

(6)

Corollary 1 If instead of changing all parameters in the
clique C, we are only allowed to change a single parameter
θc by ∆θc, the solution of Theorem 1 becomes:
α(θc)∆θc ≥ −(ϕ(z, e) − k · ϕ(e)),
which returns a solution interval of possible ∆θc.

(7)

Therefore, to solve for ∆θc in Inequality 7 for all parame-
ters in the Markov network, we need to compute the origi-
nal values ϕ(z, e) and ϕ(e), which should already be known
when computing the original probability of z | e, and the par-
tial derivatives ∂φ(z, e)/∂θc and ∂φ(e)/∂θc, to ﬁnd α(θc) in
Equation 6 for all parameters θc. To do this, we can use a pro-
cedure in time complexity O(n exp(w)) where n is the num-
ber of variables and w is the treewidth of the Markov network,
similar to the one proposed to compute partial derivatives in
a Bayesian network [Darwiche, 2003]. This can greatly help
users debug their network when they are faced with query re-
sults that do not match their expectations.

We now use our example network to illustrate this proce-
dure. The probability distribution P r induced by the cur-
rent parameterization gives us the conditional query value
P r( ¯a2 | a1) = .789. Assume that we would like to change
a single parameter in the clique {A2, B1} to ensure the con-
straint P r( ¯a2 | a1) ≥ .9. We need to compute the α values
for each parameter in the clique using Equation 6, and then
use Inequality 7 to solve for the minimal ∆θc. The solutions
are:

∆θa2,b1 ≤ −2.93;
∆θa2, ¯b1 ≤ −1.467;
∆θ ¯a2,b1 ≥ 12;
∆θ ¯a2, ¯b1 ≥ 8.8.

However, notice that since the parameter values have to
be non-negative, the solution for ∆θa2,b1 is impossible to
achieve. Therefore, no possible single parameter change on
θa2,b1 is possible to ensure our query constraint. However,
we can decrease the parameter θa2, ¯b1 from 2 to .533 to ensure
our query constraint.
If we are interested in changing all the parameters in the
clique {A2, B1} to satisfy our query constraint, we need to
ﬁnd a solution that satisﬁes Inequality 5. As a consequence,
we are now faced with multiple solutions of changing the pa-
rameters, and we want to commit to one which disturbs the
network the least. We will discuss this in the next section us-
ing a distance measure which measures network perturbation.

4 Bounding belief change between Markov

networks using a distance measure

We now answer the following question in the context of
Markov networks: what is the bound on the difference be-
tween some query value induced by two networks that have
the same structure but different parameter values? We will
answer it by using a previously proposed distance measure.
Let P r and P r(cid:48) be two probability distributions over
the same set of variables X. We use a distance measure
D(P r, P r(cid:48)) deﬁned as follows [Chan and Darwiche, 2005]:

D(P r, P r(cid:48)) def= ln max

x

P r(cid:48)(x)
P r(x)

− ln min

x

P r(cid:48)(x)
P r(x) ,

where we will deﬁne 0/0 def= 1 and ∞/∞ def= 1.3
The signiﬁcance of this distance measure is that if the dis-
tance measure is D(P r, P r(cid:48)) = d, the change in the proba-
bility of any conditional query z | e is bounded by:

e−d ≤ O(cid:48)(z | e)
O(z | e)

≤ ed,

where O(cid:48)(z | e) and O(z | e) are the odds of z | e under
distributions P r(cid:48) and P r respectively.4 Given p = P r(cid:48)(z |
e), this result can also be expressed in terms of probabilities:

p · e−d

p(e−d − 1) + 1

≤ P r(cid:48)(z | e) ≤

p · ed

p(ed − 1) + 1 .

(8)

The advantage of this distance measure is that it can be
computed using local information for Bayesian networks.
Given distributions P rB and P rB(cid:48) induced by two Bayesian
networks B and B(cid:48) that differ by only the CPT of a single
variable X, if P r(u) > 0 for every u of parents U, the dis-
tance measure between P rB and P rB(cid:48) can be computed as:

D(P rB, P rB(cid:48)

) = D(ΘX|U, Θ(cid:48)
θ(cid:48)
x|u
θx|u

= ln max
x,u

X|U)
− ln min

x,u

θ(cid:48)
x|u
θx|u

.

(9)

This is useful because we can compute the bound on any
query change using Inequality 8 by computing the local dis-
tance between the CPTs ΘX|U and Θ(cid:48)
X|U in B and B(cid:48). For
example, if X is binary, and we only change a single parame-
ter θx|u while applying a complementary change on θ¯x|u, the
bound on the change in query z | e is given by:
≤ O(θ(cid:48)
x|u)
O(θx|u) ,

O(θx|u)
O(θ(cid:48)
x|u)
x|u) and O(θx|u) are the odds of parameters θ(cid:48)
where O(θ(cid:48)
x|u
and θx|u respectively.
Intuitively this means the relative
change in the query odds is bounded by the relative change
in the parameter odds.

≤ OB(cid:48)(z | e)
OB(z | e)

(10)

ness, symmetry and the triangle inequality.

3This measure is a distance measure because it satisﬁes positive-
4The odds of z | e under distribution P r is deﬁned as: O(z |
= P r(z | e)/(1 − P r(z | e)).

e)

def

We can get a similar result for Markov networks, where
the distance measure between distributions induced by two
Markov networks that differ by only a single clique table can
be computed by the distance measure between the tables.
Theorem 2 Given distributions P rM and P rM(cid:48)
induced by
two Markov networks M and M(cid:48) which differ by only the
parameters in a single clique C, such that the clique tables
are ΘC and Θ(cid:48)
C respectively, the distance measure between
P rM and P rM(cid:48)

is given by:

D(P rM , P rM(cid:48)

) = D(ΘC, Θ(cid:48)
C)
θ(cid:48)
− ln min
c
θc

= ln max

c

c

θ(cid:48)
c
θc

,

(11)

if ∂ψ(x)/∂θc (cid:54)= 0 for all c ∼ x.5
Proof Given instantiation c such that c ∼ x, the joint po-
tential ψ of x is linear in the parameter θc, and the ratio of
ψ(cid:48)(x) and ψ(x) induced by M(cid:48) and M respectively is:

ψ(cid:48)(x)
ψ(x)
if ∂ψ(x)/∂θc (cid:54)= 0. We have:

= θ(cid:48)

c
θc

,

P rM(cid:48)(x)
P rM (x)

= K(cid:48)ψ(cid:48)(x)
Kψ(x)

= K(cid:48)θ(cid:48)

c
Kθc

.

Note that since the parameters have changed, the normalizing
constants K and K(cid:48) for networks M and M(cid:48) respectively are
different. Therefore, the distance measure between P rM and
P rM(cid:48) is given by:

P rM(cid:48)(x)
P rM (x)

x

= ln max

) = ln max

D(P rM , P rM(cid:48)

P rM(cid:48)(x)
P rM (x)
K(cid:48)θ(cid:48)
c
Kθc
θ(cid:48)
= ln max
c
θc
= D(ΘC, Θ(cid:48)
if ∂ψ(x)/∂θc (cid:54)= 0 for all c ∼ x.

− ln min
C),

− ln min
θ(cid:48)
c
θc

c

c

c

x

− ln min
K(cid:48)θ(cid:48)
c
Kθc

c

Therefore, the global distance measure between the distrib-
utions induced is equal to the local distance measure between
the individual clique tables. This is useful for computing the
bound on query change after changing the parameters in a
clique table. For example, what is the bound on the change
in some query value if we apply an arbitrary change on the
single parameter θc? The distance measure in this case is:

(cid:175)(cid:175)(cid:175)(cid:175)ln θ(cid:48)

c
θc

(cid:175)(cid:175)(cid:175)(cid:175) ,

D(P rM , P rM(cid:48)

) =

5This condition is satisﬁed when the network parameters are all
strictly positive. However, this is a sufﬁcient condition, not a nec-
essary condition. The necessary condition is there exists some x
such that ∂ψ(x)/∂θc (cid:54)= 0 for all c. This means that changing any
parameter θc will have some impact on the joint potential ψ.

Figure 2: The amount of parameter change δ that would guar-
antee the query P r(z | e) = .75 to stay within the interval
[.667, .818], as a function of the current Bayesian network
parameter value p.

and the change in query z | e is bounded by:
≤ θ(cid:48)

≤ OM(cid:48)(z | e)
OM (z | e)

θc
θ(cid:48)

.

c
θc

c

(12)

This means for Markov networks, the relative change in
query odds is bounded by the relative change in the parame-
ter itself, not the relative change in the parameter odds as for
Bayesian networks. This is an important distinction between
Markov networks and Bayesian networks.
As an example, suppose we have a network and we want
to ensure the robustness of the query P r(z | e) after we ap-
ply a parameter change. Assume that we deﬁne robustness
as the relative change in any query odds to be less than 1.5.
For example, if currently P r(z | e) = .75, the new query
value must stay in the interval [.667, .818] after the parameter
change. We may ask, how much change can we apply to a
parameter if we want to ensure robustness? The answers for
Bayesian networks and Markov networks are different due to
our previous results, as we will show next.

For a Bayesian network, the amount of permissible change
is determined by Inequality 10, and is plotted in Figure 2. We
can see that the amount of permissible change is small when
the parameters have extreme values close to 0 or 1, because
the relative odds change is large when even a very small ab-
solute change is applied.

On the other hand, for a Markov network, the amount of
permissible change is determined by Inequality 12, and is
plotted in Figure 3. We can see that the amount of permis-
sible change is proportional to the parameter values, because
relative change is used instead of relative odds change.

Therefore, for a Bayesian network, the sensitivity of the
network with respect to a parameter is largest for extreme
values close to 0 or 1, and becomes smaller as the parame-
ter approaches .5, while for a Markov network, the sensitivity
of the network with respect to a parameter is proportional to
its value, and increases as it grows larger.

The distance measure is useful in many aspects of sensi-
tivity analysis. For example, given the possible single para-
meter changes in our example in the previous section, we can
choose the one which perturbs the network the least accord-
ing to the distance measure. In this case, the most preferred

0.20.40.60.81p-0.2-0.15-0.1-0.050.050.10.150.2δin how their parameters are interpreted. Our results allow
for the effective debugging of Markov networks by chang-
ing parameters in a single potential for ensuring query con-
straints. We also showed how to compute a bound on any
query change between two Markov networks that have the
same structure, but differ in the parameters of a single poten-
tial, and to choose parameter changes which minimize net-
work disturbance. Finally, we identiﬁed a key semantical dif-
ference between Bayesian networks and Markov networks,
relating to how parameter changes should be quantiﬁed.
References
[Castillo et al., 1997] Enrique Castillo,

Jos´e Manuel
Guti´errez, and Ali S. Hadi. Sensitivity analysis in discrete
Bayesian networks.
IEEE Transactions on Systems,
Man, and Cybernetics, Part A (Systems and Humans),
27:412–423, 1997.

[Chan and Darwiche, 2002] Hei Chan and Adnan Darwiche.
When do numbers really matter? Journal of Artiﬁcial In-
telligence Research, 17:265–287, 2002.

[Chan and Darwiche, 2004] Hei Chan and Adnan Darwiche.
Sensitivity analysis in Bayesian networks: From single
to multiple parameters.
In Proceedings of the Twentieth
Conference on Uncertainty in Artiﬁcial Intelligence (UAI),
pages 67–75, Arlington, Virginia, 2004. AUAI Press.

[Chan and Darwiche, 2005] Hei Chan and Adnan Darwiche.
A distance measure for bounding probabilistic belief
change. International Journal of Approximate Reasoning,
38:149–174, 2005.

[Darwiche, 2003] Adnan Darwiche. A differential approach
to inference in Bayesian networks. Journal of the ACM,
50:280–305, 2003.

[Geman and Geman, 1984] Stuart Geman and Donald Ge-
man. Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE Transactions in Pat-
tern Analysis and Machine Intelligence, 6:721–741, 1984.
[Jensen, 1999] Finn V. Jensen. Gradient descent training of
Bayesian networks. In Proceedings of the Fifth European
Conference on Symbolic and Quantitative Approaches to
Reasoning with Uncertainty (ECSQARU), pages 190–200,
Berlin, Germany, 1999. Springer-Verlag.

[Jensen, 2001] Finn V. Jensen. Bayesian Networks and De-

[Kjærulff and van der Gaag, 2000] Uffe

cision Graphs. Springer-Verlag, New York, 2001.
Kjærulff

and
Linda C. van der Gaag. Making sensitivity analysis
computationally efﬁcient. In Proceedings of the Sixteenth
Conference on Uncertainty in Artiﬁcial Intelligence (UAI),
pages 317–325, San Francisco, California, 2000. Morgan
Kaufmann Publishers.

[Laskey, 1995] Kathryn B. Laskey. Sensitivity analysis for
probability assessments in Bayesian networks.
IEEE
Transactions on Systems, Man, and Cybernetics, 25:901–
909, 1995.

[Pearl, 1988] Judea Pearl. Probabilistic Reasoning in Intel-
ligent Systems: Networks of Plausible Inference. Morgan
Kaufmann Publishers, San Francisco, California, 1988.

Figure 3: The amount of parameter change δ that would guar-
antee the query P r(z | e) = .75 to stay within the interval
[.667, .818], as a function of the current Markov network pa-
rameter value p.

single parameter change is to decrease the parameter θa2, ¯b1
from 2 to .533, incurring a distance measure of 1.322.

Moreover, we can also use the distance measure to ﬁnd the
optimal solution for changing all parameters in a clique table,
which is the solution that satisﬁes Inequality 5 and minimizes
the distance measure. As the distance measure D(ΘC, Θ(cid:48)
C)
is computed using the maximum relative change in the pa-
rameters, the relative changes in all parameters must be the
same for the optimal solution, because to obtain another solu-
tion that satisﬁes the constraint, we must increase the relative
change in one parameter while decreasing the relative change
in another, thereby incurring a larger distance measure.

For example, in our example from the previous section, to
ensure our query constraint, we would like to decrease the pa-
rameters θa2,b1 and θa2, ¯b1 and increase the parameters θ ¯a2,b1
and θ ¯a2, ¯b1, such that the relative changes in all parameters are
the same. However, since only the ratios between the para-
meters are important, we can keep the ﬁrst two parameters
constant and only increase the last two parameters. The opti-
mal solution of Inequality 5 is:

∆θa2,b1 = 0;
∆θa2, ¯b1 = 0;
∆θ ¯a2,b1 = 6.257;
∆θ ¯a2, ¯b1 = 8.676.

This parameter change incurs a distance measure of .350. It
involves all parameters in the clique table and incurs a smaller
distance measure than any of the single parameter changes
computed earlier.

Finally, if we change the parameters in different clique ta-
bles which do not share any variables, the distance measure
can be computed as the sum of the local distance measures
between the clique tables. This result is also similar to the
case of Bayesian networks [Chan and Darwiche, 2004].

5 Conclusion
In this paper, we expanded some of the main results in
the topic of sensitivity analysis from Bayesian networks to
Markov networks. We were able to ﬁnd many parallels be-
tween the results in both models even given the differences

0.20.40.60.81p-0.4-0.3-0.2-0.10.10.20.30.4δ
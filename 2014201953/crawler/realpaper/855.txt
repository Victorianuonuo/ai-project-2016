Improving Embeddings by Flexible Exploitation of Side Information

Ali Ghodsi

Dana Wilkinson

University of Waterloo

University of Waterloo

Finnegan Southey

Google Inc.

Abstract

Dimensionality reduction is a much-studied task in
machine learning in which high-dimensional data
is mapped, possibly via a non-linear transforma-
tion, onto a low-dimensional manifold. The result-
ing embeddings, however, may fail to capture fea-
tures of interest. One solution is to learn a distance
metric which prefers embeddings that capture the
salient features. We propose a novel approach to
learning a metric from side information to guide the
embedding process.
Our approach admits the use of two kinds of side
information. The ﬁrst kind is class-equivalence in-
formation, where some limited number of pairwise
“same/different class” statements are known. The
second form of side information is a limited set of
distances between pairs of points in the target met-
ric space. We demonstrate the effectiveness of the
method by producing embeddings that capture fea-
tures of interest.

1 Introduction

Many machine learning approaches use distances between
data points as a discriminating factor.
In some cases, the
plain Euclidean distance between points is meaningful and
such methods can work well. Such distances are the starting
point of many popular dimensionality-reduction techniques,
such as MDS [Cox and Cox, 2001], LLE [Roweis and Saul,
2000], Isomap [Tenenbaum et al., 2000], LEM [Belkin and
Niyogi, 2003], and SDE [Weinberger and Saul, 2004]. Often,
however, this distance does not capture the distinction one is
trying to characterize. Approaches such as kernelized meth-
ods address this issue by mapping the points into new spaces
where Euclidean distances may be more useful. An alterna-
tive approach is to construct a new distance metric over the
points and use it in place of Euclidean distances, as explored
by Xing et al. [Xing et al., 2003]. In this approach, some
amount of side information is employed to learn a distance
metric that captures the desired distinction.

In some situations it may be possible to obtain a small
amount of information regarding the similarity of some points
in a particular data set. Consider a large collection of images.

While it would be expensive to have a human examine and
label the entire set, it might be feasible to have a human ex-
amine a small subset and provide information on how pairs of
images relate to each other. Similarly, some expensive exper-
iment might yield useful similarity information for a subset
of the data. This paper will show how two kinds of such side
information can be used in a preprocessing step for embed-
ding techniques, leading to embeddings that capture the tar-
get properties. The method improves over earlier work in this
area by using standard ”off-the-shelf” optimization methods
and by allowing more ﬂexibility in the side information used.
The ﬁrst kind of side information identiﬁes pairs of points
that belong to the same class or pairs that belong to differ-
ent classes. Note that this information is about the class-
equivalence/inequivalence of points but does not give the ac-
tual class labels. Consider four points, x1, x2, x3, and x4.
Given side information that x1 and x2 are in the same class,
and that x3 and x4 also share a class, we cannot be certain
whether the four points fall into one or two classes.

The second kind of side information takes the form of par-
tial information about the similarity of points in the form of
distances. For some pairs of points, one may have distances
corresponding to an informative metric space, obtained by
some expensive measurement. Molecular conformation prob-
lems are a good example. Some of the distances between
pairs of atoms in a given molecule can be determined by nu-
clear magnetic resonance spectroscopy, but the procedure is
costly and certainly not guaranteed to provide all such dis-
tances. Determining the rest of the distances can help in clas-
sifying the conformation of the molecule (see [Crippen and
Havel, 1988] for more details).

These two kinds of side information can be used to learn
a new distance metric. The distances between points in this
new space can then be used with any embedding technique.
We start by showing how class-equivalence side information
can be used to learn such a metric and show the effect of us-
ing this new metric with a variety of embedding techniques.
We then show how multiple applications of this method can
be used to combine several learned distance metrics together
into one in order to capture multiple attributes in the embed-
ding. We also discuss how the class-equivalence approach
can be kernelized, allowing for nonlinear transformations of
the metric. Finally, we formulate a method for using the sec-
ond type of side information, where we have partial infor-

IJCAI-07

810

mation about desirable target distances between some pairs
of points. Experimental results demonstrate the value of this
preprocessing step.

1.1 Related Work
Our use of class equivalence relations to learn a metric fol-
lows [Xing et al., 2003]. In that work, a new distance met-
ric is learned by considering side information. Xing et al.
used side information identifying pairs of points as “similar”.
They then construct a metric that minimizes the distance be-
tween all such pairs of points. At the same time, they at-
tempt to ensure that all “dissimilar” points are separated by
some minimal distance. By default, they consider all points
not explicitly identiﬁed as similar to be dissimilar. They
present algorithms for optimizing this objective and show re-
sults using the learned distance for clustering, an application
in which an appropriate distance metric is crucial. Other work
on learning distance metrics has been primarily focused on
classiﬁcation, with side information in the form of class la-
bels [Globerson and Roweis, 2006; Weinberger et al., 2006;
Goldberger et al., 2005].

2 Learning a Metric from Class-Equivalence

Side Information

We will start with the simpler similar/dissimilar pair case, for-
malizing this notion of side information and stating an objec-
tive that will be optimized using standard semideﬁnite pro-
gramming software. The use of this kind of side information
allows one to select a characteristic for distinction. For ex-
ample, one may have several images of faces. One sensible
cluster is by presence or absence of a beard. Another is by the
presence or absence of glasses. Different indications of simi-
larity allow the capturing of either distinction. The following
takes the same basic approach as [Xing et al., 2003] but of-
fers a simpler optimization procedure using “off-the-shelf”
optimization methods instead of their iterative method.

2.1 Derivation
Given a set of t points, {xi}t
, we identify two kinds
of class-related side information. First, a set class-equivalent
(or similar) pairs of points

i=1 ⊆ Rn

S :

(xi, xj ) ∈ S if xi and xj are similar

and, second, a set of class-inequivalent (dissimilar) pairs

O :

(xi, xj) ∈ O if xi and xj are dissimilar

We then wish to learn a matrix A that induces a distance

metric D(A)

over the points

(cid:2)

D(A)(xi, xj) = (cid:4)xi − xj (cid:4)
A

=

(xi − xj)T A(xi − xj)

where A (cid:5) 0.

We deﬁne the following loss function, which, when mini-
mized, attempts to minimize the squared induced distance be-
tween similar points and maximize the squared induced dis-
tance between dissimilar points
(cid:4)xi − xj(cid:4)2

(cid:4)xi − xj(cid:4)2

(cid:3)

(cid:3)

L(A) =

−

A

A

(xi,xj)∈S

(xi,xj )∈O

The optimization problem then becomes

min

A

L(A) s.t. A (cid:5) 0 and Tr(A) = 1

(1)

The ﬁrst constraint (positive semideﬁniteness) ensures a
Euclidean metric. The second excludes the trivial solution
where all distances are zero. The constant in this constraint is
arbitrary, affecting only the scale of the space. This objective
will be optimized using standard semideﬁnite programming
software and so it must be converted to a linear objective.
Expanding the loss function

L(A) =

(cid:3)
(cid:3)

(xi,xj)∈S

(xi − xj)T A(xi − xj) −

(xi − xj )T A(xi − xj )

(xi,xj)∈O

each squared distance term must be converted. We start
by observing that vec(XY Z) = (Z T ⊗ X)vec(Y ), where
vec() simply rearranges a matrix into a vector by concate-
nating columns and ⊗ is the Kronecker product. Note that
(xi − xj)T A(xi − xj) = vec((xi − xj )T A(xi − xj )) be-
cause the left-hand side is a scalar. Using this and the fact
that (aT ⊗ bT ) = vec(baT )T
, we can rewrite the squared
distance terms as

(xi − xj )T A(xi − xj )

= vec((xi − xj )T A(xi − xj))
= ((xi − xj )T ⊗ (xi − xj)T )vec(A)
= vec((xi − xj )(xi − xj)T )T
vec(A)
vec((xi − xj )(xi − xj)T )
= vec(A)T

The linear loss function is then

L(A) =

vec(A)T

vec((xi − xj )(xi − xj)T ) −

(cid:3)
(cid:3)

(xi,xj )∈S

(xi,xj )∈O

= vec(A)T
(cid:3)

−

vec(A)T
⎡
⎣ (cid:3)

vec((xi − xj)(xi − xj )T )

vec((xi − xj)(xi − xj )T )

(xi,xj )∈S

⎤
⎦
vec((xi − xj)(xi − xj)T )

(xi,xj)∈O

This form, along with the two constraints from (1), can
be readily submitted to an SDP solver to optimize the matrix
A1. Aside from this convenient form, this formulation has
other advantages over that used by Xing et al., especially with
respect to the side information we can convey.

Xing et al. require at least one dissimilar pair in order to
avoid the trivial solution where all distances are zero. The
constraint on the trace that we employ means that we need
not place any restrictions on pairings. Side information can
consist of similar pairs only, dissimilar pairs only, or any com-
bination of the two; the method still avoids trivial solutions.

1We use the MATLAB SDP solver SeDuMi [Sturm, 1999]

IJCAI-07

811

Furthermore, in the absence of speciﬁc information regarding
dissimilarities, Xing et al. assume that all points not explicitly
identiﬁed as similar are dissimilar. This information may be
misleading, forcing the algorithm to separate points that may,
in fact, be similar. The formulation presented here allows one
to specify only the side information one actually has, parti-
tioning the pairings into similar, dissimilar, and unknown.

2.2 Results

Once a metric has been learned, the new distances can be used
with any embedding technique. To demonstrate the beneﬁts
of this approach, we generated embeddings with and without
the preprocessing step, informing the preprocessed embed-
dings about some characteristic of interest, and then exam-
ined the result. To show that the side information truly in-
forms the embedding, two sets of informed embeddings were
generated from the same data set, each with side information
pertaining to two different characteristics. Structure in the
resulting embeddings that captures the two different charac-
teristics within a single data set is evidence that the method
works as intended.

Embeddings were generated using a variety of techniques
including MDS, Isomap, LEM, LLE, and SDE. The data
set consisted of 200 images of faces and two distinctions
are identiﬁed by side information:
faces with beards vs.
faces without beards and faces with glasses vs. faces with-
out glasses. In one set of experiments (which we will call
all-similar), all similar pairs were identiﬁed but no dissim-
ilar pairs. The second set (which we will call ﬁve-pairs),
simulating a situation where labelling is expensive, identi-
ﬁes only four similar pairs and one dissimilar pair. The pairs
were selected at random. Techniques using the preprocessor
are labelled as “Equivalence-Informed” (e.g., Equivalence-
Informed MDS). In each case, a two-dimensional embedding
is shown. The two classes are marked with X and O, respec-
tively. Additionally, a subset of the images are displayed on
the plot (plots with all images are unreadable). Some plots
have been omitted due to space constraints.

Inspection reveals that, in general, the informed versions
of these embeddings manage to separate the data based on
the target property, whereas the uninformed versions are typ-
ically chaotic. Even when separation is poor in the informed
embedding, there is usually much more structure than in the
uninformed embedding. Equivalence-Informed MDS (Fig-
ures 1 and 2, ﬁve-pairs beards and all-pairs glasses omitted)
offers mixed results, especially with ﬁve-pairs for glasses vs.
no glasses (Figure 2), but MDS is a comparatively crude em-
bedding technique in any case. Isomap with all-pairs works
very well (Figure 3, glasses omitted) and with ﬁve-pairs it still
manages to group the two classes but does not separate them
well (Figure 4, glasses omitted). LEM separates well with
all-pairs (Figure 5, glasses omitted) but shows weak separa-
tion in the ﬁve-pairs case (Figure 6, glasses omitted). The
same effective grouping and varying separation occurs for
LLE (ﬁgures omitted). Finally, SDE groups well, but is weak
in separation in all cases (Figures 7 and 8, glasses omitted).

These results show that the side information can be effec-
tively exploited to capture characteristics of interest where
uninformed embedding techniques fail to automatically dis-

cover them. Moreover, two different characteristics (beards
and glasses) have been captured within the same data set,
even when using only small quantities of class-equivalence
side information. Finally, the preprocessor is effective with a
wide range of embedding techniques.

MDS

Equivalence−informed MDS

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d
 
d
n
2

i

1st dimension

1st dimension

Figure 1: MDS and Equivalence-Informed MDS with
(un)bearded distinction (all equivalent pairs)

MDS

Equivalence−informed MDS

i

n
o
s
n
e
m
d

i

i

n
o
s
n
e
m
d

i

 

d
n
2

 

d
n
2

1st dimension

1st dimension

Figure 2: MDS and Equivalence-Informed MDS with
glasses/no glasses distinction (4 equivalent/1 inequiv. pairs)

Isomap

Equivalence−informed Isomap 

i

n
o
s
n
e
m
d

i

i

n
o
s
n
e
m
d

i

 

d
n
2

 

d
n
2

1st dimension

1st dimension

Figure 3:
(un)bearded distinction (all equivalent pairs)

Isomap and Equivalence-Informed Isomap with

Finally, Figures 9 and 10 are provided to give a numerical
notion of the improvement offered by informed embeddings.
The plots show the misclassiﬁcation rate of k-means cluster-
ing in recovering the true classes of the images, after applying
each of the uninformed and informed embeddings (indicated
by, e.g., ”MDS” and ”Inf. MDS”). The informed methods
achieve a lower error rate than their uninformed counterparts
by better separating the data (except 4-pairs SDE).

IJCAI-07

812

Isomap

Equivalence−informed Isomap 

SDE

Equivalence−informed SDE

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d
 
d
n
2

i

1st dimension

1st dimension

1st dimension

1st dimension

Figure 4:
(un)bearded distinction (4 equivalent/1 inequivalent pairs)

Isomap and Equivalence-Informed Isomap with

Figure 7:
(un)bearded distinction (all equivalent pairs)

SDE and Equivalence-Informed SDE with

LEM

Equivalence−informed LEM

SDE

Equivalence−informed SDE

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d
 
d
n
2

i

1st dimension

1st dimension

1st dimension

1st dimension

Figure 5:
(un)bearded distinction (all equivalent pairs)

LEM and Equivalence-Informed LEM with

Figure 8:
(un)bearded distinction (4 equivalent/1 inequivalent pairs)

SDE and Equivalence-Informed SDE with

3 Multiple-Attribute Metric Learning with

Class-Equivalence Side Information

In some cases, there may be more than one distinction to cap-
ture in data (e.g., glasses vs. no glasses and beards vs. no
beards). The method above can be extended to construct a
distance metric using multiple sets of side information, each
corresponding to a different criterion. Multiple metrics are
learned and then combined to form a single metric.

Suppose there are k different sets of side information over
the same set of points. Using the optimization described
above, k transformations, A1, · · · , Ak can be learned. From
each Ai, the dominant eigenvector vi can be taken and a new
matrix assembled where each column is one of these eigen-
vk ]. This combined matrix de-
vectors,

¯A = [ v1

· · ·

LEM

Equivalence−informed LEM

i

n
o
s
n
e
m
d

i

i

n
o
s
n
e
m
d

i

 

d
n
2

 

d
n
2

1st dimension

1st dimension

ﬁnes a rank k linear subspace onto which we can project
¯A, one
the data. Applying singular value decomposition to
can form a matrix U from the k eigenvectors corresponding
to the largest eigenvalues. The ﬁnal transformation is then
ˆA = U U T
, which is an orthonormal basis combining the dis-
tinctions drawn by each kind of side information.

3.1 Results

We demonstrate the effectiveness of this method by creating
a single embedding that captures two distinctions. We re-
peat the earlier experiments but using both the (un)bearded
and glasses/no glasses criteria together. Results were gen-
erated for all the embedding methods and their multiple-
attribute, equivalence-informed versions but some are omit-
ted for space. In all cases, the side information consisted of
all similar pairs and no dissimilar pairs.

The informed embeddings tend to discover the four distinct
categories of faces (combinations of the presence and absence
of beards and glasses). Informed MDS (Figure 11) separates

t

e
a
R

 
r
o
r
r

E

0.5

0.4

0.3

0.2

0.1

0

Inf. M D S
M D S

Inf. Iso m ap
Iso m ap

Inf. LLE
LLE

Inf. LE M
LE M

Inf. S D E
S D E

t

e
a
R

 
r
o
r
r

E

0.5

0.4

0.3

0.2

0.1

0

Inf. M D S
M D S

Inf. Iso m ap
Iso m ap

Inf. LLE
LLE

Inf. LE M
LE M

Inf. S D E
S D E

Figure 6:
(un)bearded distinction (4 equivalent/1 inequivalent pairs)

LEM and Equivalence-Informed LEM with

Figure 9: Clustering Error for beards vs. no beards (Left: all
equivalent pairs Right: 4 equiv/1 inequiv pairs)

IJCAI-07

813

e
t
a
R

 
r
o
r
r

E

0.5

0.4

0.3

0.2

0.1

0

Inf. M D S
M D S

Inf. Iso m ap
Iso m ap

Inf. LLE
LLE

Inf. LE M
LE M

Inf. S D E
S D E

e
t
a
R

 
r
o
r
r

E

0.5

0.4

0.3

0.2

0.1

0

Inf. M D S
M D S

Inf. Iso m ap
Iso m ap

Inf. LLE
LLE

Inf. LE M
LE M

Inf. S D E
S D E

Figure 10: Clustering Error for glasses vs. no glasses (Left:
all equivalent pairs Right: 4 equiv/1 inequiv pairs)

these quite well, as do Informed Isomap (Figure 12) and In-
formed LLE (Figure 13). Results with Informed LEM (ﬁgure
omitted) are roughly grouped. Finally, Informed SDE (Figure
14) does has trouble separating two categories, but a lot of
useful structure is present. In all cases, the informed embed-
dings have distinct structure that the uninformed embeddings
fail to capture. The only feature the uninformed embeddings
seem to capture is the relative darkness of the images.

MDS

Multi−attribute−informed−MDS

LLE

Multi−attribute−informed−LLE

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d
 
d
n
2

i

1st dimension

1st dimension

Figure 13: LLE and Multi-Attribute Informed LLE with
(un)bearded and (no)glasses attributes (all equivalent pairs)

SDE

Multi−attribute−informed−SDE

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d

i

i

n
o
s
n
e
m
d

i

 

d
n
2

 

d
n
2

1st dimension

1st dimension

Figure 11: MDS and Multi-Attribute Informed MDS with
(un)bearded and (no)glasses attributes (all equivalent pairs)

These results show that the multiple-attribute approach can
capture the desired properties in a single embedding, while
the uninformed versions fail to capture either of the properties
of interest. Again, the method is straightforward to apply and
works with a variety of embedding techniques.

4 Kernelizing Metric Learning
Not uncommonly, nonlinear transformations of the data are
required to successfully apply learning algorithms. One ef-
ﬁcient method for doing this is via a kernel that computes a

1st dimension

1st dimension

Figure 14: SDE and Multi-Attribute Informed SDE with
(un)bearded and (no)glasses attributes (all equivalent pairs)

similarity measure between any two data points. In this sec-
tion, we show how to learn a metric in the feature space im-
plied by a kernel, allowing our use of side information to be
extended to nonlinear mappings of the data.

Conceptually, the points are mapped into a feature space
by some nonlinear mapping Φ() and a metric is learned in
that space. Actually applying the mapping is often undesir-
able (e.g., the feature vectors may have inﬁnite dimension),
so we employ the well known kernel trick, using some ker-
nel K(xi, xj ) that computes inner products between feature
vectors without explicitly constructing them.

The squared distances in our objective have the form (xi −
xj)T A(xi − xj ). Because A is positive semideﬁnite, it can
be decomposed into A = W W T
. W can then be expressed
as a linear combination of the data points, W = Xβ, via the
kernel trick. Rewriting the squared distance

Isomap

Multi−attribute−informed−Isomap

(xi − xj)T A(xi − xj)

i

n
o
s
n
e
m
d

i

i

n
o
s
n
e
m
d

i

 

d
n
2

 

d
n
2

= (xi − xj)T W W T (xi − xj)
= (xi − xj)T XββT X T (xi − xj )
= (xT

j X)ββT (X T xi − X T xj)
= (X T xi − X T xj)T A(X T xi − X T xj )

i X − xT

1st dimension

1st dimension

Figure 12: Isomap and Multi-Attribute Informed Isomap with
(un)bearded and (no)glasses attributes (all equivalent pairs)

where A = ββT
, we have now expressed the distance in
terms of the matrix to be learned, A, and inner products be-
tween data points, which can be computed via the kernel, K.
The optimization of A proceeds as in the non-kernelized ver-
sion but with one additional constraint. The rank of A must
be t because β is t by n and A =ββT
. However, this con-

IJCAI-07

814

straint is problematic, so we drop it during the optimization
and then ﬁnd a rank t approximation of A via singular value
decomposition.

(cid:10)

By decomposing Q = ST S, a matrix of the form

J =

I

(Svec(A))T

Svec(A)

2vec(A)

T R + t

(cid:11)

5 Using Partial Distance Side Information

We will now discuss the use of the second kind of side in-
formation mentioned at the beginning of this paper. Recall
that in this case, we have side information that gives exact
distances between some pairs of points in some space natural
to the data. Such partial distance information can be used to
inform our learned metric. Given a set of similarities in the
form of pairs for which distances are known

S :

(xi, xj) ∈ S if the target distance dij is known

can be constructed. By the Schur complement, if J (cid:5) 0, then
the following relation holds

2vec(A)T R + t − vec(A)T ST Svec(A) ≥ 0

Note that this quantity is scalar. As long as J is positive
semideﬁnite, the scalar t is an upper bound on the loss

vec(A)T ST Svec(A) − 2vec(A)T R
vec(A)T Qvec(A) − 2vec(A)T R

=

≤ t

the following cost function is employed, which attempts to
preserve the set of known distances

(cid:3)

(cid:8)(cid:8)(cid:8)(cid:4)xi − xj (cid:4)2

− dij

A

(cid:8)(cid:8)(cid:8)2

L(A) =

(xi,xj )∈S

Therefore, minimizing t subject to J (cid:5) 0 also minimizes the
objective. This optimization problem can be readily solved
by standard semideﬁnite programming software
s.t. A (cid:5) 0 and J (cid:5) 0

min

t

A

The optimization problem is then

5.1 Results

L(A)

min

A

s.t. A (cid:5) 0

A convenient form for this optimization is obtained using the
same approach for quadratic terms used earlier

(cid:8)(cid:8)(cid:8)2

− dij

(cid:8)(cid:8)(cid:8)(cid:4)xi − xj (cid:4)2
(cid:8)(cid:8)

vec(A)T

A

vec(Bij ) − dij

(cid:8)(cid:8)2

(cid:3)
(cid:3)
(cid:3)

(xi,xj )∈S

(xi,xj )∈S

L(A) =

=

=

vec(A)T

vec(Bij )vec(Bij )T

vec(A)

(xi,xj )∈S

+d2

ij − 2dijvec(A)T

vec(Bij)

where Bij = (xi − xj)(xi − xj)T
ij term in
the above is a constant so it can be dropped for the purposes
of minimization, and the loss rewritten as

. Note that the d2

⎡
⎣ (cid:3)

(xi,xj )∈S

−2

vec(A)

vec(Bij )vec(Bij )T
⎤
(cid:3)
⎦
dijvec(Bij )

L(A) = vec(A)T

(xi,xj)∈S
= vec(A)T [Qvec(A) − 2R]

(cid:9)

(cid:9)
where Q =

(xi,xj)∈S vec(Bij)vec(Bij)T

and R =
dijvec(Bij ). This objective is still quadratic but
a linear objective can be obtained via the Schur complement
[Boyd and Vandenberghe, 2004], as we outline here.

(xi,xj)∈S

The Schur complement relates the positive semideﬁnite-
ness of the matrix on the left and the expression on the right
by an if-and-only-if relation

(cid:5) 0 ⇔ Z − Y T X −1Y (cid:5) 0

(cid:10)

(cid:11)

X Y
Y T Z

Again, we demonstrate effectiveness by generating unin-
formed and informed embeddings but with target distances
speciﬁed for some pairs of points. The distances used here are
a portion of the distance DA
computed in Section 2.2. That is,
there are 200 images of faces and two distinctions are identi-
ﬁed by class-equivalence side information: faces with beards
vs. faces without beards and faces with glasses vs. faces with-
out glasses. For each distinction, the distance matrix DA
is
computed, where no dissimilar pairs but all similar pairs are
identiﬁed. Then 30 pairs of points are selected at random
and their distances in DA
are used as the side information for
learning a new distance metric. This new distance metric is
then used to inform the embeddings.

Results using these distances are somewhat noisier than
with the class-equivalence informed embeddings but still pro-
vide good structure compared with the uninformed versions,
and they group the data. Informed MDS (Figure 15, glasses
omitted) and Informed Isomap (Figures 16, glassed omitted)
group quite effectively, with a few images placed inappropri-
ately.
Informed LEM (Figures 17 and 18), Informed LLE
(ﬁgures omitted), and Informed SDE (ﬁgures omitted) all
give results similar to one another, and all show noisier per-
formance on the glasses attribute than on the beards (beards
are likely easier since they create a more substantial differ-
ence in an image than glasses). All methods group bearded-
ness quite well. These last results show that preprocessing
based on partial distance information is effective, although
less so than the class-equivalence information. The informed
embeddings are still more useful than the uninformed.

6 Conclusions

Many machine learning techniques handle complex data by
mapping it into new spaces and then applying standard tech-
niques, often utilizing distances in the new space. Learning a
distance metric directly is an elegant means to this end. This
research shows how side information of two forms, class-
equivalence information and partial distance information, can

IJCAI-07

815

MDS

Distance−informed MDS

LEM

Distance−informed LEM

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d
 
d
n
2

i

1st dimension

1st dimension

1st dimension

1st dimension

Figure 15: MDS and Distance-Informed MDS with
(un)bearded distinction (30 distances)

Figure 18:
(no)glasses distinction (30 distances)

LEM and Distance-Informed LEM with

Isomap

Distance−informed Isomap

i

n
o
s
n
e
m
d
 
d
n
2

i

i

n
o
s
n
e
m
d
 
d
n
2

i

1st dimension

1st dimension

Figure 16:
(un)bearded distinction (30 distances)

Isomap and Distance-Informed Isomap with

be used to learn a new distance as a preprocessing step for
embedding. The results demonstrate that side information al-
lows us to capture attributes of the data within the embedding
in a controllable manner. Even a small amount of side infor-
mation can improve the embedding’s structure. Furthermore,
the method can be kernelized and also used to capture mul-
tiple attributes in the same embedding. Its advantages over
existing metric learning methods are a simpler optimization
formulation that can be readily solved with off-the-shelf soft-
ware and greater ﬂexibility in the nature of the side informa-
tion one can use.
In all, this technique represents a useful
addition to our toolbox for informed embeddings.

LEM

Distance−informed LEM

i

n
o
s
n
e
m
d
d
n
2

 

i

i

n
o
s
n
e
m
d
d
n
2

 

i

1st dimension

1st dimension

Figure 17:
(un)bearded distinction (30 distances)

LEM and Distance-Informed LEM with

References
[Belkin and Niyogi, 2003] M. Belkin and P. Niyogi. Lapla-
cian eigenmaps for dimensionality reduction and data rep-
resentation. Neural Computation, 15(6):1373–1396, 2003.
[Boyd and Vandenberghe, 2004] S. Boyd and L. Vanden-
berghe. Convex Optimization. Cambridge University
Press, New York, New York, 2004.

[Cox and Cox, 2001] T. Cox and M. Cox. Multidimensional

Scaling. Chapman Hall, 2nd edition edition, 2001.

[Crippen and Havel, 1988] G. Crippen and T. Havel. Dis-
tance geometry and molecular conformation. Research
Studies Press Ltd., Letchworth, 1988.

[Globerson and Roweis, 2006] Amir Globerson and Sam
Roweis. Metric learning by collapsing classes. In NIPS-
2005, pages 451–458. 2006.

[Goldberger et al., 2005] Jacob Goldberger, Sam Roweis,
Geoffrey Hinton, and Ruslan Salakhutdinov. Neighbour-
hood components analysis. In NIPS-2004, pages 513–520.
2005.

[Roweis and Saul, 2000] Sam Roweis and Lawrence Saul.
Nonlinear dimensionality reduction by locally linear em-
bedding. Science, 290(5500):2323–2326, December 2000.
[Sturm, 1999] J. Sturm. Using SeDuMi 1.02, a MATLAB
toolbox for optimization over symmetric cones. Optim.
Methods Softw., 11/12(1-4):625–653, 1999.

[Tenenbaum et al., 2000] J. Tenenbaum, V. de Silva, and
J. Langford. A global geometric framework for nonlinear
dimensionality reduction. Science, 290:2319–2323, 2000.
[Weinberger and Saul, 2004] K. Q. Weinberger and L. K.
Saul. Unsupervised learning of image manifolds by
semideﬁnite programming.
In CVPR-2004, volume II,
pages 988–995, 2004.

[Weinberger et al., 2006] Kilian Weinberger, John Blitzer,
and Lawrence Saul. Distance metric learning for large
margin nearest neighbor classiﬁcation.
In NIPS-2005,
pages 1475–1482. 2006.

[Xing et al., 2003] Eric P. Xing, Andrew Y. Ng, Michael I.
Jordan, and Stuart Russell. Distance metric learning with
application to clustering with side-information. In NIPS-
2002, pages 505–512. 2003.

IJCAI-07

816


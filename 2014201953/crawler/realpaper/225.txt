Using Predictive Representations to Improve Generalization in

Reinforcement Learning

Eddie J. Rafols, Mark B. Ring, Richard S. Sutton, Brian Tanner

Department of Computing Science

University of Alberta

Edmonton, Alberta, Canada T6G 2E8

{erafols,markring,sutton,btanner}@cs.ualberta.ca

Abstract

The predictive representations hypothesis holds
that particularly good generalization will result
from representing the state of the world in terms
of predictions about possible future experience.
This hypothesis has been a central motivation be-
hind recent research in, for example, PSRs and
TD networks.
In this paper we present the ﬁrst
explicit investigation of this hypothesis. We show
in a reinforcement-learning example (a grid-world
navigation task) that a predictive representation in
tabular form can learn much faster than both the
tabular explicit-state representation and a tabular
history-based method.

Introduction

1
A predictive representation is one that describes the world in
terms of predictions about future observations. The predictive
representations hypothesis holds that such representations are
particularly good for generalization. A good representation is
one that captures regularities of the environment in a form
useful to the learning agent; and in a reinforcement-learning
task, something is “useful” if it increases the agent’s ability to
receive rewards. Thus, representations generalize well when
the regularities they capture allow an agent to learn more ef-
ﬁciently how to increase its cumulative reward.

Why do we believe that predictive representations might
generalize particularly well? Good generalization tends to
result when similar situations have similar representations. In
interactive tasks, two situations are similar when executing
an action sequence in one leads to about the same results as
executing the same action sequence in the other, regardless of
what the action sequence is. The more similar the results tend
to be, the more similar the situations are, and, therefore, the
more similar their predictive representations will be.

The predictive representations hypothesis makes an espe-
cially broad claim, and as a result it is especially resistant to
testing or proof. In particular, questions regarding representa-
tion and generalization often involve the confounding issues
of function approximation and representation acquisition —
issues that we have developed speciﬁc measures to avoid. In
reinforcement learning, for example, generalization is com-
monly achieved through function approximation: by learning

weightings from a set of features over the agent’s past and
present perceptions. Good generalization occurs when the
features are well chosen and are roughly independent of each
other. Predictive representations, too, are generally used in
conjunction with function approximation, where each predic-
tion is treated as a feature.

Yet function approximation is itself a complex issue, re-
quiring choices of method, optimization of learning param-
eters, etc., each of which can inﬂuence the results in its
own way. To avoid such confusion and test the predictive-
representations hypothesis most directly, we have controlled
for the possible confounding effects of function approxima-
tion by developing a strictly tabular form of predictive repre-
sentation. Furthermore, our focus is on how predictive repre-
sentations affect generalization, not on how these representa-
tions are acquired, and so we assume they have already been
acquired by some other process.

We illustrate the power of predictive representations in a
grid-world navigation task where the agent’s action and ob-
servation space is chosen to be particularly impoverished,
thus imposing a high degree of perceptual ambiguity. With
only a small subset of the predictions necessary for a full
representation of the environmental state, a reinforcement-
learning agent can learn quickly and still achieve nearly opti-
mal performance.

2 Predictive Representations
Several recently introduced methods for modeling dynamical
systems have been inspired by the predictive-representations
hypothesis. Among these are predictive state representa-
tions (PSRs) [Littman et al., 2002] and temporal-difference
networks (TD networks) [Sutton and Tanner, 2005]. Current
research on predictive representations has focused principally
on the acquisition of the representation (though their use for
control is also beginning to be explored [James et al., 2004;
Izadi and Precup, 2003]).

PSRs are based on the concept of a core test—a sequence
of actions followed by an observation—similar to the tests of
Rivest and Schapire [1994]. An agent records the outcome
of each core test as either a success or failure depending on
whether the predicted observation matches the actual obser-
vation. The agent maintains the probability of success for
each core test, and this value becomes a feature in the agent’s
representation of the world. Knowledge of the world can be

expressed as a function (generally a linear combination) of
these features. Core tests are continually acquired until the
features form a sufﬁcient statistic, which means that they cap-
ture all relevant, knowable information about the environment
and can maintain such information from one time step to the
next. (More will be said about sufﬁcient statistics below.)

TD networks consist of two conceptually separate net-
works: a question network and an answer network. The
question network poses questions about possible future ob-
servations. The answer network learns to predict answers to
those questions. The questions are therefore analogous to the
core tests of PSRs and the answer network is analogous to
the function that computes the probabilities of core-test suc-
cesses. However, the units of the question network can make
predictions about not just the agent’s observations, but also
about the values of other network units, and these predictions
of predictions allow the TD network to represent many possi-
ble core tests in a compact form. Furthermore, these predic-
tions are generally, but not necessarily, action conditional.

The two approaches just outlined share the common aspi-
ration of representing the world as a set of predictions about
future observations. While research into predictive represen-
tations is still in its nascent phases, we attempt to make a pre-
diction of our own about possible future observations. In this
spirit we pose the following question: if a method does indeed
prove successful at acquiring predictive representations, will
these representations be good at generalization?

3 Tabular Predictive Representations
In order to focus exclusively on prediction as a basis for gen-
eralization, unclouded by issues involving representation ac-
quisition, we assume the agent has already acquired the abil-
ity to make correct predictions. To eliminate issues involving
function approximation, we consider deterministic tasks with
a single, binary observation (though our method can also be
applied to multiple binary observations, and we describe an
extension in the future-work section for accommodating con-
tinuous observations or stochastic environments). To further
focus our tests, we choose to look only at predictions that are
contingent upon the agent’s actions, though this restriction is
not a requirement of predictive representations in general.

3.1 Identically Predictive Classes
We start with a set of binary tests resembling PSR core tests.
Each test is of length n, meaning that it is a sequence of n
actions followed by a single observation bit. We construct all
possible tests of length 1 through n and produce a panel of test
results in each state, one entry per test. Given N binary tests,
the panel may take on 2N distinct conﬁgurations of outcomes
(assuming the environment is deterministic). If the agent has
a actions available, then there are at most an distinct tests of
length n, and the number of tests of length n or less is:

nX

N =

ai

i=1

= an+1 − 1.

If two states cannot be distinguished by any of the N tests,
then the panel of results is identical in both states, and we

say these states are identically predictive for that value of n
and belong to the same identically predictive class (similar to
the states of Rivest and Schapire’s Simple-Assignment Au-
tomata [1994], but constrained by n).

If for a given n an environment has c identically predictive
classes, we arbitrarily number them 1 through c, and when
an agent visits an environmental state, it observes the number
for that state’s identically predictive class. This number is
therefore the agent’s predictive representation in tabular form.
As an example, suppose that a = 3, and n = 1, then N =
3, and the panel consists of three tests (one for each action).
Each test can result in an observation of 0 or 1, so there are
23 possible panel conﬁgurations and c = 8 (at most). Every
state is aggregated into one of these eight classes, labeled 1
through 8, and the agent observes a 1, 2, 3, 4, 5, 6, 7, or 8 in
each state that it visits.

In general, as n increases, both N and c increase, so there
are fewer states per class on average and the agent’s represen-
tation of its environment becomes more expressive.

However, not all 2N panel conﬁgurations are necessarily
represented in the environment: clearly, there can never be
more classes c then there are environmental states S, so if
N > S then some tests must be equivalent (meaning there
are no environmental states where the tests yield different
results). Furthermore, as n increases, N increases exponen-
tially, yet c tends to increase quite slowly in environments
with even a moderate amount of regularity.

Eventually, increasing n no longer increases c, and the
classes represent a sufﬁcient statistic. At that point c may still
be less than S if there are environmental states that cannot be
distinguished by any test of any length.

3.2 Sarsa(0) with Identically Predictive Classes

All agents in this paper are trained using the reinforcement-
learning algorithm known as episodic tabular Sarsa(0) [Sut-
ton and Barto, 1998].
In the traditional Markov case—
where the agent directly observes the environmental state—
an action-value function is learned over the space of environ-
mental states and actions.
In this algorithm, the estimated
value Q(s, a) of each experienced state–action pair s, a is
updated based on the immediate reward r and the estimated
value of the next state–action pair; i.e,

∆Q(s, a) = α[r + Q(s0, a0) − Q(s, a)],

where α is a learning-rate parameter.

Episodic tabular Sarsa(0) is implemented over the predic-
tive state space by mapping environmental states to their cor-
responding identically predictive classes, as described in the
previous section. The function C(·) provides this mapping,
and the resulting classes are then treated by the Sarsa agent
as though they were environmental states:

∆Q(C(s), a) = α[r + Q(C(s0), a0) − Q(C(s), a)]

(1)

Because no distinction is made between the states within a
class, the learning that occurs in one environmental state ap-
plies to all states mapped to the same class.

further tests ever reduce the number of classes or change the
way states have been assigned to classes.

But an agent whose predictive representation is a sufﬁcient
statistic may still not distinguish all environmental states.
This idea is illustrated in Figure 1 where there are 68 distinct
environmental states (four orientations in each grid cell).
Given an agent that has three actions (Rotate 90◦ Right,
Rotate 90◦ Left, Advance) and that observes only whether
or not there is a wall directly in front of it, each arm of the
cross will produce exactly the same sets of predictions. To the
predictive agent, the arms are identical because there is no test
that can distinguish a state in one arm from the corresponding
states in the other three arms, even with inﬁnite-length tests.
Therefore, for this environment there are a maximum of 17
identically predictive classes (four orientations in four arm
cells, plus only one in the center because all orientations in
the center are identically predictive).

This example also illustrates how an agent’s learning speed
can be improved through the use of predictive representa-
tions. If the task is to navigate to the cell marked X, an agent
observing identically predictive class numbers will learn an
optimal policy much faster than an agent observing environ-
mental state, because the predictive-class agent has only a
quarter of the situations to learn about and therefore requires
far less experience to learn about all possible situations.

4 Tabular History-based Representations
The obvious competitors to predictive representations are
history-based representations. Of these,
the ﬁxed-length
or Markov-k approaches [Ring, 1994; McCallum., 1996;
Mitchell, 2003] most clearly lend themselves to a tabular for-
mat. To promote a fair comparison between representations,
we deﬁne a k-length history to be an observation followed by
k action-observation pairs. In tabular format, each possible
history is uniquely labeled.

Predictive representations and ﬁxed-length history repre-
sentations offer fundamentally different kinds of generaliza-
tion. Speciﬁcally, most environmental states can be reached
via multiple different paths, each corresponding to a different
history; conversely, a single action sequence (history) may
reach multiple environmental states by starting from different
states. Therefore, the mapping between environmental states
and ﬁxed-length history sequences is many to many. But the
mapping between environmental states and identically pre-
dictive classes is many to one.

Conceptually, the reason for the difference is that an agent
can take many paths to arrive in a state, but once there, has
only one set of possible futures; and the set of possible futures
is absolutely ﬁxed for each environmental state. For example,
if k = 1 the agent has at least two ways of reaching every state
in Figure 1—rotating right or rotating left. If k = 2 there
are at least 4 different histories for each state. The number
of ﬁxed-length history representations that can lead to each
environmental state increases exponentially with k, so as k
increases, there are an exponential number of cases that the
agent must learn about. (This problem does not automatically
disappear by using function approximation methods.)

In contrast, the set of possible futures available from each

Figure 1: An agent in this world can have one of 4 orienta-
tions in each of the 17 grid squares. Its observation consists
of one “nose touch” bit that senses only whether there is a
wall directly in front of it. It chooses actions from {Rotate
90◦ Right, Rotate 90◦ Left, Advance}. There are therefore
68 distinct environmental states, but a predictive representa-
tion will identify (at most) 17 identically predictive classes.

3.3 Performance and Generalization
In episodic tasks the performance of a reinforcement-learning
agent can be quantiﬁed by the total reward it receives per
episode. Given inﬁnite time, agents that observe the (Markov)
state of the environment can achieve optimal performance.

In practice, however, learning an optimal policy may take
arbitrarily long for an arbitrarily large state space, and these
days one can rarely ﬁnd enough time, let alone an inﬁnite
or even an arbitrary amount of it. It is therefore desirable in
many cases to use methods that speed learning, even if the
ﬁnal solution is not absolutely optimal.

We anticipate a trade-off between the agent’s asymptotic
performance and its speed of learning. As c increases, the
classes more closely resemble the Markov state of the envi-
ronment and the agent’s asymptotic performance approaches
optimal. As c decreases, there are fewer distinct cases to
learn about and learning speed improves, but there is a risk
that the states comprising a class will disagree about the op-
timal action for that class. This disagreement can lead to
sub-optimal action selection, and in extreme cases the con-
ﬂict may be catastrophic, precluding the discovery of any
reasonable policy. Therefore, a good representation will ﬁnd
low values of c while minimizing the amount of disagree-
ment within classes. The predictive-representations hypothe-
sis speciﬁcally holds that the classes formed through predic-
tive representations will do just that.

3.4 Predictive Classes and Sufﬁcient Statistics
An agent’s representation of the world is a sufﬁcient statis-
tic if it cannot be improved through any further experience
with the world. In the case of predictive representations, a
sufﬁcient statistic implies that there are no additional predic-
tions that can add knowledge or improve performance. On
the other hand, adding more predictions to a sufﬁcient statis-
tic can never worsen asymptotic performance.

In the tabular case, if a sufﬁcient statistic consists of c
classes, no number of additional tests will increase c.
(If
by adding a test we were to end up with c0 classes, where
c0 > c, then our original representation could not have been
a sufﬁcient statistic, since it did not represent the knowledge
captured by the newest c0 − c class distinctions.) Nor will any

xAgent
Markov
Predictive

Fixed-History

Random

-
n
2
3
4
5
6
7
k
2
3
4
5
6

case

1
2
3

Unique

Observations

1696

% Environmental

States
100%

67
185
308
416
497
568

50
205
790
2,938
10,660

1,526
1,611
1,679

3.9%
10.7
17.8
24.1
28.8
32.9

2.9%
11.9
45.7
170.0
616.9

90%
95
99

Figure 3: The four representational schemas in their tested in-
stantiations and the degree of state aggregation in each case.
In the predictive case, the “unique observations” column rep-
resents the number of identically predictive classes, c. For
ﬁxed-length history, this column represents the number of
unique histories that occurred during training.

label. In the predictive case the agent observed a label cor-
responding to the identically predictive class (as described in
Section 3.2) for six different values of n. For each value of n,
states were assigned to classes as described in Section 3.1. In
the ﬁxed-history case, the agent observed a label correspond-
ing to the k-step history it had just experienced (Section 4)
for ﬁve different values of k. To see whether our results in
the predictive case were merely due to beneﬁcial properties
of state aggregation, we tried randomly assigning states to
classes and then training according to Equation 1.

Figure 3 shows the vital stats for each of the representa-
tional methods tested. The amount of state aggregation that
occurs in each method is shown in terms of the ratio of unique
observations to environmental states.

6 Results
Performance
representational
schemas given in Figure 3 are graphed in Figure 4, with the
exception of the random state aggregation method, which per-
formed too poorly to be graphed meaningfully.

the different

results

for

Each point in the graph represents the average number
of steps per episode over the previous 10 episodes. The
curves are averaged over 10,000 trials, each trial being 1,000
episodes. At the end of each trial, the agent’s action values
are reset, and learning begins from scratch. Over the course
of 1,000 episodes, the Markov case shows a smooth, steadily
improving curve, which by the 1, 000th episode is performing
very close to optimal.

Though ﬁxed-length history representations learn more
their learning speeds are

slowly than the other methods,

Figure 2: The “ofﬁce layout” grid-world used for the naviga-
tion task. The agent starts an episode in one of the six top
rooms, and ﬁnishes in the square marked G.

state is the same no matter how that state is reached, and as a
result, the number of predictive classes is never more than the
number of distinct environmental states. The nature of this
mapping makes generalization in the predictive case far more
intuitive, and to some extent the predictive-representations
hypothesis is based on this clearer intuition.

5 Experiment Design
We examined the predictive-representations hypothesis by
ﬁrst designing a grid-world with a large degree of regularity
(Figure 2). We then tested Sarsa(0) agents with four different
methods of representation on a task in this environment to see
how well each method was able to exploit the environment’s
regularities.

The agent’s task was to navigate to the goal cell (marked G)
from any randomly chosen square in one of the top six rooms.
(The environment was designed to resemble a typical ofﬁce
layout, and the task can be likened to ﬁnding the quickest
route to the staircase.) Representations that generalize well
should allow their respective agents to exploit the regularities
in the environment to improve their speed of learning.

As in Figure 1, the agent has a one-bit observation and
three possible actions. The rewards for the task are +1 for
reaching the goal state and −1 on all other timesteps. All
transitions in the environment are deterministic and the envi-
ronment has a total of 1696 states, 840 of which are possible
start states. On average, there are 42.2 steps along the op-
timal path from start to goal. The task is formulated to be
undiscounted and episodic; thus the agent is transported to a
randomly chosen starting position upon reaching the goal.

In every case actions were chosen according to an -greedy
policy;  was set to 0.1, and α was set to 0.25, which are
typical values for Sarsa agents in episodic tasks.
The four representational schemas tested were:
• Markov
• n-depth predictive classes
• k-step ﬁxed-length histories
• random state aggregation
In the Markov case the agent directly observed its current
environmental state, each state being represented by a unique

GFigure 4: A comparison of three representational methods on the task of Figure 2. The representational schemas shown are:
explicit environmental state, history-based representations for k = 2 through 6, and identically predictive classes for n = 3
through 6. The vertical axis shows average steps taken per episode over the past 100 episodes of training. Note that the axes
are logarithmic and the vertical axis intersects the horizontal axis at the optimum.

fastest for small values of k and their ﬁnal results are best for
large values of k, demonstrating the trade-off between repre-
sentational expressiveness and learning speed. The number of
histories increases (exponentially) with k, which negatively
impacts learning speed but positively impacts the ﬁnal results
of learning. Interestingly, on individual trials the curves for
the history representation resemble a step function. Appar-
ently, the -greedy action selection chooses poorly for many
episodes until the agent suddenly learns some key piece of
information that dramatically improves performance. As k
grows, the agent takes longer to discover this piece of infor-
mation but then converges to a better solution.

The results look promising for predictive representations.
They allow both speedy learning and convergence to a good
policy.
In general, the results for the identically predictive
representations are similar to those for the ﬁxed-history rep-
resentations in that convergence speed decreases and conver-
gence quality increases as n increases. However, in con-
trast to the ﬁxed-history representation, the number of identi-
cally predictive classes increases quite slowly with n and the
generalization beneﬁt of the predictive classes is clear. The
representation effectively aggregates similar states, allowing
the agent to converge to near-optimal solutions. This result
closely matches our intuition and expectations. Only in the
n = 3 case was there no improvement in learning speed, in-
dicating there may be a maximum degree of state aggregation
beyond which learning speed is not improved—also an ex-
pected and intuitive result from section 3.3. Results in the
n = 2 and n = 7 cases match these trends but are not shown
so as to keep the graph at least vaguely readable.

The case of random state aggregation is not shown in the

graphs because these agents performed so poorly. With only
10% state aggregation (90% of the states not aggregated), ran-
dom state aggregation brought about catastrophic failure in
over 80% of the agents tested, meaning that they show no
progress in their training. To avoid catastrophic failure in
90% of the agents, the amount of state aggregation had to be
1% or less, meaning that only one state in a hundred shared a
class with another state.

In contrast, there were no cases of catastrophic failure in

any of the agents using predictive representations.

It might be argued that the predictive representations bene-
ﬁt unfairly from the preprocessing done to create the classes,
which the history-based methods did not have, and which an
actual learning agent may not reasonably be expected to ac-
quire. Or it might be argued that history-based methods could
somehow be augmented to combine all ways of reaching a
state into equivalence classes as our tabular representations
do for predictions. But these objections miss two fundamen-
tal aspects of this research and of predictive representations
in general. First, it is not our intention here to prove the supe-
riority of one particular learning algorithm to any other, but
only to test out, in advance, whether the research direction
of predictive representations looks promising. (The evidence
collected clearly gives reason for optimism.) Second, acqui-
sition of a predictive representation is always directed toward
acquisition of a sufﬁcient statistic. Once an agent acquires
the predictions necessary for a sufﬁcient statistic, then it can
in principle create the class mappings used here.
(In fact,
those mappings would fall directly out of a TD network rep-
resentation, being merely a subset.) It is not clear what the
equivalent of a sufﬁcient statistic might be for history-based

k = 6k = 5k = 4k =3k = 2n = 4n = 3EnvironmentalStaten = 5n = 6EpisodesSteps42.2100100100010001000010000010methods, nor whether it could be reasonably acquired.

Another objection is that performance could be improved
even more by hand-coding a mapping of states to classes.
But, don’t be so sure! While it is obvious that an optimal
policy could be encoded by just three classes (one for each
action), ﬁnding a mapping that encourages learning is a com-
pletely different matter, and it is difﬁcult to design a map-
ping by hand that (a) aggregates the states into a small enough
number of classes to allow good generalization, (b) does not
risk catastrophic failure by mapping two states into a class
that makes learning impossible, and (c) is not essentially
based on our intuitive notions of prediction.

7 Conclusions and Future Work
This paper makes an initial attempt to answer the question: if
there were a learning system that could represent the world in
terms of predictions about possible future experience, would
that representation turn out to be useful for generalization in
reinforcement-learning tasks? While this question is itself
quite broad (fully answering it might require thorough test-
ing in the presence of many confounding factors), our results
are clear and lend weight to the possibility of a yes answer.

In comparison to other representational schemas, our tests
suggest that predictive representations may generalize a state
space very well, allowing faster learning without obvious risk
of catastrophic failure through poor state aggregation.

But there is another signiﬁcant conclusion to be reached. It
is fairly common in reinforcement-learning tasks to provide
agents with the environmental state in tabular form. Though it
was our intention only to examine predictive representational
methods, the particular tabular method we developed shows
signiﬁcant advantages over the Markov representation often
seen in the reinforcement-learning literature. We suggest it
may have unintended practical application to tasks with large
state spaces when the environmental state is readily avail-
able. Mapping states into identically predictive classes may
speed learning and still allow nearly optimal performance to
be achieved. This certainly deserves further investigation.

We would also like to test predictive representations in
non-deterministic tasks and in environments with continuous-
valued observations.
In the discrete, deterministic case we
can make binary predictions of observations, but in envi-
ronments with continuous-valued observations, the predic-
tions would also be continuous, and in the non-deterministic
tasks the predictions would be continuous-valued probabili-
ties. One solution is to use tile coding to discretize the con-
tinuous values, reducing them to binary values that can be
treated as described above. A second approach is to view the
panel of test results as vectors and measure their similarity
with a metric such as Euclidean distance. Vectors within a
certain distance would belong to the same identically predic-
tive class.

It should be noted that we have avoided the issue of poor
or malicious placement of rewards. What would happen, for
example, if the goal in Figure 1 were placed in the middle of
one of the arms? The agent, not able to distinguish between
one arm and the others, would actually be hindered by using
a predictive representation. The solution is to incorporate re-

ward(s) into the predictive model of the world. The ability
to predict rewards would increase the expressiveness of the
representation and could assist the integration of predictive
representations into reinforcement-learning algorithms.

Finally, there is an obvious enhancement to the classiﬁca-
tion method we employed in this paper. Since small values of
n speed learning the most but large values of n converge to
a better policy, it would make sense to gradually increase n,
splitting up the identically predictive classes into smaller and
smaller groups whenever learning begins to converge. Since
all the states in each newly derived class would come strictly
from the same parent class, the action values for the new
classes could be initialized to those of the parent class, and
all training done up to that point would be preserved.

Acknowledgments
We thank Anna Koop for her participation in our early dis-
cussions on this paper and for being a good sport. Thanks to
Mark Lee for help with the simulation code. This research
was supported in part by grants from iCORE, and NSERC,
and by DARPA grant HR0011-04-1-0050.

References
[Izadi and Precup, 2003] Masoumeh T. Izadi and Doina Pre-
cup. A planning algorithm for predictive state representa-
tions. In Georg Gottlob and Toby Walsh, editors, IJCAI,
pages 1520–1521. Morgan Kaufmann, 2003.

[James et al., 2004] Michael R. James, Satinder Singh, and
Michael L. Littman. Planning with predictive state repre-
sentations. In ICMLA-04, Proceedings of the International
Conference on Machine Learning and Applications, pages
304–311, 2004.

[Littman et al., 2002] Michael L. Littman, Richard S. Sut-
ton, and Satinder Singh. Predictive representations of
state. In Advances in Neural Information Processing Sys-
tems 14. MIT Press, 2002.

[McCallum., 1996] Andrew Kachites McCallum. Reinforce-
ment Learning with Selective Perception and Hidden State.
PhD thesis, Department of Computer Science, University
of Rochester, Rochester, New York, August 1996.

[Mitchell, 2003] Matthew W. Mitchell. Using markov-k
In Hamid R.
memory for problems with hidden-state.
Arabnia and Elena B. Kozerenko, editors, MLMTA, pages
242–248. CSREA Press, 2003.

[Ring, 1994] Mark B. Ring. Continual Learning in Rein-
forcement Environments. PhD thesis, University of Texas
at Austin, Austin, Texas 78712, August 1994.

[Rivest and Schapire, 1994] Ronald L. Rivest and Robert E.
Schapire. Diversity-based inference of ﬁnite automata. J.
ACM, 41(3):555–589, 1994.

[Sutton and Barto, 1998] Richard S. Sutton and Andrew G.
Barto. Reinforcement Learning: An Introduction. MIT
Press, Cambridge, MA, 1998.

[Sutton and Tanner, 2005] Richard S. Sutton and Brian Tan-
ner. Temporal-difference networks. In Advances in Neural
Information Processing Systems 17. MIT Press, 2005.


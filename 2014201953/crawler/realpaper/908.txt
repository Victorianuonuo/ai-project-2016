A Scalable Kernel-Based Algorithm for Semi-Supervised Metric Learning

∗

Dit-Yan Yeung, Hong Chang, Guang Dai

Department of Computer Science and Engineering
Hong Kong University of Science and Technology

Clear Water Bay, Kowloon, Hong Kong

{dyyeung,hongch,daiguang}@cse.ust.hk

Abstract

In recent years, metric learning in the semi-
supervised setting has aroused a lot of research in-
terests. One type of semi-supervised metric learn-
ing utilizes supervisory information in the form
of pairwise similarity or dissimilarity constraints.
However, most methods proposed so far are either
limited to linear metric learning or unable to scale
up well with the data set size.
In this paper, we
propose a nonlinear metric learning method based
on the kernel approach. By applying low-rank ap-
proximation to the kernel matrix, our method can
handle signiﬁcantly larger data sets. Moreover, our
low-rank approximation scheme can naturally lead
to out-of-sample generalization. Experiments per-
formed on both artiﬁcial and real-world data show
very promising results.

1 Introduction

1.1 Semi-Supervised Learning

In supervised learning, we are given a training sample in the
form of input-output pairs. The learning task is to ﬁnd a func-
tional relationship that maps any input to an output such that
disagreement with future input-output observations is mini-
mized. Classiﬁcation and regression problems are the most
common supervised learning problems for discrete-valued
and continuous-valued outputs, respectively. In unsupervised
learning, we are given a training sample of objects with no
output values, with the aim of extracting some structure from
them to obtain a concise representation or to gain some un-
derstanding of the process that generated the data. Clustering,
density estimation, and novelty detection problems are com-
mon unsupervised learning problems.

Over the past decade or so, there has been growing inter-
est in exploring new learning problems between the super-
vised and unsupervised learning extremes. These methods
are generally referred to as semi-supervised learning meth-
ods, although there exist large variations in both the problem

∗

This research has been supported by Competitive Earmarked
Research Grant 617404 from the Research Grants Council of the
Hong Kong Special Administrative Region, China.

formulation and the approach to solve the problem. The semi-
supervised learning literature is too large to do a comprehen-
sive review here. Interested readers are referred to some good
surveys, e.g., [Zhu, 2006].

One way to categorize many,

though not all, semi-
supervised learning methods is to consider the type of su-
pervisory information available for learning. Unlike unsu-
pervised learning tasks, supervisory information is available
in semi-supervised learning tasks. However, the information
is in a form that is weaker than that available in typical su-
pervised learning tasks. One type of (weak) supervisory in-
formation assumes that only part (usually a limited part) of
the training data are labeled. This scenario is commonly en-
countered in many real-world applications. One example is
the automatic classiﬁcation of web pages into semantic cate-
gories. Since labeling web pages is very labor-intensive and
hence costly, unlabeled web pages are far more plentiful on
the web. It would be desirable if a classiﬁcation algorithm
can take advantage of the unlabeled data in increasing the
classiﬁcation accuracy. Many semi-supervised classiﬁcation
methods [Zhu, 2006] belong to this category.

Another type of supervisory information is even weaker
in that it only assumes the existence of some pairwise con-
straints indicating similarity or dissimilarity relationships be-
tween training examples. In video indexing applications, for
example, temporal continuity in the data can be naturally used
to impose pairwise similarity constraints between successive
frames in video sequences. Another example is in proteomic
analysis, where protein-protein interactions can naturally be
represented as pairwise constraints for the study of proteins
encoded by the genes (e.g., Database of Interacting Pro-
teins (DIP), http://dip.doe-mbi.ucla.edu/). Yet
another example is the anti-spam problem. Recent studies
show that more than half of the e-mail in the Internet today
is spam, or unsolicited commercial e-mail. One recent ap-
proach to spam detection is based on the trustworthiness of
social networks [Boykin and Roychowdhury, 2005]. Such
social networks can naturally be represented as graphs with
edges between nodes representing pairwise relationships in
the social networks.

While supervisory information in the form of limited la-
beled data can be transformed into pairwise similarity and
dissimilarity constraints, inverse transformation is in general
not possible except for the special case of two-class problems.

IJCAI-07

1138

In this sense, the second type of supervisory information is of
a weaker form and hence the corresponding learning problem
is more difﬁcult to solve. The focus of our paper is on this
category of semi-supervised learning problems.

1.2 Semi-Supervised Metric Learning Based on

Pairwise Constraints

While many semi-supervised learning methods assume the
existence of limited labeled data [Zhu, 2006], there are much
fewer methods that can work with pairwise constraints only.
We survey the most representative methods in this subsection.

Very often, the pairwise constraints simply state whether
two examples belong to the same class or different classes.
[Wagstaff and Cardie, 2000] ﬁrst used such pairwise infor-
mation for semi-supervised clustering tasks by modifying the
standard k-means clustering algorithm to take into account
the pairwise similarity and dissimilarity constraints. Exten-
sions have also been made to model-based clustering based
on the expectation-maximization (EM) algorithm for Gaus-
sian mixture models [Shental et al., 2004; Lu and Leen,
2005]. However, these methods do not explicitly learn a dis-
tance function but seek to satisfy the constraints, typically for
clustering tasks. Hence, they are sometimes referred to as
constraint-based clustering methods.

As a different category, some methods have been proposed
to learn a Mahalanobis metric or some other distance function
based on pairwise constraints. [Xing et al., 2003] formulated
a convex optimization problem with inequality constraints to
learn a Mahalanobis metric and demonstrated performance
improvement in a subsequent clustering task. Solving a simi-
lar problem to learn a Mahalanobis metric, the relevant com-
ponent analysis (RCA) algorithm [Bar-Hillel et al., 2003;
2005] was proposed as a simpler and more efﬁcient algorithm
than that of [Xing et al., 2003]. However, RCA can make use
of similarity constraints only. [Hertz et al., 2004] proposed
a distance function learning method called DistBoost. How-
ever, there is no guarantee that the distance function learned
is a metric. [Bilenko et al., 2004] explored the possibility of
integrating constraint-based clustering and semi-supervised
clustering based on distance function learning. The distance
function learning methods reviewed above either learn a Ma-
halanobis metric that corresponds to linear transformation
only, or learn a distance function that is not a metric. In our
previous work [Chang and Yeung, 2004], we proposed a met-
ric learning method that corresponds to nonlinear transfor-
mation. While this method is more powerful than the linear
methods, the optimization problem is non-convex and hence
is more complicated to solve.

In this paper, we focus on the distance function learn-
ing approach because distance functions are central to many
learning models and algorithms. This also makes it easier to
achieve out-of-sample generalization. Moreover, we focus on
learning metrics because this allows us to formulate the met-
ric learning problem based on the kernel approach [Sch¨olkopf
and Smola, 2002], which provides a disciplined, computa-
tionally appealing approach to nonlinear metric learning.

1.3 Organization of Paper

In Section 2, we propose a simple and efﬁcient kernel-based
metric learning method based on pairwise similarity con-
straints. Like many kernel methods, a limitation of this
method is that it does not scale up well with the sample size.
In Section 3, we address the scalability issue by applying
low-rank approximation to the kernel matrix. This extended
method can also naturally give rise to out-of-sample gener-
alization, which is addressed in Section 4. We present some
experimental results in Section 5 to demonstrate the effec-
tiveness of our metric learning algorithm. Finally, Section 6
concludes the paper.

2 Kernel-Based Metric Learning

2.1 Problem Setup

Let {xi}n
i=1 be a set of n data points in some input space X .
Suppose we have a Mercer kernel ˆk which induces a nonlin-
ear feature map ˆφ from X to some reproducing kernel Hilbert
space H [Sch¨olkopf and Smola, 2002]. The corresponding set
of feature vectors in H is { ˆφ(xi)}n
ˆK = [ˆk(xi, xj)]n×n = [(cid:3) ˆφ(xi), ˆφ(xj )(cid:4)]n×n. Choices for ˆk

i=1 and the kernel matrix is

include the Gaussian RBF kernel and the polynomial kernel.
We apply a centering transform such that the feature vectors

in H have zero mean. The resulting kernel matrix K can be
computed as K = [k(xi, xj )]n×n = [(cid:3)φ(xi), φ(xj )(cid:4)]n×n =
H ˆKH, where the centering matrix H = I − (1/n)11T
I being the n× n identity matrix and 1 being the n× 1 vector

with

of ones.

We consider a type of semi-supervised metric learning in
which the supervisory information is given in the form of
pairwise similarity constraints.1 Speciﬁcally, we are given
a set of point pairs, which is a subset of X × X , as S =
{(xi, xj) | xi and xj belong to the same class}. Our goal is to
make use of S to learn a better metric through modifying the

kernel so that the performance of some subsequent task (e.g.,
clustering, classiﬁcation) based on the metric is improved af-
ter kernel learning.

2.2 Kernel Learning
(cid:2)p
Since the kernel matrix K is symmetric and positive semi-
deﬁnite, we can express it as K =
r =
r=1 λrKr, where λ1 ≥ ··· ≥ λp > 0 are the p ≤ n
positive eigenvalues of K, v1, . . . , vp are the corresponding
normalized eigenvectors, and Kr = vrvT
r .

r=1 λrvrvT

(cid:2)p

We consider a restricted form of kernel matrix learning
by modifying K through changing the λr’s while keep-
ing all Kr’s ﬁxed. To ensure that the eigenvalues are
nonnegative, we rewrite K as Kβ = [kβ(xi, xj)]n×n =
[(cid:3)φβ(xi), φβ(xj)(cid:4)]n×n =
r Kr, which represents
a family of kernel matrices parameterized by β =
(β1, . . . , βp)T

(cid:2)p

r=1 β2

.

1As we will see later in the formulation of the optimization prob-
lem for kernel learning, these constraints are “soft” constraints rather
than “hard” constraints in that they are only preferred, not enforced.
This makes it easy to handle noisy constraints, i.e., erroneous super-
visory information, if we so wish.

IJCAI-07

1139

We perform kernel learning such that the mean squared Eu-

clidean distance induced by Kβ between feature vectors in H
corresponding to point pairs in S is reduced. Thus the crite-

rion function for optimization is

(cid:3)
⎡
⎣ 1|S|

(xi,xj)∈S

β2
r

JS(β)

=

=

1|S|
p(cid:3)

r=1

[(Kβ)ii + (Kβ)jj − 2(Kβ)ij ]
(cid:3)

⎤
⎦
T Kr(bi − bj)

(bi − bj)

(xi,xj )∈S

= βT DS β,

(1)

where bi is the ith column of the p × p identity matrix2 and
DS is a p × p diagonal matrix with diagonal entries

(DS )rr =

=

1|S|

1|S|

(cid:3)
(cid:3)

(xi,xj )∈S

(xi,xj )∈S

(bi − bj)

T Kr(bi − bj)

[(bi − bj)

T vr]

2 ≥ 0.

(2)

To prevent β from degenerating to the zero vector 0 and
to eliminate the scaling factor, we minimize the convex func-
tion JS(β) subject to the linear constraint 1T β = c for some
constant c > 0. This is a simple convex optimization prob-
lem with a quadratic objective function and a linear equality
constraint. We introduce a Lagrange multiplier ρ to minimize
the following Lagrangian:

JS (β, ρ) = JS(β) + ρ(c − 1T β).

(3)

The optimization problem can be solved easily to give the

optimal value of β as the following closed-form solution:

β =

cD−1S 1
1T D−1S 1

.

(4)

(cid:2)p

Note that D−1S exists as long as all the diagonal entries of
DS are positive, which is usually true.3 We set the constant
c =

√
λr.

r=1

3 Scalable Kernel Learning

The kernel learning method described above requires per-
forming eigendecomposition on K. In case n is very large
and hence K is a large matrix, operations such as eigende-
composition on K are computationally demanding. In this
section, we apply low-rank approximation to extend the ker-
nel learning method so that it scales up well with n.

2This indicator variable will be “overloaded” later to refer to a
column of any identity matrix whose size is clear from the context.
3In case DS is really singular (though a rare case), a common
way to make it invertible is to add a term I to DS where  is a
small positive constant.

(5)

3.1 Low-Rank Approximation
We apply low-rank approximation to approximate K by an-
other n × n symmetric and positive semi-deﬁnite matrix

(cid:8)K:

K (cid:8) (cid:8)K = WLWT ,
is an n × m matrix and L ∈ R

n×m

where W ∈ R
an m × m symmetric and positive semi-deﬁnite matrix, with
m (cid:10) n.

m×m

is

There are different ways to construct L for low-rank ap-
proximation. We consider one way which constructs L us-
ing a subset of the n data points. We refer to these points as
landmarks [de Silva and Tenenbaum, 2003; Weinberger et al.,
2005; Silva et al., 2006]. Without loss of generality, we as-
sume that the n points are ordered in such a way that the ﬁrst
m points form the set of landmarks {xi}m
i=1. We should en-
sure that all points involved in S are chosen as landmarks.

Other landmarks are randomly sampled from all the data
points. Similar to K in the previous section, L is obtained
here by applying the centering transform to ˆL, as L = HˆLH,
where ˆL = [ˆk(xi, xj )]m×m = [(cid:3) ˆφ(xi), ˆφ(xj )(cid:4)]m×m is the
upper-left m × m submatrix of ˆK.

We apply eigendecomposition on L and express it as

q(cid:3)

L =

μrαrαT

r = VαDμVT
α,

(6)

r=1

where μ1 ≥ ··· ≥ μq > 0 are the q ≤ m positive eigenvalues
of L, α1, . . . , αq are the corresponding normalized eigenvec-
(cid:8)Kr,
tors, Dμ = diag(μ1, . . . , μq), and Vα = [α1, . . . , αq]. Sub-
stituting (6) into (5), we can rewrite

(cid:8)K as

(cid:8)K =

(cid:2)q

r=1 μr

(cid:8)Kr = (Wαr)(Wαr)T

where

.

3.2 Kernel Learning
We apply low-rank approximation to devise a scalable kernel
learning algorithm which can be seen as an extension of the
algorithm described in Section 2. We use
K and deﬁne the following parameterized family of kernel
matrices:
a q × 1 vector rather than a p × 1 vector.
that the constant c is set to
diagonal matrix with diagonal entries

(cid:8)K to approximate
(cid:8)Kr. Note, however, that β is now
(cid:2)q
√
The optimal value of β has the same form as (4), except
μr and DS is now a q × q

(cid:8)Kβ =

(cid:2)q

r=1 β2
r

r=1

(cid:3)

(DS )rr =

1|S|

(xi,xj)∈S

[(bi − bj)

T

(Wvr)]

2 ≥ 0.

(7)

3.3 Computing the Embedding Weights
A question that remains to be answered is how to obtain W
for low-rank approximation. We use a method that is similar
to locally linear embedding (LLE) [Roweis and Saul, 2000;
Saul and Roweis, 2003], with two differences. First, we only
use the ﬁrst part of LLE to obtain the weights for locally linear
ﬁtting. Second, we perform locally linear ﬁtting in the kernel-

induced feature space H rather than the input space X .
landmark, i.e., 1 ≤ i ≤ m, then

Let W = [wij ]n×m and wi = (wi1, . . . , wim)T

. If xi is a

(cid:9)

wij =

1 i = j
0 otherwise.

(8)

IJCAI-07

1140

If xi is not a landmark, then we minimize the following func-
tion to obtain wi:

E(wi) =

wij φ(xj )

,

(9)

(cid:10)(cid:10)(cid:10)2

(cid:3)

φ(xj)∈Ni

(cid:10)(cid:10)(cid:10)φ(xi) −
(cid:2)

φ(xj )∈Ni

subject to the constraints

where Ni is the set of K nearest landmarks of φ(xi) in H,
wij = 1T wi = 1 and
wij = 0 for all φ(xj ) (cid:11)∈ Ni. We can rewrite E(wi) as
wij wik(φ(xi) − φ(xj ))
(φ(xi) − φ(xk))

φ(xj ),φ(xk)∈Ni

E(wi) =

(cid:3)

T ·

= wT

i Giwi,

(10)

where

Gi = [k(xi, xi) + k(xj, xk) − k(xi, xj) − k(xi, xk)]K×K
is the local Gram matrix of xi in H.

To prevent wi from degenerating to 0, we minimize E(wi)
wij = 1T wi = 1 and
wij = 0 for all φ(xj ) (cid:11)∈ Ni. As above for the kernel learn-

subject to the constraints

φ(xj )∈Ni

(cid:2)

(11)

ing problem, we solve a convex optimization problem with a
quadratic objective function and a linear equality constraint.
The Lagrangian with a Lagrange multiplier α is as follows:

L(wi, α) = wT

i Giwi + α(1 − 1T wi).

The closed-form solution for this optimization problem is

given by wi = (G−1

i 1)/(1T G−1

i 1) if G−1

exists.4

i

Instead of performing matrix inversion, a more efﬁcient
way of ﬁnding the solution is to solve the linear system of
equations Gi ˆwi = 1 for ˆwi and then compute wi as wi =
ˆwi/(1T ˆwi) to ensure that the equality constraint 1T wi = 1
is satisﬁed.

We assume above that the neighborhood relationships be-

tween points in H and the local Gram matrices remain ﬁxed

during the kernel learning process. A simple extension of this
basic algorithm is to repeat the above procedure iteratively
using the learned kernel at each iteration. Thus the basic al-
gorithm is just a special case of this iterative extension when
the number of iterations is equal to one.

4 Out-of-Sample Generalization

The exact form of out-of-sample generalization depends on
the operation we want to perform. For example, the n given
points are ﬁrst clustered into C ≥ 2 classes after kernel learn-
ing, and then a new data point x is classiﬁed into one of the
C classes. We are interested in the case where both the clus-
tering of the n points and the classiﬁcation of new points are
based on the same Euclidean metric in H.

The key idea of our out-of-sample generalization scheme
rests on the observation that kernel principal component anal-
ysis (KPCA) [Sch¨olkopf et al., 1998] can be performed on
{xi}n
i=1 to obtain an embedding in a q-dimensional subspace

4Similar to DS above, we may add I to make sure that Gi is

invertible.

way.

μ VT

(cid:2)

φ(xj)∈Ni

i=m+1 and any

(cid:2)m

i=m+1, from (9), we use

to denote the embedding of

Y of H, so that the non-landmark points {xi}n
out-of-sample point x can be embedded into Y in the same
Let {u1, . . . , uq} be an orthonormal basis with each ur ∈
H being a unit vector along the direction of the rth prin-
cipal component. We deﬁne U = [u1, . . . , uq]. Then
each landmark xi (i = 1, . . . , m) can be embedded into
Y to give a q-dimensional vector yi = UT φ(xi). Since
ur = 1√
j=1 αjrφ(xj ), we can express yi as yi =
μr
−1/2
αLbi = D1/2
μ VT
αbi. Let Ym = [y1, . . . , ym]. So
D
Ym = D1/2
μ VT
α.
(cid:8)φ(xi) =
For the non-landmark points {xi}n
(cid:8)yi = UT(cid:8)φ(xi) = YmWT bi = D1/2
(cid:8)φ(x) =

wij φ(xj ) to approximate φ(xi) and (cid:8)yi

(13)
for any out-of-sample example x, we use

(cid:8)φ(xi) in Y. Thus we have

can be determined as in Section 3.3.
i=m+1, we can ob-

denote the embedding of
w = (w1, . . . , wm)T
Similar to the non-landmark points {xi}n
tween(cid:8)yi and(cid:8)y in Y before kernel learning can be expressed

wjφ(xj ) to approximate φ(x) and (cid:8)y to
(cid:8)φ(x) in Y. The embedding weights
(cid:8)y = D1/2
(xi, x) = (cid:12)(cid:8)yi −(cid:8)y(cid:12)2
i W − wT
i W − wT

(14)
Based on (13) and (14), the squared Euclidean distance be-

)VαDμVT
)L(WT bi − w).

α(WT bi − w)

= (bT
= (bT

αWT bi.

φ(xj)∈Ni

μ VT

Similarly,

μ VT

(cid:2)

αw.

(15)

tain

The squared Euclidean distance after kernel learning is
β(xi, x) = (bT
d2
where Dβ = diag(β2

i W − wT
1 , . . . , β2

)VαDβVT
q ).

α(WT bi − w), (16)

5 Experimental Results
In this section, we present some experiments we have per-
formed based on both artiﬁcial and real-world data.

5.1 Experimental Setup
We compare two versions of our kernel-based metric learning
method described in Sections 2 and 3 with RCA [Bar-Hillel et
al., 2003; 2005], which is a promising linear metric learning
method that usually performs equally well as other computa-
tionally more demanding methods. For baseline comparison,
we also include two metrics without metric learning. They
are the Euclidean metric in the input space and the Euclidean
metric in the feature space induced by a Gaussian RBF ker-
nel.

For each data set, we randomly generate 10 differentS sets.
For small data sets, we can learn Kβ without low-rank ap-
(cid:8)Kβ. We use the iterative extension
proximation. For large data sets, in addition to the data points
involved in S, we also randomly select some other points as

landmarks for learning
of the scalable kernel learning algorithm with the number of
iterations equal to 3. We also measure the change in metric
learning performance as the number of landmarks increases.

(12)

as

d2

IJCAI-07

1141

2

1

0

−1

−2
−2

0.08

0.04

0

−0.04

−0.08

−0.08

−1

0

1

2

(a)

−0.04

0

0.04

0.08

2

1

0

−1

−2
−2

0.2

0.1

0

−0.1

−0.2

−0.08

2

1

0

−1

−2

−1

0

1

2

−2

−1

0

1

2

(b)

(c)

0.6

0.3

0

−0.3

−0.6

−0.04

0

0.04

0.08

−0.2

−0.1

0

0.1

0.2

Figure 1: XOR illustration.

(cid:8)Kβ (m = 40).

(f)

(d)

(e)

(f)

(a) input data and similarity constraints; (b) new data points; (c) RCA; (d) RBF; (e) Kβ;

5.2 An Illustrative Example

Figure 1 uses the XOR data set to compare the performance
of different metrics, some with metric learning. Figure 1(a)
shows 360 data points in the input space. The points with
the same color and mark belong to the same class. The ran-
domly generated point pairs corresponding to the similarity

constraints in S are shown in solid lines. Figure 1(b) shows

40 new data points in the input space. The results obtained by
RCA, RBF kernel and two versions of our method are shown
in Figure 1(c)–(f). RCA performs metric learning directly in
the input space, which can be generalized to new data using
the learned linear transformation. For the kernel methods,
we apply KPCA using the (learned) kernel matrix to embed
the data points to a 2-dimensional space, as shown in Fig-
ure 1(d)–(f). New data points can be embedded using the
generalization method described in Section 4. As expected,
RCA does not perform satisfactorily for the XOR data set
since it can only perform linear transformation. On the other
hand, our kernel-based metric learning method can group the
points according to their class membership. The result of the
scalable kernel learning method with low-rank approximation
based on 40 landmarks is almost as good as that of the basic
algorithm based on all 400 data points. Moreover, the result
on embedding of new data points veriﬁes the effectiveness of
our out-of-sample generalization method.

5.3 Quantitative Performance Comparison

Let {yi}n

i=1 be the set of true class labels of the n data points.

We deﬁne the following performance measure:

J =

db
dw

,

(cid:2)

yi=yj

yi(cid:4)=yj

(cid:2)

(cid:12)xi − xj(cid:12) is the mean between-
where db = (1/nb)
class distance with nb being the number of point pairs with
(cid:12)xi − xj(cid:12) is
different class labels, and dw = (1/nw)
the mean within-class distance with nw being the number of
point pairs with the same class label. Note that J is closely re-
lated to the optimization criterion JS in (1), except that JS is
deﬁned for the labeled data (i.e., data involved in the pairwise
constraints) only while J is for all data assuming the exis-
tence of true class labels. For kernel methods, we use φ(xi)
or
the mean distances and hence J . A larger value of J corre-
sponds to a better metric due to its higher class separability.

(cid:8)φ(xi) in place of xi and apply the kernel trick to compute

We ﬁrst perform some experiments on a much larger XOR
data set with 8,000 data points. We randomly select 50 sim-

ilarity constraints to form S and measure the metric learning
performance in terms of the J value for an increasing number
of landmarks. Table 1 shows the results for different metrics.
For the metric learning methods (i.e., RCA and our method),
we show for each trial the mean (upper) and standard devia-
tion (lower) over 10 random runs corresponding to different

S sets. From the results, we can see that our method outper-

forms the other methods signiﬁcantly. Moreover, using more
landmarks generally gives better results.

We further perform some experiments on real-world data
sets. One of them is the Isolet data set from the UCI Machine
Learning Repository which contains 7,797 isolated spoken
English letters belonging to 26 classes, with each letter repre-
sented as a 617-dimensional vector. Other data sets are hand-
written digits from the MNIST database.5 The digits in the
database have been size-normalized and centered to 28×28

(17)

5http://yann.lecun.com/exdb/mnist/

IJCAI-07

1142

Table 1: Performance comparison in terms of J value for XOR data set (|S| = 50, m = 100:100:800).

INPUT DATA

RBF

RCA

m = 100 m = 200 m = 300

OUR METHOD

m = 400

m = 500 m = 600 m = 700 m = 800

1.2460

1.4253

1.2552
±0.19

2.8657
±1.28

3.0886
±0.42

3.5640
±0.78

3.8422
±1.25

4.2378
±0.99

4.6395
±0.68

4.8334
±0.90

4.7463
±0.65

Table 2: Performance comparison in terms of J value for Isolet and MNIST data sets (|S| = 50, m = 100).

INPUT DATA
RBF
RCA

ISOLET

1.3888
1.2477
1.3049

±0.0030

{0, 1}

{1, 3}

{1, 5}

{1, 7}

1.3914
1.2460
1.2970

±0.0324

1.2124
1.1447
1.1779

±0.0270

1.1920
1.1357
1.1705

±0.0283

1.2458
1.1570
1.1820

±0.0399

MNIST
{1, 9}

1.2379
1.1548
1.2086

±0.0391

OUR METHOD

2.7938

±0.0430

2.9078

±0.4614

1.7070

±0.2252

1.4015

±0.1400

1.5463

±0.2055

1.7023

±0.3567

1.8620

±0.3160

1.6233

±0.1996

1.9608

±0.1884

{0, 1, 2}

{6, 7, 8}

{0, 1, 9}

{3, 4, 5, 6}

1.2408
1.1598
1.1844

±0.0311

1.1534
1.0963
1.1207

±0.0160

1.2779
1.1796
1.2087

±0.0200

1.1162
1.0729
1.0793

±0.0110

1.2945

±0.0667

gray-level images. Hence the dimensionality of the input
space is 784. In our experiments, we randomly choose 2,000
images for each digit from a total of 60,000 digit images in
the MNIST training set. We use 50 similarity constraints and
100 landmarks in the experiments. The results for Isolet and
different digit data sets are shown in Table 2. From the re-
sults, we can again see that the metric learned by our method
is the best in terms of the J measure.

6 Concluding Remarks

We have presented a simple and efﬁcient kernel-based semi-
supervised metric learning method based on supervisory in-
formation in the form of pairwise similarity constraints. Not
only does it scale up well with the data set size, it can also
naturally lead to out-of-sample generalization. Although pre-
vious studies by other researchers showed that pairwise dis-
similarity constraints usually cannot help much in many real-
world applications, there are situations when incorporating
them may still be helpful and hence we plan to extend our
method to incorporate dissimilarity constraints as well.
In
our low-rank approximation scheme, besides including those

points involved in S, we also randomly sample some other
points as landmarks. A recent study [Silva et al., 2006] shows
that non-uniform sampling of landmarks for manifold learn-
ing can give parsimonious approximations using only very
few landmarks. We will pursue research along this direction
in our future work.

References

[Bar-Hillel et al., 2003] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. Learn-
ing distance functions using equivalence relations.
In Proceedings of the Twenti-
eth International Conference on Machine Learning, pages 11–18, Washington, DC,
USA, 21–24 August 2003.

[Bar-Hillel et al., 2005] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. Learn-
ing a Mahalanobis metric from equivalence constraints. Journal of Machine Learn-
ing Research, 6:937–965, 2005.

[Bilenko et al., 2004] M. Bilenko, S. Basu, and R.J. Mooney. Integrating constraints
and metric learning in semi-supervised clustering. In Proceedings of the Twenty-
First International Conference on Machine Learning, pages 81–88, Banff, Alberta,
Canada, 4–8 July 2004.

[Boykin and Roychowdhury, 2005] P.O. Boykin and V.P. Roychowdhury. Leveraging

social networks to ﬁght spam. IEEE Computer, 38(4):61–68, 2005.

[Chang and Yeung, 2004] H. Chang and D.Y. Yeung. Locally linear metric adaptation
for semi-supervised clustering.
In Proceedings of the Twenty-First International
Conference on Machine Learning, pages 153–160, Banff, Alberta, Canada, 4–8 July
2004.

[de Silva and Tenenbaum, 2003] V. de Silva and J.B. Tenenbaum. Global versus local
methods in nonlinear dimensionality reduction. In S. Becker, S. Thrun, and K. Ober-
mayer, editors, Advances in Neural Information Processing Systems 15, pages 705–
712. MIT Press, Cambridge, MA, USA, 2003.

[Hertz et al., 2004] T. Hertz, A. Bar-Hillel, and D. Weinshall. Boosting margin based
distance functions for clustering. In Proceedings of the Twenty-First International
Conference on Machine Learning, pages 393–400, Banff, Alberta, Canada, 4–8 July
2004.

[Lu and Leen, 2005] Z. Lu and T.K. Leen. Semi-supervised learning with penalized
probabilistic clustering. In L.K. Saul, Y. Weiss, and L. Bottou, editors, Advances in
Neural Information Processing Systems 17, pages 849–856. MIT Press, Cambridge,
MA, USA, 2005.

[Roweis and Saul, 2000] S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduc-

tion by locally linear embedding. Science, 290(5500):2323–2326, 2000.

[Saul and Roweis, 2003] L.K. Saul and S.T. Roweis. Think globally, ﬁt locally: un-
supervised learning of low dimensional manifolds. Journal of Machine Learning
Research, 4:119–155, 2003.

[Sch¨olkopf and Smola, 2002] B. Sch¨olkopf and A.J. Smola. Learning with Kernels.

MIT Press, Cambridge, MA, USA, 2002.

[Sch¨olkopf et al., 1998] B. Sch¨olkopf, A. Smola, and K.R. M ¨uller. Nonlinear com-
ponent analysis as a kernel eigenvalue problem. Neural Computation, 10(5):1299–
1319, 1998.

[Shental et al., 2004] N. Shental, A. Bar-Hillel, T. Hertz, and D. Weinshall. Comput-
ing Gaussian mixture models with EM using equivalence constraints. In S. Thrun,
L. Saul, and B. Sch¨olkopf, editors, Advances in Neural Information Processing Sys-
tems 16. MIT Press, Cambridge, MA, USA, 2004.

[Silva et al., 2006] J.G. Silva, J.S. Marques, and J.M. Lemos. Selecting landmark
points for sparse manifold learning. In Advances in Neural Information Process-
ing Systems 18. MIT Press, Cambridge, MA, USA, 2006. To appear.

[Wagstaff and Cardie, 2000] K. Wagstaff and C. Cardie. Clustering with instance-level
constraints. In Proceedings of the Seventeenth International Conference on Machine
Learning, pages 1103–1110, Stanford, CA, USA, 29 June – 2 July 2000.

[Weinberger et al., 2005] K.Q. Weinberger, B.D. Packer, and L.K. Saul. Nonlinear
dimensionality reduction by semideﬁnite programming and kernel matrix factoriza-
tion. In Proceedings of the Tenth International Workshop on Artiﬁcial Intelligence
and Statistics, pages 381–388, Barbados, 6–8 January 2005.

[Xing et al., 2003] E.P. Xing, A.Y. Ng, M.I. Jordan, and S. Russell. Distance met-
ric learning, with application to clustering with side-information.
In S. Becker,
S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing
Systems 15, pages 505–512. MIT Press, Cambridge, MA, USA, 2003.

[Zhu, 2006] X. Zhu. Semi-supervised learning literature survey. Technical Report
1530, University of Wisconsin – Madison, Department of Computer Science, Madi-
son, Wisconsin, USA, September 7 (last modiﬁed) 2006.

IJCAI-07

1143


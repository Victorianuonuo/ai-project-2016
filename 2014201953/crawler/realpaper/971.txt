On Modeling Multiagent Task Scheduling

as a Distributed Constraint Optimization Problem

Evan A. Sultanik and Pragnesh Jay Modi and William C. Regli

{eas28,pmodi,wregli}@cs.drexel.edu

Department of Computer Science, Drexel University, Philadelphia, USA

Abstract

This paper investigates how to represent and solve
multiagent task scheduling as a Distributed Con-
straint Optimization Problem (DCOP). Recently
multiagent researchers have adopted the C TÆMS
language as a standard for multiagent task schedul-
ing. We contribute an automated mapping that
transforms C TÆMS into a DCOP. Further, we
propose a set of representational compromises for
C TÆMS that allow existing distributed algorithms
for DCOP to be immediately brought to bear on
C TÆMS problems. Next, we demonstrate a key
advantage of a constraint based representation is
the ability to leverage the representation to do efﬁ-
cient solving. We contribute a set of pre-processing
algorithms that leverage existing constraint propa-
gation techniques to do variable domain pruning on
the DCOP. We show that these algorithms can result
in 96% reduction in state space size for a given set
of C TÆMS problems. Finally, we demonstrate up
to a 60% increase in the ability to optimally solve
C TÆMS problems in a reasonable amount of time
and in a distributed manner as a result of applying
our mapping and domain pruning algorithms.

Introduction

1
The coordinated management of inter-dependent plans or
schedules belonging to different agents is an important, com-
plex, real-world problem. Diverse application domains such
as disaster rescue, small-team reconnaissance, security pa-
trolling and others require human teams to schedule their
tasks and activities in coordination with one another.
It is
envisioned that automated planning and scheduling processes
in the form of agents operating on portable computing hard-
ware can assist human teams in such domains. The prob-
lem is inherently a distributed one; an agent that manages the
schedule of a human user must effectively manage the inter-
dependencies between its schedule and the schedules of other
users by exchanging information and coordinating with other
agents. No single agent has a global view of the problem;
instead, each must make local planning and scheduling deci-
sions through collaboration with other agents to ensure a high
quality global schedule.

A key step towards addressing the distributed scheduling
problem is to devise appropriate representations. In this pa-
per, we have three goals in mind. We desire a representa-
tion that (a) captures the problem’s distributed multiagent as-
pects, (b) gives formal guarantees on solution quality, and (c)
is amenable to distributed solving by existing methods.

In regard to goal (a), the Coordinators Task Analysis En-
vironmental Modeling and Simulation (C TÆMS) represen-
tation [Boddy et al., 2006] is a general language (based on
the original TÆMS language [Decker, 1996]) that was jointly
designed by several multiagent systems researchers explic-
itly for multiagent task scheduling problems. C TÆMS is a
Hierarchical Task Network (HTN) style representation where
the task nodes in the network have probabilistic utility and
duration. C TÆMS is an extremely challenging class of
scheduling problem and has been adopted as a common chal-
lenge problem domain representation for distributed multia-
gent task scheduling research.

We propose a distributed constraint reasoning approach
for solving problems expressed in the C TÆMS language.
Our methodology is one of problem transformation in which
we create a mapping that converts a C TÆMS instance into
an instance of the Distributed Constraint Optimization Prob-
lem (DCOP) [Modi et al., 2005]. DCOP was devised to
model constraint reasoning problems where several agents
must communicate to ensure that solutions are globally op-
timal. Several distributed algorithms for DCOP currently ex-
ist in the literature including distributed dynamic program-
ming (DPOP [Petcu and Faltings, 2005]), cooperative media-
tion (OptAPO [Mailler and Lesser, 2004]) and asynchronous
backtracking (Adopt [Modi et al., 2005]).

The expressivity of C TÆMS comes at a cost; it is arguable
that the primary concern in the design of the C TÆMS lan-
guage was to comprehensively express the complexities of
the distributed scheduling problem, while less concern was
placed on goals (b) and (c) enumerated above. Indeed, current
distributed solution techniques cannot be applied straightfor-
wardly or are unable to provide solution quality guarantees.
Thus, to make progress on this difﬁcult problem, we con-
tribute a set of representational compromises in the form of
a subset of C TÆMS, called DSC TÆMS, that is soluble by
current DCOP algorithms.

Finally, we demonstrate three key advantages of our con-
straint based approach. First, we provide a mapping that au-

IJCAI-07

1531

tomatically transforms a C TÆMS problem into an equiva-
lent DCOP. That is, the optimal solution to a given C TÆMS
problem is also the optimal solution to the DCOP obtained
by applying our problem transformation mapping, i.e., our
mapping preserves the problem exactly. Second, arguably
one of the primary advantages of adopting a constraint based
approach is the ability to apply existing constraint propaga-
tion techniques from the constraint programming literature.
Thus, we leverage our problem transformation by applying
constraint propagation pre-processing techniques to perform
domain pruning to signiﬁcantly reduce the search space. We
demonstrate a 96% reduction in state space size for a set of
benchmark problems. Finally, another advantage of our ap-
proach is that by mapping the C TÆMS problem into a DCOP,
we are able to immediately apply existing solution techniques
for DCOP to the C TÆMS problem domain.

2 Related Work

Numerous techniques have been employed to address
the problem of multiagent coordination and scheduling.
Musliner, et al., map C TÆMS to a Markov decision pro-
cess which is then used to generate a policy dictating when
and how agents execute methods [Musliner et al., 2006].
Musliner, et al., also propose distributed constraint sat-
isfaction as a method for negotiating a feasible schedule
by maximizing previously-calculated local expected quali-
ties [Musliner et al., 2006]. Smith, et al., address the prob-
lem of dynamic schedule revision (due to changes in the
model) using Simple Temporal Networks [Smith et al., 2006].
Phelps and Rye address this same problem through a domain-
independent implementation of Generalized Partial Global
Planning, in which proxy methods allow for distributed ap-
proximation of the performance characteristics of potentially
complex tasks [Phelps and Rye, 2006].

Constraint propagation is a general family of techniques
that check a constraint based representation for consistency to
reduce problem search space, often in a pre-processing phase.
Bart´ak provides a survey of the general topic in [Bart´ak,
2001]. One of the primary advantages of adopting a con-
straint based approach is the ability to apply constraint prop-
agation techniques such as node, arc and path consistency. As
such, constraint propagation is well-studied with several algo-
rithms available that vary in complexity and pre-processing
resource requirements. The ﬁrst arc consistency algorithms
were formalized in [Mackworth, 1977] including the widely
used AC-3 algorithm. Bessi`ere, et al., improve on average-
case efﬁciency with the AC-6 and AC-7 algorithms [Bessi`ere
et al., 1999]. More recently, researchers have introduced dis-
tributed methods for arc consistency processing including the
DisAC-9 algorithm [Hamadi, 2002] and the DMAC proto-
col [Silaghi et al., 2001].

3 Formalization

This section formalizes the notion of a DCOP and speciﬁes
the subset of C TÆMS on which we focus.

3.1 Distributed Constraint Optimization
A DCOP is a subclass of distributed decision processes in
which a set of agents are responsible for assigning the values
of their respective variables subject to a set of constraints. A
DCOP can be deﬁned as a tuple (cid:2)A, V, D, F, α, σ(cid:3), where
A is a set of agents;
is a set of variables, {v1, v2, . . . , v|V |};
V
D is a set of domains, {D1, D2, . . . , D|V |}, where each
D ∈ D is a ﬁnite set containing the feasible values
for its associated variable;
is a set of |V |2 cost functions, one for each pair of
variables, such that fij : Di × Dj → N0 ∪ {∞}.
Each cost function maps every possible variable as-
signment to its associated cost, for all pairs of vari-
ables and for all possible assignments. These func-
tions can also be thought of as constraints;
is function α : V → A mapping variables to their
associated agent. α(vi) (cid:8)→ aj means that it is aj’s
responsibility to assign the value of vi; and
is a function σ : F → N0 that aggregates the indi-
vidual costs1.

F

α

σ

The objective of a DCOP is to have each agent assign val-
ues to its associated variables in order to minimize σ(F )
for a given assignment of the variables. This deﬁnition was
adapted from [Davin and Modi, 2005] and has been modiﬁed
for the sake of brevity, clarity, and applicability.
3.2 C TÆMS
C TÆMS is a derivative of
the TÆMS modeling lan-
guage [Decker, 1996] and can be used for representing in-
stances of task domains of the distributed multiagent schedul-
ing problem. Unlike other HTN representations, C TÆMS
emphasizes modeling of the interrelationships between tasks
more so than those between agents and their environ-
ment [Musliner et al., 2006]. For the sake of exposition and
formalism, we represent C TÆMS instances using set theory,
whereas the actual speciﬁcation is a grammar. We represent
a C TÆMS instance as a tuple (cid:2)N, M , T , G, μ, φ, ω(cid:3), where
N is a set of agents;
M is a set of methods;
T is a set of tasks;
E
G is a special task known as the “Task Group;”
μ
φ

is a set of “Non-Local Effects”;
is a function μ : M → A;
is a function φ : M ∪ T → T ∪ {∅} that maps
methods and tasks to their parent task; and
is a quality accumulation function ω : M ∪ T →
+∪{0} that returns the quality of a method or task.
For T ∈ T , ω(T ) is usually deﬁned as a function of
the associated qualities of all X ∈ φ−1(T ).

Each M ∈ M is itself a tuple, (cid:2)l, u, d(cid:3), where
l
is the earliest start time of the method;
u
is a deadline for the method; and
d
is the expected duration of the method.
Note that l ≤ u, but l + d might not necessarily be less than
or equal to u. Each T ∈ T is deﬁned as a similar tuple,
except tasks (as a type of virtual method aggregator) do not

ω

R

1We use the notation “N0” to denote the non-negative integers.

IJCAI-07

1532

have explicit durations. Also, these bounds on execution time
are inherited from parents. In other words, lφ(X) ≤ lX and
uφ(X) ≥ uX. It is assumed that φ creates parent-child rela-
tionships such that the resulting hierarchy is a tree rooted at
G. The range of φ ensures that all methods are leaves.

C TÆMS prescribes four primary types of quality accumu-
lation function (QAF): SUM, MIN, MAX, and SYNCSUM (the
qualities of the children are summed if and only if all of the
children are scheduled to start execution at the same time).

E is a tuple containing sets of functions, called “Non-Local
Effects” (NLEs) in C TÆMS, that deﬁne temporal precedence
constraints between methods and tasks: (cid:2)E, D, F, H(cid:3). Each
function in each of the sets maps pairs of methods and tasks
to a boolean; if the function maps to TRUE, then the associ-
ated NLE holds for that pair of methods/tasks. E contains a
set of hard “enables” NLEs (if X enables Y , then Y cannot
execute until X has accumulated positive quality). D is a set
of “disables” NLEs, which is the opposite of an enables NLE.
Finally, F and H are respectively “facilitates” and “hinders,”
both being soft NLEs that either increase or decrease the qual-
ity of targeted methods/tasks depending on the quality of the
source method/task.

A “schedule” is a grammar within the C TÆMS speciﬁca-
tion for deﬁning the chosen start times of methods; it can be
formalized as a function s : M → N0 ∪ {∅}. A start time
of ∅ means that the method will not be executed. A feasible
schedule obeys both mutex and precedence constraints (i.e. an
agent may not execute more than one method at a time and all
NLEs are obeyed). The objective is to create a feasible sched-
ule that maximizes ω(G).

4 Mapping C TÆMS to a DCOP
The basis of our approach is to map a given C TÆMS rep-
resentation of a distributed scheduling problem to an equiva-
lent DCOP whose solution leads to an optimal schedule. The
technical challenge lies in ensuring that the resulting DCOP’s
solution leads to an optimal schedule. The following sections
formalize the approach.
4.1 Challenges
DCOPs are a promising solution technique, but there exist
several challenges that must ﬁrst be addressed:
Representation. There are different semantics in which the
problem can be represented: one could have a variable
for each method in the C TÆMS instance that is assigned
the value of its start time, or one could have a variable for
each instance in time that will be assigned the method
that will be executed at that time.

Determinism. C TÆMS prescribes probability distributions
over certain parameters, such as method duration, which
is a problem since DCOPs are deterministic.

Aggregation Functions. DCOP solution algorithms do not
allow for arbitrary aggregation functions (σ). For exam-
ple, ADOPT requires associativity, commutativity, and
monotonicity of the aggregation function in order to en-
sure optimality [Modi et al., 2005].

n-ary Constraints. Although DCOPs allow for constraints
of arbitrary arity, discovery of efﬁcient algorithms for

solving problems containing greater-than-binary con-
straints is still an open problem [Modi et al., 2005].

Finite Domains. Regardless of how the problem is repre-
sented, time must be discretized since both the number
of variables and the cardinality of variables’ domains
must be ﬁnite.

4.2 Method
The representational challenge is met by creating one DCOP
variable for each method; the variables will be assigned the
start times of their associated methods. The variables’ do-
mains will therefore contain the feasible start times of the
associated method. Time is discretized into quanta for the
domains. Likewise, the probability distributions in C TÆMS
are discretized using expected values.
For each agent in the C TÆMS instance, n ∈ N, create
an associated DCOP agent, a ∈ A. For each method, M ∈
M , create an associated variable vM ∈ V . Therefore, the
α function of the DCOP can be deﬁned as an analog of the
μ function of C TÆMS. The domains of the variables will
contain all possible start times of the method (including an
option for the method to forgo execution). In order to encode
the mutex constraints,

(∀n ∈ N : (∀(Mi, Mj) ∈ μ−1(n) × μ−1(n) :

fij(x ∈ Di, y ∈ Dj) (cid:8)→ ∞

when (x < y < x + dMi) ∨ (cid:2)

y < x < y + dMj

)).

(cid:3)

In other words, for all agents n ∈ N ﬁnd all pairs of methods
Mi and Mj that share agent n and create a hard DCOP con-
straint (i.e. of inﬁnite cost) for all pairs of equal domain val-
ues for the associated variables. This will ensure that an agent
may not execute multiple methods at once. NLEs (i.e. prece-
dence constraints) are encoded similarly. For example, the
enables NLEs are encoded as follows. Each e ∈ E is a func-
tion e : (M ∪T )×(M ∪T ) → B mapping pairs of methods
and tasks to a boolean. Let ϕ : T × (M ∪ T ) → B be a
function such that ϕ(X, Y ) (cid:8)→ TRUE ⇐⇒ φ(X) (cid:8)→ Y , and
let ϕ+ be the transitive closure of ϕ. ϕ+(X, Y ) implies that
X is in the subtree rooted at Y . Therefore,

e(X, Y ) =⇒ (∀X(cid:2), Y (cid:2) | ϕ+(X(cid:2), X) ∧ X(cid:2)
is a method :
must have ﬁnished executing before Y (cid:2)

∧ ϕ+(Y (cid:2), Y ) ∧ Y (cid:2)

X(cid:2)

is a method

can start).

In other words, e(X, Y ) means that all methods in the subtree
rooted at X must have ﬁnished executing before any of the
methods in the subtree rooted at Y may be executed2. For
each e ∈ E, if the transitive closure e+(X, Y ) maps to TRUE,
X is said to precede Y in an “NLE chain.” Then,

(∀e ∈ E : (∀X, Y
(∀Mi, Mj

e(X, Y ) :

|
| ϕ+(Mi, X) ∧ Mi ∈ M ∧

ϕ+(Mj, Y ) ∧ Mj ∈ M
fij(x ∈ Di, y ∈ Dj) (cid:8)→ ∞
when y < x + dMi

))).

:

2Note that this deﬁnition is slightly more restrictive than that
of [Boddy et al., 2006], in which Y cannot execute until X has
accumulated positive quality: ω(X) > 0.

IJCAI-07

1533

Finally, create one soft |M|-ary constraint between all of

the variables that aggregates the QAFs in the HTN.
4.3 DCOP-Solvable C TÆMS
The fundamental shortcoming of the mapping proposed in the
previous section is the use of n-ary constraints, which are ex-
tremely inefﬁcient (and often not supported) by current solu-
tion techniques. DCOP-Solvable C TÆMS (DSC TÆMS) is
our proposed a subset of C TÆMS that allows for immediate
application of current DCOP algorithms.

DSC TÆMS consists of only enables and disables NLEs,
and may only contain either: (1) all SUM and SYNCSUM
QAFs; (2) all MAX QAFs; or (3) all MIN QAFs. In each case
the mapping is exactly the same as that previously introduced,
however, the |M|-ary soft constraint is decomposed into |M|
unary constraints. Add one unary soft constraint for all meth-
ods’ variables as follows: (∀Mi ∈ M : fi(∅) (cid:8)→ ω (Mi)).

If a method is not scheduled to execute, its unary con-
straint will have a cost equal to the quality that the method
would have contributed to G had it been executed. This map-
ping will produce a DCOP with |M| variables and worst-case
O(|M|2) constraints.
In case 1, when all of the QAFs are
summation, use summation for DCOP aggregation function
σ. Likewise, in case 2, use maximization for σ. Finally, in
the case of minimization, one can create the dual of the mini-
mization problem and apply the MAX aggregation function.
It is theoretically possible to extend DSC TÆMS to allow
for cases in addition to the three listed above, assuming n-ary
constraints are feasible for small n. For example, one could
encode a DCOP containing all four types of QAF by adding
an n-ary soft constraint for each maximal subtree of the HTN
that is rooted by either a MAX or MIN QAF, adding a unary
soft constraint (as usual for DSC TÆMS) for each method not
a member of one such maximal subtree.
(∀Ti ∈ T | ω(Ti) is MAX or MIN ∧
(∀Tj ∈ ϕ+(Ti, Tj) : ω(Tj) is SUM or SYNCSUM) :
Create an n-ary soft constraint encoding the QAFs

in the subtree rooted at Ti).

A detailed analysis of the correctness and complexity of

our mapping is available in [Sultanik, 2006].

5 Efﬁciency Optimizations
Nothing thus far in the mapping precludes domains from be-
ing inﬁnite, which is one of the challenges (and limitations)
of using DCOP. From a practical standpoint, this is also a
problem because most DCOP solution techniques have expo-
nential computational complexity with respect to both the do-
main size and number of variables. Since the number of vari-
ables is generally inﬂexible, not only do we need to make the
domains ﬁnite but we ideally need to make them as small as
possible while ensuring that the solution space of the DCOP
still contains the optimal solution.
5.1 Na¨ıve Domain Bounding
It is possible to create a ﬁnite (although not necessarily tight)
upper bound on the start times of the methods. Let us consider

a C TÆMS instance in which all methods have an earliest start
time of zero. In this case, assuming all of the methods will
be executed, the optimal start time of a method cannot be
greater than the sum of the expected durations of all of the
other methods. In the general case of heterogeneous earliest
start times, we can deﬁne an upper bound on the start time of
a method M as the maximum ﬁnite earliest start time in M
plus the duration of all other methods.

5.2 Bound Propagation
Although the nature of the distributed scheduling problem im-
plies that a child’s bounds are inherited from (and therefore
cannot be looser than) its parent’s, C TÆMS neither requires
nor enforces this. Bounds can be propagated down the tree
from the root to improve upon the na¨ıve bounding. A dis-
tributed method for implementing this procedure (requiring
only local knowledge) is given in Algorithm 1. To calculate
the bounds for the methods of agent a, the algorithm would
be invoked as RECURSE-EXEC-BOUNDS(a, G, C, 0,∞).

Algorithm 1 RECURSE-EXEC-BOUNDS(a, T, C, (cid:7), υ)
Require: a is the agent from whose perspective we will create the bounds, T is the task
rooting the tree whose bounds we will create, C is a C TÆMS problem instance, (cid:3)
is a lower bound on the bounds of T , and υ is an upper bound on the bounds of T .
Ensure: βa : T → (N0 ∪ {∞}) × (N0 ∪ {∞}) is a function mapping all tasks in
the subtree rooted at T to lower and upper bounds on their start times. The subscript
a exists to emphasize the point that each agent has its own β function; the mapping
of each β function is contingent upon the extent of knowledge the agent has been
given within the problem instance.

e ←EARLIEST-START-TIME(C, T )
d ←DEADLINE(C, T )
if e (cid:7)= ∞ ∧ e > l then

end if

l ← e

u ← d

end if
if d (cid:7)= −∞ ∧ d < u then

1: l ← (cid:3)
2: u ← υ
3: if VISIBLE-TO?(C, T, a) then
4:
5:
6:
7:
8:
9:
10:
11:
12: end if
13: βa(T ) (cid:9)→ (l, u)
14: for all S ∈SUBTASKS(C, T ) do
15:
if IS-METHOD?(C, S) then
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
end if
35: end for

end if
βa(S) (cid:9)→ (lm, um)

um ← d

lm ← e

end if

else

This means S is a method (i.e. a special type of task)
if VISIBLE-TO?(C, S, a) then

lm ← l
um ← u
e ←EARLIEST-START-TIME(C, S)
d ←DEADLINE(C, S)
a ←EXPECTED-DURATION(C, S)
if e (cid:7)= ∞ ∧ e > lm then

end if
if d (cid:7)= −∞ ∧ d − a < um then

This means S is a regular task.
RECURSE-EXEC-BOUNDS(a, S, C, l, u)

5.3 Constraint Propagation
A binary
arc
(∀di ∈ Di : (∃dj ∈ Dj : fij (di, dj) (cid:18)= ∞)).

constraint,

fij,

is

consistent

if
A DCOP

IJCAI-07

1534

is said to be arc consistent if all f ∈ F are arc consis-
tent [Bart´ak, 2001]. We use forward constraint propagation
down the NLE chains to prune the domains, ensuring arc
consistency of the DCOP. A distributed method for constraint
propagation is given in Algorithm 2. Note that this algorithm
makes use of a function BROADCAST-BOUNDS(C, βa, a),
which has the following postcondition: agent a’s bounds will
be broadcast to all other agents that share an NLE with the
method/task associated with the given bound. Algorithm 2
works by having agents continually broadcast their current
start time bounds for their methods; if they receive a bound
that violates arc consistency, they increase the lower bound
on their method’s start
is arc
consistent and re-broadcast the new bounds. Since the lower
bounds monotonically increase and are bounded above, the
algorithm must terminate. An analysis of the messaging
overhead of this algorithm is presented in §6.
Algorithm 2 DIST-CONSTRAINT-PROP(C, βa, a, Q)
Require: C is a C TÆMS problem instance and βa is the associated bound function
for the agent, a, that is running this instance of the algorithm. Q is a queue that
is continuously updated with incoming broadcasts.

the constraint

time until

else

1: BROADCAST-BOUNDS(C, β, a)
2: while Q (cid:7)= ∅ do
(cid:12)f, r, s, t, (l, u)(cid:13) ←POP(Q)
3:
4:
if r =SOURCE then
βa(s) (cid:9)→ (l, u)
5:
ls ← l
6:
us ← u
7:
(lt, ut) ← βa(t)
8:
9:
βa(t) (cid:9)→ (l, u)
10:
lt ← l
11:
ut ← u
12:
(ls, us) ← βa(s)
13:
14:
15:
16:
17:
18:
19:
20:
end if
21: end while

βa(t) (cid:9)→ (lt + δ, ut)
βa(t) (cid:9)→ (lt + δ, ut)
BROADCAST-BOUNDS(C, βa, a)

end if
δ ← ls+EXPECTED-DURATION(C, s) − lt
if δ > 0 ∧ lt + δ ≤ ut then

6 Results
Using the C TÆMS scenario generator created for DARPA’s
COORDINATORs project, we randomly-generated a set of
100 C TÆMS instances, each with four agents, three-to-four
windows3, one-to-three agents per window, and one-to-three
NLE chains. The scenario generator does not ensure the ex-
istence of a feasible schedule for its resulting C TÆMS in-
stances. Even with the small simulation parameters and us-
ing all of our domain reduction techniques, the average state
space size of these problems was astronomical: on the order
of 1077. Therefore, some of the problems inevitably require
inordinate amounts of computation time. There seems to be
a phase transition in the problems, such that some are soluble
within the ﬁrst couple thousand cycles of the DCOP algo-
rithm, while the rest keep searching for an optimal solution
into the millions of cycles. In terms of computation time, this
equates to several orders of magnitude difference: seconds

3“Windows” are tasks whose parent is the task group (i.e., they

are tasks at the second layer from the root in the HTN).

versus days. This necessitated a threshold—that we’ve set
to 10,000 DCOP cycles—above which a C TÆMS instance
is simply declared “insoluble.” We have not found a single
C TÆMS instance that has been soluble in greater than 5000
cycles (given a reasonable amount of computation time).

We used the DCOP algorithm Adopt [Modi et al., 2005]
to solve the resulting DCOPs. Of the 100 random prob-
lems, none were soluble by the na¨ıve domain bounding ap-
proach. Applying bound propagation (Algorithm 1) to the
na¨ıve bounding resulted in 2% of the problems becoming sol-
uble. Applying all of our methods resulted in 26% solubil-
ity. Using an upper one-sided paired t-test, we can say with
95% certainty that Algorithm 2 made an average domain size
reduction of 7.62% over the domains produced from Algo-
rithm 1. If we look at this reduction in terms of state space
size, however, it becomes much more signiﬁcant: an average
94% decrease in state space size. Table 1 presents the state
space reduction efﬁciency of our constraint propagation tech-
nique in terms of problem solubility. Since constraint prop-
agation was fairly consistent in the percentage of state space
reduced between those problems that were soluble and those
that were insoluble, this suggests that the 74% of the prob-
lems that remained insoluble were due to the large state space
size inherent in their structure. For example, the insoluble
problems’ state spaces were, on average, one million times as
large as those that were soluble.

We also conducted tests over C TÆMS problems of differ-
ing complexity (by varying the number of windows and NLE
chains). The number of windows is correlated to the number
of variables in the resulting DCOP, while the number of NLEs
is correlated to the number of constraints. These data are pre-
sented in Table 2. Notice that problems bounded na¨ıvely were
never soluble. Over the most complex problems with 6 win-
dows and 3 NLEs chains, Algorithm 2 required an average of
144.94 messages (with a standard deviation of 16.13). This
was negligible in comparison to the number of messages re-
quired to arrive at an optimal solution.

7 Discussion

We have presented a mapping from the C TÆMS modeling
language to an equivalent DCOP. We have shown that the
resulting DCOP of a subset of this language, DSC TÆMS,
is soluble using existing techniques, and whose solution is
guaranteed to lead to an optimal schedule. We have empir-
ically validated our approach, using various existing tech-
niques from the constraint processing literature, indicating
that these problems are in fact soluble using our method.

We are optimistic in extending our mapping to sub-
sume a larger subset of C TÆMS. There are also various
heuristic techniques in the literature, such as variable order-
ing [Chechetka and Sycara, 2005], that can be applied to the
mapping while retaining formal guarantees on solution qual-
ity. If the resulting schedule need not be optimal (i.e. feasibil-
ity is sufﬁcient), approximation techniques for DCOPs exist.
With the groundwork laid in solving distributed multiagent
coordination problems with distributed constraint optimiza-
tion, we have many extensions in which to investigate.

IJCAI-07

1535

SOLUBILITY
Solved
Unsolved

AVG. DOMAIN

AVG. FINAL
SIZE REDUCTION DOMAIN SIZE

8.02%
7.61%

35.29
35.24

STATE SPACE
REDUCTION

97%
94%

FINAL STATE
SPACE SIZE
2.34 × 1071
1.47 × 1077

Table 1: Efﬁciency of Algorithm 2 at reducing average domain size and state space size, in terms of solubility.

# Windows

# NLE Chains Na¨ıve

CP Na¨ıve

% Soluble

3
3
3
4
4
4
5
5
5
6

*

0
2
4
0
2
4
0
2
4
3

CP Na¨ıve

Avg. # Cycles Avg. # Messages
CP
2569.25
3114.14
3854.2
1758.5
2783
3887
1364.5
2634
5748.67
4025.08

143.87
143.40
220.73
83.89
66.18
108.72
122.1
242.4
248.8
196.42

-
-
-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-
-
-

40.00
36.67
46.67
33.33
11.76
33.33
60.00
60.00
50.00
41.67

0
0
0
0
0
0
0
0
0
0

Table 2: Solubility statistics for different complexities of C TÆMS instances. All simulations were conducted with four agents.
None of the problems bounded using the na¨ıve method were soluble. * This is the default conﬁguration for the C TÆMS
scenario generator.

References
[Bart´ak, 2001] Roman Bart´ak. Theory and practice of constraint
propagation. In J. Figwer, editor, Proceedings of the 3rd Work-
shop on Constraint Programming in Decision Control, Poland,
June 2001.

[Bessi`ere et al., 1999] Christian Bessi`ere, Eugene C. Freuder, and
Jean-Charles R´egin. Using constraint metaknowledge to reduce
arc consistency computation. Artiﬁcial Intelligence, 107:125–
148, 1999.

[Boddy et al., 2006] Mark S. Boddy, Bryan C. Horling, John
Phelps, Robert P. Goldman, Regis Vincent, A. Chris Long,
Robert C. Kohout, and Rajiv T. Maheswaran. C-TAEMS lan-
guage speciﬁcation v. 2.02, 2006.

[Chechetka and Sycara, 2005] Anton Chechetka and Katia Sycara.
A decentralized variable ordering method for distributed con-
straint optimization. In AAMAS ’05: Proceedings of the fourth
international joint conference on Autonomous agents and mul-
tiagent systems, pages 1307–1308, New York, NY, USA, 2005.
ACM Press.

[Davin and Modi, 2005] John Davin and Pragnesh Jay Modi. Im-
pact of problem centralization in distributed constraint optimiza-
tion algorithms. In Proceedings of Forth International Joint Con-
ference on Autonomous Agents and Multiagent Systems, pages
1057–1063, July 2005.

[Decker, 1996] Keith Decker. TAEMS: A Framework for Environ-
ment Centered Analysis & Design of Coordination Mechanisms.
In Foundations of Distributed Artiﬁcial Intelligence, Chapter 16,
pages 429–448. G. O’Hare and N. Jennings (eds.), Wiley Inter-
Science, January 1996.

[Hamadi, 2002] Youssef Hamadi.

Optimal distributed arc-

consistency. Constraints, 7:367–385, 2002.

ternational Joint Conference on Autonomous Agents and Mul-
tiagent Systems, pages 438–445, Washington, DC, USA, 2004.
IEEE Computer Society.

[Modi et al., 2005] Pragnesh Jay Modi, Wei-Min Shen, Milind
Tambe, and Makoto Yokoo. Adopt: Asynchronous distributed
constraint optimization with quality guarantees. Artiﬁcial Intelli-
gence Journal, 2005.

[Musliner et al., 2006] David J. Musliner, Edmund H. Durfee, Jian-
hui Wu, Dmitri A. Dolgov, Robert P. Goldman, and Mark S.
Boddy. Coordinated plan management using multiagent MDPs.
In Proceedings of the AAAI Spring Symposium on Distributed
Plan and Schedule Management. AAAI Press, March 2006.

[Petcu and Faltings, 2005] Adrian Petcu and Boi V. Faltings. A
scalable method for multiagent constraint optimization. In Proc
of International Joint Conference on Artiﬁcial Intelligence, 2005.
GPGP—a
domain-independent implementation. In Proceedings of the 2006
AAAI Spring Symposium on Distributed Plan and Schedule Man-
agement. AAAI Press, March 2006.

[Phelps and Rye, 2006] John Phelps and Jeff Rye.

[Silaghi et al., 2001] Marius-Calin Silaghi, Djamila Sam-Haroud,
and Boi V. Faltings. Asynchronous consistency maintenance. In
Intelligent Agent Technologies, 2001.

[Smith et al., 2006] Stephen Smith, Anthony T. Gallagher,
Terry Lyle Zimmerman, Laura Barbulescu, and Zack Rubinstein.
Multi-agent management of joint schedules.
In Proceedings
of the 2006 AAAI Spring Symposium on Distributed Plan and
Schedule Management. AAAI Press, March 2006.

[Sultanik, 2006] Evan A. Sultanik. Enabling multi-agent coordi-
nation in stochastic peer-to-peer environments. Master’s thesis,
Drexel University, April 2006.

[Mackworth, 1977] Alan K. Mackworth. Consistency in networks

of relations. Artiﬁcial Intelligence, 8(1):99–118, 1977.

[Mailler and Lesser, 2004] Roger Mailler and Victor Lesser. Solv-
ing distributed constraint optimization problems using coopera-
tive mediation.
In AAMAS ’04: Proceedings of the Third In-

IJCAI-07

1536


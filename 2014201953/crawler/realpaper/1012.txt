Ambiguous Part-of-Speech Tagging for Improving

Accuracy and Domain Portability of Syntactic Parsers

Kazuhiro Yoshida

∗

Yoshimasa Tsuruoka

Yusuke Miyao

Jun’ichi Tsujii

∗†‡

†

∗

∗Department of Computer Science, University of Tokyo

†School of Informatics, University of Manchester

‡National Center for Text Mining

{kyoshida,tsuruoka,yusuke,tsujii}@is.s.u-tokyo.ac.jp

Abstract

We aim to improve the performance of a syntac-
tic parser that uses a part-of-speech (POS) tagger
as a preprocessor. Pipelined parsers consisting of
POS taggers and syntactic parsers have several ad-
vantages, such as the capability of domain adapta-
tion. However the performance of such systems on
raw texts tends to be disappointing as they are af-
fected by the errors of automatic POS tagging. We
attempt to compensate for the decrease in accu-
racy caused by automatic taggers by allowing the
taggers to output multiple answers when the tags
cannot be determined reliably enough. We empir-
ically verify the effectiveness of the method us-
ing an HPSG parser trained on the Penn Treebank.
Our results show that ambiguous POS tagging im-
proves parsing if outputs of taggers are weighted
by probability values, and the results support pre-
vious studies with similar intentions. We also ex-
amine the effectiveness of our method for adapting
the parser to the GENIA corpus and show that the
use of ambiguous POS taggers can help develop-
ment of portable parsers while keeping accuracy
high.

1 Introduction
Some parsers use POS taggers as their preprocessors, and
some use integrated models that achieve tagging and parsing
simultaneously. Because the latter approach is more general,
it is successfully used by some of the state-of-the-art parsers,
such as Charniak’s [Charniak and Johnson, 2005], as a natural
consequence of the pursuit of accuracy. However, integrated
models of tagging and parsing tend to be complex and com-
putationally expensive, both in terms of training and run-time
costs.

On the other hand, pipelined systems of POS taggers and
parsers can be built with independently developed taggers and
parsers. In such models, we can easily make use of taggers
that use state-of-the-art sequence labeling techniques, most
of which are difﬁcult to be incorporated into syntactic disam-
biguation models. Advantages of pipelined parsers also in-

clude their ability to adapt to domains. POS taggers for a new
domain are much easier to develop than full parsers, because
training corpora for POS taggers are easier to construct com-
pared to those for full parsers, which require the annotation
of nested phrase structures.

However, independence assumption of taggers and parsers
may degrade the overall accuracy of the parsers. Wat-
son [2006] reported that using an automatic POS tagger
caused the F1 score of grammatical relations output by a
parser to drop by 2.02 points. She attempted to weaken the
independence assumption by letting the taggers output multi-
ple tags for each word, weighted by probability values. Her
approach improved the F1 score by 0.66 points.

In this paper, we verify Watson’s results on ambiguous
POS tagging using an HPSG [Pollard and Sag, 1994] parser
developed and trained on the Penn Treebank [Marcus et al.,
1994]. At the same time, we investigate the effectiveness of
ambiguous POS tagging for domain adaptation of parsers us-
ing the GENIA corpus [Ohta et al., 2002] as the test set. Ex-
perimental results show that the multiple output without prob-
ability values cannot improve the parser much and suggest
the importance of probability distribution of multiple tags ob-
tained by POS taggers. Additionally, we show that the pos-
itive effect of ambiguous POS tagging is maintained for do-
mains unfamiliar to the parser.

2 Background

In this section, we describe the POS tagger and syntactic
parser used in our experiments. These taggers and parsers
were combined to make a pipelined syntactic parser for raw
texts using the strategy described in the next section.

As both of our tagging and parsing models are based
on log-linear classiﬁers, we ﬁrst brieﬂy introduce log-linear
models and then describe our tagger and parser.

2.1 Log-linear models

Log-linear models are among the most widely used machine
learning techniques in NLP literature, and we use the models
both for POS taggers and syntactic parsers. A conditional log-
linear model calculates the probability of an event E given the

IJCAI-07

1783

Input: Sentence s
Output: Tag sequence t1, . . . tn
Algorithm:

1.
2.

foreach ti do ti := NULL
foreach ti = NULL

Compute probability distribution
P (ti|ti−2, ti−1, ti+1, ti+2, s) (1)
by log-linear models

3. Let i be easiest place to tag in
ti := (Most probable tag for ti)
repeat 2 and 3 until
ti (cid:2)= NULL for each i

4.

Figure 1: Algorithm for POS tagging

condition C as follows:

(cid:2)

P (E|C) =

exp(

i λifi(E, C))
ZC

,

where the real-valued functions fi are used to indicate use-
ful features for predicting E, parameters λi are optimized
to ﬁt the model to the training data, and ZC is a normal-
ization constant. Several criteria for estimating the param-
eters of log-linear models exist, and we used MAP estimation
with Gaussian prior distribution [Chen and Rosenfeld, 1999],
which is most commonly and successfully applied in various
NLP tasks.

2.2 POS tagger

We employ the bidirectional inference model proposed by
Tsuruoka et al. [2005] for our POS taggers. The model con-
sists of 16 log-linear models, each of which provides the
probability

P (ti|ti−2, ti−1, ti+1, ti+2, s),

(1)

where s is a given sentence, and ti is the POS tag for the ith
word. An algorithm used by the POS taggers is shown in Fig-
ure 1. The key idea of the algorithm is that it does not work
in the usual left-to-right manner, instead it iteratively tries to
tag words that can be tagged most easily. “Easiness” of tag-
ging is measured by the highest probability value among the
probability distribution of the tags. When we calculated Eq.
1 in step 2 of the algorithm, each from ti−2, ti−1, ti+1, ti+2
could be NULL, so we prepared 24 = 16 classiﬁers to cover
every pattern of the appearances of NULL. Each token in a
training corpus was used as training data for all 16 classiﬁers.
The features used by the classiﬁers were the surface strings of
words, base forms of words obtained by a dictionary, preﬁxes
and sufﬁxes, and POSs of already tagged words.

Though the algorithm described above is totally determin-
istic, a slight modiﬁcation with beam search strategy could
make it output an approximation of k-best tag sequences with
probability values.

2.3 Grammar and parser

The parser we used is based on Head-Driven Phrase Struc-
ture Grammar (HPSG), which was developed using the Penn
Treebank [Miyao et al., 2004]. The disambiguation model for

the HPSG grammar is a combination of two log-linear mod-
els.

The ﬁrst log-linear model is for selecting lexical entries for
words of a POS-tagged sentence, which estimates the proba-
bility

P (li|wi, ti),

(2)

where wi, ti, and li represent an ith word, POS tag, and lex-
ical entry in a sentence, respectively. The information of wi
and ti are used in combination as features of the log-linear
model.

The second model is for selecting the most probable parse
from an HPSG parse forest F which is constructed from lex-
ical entries that are weighted by the ﬁrst model:

P (T |l1,...,n, w1,...,n, t1,...,n),

where T ∈ F . The overall disambiguation model is

argmax

T ∈F

P (T |w1,...,n, t1,...,n) =

argmax

T ∈F

P (T |l1,...,n, w1,...,n, t1,...,n)

P (li|wi, ti).

(cid:3)

i

The parsing algorithm for the disambiguation model is a
CYK-like chart parsing with iterative beam search [Ninomiya
et al., 2005].

3 Combining POS taggers and parsers

In pipelined parsers, POS taggers and syntactic parsers are
developed separately, and we can freely change taggers for
parsers, without any special care for the algorithms. To make
pipelined systems of ambiguous POS taggers and parsers
work the same way, we restrict the interface of ambiguous
taggers and formalize the condition of the syntactic disam-
biguation models that can be applied to the output of ambigu-
ous taggers without modiﬁcation.

We concentrate on the following situation: for each word
in a sentence, POS taggers output a set of candidate POS tags,
and the obtained tags are used to restrict parses in the parsing
model.

In our experiments, we compared the following three types

of taggers.

single The single tagger outputs the most probable single
tag for each word.

multi The multi tagger outputs a set of candidate POS tags
for each word.

prob The prob tagger is similar to multi, but each POS tag is
weighted by its probability. That is, it provides the probability
distribution P (tij|S), where S is an input sentence and tij
represents the jth candidate POS tag of the ith word.

We assume that the parsing model consists of two indepen-
dent models: terminal and non-terminal. A parse T of a given
sentence S consists of the terminal structure T t
and non-
terminal structure T nt
, and terminal and non-terminal models

IJCAI-07

1784

are used to estimate the probability distributions P (T t|S) and
P (T nt|T t, S). The best parse is given by

LP/LR

F1

87.12/86.71

86.91

argmax
T nt,T t

P (T nt, T t|S) =

argmax
T nt,T t

P (T nt|T t, S)P (T t|S).

Then, we assume that the terminal structure T t
can be fur-
ther decomposed into a sequence of terminal labels li, . . . , ln,
where the label li corresponds to the ith word of the sentence,
and terminal labels are independent of one another in the ter-
minal model:

P (T t|S) =

P (li|S).

(cid:3)

i

The overall disambiguation model becomes

argmax
T nt,T t

P (T nt|l0, . . . , ln, S)

P (li|S).

(3)

(cid:3)

i

We assume that a tractable estimation method and disam-
biguation algorithm applicable to Eq. 3 exist.

Let us then rewrite the distribution P (li|S) to make it de-
pendent on outputs of POS taggers, so that we can incorporate
the information of POS tags into the parsing model.

(cid:4)

P (li|S) =

P (tij , S)P (li|tij , S),

(4)

j

where tij is the jth element of a set of POS tags assigned to
the ith word of the sentence, and P (tij , S) is the probability
of that tag calculated by the prob tagger (multi and single tag-
gers can be integrated into the model similarly by assuming
the tags assigned to the same word by the taggers are equally
probable). By replacing P (li|S) that appears in Eq. 3 with
Eq. 4, we can apply the same disambiguation algorithm as
Eq. 3 to the combined system of an ambiguous POS tagger
and the parsing model.

This strategy is applicable to a wide range of grammars and
disambiguation models including PCFG, where the terminal
model is used to assign POS tags to words and lexicalized
grammars such as HPSG, where the terminal model assigns
lexical entries to words. The HPSG parser described in Sec-
tion 2.3 is straightforwardly adapted to this model, by taking
Eq. 2 as a terminal model.

One problem with this strategy is the increased ambiguity
introduced by multiple tags. As reported by Watson [2006],
the increase in computational costs can be suppressed by ap-
plying appropriate parsing algorithms. The parsing algorithm
we used [Ninomiya et al., 2005] is suitable for such purposes,
and we will show experimentally that the disambiguation of
the above model can be performed with efﬁciency similar to
models with single taggers.

Implementation of prob tagger

3.1
There can be various methods for implementing the prob tag-
ger described above. Our implementation outputs approxi-
mation of tag probabilities by marginalizing the probability
of k-best tag sequences obtained by the algorithm described
in Section 2.2.

Table 1: Accuracy on section 22 with correct POS tags

To control the ambiguity of the tagger, we introduce a pa-
rameter θ which is used to threshold the candidate tags. If the
marginalized probability of a candidate tag is smaller than the
probability of the best tag of the same word multiplied by θ,
the candidate is discarded.

Figure 2 illustrates example outputs of each tagger from
the 3 best analysis of the sentence, “Mr. Meador had been
executive vice president of Balcor,” without thresholding.

Marginalization of probability distribution output by POS
taggers loses information about the sequential dependency of
tags, which can harm the performance of the parser. For ex-
ample, let us consider the sentence, “Time ﬂies like an arrow.”
When there are two candidate POS tag sequences “NN VBZ
IN DT NN” and “VB NNS IN DT NN,” the output of multi
tagger will be “{NN, VB} {VBZ, NNS} IN DT NN,” which
can induce a tag sequence “VB VBZ IN DT NN” that is not
included in the original candidates.

4 Experiments

The ﬁrst experiment veriﬁed the effect of ambiguous POS
tagging using the Penn Treebank for both training and test-
ing. The second experiment examined the parser’s capability
of domain adaptation, using the Penn Treebank for training
the parser, biomedical texts for training the tagger, and the
GENIA corpus for testing.

Both experiments used the same HPSG grammar and
parser. The grammar was developed using the Penn Treebank
sections 02-21, and the disambiguation model was trained
on the same data. The accuracy of the parser evaluated on
the Penn Treebank section 221 using the correct POS tags is
shown in Table 1. Labeled precision (LP) and labeled re-
call (LR) are the precision and recall of dependencies with
predicate-argument labels, and F1 is the harmonic mean of
LP and LR. Each dependency between a pair of words is
counted as one event. These measures were also used in the
following experiments. The ﬁgures in Table 1 can be consid-
ered the practical upper bounds, because they are not affected
by incorrect predictions of POS taggers. (We will refer to the
results using the correct POS tags as gold.) In the follow-
ing, comparisons of the systems were performed using the F1
scores.

4.1 Parsing Penn Treebank

The POS taggers described in this section were trained on the
Penn Treebank sections 00-18, and the performance of the
tagger on the development set was 96.79%. As described
in Section 3.1, our taggers multi and prob have a hyper-
parameter θ for controlling the number of alternative answers

1Following the convention of literature on Penn Treebank pars-
ing, we used section 22 for development, and section 23 for the ﬁnal
test.

IJCAI-07

1785

Input

3 best sequences

(probability)

single
multi
prob

(probability)

Meador

NNP
NN
NNP
NNP

Mr
NNP
NN
NNP
NNP

had
been
VBD VBN
VBD VBN
VBD VBN
VBD VBN
{NNP, NN}
{NNP, NN} VBD VBN
NNP(0.925) NNP(0.925) VBD VBN
NN(0.075)

NN(0.075)

executive

JJ
JJ
NN
JJ

{JJ, NN}
JJ(0.955)
NN(0.045)

vice
NN
NN
NN
NN
NN
NN

president

NN
NN
NN
NN
NN
NN

of
IN
IN
IN
IN
IN
IN

Balcor
NNP
NNP
NNP
NNP
NNP
NNP

.
.
.
.
.
.
.

(0.879)
(0.075)
(0.046)

Figure 2: Example of tagging results

multi θ = 0.1
multi θ = 0.01
multi θ = 0.001
multi θ = 0.0001
prob θ = 0.1
prob θ = 0.01
prob θ = 0.001
prob θ = 0.0001

LP/LR

85.24/84.68
84.13/83.64
79.29/78.68
71.01/70.36
85.26/84.73
85.28/84.88
85.19/84.80
85.08/84.60

F1

84.96
83.88
78.98
70.68
84.99
85.08
84.99
84.84

Table 2: Accuracy with different θ

included in the output, where a smaller θ means a larger num-
ber of alternative answers. Before comparing ambiguous tag-
gers (i.e., multi and prob) with single, we ﬁrst determined
which of multi and prob and which value of θ perform best.

The performance of the parser for various θ is shown in
Table 2. The parsers with the multi tagger was outperformed
by prob in all cases2, so we only used the prob tagger in the
following experiments. The performance of the parser with
the prob tagger has a peak around θ = 0.01. The accuracy
slowly decreases for smaller θs, the reason for which might
be the problem noted in the last paragraph of Section 3.1. We
used the best performing θ (= 0.01) in the following experi-
ments.

It was interesting to see whether the contribution of am-
biguous tagging changed according to the performance of
the syntactic disambiguation model, because if ambiguous
POS tagging did not work well with a poorer disambiguation
model, ambiguous tagging could be considered not to help
the performance in domains unfamiliar for the parser, where
the performance of the disambiguation model was likely to
be lower. We observed a change in performance of the parser
when the number of sentences used for developing the gram-
mar and for training the syntactic disambiguation model was
changed. Table 3 shows the performance of the parser trained
with various numbers of sentences selected from the Penn
Treebank sections 02-21. Figure 3 shows F1 scores of the
systems trained on 3000, 10000, and 39832 sentences.

Ambiguous POS tagging with the prob tagger could be
contended to contribute to the performance of the parser, be-
cause the performance of the parser with the prob tagger was
consistently better than that of the single tagger in all of the

2Actually, multi has a negative effect, compared with the results
of single shown in the ’all’ row of Table 3, which has an F1 score of
84.99.

# of train-
ing data

300

1000

3000

10000

39832
(all)

Tagging
strategy
gold
single
prob
gold
single
prob
gold
single
prob
gold
single
prob
gold
single
prob

LP/LR

F1

63.63/36.57
62.51/35.43
61.46/39.62
75.21/68.41
73.84/66.33
73.56/68.10
81.72/78.94
79.97/77.42
79.90/78.34
85.35/84.32
83.50/82.09
83.47/82.59
87.12/86.71
85.26/84.72
85.28/84.88

46.45
45.23
48.18
71.65
69.88
70.72
80.31
78.67
79.11
84.83
82.79
83.03
86.91
84.99
85.08

Table 3: Accuracy with different sizes of training data

1
F

 87

 86

 85

 84

 83

 82

 81

 80

 79

 78

gold
single
prob

 5000

 10000  15000  20000  25000  30000  35000  40000

# of sentences

Figure 3: F1 of different sizes of training data

LP/LR

F1

gold
single
prob

86.91/86.28
84.41/83.80
85.24/84.89

86.59
84.10
85.06

mean
parsing
time
739 msec
785 msec
936 msec

Table 4: Accuracy on test set

IJCAI-07

1786

POS tagger

LP/LR

F1

gold
single-Penn
prob-Penn
single-BIO
prob-BIO

83.21/81.57
76.84/76.48
78.48/78.11
81.75/80.65
82.60/81.44

82.38
76.66
78.29
81.20
82.02

mean
parsing
time
845 msec
799 msec
950 msec
803 msec
873 msec

Table 5: Accuracy on GENIA

above experiments if evaluated in the F1 measure. With the
entire training data used for the development of the parser,
the contribution of ambiguous POS tagging was about 0.093.
Table 4 summarizes the performance of some of our systems
evaluated using section 23 of the Penn Treebank. The overall
tendency observed on the development set remained, but the
contribution of ambiguous tagging was considerably bigger
than in the development set.4 As can be seen from the table,
prob was the slowest of all, but the difference is not so big.
This may be because Ninomiya’s method of HPSG parsing,
which outputs the ﬁrst answer found, worked tolerantly to in-
crease of ambiguity.

4.2 Parsing GENIA corpus

One of the most crucial advantages of separating POS tagging
from syntactic parsing is that the domain adaptation of such
systems can be partly achieved by just changing POS tag-
gers. In this section, we verify that the effect of ambiguous
POS tagging is also remains for domains with which parsing
models are not familiar.

The test data we used was the GENIA treebank [Tateisi et
al., 2005], which annotates Penn Treebank-style tree struc-
tures to sentences of the GENIA corpus. The GENIA corpus
consists of the abstracts of biomedical papers adopted from
MEDLINE [Campbell et al., 1997]. We trained our POS tag-
ger with the mixture of the WSJ part of the Penn Treebank,
the Penn BioIE corpus [Kulick et al., 2004], and the GENIA
corpus, which consist of 38219, 29422, and 18508 sentences,
respectively. Following Hara et. al. [2005], we adopted 50
abstracts (467 sentences) for the evaluation. Performance of
the single tagger on these sentences was 98.27%.

The result is shown in Table 5. single-Penn and prob-Penn
are single and prob taggers trained on the Penn Treebank, and
single-BIO and prob-BIO are those trained on the biomedical
domain (i.e., the Penn BioIE corpus and the GENIA corpus).
Compared to the results with gold POS tags, the use of auto-
matic taggers trained on a different domain degraded the per-
formance by about 4 to 5 points in the F1 measure. This drop
was recovered using well-tuned taggers, and remarkably, the
results of prob-BIO were only 0.36 points lower than the up-
per bound.5 The difference in processing time was, again, not

3Though this ﬁgure alone seems not to be signiﬁcant, we will
show that the results of the same experiment using the test set have
signiﬁcant improvements.

4The improvement of prob over single is signiﬁcant with p <

10−4 according to stratiﬁed shufﬂing tests [Cohen, 1995].

5The improvement, in terms of recall, of prob over single is sig-

so signiﬁcant.

In summary, we showed that the adaptation of POS taggers
can signiﬁcantly improve the performance of parsers in new
domains, and the use of ambiguous POS tagging can extend
the improvement further.

5 Related Work

Charniak et al. [1996] investigated the use of POS taggers that
output multiple tags for parsing and concluded that single tag-
gers are preferable, because the accuracy of the tag sequences
selected by parsers did not improve signiﬁcantly, while the in-
crease in computational cost is considerable. Watson [2006]
revisited the same task and observed that in terms of the ac-
curacy of parsing, multiple taggers improve the performance
signiﬁcantly. Our results show that the taggers should pass to
the parser not only multiple answers, but also probability val-
ues for each candidate. Watson’s results also imply that ap-
propriate parsing strategies can make the increase of compu-
tational cost not as problematic as Charniak et al. suggested.
Our results again agree with Watson’s at this point.

Clark et al. [2004] introduced supertaggers that output
multiple supertag candidates, and the taggers were used as
a front end of their CCG parser. Because the training of su-
pertaggers require corpora more deeply annotated than POS
tagged ones, their method should have difﬁculties if we ex-
ploit taggers for domain adaptation.

Adaptation of general parsers to a biomedical domain has
already attracted several researchers. Because the biggest dif-
ference between general English and biomedical literature
is their vocabularies, use of lexical resources and tools is a
natural approach. Lease et al. [2005] modiﬁed Charniak’s
parser [1999] to make it conform to GENIA’s tagging con-
vention and made use of biomedical dictionaries to restrict
the parser output. They obtained an impressive error reduc-
tion of 14.2% and expected that further improvement is pos-
sible using named-entity recognizers.

We can see that the performance shown in Section 4.2
is signiﬁcantly lower than those in 4.1. This suggests that
changing POS taggers can only partially achieve domain
adaptation, so we should consider training syntactic disam-
biguation models on the target domain if we desire further
improvement. This is what Hara et al. [2005] explored, and
their results suggested that a small number of in-domain data
could bring considerable improvements of the performance if
out-of-domain data was as large as the Penn Treebank. Be-
cause their evaluation was done with POS tagged sentences,
whether this is still the case for the task of parsing raw texts
is an open question

6 Conclusion

In this paper, we demonstrated that the accuracy of parsers
could be improved by allowing POS taggers to output mul-
tiple answers for some words. When the performance of the
parser was evaluated in terms of precision/recall of predicate
argument relations, the performance of the parser with the

niﬁcant with p < 10−4 according to stratiﬁed shufﬂing tests. The
improvement of precision is not signiﬁcant.

IJCAI-07

1787

[Marcus et al., 1994] Mitchell P. Marcus, Beatrice Santorini,
and Mary Ann Marcinkiewicz. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2), 1994.

[Miyao et al., 2004] Yusuke Miyao, Takashi Ninomiya, and
Jun’ichi Tsujii. Corpus-oriented Grammar Development
for Acquiring a Head-driven Phrase Structure Grammar
from the Penn Treebank. In Proc. of IJCNLP-04, 2004.

[Ninomiya et al., 2005] Takashi Ninomiya, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun’ichi Tsujii.
Efﬁcacy
of Beam Thresholding, Uniﬁcation Filtering and Hybrid
Parsing in Probabilistic HPSG Parsing. In Proc. of IWPT
2005, pages 103–114, 2005.

[Ohta et al., 2002] Tomoko Ohta, Yuka Tateisi, Hideki
Mima, and Jun’ichi Tsujii. GENIA Corpus: an Annotated
Research Abstract Corpus in Molecular Biology Domain.
In Proceedings of the Human Language Technology Con-
ference (HLT 2002), March 2002.

[Pollard and Sag, 1994] Carl Pollard and Ivan A. Sag. Head-
Driven Phrase Structure Grammar. The University of
Chicago Press, 1994.

[Tateisi et al., 2005] Yuka Tateisi, Akane Yakushiji, Tomoko
Ohta, and Jun’ichi Tsujii. Syntax Annotation for the GE-
NIA corpus. In Proceedings of the IJCNLP 2005, Com-
panion volume, pages 222–227, October 2005.

[Tsuruoka and Tsujii, 2005] Yoshimasa

and
Jun’ichi Tsujii. Bidirectional Inference with the Easiest-
First Strategy for Tagging Sequence Data. In Proceedings
of HLT/EMNLP 2005, pages 467–474, 2005.

Tsuruoka

[Watson, 2006] R. Watson. Part-of-speech Tagging Models

for Parsing. In Proc. of CLUK 2006, 2006.

ambiguous POS tagger consistently outperformed that with
the conventional unambiguous tagger throughout the experi-
ments, and this was also the case for a domain that was not
used to train the parser. If the goal is to optimize the accuracy
of predicate-argument structures, we can say ambiguous tag-
ging is proﬁtable, and introducing ambiguous tagging without
harming the capability of domain adaptation is possible.

One apparent deﬁciency of our system is that it cannot take
into account the dependencies among outputs of the POS tag-
gers. Perhaps we can achieve further improvement by run-
ning separate parsing processes for each candidate POS se-
quence that is output by the POS tagger, but this approach
might not be acceptable considering computational cost.

References

[Campbell et al., 1997] K. Campbell, D. Oliver,

and
E. Shortliffe.
The Uniﬁed Medical Language Sys-
tem: Toward a Collaborative Approach For Solving
Terminological Problems, 1997.

[Charniak and Johnson, 2005] E. Charniak and M. Johnson.
Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proc. of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL’05), pages
173–180, June 2005.

[Charniak et al., 1996] Eugene Charniak, Glenn Carroll,
John Adcock, Anthony R. Cassandra, Yoshihiko Gotoh,
Jeremy Katz, Michael L. Littman, and John McCann. Tag-
gers for Parsers. Artiﬁcial Intelligence, 85(1-2):45–57,
1996.

[Charniak, 1999] Eugene Charniak. A Maximum-Entropy-

Inspired Parser. Technical Report CS-99-12, 1999.

[Chen and Rosenfeld, 1999] S. Chen and R. Rosenfeld. A
Gaussian prior for smoothing maximum entropy models.
In Technical Report CMUCS, 1999.

[Clark and Curran, 2004] Stephen Clark and James Curran.
The importance of supertagging for wide-coverage CCG
parsing.
In 20th International Conference on Computa-
tional Linguistics, 2004.

[Cohen, 1995] Paul R. Cohen. Empirical methods for artiﬁ-
cial intelligence. MIT Press, Cambridge, MA, USA, 1995.

[Hara et al., 2005] Tadayoshi Hara, Yusuke Miyao, and
Jun’ichi Tsujii. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain.
In Natu-
ral Language Processing IJCNLP 2005, volume 3651 of
Lecture Notes in Artiﬁcial Intelligence. Springer-Verlag
GmbH, 2005.

[Kulick et al., 2004] S. Kulick, A. Bies, M. Liberman,
M. Mandel, R. McDonald, M. Palmer, A. Schein, and
L. Ungar. Integrated annotation for biomedical informa-
tion extraction. In Proc. of HLT/NAACL 2004, 2004.

[Lease and Charniak, 2005] Matthew Lease and Eugene
In IJCNLP,

Charniak. Parsing Biomedical Literature.
pages 58–69, 2005.

IJCAI-07

1788


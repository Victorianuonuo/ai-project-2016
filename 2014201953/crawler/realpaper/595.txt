FastSLAM 2.0:  An Improved Particle Filtering Algorithm for Simultaneous 

Localization and Mapping that Provably Converges 

Michael Montemerlo  and  Sebastian Thrun 

Daphne Roller  and  Ben Wegbreit 

School of Computer Science 
Carnegie Mellon University 

Pittsburgh, PA  15213 

Computer Science Department 

Stanford University 

Stanford, CA 94305-9010 

mmde@cs.cmu.edu, thrun@csxmu.edu koller@cs.stanford.edu, ben@wegbreit.com 

Abstract 

In  [15],  Montemerlo et al.  proposed an algorithm called 
FastSLAM as an efficient and robust solution to the simul(cid:173)
taneous localization and mapping problem. This paper de(cid:173)
scribes  a modified  version  of FastSLAM  that overcomes 
important deficiencies of the original algorithm. We prove 
convergence of this new algorithm for linear SLAM prob(cid:173)
lems and provide real-world experimental results  that il(cid:173)
lustrate  an  order of magnitude  improvement  in  accuracy 
over the original FastSLAM algorithm. 

Introduction 

1 
Simultaneous  localization  and  mapping  (SLAM)  is  a  highly 
active  research  area  in  robotics and AI.  The  SLAM problem 
arises when a moving vehicle (e.g. a mobile robot, submarine, 
or  drone)  simultaneously  estimates  a  map  of its  environment 
and its pose relative to that map.  In the absence of global po(cid:173)
sition information, the vehicle's pose estimate will become in(cid:173)
creasingly  inaccurate,  as will  its map.  Since  maps may con(cid:173)
tain  thousands  of entities,  acquiring  large,  accurate  maps  is 
a  challenging  statistical  estimation  problem,  especially when 
performed in real-time. 

Most present-day research on SLAM originates from a sem(cid:173)
inal paper by Smith and Cheeseman [21], which proposed the 
use  of the  extended  Kalman  filter  (EKF)  for  solving  SLAM. 
This paper is based on the insights that errors in the map and 
pose errors are naturally correlated, and that the covariance ma(cid:173)
trix maintained by the EKF expresses such correlations.  New-
mann  [18]  recently proved  that  the  EKF  converges  for linear 
SLAM  problems,  where  the  motion  model  and  observation 
model are linear functions with Gaussian noise (see below). 

Unfortunately, EKF covariance matrices are quadratic in the 
size of the map, and updating them requires time quadratic in 
the  number  of landmarks  N.  This  quadratic  complexity  has 
long been recognized to be a major obstacle in scaling SLAM 
algorithms to maps with more than a few hundred features.  It 
also limits the applicability of SLAM  algorithms  to problems 
with ambiguous landmarks, which induces a data association 
problem  [2;  22].  Today's  most  robust  algorithms  for  SLAM 
with  unknown  data  association  maintain  multiple  hypotheses 
(tracks), which increase their computational complexity. 

Consequently, there has been a  flurry  on research on more 
efficient  SLAM  techniques  (see  e.g.,  [11]).  One  group  of 
researchers  has  developed  techniques  that  recursively  divide 
maps  into  submaps,  thereby  confining  most  computation  to 
small regions.  Some of these approaches still maintain global 

correlations  among  those  submaps,  hence  are  quadratic  but 
with a much reduced constant factor [1; 7; 22; 25].  Others re(cid:173)
strict the update exclusively to local maps [12], hence operate 
in constant time (assuming known data association). 

A  second  group  of  researchers  has  developed  techniques 
that  represent  maps  through  potential  functions  between  ad(cid:173)
jacent  landmarks,  similar  to  Markov  random  fields.  The  re(cid:173)
sulting representations require memory linear in the number of 
landmarks  [19;  23].  Under appropriate approximations,  such 
techniques  have  been  shown  to  provide  constant time  updat(cid:173)
ing (again for known data association). Unfortunately, no con(cid:173)
vergence  proof exists  for any  of these  extensions  of the  EKF, 
even for the generic case of linear SLAM. Furthermore, if land(cid:173)
marks are ambiguous, all of these approaches have to perform 
search to find appropriate data association hypotheses, adding 
a logarithmic factor to their update complexity. 

The FastSLAM algorithm, proposed in  [15]  as an efficient 
approach to SLAM based on particle  filtering  [6], does not fall 
into  either  of the  categories  above.  FastSLAM  takes  advan(cid:173)
tage of an important characteristic of the SLAM problem (with 
known data association):  landmark estimates are conditionally 
independent given the robot's path [17]. FastSLAM uses a par(cid:173)
ticle  filter  to sample over robot paths.  Each particle possesses 
N  low-dimensional  EKFs,  one  for  each  of the  N  landmarks. 
This representation requires O(NM)  memory,  where  M  is the 
number  of particles  in  the  particle  filter.  Updating  this  filter 
requires  0(M log N)  time,  with  or  without  knowledge  of the 
data associations.  However, the number of particles M  needed 
for convergence is unknown and has been suspected to be ex(cid:173)
ponential in the size of the map, in the worst-case. 

This paper proposes an improved version of the FastSLAM 
algorithm.  The  modification  is  conceptually  simple:  When 
proposing a new robot pose—an essential step in FastSLAM*s 
particle  filter—our  proposal distribution relies not only on the 
motion estimate (as is the case in FastSLAM), but also on the 
most  recent  sensor  measurement.  Such  an  approach  is  less 
wasteful  with  its  samples  than  the  original  FastSLAM  algo(cid:173)
rithm, especially in situations where the noise in motion is high 
relative to the measurement noise. 

To obtain a suitable proposal distribution, our algorithm lin(cid:173)
earizes  the  motion  and  the  measurement  model  in  the  same 
manner as the EKF. As a result, the proposal distribution can be 
calculated in closed form.  This extension parallels prior work 
by  Doucet and colleagues,  who proposed a  similar modifica(cid:173)
tion  for general  particle  filters  [6]  and  Markov  Chain  Monte 
Carlo techniques  for neural  networks  [4].  It  is  similar to  the 
arc  reversal  technique  proposed  for particle  filters  applied  to 

ROBOTICS 

1151 

Bayes  networks  [10],  and  it  is  similar to  recent  work  by  van 
der Merwe  [24],  who  uses an unscented filtering step  [9]  for 
generating proposal  distributions that accommodate the mea(cid:173)
surement. 

While this modification is conceptually simple, it has impor(cid:173)
tant ramifications.  A key contribution of this paper is a conver(cid:173)
gence proof for linear SLAM problems using a single particle. 
The  resulting  algorithm  requires  constant  updating  time.  To 
our knowledge,  the best previous  SLAM algorithm for which 
convergence  was shown requires quadratic update time.  Fur(cid:173)
thermore, we observe experimentally that our new FastSLAM 
algorithm, even with a single particle, yields significantly more 
accurate  results  on  a  challenging  real-world  benchmark  [7] 
than the previous version of the algorithm.  These findings are 
of significance, as many mobile robot systems are plagued by 
control  noise,  but  possess relatively  accurate  sensors.  More(cid:173)
over, they contradict a common belief that maintaining the en(cid:173)
tire covariance matrix is required for convergence  [5]. 

2  Simultaneous Localization and  Mapping 
SLAM  addresses the problem of simultaneously recovering a 
map and a vehicle pose from sensor data.  The map contains 
N features (landmarks) and shall be denoted 
The path of the vehicle will be denoted 
is a time index and st is the pose of the vehicle at time t. 
Most  state-of-the-art  SLAM  algorithms  calculate  (or  ap(cid:173)
proximate) variants of the following posterior distribution: 

where 

(1) 
is  a  sequence  of measurements  (e.g., 

where . 
range and bearing to nearby landmarks), and 
is  a  sequence  of  robot  controls  (e.g.,  velocities  for  robot 
wheels).  (As usual, we assume without loss of generality that 
only a single landmark is observed at each time 
The vari(cid:173)
are data association  variables —  each 
ables 

specifies  the  identity  of the  landmark  observed  at  time 

is known; we relax this assumption be(cid:173)

Initially, we assume 
low. 

asserted  in the time interval 

To calculate the posterior (1), the vehicle is given a proba(cid:173)
bilistic motion model, in the form of the conditional probability 
distribution 
This distribution describes how a 
control 
affects  the 
resulting pose.  Additionally, the vehicle is given a probabilis(cid:173)
describing 
tic measurement model, denoted 
how  measurements  evolve  from  state. 
In  accordance  to  the 
rich SLAM literature, we will model both models by nonlinear 
functions with independent Gaussian noise: 

(2) 
(3) 
and 6t are Gaus(cid:173)
, respectively. 

Here g and //. are nonlinear functions, and 
sian noise variables with covariance  > and 
3  FastSLAM 
FastSLAM [15] is based on the important observation [17] that 
the posterior can be factored 

(4) 

This  factorization  is  exact and universal  in  SLAM problems. 
It states that if one (hypothetically) knew the path of the vehi(cid:173)
cle, the landmark positions could be estimated independently 

1152 

of each other (hence the product over n). In practice, of course, 
one does not know the vehicle's path.  Nevertheless, the inde(cid:173)
pendence makes it possible to factor the posterior into a term 
that estimates the probability of each path, and TV  terms that 
estimate  the  position  of the  landmarks,  conditioned  on  each 
(hypothetical) path. 

FastSLAM  samples  the  path  using  a  particle  filter.  Each 
particle  has  attached  its  own  map,  consisting  of N  extended 
Kalman  filters.  Formally,  the 
contains a 
along  with  N  Gaussian  landmark  estimates,  de(cid:173)
path 
scribed by the mean 

and covariance 

th  particle 

(5) 

We  briefly  review  the  key  equations  of  the  regular  Fast(cid:173)
SLAM  algorithm,  and  refer the  reader to  [15].  Each  update 
in  FastSLAM  begins  with  sampling  new  poses based  on  the 
most recent motion command 

(6) 
Note that this proposal distribution only uses the motion com(cid:173)
mand ut, but ignores the measurement 
Next, FastSLAM updates the estimate of the observed land(cid:173)
mark^),  according  to  the  following posterior.  This  posterior 
takes the measurement 

into consideration: 

is a constant.  This posterior is the normalized product 
Here 
of two Gaussians as indicated.  However,  if 
is non-linear, the 
product  will not be  Gaussian  in  general.  To  make  the  result 
Gaussian,  FastSLAM employs the standard EKF "trick"  [13]: 
g is approximated by a linear function (see below).  Under this 
approximation,  (7)  is  equivalent  to  the  measurement  update 
equation familiar from the EKF literature [13]. 

In a final step, FastSLAM corrects for the fact that the pose 
sample 
has  been  generated  without  consideration  of the 
most recent measurement.  It does so by  resampling the parti(cid:173)
cles  [20].  The probability  for the 
th particle to be sampled 
(with  replacement)  is  given  by  the  following  variable 
commonly referred to as importance factor: 

As  shown  in  [15],  the  resampling  operation  can  be  imple(cid:173)
mented  in  0(M log N)  time using trees,  where  M  is the num(cid:173)
ber  of samples  and  N  the  number  of landmarks  in  the  map. 
However,  the  number  of particles  M  needed  for  convergence 
remains an open question. 

FastSLAM has been extended to SLAM with unknown data 
associations [14]. If the data association is unknown, each par(cid:173)
in FastSLAM makes its own local data association de(cid:173)
ticle 
cision 
by maximizing the measurement likelihood.  The 
formula tor finding the most likely data association maximizes 
the resulting importance weight: 

(8) 

ROBOTICS 

makes the dependence of the factor 

on the 
Here 
variable 
explicit.  A key characteristic of FastSLAM  is that 
each particle makes its own local data association.  In contrast, 
EKF techniques must commit to a single data association hy(cid:173)
pothesis for the entire  filter.  Results  in  [14]  show empirically 
that  this  difference  renders  FastSLAM  significantly  more  ro(cid:173)
bust to noise than EKF-style algorithms. 

4  FastSLAM 2.0 
Our new FastSLAM algorithm  is based on an obvious ineffi(cid:173)
ciency  arising  from  the  proposal  distribution  of regular  Fast(cid:173)
SLAM. In regular FastSLAM, the pose 
is sampled in ac(cid:173)
cordance to the prediction arising from the motion command 
as  specified  in  (6).  It does not consider the measurement 
acquired at time 
instead, the measurement is incorporated 
through resampling.  This approach is particularly troublesome 
if the noise in the vehicle  motion is  large relative to the mea(cid:173)
surement noise.  In such situations, sampled poses will mostly 
fall  into areas of low measurement likelihood, and will subse(cid:173)
quently be terminated in the resampling phase with high proba(cid:173)
bility.  Unfortunately, many real-world robot systems are char(cid:173)
acterized by relatively high motion noise.  As illustrated in the 
experimental  results  section  of this paper,  the  waste  incurred 
by this inefficient sampling scheme can be significant. 

4.1  Sampling The Pose 
FastSLAM 2.0 implements a single new idea:  Poses are sam(cid:173)
pled  under consideration  of both  the  motion 
and the mea(cid:173)
This  is  formally denoted by the  following sam(cid:173)
surement 
pling  distribution,  which  now takes  the  measurement 
into 
consideration: 

to 

(6), 

incorporating 

(9) 
In  comparison 
the  measurement 
only  makes  sense  if  we  incorporate  our  current  estimate 
the  variables 
of  the  observed 
(which  are  included  of the  condi(cid:173)
tioning variables above).  So in essence, the difference to Fast(cid:173)
SLAM  is that the  measurement 
is incorporated.  However, 
this change has important ramifications. 

landmark—obtained 

from 

The proposal distribution (9) can be reformulated as follows: 

That  is,  the  proposal  distribution  is  the  product  of two  fac(cid:173)
tors:  the  familiar next state distribution 
and 
Calculating  the  latter 
the  probability  of the  measurement 
involves an integration over possible landmark locations 

Unfortunately, sampling directly from this distribution is im(cid:173)

possible in the general case; it does not even possess a closed 
form.  Luckily,  a closed  form  solution can be attained if g  is 
approximated by a linear function (h may remain non-linear!): 

denotes  the  predicted  measure-

the predicted robot pose, and 
the predicted landmark location.  The matrices 
the Jacobians  (first  derivatives)  of  with  respect to 

and 

and s, respectively: 

Under this EKF-style approximation, the proposal distribution 
(9) is Gaussian with the following parameters: 

(11) 

(12) 

(13) 

(14) 

(15) 

4.2  Updating  The  Observed  Landmark  Estimate 
The updating step remains the same as in FastSLAM (see (7)). 
As stated in the previous section, 
is linearized to retain Gaus-
sianity  of the  posterior.  This  leads  to  the  following  update 
equations,  whose  derivation  is  equivalent to  that  of the  stan(cid:173)
dard EKF measurement update [13]: 

4.3  The  Importance Weights 
Resampling  is  necessary  even  in  our  new  version  of  Fast(cid:173)
SLAM, since the particles generated do not yet match the de(cid:173)
sired  posterior.  The  culprit  is  the  normalizer 
in  (10), 
which may be different for different particles m.  This normal(cid:173)
izer is the  inverse of the probability  of the measurement under 
the 
To 
account for this mismatch, our algorithm resamples in propor(cid:173)
tion to the following importance factor: 

th particle: 

This expression can once again be  approximated as a Gaus(cid:173)
sian by linearizing 
and its 
covariance is 

The mean of this Gaussian is 

(19) 

4.4  Unknown Data  Associations 
The approach for handling data association is similar to the one 
in regular FastSLAM: Again, we select the data association 
that  maximizes  the probability  of the  measurement 
m-th particle: 

for  the 

ROBOTICS 

1153 

At  first  glance,  one  may  be  tempted  to  substitute  wt
[m]  for 
the  probability  on  the  right-hand  side,  as  in  regular  Fast-
SLAM.  However, 
does  not  consider  the  sampled  pose 
whereas the expression here does. This leads to a slightly 

different probability, which is calculated as follows. 

5  Convergence 
A  key  result  in  this  paper  is  the  fact  that  our  new  version  of 
FastSLAM converges tor  M=1  particle, for a restricted class 
of linear  Gaussian  problems  (the  same  for  which  KFs  con(cid:173)
verge [5;  18]).  Specifically, our result applies to SLAM prob(cid:173)
lems characterized by the following linear form: 

Linearization  of  g  leads  to  a  Gaussian  over  zt  with  mean 
Both  are  functions  of 

and covariance 

the data association variable 

4.5  Feature  Management 
Finally, in cases with unknown data associations, features have 
to  be  created  dynamically.  As  is  common  for  SLAM  algo(cid:173)
rithms  [5],  our approach creates new  features when the mea(cid:173)
surement  probability  in  (20)  is  below  a  threshold.  However, 
real-world  data  with  frequent  outliers  will  generate  spurious 
landmarks  using  this  rule.  Following  [5],  our  approach  re(cid:173)
moves such spurious landmarks by keeping track of their pos(cid:173)
terior probability of existence.  Our mechanism analyzes mea(cid:173)
surement to the presence and absence of features. Observing a 
landmark provides positive evidence for its existence, whereas 
not observing it when 
falls within the robot's perceptual 
range  provides  negative  evidence.  The  posterior  probability 
of landmark existence  is accumulated by the following Bayes 
filter,  whose  log-odds  form  is  familiar  from  the  literature  on 
occupancy grid maps [16]: 

(22) 

a

n

Here 
are  the  log-odds  of the physical  existence  of land(cid:173)
mark 
in map  
 the prob(cid:173)
abilistic evidence provided by a measurement.  Under appro(cid:173)
priate  definition  of the  latter,  this  rule  provides  for  a  simple 
evidence counting rule.  If the  log odds drops below a prede(cid:173)
fined threshold, the corresponding landmark is removed from 
the map.  This mechanism enables particles to free themselves 
of spurious  features. 

is

d

(23) 
(24) 
Linear SLAM can be thought of as a robot operating in a Carte(cid:173)
sian  space  equipped  with  a  noise-free  compass,  and  sensors 
that measure  distances  to features  along  the  coordinate  axes. 
The following theorem,  whose proof can be found in the ap(cid:173)
pendix, states the convergence of our new FastSLAM variant: 
Theorem.  For  linear  SLAM,  FastSLAM  with  A / =l  parti(cid:173)
cles converges in expectation to the correct map if all features 
are observed infi nitely often, and if the location of one feature 
is known in advance. 

This theorem parallels a similar result previously published 
for the  Kalman  filter  [5;  18].  However,  this result  applies  to 
the Kalman  filter,  whose update requires time quadratic in the 
number  of  landmarks  N.  With  A / = l,  the  resampling  step 
becomes  obsolete  and  each  update  takes  constant  time.  To 
our  knowledge,  our  result  is  the  first  convergence  result  for 
a constant-time SLAM algorithm.  It even holds if all  features 
are arranged in a large loop, a situation often thought of as the 
worst case for SLAM problems  [8]. 
6  Experimental Results 
Systematic  experiments  showed that  FastSLAM  2.0 provides 
excellent  results  with  surprisingly  few  particles,  including 
M= 1.  Most  of  our  experiments  were  carried  out  using  a 
benchmark data set collected with an outdoor vehicle in Victo(cid:173)
ria Park, Sydney [7].  The vehicle path is 3.5km long, and the 
map is 320 meters wide.  The vehicle is equipped with differ(cid:173)
ential GPS that is used for evaluation only.  Fig.  la shows the 
map of the terrain, along with the path obtained by raw odome-
try (which is very poor, the average RMS error is 93.6 meters). 
This  data set is presently the most popular benchmark  in  the 
SLAM research community [3]. 

Figs,  lb&c  show  the  result  of  applying  FastSLAM  with 
M =l  particle  to  the  data  set,  without  (Fig.  lb)  and  with 
(Fig.  lc) the feature management approach described  in  Sec(cid:173)
tion 4.5.  In both  cases,  the estimated  vehicle  path  is  shown 

1154 

ROBOTICS 

efficient  use  of the particles,  particularly  in  situations  in  which 
the motion  noise  is high  in relation to the measurement noise. 
A  main  contribution  of this  paper  is  a convergence  proof for 
FastSLAM  with  a  single  particle.  This  proof is  an  improve(cid:173)
ment over previous  formal  results, which  applied to algorithms 
much  less efficient than  the  current one.  In fact, this result is  a 
first  convergence  result  for a constant  time  S L AM  algorithm. 
The theoretical  finding  is complemented by experimental re(cid:173)
sults  using  a  standard benchmark  data  set.  The new  algorithm 
is  found  to  outperform  the  previous  FastSLAM  algorithm  and 
the EKF approach to  S L AM by a large margin.  In fact, a single 
particle  suffices  to  generate  an  accurate  map  of a  challenging 
benchmark  data  set.  Despite  this  surprising  result,  the  use  of 
multiple  particles  is  clearly  warranted  in  situations  with  am(cid:173)
biguous  data  association.  We  believe  that  our  results  illustrate 
that  S L AM  can  be  solved  robustly  by  algorithms  that  are  sig(cid:173)
nificantly  more  efficient  than  EKF-based  algorithms. 
8  Acknowledgments 
This  work  was  supported by DARPA's  M A RS  and MARS2020 
program.  Daphne  Koller  was  supported by  the  Office  of Naval 
Research,  Young  Investigator  (PECASE)  grant  N00014-99-1-
0464.  We  thank  the  Hertz  Foundation  for  their  support  of 
Michael  Montemerlo's  graduate  research. 
References 
[ l ]  T. Bailey.  Mobile Robot Localisation and Mapping in Extensive 

Outdoor Environments.  PhD thesis, Univ. of Sydney, 2002. 

f2)  Y. Bar-Shalom and T. E. Fortmann.  Tracking and Data Associ(cid:173)

ation.  Academic Press,  1988. 

[3]  SLAM 

summer 
http://www.cas.kth.se/SLAM/ 

school 

2002 

[4]  N. de Freitas, M. Niranjan, A. Gee, and A. Doucet.  Sequential 
montc  carlo  methods  to  train  neural  network  models.  Neural 
Computation,  12(4), 2000. 

[5]  G. Dissanayakc, P. Newman, S. Clark, H.F. Durrant-Whytc, and 
M. Csorba.  A solution to the simultaneous localisation and map 
building (SLAM) problem.  IEEE Trans. Robotics and Automa(cid:173)
tion, 2001. 

[6]  A. Doucet, J.F.G. de Freitas, and N.J. Gordon, editors.  Sequen(cid:173)

tial Monte Carlo Methods In Practice.  Springer,  2001. 

[7]  J. Guivant and E.  Nebot.  Optimization of the simultaneous lo(cid:173)
calization and map building algorithm  for real time  implemen(cid:173)
tation. Trans, of Robotics and Automation, 2001. 

[8]  J.-S.  Gutmann and  K. Konolige.  Incremental  mapping of large 

cyclic environments.  Proc.  CIRA, 2000. 

[9]  S. J. Julier and J. K. Uhlmann.  A new extension of the Kalman 

fi Iter to nonlinear systems.  Proc. AeroSense,  1997. 

[10]  K. Kanazawa, D. Roller, and S.J. Russell.  Stochastic simulation 

algorithms for dynamic probabilistic networks.  UAI-95. 

[11]  Notes  ICRA  Workshop  Concurrent  Mapping and Localization 

for Autonomous Mobile  Robots,  2002 

[12]  J.J.  Leonard  and  H.J.S.  Feder.  A  computationally  efficient 
method  for  large-scale  concurrent  mapping  and  localization. 
Proc.JRSS, 1999. 

[13]  P.  May  beck.  Stochastic  Models,  Estimation,  and  Control,  Vol(cid:173)

ume I.  Academic Press,  1979. 

[14]  M.  Montcmcrlo  and  S.  Thrun. 

Simultaneous  localization 
and mapping with unknown data association using FastSLAM. 
Proc. ICRA, 2003, to appear 

[15]  M.  Montemerlo,  S.  Thrun,  D.  Roller,  and  B.  Wegbreit.  Fast(cid:173)
SLAM: A factored solution to the simultaneous localization and 
mapping problem. Proc. AAA1, 2002. 

[16]  H.  P.  Moravec.  Sensor  fusion  in  certainty  grids  for  mobile 

robots.  Al Magazine, 9(2),  1988. 

Figure 2:  RMS map error for regular FastSLAM (dashed line) versus 
FastSLAM 2.0 (solid line) on (a) the Victoria Park data (b) simulated 
data. FastSLAM 2.0's results even with a single particle are excellent. 
as a solid  line,  and the GPS  information  is shown as a  dashed 
line.  Results  of the  same  accuracy  were  previously  achieved 
only  with  0{N2)  EKF-style  methods  [7]  and  with  FastSLAM 
using  A/=5(J  particles.  The  feature  management  rule  reduces 
the  number of landmarks  in  the  map  from  768  (Fig.  lb)  to  343 
(Fig.  lc). 

Fig.  2  plots  the  RMS  error  of  the  vehicle  position  esti(cid:173)
mate  as  function  of  the  number  of  particles  for  the  Victoria 
data  set  (panel  a)  and  for  synthetic  simulation  data  (panel  b) 
taken  from  [15].  While  our new algorithm  does  approximately 
equally  well  for  any  number  of  particles,  regular  FastSLAM 
performs  poorly  for  very  small  particle  sets.  We  suspect  that 
the  poor  performance  of regular  FastSLAM  is  due  to  the  fact 
that  the  vehicle  possesses  relatively  inaccurate  odometry  (see 
Fig.  la),  yet  uses a low-noise  range  finder for landmark  detec(cid:173)
tion  (a  common  configuration  in  outdoor  robotics),  leading  to 
the  generation  of many  particles  of low  likelihood. 

The  small  number  of  examples  needed  to  obtain  state-of-
the-art  estimation  translates  to  unprecedented  efficiency  of the 
new  filter.  The  following  table  shows  the  results  required  to 
process  the  Victoria  Park  data  set  on  a  1GHz  Pentium  PC: 

[~EKF 

regular FastSLAM,  M=50 particles 

I  FastSLAM 2.0,  M=1  particle 

I  7,807 sec  I 

315 sec 

54 sec  | 

| 

In  comparison,  the  data  acquisition  required  1,550  seconds. 
Thus,  while  EKFs  cannot  be  run  in  real-time,  our  new  algo(cid:173)
rithm  requires  less  than  4%  of the  vehicle's  trajectory  time. 
7  Discussion 
This  paper  describes  a  modified  FastSLAM  algorithm  that 
is  uniformly  superior  to  the  FastSLAM  algorithms  proposed 
in  [15].  The  new  FastSLAM  algorithm utilizes a different pro(cid:173)
posal  distribution which  incorporates the most recent  measure(cid:173)
ment in the pose prediction process.  In doing so, it makes more 

ROBOTICS 

1155 

[17]  K.  Murphy.  Bayesian  map  learning  in  dynamic environments. 

Proc. NIPS, 1999. 

[18] P. Newman. On the Structure and Solution of the Simultaneous 
Localisation  and Map  Building Problem.  PhD  thesis,  Univ.  of 
Sydney, 2000. 

[ 19] M.A. Paskin. Thin junction tree fi Iters for simultaneous localiza(cid:173)
tion and mapping.  TR UCB/CSD-02-1198, UC Berkeley, 2002. 
[20]  D.B. Rubin.  Using the SIR algorithm to simulate posterior dis(cid:173)
tributions.  In Bayesian Statistics 3. Oxford Univ. Press, 1988. 

[21]  R. C.  Smith and P. Cheeseman.  On the representation and es(cid:173)
timation of spatial uncertainty.  Int. J.  Robotics Research,  5(4), 
1986. 

[22]  J.D. Tardos, J. Neira, P.M. Newman, and J.J. Leonard.  Robust 
mapping  and  localization  in  indoor  environments  using  sonar 
data. Int. J. Robotics Research, 21(4), 2002. 

[23]  S.  Thrun,  D.  Koller,  Z.  Ghahramani,  H.  Durrant-Whyte,  and 
A.Y.  Ng.  Simultaneous  mapping  and  localization  with  sparse 
extended information fi Iters.  Pmc.  WAFR, 2002. 

[24]  R. van der Merwe, N. de Freitas,  A. Doucet, and E. Wan.  The 

unscented particle fl Iter. Proc. NIPS, 2001. 

[25]  S.B. Williams, G. Dissanayake, and H. Durrant-Whyte.  An ef-

fi cient  approach  to  the  simultaneous  localisation  and  mapping 
problem.  Proc. ICRA, 2002. 

Appendix 

Substituting  this  back  into  (30)  and  subsequently  substituting 
according to (25) gives us: 

are pos(cid:173)
is a 

is larger in magnitude if and only  if 

The lemma follows from the fact that 
itive semidefl nite, hence the inverse  of 
contraction matrix. 
depends  on  the  sign  of 
exceed 

in this case. 

cannot 
qed. 
Of particular  interest  is the result  of observing the anchoring land(cid:173)
mark,  by  which  we  mean  the  landmark  whose  location  is  known. 
Without loss of generality, we assume that this landmark is 

however, 

Lemma 2.  If the  robot  observes  the  anchoring  landmark 

its 

pose  error will shrink  in  expectation. 

Proof, The anchoring landmark has zero error: 

and its 

covariancc is also zero: 

Plugging this into (32), we get: 

Finally, a lemma similar to Lemma  1  can be stated on the effect of 
Its proof is analogous that of Lemma 

pose errors a on map errors 
1, with reverse roles of a and 

Lemma 3. If the pose error < 

is  smaller than  the  error 

of the  observed  landmark  zt  in  magnitude,  observing 
landmark error 
the landmark  error 
will  not exceed 

shrinks the 
is larger than 
the latter may increase,  but in expectation 

in expectation. Conversely, 

Proof of Theorem.  Let  

denote landmark  error that  is  largest 

in magnitude among all landmark errors at time  . 

(34) 

Lemma 3 suggests that this error may increase in expectation, but only 
if the absolute robot pose  error 
exceeds this error in magnitude. 
However, in expectation this will only be the case for a limited num(cid:173)
ber of iterations.  In particular,  Lemma  1  guarantees that 
may 
only shrink in expectation.  Furthermore,  Lemma  2  states that every 
time the anchoring landmark is observed,  this error will  shrink by  a 
fi nite amount, regardless of the magnitude  of 
will 
ultimately become smaller in magnitude (and in expectation) than the 
largest landmark error.  Once this has happened, Lemma 3 states that 
the  latter will  shrink  in  expectation  every  time  the  landmark  is  ob(cid:173)
and 
served whose error is largest.  It is now easy to see that both 
converge to zero:  Observing the anchoring landmark induces a 
fi nite reduction  as  stated  in (33).  To increase 
to  its old value 
in  expectation,  the  total  landmark  error  must  shrink  in  expectation 
(Lemma 3).  This leads to an eternal  shrinkage of the total  landmark 
error down to zero.  Since this error is an upper bound for the expected 
pose error (see Lemma  1),  we also have convergence  in expectation 
for the robot pose error. 
qed. 

. Hence, 

1156 

ROBOTICS 


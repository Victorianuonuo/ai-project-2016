A–ne Algebraic Decision Diagrams (AADDs) and their

Application to Structured Probabilistic Inference

Scott Sanner

Department of Computer Science

University of Toronto

Toronto, ON M5S 3H5, CANADA

ssanner@cs.toronto.edu

David McAllester

TTI at Chicago

1427 East 60th Street

Chicago, IL 60637, USA
mcallester@tti-c.org

Abstract

We propose an a–ne extension to ADDs
(AADD) capable of compactly representing
context-speciﬂc, additive, and multiplicative
structure. We show that the AADD has worst-
case time and space performance within a mul-
tiplicative constant of that of ADDs, but that it
can be linear in the number of variables in cases
where ADDs are exponential in the number of
variables. We provide an empirical comparison
of tabular, ADD, and AADD representations
used in standard Bayes net and MDP inference
algorithms and conclude that the AADD per-
forms at least as well as the other two represen-
tations, and often yields an exponential perfor-
mance improvement over both when additive
or multiplicative structure can be exploited.
These results suggest that the AADD is likely
to yield exponential time and space improve-
ments for a variety of probabilistic inference al-
gorithms that currently use tables or ADDs.

Introduction

1
Algebraic decision diagrams (ADDs) [1] provide an e–-
cient means for representing and performing arithmetic
operations on functions from a factored boolean domain
to a real-valued range (i.e., Bn ! R). They rely on two
main principles to do this:
1. ADDs represent a function Bn ! R as a directed
acyclic graph { essentially a decision tree with re-
convergent branches and real-valued terminal nodes.

2. ADDs enforce a strict variable ordering on the de-
cisions from the root to the terminal node, enabling
a minimal, canonical diagram to be produced for a
given function. Thus, two identical functions will al-
ways have identical ADD representations under the
same variable ordering.

As shown in Figure 1, ADDs often provide an e–-
cient representation of functions with context-speciﬂc
independence [2], such as functions whose structure is
conjunctive (1a) or disjunctive (1b) in nature. Thus,

a) Conjunctive ADD
      Structure

b) Disjunctive ADD
      Structure

c) Additive and Multiplicative
      ADD Structure

A

A

A

B

B

B

B

C

1

0

1

C

0

C

C

C

C

7

7

6

6

5

5

4

4

3

3

2

2

1

1

0

0

Figure 1: Some example ADDs showing a) conjunctive
structure (f = if (a ^ b ^ c; 1; 0), b) disjunctive structure
(f = if (a _ b _ c; 1; 0)), and c) additive (f = 4a + 2b + c)
and multiplicative (f = (cid:176) 4a+2b+c) structure (top and bottom
sets of terminal values, respectively). The high (true) edge is
solid, the low (false) edge is dotted.

ADDs can oﬁer exponential space savings over a fully
enumerated tabular representation. However, the com-
pactness of ADDs does not extend to the case of additive
or multiplicative independence, as demonstrated by the
exponentially large representations when this structure
is present (1c). Unfortunately such structure often oc-
curs in probabilistic and decision-theoretic reasoning do-
mains, potentially leading to exponential running times
and space requirements for inference in these domains.

2 A–ne Algebraic Decision Diagrams
To address the limitations of ADDs, we introduce an
a–ne extension to the ADD (AADD) that is capable of
canonically and compactly representing context-speciﬂc,
additive, and multiplicative structure in functions from
Bn ! R. We deﬂne AADDs with the following BNF:

G ::= c + bF
F ::= 0 j if (F var; ch + bhFh; cl + blFl)

Here, ch and cl are real (or (cid:176)oating-point) constants in
the closed interval [0; 1], bh and bl are real constants in
the half-open interval (0; 1], F var is a boolean variable
associated with F , and Fl and Fh are non-terminals of
type F . We also impose the following constraints:

a) Additive AADD Structure
            f = 2a + b

b) Multiplicative AADD Structure
             f =  2a

< 1

b

;

< 0 , 3 >

<

3 ,

1 − 

3

>

 

< c   , b   >

1

1

< c   , b   >

2

2

Fvar
1

op

Fvar
2

 

< c

, b

>

1,h

1,h

< c

, b

1,l

>

1,l

< c

, b

>

2,h

2,h

< c

, b

2,l

>

2,l

Fvar
1,h

Fvar
1,l

Fvar
2,h

Fvar
2,l

< 2/3, 1/3 >

< 1, 0 >

A

B

0

< 0 , 1/3 >

0 ,<

2

−
1 − 

3

3

>

< 0 , 0 >

< 0 , 0 >

A

B

0

<

−
1 − 

3

3

−1,
1 − 

>

3

< 1, 0 >

Figure 2: Portions of the ADDs from ﬂgure 1c expressed as
generalized AADDs. The edge weights are given as hc; bi.

1. The variable F var does not appear in Fh or Fl.
2. min(ch; cl) = 0
3. max(ch + bh; cl + bl) = 1
4. If Fh = 0 then bh = 0 and ch > 0. Similarly for Fl.
5. In the grammar for G, we require that if F = 0 then

b = 0, otherwise b > 0.

Expressions in the grammar for F will be called normal-
ized AADDs and expressions in the grammar for G will
be called generalized AADDs.1

Let V al(F; ‰) be the value of AADD F under variable
value assignment ‰. This can be deﬂned recursively by
the following equation:

V al(F; ‰) =8<
:

F = 0 :
0
F 6= 0 ^ ‰(F var) = true :
ch + bh ¢ V al(Fh; ‰)
F 6= 0 ^ ‰(F var) = f alse : cl + bl ¢ V al(Fl; ‰)
Lemma 2.1. For any normalized AADD F we have that
V al(F; ‰) is in the interval [0; 1], min‰ V al(F; ‰) = 0,
and if F 6= 0 then max‰ V al(F; ‰) = 1.
We say that F satisﬂes a given variable ordering if
F = 0 or F is of the form if (F var; ch + bhFh; cl + blFl)
where F var does not occur in Fh or Fl and F var is the
earliest variable under the given ordering occuring in F .
We say that a generalized AADD of form c + bF satisﬂes
the order if F satisﬂes the order.

Lemma 2.2. Fix a variable ordering. For any non-
constant function g mapping Bn ¡! R, there exists a
unique generalized AADD G satisfying the given vari-
able ordering such that for all ‰ 2 Bn we have g(‰) =
V al(G; ‰).
Both lemmas can be proved by straightforward induc-
tion on n. The second lemma shows that under a given
variable ordering, generalized AADDs are canonical, i.e.,
two identical functions will always have identical AADD
representations.

As an example of two AADDs with compact additive
and multiplicative structure, Figure 2 shows portions of

1Since normalized AADDs in grammar F are restricted
to the range [0; 1], we need the top-level positive a–ne trans-
form of generalized AADDs in grammar G to allow for the
representation of functions with arbitrary range.

Figure 3: Two AADD nodes F1 and F2 and a binary oper-
ation op with the corresponding notation used in this paper.

the exponentially sized ADDs from Figure 1c represented
by generalized AADDs of linear size.

We let op denote a binary operator on AADDs with
possible operations being addition, substraction, multi-
plication, division, min, and max denoted respectively
as ', “, ›, ﬁ, min(¢;¢), and max(¢;¢).
3 Related Work
There has been much related work in the formal veriﬂ-
cation literature that has attempted to tackle additive
and multiplicative structure in representation of func-
tions from Bn ! Bm. These include *BMDs, K*BMDs,
EVBDDs, FEVBDDs, HDDs, and *PHDDs [3]. While
space limitations prevent us from discussing all of the
diﬁerences between AADDs and these data structures,
we note two major diﬁerences: 1) These data structures
have diﬁerent canonical forms relying on GCD factoriza-
tion and do not satisfy the invariant property of AADDs
that internal nodes have range [0; 1]. 2) The fact that
these data structures have integer terminals (Bm) re-
quires a rational or direct (cid:176)oating-point representation
of values in R.
In our experience, this renders them
unusable for probabilistic inference due to considerable
numerical precision error and computational di–culties.

4 Algorithms
We now deﬂne AADD algorithms that are analogs of
those given by Bryant [4]2 except that they are extended
to propagate the a–ne transform of the edge weights
on recursion and to compute the normalization of the
resulting node on return.

All algorithms rely on the helper function GetGN ode
given in Algorithm 1 that takes an unnormalized AADD
node of the form if (v; ch+bhFh; cl+blFl) and returns the
cached, generalized AADD node of the form hcr +brFri.3
4.1 Reduce
The Reduce algorithm given in Algorithm 2 takes an
arbitrary ordered AADD, normalizes and caches the in-
ternal nodes, and returns the corresponding generalized
AADD. One nice property of the Reduce algorithm is
that one does not need to prespecify the structure that

2Bryant gives algorithms for binary decision diagrams
(BDDs), but they are essentially identical to those for ADDs.
3In the algorithms we use the representation hc; b; Fi,
while in the text we often use the equivalent notation hc+bFi
to make the node semantics more clear.

Algorithm 1: GetGN ode(v;hch; bh; Fhi; hcl; bl; Fli) ¡!
hcr; br; Fri
input

: v; hch; bh; Fhi; hcl; bl; Fli : Var, oﬁset, mult,
and node id for high/low branches

output : hcr; br; Fri : Return values for oﬁset,
begin

multiplier, and canonical node id

// If branches redundant, return child
if (cl = ch ^ bl = bh ^ Fl = Fh) then

return hcl; bl; Fli;

// Non-redundant so compute canonical form
rmin := min(cl; ch); rmax := max(cl + bl; ch + bh);
rrange := rmax ¡ rmin;
ch := (ch ¡ rmin)=rrange;
cl := (cl ¡ rmin)=rrange;
bl := bl=rrange;
if (hv; hch; bh; Fhi; hcl; bl; Flii ! id is not in node
cache) then

bh := bh=rrange;

id := currently unallocated id;
insert hv; hch; bh; Fhi; hcl; bl; Flii ! id in cache;

// Return the cached, canonical node
return hrmin; rrange; idi ;

end

the AADD should exploit. If the represented function
contains context-speciﬂc, additive, or multiplicative in-
dependence, the Reduce algorithm will compactly repre-
sent this structure uniquely and automatically w.r.t. the
variable ordering as guaranteed by previous lemmas.

4.2 Apply
The Apply routine given in Algorithm 3 takes two gen-
eralized AADD operands and an operation as given in
Figure 3 and produces the resulting generalized AADD.
The control (cid:176)ow of the algorithm is straightforward: We
ﬂrst check whether we can compute the result immedi-
ately, otherwise we normalize the operands to a canonical
form and check if we can reuse the result of a previously
cached computation. If we can do neither of these, we
then choose a variable to branch on and recursively call
the Apply routine for each instantiation of the variable.
We cover these steps in-depth in the following sections.

Terminal computation
The function ComputeResult given in the top half of
Table 1, determines if the result of a computation can
be immediately computed without recursion. The ﬂrst
entry in this table is required for proper termination of
the algorithm as it computes the result of an operation
applied to two terminal 0 nodes. However, the other
entries denote a number of pruning optimizations that
immediately return a node without recursion. For ex-
ample, given the operation h3 + 4F1i'h5 + 6F1i, we can
immediately return the result h8 + 10F1i.
Recursive computation
If a call to Apply is unable to immediately compute a
result or reuse a previously cached computation, we must
recursively compute the result. For this we have two

Algorithm 2: Reduce(hc; b; Fi) ¡! hcr; br; Fri

input
output : hcr; br; Fri : Return values for oﬁset,
begin

: hc; b; Fi : Oﬁset, multiplier, and node id
multiplier, and node id

// Check for terminal node
if (F = 0) then

return hc; 0; 0i;

// Check reduce cache
if ( F ! hcr; br; Fri is not in reduce cache) then

// Not in cache, so recurse
hch; bh; Fhi := Reduce(ch; bh; Fh);
hcl; bl; Fli := Reduce(cl; bl; Fl);
// Retrieve canonical form
hcr; br; Fri := GetGN ode(F var; hch; bh; Fhi; hcl; bl; Fli);
// Put in cache
insert F ! hcr; br; Fri in reduce cache;

// Return canonical reduced node
return hc + b ¢ cr; b ¢ br; Fri;

end

1

2

6= F var

cases (the third case where both operands are 0 terminal
nodes having been taken care of in the previous section):
F1 or F2 is a 0 terminal node, or F var
: We
assume the operation is commutative and reorder the
operands so that F1 is the 0 node or the operand whose
variable comes later in the variable ordering.4 Then we
propagate the a–ne transform to each of F2’s branches
and compute the operation applied separately to F1 and
each of F2’s high and low branches. We then build an if
statement conditional on F var
and normalize it to obtain
the generalized AADD node hcr; br; Fri for the result:
hch; bh; Fhi = Apply(hc1; b1; F1i; hc2 + b2c2;h; b2b2;h; F2;hi; op)
hcl; bl; Fli = Apply(hc1; b1; F1i; hc2 + b2c2;l; b2b2;l; F2;li; op)
hcr; br; Fri = GetGN ode(F var
F1 and F2 are non-terminal nodes and F var
1 = F var
:
Since the variables for each operand match, we know the
result hcr; br; Fri is simply a generalized if statement
branching on F var
) with the true case being
the operator applied to the high branches of F1 and F2
and likewise for the false case and the low branches:

; hch; bh; Fhi; hcl; bl; Fli)

(= F var

1

2

2

2

2

hch; bh; Fhi = Apply(hc1 + b1c1;h; b1b1;h; F1;hi;

hc2 + b2c2;h; b2b2;h; F2;hi; op)

1

hc2 + b2c2;l; b2b2;l; F2;li; op)

hcl; bl; Fli = Apply(hc1 + b1c1;l; b1b1;l; F1;li
hcr; br; Fri = GetGN ode(F var
4We note that the ﬂrst case prohibits the use of the non-
commutative “ and ﬁ operations. However, a simple solution
would be to recursively descend on either F1 or F2 rather than
assuming commutativity and swapping operands to ensure
descent on F2. To accommodate general non-commutative
operations, we have used this alternate approach in our spec-
iﬂcation of the Apply routine given in Algorithm 3.

; hch; bh; Fhi; hcl; bl; Fli)

Canonical caching
If the AADD Apply algorithm were to compute and
cache the results of applying an operation directly to the
operands, the algorithm would provably have the same
time complexity as the ADD Apply algorithm. Yet, if we
were to compute h0+1F1i'h0+2F2i and cache the result
hcr + brFri, we could compute h5 + 2F1i ' h4 + 4F2i =
h(2cr + 9) + 2brFri without recursion.
This suggests a canonical caching scheme that normal-
izes all cache entries to increase the chance of a cache hit.
The actual result can then be easily computed from the
cached result by reversing the normalization. This en-
sures optimal reuse of the Apply operations cache and
can lead to an exponential reduction in running time
over the non-canonical caching version.

We introduce two additional functions to perform this
caching: GetN ormCacheKey to compute the canonical
cache key, and M odif yResult to reverse the normaliza-
tion in order to compute the actual result. These algo-
rithms are summarized in the bottom half of Table 1.

4.3 Other Operations
While space limitations prevent us from covering all of
the operations that can be performed (e–ciently) on
AADDs, we brie(cid:176)y summarize some of them here:
min and max computation: The min and max of a
generalized AADD node hc + bFi are respectively c and
c + b due to [0; 1] normalization of F .
Restriction: The restriction of a variable xi in a func-
tion to either true or false (i.e. Fjxi=T =F ) can be com-
puted by returning the proper branch of nodes contain-
ing that test variable during the Reduce operation.
Sum out/marginalization: A variable xi can be
summed (or marginalized) out of a function F simply
by computing the sum of the restricted functions (i.e.
Fjxi=T ' Fjxi=F ).
Negation/reciprocation: While it may seem that
negation of a generalized AADD node hc + bFi would
be as simple as h¡c + ¡bFi, we note that this vio-
lates our normalization scheme which requires b > 0.
Consequently, negation must be performed explicitly as
hc+bF i ) must
0 “ hc + bFi. Likewise, reciprocation (i.e.,
be performed explicitly as 1 ﬁ hc + bFi.
Variable reordering: A generalization of Rudell’s
ADD variable reordering algorithm [5] that recomputes
edge weights of reordered nodes can be applied to
AADDs without loss of e–ciency.

1

4.4 Cache Implementation
If one were to use a naive cache implementation that re-
lied on exact (cid:176)oating-point values for hashing and equal-
ity testing, one would ﬂnd that many nodes which should
be the same under exact computation often turn out to
have oﬁsets or multipliers diﬁering by §1e-15 due to nu-
merical precision error. This can result in an exponential
explosion of nodes if not controlled. Consequently, it is
better to use a hashing scheme that considers equality
within some range of numerical precision error †. While

Algorithm 3: Apply(hc1; b1; F1i; hc2; b2; F2i; op) ¡!
hcr; br; Fri
: hc1; b1; F1i; hc2; b2; F2i; op : Nodes and op
input
output : hcr; br; Fri : Generalized node to return
begin
// Check if result can be immediately computed
if
hcr; br; Fri is not null ) then

(ComputeResult(hc1; b1; F1i; hc2; b2; F2i; op) !
return hcr; br; Fri;

2; b0

// Get normalized key and check apply cache
hhc0
if ( hhc0
2; b0
in apply cache) then

1i; hc0
1; b0
GetN ormCacheKey(hc1; b1; F1i; hc2; b2; F2i; op);
1; b0
2; F2i; opi ! hcr; br; Fri is not

2ii :=
1; F1i; hc0

// Not terminal, so recurse
if (F1 is a non-terminal node) then

if (F2 is a non-terminal node) then

if (F var

1

comes before F var

2

) then

;

;

var := F var

1

else

var := F var

2

else

var := F var

1

;

else

var := F var

2

;

) then

1 ¢ c1;h;

1

else

F v1

h := F1;h;

F v1
l
cv1
l
bv1
l

// Propagate a–ne transform to branches
if (F1 is non-terminal ^ var = F var
1 + b0

1 + b0
1 ¢ b1;l;
cv1
l=h := c0
1;

:= F1;l; F v1
cv1
:= c0
h := c0
1 ¢ c1;l;
bv1
:= b0
h := b0
1 ¢ b1;h;
bv1
l=h := b0
1;
if (F2 is non-terminal ^ var = F var
2 + b0

:= F2;l; F v2
cv2
:= c0
h := c0
2 ¢ c2;l;
bv2
:= b0
h := b0
1 ¢ b2;h;
bv2
l=h := b0
2;

2 + b0
2 ¢ b2;l;
cv2
l=h := c0
2;

F v2
l
cv2
l
bv2
l

h := F2;h;

l=h := F1;

l=h := F2;

F v2

else

2

2 ¢ c2;h;

) then

l

; bv1
l
h ; bv1

// Recurse and get cached result
; F v1
hcl; bl; Fli := Apply(hcv1
l i; op);
hch; bh; Fhi := Apply(hcv1
h ; F v1
h i; op);
hcr; br; Fri := GetGN ode(var;hch; bh; Fhi; hcl; bl; Fli);
// Put result in apply cache and return
insert hc0
into apply cache;

2; F2; opi ! hcr; br; Fri

l i; hcv2
h i; hcv2

; F v2
h ; F v2

; bv2
l
h ; bv2

1; F1; c0

2; b0

1; b0

l

return M odif yResult(hcr; br; Fri);

end

it is di–cult to guarantee such an exact property for
an e–cient hashing scheme, we next outline an approx-
imate approach that we have found to work both e–-
ciently and nearly optimally in practice.

The node cache used in GetGN ode and the operation
result cache used in Apply both use cache keys containing
four (cid:176)oating-point values (i.e., the oﬁsets and multipliers

Operation and Conditions
hc1 + b1F1i hopi hc2 + b2F2i; F1 = F2 = 0
max(hc1 + b1F1i; hc2 + b2F2i); c1 + b1 • c2
max(hc1 + b1F1i; hc2 + b2F2i); c2 + b2 • c1
hc1 + b1F1i ' hc2 + b2F2i; F1 = F2
max(hc1 + b1F1i; hc2 + b2F1i); F1 = F2;
(c1 ‚ c2 ^ b1 ‚ b2) _ (c2 ‚ c1 ^ b2 ‚ b1)

ComputeResult(hc1; b1; F1i; hc2; b2; F2i; op) ¡! hcr; br; Fri
Return Value
h(c1 hopi c2) + 0 ¢ 0i
hc2 + b2F2i
hc1 + b1F1i
h(c1 + c2) + (b1 + b2)F1i
c1 ‚ c2 ^ b1 ‚ b2 : hc1 + b1F1i
c2 ‚ c1 ^ b2 ‚ b1 : hc2 + b2F1i

Note: for all max operations above, return opposite for min

hc1 + b1F1i hopi hc2 + b2F2i; F2 = 0; op 2 f'; “g
hc1 + b1F1i hopi hc2 + b2F2i; F2 = 0; c2 ‚ 0; op 2 f›; ﬁg

h(c1 hopi c2) + b1F1i
h(c1 hopi c2) + (b1 hopi c2)F1i
Note: above two operations can be modiﬂed to handle F1 = 0 when op 2 f'; ›g

other

null

GetN ormCacheKey(hc1; b1; F1i; hc2; b2; F2i; op) ¡! hhc0

1; b0

2; b0

1ihc0

Operation and Conditions

Normalized Cache Key and Computation

2ii and M odif yResult(hcr; br; Fri) ¡! hc0

r; F 0
ri
Result Modiﬂcation

r; b0

hc1 + b1F1i ' hc2 + b2F2i; F1 6= 0
hc1 + b1F1i “ hc2 + b2F2i; F1 6= 0
hc1 + b1F1i › hc2 + b2F2i; F1 6= 0
hc1 + b1F1i ﬁ hc2 + b2F2i; F1 6= 0
max(hc1 + b1F1i; hc2 + b2F2i);
F1 6= 0, Note: same for min
any hopi not matching above:
hc1 + b1F1i hopi hc2 + b2F2i

hcr + brFri = h0 + 1F1i ' h0 + (b2=b1)F2i
hcr + brFri = h0 + 1F1i “ h0 + (b2=b1)F2i
hcr + brFri = h(c1=b1) + F1i › h(c2=b2) + F2i
hcr + brFri = h(c1=b1) + F1i ﬁ h(c2=b2) + F2i
hcr + brFri = max(h0 + 1F1i; h(c2 ¡ c1)=b1 + (b2=b1)F2i)

h(c1 + c2 + b1cr) + b1brFri
h(c1 ¡ c2 + b1cr) + b1brFri
hb1b2cr + b1b2brFri
h(b1=b2)cr + (b1=b2)brFri
h(c1 + b1cr) + b1brFri

hcr + brFri = hc1 + b1F1i hopi hc2 + b2F2i

hcr + brFri

Table 1: Input and output summaries of the ComputeResult, GetN ormCacheKey, and M odif yResult routines.

for two AADD nodes).
If we consider this 4-tuple of
(cid:176)oating-point values to be a point in Euclidean space,
then we can measure the error between two 4-tuples
hu1; u2; u3; u4i and hv1; v2; v3; v4i as the L2 (Euclidean)
distance between these points. As a consequence of the
triangle inequality for four dimensions, we know that

<v ,v >1 2

 

2 + v2

2 + v3

2 + v4

2 + u4

2 + u2

2 + u3

2-pv1

p(u1 ¡ v1)2 + (u2 ¡ v2)2 + (u3 ¡ v3)2 + (u4 ¡ v4)2 • †
)jpu1
2j • †.
This is shown graphically for two dimensions in Figure 4.
Based on this inequality, we can use the following ap-
proximate hashing scheme for hv1; v2; v3; v4i: Compute
the L2 distance between hv1; v2; v3; v4i and the origin;
For hashing, extract only the bits from the (cid:176)oating-point
base representing a fractional portion greater than † and
use this as the integer representation of the hash key
(we are eﬁectively discretizing the distances into buckets
of width †); For equality, test that the true L2 metric
between hu1; u2; u3; u4i and hv1; v2; v3; v4i is less than †.
While this hashing scheme does not guarantee that all
4-tuples having distance from the origin h0; 0; 0; 0i within
† hash to the same bucket (some 4-tuples within † could
fall over bucket boundaries), we found that with bucket
width † = 1e-9 and numerical precision error generally
less than 1e-13, there was only a small probability of
two nodes equal within numerical precision straddling a
bucket boundary. For the empirical results described in
Section 6, this hashing scheme was su–cient to prevent
any uncontrollable cases of numerical precision error.

<u ,u >2

1

 

1d

2d

<0,0>

2 + u2

Figure 4: A 2D visualization of the hashing scheme we use.
All points within † of hu1; u2i (the dark circle) lie within the
ring having outer and inner radius pu1
2 § †. Thus,
a hashing scheme which hashes all points within the ring to
the same bucket guarantees that all points within † of hu1; u2i
also hash to the same bucket. We can use squared distances
to avoid a p , thus the hash key for hv1; v2i can be e–ciently
computed as the squared distance from hv1; v2i to the origin.
5 Theoretical Results
Here we present two fundamental results for AADDs:

Theorem 5.1. The time and space performance of
Reduce and Apply for AADDs is within a multiplicative
constant of that of ADDs in the worst case.

Proof Sketch. Under the same variable ordering, an
ADD is equivalent to a non-canonical AADD with ﬂxed
edge weights c = 0; b = 1. Thus the ADD Reduce
and Apply algorithms can be seen as analogs of the
AADD algorithms without the additional constant-time

e
e
e
Running Time vs. #Vars for Sum

Running Time vs. #Vars for Product

Running Time vs. #Vars for Max

)
s
m

i

 

(
 
e
m
T
g
n
n
n
u
R

i

Table
ADD
AADD

10000

8000

6000

4000

2000

0

6

8

10

)
s
m

i

 

(
 
e
m
T
g
n
n
n
u
R

i

Table
ADD
AADD

10000

8000

6000

4000

2000

0

6

8

10

14

16

18

12000

10000

)
s
m

Table
ADD
AADD

i

 

(
 
e
m
T
g
n
n
n
u
R

i

14

16

18

8000

6000

4000

2000

0

6

8

10

12

# Variables

#Nodes/Entries vs. #Vars for Sum

12

# Variables

#Nodes/Entries vs. #Vars for Product

12

# Variables

14

16

18

#Nodes/Entries vs. #Vars for Max

x 105

Table
ADD
AADD

s
e
i
r
t

n
E
/
s
e
d
o
N
#

6

5

4

3

2

1

0

x 105

Table
ADD
AADD

s
e
i
r
t

n
E
/
s
e
d
o
N
#

6

5

4

3

2

1

0

x 105

Table
ADD
AADD

s
e
i
r
t

n
E
/
s
e
d
o
N
#

6

5

4

3

2

1

0

6

8

10

12

# Variables

14

16

18

6

8

10

12

# Variables

14

16

18

6

8

10

12

# Variables

14

16

18

Figure 5: Comparison of Apply operation running time (top) and table entries/nodes (bottom) for tables, ADDs and AADDs.

Left to Right: (Pi 2ixi) ' (Pi 2ixi), ((cid:176)Pi 2ixi ) › ((cid:176)Pi 2ixi ), max(Pi 2ixi;Pi 2ixi). Note the linear time/space for AADDs.

and constant-space overhead of normalization. Since
normalization can only increase the number of Reduce
and Apply cache hits and reduce the number of cached
nodes, it is clear that an AADD must generate equal or
fewer Reduce and Apply calls and have equal or fewer
cached nodes than the corresponding ADD. This allows
us to conclude that in the worst case where the AADD
generates as many Reduce and Apply calls and cache hits
as the ADD, the AADD is still within a multiplicative
constant of the time and space required by the ADD.
Theorem 5.2. There exist functions F1 and F2 and an
operator op such that the running time and space per-
formance of Apply(F1; F2; op) for AADDs can be linear
in the number of variables when the corresponding ADD
operations are exponential in the number of variables.
Proof Sketch. Two functions and Apply operation exam-
i=1 2ixi and
i=1 (cid:176)2ixi . (Examples of these operands as
ADDs and AADDs were given in Figures 1c and 2.) Be-
cause these computations result in a number of terminal
values exponential in n, the ADD operations must re-
quire time and space exponential in n. On the other
hand, it is known that the operands can be represented
in linear-sized AADDs. Due to this structure, the Apply
algorithm will begin by recursing on the high branch of
both operands to depth n. Then, at each step as it re-
turns and recurses down the low branch, the respective
additive and multiplicative diﬁerences will be normalized
out due to the canonical caching scheme, thus yielding
cache hits for all low branches. This results in n cached
nodes and 2n Apply calls for the AADD operations.

ples where this holds true arePn
i=1 (cid:176)2ixi ›Qn
Qn

i=1 2ixi'Pn

6 Empirical Results
6.1 Basic Operations
Figure 5 demonstrates the relative time and space per-
formance of tables, ADDs, and AADDs on a number of
basic operations. These verify the exponential to linear
space and time reductions proved in Theorem 5.2.

6.2 Bayes Nets
For Bayes nets (BNs), we simply evaluate the variable
elimination algorithm [6] (under a ﬂxed, greedy tree-
width minimizing variable ordering) with the conditional

probability tables (CPTs) Pj and corresponding opera-
tions replaced with those for tables, ADDs, and AADDs:

Xxi =2Query YP1:::Pj

P1(x1jP arents(x1))¢¢¢ Pj(xjjP arents(xj))

running time of 100 random queries

Table 2 shows the total number of table entries/nodes
required to represent the original network and the
total
(each
consisting of one query variable and one evidence
variable)
for a number of publicly available BNs
(http://www.cs.huji.ac.il/labs/compbio/Repository) and
two noisy-or and noisy-max CPTs with 15 parent nodes.
Note that the intermediate probability tables were
often too large for the tables or ADDs, but not the
AADDs, indicating that the AADD was able to exploit
additive or multiplicative structure in these cases. Also,
the AADD yields an exponential to linear reduction on
the Noisy-Or-15 problem and a considerable computa-
tional speedup on the Noisy-Max-15 problem by exploit-
ing the additive and multiplicative structure inherent in
these special CPTs [7].

6.3 Markov Decision Processes
For MDPs, we simply evaluate the value iteration algo-
rithm [8] using a tabular representation and its exten-
sion for decision diagrams [9] to factored MDPs from
the SysAdmin domain [10].5 Here we simply substitute
tables, ADDs, and AADDs for the reward function R,
value function V , and transition model dynamics Pi in
the following factored MDP value iteration update:
V t+1(s1; : : : ; sn) = R(s1; : : : ; sn) +

(cid:176) max

a Xs0

1:::s0
n

" YP1:::Pn

Pi(s0

ijP arents(s0

i); a)# V t(s0

1; : : : ; s0

n)

Figure 6 shows the relative performance of value itera-
tion until convergence within 0:01 of the optimal value

5In these domains, there is a network of computers whose
operational status (i.e., running or crashed) at each time step
is a stochastic function of the previous status of computers
with incoming connections. At each time step the task of the
system administrator is to choose the computer to reboot in
order to maximize the sum of operational computers over a
discounted reward horizon.

i

)
s
(
 
e
m
T
 
g
n
n
n
u
R

i

Table
ADD
AADD

14000

12000

10000

8000

6000

4000

2000

0

6

7

8

9

# Computers

i

)
s
(
 
e
m
T
 
g
n
n
n
u
R

i

6

5

4

3

2

1

0

10

11

12

i

)
s
(
 
e
m
T
 
g
n
n
n
u
R

i

14000

12000

10000

8000

6000

4000

2000

0

6

Bayes Net

Table

ADD

AADD

# Table Entries Running Time # ADD Nodes Running Time # AADD Nodes Running Time

Alarm
Barley
Carpo
Hailﬂnder
Insurance
Noisy-Or-15
Noisy-Max-15

1,192
470,294
636
9045
2104
65566
131102

2.97 s
EML⁄
0.58
26.4 s
278 s
27.5 s
33.4 s

689
139,856
955
4511
1596
125356
202148

2.42 s
EML⁄
0.57 s
9.6 s
116 s
50.2 s
42.5 s

405
60,809
360
2538
775
1066
40994

1.26 s
207 m
0.49 s
2.7 s
37 s
0.7 s
5.8 s

Table 2: Number of table entries/nodes in the original network and variable elimination running times using tabular, ADD,
and AADD representations for inference in various Bayes nets. ⁄EML denotes that a query exceeded the 1Gb memory limit.

Running Time vs. #Computers for Star Config

x 104Running Time vs. #Computers for Bidirectional Ring Config

Table
ADD
AADD

Running Time vs. #Computers for Independent Rings Config

Table
ADD
AADD

#Nodes/Entries vs. #Computers for Star Config

#Nodes/Entries vs. #Computers for Bidirectional Ring Config

Table
ADD
AADD

5000

4000

3000

2000

1000

s
e
i
r
t

n
E
/
s
e
d
o
N
#

Table
ADD
AADD

5000

4000

3000

2000

1000

s
e
i
r
t

n
E
/
s
e
d
o
N
#

10

8

6

4

2

s
e
i
r
t

n
E
/
s
e
d
o
N
#

Table
ADD
AADD

6

7

8

9

10

11

12

7

8

9

10

11

12

13

14

# Computers

x 104#Nodes/Entries vs. #Computers for Independent Rings Config

# Computers

0

6

7

8

 

9

# Computers

10

11

12

 

0

6

7

8

9

# Computers

10

11

12

 

0

6

7

8

9

10

# Computers

11

12

13

14

Figure 6: MDP value iteration running times (top) and number of entries/nodes (bottom) in the ﬂnal value function using
tabular, ADD, and AADD representations for various network conﬂgurations in the SysAdmin problem.

for networks in a star, bidirectional, and independent
ring conﬂguration. While the reward and transition dy-
namics in the SysAdmin problem have considerable ad-
ditive structure, we note that the exponential size of the
AADD (as for all representations) indicates that little
additive structure survives in the exact value function.
Nonetheless, the AADD-based algorithm still manages
to take considerable advantage of the additive structure
during computations and thus performs comparably or
exponentially better than ADDs and tables.

7 Concluding Remarks
We have presented the AADD and have proved that its
worst-case time and space performance are within a mul-
tiplicative constant of that of ADDs, but can be linear in
the number of variables in cases where ADDs are expo-
nential in the number of variables. And we have provided
an empirical comparison of tabular, ADD, and AADD
representations used in Bayes net and MDP inference
algorithms, concluding that AADDs perform at least as
well as the other two representations, and often yield an
exponential time and space improvement over both.

In conclusion, we should emphasize that we have not
set out to show that variable elimination and value it-
eration with AADDs are the best Bayes net and MDP
inference algorithms available { many other approaches
propose to handle similar structure e–ciently via special-
purpose problem-structure or algorithm modiﬂcations.
Rather, we intended to show that transparently substi-
tuting AADDs in two diverse probabilistic inference al-
gorithms that used tables or ADDs could yield exponen-

tial performance improvements over both by exploiting
context-speciﬂc, additive, and multiplicative structure
in the underlying representation. These results suggest
that the AADD is likely to yield exponential time and
space improvements for a variety of probabilistic infer-
ence algorithms that currently use tables or ADDs, or at
the very least, a gross simpliﬂcation of these algorithms.

References
[1] R.I. Bahar, E.A. Frohm, C.M. Gaona, G.D. Hachtel,
E. Macii, A. Pardo, F. Somenzi: Algebraic Decision
Diagrams and Their Applications. ICCAD. (1993)

[2] Boutilier, C., Friedman, N., Goldszmidt, M., Koller,
Context-speciﬂc independence in Bayesian net-

D.:
works. UAI. (1996)

[3] Drechsler, R., Sieling, D.: BDDs in theory and practice.

Software Tools for Technology Transfer. (2001)

[4] Bryant, R.E.: Graph-based algorithms for Boolean func-
tion manipulation. IEEE Trans. on Computers. (1986)
[5] Rudell, R.: Dynamic variable ordering for ordered bi-

nary decision diagrams. ICCAD. (1993)

[6] Zhang, N.L., Poole, D.: A simple approach to bayesian

network computations. CCAI. (1994)

[7] Takikawa, M., D’Ambrosio, B.: Multiplicative factor-

ization of noisy-max. UAI. (1999)

[8] Bellman, R.E.: Dynamic Programming. Princeton Uni-

versity Press, Princeton, NJ (1957)

[9] Hoey, J., St-Aubin, R., Hu, A., Boutilier, C.: SPUDD:
Stochastic planning using decision diagrams. UAI.
(1999)

[10] Guestrin, C., Koller, D., Parr, R.: Max-norm projec-

tions for factored MDPs. IJCAI. (2001)


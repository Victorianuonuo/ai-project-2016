A Fusion of Stacking with Dynamic Integration 

Niall Rooney, David Patterson 

Northern Ireland Knowledge Engineering Laboratory 

Faculty of Engineering, University of Ulster 

Jordanstown, Newtownabbey, BT37 OQB, U.K. 

{nf.rooney, wd.patterson}@ulster.ac.uk 

Abstract 

In this paper we present a novel method that 
fuses the ensemble meta-techniques of Stack-
ing and Dynamic Integration (DI) for regres-
sion  problems,  without  adding  any  major 
computational overhead. The intention of the 
technique  is  to  benefit  from  the  varying  per-
formance  of  Stacking  and  DI  for  different 
data  sets,  in  order  to  provide  a  more  robust 
technique. We detail an empirical analysis of 
the technique referred to as weighted Meta –
Combiner (wMetaComb) and compare its per-
formance to Stacking and the DI technique of 
Dynamic Weighting  with Selection. The em-
pirical  analysis  consisted  of  four  sets  of  ex-
periments  where  each  experiment  recorded 
the cross-fold evaluation of each technique for 
a  large  number  of  diverse  data  sets,  where 
each base model is created using random fea-
ture  selection  and  the  same  base  learning  al-
gorithm. Each experiment differed in terms of 
the  latter  base  learning  algorithm  used.  We 
demonstrate that for each evaluation, wMeta-
Comb  was able to outperform DI and  Stack-
ing for each experiment and as such fuses the 
two underlying mechanisms successfully. 

 

1  Introduction 
The objective of ensemble learning is to integrate a number 
of base learning models in an ensemble so that the generali-
zation performance of the ensemble is better than any of the 
individual base learning models. If the base learning models 
are created using the same learning algorithm, the ensemble 
learning schema is homogeneous otherwise it is heterogene-
ous. Clearly in the former case, it is important that the base 
learning  models  are  not  identical,  and  theoretically  it  has 
been shown that for such an ensemble to be effective, it is 

ˆ
f
N

x y
,
=<

=<
MI

important that the base learning models are sufficiently ac-
curate and diverse in their predictions [Krogh & Vedelsby, 
1995],  where  diversity  in  terms  of  regression  is  usually 
measured by the ambiguity or variance in their predictions. 
To achieve this in terms of homogeneous learning, methods 
exist  that  either  manipulate,  for  each  model,  the  training 
data  through  instance  or  feature  sampling  methods  or  ma-
nipulate  the  learning  parameters  of  the  learning  algorithm. 
Brown et al.[2004] provide a recent survey of such ensem-
ble generation methods. 
Having  generated  an  appropriate  set  of  diverse  and  suffi-
ciently accurate models, an ensemble integration method is 
required to create a successful ensemble. One  well–known 
approach to ensemble integration is to combine the models 
using  a  learning  model  itself.  This  process,  referred  to  as 
Stacking [Wolpert, 1992], forms a meta-model created from 
a meta-data set. The meta-data set is formed from the pre-
dictions of the base models and the output variable of each 
training  instance.  As  a  consequence,  for  each  training  in-
>   belonging  to  training  set  D ,  meta-
stance 
I
ˆ
x y
x is 
f x
( ),
( ),..
instance 
1
where N is the size of 
i i
the prediction of base model  ,
the  ensemble.  To  ensure  that  the  predictions  are  unbiased, 
the  training  data  is  divided  by  a  J -fold  cross-validation 
process into  J subsets, so that for each iteration of a cross-
validation process, one of the subsets is removed from the 
training data and the models are re-trained on the remaining 
data. Predictions are then made by each base model on the 
held out sub-set.  The meta-model is trained on the accumu-
lated meta-data after the completion of the cross validation 
process and each of the base models is trained on the whole 
training data. Breiman[1996b] showed the successful appli-
cation of this method for regression problems using a linear 
regression  method  for  the  meta-model  where  the  coeffi-
cients of the linear regression model were constrained to be 
non-negative.  However  there  is  no  specific  requirement  to 
use linear regression as the meta-model [Hastie et al., 2001] 
and  other  linear  algorithms  have  been  used  particularly  in 
the  area  of  classification  [Seewald,  2002,  Džeroski,  & 
Ženko, 2004]. 
A technique which appears at first very similar in scope to 
Stacking (and can be seen as a variant thereof) is that pro-
posed by Tsymbal et al.[2003] referred to as Dynamic Inte-
gration.  This  technique  also  runs  a  J-fold  cross  validation 

> is formed, where  ˆ ( )
N=
1..

if

IJCAI-07

2844

2  Methodology 
In this section, we describe in detail the process of wMeta-
Comb. The training phase of wMetaComb is shown in Fig-
ure 1..  
Algorithm : wMetaComb: Training Phase
Input: D
Output:

ˆ SRf

N

,

ˆ
f =
i i
, 1..

, ˆ DI
f
(* step 1 *) 
partition  D  into J–fold partitions 
SRMD = ∅
initialise meta-data set 
DIMD = ∅
initialise meta-data set 
For i = 1..J-2 

JD =
i
1..

\

=
D D
i
=
D
i

D
train
D
test
build N learning models 
For each instance 
, y
x

<

ˆ
f =
i i
, 1..
>
in
Form meta-instance 
Form meta-instance 
SRMD
Add 
DIMD
Add 

SRMI
DIMI

to
to 

 
 
 
 
EndFor

N

 using 
testD
SRMI
DIMI

<
<

:
:

trainD

ˆ
f
1
x
,

ˆ
x
x
y
f
( ),
( ),..,
N
Err
Err
x
( ),..,
N
1

>

x
( ),

y

>

( )

,x y

( ),..,

x Err x
,

of 
x y
( ),

the 
>   where 

process for each base model, however the process by which 
it  creates  and  uses  meta-data  is  different.  Each  meta-
instance 
format 
consists 
following 
<
Err
iErr x  is the predic-
N
1
tion  error  made  by  each  base  model  for  each  training  in-
> .  Rather  than  forming  a  meta-model  based 
<
stance 
on  the  derived  meta-data,  Dynamic  Integration  uses  a  k-
nearest  neighbour  (k-NN)  lazy  model  to  find  the  nearest 
neighbours  based  on  the  original  instance  feature  space 
(which  is  a  subspace  in  the  meta-data)  for  a  given  test  in-
stance. It then determines the total training error of the base 
models  recorded  for  this  meta-nearest  neighbour  set.  The 
level of error is used to allow the dynamic selection of one 
of the base models or an appropriate weighted combination 
of all or a select number of base models to make a predic-
tion  for  the  given  test  instance.  Tsymbal  et  al.  [2003]  de-
scribes three Dynamic Integration techniques for classifica-
tion  which  have  been  adapted  for  regression  problems 
[Rooney et al., 2004]. In this paper we consider one variant 
of these techniques referred to as Dynamic Weighting with 
Selection (DWS) which applies a weighting combination to 
the  base  models  based  on  their  localized  error,  to  make  a 
prediction,  but  excludes  first  those  base  models  that  are 
deemed too inaccurate to be added to the weighted combina-
tion. 
A method of injecting diversity into a homogeneous ensem-
ble is to use random subspacing [Ho, 1998a, 1998b]. Ran-
dom subspacing is based on the process whereby each base 
model is built with data with randomly subspaced features. 
Tsymbal et al. [2003] revised this mechanism so that a vari-
able length of features are randomly subspaced. They shown 
this method to be effective in creating ensembles that were 
integrated using DI methods for classification. 
It  has  been  shown  empirically  that  Stacking  and  Dynamic 
Integration  based  on  integrating  models,  generated  using 
random  subspacing,  are  sufficiently  different  from  each 
other in that they can give varying generalization perform-
ance on the same data sets [Rooney et al., 2004]. The main 
computational  requirement  in  both  Stacking  and  Dynamic 
Integration, is the use of cross-validation of the base models 
to form meta-data. It requires very little additional computa-
tional  overhead  to  propose  an  ensemble  meta-learner  that 
does both in one cross validation of the ensemble base mod-
els and as a consequence, is able to create both a Stacking 
meta-model  and  a  Dynamic  Integration  meta-model,  with 
the required knowledge for the ensemble meta-learner to use 
both models to form predictions for test instances. The focus 
of this paper is on the formulation and evaluation of such an 
ensemble  meta-learner,  referred  to  as  weighted  Meta-
Combiner (wMetaComb). We investigated whether wMeta-
Comb will on average outperform both Stacking for regres-
sion (SR) and DWS for a range of data sets and a range of 
base  learning  algorithms  used  to  generate  homogeneous 
randomly subspaced models. 

 

 
 
 
 

 
 
 
 

using the meta-data set 

using the meta-data set 

SRMD
DIMD

TotalErr using 

DI

testD

 using 

trainD

ˆ
f =
i i
, 1..
>
in
SRMI
DIMI

N
testD
<
:
<
:

ˆ
f
1
x
,

ˆ
f
y
x
x
( ),..,
( ),
N
Err
Err
x
( ),..,
N
1

>

x
( ),

y

>

 
Endfor
Build SR meta-model  ˆ SRf
Build  DI meta-model  ˆ DI
f
(* step 2 *) 
i=J 
D=
D
i
TotalErr and 
Determine 
(* step 3 *) 
i = J-1 
=
D D
D
i
train
=
D
D
i
test

test

SR

\

<

 build N learning models 
For each instance 
, y
x
Form meta-instance 
Form meta-instance 
SRMD
Add 
DIMD
Add 
EndFor
 on  D
Retrain the 
Build SR meta-model  ˆ SRf
Build DI meta-model  ˆ DI

SRMI
DIMI

ˆ
f =
i i
, 1..

to
to 

f

N

using the meta-data set 
using the meta-data set 

SRMD
DIMD

Figure 1. Training Phase of wMetaComb 

The first step in the training phase of wMetaComb consists 
of  building  SR  and  DI  meta-models  on  J-1  folds  of  the 
training data (step 1). The second step involves testing the 
meta-models using the remaining training fold, as an indica-
tor of their potential  generalization performance.   It is im-
portant  to  do  this  as  even  though  models  are  not  created 

IJCAI-07

2845

DI

SR

using the complete meta-data, they are tested using data that 
they are not trained on, in order to give a reliable estimate of 
their test performance. The performance of the meta-models 
TotalError and 
is  recorded  by  their  total  absolute  error, 
TotalError within  step  2.  The  meta-models  are  then  re-
built  using  the  entire  meta-data  completed  by  step  3.  The 
computational  overhead  of  wMetaComb  in  comparison  to 
SR  by  itself  centres  only  on  the  cost  of  training  the  addi-
tional meta-model. However as the additional meta-model is 
based on a lazy nearest neighbour learner, this cost is trivial. 
The prediction  made by  wMetaComb is  made by a simple 
weighted  combination  of  its  meta-models  based  on  their 
total  error  performance  determined  in  step  2.  Denote 
> .  The  individual 
TotalErr
weight  for  each  meta-model  is  determined  by  finding  the 
normalized error for each meta-model: 
 
normerror
i

TotalError
i

TotalError
i

TotalError

TotalError

=<

,SR

=

  

DI

/

 

=
1..2

i

A normalized weighting  for each meta-model is given by: 
 

normerror
i

T

 

−=
imw e
 

 

norm mw
i

_

=

mw
i
mw
i

 

i

(1) 

(2) 

 
Equation  1  is  influenced  by  a  weighting  mechanism  that 
Wolpert  &  Macready  [1996]  proposed  for  combining  base 
models  generated  by  stochastic  process  such  as  used  in 
Bagging  [Breiman,1996a]. 
The  weighting  parameter  T determines  how  much  relative 
influence to give to each meta-model based on their normal-
e.g. 
ized 
error 
and 
normError =
,  then  Table  1  gives  the  normalized 
weight values for different values of  T . This indicates that a 
low value of  T  will give large imbalance to the weighting, 
whereas a high value of  T  almost equally weights the meta-
learners regardless of their difference in normalized errors. 
 

normError =

suppose 

0.4

0.6

1

2

_

T 
norm mw  
1
0.88 
0.1 
0.55 
1.0 
10.0  0.505 

_

norm mw  
2
0.12 
0.45 
0.495 

 
Table 1. The effect of the weighting parameter 
 
Based  on  these  considerations,  we  applied  the  following 
heuristic function for the setting of  T , which increases the 
imbalance  in  the  weighting  dependent  on  how  great  the 
normalized errors differ. 

|

=

T

−
normError
1
0.01

normError
2

|

 

(3) 

 
wMetaComb forms a prediction for a test instance  x by the 
following combination of the predictions made by the meta-
models: 
 
 

ˆ
norm mw f
*

_

SR

x
( )

1

+

ˆ
norm mw f
.

_

DI

x
( )

2

 

3  Experimental Evaluation and Analysis 
In  these  experiments,  we  investigated  the  performance  of 
wMetaComb in comparison to SR and DWS. The work was 
carried  out  based  on  appropriate  extensions  to  the  WEKA 
environment, such as the implementation of DWS. The en-
semble  sets  consisted  of  25  base  homogeneous  models 
where  the  data  for  each  base  model  was  randomly  sub-
spaced [Tsymbal et al., 2003]. Note that random subspacing 
was a pseudo-random process so that a feature sub-set gen-
erated for each model i in an ensemble set is the same for all 
ensemble  sets  created  using  different  base  learning  algo-
rithms and the same data set. The meta-learner for SR and 
the Stacking component within wMetaComb was based on 
the model tree M5 [Quinlan, 1992], as this has a larger hy-
pothesis space than Linear Regression. The number of near-
est neighbours for DWS k-NN  meta-learner  was set to 15. 
The  performance  of  DWS  can  be  affected  by  the  size  of 
k for  particular  data  sets,  but  for  this  particular  choice  of 
data  sets,  previous  empirical  analysis  had  shown  that  the 
value  of  15,  gave  overall  a  relatively  strong  performance. 
The  value  of  J  during  the  training  phase  of  each  meta-
technique was set to 10. 
In order to evaluate the performance of the different ensem-
ble  techniques,  we  chose  30  data  sets  available  from  the 
WEKA  environment  [Witten  &  Frank,  1999].  These  data 
sets  represent  a  number  of  synthetic  and  real-world  prob-
lems  from  a  variety  of  different  domains,  have  a  varying 
range in their attribute sizes and many contain a mixture of 
both discrete and continuous attributes. Data sets which had 
more than 500 instances, were sampled without replacement 
to reduce the size of the data set to a maximum of 500 in-
stances,  in  order  to  make  the  evaluation  computationally 
tractable. Any missing values in the data set were replaced 
using a mean or modal technique. The characteristics of data 
sets are shown in Table 2. 
We  carried  out  a  set  of  four  experiments,  where  each  ex-
periment differed purely in the base learning algorithm de-
ployed  to  generate  base  models  in  the  ensemble.  Each  ex-
periment  calculated  the  relative  root  mean  squared  error 
(RRMSE) [Setiono et al., 2002] of each ensemble technique 
for each data set based on a 10 fold cross validation meas-
ure. 
 

IJCAI-07

2846

Data set 

Original  
Data set 
size 

Data set 
size in  
experi-
ments 

abalone 
autohorse 
autoMpg 
autoprice 
auto93 
bank8FM 
bank32nh 
bodyfat 
breastTumor 
cal_housing 
cloud 
cpu 
cpu_act 
cpu_small 
echomonths 
elevators 
fishcatch 
friedman1 
housing 
house_8L 
house_16H 
lowbwt 
ma-
chine_cpu 
pollution 
pyrim 
sensory 
servo 
sleep 
strike 
veteran 

4177  
205 
398 
159 
93 
8192 
8192 
252 
286 
20640 
108 
209 
8192 
8192  
130 
16559 
158 
40768  
506 
22784 
22784  
189 
209 

60 
74 
576  
167 
62 
625 
137 

500 
205 
398 
159 
93 
500 
500 
252 
286 
500 
108 
209 
500 
500 
130 
500 
158 
500 
500 
500 
500 
189 
209 

60 
74 
500 
167 
62 
500 
137 

Number of  
input 
Attributes 
Con-
tinuous 
8 
15 
5 
15 
16 
8 
32 
14 
1 
8 
4 
6 
21 
12 
6 
18 
5 
10 
13 
8 
16 
7 
6 

Dis-
crete 
0 
10 
3 
10 
5 
0 
0 
0 
8 
0 
2 
1 
0 
0 
3 
0 
2 
0 
0 
0 
0 
2 
0 

15 
27 
0 
4 
7 
5 
7 

0 
0 
11 
0 
0 
1 
0 

Total 

8 
25 
8 
25 
21 
8 
32 
14 
9 
8 
6 
7 
21 
12 
9 
18 
7 
10 
13 
8 
16 
9 
6 

15 
27 
11 
4 
7 
6 
7 

Table 2. The data sets’ characteristics 

The RRMSE is a percentage measure. By way of compari-
son  of  the  ensemble  approaches  in  general,  we  also  deter-
mined  the  RRMSE  for  a  single  model  built  on  the  whole 
unsampled data, using the same learning algorithm as used 
in  the  ensemble  base  models.  We  repeated  this  evaluation 
four times, each time using a different learning algorithm to 
generate  base  models.  The  choice  of  learning  algorithms 
was  based  on  a  choice  of  simple  and  efficient  methods  in 
order to make the study computationally tractable, but also 
to have representative range of methods with different learn-
ing  hypothesis  spaces  in  order  to  determine  whether 
wMetaComb was a robust method independent of the base 
learning algorithm deployed.  The base learning algorithms 
used for the four experiments were 5 nearest neighbours (5-
NN), Linear Regression (LR), Locally  weighted regression 
[Atkeson et al., 1997] and the model tree learner M5. 
The results of the evaluation were assessed by performing a 
2 tailed paired t test (p=0.05) on the cross fold RRMSE for 
each  technique  in  comparison  to  each  other  for  each  data 
set. The results of the comparison between one technique A 
and  another  B  were 
the  form 
wins:losses:ties (ASD)  where wins is a count of the number 

then  summarised 

in 

+

losses

of  data  sets  where  technique  A  outperformed  technique  B 
significantly,  losses  is  a  count  of  the  number  of  data  sets 
where technique B outperformed technique  A  significantly 
and ties a count where there was no significant difference. 
The  significance  gain  is  the  difference  in  the  number  of 
wins and the number of losses. If this zero or less, no gain 
was  shown.  If  the  gain  is  less  than  3  this  is  indicative  of 
only a slight improvement. 
We  determined  the  average  significance  difference  (ASD)
between the RRMSE of the two techniques, averaged over 
wins
data  sets  only  where  a  significant  difference 
was shown. If technique A outperformed B as shown by the 
significance ratio, the ASD gave us a percentage measure of 
the degree by which A outperformed B (if technique A was 
outperformed  by  B,  then  the  ASD  value  is  negative).  The 
ASD by itself of course has no direct indication of the per-
formance  of  the  techniques  and  is  only  a  supplementary 
measure.  A  large  ASD  does  not  indicate  that  technique  A 
performed considerably better than B if the difference in the 
number of wins to losses was small. Conversely even if the 
number of wins was much larger than the number of losses 
but the ASD was small, this reduces the impact of the result 
considerably. This section is divided into a discussion of the 
summary of results  for each  individual base learning algo-
rithm then a discussion of the overall results.  
3.1 Base Learner: k-NN 

Table 3 shows a summary of the results for the evaluation 
when the base learning algorithm was 5-NN. Clearly all the 
ensemble  techniques  strongly  outperformed  single  5-NN 
whereby in terms of significance alone, wMetaComb shown 
the  largest  improvement  with  a  gain  of  22.  Although  SR 
showed fewer data sets with significant improvement, it had 
a  greater  ASD  than  wMetaComb.  wMetaComb  outper-
formed  DWS  with  a  gain  of  7,  and  showed  a  small  im-
provement over SR, with a gain of 3. SR showed a signifi-
cance gain of only 1 over DWS. 

Technique 
5-NN 

SR

DWS 

wMetaComb 

5-NN 

 

11:0:19 
 (16.07) 
20:0:10 
 (12.28) 
22:0:8  
(14.47) 

SR 
0:11:19  
(-16.07) 

 

2:3:25 
 (-3.82) 
3:0:27  
(6.8) 

DWS 
0:20:10  
(-12.28)  
3:2:25 
 (3.82) 

 

8:1:21 
 (5.28) 

wMetaComb 
0:22:8 
 (-14.47) 
0:3:27  
(-6.8) 
1:8:21  
(-5.28) 

Table  3.  Comparison  of  methods  when  base  learning  algorithm 
was 5-NN 

IJCAI-07

2847

3.2 Base Learner: LR
Table 4 shows that all ensembles techniques outperformed 
single  LR,  with  wMetaComb  showing 
largest 
improvement  with  a  gain  of  13  and  an  ASD  of  11.99. 
wMetaComb  outperformed  SR  and  DWS  with  a  larger 
improvement shown in comparison to DWS than to SR both 
in  terms  of  significance  and  ASD.  SR  outperformed  DWS 
with a significance gain of 7 and ASD of 5.1. 

the 

3.4 Base Learner: M5 

Technique 
M5 

M5 
 

SR

DWS 

wMetaComb 

1:3:26 
 (-4.4) 
6:1:23 
 (3.45) 
8:0:22 
 (4.51) 

SR 
3:1:26 
 (4.4) 

 

6:2:22 
 (2.62) 
11:0:19 
 (4.48) 

DWS 
1:6:23  
(-3.45) 
2:6:22 
 (-2.62) 

 

6:0:24 
 (3.53) 

wMetaComb 
0:8:22  
(-4.51) 
0:11:19 
 (-4.48) 
0:6:24  
(-3.53) 

Technique 
LR 

LR 
 

SR

DWS 

wMetaComb 

10:2:18  
(11.38) 
11:1:18  
(5.56) 
13:0:17  
(11.99) 

SR 
2:10:18 
(-11.38) 

 

4:11:15  
(-5.1) 
5:0:25  
(5.58) 

DWS 
1:11:18  
(-5.56) 
11:4:15  
(5.1) 
 

13:3:14  
(6.02) 

wMetaComb 
0:13:17 
 (-11.99) 
0:5:25  
(-5.58) 
3:13:14  
(-6.02) 

Table  4.  Comparison  of  methods  when  base  learning  algorithm 
was LR 

3.3 Base Learner: LWR 
Table 5 shows that all ensembles techniques outperformed 
single  LWR,  with  wMetaComb  showing 
largest 
improvement  with  a  significance  gain  of  16.  wMetaComb 
outperformed  SR  and  DWS  with  a  larger  improvement 
shown  over  DWS  than  SR  in  terms  of  significance  and 
ASD.  SR outperformed DWS with a significance gain of 4 
and  an ASD of 6.24. 

the 

Technique 
LWR 

SR

DWS 

wMetaComb 

LWR 

 

9:1:20 
 (13.81) 
16:0:14 
 (12.28) 
16:0:14 
 (15.91) 

SR 
1:9:20 
(-13.81) 

 

2:6:22  
(-6.24) 
5:0:25  
(10.48) 

DWS 
0:16:14 
(-12.28) 
6:2:22  
(6.24) 

 

9:0:21 
 (8.28) 

wMetaComb 
0:16:14 
(-12.28) 
0:5:25 
(-10.48) 
0:9:21 
 (-8.28) 

Table  5.  Comparison  of  methods  when  base  learning  algorithm 
was LWR 
Table 5 shows that all ensembles techniques outperformed 
single  LWR,  with  wMetaComb  showing 
largest 
improvement  with  a  significance  gain  of  16.  wMetaComb 
outperformed  SR  and  DWS  with  a  larger  improvement 
shown  over  DWS  than  SR  in  terms  of  significance  and 
ASD.  SR outperformed DWS with a significance gain of 4 
and ASD of 6.24. 

the 

Table  6.  Comparison  of  methods  when  base  learning  algorithm 
was M5 
Table 6 shows a much reduced performance for the ensem-
bles in comparison to the single model, when the base learn-
ing algorithm was M5. In fact, SR showed no improvement 
over  M5,  and  wMetaComb  and  DWS  although  they  im-
proved on M5 in terms of significance did not show an ASD 
as large as for the previous learning algorithms. The reduced 
performance with M5 is indicative that in the case of certain 
learning algorithms, random subspacing by itself is not able 
to generate sufficiently accurate base models, and needs to 
be enhanced using search approaches that improve the qual-
ity of ensemble members such as considered in [Tsymbal et 
al.,  2005].    wMetaComb  showed  improvements  over  SR 
and to a lesser degree both in terms of significance and ASD 
values. 
Overall, it can be seen that wMetaComb gave the strongest 
performance of the ensembles for  3 out of 4 base learning 
algorithms  (5-NN,  LR,  M5)  in  comparison  to  the  single 
model, in terms of its significance gain, and tied in compari-
son  to  DWS  for  LWR.  For  3  out  of  the  4  learning  algo-
rithms  (LR,  LWR,  M5),  wMetaComb  also  had  the  highest 
ASD  compared  to  the  single  model.  wMetaComb  outper-
formed  SR  for  all  learning  algorithms.  The  largest  signifi-
cant  improvement  over  SR  was  recorded  with  M5  with  a 
significance gain of 11, and the largest ASD in comparison 
to SR was recorded with LWR of 10.48. The lowest signifi-
cant  improvement  over  SR  was  recorded  with  5-NN,  LR, 
and LWR with a significant gain of 5, and lowest ASD of 
4.48 with M5. 
wMetaComb outperformed DWS for all four base learning 
algorithms. The degree of improvement was larger in com-
parison  to  DWS  than  SR.  The  largest  significant  improve-
ment  over  DWS  was  shown  with  LR  with  a  significance 
gain of 10 and the largest ASD of 8.28 with LWR. The low-
est improvement was shown with M5 both in terms of gain 
and  ASD  which  were  6  and  3.53  respectively.  In  effect, 
wMetaComb  was  able  to  fuse  effectively  the  overlapping 
expertise  of  the  two  meta-techniques.  This  was  seen  to  be 
case  independent  of  the  learning  algorithm  employed.  In 
addition, wMetaComb provided benefit whether overall for 
a given experiment, SR was stronger than DWS or not (as 

IJCAI-07

2848

ference  on  Artificial  Intelligence,  pp.  343-348,  World 
Scientific, 1992. 

[Rooney et al., 2004] Rooney, N., Patterson, D., Anand S, 
& Tsymbal, A. Dynamic Integration of Regression Mod-
els.  In  Proceedings  of  the  5th  International  Multiple 
Classifier Systems Workshop. LNCS Vol. 3077, pp. 164-
173, Springer, 2004. 

[Seewald, 2002] Seewald, A. Exploring the Parameter State 
Space of Stacking. In  Proceedings of the 2002 IEEE In-
ternational  Conference  on  Data  Mining,  pp.  685-688, 
2002.

[Setiono et al., 2002] Setiono, R., Leow, W.K., & Zurada, J.  
Extraction of Rules from Artificial Neural Networks for 
Nonlinear  Regression.  IEEE  Transactions  on  Neural 
Networks, 13(3):564-577, 2002. 

[Tsymbal et al., 2003] Tsymbal, A., Puuronen, S., Patterson, 
D. Ensemble feature selection with the simple Bayesian 
classification,  Information  Fusion,  4:87-100,  Elsevier, 
2003.

[Tsymbal et al., 2005] Tysmbal, A. Pechenizkiy, M., Cun-
ningham,  P.  Sequential  Genetic  Search  for  Ensemble 
Feature Selection. In Proc. of the 19th International Joint 
Conference on Artificial Intelligence, pp. 877-882, 2005. 
[Witten & Frank, 1999] Witten, I. and  Frank, E. Data Min-
ing: Practical Machine Learning Tools and Techniques 
with Java Implementations, Morgan Kaufmann, 1999. 

[Wolpert, 1992] Wolpert, D. Stacked Generalization. Neural 

Networks 5, pp. 241-259, 1992. 

[Wolpert & Macready, 1996] Wolpert, D. & Macready, W. 
Combining Stacking with Bagging to Improve a Learn-
ing  Algorithm,  Santa  Fe  Institute  Technical  Report  96-
03-123, 1996. 

was the case with LR and LWR, but not M5), indicating the 
general robustness of the technique. 

4  Conclusions 
We  have  presented  a  technique  called  wMetaComb  that 
merges  the  technique  of  Stacking  for  regression  and  Dy-
namic  integration  for  regression,  and  showed  that  it  was 
successfully  able  to  benefit  from  the  individual  meta-
techniques’ often complementary expertise for different data 
sets. This was demonstrated based on 4 sets of experiments 
each  using  different  base  learning  algorithms  to  generate 
homogeneous  ensemble  sets.  It  was  shown  that  for  each 
experiment,  wMetaComb  outperformed  Stacking  and  the 
Dynamic Integration technique of Dynamic Weighting with 
Selection. In future, we intend to generalize the technique so 
as  to  allow  the  combination  of  more  than  two  meta-
ensemble  techniques.    In  this,  we  may  consider  combina-
tions  of  Stacking  using  different  learning  algorithms  or 
other Dynamic integration combination methods.

References 
[Atkeson  et  al.,  1997]  Atkeson,  C.  G.,  Moore,  A.  W.,  and 
Schaal,  S..  Locally  weighted  learning.  Artificial  Intelli-
gence Review, 11:11-73, Kluwer, 1997. 

[Breiman,  L.,  1996a].  Bagging  Predictors.  Machine  learn-

ing, 24:123-174, Kluwer, 1996.  

[Breiman, L., 1996b] Stacked Regressions. Machine Learn-

ing, 24:49-64, Kluwer, 1996. 

[Brown et al., 2004] Brown,  G., Wyatt, J., Harris, R. amd 
Yao,  X.  Diversity  Creation  Methods:  A  Survey  and 
Information  Fusion  6(1):5-20,  El-
Categorisation. 
sevier,2004. 

[Džeroski,  &  Ženko,  2004]  Džeroski,  S.  &  Ženko,  B.  Is 
Combining Classifiers with Stacking Better than Select-
ing  the  Best  One?  Machine  Learning  54(3):255-273, 
Kluwer. 2004. 

[Hastie et al., 2001] Hastie, T., Tibshirani, R., Friedman, J. 
The Elements of Statistical Learning:Data mining, Infer-
ence and Prediction. Springer series in Statistics , 2001. 
[Ho,  1998a]  Ho,  T.K.  The  random  subspace  method  for 
constructing decision forests. IEEE Transactions on Pat-
tern  Analysis  and  Machine  Intelligence,  20(8):832-844, 
1998.

[Ho,  1998b]  Ho, T.K.  Nearest  Neighbors  in  Random  Sub-
spaces. In Proceedings of the Joint IAPR Workshops on 
Advances in Pattern Recognition, LNCS, Vol. 1451, pp. 
640-648, Springer-Verlag, 1998. 

[Krogh & Vedelsby, 1995] Krogh, A. and Vedelsby, J. Neu-
ral  Networks  Ensembles,  Cross  validation,  and  Active 
Learning.  In Advances in Neural Information Process-
ing Systems , pp. 231-238, MIT Press, 1995. 

[Quinlan,  1992]  Quinlan,  R.  Learning  with  continuous 
classes, In Proceedings of the 5th Australian Joint Con-

IJCAI-07

2849


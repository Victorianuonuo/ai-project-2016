An Analysis of Laplacian Methods for Value Function Approximation in MDPs

Marek Petrik

Department of Computer Science

University of Massachusetts

Amherst, MA 01003
petrik@cs.umass.edu

Abstract

Recently, a method based on Laplacian eigenfunc-
tions was proposed to automatically construct a ba-
sis for value function approximation in MDPs. We
show that its success may be explained by drawing
a connection between the spectrum of the Lapla-
cian and the value function of the MDP. This expla-
nation helps us to identify more precisely the con-
ditions that this method requires to achieve good
performance. Based on this, we propose a modi-
ﬁcation of the Laplacian method for which we de-
rive an analytical bound on the approximation er-
ror. Further, we show that the method is related
the augmented Krylov methods, commonly used to
solve sparse linear systems. Finally, we empiri-
cally demonstrate that in basis construction the aug-
mented Krylov methods may signiﬁcantly outper-
form the Laplacian methods in terms of both speed
and quality.

Introduction

1
Markov Decision Processes (MDP) [Puterman, 2005] are a
widely-used framework for planning under uncertainty.
In
this paper, we focus on the discounted inﬁnite horizon prob-
lem with discount γ such that 0 < γ < 1. We also assume
ﬁnite state and action spaces. While solving this problem re-
quires only polynomial time, many practical problems are too
large to be solved precisely. This motivated the development
of approximate methods for solving very large MDPs that are
sparse and structured.

Value Function Approximation (VFA) is a method for ﬁnd-
ing approximate solutions for MDPs, which has received a
lot of attention [Bertsekas and Tsitsiklis, 1996]. Linear ap-
proximation is a popular VFA method, because it is simple to
analyze and use. The representation of the value function in
linear schemes is a linear combination of basis vectors. The
optimal policy is usually calculated using approximate value
iteration or approximate linear programming [Bertsekas and
Tsitsiklis, 1996].

The choice of the basis plays an important role in solving
the problem. Usually, the basis used to represent the space
is hand-crafted using human insight about some topologi-
cal properties of the problem [Sutton and Barto, 1998]. Re-

(cid:2)v∗ − ˆv(cid:2)μ ≤

lim sup
k→∞

2γ

(1 − γ)2

sup
k

(cid:2)vk − ˜vk(cid:2)μk

,

cently, a new framework for automatically constructing the
basis was proposed in [Mahadevan, 2005]. This approach
is based on analysis of the neighborhood relation among the
states. The analysis is inspired by spectral methods, often
used in machine-learning tasks. We describe this framework
in more detail in Section 2.

In the following, we use v to denote the value function,
and r to denote the reward vector. The matrix I denotes an
identity matrix of an appropriate size. We use n to denote the
number of states of the MDP.

An important property of many VFA methods is that they
are guaranteed to converge to an approximately optimal so-
lution. In methods based on approximate policy iteration the
maximal distance of the optimal approximate value function
ˆv from the optimal value function v∗ is bounded by [Munos,
2003]:

where vk and ˜vk are true and approximated value at step k
given the current policy. The norm (cid:2) · (cid:2)μ denotes a weighted
quadratic norm with distribution μ. The distribution μ is ar-
bitrary, and μk depends on μ and the current transition ma-
trix [Munos, 2003]. Similar bounds hold for algorithms based
on Bellman residual minimization, when (cid:2)vk − ˜vk(cid:2) is re-
) ˜vk(cid:2), where Pπk is a transition
placed by (cid:2)r − (I − γPπk
matrix of the current policy. In addition, these bounds hold
also for the max norm [Bertsekas and Tsitsiklis, 1996]. In the
following, we refer to the value function vk and its approx-
imation ˜vk as v and ˜v respectively, without using the index
k.
The main focus of the paper is the construction a good basis
for algorithms that minimize (cid:2)v − ˜v(cid:2)2 in each iteration. We
focus only on the quadratic approximation bound, because
the other bounds are related. Notice that (cid:2) · (cid:2)∞ ≤ (cid:2) · (cid:2)2.

The paper is organized as follows. Section 2 describes
the spectral methods for VFA in greater detail. The main
contribution of the paper is an explanation of the good per-
formance of spectral methods for VFA and its connection to
methods for solving sparse linear systems. This is described
in Section 3, where we also propose two new alternative al-
gorithms. In Section 4, we show theoretical error bounds of
one of the methods, and then in Section 6 we demonstrate
that our method may signiﬁcantly outperform the previously
proposed spectral methods.

IJCAI-07

2574

(cid:2)

2 Proto-Value Functions
In this section, we describe in greater detail the proto-value
function methods proposed in [Mahadevan, 2005]. These
methods use the spectral graph framework to construct a ba-
sis for linear VFA that respects the topology of the problem.
The states of the MDP for a ﬁxed policy represent nodes
of an undirected weighted graph (N, E). This graph may
be represented by a symmetric adjacency matrix W , where
Wxy represents the weight between the nodes. A diagonal
matrix of the node degrees is denoted by D and deﬁned as
Dxx =
y∈N Wxy. There are several deﬁnitions of the
graph Laplacian, with the most common being the normal-
ized Laplacian, deﬁned for W as L = I − D− 1
2 . The
advantage of the normalized Laplacian is that it is symmet-
ric, and thus its eigenvectors are orthogonal. Its use is closely
related to the random walk Laplacian Lr = I − D−1W and
the combinatorial Laplacian Lc = D − W , which are com-
monly used to motivate the use of the Laplacian by making a
connection to random walks on graphs [Chung, 1997].

A function f on a graph (N, E) is a mapping from each
vertex to R. One well-deﬁned measure of smoothness of a
function on a graph is its Sobolev norm. The bottom eigen-
vectors of Lc may be seen as a good approximation to smooth
functions, that is, functions with low Sobolev norm [Chung,
1997].

2 W D− 1

The application of the spectral framework to VFA is
straightforward. The value function may be seen as a func-
tion on the graph of nodes that correspond to states. The edge
weight between two nodes is determined by the probability of
transiting either way between the corresponding states, given
a ﬁxed policy. Usually, the weight is 1 if the transition is pos-
sible either way and 0 otherwise. Notice that these weights
must be symmetric, unlike the transition probabilities. If we
assume the value function is smooth on the induced graph, the
spectral graph framework should lead to good results. While
the adjacency matrix with edge weights of 1 was usually used
before, [Mahadevan, 2005] also discusses other schemes.

While the approach is interesting, it suffers from some
problems.
In our opinion, the good performance of this
method has not been sufﬁciently well explained, because the
construction of the adjacency matrix is not well motivated.
In addition, the construction of the adjacency matrix makes
the method very hard to analyze. As we show later, using the
actual transition matrix instead of the adjacency matrix leads
to a better motivated algorithm, which is easy to derive and
analyze.

The requirement of the value function being smooth was
partially resolved by using diffusion wavelets in [Mahade-
van and Maggioni, 2005; Maggioni and Mahadevan, 2006].
Brieﬂy, diffusion wavelets construct a low-order approxima-
tion of the inverse of a matrix. An disadvantage of using the
wavelets is the high computational overhead needed to con-
struct the inverse approximation. The advantage is, however,
that once the inverse is constructed it may be reused for dif-
ferent rewards as long as the transition matrix is ﬁxed. Thus,
we do not compare our approach to diffusion wavelets due to
the different goals and computational complexities of the two
methods.

3 Analysis
In this section we show that if we use the actual transition
matrix instead of the random walk Laplacian of the adjacency
matrix, the method may be well justiﬁed and analyzed. We
assume in the following that the transition matrix P and the
reward function r are available.

It is reasonable to explain the performance using P instead
of Lr, because in the past applications P was usually very
similar to I − Lr. This is because the transition in these prob-
lems were symmetric, and the adjacency matrix was based
on a random policy. Notice that the eigenvectors of Lr and
I − Lr are the same. Moreover, if λ is an eigenvalue of Lr,
then 1 − λ is an eigenvalue of I − Lr.
3.1 Spectral Approximation
Assuming that P is the transition matrix for a ﬁxed Markov
policy, we can express the value function as [Puterman,
2005]:

v = (I − γP )−1r =

(γP )ir.

(3.1)

∞(cid:3)

i=0

The second equality follows from the Neumann series expan-
sion. In fact, synchronous backups in the modiﬁed policy it-
eration calculate this series adding one term in each iteration.
We assume that the transition matrix is diagonalizable. The
analysis for non-diagonalizable matrices would be similar,
but would have to use Jordan decomposition. Let x1 . . . xn
be the eigenvectors of P with corresponding eigenvalues
λ1 . . . λn. Moreover, without loss of generality, (cid:2)xj(cid:2)2 = 1
for all j. Since the matrix is diagonalizable, x1 . . . xn are
linearly independent, and we can decompose the reward as:

n(cid:3)

j=1

r =

cjxj,

for some c1 . . . cn. Using P ixj = λi
1

n(cid:3)

∞(cid:3)

n(cid:3)

jxj, we have

v =

(γλj)icjxj =

cjxj =

j=1

i=0

j=1

n(cid:3)

j=1

djxj.

Considering a subset U of eigenvectors xj as a basis will

lead to the following bound on approximation error:

1 − γλj
(cid:3)

|dj|.

j /∈U

(cid:2)v − ˜v(cid:2)2 ≤

(3.2)

Therefore, the value function may be well approximated by
considering those xj with greatest |dj|. Assuming that all cj
are equal, then the best choice to minimize the bound (3.2) is
to consider the eigenvectors with high λj. This is identical to
taking the low order eigenvectors of the random walk Lapla-
cian, as proposed in the spectral proto-VFA framework, only
that the transition matrix is used instead. Using the analysis
above, we propose a new algorithm in Subsection 3.3.
3.2 Krylov Methods
There are also other well-motivated base choices besides the
eigenvectors. Another choice is to use some of the vectors
in the Neumann series (3.1). We denote these vectors as

IJCAI-07

2575

yi = P ir for all i ∈ (cid:5)0,∞). Calculating the value func-
tion by progressively adding all vectors in the series, as done
in the modiﬁed policy iteration, potentially requires an in-
ﬁnite number of iterations. However, since the linear VFA
methods consider any linear combination of the basis vec-
tors, we need at most n linearly independent vectors from the
sequence {yi}∞
i=0. Then it is preferable to choose those yi
with small i, since these are simple to calculate.

Interestingly, it can be shown that we just need y0 . . . ym−1
to represent any value function, where m is the degree of the
minimal polynomial [Ipsen and Meyer, 1998] of (I − γP ).
Moreover, even taking fewer vectors than that may be a good
choice in many cases. To show that y0 . . . ym−1 vectors are
sufﬁcient to precisely represent any value function, let the
minimal polynomial be: p(A) =

i=0 αiAi. Then let

(cid:2)m

m−1(cid:3)

i=0

B =

1
α0

αi+1Ai.

By algebraic multiplication, we have BA = I. Having this,
the value function may be represented as:

m−1(cid:3)

m−1(cid:3)

v = Br =

1
α0

αi+1(I − γP )ir =

βiyi,

i=0

i=0

for some βi. A more rigorous derivation may be found for
example in [Ipsen and Meyer, 1998; Golub and Loan, 1996].
The space spanned by y0 . . . ym−1 is known as Krylov space
(or Krylov subspace), denoted as K(P, r). It has been previ-
ously used in a variety of numerical methods, such as GM-
RES, or Lancoz, and Arnoldi [Golub and Loan, 1996]. It is
also common to combine the use of eigenvectors and Krylov
space, what is known as an augmented Krylov methods [Saad,
1997]. These methods actually subsume the methods based
on simply considering the largest eigenvectors of P . We dis-
cuss this method in Subsection 3.3.
3.3 Algorithms
In this section we propose two algorithms based on the pre-
vious analysis for constructing a good basis for VFA. These
algorithms deal only with the selection of the basis and they
can be arbitrarily incorporated into any algorithm based on
approximate policy iteration.

The ﬁrst algorithm, which we refer to as the weighted spec-
tral method, is to form the basis from the eigenvectors of the
transition matrix. This is in contrast with the method pre-
sented in [Mahadevan, 2005], which uses any of the Lapla-
cians. Moreover, we propose to choose the vectors with great-
est |dj| value, in contrast to choosing the ones with largest λj.
The reason is that this leads to minimization of the bound in
(3.2).

A practical implementation of this algorithm faces some
major obstacles. One issue is that there is no standard eigen-
vector solver that can efﬁciently calculate the top eigenvec-
tors with regard to |dj| of a sparse matrix. However, such a
method may be developed in the future. In addition, when the
transition matrix is not diagonalizable, we need to calculate
the Jordan decomposition, which is very time-consuming and
unstable. Finally, some of the eigenvectors and eigenvalues

Require: P , r, k - number of eigenvectors in the ba-

sis, l - total number of vectors
Let z1 . . . zk be the top real eigenvectors of P
zk+1 ← r
for i ← 1 . . . l + k do
if i > k + 1 then

zi ← P zi−1

end if
for j ← 1 . . . (i − 1) do
zi ← zi − (cid:5)zj, zi(cid:8)zj
end for
if (cid:2)zi(cid:2) ≈ 0 then
break
end if
zi ← 1(cid:6)zi(cid:6) zi
end for

Figure 1: Augmented Krylov method for basis construction.

may contain complex numbers, what would require a revi-
sion of the approximate policy iteration algorithms. If these
issues were resolved, this may be a viable alternative to other
methods.

The second algorithm we propose is to use the vectors from
the augmented Krylov method, that is, to combine the vectors
in the Krylov space with a few top eigenvectors. A pseudo-
code of the algorithm is in Figure 1. The algorithm calculates
an orthonormal basis of the augmented Krylov space using a
modiﬁed Gram-Schmidt method.

Using the Krylov space eliminates the problems with non-
diagonalizable transition matrices and complex eigenvectors.
Another reason to combine these two methods is to take ad-
vantage of approximation properties of both methods. Intu-
itively, Krylov vectors capture the short-term behavior, while
the eigenvectors capture the long-term behavior. Though,
there is no reliable decision rule to determine the right num-
ber of augmenting eigenvectors, it is preferable to keep their
number relatively low. This is because they are usually more
expensive to compute than vectors in the Krylov space.

4 Approximation Bounds
In this section, we brieﬂy present a theoretical error bound
guaranteed by the augmented Krylov methods. The bound
characterizes the worst-case approximation error of the value
function, given that the basis is constructed using the current
transition matrix and reward vector. We focus on bounding
the quadratic norm, what also implies the max norm.
We show the bound for (cid:2)r − ˜v + γP ˜v(cid:2)2. This bound ap-
plies directly to algorithms that minimize the Bellman resid-
ual [Bertsekas and Tsitsiklis, 1996], and it also implies:
(cid:2)v − ˜v(cid:2)∞ = (cid:2)(I − γP )−1r − (I − γP )−1(I − γP )˜v(cid:2)∞

≤

1
1 − γ

(cid:2)r − (I − γP )˜v(cid:2)2.

This follows from the Neumann series expansion of the in-
verse and from (cid:2)P(cid:2)∞ = 1.

IJCAI-07

2576

In the following, we denote the set of m Krylov vectors as
Km and the chosen set of top eigenvectors of P as U. We
also use E(c, d, a) to denote an ellipse in the set of complex
numbers with center c, focal distance d, and major semi-axis
a. The approximation error for a basis constructed for the cur-
rent policy may be bounded as the following theorem states.
Theorem 4.1. Let (I − γP ) = XSX−1 be a diagonalizable
matrix. Further, let
φ =

(cid:2)XT X−1x(cid:2)2,

max

x∈U⊥(cid:6)x(cid:6)2=1

where T is a diagonal matrix with 1 in place of eigenvalues of
eigenvectors in |U|. The approximation error using the basis
(cid:5)
Km ∪ U is bounded by:
(cid:2)r − (I − γP )˜v(cid:2)2 ≤ max

κ(X)

(cid:4)

,

Cm( a1
d1
Cm( c1
d1

)
) , φ

Cm( a
d )
Cm( c
d)

where Cm is the Chebyshev polynomial of the ﬁrst kind of de-
gree m, and κ(X) = (cid:2)X(cid:2)2(cid:2)X−1(cid:2)2. The parameters a, c, d
and a1, c1, d1 are chosen such that E(c1, d1, a1) includes the
lower n−|U| eigenvalues of (I−γP ), and E(c, d, a) includes
all its eigenvalues.

The value of φ depends on the angle between the invari-
ant subspace U and the other eigenvectors of P . If they are
perpendicular, such as when P is symmetric, then φ = 0.
Proof. To simplify the notation, we denote A = (I − γP ).
First, we show the standard bound on approximation by a
Krylov space [Saad, 2003], ignoring the additional eigenvec-
tors. In this case, the objective is to ﬁnd such w ∈ Km that
minimizes the following:

(cid:2)r − Aw(cid:2)2 = (cid:2)r − n(cid:3)

w(i)Air(cid:2)2 = (cid:2) n(cid:3)

i=0

i=1

−w(i)Air(cid:2)2,
where w(0) = −1. Notice that this deﬁnes a polynomial in
A multiplied by r with the constant factors determined by w.
Let Pm denote the set of polynomials of degree at most m
such that every p ∈ Pm satisﬁes p(0) = 1. The minimization
problem may be then expressed as ﬁnding a polynomial p ∈
Pm that minimizes (cid:2)p(A)r(cid:2)2. This is related to the value
of the polynomial on complex numbers as [Golub and Loan,
1996]:

(cid:2)p(A)(cid:2)2 ≤ κ(X) max

|p(λi)|,

i=1,...,n

where λi are the eigenvalues of A. A more practical bound
may be obtained using Chebyshev polynomials, as for exam-
ple in [Saad, 2003], as follows:
|p(λi)| ≤ min
p∈Pm

|p(λ)| ≤ Cm( a
d )
min
d) ,
p∈Pm
Cm( c
where the ellipse E(c, d, a) covers all eigenvalues of A.

max
i=1,...,n

λ∈E(c,d,a)

max

In the approximation with eigenvectors, the minimization

may be expressed as follows:

min

min
q∈U

(cid:2)r − Aw − AU q(cid:2)2 =

(cid:2)r − A˜v(cid:2)2 = min
w∈Km
v
(cid:2)p(A)r + AU q(cid:2)2 =
= min
p∈Pm
(cid:2)p(A)(I − PU )r + p(A)PU r − AU q(cid:2)2
= min
p∈Pm
= min
p∈Pm

min
q∈U
min
q∈U
(cid:2)p(A)(I − PU )r(cid:2)2.

where PU is the least squares projection matrix to U. Note
that (cid:2)p(A)PU r − AU y(cid:2) = 0, since U is an invariant space
of A. Moreover, (I − PU ) ∗ r is perpendicular to U. The
theorem then follows from the approximation by Chebyshev
polynomials as described above, and the deﬁnition of matrix-
valued function approximation [Golub and Loan, 1996].

This bound shows that it is important to choose an invari-
ant subspace that corresponds to the top eigenvalues of P . It
also shows that U should be chosen to be to the highest de-
gree perpendicular to the remaining eigenvectors. Finally, the
bound also implies that the approximation precision increases
with lower γ, as this decreases the size of the ellipse to cover
the eigenvalues.
5 Experiments
In this section we demonstrate the proposed methods on the
two-room problem similar to the one used in [Mahadevan,
2005; Mahadevan and Maggioni, 2005]. This problem is a
typical representative of some stochastic planning problems
encountered in AI.

The MDP we use is a two-room grid with a single-cell
doorway in the middle of the wall. The actions are to move
in any of the four main direction. We use the policy with an
equal probability of taking any action. The size of each room
is 10 by 10 cells. The problem size is intentionally small
to make explicit calculation of the value function possible.
Notice that because of the structure of the problem and the
policy, the random walk Laplacian has identical eigenvectors
as the transition matrix in the same order. This allows us to
evaluate the impact of choosing the eigenvectors in the order
proposed by the weighted spectral method.

We used the problem with various reward vectors and dis-
count factors, but with the same transition structure. The
reward vectors are synthetically constructed to show possi-
ble advantages and disadvantages of the methods. They are
shown projected onto the grid in Figure 2. Vector “Reward 1”
represents an arbitrary smooth reward function. Vectors “Re-
ward 2” and “Reward 3” are made perpendicular to the top
40 and the top 190 eigenvectors of the random walk Lapla-
cian respectively.

A lower discount rate is generally more favorable to Krylov
methods, because the value is closer to the value of the reward
received in the state. This can also be seen from Theorem 4.1,
because γ shrinks the eigenvalues. Therefore, we use prob-
lems that are generally less favorable to Krylov methods, we
evaluate the approach using two high discount rates, γ = 0.9
and γ = 0.99.

In our experiments, we evaluate the Mean Squared Er-
ror (MSE) of the value approximation with regard to the num-
ber of vectors in the basis. The basis and the value function
were calculated based on the same policy with equiprobable
action choice. We obtained the true value function by explic-
itly evaluating (I − γP )−1r. Notice, however, that the true
value does not need to be used in the approximation, it is only
required to determine the approximation error. We compared
the following 4 methods:
Laplacian The technique of using the eigenvectors of the
random walk Laplacian of the adjacency matrix, as pro-

IJCAI-07

2577

Reward 1

Reward 2

Reward 3

1000

d
r
a
w
e
R

500

0

−500
10

20

5

Y

10
X

0

0

400

200

0

−200

−400
10

d
r
a
w
e
R

20

5

Y

0

0

10
X

20

5

Y

10
X

0

0

Figure 2: Reward vectors projected onto the grid

Reward 2

Reward 1

Laplacian
WL
Krylov
KA

1010

100

E
S
M

10−10

10−20

1010

100

E
S
M

10−10

10−20

Reward 3

100

Vectors

200

10−30

0

100

Vectors

200

10−30

0

100

Vectors

200

d
r
a
w
e
R

10

5

0
10

1010

100

E
S
M

10−10

10−20

10−30

0

Figure 3: Mean squared error of each method, using discount of γ = 0.9. The MSE axis is in log scale.

posed in [Mahadevan, 2005]. The results of the normal-
ized Laplacian are not shown because they were practi-
cally identical to the random walk Laplacian.

WL This is the weighted spectral method, described in Sub-
section 3.3, which determines the optimal order of the
eigenvectors to be used based on the value of dj.

Krylov This is the augmented Krylov method with no eigen-

vectors, as described in Subsection 3.3.

KA This is the augmented Krylov method with 3 eigenvec-
tors, as proposed in Figure 1. The number of eigenvec-
tors was chosen before running any experiments on this
domain.

The results for γ = 0.9 are in Figure 3 and for γ = 0.99
are in Figure 4. “Reward 3” intentionally violates the smooth-
ness assumption, but it is the easiest one to approximate for
all methods except the Laplacian. In the case of “Reward 1”
and γ = 0.99, the weighted spectral method and the random
walk Laplacian outperform the ordinary Krylov method for
the ﬁrst 10 vectors. However, with additional vectors, both
Krylov and augmented Krylov methods signiﬁcantly outper-
form them.

Our results suggest that Krylov methods may offer superior
performance compared to using the eigenvectors of the Lapla-
cian. Since these methods have been found very effective in
many sparse linear systems [Saad, 2003], they may perform
well for basis construction also in other MDP problems. In
addition, constructing a Krylov space is typically faster than

constructing the invariant space of eigenvectors. Using Mat-
lab on a standard PC, it took 2.99 seconds to calculate the 50
top eigenvectors, while it took only 0.24 second to calculate
the ﬁrst 50 vectors of the Krylov space for “Reward 1”.

6 Discussion
We presented an alternative explanation of the success of
Laplacian methods used for value approximation. This ex-
planation allows us to more precisely determine when the
method may work. Moreover, it shows that these meth-
ods are closely related to augmented Krylov methods. We
demonstrated on a limited problem set that basis constructed
from augmented Krylov methods may be superior to one con-
structed from the eigenvectors. In addition, both the weighted
spectral method and the augmented Krylov method do not as-
sume the value function to be smooth. Moreover, calculating
vectors in the Krylov space is typically cheaper than calculat-
ing the eigenvectors.

An approach for state-space compression of Partially Ob-
servable MDPs (POMDP) that also uses Krylov space was
proposed in [Poupart and Boutilier, 2002]. While POMDPs
are a generalization of MDPs, the approach is not practical
for large MDPs since it implicitly assumes that the number of
observations is small. This is not true for MDPs, because the
number of observations is equivalent to the number of states.
In addition, the objectives of this approach are somewhat dif-
ferent, though the method is similar.

IJCAI-07

2578

1010

100

E
S
M

10−10

10−20

10−30

0

Reward 1

Laplacian
WL
Krylov
KA

1010

100

Reward 2

E
S
M

10−10

10−20

10−30

0

100

Vectors

200

Reward 3

1010

100

E
S
M

10−10

10−20

100

Vectors

200

10−30

0

100

Vectors

200

Figure 4: Mean squared error of each method, using discount of γ = 0.99. The MSE axis is in log scale.

One important issue that we did not address is an applica-
tion to state spaces that are too large to be enumerated, such
as factored MDPs. Because the vectors in Krylov space are
as large as the whole state space, they cannot be enumerated
either. However, the approach may be extended along similar
lines as discussed in [Poupart and Boutilier, 2002].

In reinforcement learning, the MDP needs to be solved us-
ing only an estimation of the transition matrix from sampling.
An additional problem is that the basis is not deﬁned for states
that were not sampled. This was addressed for eigenvectors
for example in [Mahadevan et al., 2006]. Thus augmenting
the Krylov space by the eigenvectors also has the advantage
that these methods may be directly applied.

We focused mainly on the VFA for a ﬁxed policy with-
out explicitly considering the control part of the problem.
While these methods may be combined arbitrarily, a future
challenge is to determine an efﬁcient combination.

Our results suggest that exploring the connection between
the basis construction in VFA and sparse linear solvers may
bring about interesting advances in the future.

Acknowledgements
The author was supported by the National Science Founda-
tion under Grant No. IIS-0328601. I also thank Sridhar Ma-
hadevan, Hala Mostafa, Shlomo Zilberstein, and the anony-
mous reviewers for valuable comments.

References
[Bertsekas and Tsitsiklis, 1996] Dimitri P. Bertsekas and
John N. Tsitsiklis. Neuro-dynamic programming. Athena
Scientiﬁc, 1996.

[Chung, 1997] Fang Chung. Spectral graph theory. Ameri-

can Mathematical Society, 1997.

[Golub and Loan, 1996] Gene H. Golub and Charles F. Van
John Hopkins University

Loan. Matrix computations.
Press, 3rd edition, 1996.

[Ipsen and Meyer, 1998] Ilse C. F. Ipsen and Carl D. Meyer.
The idea behind Krylov methods. American Mathematical
Monthly, 105(10):889–899, 1998.

[Maggioni and Mahadevan, 2006] Mauro Maggioni

and
Sridhar Mahadevan. Fast direct policy evaluation using

multiscale analysis of markov diffusion processes.
In
Proceedings of the International Conference on Machine
Learning, pages 601–608. ACM Press, 2006.

[Mahadevan and Maggioni, 2005] Sridhar Mahadevan and
Mauro Maggioni. Value function approximation with dif-
fusion wavelets and Laplacian eigenfuctions. In Proceed-
ings of Advances in Neural Information Processing Sys-
tems, 2005.

[Mahadevan et al., 2006] Sridhar Mahadevan, Mauro Mag-
gioni, Kimberly Ferguson, and Sarah Osentoski. Learning
representation and control in continuous Markov decision
processes. In Proceedings of the National Conference on
Artiﬁcial Intelligence, 2006.

[Mahadevan, 2005] Sridhar Mahadevan.

Samuel meets
Amarel: Automating value function approximation using
global state space analysis. In Proceedings of the National
Conference on Artiﬁcial Intelligence, 2005.

[Munos, 2003] Remi Munos. Error bounds for approximate
policy iteration. In Proceedings of the International Con-
ference on Machine Learning, pages 560–567, 2003.

[Poupart and Boutilier, 2002] Pascal Poupart

and Craig
Value-directed compression of POMDPs.
Information

Boutilier.
In Proceedings of Advances in Neural
Processing Systems, pages 1547–1554, 2002.

[Puterman, 2005] Martin L. Puterman. Markov decision pro-
cesses: Discrete stochastic dynamic programming. John
Wiley & Sons, Inc., 2005.

[Saad, 1995] Yosef Saad. Preconditioned Krylov subspace
methods for CFD applications. In W. G. Hasbashi, editor,
Solution Techniques for Large Scale CFD Problems, page
141. Wiley, 1995.

[Saad, 1997] Yousef Saad. Analysis of augmented Krylov
subspace methods. SIAM Journal on Matrix Analysis and
Applications, 18(2):435–449, April 1997.

[Saad, 2003] Yousef Saad. Iterative methods for sparse lin-

ear systems. SIAM, 2nd edition, 2003.

[Sutton and Barto, 1998] Richard S. Sutton and Andrew

Barto. Reinforcement learning. MIT Press, 1998.

IJCAI-07

2579


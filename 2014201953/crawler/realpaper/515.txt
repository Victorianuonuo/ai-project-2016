SVMC: Single-Class Classification With Support Vector Machines 

Department of Computer Science 

University of Illinois at Urbana-Champaign 

H wan jo Yu 

Urbana,IL 61801 USA 
hwanjoyu@uiuc.edu 

Abstract 

Single-Class  Classification  (SCC)  seeks  to  distin(cid:173)
guish  one  class  of data  from  the  universal  set  of 
multiple classes.  We present a new SCC algorithm 
that  efficiently  computes  an  accurate  boundary  of 
the  target  class  from  positive  and  unlabeled  data 
(without labeled negative data). 

Introduction 

1 
Single-Class  Classification  (SCC)  seeks  to  distinguish  one 
class of data from the universal set of multiple classes,  (e.g., 
distinguishing apples from fruits, identifying "waterfall" pic(cid:173)
tures  from  image  databases,  or  classifying  personal  home-
pages from the Web) (Throughout the paper, we call the target 
class positive and the complement set of samples negative.) 
Since  it  is  not  natural  to  collect the "non-interesting" ob(cid:173)
jects (i.e., negative data) to train the concept of the "interest(cid:173)
ing" objects (i.e., positive data), SCC problems are prevalent 
in  real  world  where  positive  and  unlabeled  data  are  widely 
available  but negative data  are  hard  or expensive to  acquire 
[Yu  et  al.  2002;  Letouzey  et al. , 2000;  DeComite  et  a/., 
1999].  For example, in text or Web page classification (e.g., 
personal  homepage  classification),  collecting negative train(cid:173)
ing data (e.g., a sample of "non-homepages") is delicate and 
arduous  because  manually  collected  negative  data  could  be 
easily  biased  because  of a  person's  unintentional  prejudice, 
which could be detrimental  to classification accuracy.  In an 
example  of diagnosis  of a  disease,  positive data  are  easy  to 
access (e.g., all patients who have the disease) and unlabeled 
data are abundant (e.g., all patients), but negative data are ex(cid:173)
pensive  if detection  tests  for the  disease  are  expensive since 
all patients in the database cannot be assumed to be negative 
samples  if they have  never been tested.  Further applications 
can be also found in pattern recognition, image retrieval, clas(cid:173)
sification for data mining, rare class classification, etc. In this 
paper, we focus on this SCC problem from positive and unla(cid:173)
beled data (without labeled negative data). 

1.1  Previous  Approaches  for  SCC 
Traditional  (semi-)supervised  learning schemes are not suit(cid:173)
able for SCC without labeled negative data because:  (1) the 
portions of positive and negative spaces are seriously unbal(cid:173)
anced  without  being  known  (i.e.,  Pr(P)  «  Pr(P)),  and 

(2)  the  absence  of negative  samples  in  the  labeled  data  set 
makes  unfair the  initial  parameters  of the  model  and  thus  it 
leads to unfair guesses for the unlabeled data. 

Active  learning methods also try to minimize the  labeling 
labors to construct an accurate classification function by a dif(cid:173)
ferent approach  that involves an  interactive process between 
the learning system and users [Tong and Koller, 2000]. 

Valiant  in  1984  [Valiant,  1984] pioneered learning theory 
from positive examples based on rule learning. In 1998, F. De(cid:173)
nis defined the Probably Approximately Correct (PAC) learn(cid:173)
ing model  for positive and unlabeled examples, and showed 
that k-DNF (Disjunctive Normal Form) is learnable from pos(cid:173)
itive and unlabeled examples [Denis,  1998]. After that, some 
experimental  attempts  to  learn  using positive and  unlabeled 
data  have  been  tried  using k:-DNF or  C4.5  [Letouzey et al., 
2000;  DeComite  et  al,  1999].  Rule  learning  methods  are 
simple and efficient for learning nominal features but tricky to 
use for the problems of continuous features, high dimensions, 
or sparse instance spaces. 

Positive Example-Based Learning (PEBL) framework was 
proposed for Web page classification  [Yu et ai, 2002].  Their 
method  is  limited  to  the  Web  domain  with  binary  features, 
and its training efficiency is poor because of using SVM iter-
atively whose training time is already at least quadratic to the 
size of training data set.  This problem becomes critical when 
the size of unlabeled data set is large. 

A probabilistic method for the SCC problem has been re(cid:173)
cently  proposed  for  the  text  domain  [Liu  et  ai,  2002].  As 
they  specified  in  the  paper,  their method -  a  revision  of the 
EM  algorithm  -  performs  badly  on  "hard"  problems  due  to 
the fundamental limitations of the generative model assump(cid:173)
tion,  the attribute independence assumption which results in 
linear separation,  and  the requirement of good  estimation  of 
prior probabilities. 

OSVM  (One-Class  SVM)  also  distinguishes  one  class  of 
data  from  the  rest  of  the  feature  space  given  only  a  pos(cid:173)
itive  data  set  [Tax  and  Duin,  2001;  Manevitz  and  Yousef, 
2001].  Based on a strong mathematical foundation,  OSVM 
draws a nonlinear boundary of the positive data set in the fea(cid:173)
ture  space  using  two parameters - v  (to  control  the  noise  in 
the training data) and 
(to  control  the  "smoothness"  of the 
boundary).  They have the  same  advantages as  SVM,  such 
as efficient handling of high dimensional spaces and system(cid:173)
atic nonlinear classification using advanced kernel functions. 

LEARNING 

567 

Figure  1:  Boundaries of SVM and OSVM on a synthetic data set.  big dots: positive data, small dots:  negative data 

However,  OSVM  requires  a much  larger amount  of positive 
training data to induce an accurate class boundary because its 
support vectors  (SVs)  of the  boundary  only  comes  from  the 
positive  data  set and  thus  the  small  number of positive  SVs 
can hardly cover the  major directions  of the boundary  espe(cid:173)
cially  in  high  dimensional  spaces.  Due  to  the  SVs  coming 
only  from positive data,  OSVM tends to ovcrfit and  undcrfit 
easily. Tax proposed a sophisticated method which uses artif-
ically generated unlabeled data to optimize the OSVM's pa(cid:173)
rameters that "balance" between ovcrfitting and  undcrfitting 
[Tax and  Duin,  2001].  However, their optimization method 
is infeasibly inefficient in high dimensional spaces, and even 
with the best parameter setting,  its performance still lags far 
behind the original SVM with negative data due to the short(cid:173)
age of SVs which makes "incomplete" the boundary descrip(cid:173)
tion.  Figure  1(a)  and  (b)  show the  boundaries  of SVM  and 
OSVM  on  a  synthetic  data  set  in  a two-dimensional  space. 
(We used L1BSVM version 2.33'  for SVM  implementation.) 
In  this  low-dimensional  space  with  "enough"  data,  the  ob-
stensibly "smooth" boundary of OSVM is not the result of the 
good generalization but instead is from the poor expressibility 
caused by  the  "incomplete"  SVs,  which  will  become  much 
worse  in  high-dimensional  spaces  where  more  SVs  around 
the boundary are needed to cover major directions in the high-
dimensional spaces.  When we increase the number of SVs in 
OSVM, it ovcrfits rather than being more accurate as shown 
in Figure  l(c)and(d). 
1.2  Contributions and Paper Layout 
We  first  discuss  the  "optimal"  SCC  boundary,  which  moti(cid:173)
vates  our new  SCC  framework Mapping-Convergence (MC), 
where the algorithms  under the  MC  framework generate  the 
boundary close to the optimum (Section 2).  In Section 3, we 
present an  efficient  SCC algorithm Support  Vector Mapping 
Convergence  (SVMC)  under the  MC  framework.  We prove 
that  although  SVMC  iterates  under  the  MC  framework  for 
the "near-optimal" result,  its training time  is independent of 
the number of iterations, which is asymptotically equal to that 
of a  SVM.  We  empirically  verify  our analysis  of SVMC  by 
extensive  experiments  on  various  domains  of real  data  sets 
such as text classification (e.g., Web page classification), pat(cid:173)
tern recognition (e.g., letter recognition), and bioinformatics 
(e.g., diagnosis of breast cancer), which shows the outstand(cid:173)
ing performance of SVMC in a wide spectrum of SCC prob-

1 http://www.csie.ntu.edu.tw/~cjlin/libsvm 

lems (with nominal or continuous attributes, linear or nonlin(cid:173)
ear separation, and low or high dimensions) (Section 4). 

1.3  Notation 
We use the following notation throughout this paper. 

•  x is a data instance such that 
•  V is a subspace for positive class within U, from which 

positive data set P is sampled. 

•  U (unlabeled data set) is a uniform sample of the univer(cid:173)

sal set. 

•  U is the feature space for the universal set such that U  C 

Rm  where m  is the number of dimensions. 

For an  example  of Web  page  classification,  the  universal 
set  is the entire  Web,  U  is a uniform  sample of the Web,  P 
is  a  collection  of Web  pages  of interest,  and  x  €  Rm  is  an 
instance of a Web page. 

2  Mapping Convergence (MC) Framework 
2.1  Motivation 
In  machine  learning  theory,  the  "optimal"  class  boundary 
function (or hypothesis) h(x) given a limited number of train(cid:173)
ing  data  set  {(x,/)}  (I  is  the  label  of x)  is  considered  the 
one that gives the best generalization performance which de(cid:173)
notes the performance on "unseen" examples rather than on 
the training data. The performance on the training data is not 
regarded as a good evaluation measure  for a hypothesis be(cid:173)
cause the hypothesis ends up overfitting when it tries to fit the 
training data too hard.  (When a problem is easy (to classify) 
and the boundary function is complicated more than it needs 
to be,  the boundary is likely overfitted.  When a problem  is 
hard and the classifier is not powerful enough, the boundary 
becomes  undcrfit.)  SVM  is  an  excellent  example  of super(cid:173)
vised  learning  that  tries  to  maximize  the  generalization  by 
maximizing  the  margin  and  also  supports  nonlinear  separa(cid:173)
tion  using  advanced  kernels,  by  which  SVM  tries  to  avoid 
overfitting and underfitting [Burges, 1998]. 

The "optimal" SCC classifier without labeled negative data 
also  needs  to  maximize  the  generalization  somehow  with 
highly expressive power to avoid ovcrfitting and undcrfitting. 
To  illustrate  an  example  of the  "near-optimal"  SCC  bound(cid:173)
ary without labeled negative data, consider the synthetic data 
set (Figure 2) simulating a real situation where within U, (1) 
the  universal  set is composed of multiple groups of data, (2) 

568 

LEARNING 

Figure 2:  Synthetic data set  simulating a real  situation.  P:  big 
dots,  U:  all  dots  (big  and  small  dots) 

the boundary  around  the positive data  set  in  the  feature  space 
which  also  maximizes  the  margin. 

Figure  3:  Example  of the  spaces  of the  MC  framework  in  U 

the  positive  class  V  is  one  of them  (supposing  V  is  the  data 
group  in  the  center),  and  (3)  the positive  data  set  P  is  a  sam(cid:173)
ple  from  V  (assuming  that  the  big  dots  are  the  sample  P). 
O S VM  draws  V,  a  tight  boundary  around  P,  as  shown  in 
Figure  2(a),  which  overfits  the  true  class  area  V  due  to  the 
absence  of the  knowledge  of the  distribution  of U.  However, 
the  "near-optimal"  SCC  classifiers  must  locate  the  boundary 
between  V  and  U  outside  V  (Figure  2(b))  and  thus  maximize 
the  generalization.  The  MC  framework  using  U  systemati(cid:173)
cally  draws  the  boundary  of Figure  2(b). 

Negative  S t r e n g th 

2.2 
Let  h(x)  be  the  boundary  function  of the  positive  class  in  U, 
which  outputs  the distance  from  the boundary  to  the  instance 
x  in  U  such  that 

Input: 
Output: 

- positive data set  P,  unlabeled data set U 
- a boundary function h, 

:  an algorithm identifying "strong negatives" from U 
:  a supervised learning algorithm that maximizes the margin 

Algorithm: 
1. Use 

to construct a classifier ho  from P and U which classifies 

only "strong negatives" as negative and the others as positive 

examples from U classified as negative by ho 
:= examples from U classified as positive by ho 

2. Classify  U  by  h0 

3. Set TV :=  and i  :=  0 
4. Do loop 
4.1.  N 
4.2. Use 
4.3. Classify < 

:=  NUN, 

to construct  hi  1  from P and TV 

if  x 
if  x 
if  x 
from 

is  a  positive 
is  a  negative 
is 
the  boundary 

located 

instance, 
instance, 

farther 

than  x' 

in  U. 

:= examples  from  Pi  classified as negative by  hi+1 
examples  from  Px  classified as positive  by  hi+1 

4.4. i  :=  i  +  1 
4.5. Repeat until 

5. return  hi 

\h(x')\, 

Definition  1  (Strength  of negative  instances).  For  two  neg(cid:173)
ative  instances  x  and  x'  such  that  h(x)  <  0  and  h(x')  <  0, 
is  stronger  than  x'. 
if\h(x)\  > 
then  x 
Example  1.  Consider  a 
resume  page  classification  function 
h(x)  from  the  Web  (U).  Suppose  there  are  two  negative  data 
objects  x  and  x'  (non-resumepages)  in  U  such  that  h(x)  <  0 
"how  to  write  a  resume"  page,  and 
and  h(x')  <  0:  x 
x'  is 
In  U,  x'  is  considered 
more  distant  from  the  boundary  of  the  resume  class  because  x 
has  more  relevant features  to  the  resume  class  (e.g.,  the  word 
"resume  "  in  text)  though  it  is  not  a  true  resume page. 

"how  to  write  an  article  "  page. 

is 

MC  F r a m e w o rk 

2.3 
The  MC  framework  is  composed  of two  stages:  the  mapping 
stage  and  the  convergence  stage. 
In  the  mapping  stage,  the 
algorithm  uses  a  weak  classifier  
,  which  draws  an  initial 
approximation  of "strong  negatives"  -  the  negative  data  lo(cid:173)
cated  far  from  the  boundary  of the  positive  class  in  U  (steps 
1  and 2  in  Figure 4).  Based  on the  initial  approximation,  the 
convergence stage runs in iteration using a second base classi(cid:173)
fier  $ 2,  which maximizes the margin to make a progressively 
better  approximation  of negative  data  (steps  3  through  5  in 
Figure  4).  Thus  the  class  boundary  eventually  converges  to 

Figure  4:  MC  framework 

Assume  that  V  is  a  subspace  tightly  subsuming  P  within 
U  where  the  class  of the  boundary  function  for  V  is  from  the 
(e.g.,  SVM).  In  Figure 4,  let  No  be the negative 
algorithm 
be  the  positive  space  within  U  divided  by  h0 
space  and 
),  and  let  Ni  be  the  negative  space 
(a  boundary  drawn  by  
divided  by  hi  (a 
and 
boundary  drawn  by  
Then,  we  can  induce  the  follow(cid:173)
ing  formulae  from  the  MC  framework  of Figure  4.  (Figure  3 
illustrates  an  example  of the  spaces  of the  framework  in  U.) 

be  the  positive  space  within 

(1) 

(2) 

where  I  is  the  number  of iterations  in  the  MC  framework. 
Theorem  1  (Boundary  Convergence).  Suppose  U 
formly  distributed  in  U. 

If algorithm 

does  not  generate 

is  uni(cid:173)

LEARNING 

569 

false negatives, and  algorithm  maximizes margin,  then (I) 
the class boundary of the MC framework converges into the 
boundary that  maximally separates  P  and U  outside 
and 
(2)  I  (the  number  of iterations)  is  logarithmic  in  the  margin 
between 

and 

and 

divides the rest of the space 

because a classifier 

constructed by the 
does not generate false negative. A classifier h\ 
trained from the separated 

Proof  , 
algorithm 
constructed by the algorithm 
space 
which is equal to 
that maximizes the margin between 
and the other becomes 
becomes 
classifier 
constructed by the same algorithm 
from the separated space 
the space 
into 
margins.  Thus, 
always  has  the  margin  of half  of 
(for 
between 

into two classes with a boundary 
The first part 
Repeatedly, a 
trained 
divides the  rest of 
with  equal 

Therefore,  I  will  be  logarithmic  in  the  margin 
and 

and 

and 

and 

The  iteration  stops  when 

where  there  exists  no 
sample  of U  outside 
Therefore,  the  final  boundary  will 
be located between P and U outside  while maximizing the 
margin between them. 

Theorem  1  proves that under certain conditions,  the final 
boundary will be located between P and U outside 
How(cid:173)
ever, in the example of Figure 2(b), our framework generates 
the  "better"  boundary  located  between  P  and  U  outside  V 
because in theorem  1, we made a somewhat strong assump(cid:173)
tion, i.e., U is uniformly distributed, to guarantee the bound(cid:173)
ary convergence.  In a more realistic situation where there is 
some distance S between classes-Figure 2 shows some gaps 
between classes-if the margin between 
becomes 
smaller  than  at some iteration,  the convergence stops be(cid:173)
becomes empty.  The margin  b e t w e e n a nd 
cause 
ap(cid:173)
and thus the boundary is not likely to stop con(cid:173)
unless  U  is  severely  sparse. 

proaches to 
verging  when  it  is  far  from 
Thus, we have the following claim: 
Claim 1.  The  boundary  of  MC  is  located  between  P  and  U 
outside V if U and P are not severely sparse and there exists 
visible gaps between V and U. 

reduces by half at each iteration as the boundary 

and 

Validity of the component  algorithms 

J. 

must not generate false negatives. 

Most  classification  methods  have  a  threshold  to  control  the 
trade-off between  precision  and  recall.  We  can  adjust  the 
threshold  of 
so that it makes near 100% recall by sacrific(cid:173)
ing precision.  (Some violations of this can be handled by the 
soft constraint  of 
(e.g., SVM).) Determining the threshold 
can  be  intuitive  or automatic  when  not  concerning  the pre(cid:173)
cision  quality  much.  The  precision  quality  of 
does  not 
affect the accuracy of the  final  boundary as far as it approxi(cid:173)
mates a certain amount of negative data because the boundary 
will converge eventually.  Figure 5 visualizes the boundary af(cid:173)
ter each  iteration  of SVMC.  The  mapping stage  only  identi(cid:173)
fies very strong negatives by covering a wide area around the 
positive data (Figure 5(a)).  (We used OSVM for the algorithm 
of the mapping stage.  We intuitively set the parameters of 
OSVM such that it covers all the positive data without much 
concern  for  false  positives.)  Although  the  precision  quality 
of mapping is poor, the boundary at each iteration converges 
(Figures  5(b)  and  (c)),  and  the  final  boundary  is  very  close 
to  the  true  boundary  drawn  by  SVM  with  P  and  N  (Figure 
1(a) and  5(d)).  Our experiments in  Section 4 also show that 
the final boundary becomes very accurate although the initial 
boundary of the mapping  stage  is very rough by the "loose" 
setting of the threshold of 

. 

2.  2 must maximize margin. 

SVM and Boosting are currently the most popular supervised 
learning algorithms that maximize the margin.  With a strong 
mathematical foundation,  SVM automatically finds the opti(cid:173)
mal boundary without a validation process and without many 
parameters to tune.  The small numbers of theoretically moti(cid:173)
vated parameters also work well for an intuitive setting.  For 
these reasons, we use SVM for 
for our research.  In prac(cid:173)
tice,  the  soft  constraint  of SVM  is  necessary  to  cope  with 
noise  or  outliers.  The  soft  constraint  of SVM  can  affect  / 
and  the  accuracy  of the  final  boundary.  However,  P  is  not 
likely to have a  lot of noise  in practice because  it  is usually 
carefully collected by users.  In our experiments, a low setting 
(i.e., v  = 0.01)  of 
(the parameter to control the rate of noise 
in the training data) performs well for all cases for this reason. 
(We used I/-SVM for the semantically meaningful parameter 
[Chang and Lin, 2001].) 

570 

LEARNING 

3  Support Vector Mapping Convergence 

(SVMC) 

3.1  Motivation 
The  classification time  of the  final  boundary of SMC  ("Sim(cid:173)
=  SVM)  is  equal  to  that  of SVM  because 
ple" MC with 
the final boundary is a boundary function  of 
The training 
time of SMC can be very long  if 
is very large because the 
training time of SVM highly depends on the size of data set 

and SMC runs iteratively. 

iterations 

andtsvM  = 

is  the  training  time  of  a  classifier 

assuming the number of 
0(|f/|2)  where 
(tsvM  is  known  to  be  at  least  quadratic  to 
and  linear  to 
the number of dimensions).  Refer to  [Chang and  Lin,  2001] 
for more discussion on the complexity of SVM. However, de(cid:173)
creasing the sampling density of U to reduce the training time 
hurts the  accuracy of the  final  boundary  because the density 
of U  will  directly  affect  the  quality  of the  SVs  of the  final 
boundary. 

3.2  S V MC 
SVMC  prevents  the  training  time  from  increasing  dramat(cid:173)
ically  as  the  sample  size  grows.  We  prove  that  although 
SVMC  iterates  under  the  MC  framework  for  the  "near-
optimal" result, its training time is independent of the number 
of iterations, and thus its training time is asymptotically equal 
to that of a SVM. 

The approach of SVMC is to  use minimally required data 
set  at each  iteration  such  that the data  set does  not degrade 
the  accuracy of the boundary while  it saves the training time 
of each  SVM  maximally.  To  illustrate  how  SVMC achieves 
this,  consider  the  point  of starting  the  third  iteration  (when 
in  SMC.  (See  step 4.1  in  Figure 4.)  After we  merge 
into  N,  we may not  need all  the data  from  N  in order to 
construct  h3  because the data far from h3 may not contribute 
to the SVs. The set of negative SVs of h2 is the representative 
so we only keep the negative SVs of 
data set for 
h2 and the newly induced data set 
to support the negative 
side of/13. 
Claim 2 (Minimally  required negative data).  Minimally 
required negative  data  at 
iterationthat 
makes h{+1 is as accurate as the boundary constructed from 

and 

th 

and 

negative  support  vectors  of  h{. 
Rationale.  The  negative  SVs  of 
will  be  f r o m a nd 
the  negative  SVs  of hi  because 
is the closest data  set to 
/ij+i  and because  the  directions  not supported by 
in  the 
feature  space  will  be  supported  by  the  negative  SVs  of h^ 
which  are  the  representing  data  set  for 
However, 
if any  of the  negative  SVs  of hi  is  excluded  in  constructing 
might suffer because the negative SVs of hi  need 
to support the direction that TVj does not support in the feature 
space. Thus, 
negative  support  vectors  of  hi  are  the 
minimally required negative data set at (i +  l)th iteration.  □ 
For  the  minimally  required  data  set  for  the positive  side, 
we cannot definitely exclude any data object from P at each 

iteration because positive  SVs are determined depending on 
negative SVs, and it is hard to determine the positive data that 
cannot be SVs independent of negative SVs or SVM parame(cid:173)
ters. 

Surprisingly, adding the following statement between step 
4.4  and  4.5  of the  original  MC  framework  of Figure 4  com(cid:173)
pletes the SVMC algorithm. 

Reset  N  w i th  negative  SVs  of 

Theorem 2 (Training time of SVMC). Suppose 

Proof.  For  simplicity  of  the  proof,  we  approximate  each 
value as follows. 

Theorem  2  states  that  the  training  complexity  of SVMC 
is  asymptotically  equal  to  that  of SVM.  Our experiments  in 
Section 4 also show that SVMC trains much faster than SMC 
while  it remains the  same accuracy.  Figure  5  visualizes the 
boundary after each iteration of SVMC on the same data set 
of Figure  1. 

4  Empirical Results 
In this section, we show the empirical verification of our anal(cid:173)
ysis on SVMC by extensive experiments on various domains 
of real data sets - Web page classification,  letter recognition, 
and  diagnosis  of breast  cancer  -  which  show  the  outstand(cid:173)
ing performance of SVMC  in a wide spectrum of SCC prob(cid:173)
lems (with nominal or continuous attributes, linear or nonlin(cid:173)
ear separation, and low or high dimensions). 

4.1  Datasets and Methodology 
Due  to  space  limitations,  we  reports  only  the  main  results. 
Our evaluation is based on the F\  measure 
r), p is precision  and  r  is  recall)  as  was  used  in  [Liu  et al, 
2002] - one of the most  recent works  on  SCC  from  positive 
and unlabeled data2. We also report the accuracy. 

We used the letter recognition and breast cancer data sets 
from the  UC1  machine  learning  repository3  for direct com(cid:173)
parisons with OSVM. (OSVM is often used for letter or digit 

2Refer to [Liu et al., 2002]  for the justification of using the Fl 

measure for SCC. 

3 http://www.ics.uci.cdu/~mlcarn/M LRepository.html 

LEARNING 

571 

Class 

letter  A 

'B' 
'C' 
'D' 
'E' 

b-canccr 
course 
faculty 
student 

\P\ 
521 
571 
485 
525 
518 
135 
482 
532 
816 

\u\ 
10007 
10004 
9996 
10050 
10026 
259 
4179 
4209 
4154 

I'VI 
384 
377 
371 
402 
392 
77 
448 
592 
825 

TSVM 

0.9929, 99.96 
0.9651,99.83 
0.9860, 99.23 
0.9820,99.91 
0.9798, 99.90 
0.9628, 98.87 
0.8969, 97.82 
0.9032, 97.68 
0.9240, 97.27 

SMC 

0.9840, 99.91 
0.9046, 99.50 
0.9641, 99.82 
0.9300, 99.63 
0.9419, 99.70 
0.9585, 98.75 
0.8259, 96.65 
0.8420, 95.89 
0.8495, 94.29 

F1,  Accuracy  (%) 

SVMC 

0.9840, 99.91 
0.9204, 99.59 
0.9641, 99.82 
0.9300, 99.63 
0.9396, 99.69 
0.9585, 98.75 
0.8434, 96.89 
0.8705, 96.58 
0.8505,94.15 

OSVM 

0.8457, 99.22 
0.7207, 98.69 
0.7354, 98.82 
0.6921,98.60 
0.7112,98.78 
0.6315,83.32 
0.2028, 36.59 
0.2621,50.11 
0.3384, 32.52 

SVM.NN 

0.0811,97.26 
0.0834, 97.58 
0.0758, 97.55 
0.0902, 97.52 
0.1333,97.64 
0.2434, 36.06 
0.0880, 89.30 
0.0168,86.05 
0.0000,80.14 

T-Time (sec.) 
SMC 
171.77 
75.61 
155.70 
80.47 
98.13 
0.131 
636.70 
749.63 
1181.59 

SVMC 
45.37 
14.34 
29.93 
16.06 
21.57 
0.025 
143.29 
217.85 
296.50 

Table  1:  Performance  results.  \Pu\'  #  of positives  in  U\  T-Time:  Training  Time 

recognition  [Tax and  Duin,  2001].)  We also used Webkb4  for 
Web  page  classification  as  used  in  [Liu  et  ai,  2002]  for  the 
indirect  comparison  with  it.  We  set  up  the  experiment  en(cid:173)
vironment  in  the  same  way  as  [Liu  et  ai,  2002],  except  our 
setup  is  more  realistic:  In  [Liu  et  ai,  2002],  U  is  composed 
of b%  of positives  (e.g.,  student)  and  samples  from  another 
class  (e.g.,  course).  Our  U  is  b%  of positives  (e.g.,  student) 
and  the  remainder  is  from  all  other classes.  (Refer to  [Liu  et 
ai,  2002]  for  the  rest  of the  data  set  description.) 

4.2  Results 
Table  4  shows  the  performance  results.  T S VM  (Traditional 
SVM)  shows  the  ideal  performance  using  S VM  from  P  and 
manually  classified  N  from  U.  SVMJMN  ( S VM  with  Noisy 
Negatives)  is  S VM  from  P,  with  U  as  a  substitute  for  N.  (U 
can  be  thought  of as  a  good  approximation  of N.)  Note  that 
for  T S V M,  SVMJMN,  and  MC  (SMC  and  SVMC),  we  used 
theoretically  motivated  fixed  parameters  without  performing 
explicit  optimization  or  validation  process.  For  O S V M,  we 
thoroughly searched for the best parameters based on the test(cid:173)
ing  data  set  since  optimizing parameters  as  specified  in  [Tax 
and Duin, 2001]  is  infeasibly inefficient especially in high di(cid:173)
mensional  spaces. 

MC  (SMC  and  SVMC)  without  labeled negative data show 
performance  close  to  that  of  T S V M.  SVMC  trains  much 
faster than  SMC  for most  data sets.  The performance of SMC 
and  SVMC  is comparable.  (They differ a little  because of the 
soft  constraints  of S VM  and  noise  in  the  data.)  O S VM  per(cid:173)
forms  fairly  well  on  letter  recognition  and  breast  cancer  (of 
low  dimensionality  with  large  amounts  of data)  but  poor  on 
Webkb  (of high  dimensionality).  S V M . NN  suffers  from  very 
low  F\  scores  because  negative  prediction  dominates  due  to 
many  false positives in  the training data. 

5  Conclusion 
We  present  the  MC  framework  and  its  instance  algorithm 
SVMC,  a new  SCC method from positive and unlabeled data 
(without labeled negative data).  SVMC without labeled nega(cid:173)
tive data computes an accurate classification boundary around 
the  positive  data  using  the  distribution  of unlabeled  data  in  a 
systematic  way. 

4http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-

11/www/wwkb/ 

References 
[Burges,  1998]  C.  J.  C.  Burges.  A  tutorial  on  support vector ma(cid:173)
chines for pattern recognition. Data Mining and Knowledge Dis(cid:173)
covery,  2:\2\-\67,  1998. 

[Chang and Lin, 2001]  C.-C.  Chang  and  C.-J.  Lin.  Training  nu-
support vector classifiers:  Thoery and algorithms.  Neural Com(cid:173)
putation,  13:2119-2147,2001. 

[DeComitee/a/.,  1999]  F.  DeComite,  F.  Denis,  and  R.  Gillcron. 
In  Pwc.  11th 
Positive  and  unlabeled  examples  help  learning. 
Int.  Conf. Algorithmic Learning Theory (ALT'99), pages 219— 
230, Tokyo, Japan, 1999. 

[Denis,  1998]  F.  Denis.  PAC  learning  from  positive  statistical 
queries.  In Proc. 10th Int. Conf. Algorithmic Learning Theory 
(ALT'99), pages  112  126, Otzenhausen, Germany,  1998. 

[Letouzey etal, 2000]  F.  Letouzey,  F.  Denis,  and  R.  Gillcron. 
Learning from positive  and  unlabeled examples.  In  Proc.  11th 
Int. Conf. Algorithmic Learning Theory (ALT'00), pages 11-30, 
Sydney, Australia, 2000. 

[Liu et ai, 2002]  B. Liu, W.  S. Lee, P. S. Yu, and X.  Li.  Partially 
In  Proc.  19th  Int. 
supervised  classification  of text  documents. 
Conf  Machine  Learning  (ICML'02),  pages  387-394,  Sydney, 
Australia, 2002. 

[Manevitz and Yousef, 2001]  L. M. Manevitz and M. Yousef.  One-
class  SVMs  for  document  classification.  Journal  of Machine 
Learning Research, 2:139-154, 2001. 

[Tax and Duin, 2001]  D.  M.  J.  Tax  and  R.  P.  W.  Duin.  Uniform 
object generation  for optimizing one-class classifiers.  Journal of 
Machine Learning Research, 2:155-173, 2001. 

[Tong and Koller, 2000]  S. Tong and D. Koller.  Support vector ma(cid:173)
chine  active  learning with  applications  to  text  classification.  In 
Proc.  17th Int.  Conf Machine Learning (ICML'00), pages 999 
1006, Stanford, CA, 2000. 

[Valiant,  1984]  L. G. Valiant.  A theory of the learnable.  Communi(cid:173)

cations of the ACM, 27:1134-1142, 1984. 

[Yu et ai, 2002]  H. Yu, J. Han, and K. C. Chang.  PEBL: Positive-
example based learning for Web page classification using SVM. 
In Proc. 8th Int.  Conf Knowledge Discovery and Data Mining 
(KDD'02), pages 239-248, Edmonton, Canada, 2002. 

572 

LEARNING 


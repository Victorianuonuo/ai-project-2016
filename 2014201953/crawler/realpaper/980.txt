A Ranking Approach to Pronoun Resolution

Pascal Denis and Jason Baldridge

Department of Linguistics

University of Texas at Austin

{denis,jbaldrid}@mail.utexas.edu

Abstract

We propose a supervised maximum entropy rank-
ing approach to pronoun resolution as an alter-
native to commonly used classiﬁcation-based ap-
proaches. Classiﬁcation approaches consider only
one or two candidate antecedents for a pronoun at
a time, whereas ranking allows all candidates to
be evaluated together. We argue that this provides
a more natural ﬁt for the task than classiﬁcation
and show that it delivers signiﬁcant performance
improvements on the ACE datasets. In particular,
our ranker obtains an error reduction of 9.7% over
the best classiﬁcation approach, the twin-candidate
model. Furthermore, we show that the ranker of-
fers some computational advantage over the twin-
candidate classiﬁer, since it easily allows the in-
clusion of more candidate antecedents during train-
ing. This approach leads to a further error reduction
of 5.4% (a total reduction of 14.6% over the twin-
candidate model).

1 Introduction

Pronoun resolution concerns the identiﬁcation of the an-
tecedents of pronominal anaphors in texts. It is an important
and challenging subpart of the more general task of corefer-
ence, in which the entities discussed in a given text are linked
to all of the textual spans that refer to them. Correct resolu-
tion of the antecedents of pronouns is important for a variety
of other natural language processing tasks, including –but not
limited to– information retrieval, text summarization, and un-
derstanding in dialog systems.

The last decade of research in coreference resolution has
seen an important shift from rule-based, hand-crafted sys-
tems to machine learning systems (see [Mitkov, 2002] for
an overview). The most common approach has been a
classiﬁcation approach for both general coreference resolu-
tion (e.g., [McCarthy and Lehnert, 1995; Soon et al., 2001;
Ng and Cardie, 2002]) and pronoun resolution speciﬁcally
(e.g., [Morton, 2000; Kehler et al., 2004]). This choice is
somewhat surprising given that pronoun resolution does not
directly lend itself to classiﬁcation; for instance, one cannot
take the different antecedent candidates as classes since there

would be far too many overall and also the number of poten-
tial antecedents varies considerably for each anaphor.

Despite this apparent poor ﬁt, a classiﬁcation approach can
be made to work by assuming a binary scheme in which pairs
of candidate antecedents and anaphors are classiﬁed as ei-
ther COREF or NOT-COREF. Many candidates usually need
to be considered for each anaphor, so this approach poten-
tially marks several of the candidates as coreferent with the
anaphor. A separate algorithm must choose a unique an-
tecedent from that set. The two most common techniques are
“Best-First” or “Closest-First” selections (see [Soon et al.,
2001] and [Ng and Cardie, 2002], respectively).

A major drawback with the classiﬁcation approach out-
lined above is that it forces different candidates for the same
pronoun to be considered independently since only a single
candidate is evaluated at a time. The probabilities assigned
to each candidate-anaphor pair merely encode the likelihood
of that particular pair being coreferential, rather than whether
that candidate is the best with respect to the others. To over-
[2003] propose a twin-
come this deﬁciency, Yang et al.
candidate model that directly compares pairs of candidate an-
tecedents by building a preference classiﬁer based on triples
of NP mentions. This extension provides signiﬁcant gains for
both coreference resolution [Yang et al., 2003] and pronoun
resolution [Ng, 2005].

A more straightforward way to allow direct comparison
of different candidate antecedents for an anaphor is to cast
pronoun resolution as a ranking task. A variety of dis-
criminative training algorithms –such as maximum entropy
models, perceptrons, and support vector machines– can be
used to learn pronoun resolution rankers.
In contrast with
a classiﬁer, a ranker is directly concerned with comparing
an entire set of candidates at once, rather than in a piece-
meal fashion. Each candidate is assigned a conditional prob-
ability (or score, in the case of non-probabilistic methods
such as perceptrons) with respect to the entire candidate
[2003] show that a ranking ap-
set. Ravichandran et al.
proach outperforms a classiﬁcation approach for question-
answering, and (re)rankers have been successfully applied
to parse selection [Osborne and Baldridge, 2004; Toutanova
et al., 2004] and parse reranking [Collins and Duffy, 2002;
Charniak and Johnson, 2005].

In intuitive terms, the idea is that while resolving a pro-
noun one wants to compare different candidate antecedents,

IJCAI-07

1588

rather than consider each in isolation. Pronouns have no in-
herent lexical meaning, so they are potentially compatible
with many different preceding NP mentions provided that
basic morphosyntactic criteria, such as number and gender
agreement, are met. Looking at pairs of mentions in isolation
gives only an indirect, and therefore unreliable, way to select
the correct antecedent. Thus, we expect pronoun resolution
to be particularly useful for teasing apart differences between
classiﬁcation and ranking models.

Our results conﬁrm our expectation that comparing all can-
didates together improves performance. Using exactly the
same conditioning information, our ranker provides error re-
ductions of 4.5%, 7.1%, and 13.7% on three datasets over the
twin-candidate model. By taking advantage of the ranker’s
ability to efﬁciently compare many more previous NP men-
tions as candidate antecedents, we achieve further error re-
ductions. These reductions cannot be easily matched by the
twin-candidate approach since it must deal with a cubic in-
crease in the number of candidate pairings it must consider.

We begin by further motivating the use of rankers for pro-
noun resolution. Then, in section (3), we describe the three
systems we are comparing, providing explicit details about
the probability models and training and resolution strategies.
Section (4) lists the features we use for all models. In sec-
tion (5), we present the results of experiments that compare
the performance of these systems on the Automatic Content
Extraction (ACE) datasets.

2 Pronoun Resolution As Ranking

[2003] goes a
The twin-candidate approach of Yang et al.
long way toward ameliorating the deﬁciencies of the single-
candidate approach. Classiﬁcation is still binary, but proba-
bilities are conditioned on triples of NP mentions rather than
just a single candidate and the anaphor. Each triple contains:
(i) the anaphor, (ii) an antecedent mention, and (iii) a non-
antecedent mention.
Instances are classiﬁed as positive or
negative depending on which of the two candidates is the
true antecedent. During resolution, all candidates are com-
pared pairwise. Candidates receive points for each contest
they win, and the one with the highest score is marked as the
antecedent.

The twin-candidate model thus does account for the rela-
tive goodness of different antecedent candidates for the same
pronoun. This approach is similar to error-correcting out-
put coding [Dietterich, 2000], an ensemble learning tech-
nique which is especially useful when the number of output
classes is large.
It can thus be seen as a group of models
that are individual experts on teasing apart two different can-
didates. Nonetheless, the approach is still hampered by the
fact that these models’ probability estimates are only based
on two candidates rather than all that are available. This
means that unjustiﬁed independence assumptions made dur-
ing model training and usage may still hurt performance.

While it is a common and often necessary strategy to adapt
a task to ﬁt a particular modeling approach, pronoun resolu-
tion has in fact been unnecessarily coerced into classiﬁcation
approaches. While the twin-candidate strategy is an improve-
ment over the single-candidate approach, it does not address

the fundamental problem that pronoun resolution is not char-
acterized optimally as a classiﬁcation task. The nature of
the problem is in fact much more like that of parse selec-
tion. In parse selection, one must identify the best analysis
out of some set of parses produced by a grammar. Different
sentences of course produce very different parses and very
different numbers of parses, depending on the ambiguity of
the grammar. Similarly, we can view a text as presenting us
with different analyses (candidate antecedents) which each
pronoun could be resolved to. (Re)ranking models are stan-
dardly used for parse selection (e.g., [Osborne and Baldridge,
2004]), while classiﬁcation has never been explored, to our
knowledge.

In classiﬁcation models, a feature for machine learning
is actually the combination of a contextual predicate1 com-
bined with a class label.
In ranking models, the features
simply are the contextual predicates themselves.
In either
case, an algorithm is used to assign weights to these fea-
tures based on some training material. For rankers, features
can be shared across different outcomes (e.g., candidate an-
tecedents or parses) whereas for classiﬁers, every feature con-
tains the class label of the class it is associated with. This
sharing is part of what makes rerankers work well for tasks
that cannot be easily cast in terms of classiﬁcation: features
are not split across multiple classes and instead receive their
weights based on how well they predict correct outputs rather
than correct labels. The other crucial advantage of rankers
is that all candidates are trained together (rather than inde-
pendently), each contributing its own factor to the training
criterion. Speciﬁcally, for the maximum entropy models used
here the computation of a model’s expectation of a feature
(and the resulting update to its weight at each iteration) is
directly based on the probabilities assigned to the different
candidates [Berger et al., 1996]. From this perspective, the
ranker can be viewed as a straightforward generalization of
the twin-candidate classiﬁer.

The idea of ranking is actually present in the linguistic lit-
erature on anaphora resolution. It is at the heart of Centering
Theory [Grosz et al., 1995] and the Optimality Theory ac-
count of Centering Theory provided by Beaver [2004]. Rank-
ing is also implicit in earlier hand-crafted approaches such as
Lappin and Leass [1994], wherein various factors are manu-
ally given weights, and goes back at least to [Hirst, 1981].

3 The Three Systems

Here, we describe the three systems that we compare: (1)
a single-candidate classiﬁcation system, (2) a twin-candidate
classiﬁcation system, and (3) a ranking system. For each sys-
tem, we give the probability models and the training and res-
olution procedures. All model parameters are estimated using
maximum entropy [Berger et al., 1996]. Speciﬁcally, we es-
timate parameters with the limited memory variable metric
algorithm implemented in the Toolkit for Advanced Discrim-
inative Modeling2 [Malouf, 2002]. We use a Gaussian prior

1Examples of contextual predicates are whether the antecedent is
a proper name in coreference or whether an S → NP VP expansion
occurs in a parse tree in parse selection.

2Available from tadm.sf.net.

IJCAI-07

1589

with a variance of 1000 — no attempt was made to optimize
the prior for each data set or system.

Maxent models are well-suited for the coreference task,
because they are able to handle many different, potentially
overlapping features without making independence assump-
tions. Previous work on coreference using maximum entropy
includes [Kehler, 1997; Morton, 1999; 2000]. In principle,
other discriminative algorithms such as perceptrons and sup-
port vector machines could be used for each of the systems,
though the output would not be probabilistic for these.

The systems are trained and tested on data originally anno-
tated with coreference chains. This means that in principle, an
anaphoric pronoun can have several antecedents. Since pro-
nouns show a strong tendency to take very local antecedents,
we take only the closest antecedent as an anchor when creat-
ing training instances.

We use the following notation for all models: π is an
anaphoric pronoun and A = {α1, . . . , αk} is a set of an-
tecedent candidates. The task of pronoun resolution is to pre-
dict the correct antecedent (cid:2)α for π out of A.

3.1 The Single-candidate Classiﬁer

For the single-candidate classiﬁer, we use the model, training
and test procedures of [Soon et al., 2001].

Model
The single-candidate classiﬁcation approach tackles coref-
erence in two steps by:
(i) estimating the probability,
Pc(COREF|(cid:2)π, αi(cid:3)), of having a coreferential outcome given
a pair of mentions (cid:2)π, αi(cid:3), and (ii) applying a selec-
tion algorithm that will single out a unique candidate out
of the subset of candidates αk for which the probability
Pc(COREF|(cid:2)π, αk(cid:3)) reaches a particular value (typically .5).
Note that in this case, the number of events created for a given
pronoun is just the cardinality of the set of candidates.

Pc(COREF|(cid:2)π, αi(cid:3)) =

n(cid:3)

i=1

exp(

exp(
(cid:3)

λifi((cid:2)π, αi(cid:3), COREF))
n(cid:3)

λifi((cid:2)π, αi(cid:3), c))

(1)

c

i=1

Training
Training instances are constructed based on pairs of mentions
of the form (cid:2)π, αi(cid:3), where π and αi are the descriptions for
an anaphoric pronoun and one of its candidate antecedents,
respectively. Each such pair is assigned either a label COREF
(i.e. a positive instance) or a label NOT-COREF (i.e. a nega-
tive instance) depending on whether or not the two mentions
corefer. In generating the training data, we create for each
anaphoric pronoun: (i) a positive instance for the pair (cid:2)π, αi(cid:3)
where αi is the closest antecedent for π, and (ii) a negative
instance for each pair (cid:2)π, αj (cid:3) where αj intervenes between
αi and π.

Resolution
Once trained, the classiﬁer is used to select a unique an-
tecedent for each anaphoric pronoun in the test documents. In
the Soon et al. [2001] system, this is done for each pronoun
π by scanning the text right to left, and pairing π with each

preceding mention αi. Each test instance (cid:2)π, αi(cid:3) thus formed
is then evaluated by the classiﬁer, which returns a probability
representing the likelihood that these two mentions are coref-
erential. Soon et al. [2001] use “Closest-First” selection: that
is, the process terminates as soon as an antecedent (i.e., a test
instance with probability > .5) is found or the beginning of
the text is reached.

3.2 The Twin-candidate Classiﬁer
The twin-candidate model was proposed by Yang et al. [2003]
in the context of coreference resolution. Ng [2005] more re-
cently used it speciﬁcally for the pronoun resolution task; for
this reason, we adopt his training and test procedures.

Model
With the twin-candidate approach, resolving anaphoric pro-
nouns is also a two step process. The ﬁrst step involves es-
timating the probability Ptc(FIRST|(cid:2)π, αi, αj(cid:3)), of the pro-
noun π corefering to the ﬁrst antecedent candidate αi. Since
this is still binary classiﬁcation, we have the dual probability
Ptc(SECOND|(cid:2)π, αi, αj(cid:3)), which expresses the likelihood of
the pronoun π being coreferential with the second antecedent
candidate αj. As with the single-candidate classiﬁer, the se-
lection of the correct antecedent is done in a separate step
based on the parameters learned by the model. But with the
twin-candidate approach, the antecedent selection algorithm
involves comparing candidates in a pairwise manner.

Ptc(FIRST|(cid:2)π, αi, αj(cid:3)) =

n(cid:3)

i=1

exp(

exp(
(cid:3)

λifi((cid:2)π, αi, αj(cid:3), FIRST))
n(cid:3)

λifi((cid:2)π, αi, αj(cid:3), c))

c

i=1

(2)

Training
Training instances are constructed based on triples of men-
tions of the form (cid:2)π, αi, αj(cid:3), where π describes a pronominal
anaphor and αi and αj are the descriptions for two of its can-
didate antecedents and αi is stipulated to be closer to π than
αj. These instances are labeled either FIRST if αi is the cor-
rect antecedent or SECOND if αj is the correct antecedent.
For this to work, one has to add an additional constraint on
the creation of instances, namely: exactly one and only one
of the two candidates can be coreferential with the pronoun.
Note that the number of instances created is rather large; it is
in fact cubic (since each triple generates two instances) in the
number of mentions in the document if one assumes that all
mentions preceding a pronoun are potential candidates. In or-
der to obviate this problem, Ng [2005] suggests using a win-
dow of 4 sentences including the sentence of the pronoun, and
the immediately preceding three sentences.

Resolution
Once trained, the twin-candidate classiﬁer is used to select a
unique antecedent for the given anaphoric pronoun π. Like
Yang et al. [2003] and Ng [2005], we use a round robin al-
gorithm to compare the members of the candidate set for π.
More speciﬁcally, test instances are created for each pair of
candidates, αi and αj , where αj precedes αi. These instances

IJCAI-07

1590

Pr(αi|π) =

(3)

NEAREST ANTE

n(cid:3)

exp(
(cid:3)

λifi(π, αi))
n(cid:3)

i=1

exp(

λifi(π, αk))

Features of the pronoun
PERS PRO

POSS PRO

THIRD PERS PRO

SPEECH PRO

REFL PRO

PRO FORM

PRO LCONX

PRO RCONX

T if π is a personal pronoun; else F
T if π is a possessive pronoun; else F
T if π is 3rd person pronoun; else F
T if π is 1st, 2nd person pronoun; else F
T if π is a reﬂexive pronoun; else F
T if π is lower-cased pronoun; else F
POS tag of word on the left of π
POS tag of word on the right of π
POS tags of words around π

PRO SCONX
Features of the antecedent candidate
ANTE WD LEN

PRON ANTE

PN ANTE

INDEF ANTE

DEF ANTE

DEM ANTE

QUANT ANTE

ANTE LCONX

ANTE RCONX

ANTE SCONX

ANTE M CT

EMBED ANTE

Relational features
S DIST

NP DIST

NUM AGR

GEN AGR

the number of tokens in α’s string
T if α is a pronoun; else F
T if α is a proper name; else F
T if α is a indeﬁnite NP; else F
T if α is a deﬁnite NP; else F
T if α is a demonstrative NP; else F
T if α is a quantiﬁed NP; else F
POS tag of word on the left of α
POS tag of word on the right of α
POS tags of words around α
number of times α’s string appears
previously in the text
T if α is the nearest NP candidate
compatible in gender, person, and
number; else F
T if α is embedded in another NP;
else F

Binned values for sentence distance
between π and α
Binned values for mention distance
between π and α
T if π and α agree in number; F if they
disagree; UNK if either NP’s number
cannot be determined
T if π and α agree in gender; F if they
disagree; UNK if either NP’s gender
cannot be determined

Table 1: Feature selection for pronoun resolution

For the pronoun features, we encoded into our features in-
formation about the particular type of pronoun (e.g., personal,
possessive, etc.) and the syntactic context of the anaphoric
pronoun. The syntactic context is here approximated as POS
tags surrounding the pronoun. For the antecedent candidates,
we also use information about the type of NP at hand as
well as POS context information. Other features encode the
salience of a given antecedent: whether the candidate NP
string has been seen up to the current point, whether it is the
nearest NP, and whether it is embedded in another larger NP.
Finally, we use features describing the relation between the
anaphoric NP and its candidate antecedent, namely distance
features (in terms of sentences and in terms of NP mentions)
and compatibility features (i.e., number and gender agree-
ment). In addition to the simple features described above, we
used various composite features. More speciﬁcally, we used
features combining: (i) distances and the type of the pronoun
(e.g., reﬂexive, possessive), (ii) the named entity for the an-
tecedent with various information on the pronoun, such as the
pronoun form and the pronoun gender, (iii) the last three char-

are presented to the classiﬁer, which determines which one of
the candidates is preferred; the winner of the comparison gets
one point. Finally, the candidate with the most points at the
termination of the round robin competition gets selected as
the antecedent for π. We use a window of three sentences as
we did in training.

3.3 The Ranker

The following describes our training and resolution proce-
dures for the ranking system.

Model
Viewed as ranking task, pronoun resolution is done in a sin-
gle step, by computing the probability Pr(αi|π), which is the
conditional probability of a particular candidate αi being the
antecedent of the anaphoric pronoun π. Here, a unique event
is created for each pronoun and its entire candidate set A. Fi-
nally, selecting the correct antecedent merely boils down to
picking the most likely candidate in this set.

k

i=1

Training
The training instances for the ranker system are built based
on an anaphoric pronoun π and the set of its antecedent can-
didates A. The candidate set is composed of: (i) the closest
antecedent for π, which is singled out as such, and (ii) a set of
non-antecedents. The construction of the latter set proceeds
by taking the closest antecedent as an anchor and adding all
the non-antecedents that occur in a window of 4 sentences
around it (including the current sentence of the antecedent,
the preceding sentence, and the two following sentences). In
contrast with the previous models, note that the comparison
between the different candidates in A is here directly part of
the training criterion; these are used in the denominator of the
above equation.

Resolution
Once trained, the ranker is used to select a unique antecedent
for each anaphoric pronoun. Given preference shown by pro-
nouns for local resolutions and in order to reduce testing time,
we build our candidate set by taking only the preceding men-
tions that occur in a window of 4 sentences, including the pro-
noun’s sentence and the 3 sentences preceding it. The ranker
provides a probability distribution for the entire candidate set,
and the candidate with the highest conditional probability is
chosen as the antecedent. In cases of ties, the alternative that
is the closest to the pronoun is chosen.

4 Feature selection

In this study, we focused on features obtainable with very
limited linguistic processing. Our features fall into three main
categories: (i) features of the anaphoric pronoun, (ii) features
of antecedent candidate NP, and (iii) relational features (i.e.,
features that describe the relation between the two mentions).
The detailed feature set is summarized in table 1.

IJCAI-07

1591

acters in the antecedent head word and the pronoun form and
gender.

5 Experiments and Results

5.1 Corpus and evaluation
For evaluation, we used the datasets from the ACE corpus
(Phase 2). This corpus is divided into three parts, correspond-
ing to different genres: newspaper texts (NPAPER), newswire
texts (NWIRE), and broadcasted news transcripts (BNEWS).
Each of these is split into a train part and a devtest part.
We used the devtest material only once, namely for test-
ing. Progress during the development phase was estimated
only by using cross-validation on the training set for the NPA-
PER section.

In our experiments, we used all forms of personal (all per-
sons) and possessive pronouns that were annotated as ACE
“markables”, i.e., the pronouns associated with the follow-
ing named entity types: FACility, GPE (geo-political entity),
LOCation, ORGanization, PERson, VEHicle, WEApons. This
excludes pleonastics and references to eventualities or to non-
ACE entities. Together, the three ACE datasets contain 7263
and 1866 such referential pronouns, for training and testing,
respectively.

Finally, note that in building our antecedent candidate sets,
we restricted ourselves to the true ACE mentions since our fo-
cus is on evaluating the classiﬁcation approaches versus the
ranking approach rather than on building a full pronoun res-
olution system. It is worth noting that previous work tends to
be vague in both these respects: details on mention ﬁltering
or providing performance ﬁgures for markable identiﬁcation
are rarely given.

No human-annotated linguistic information is used in the
input. The corpus text was preprocessed with the OpenNLP
Toolkit3 (i.e., a sentence detector, a tokenizer, a POS tagger,
and a Named Entity Recognizer).

Following common practice in pronoun resolution, we re-
port results in terms of accuracy, which is simply the ratio of
correctly resolved anaphoric pronouns. Since the ACE data is
annotated with coreference chains, we assumed that correctly
resolving a pronoun amounts to selecting one of the previous
elements in chain as the antecedent.

5.2 Comparative results
The results obtained for the three systems on the three ACE
datasets are summarized in table (2).

System BNEWS
SCC
TCC
RK

62.2
68.6
72.9

NPAPER

NWIRE

70.7
74.6
76.4

68.3
71.1
72.4

Table 2: Accuracy scores for the single-candidate classiﬁer
(SCC), the twin-candidate classiﬁer (TCC), and the ranker
(RK).

As shown by this table, the ranker system signiﬁcantly out-
performs the two classiﬁer systems, with an overall f-score

3Available from opennlp.sf.net.

of 74.0%. This corresponds to average (weighted) improve-
ments of 7.2% (i.e., an error reduction of 21%) over the
single-candidate classiﬁer and of 2.8% (i.e., an error reduc-
tion of 9.7%) over the twin-candidate classiﬁer. The scores
obtained for the ﬁrst dataset NPAPER are substantially better
than for the two other datasets; we suspect that this difference
is due to the fact that we only did development on that dataset.

5.3 Additional results
In this section, we discuss an additional experiment aimed at
getting additional insight into the potential of the ranker. In
the previous experiments, we provided a rather limited con-
text for training: we only considered mentions in a window of
4 sentences around the correct antecedent. Our main motiva-
tion for doing this was to stay as close as possible to the train-
ing conditions given in [Ng, 2005] for the twin-candidate ap-
proach, thereby giving it the fairest comparison possible. An
open question is to what extent can widening the window of
antecedent candidates help the ranker to learn better parame-
ters for pronoun resolution. To answer this question, we ran
an experiment on the same three ACE datasets and widened
the window of sentences by collecting, in addition to the clos-
est antecedent, all non-antecedents preceding the anaphor up
to 10 sentences before the antecedent:4 The results for this
experiment are reported in table (3):

System
RK (w = 10)

BNEWS

NPAPER

NWIRE

73.0

77.6

75.0

Table 3: Accuracy scores for the ranker (RK) with a window
of 10 sentences.

These ﬁgures show a signiﬁcant improvement on the ﬁrst
two datasets, with an average score of 75.4%. This translates
into an average gain of 1.4% or an error reduction of 5.4%.

6 Conclusions

We have demonstrated that using a ranking model for pro-
noun resolution performs far better than a classiﬁcation
model. On the three ACE datasets, the ranker achieves an er-
ror reductions of 9.7% over the twin-candidate classiﬁer, even
when both have exactly the same features and experimental
settings (e.g., number of sentences from which to consider
candidates). Our results thus corroborate Ravichandran et
al.’s [2003] similar ﬁnding that ranking outperforms classiﬁ-
cation for question-answering. Clearly, the ability to consider
all potential antecedents together, rather than independently,
provides the ranker with greater discriminating power.

The round robin nature of the pairwise contests in the twin-
candidate approach imposes a restrictive computational cost
on its use which limits the number of NP mentions that can
be considered in a candidate set. The ranker does not suffer
from this limitation, and we show that the ranker achieves a
further error reduction of 5.4% (or total of 14.6% over the
twin-candidate model) by increasing the size of the candidate
set used in training.

4As for the other experiments, we use a Gaussian prior of vari-
ance 1000, and we maintain the window of 4 sentences for testing.

IJCAI-07

1592

The most direct comparison with previous results is with
Ng [2005], who obtained 76.6% and 81.9% on the newspaper
and newswire parts of ACE. Our best results for these parts
were 77.6% and 75.0%. Though our focus is on comparing
classiﬁcation versus ranking, it is nonetheless interesting that
we match Ng’s model on the newspaper texts since we use a
much simpler feature set and only a single model rather than a
complex ensemble. We would thus expect the use of a ranker
in place of the twin-candidate classiﬁer would achieve further
improvements for his set-up.

The main difference between the twin-candidate approach
and the ranking approach is that under the former, candidates
are compared by pairs (the best candidate is the one that has
won the most times), whereas in the latter an ordering is im-
posed on the entire set at once. A potential advantage of the
ranking approach is that it could allow one to deﬁne features
on the candidate set itself. Another advantage of the ranker
over the preference classiﬁer is how ranking is obtained: only
the ranker guarantees a global winner.

While our ranker outperforms the classiﬁers outright, some
beneﬁt could be gained by using both approaches together. It
would be straightforward to integrate classiﬁers and rankers
in an ensemble model. For example, a ranker could use the
results of the classiﬁer as features in its model.

Acknowledgments

We would like to thank the four anonymous reviewers for
their comments. This work was supported by NSF grant IIS-
0535154.

References
[Beaver, 2004] David Beaver. The optimization of discourse

anaphora. Linguistics and Philosophy, 27(1), 2004.

[Berger et al., 1996] A. Berger, S. Della Pietra, and V. Della
Pietra. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 22(1):39–
71, 1996.

[Charniak and Johnson, 2005] Eugene Charniak and Mark
Johnson. Coarse-to-ﬁne n-best parsing and maxent dis-
criminative reranking. In Proceedings of ACL, Ann Arbor,
MI, 2005.

[Collins and Duffy, 2002] Michael Collins and Nigel Duffy.
New ranking algorithms for parsing and tagging: Kernels
over discrete structures and the voted perceptron. In Pro-
ceedings of ACL, pages 263–270, Philadelphia, PA, 2002.

[Dietterich, 2000] Thomas Dietterich. Ensemble methods in
machine learning. In J. Kittler and F. Roli, editors, First In-
ternational Workshop on Multiple Classiﬁer Systems, Lec-
ture Notes in Computer Science, pages 1–15, New York,
2000. Springer Verlag.

[Grosz et al., 1995] Barbara Grosz, Aravind Joshi, and Scott
Weinstein. Centering: A framework for modelling the lo-
cal coherence of discourse. Computational Linguistics,
2(21), 1995.

[Hirst, 1981] Graeme Hirst. Anaphora in Natural Language

Understanding: A Survey. Springer-Verlag, 1981.

[Kehler et al., 2004] A. Kehler, D. Appelt, L. Taylor, and
A. Simma. The (non)utility of predicate-argument fre-
quencies for pronoun interpretation.
In Proceedings of
HLT/NAACL, pages 289–296, 2004.

[Kehler, 1997] Andrew Kehler. Probabilistic coreference in
information extraction. In Proceedings of EMNLP, pages
163–173, 1997.

[Lappin and Leass, 1994] Shalom Lappin and Herbert J. Le-
ass. An algorithm for pronominal anaphora resolution.
Computational Linguistics, 20(4):535–561, 1994.

[Malouf, 2002] Robert Malouf. A comparison of algorithms
for maximum entropy parameter estimation. In Proceed-
ings of the Sixth Workshop on Natural Language Learning,
pages 49–55, Taipei, Taiwan, 2002.

[McCarthy and Lehnert, 1995] Joseph F. McCarthy and
Wendy G. Lehnert. Using decision trees for coreference
resolution.
In Proceedings of IJCAI, pages 1050–1055,
1995.

[Mitkov, 2002] Ruslan Mitkov. Anaphora Resolution. Long-

man, Harlow, UK, 2002.

[Morton, 1999] Thomas Morton. Using coreference for
question answering. In Proceedings of ACL Workshop on
Coreference and Its Applications, 1999.

[Morton, 2000] Thomas Morton. Coreference for NLP ap-

plications. In Proceedings of ACL, Hong Kong, 2000.

[Ng and Cardie, 2002] Vincent Ng and Claire Cardie.

Im-
proving machine learning approaches to coreference reso-
lution. In Proceedings of ACL, pages 104–111, 2002.

[Ng, 2005] Vincent Ng. Supervised ranking for pronoun res-
In Proceedings of

olution: Some recent improvements.
AAAI), 2005.

[Osborne and Baldridge, 2004] Miles Osborne and Jason
Baldridge. Ensemble-based active learning for parse se-
lection.
In Proceedings of HLT/NAACL, pages 89–96,
Boston, MA, 2004.

[Ravichandran et al., 2003] Deepak Ravichandran, Eduard
Hovy, and Franz Josef Och. Statistical QA - classiﬁer vs
re-ranker: What’s the difference? In Proceedings of the
ACL Workshop on Multilingual Summarization and Ques-
tion Answering–Machine Learning and Beyond, 2003.

[Soon et al., 2001] W. Soon, H. Ng, and D. Lim. A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521–544, 2001.

[Toutanova et al., 2004] Kristina

Penka
Markova, and Christopher Manning.
The leaf pro-
jection path view of parse trees: Exploring string kernels
for HPSG parse selection.
In Proceedings of EMNLP,
pages 166–173, Barcelona, 2004.

Toutanova,

[Yang et al., 2003] X. Yang, G. Zhou, J. Su, and C.L. Tan.
Coreference resolution using competitive learning ap-
proach. In Proceedings of ACL, pages 176–183, 2003.

IJCAI-07

1593


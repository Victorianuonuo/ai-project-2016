Semi-supervised Gaussian Process Classiﬁers

Vikas Sindhwani

Department of Computer Science

Wei Chu

CCLS

S. Sathiya Keerthi

Yahoo! Research

University of Chicago

Chicago, IL 60615, USA
vikass@cs.uchicago.edu

Columbia University

New York, NY 10115, USA

chuwei@gatsby.ucl.ac.uk

3333 Media Studios North
Burbank, CA 91504, USA
selvarak@yahoo-inc.com

Abstract

In this paper, we propose a graph-based construc-
tion of semi-supervised Gaussian process classi-
ﬁers. Our method is based on recently proposed
techniques for incorporating the geometric proper-
ties of unlabeled data within globally deﬁned ker-
nel functions. The full machinery for standard
supervised Gaussian process inference is brought
to bear on the problem of learning from labeled
and unlabeled data. This approach provides a nat-
ural probabilistic extension to unseen test exam-
ples. We employ Expectation Propagation proce-
dures for evidence-based model selection.
In the
presence of few labeled examples, this approach is
found to signiﬁcantly outperform cross-validation
techniques. We present empirical results demon-
strating the strengths of our approach.

1 Introduction

Practitioners of machine learning methods frequently en-
counter the following situation:
large amounts of data can
be cheaply and automatically collected, but subsequent label-
ing requires expensive and fallible human participation. This
has recently motivated a number of efforts to design semi-
supervised inference algorithms that aim to match the per-
formance of well-trained supervised methods, using a small
pool of labeled examples and a large collection of unlabeled
data. The scarcity of labeled examples greatly exaggerates the
need to incorporate additional sources of prior knowledge, to
perform careful model selection, and to deal with noise in la-
bels. A Bayesian framework is ideally suited to handle such
issues. In this paper, we construct semi-supervised Gaussian
processes, and demonstrate and discuss its practical utility on
a number of classiﬁcation problems.

Our methods are based on the geometric intuition that for
many real world problems, unlabeled examples often iden-
tify structures, such as data clusters or low dimensional man-
ifolds, whose knowledge may potentially improve inference.
For example, one might expect high correlation between class
labels of data points within the same cluster or nearby on a
manifold. These, respectively, are the cluster and manifold
assumptions for semi-supervised learning.

We utilize the graph-based construction of semi-supervised
kernels in [Sindhwani et al., 2005]. The data geometry is
modeled as a graph whose vertices are the labeled and unla-
beled examples, and whose edges encode appropriate neigh-
borhood relationships. In intuitive terms, in the limit of inﬁ-
nite unlabeled data we imagine a convergence of this graph
to the underlying geometric structure of the probability dis-
tribution generating the data. The smoothness enforced by
regularization operators (such as the graph Laplacian) over
functions on the vertices of this graph is transferred onto a Re-
producing Kernel Hilbert space (RKHS) of functions deﬁned
over the entire data space. This gives rise to a new RKHS
in which standard supervised kernel methods perform semi-
supervised inference. In this paper, we apply similar ideas
for Gaussian processes, with an aim to draw the beneﬁts of
a Bayesian approach when only a small labeled set is avail-
able. The semi-supervised kernel of [Sindhwani et al., 2005]
is motivated through Bayesian considerations, and used for
performing Gaussian process inference. We apply expecta-
tion propagation (EP) (see [Rasmussen and Williams, 2006]
and references therein) for approximating posterior processes
and calculating evidence for model selection.

We point out some aspects of the proposed method:
a) Our method, hereafter abbreviated as SSGP, can be
seen as providing a Bayesian analogue of Laplacian Sup-
port Vector Machines (LapSVM) and Laplacian Regularized
Least Squares (LapRLS) proposed in [Sindhwani et al., 2005;
Belkin et al., 2006] and semi-supervised logistic regression
proposed in [Krishnapuram et al., 2004].
In this paper,
we empirically demonstrate that when labeled examples are
scarce, model selection by evidence in SSGP is a signiﬁcant
improvement over cross-validation in LapSVM and LapRLS.
b) Several graph-based Bayesian approaches incorporat-
ing unlabeled data have recently been proposed. Methods
have been designed for transductive Bayesian learning us-
ing a graph-based prior in [Zhu et al., 2003; Kapoor et al.,
2005]. Since the core probabilistic model in these methods
is deﬁned only over the ﬁnite collection of labeled and un-
labeled inputs, out-of-sample extension to unseen test data
requires additional follow-up procedures. By contrast, our
approach possesses a Gaussian process model over the entire
input space that provides natural out-of-sample prediction.

c) While the focus of this paper is to derive SSGP and ver-
ify its usefulness on binary classiﬁcation tasks, we wish to

IJCAI-07

1059

comment on its potential in the following respects: (1) SSGP
also provides a general framework for other learning tasks,
such as regression, multiple-class classiﬁcation and ranking.
(2) SSGP also produces class probabilities and conﬁdence es-
timates. These can be used to design active learning schemes.
(3) Efﬁcient gradient-based strategies can be designed for
choosing multiple model parameters such as regularization
and kernel parameters.

We begin by brieﬂy reviewing Gaussian process classiﬁca-
tion in Section 2 and then propose and evaluate SSGP in the
following sections.

2 Background and Notation

In the standard formulation for learning from examples, pat-
terns x are drawn from some space X , typically a subset
and associated labels y come from some space Y.
of Rd
For ease of presentation, in this paper we will focus on bi-
nary classiﬁcation, so that Y can be taken as {+1, −1}. We
assume there is an unknown joint probability distribution
PX ×Y over the space of patterns and labels. A set of pat-
(cid:2)
terns {xi}l+u
i=1 is drawn i.i.d from the marginal PX (x) =
Y PX ×Y(x, y); the label yi associated with xi is drawn
from the conditional distribution PY (y|xi) for the ﬁrst l pat-
terns. We are interested in leveraging the unlabeled patterns
{xj}l+u

j=l+1 for improved inference.

i=1, XU = {xi}l+u

To set our notation for the discussion ahead, we will denote
XL = {xi}l
i=1 as the set of labeled examples with associated
labels YL = {yi}l
i=l+1 as the set of un-
labeled patterns. We further denote all the given patterns as
XD, i.e., XD = XL ∪ XU . Other patterns that are held out
for test purposes (or have not been collected yet) are denoted
as XT . The set of all patterns (labeled, unlabeled and test) is
denoted as X, i.e., X = XD ∪ XT , but we will often also use
X to denote a generic dataset.

In the standard Gaussian process setting for supervised
learning, one proceeds by choosing a covariance function
K : X × X (cid:3)→ (cid:5). With any point x ∈ X , there is an as-
sociated latent variable fx. Given any dataset X, the latent
variables fX = {fx}x∈X are treated as random variables in a
zero-mean Gaussian process indexed by the data points. The
covariance between fx and fz is fully determined by the co-
ordinates of the data points x and z, and is given by K(x, z).
Thus, the prior distribution over these variables is a multi-
variate Gaussian, written as: P(fX ) = N (0, ΣXX ) where
ΣXX is the covariance matrix with elements K(x, z) with
x, z ∈ X.

The Gaussian process classiﬁcation model relates the vari-
able fx at x to the label y through a probit noise model
such as y = sign(fx + ξ) where ξ ∼ N (0, σ2
n). Given
the latent variables fL associated with labeled data points, the
class labels YL are independent Bernoulli variables and their
joint likelihood is given by: P(YL|fL) =
P(yi|fxi) =
where Φ is the cumulative density function

(cid:3)l

(cid:3)l

(cid:4)

(cid:5)

i=1 Φ

i=1

yifxi
σn

of the normal distribution.

i=1

(cid:3)l
P(yi|fxi)N (0, ΣLL)/P (YL) where we use ΣLL as a
(cid:6)
shorthand for ΣXLXL . The normalization factor P(YL) =
P(YL|fL)P(fL)dfL is known as the evidence for the model
parameters (such as parameters of the covariance function
and the noise level σ2

n).

The posterior distribution above is non-Gaussian. To pre-
serve computational tractability, a family of inference tech-
niques can be applied to approximate the posterior as a Gaus-
sian. Some popular methods include Laplace approximation,
mean-ﬁeld methods, and expectation propagation (EP). In
this paper we will use the EP procedure, based on empiri-
cal ﬁndings that it outperforms Laplace approximation. In
EP, the key idea is to approximate the non-Gaussian part in
the posterior in the form of an un-normalized Gaussian, i.e.
P(yi|fxi) ≈ c N (μ, A). The parameters c, μ, A are
obtained by locally minimizing the Kullback-Liebler diver-
gence between the posterior and its approximation.

(cid:3)l

i=1

P(fxt

With a Gaussian approximation to the posterior in hand,
(cid:6)
the distribution of the latent variable fxt at a test point xt, as
given by the following integral, becomes a tractable quantity:
|fL)P(fL|YL)dfL ≈ N (μt, σ2
|YL) =
P(fxt
t ),
−1
−1
LL −
where μt = ΣT
LL μ and σ2
−1
Σ
the column vector ΣLt =
LL) ΣLt. Here,
[K(xt, x1), . . . , K(xt, xl)]T
and ΣLL is the gram matrix
of K over the labeled inputs. The conditional distribution
−1
P(fxt
LLfL
and covariance matrix K(xt, xt) − ΣT

|fL) is a multi-variate Gaussian with mean ΣT

t = K(xt, xt) − ΣT

−1
LL A Σ

−1
LLΣLt.

Lt (Σ

LtΣ

LtΣ

LtΣ

(cid:5)

(cid:4)

(cid:7)

Finally, one can compute the Bernoulli distribution over
the test label yt, which for the probit noise model becomes
P(yt|YL) = Φ
Gaussian processes, we point the reader to [Rasmussen and
Williams, 2006] and references therein.

For more details on

n + σ2
σ2
t

μt/

.

Our interest now is to utilize unlabeled data for Gaussian

process inference.

3 Semi-supervised Gaussian Processes

3.1 Kernels for Semi-supervised Learning
A symmetric positive semi-deﬁnite function K(·, ·) can serve
as the covariance function of a Gaussian process and also as a
kernel function of a deterministic Reproducing Kernel Hilbert
Space (RKHS) H of functions X → (cid:5). The RKHS and the
GP associated with the same function K(·, ·) are closely re-
lated through a classical isometry between H and a Hilbert
space of random variables spanned by the GP (i.e. random
k αkfxk and their mean square lim-
variables of the form
its)1. In this section, we review the construction of an RKHS
that is adapted for semi-supervised learning of deterministic
classiﬁers [Sindhwani et al., 2005]. The kernel of this RKHS
is then used as the covariance function of SSGP.

(cid:2)

In the context of learning deterministic classiﬁers, for
many choices of kernels, the norm (cid:9)f (cid:9)H can be interpreted as
a smoothness measure over functions f in H. The norm can
then be used to impose a complexity structure over H and

Combining this likelihood term with the Gaussian prior
for the latent variables associated with the labeled dataset
XL, we obtain the posterior distribution: P(fL|YL) =

1For an RKHS function f , the notation f (x) means the function
f evaluated at x, whereas for a GP fx refers to the latent random
variable associated with x.

IJCAI-07

1060

(cid:2)l

learning algorithms can be developed based on minimizing
functionals of the form: V (f, XL, YL) + γ(cid:9)f (cid:9)2
H, where V is
a loss function that measures how well f ﬁts the data. Re-
markably, for loss functions that only involve f through point
evaluations, a representer theorem states that the minimizer is
i=1 αiK(x, xi) so that only the αi re-
of the form f (x) =
main to be computed. This provides the algorithmic basis of
algorithms like Regularized Least Squares (RLS) and Support
Vector Machines (SVM), for squared loss and hinge loss, re-
spectively. In a semi-supervised setting, unlabeled data may
suggest alternate measures of complexity, such as smoothness
with respect to data manifolds or clusters. Thus, if the space
H contains functions that are smooth in these respects, it is
only required to re-structure H by reﬁning the norm using
labeled and labeled data XD. A general procedure to per-
form this operation is as follows: we deﬁne ˜H to be the space
of functions from H with the modiﬁed data-dependent inner
product: (cid:10)f, g(cid:11) ˜H = (cid:10)f, g(cid:11)H + f
T M g, where f and g are
the vectors {f (x)}x∈XD and {g(x)}x∈XD respectively, and
M is a symmetric positive semi-deﬁnite matrix. The norm
induced by this modiﬁed inner product combines the origi-
nal ambient smoothness with an intrinsic smoothness mea-
sure deﬁned in terms of the matrix M . The deﬁnition of M
is based on the construction of a data adjacency graph that
acts as an empirical substitute of the intrinsic geometry of
the marginal PX . M can be derived from the graph Lapla-
cian L; for example, M = L or M =
are pop-
ular choices for families of graph regularizers. The Lapla-
cian matrix of the graph implements an empirical version of
the Laplace-Beltrami operator when the underlying space is
a Riemannian manifold. This operator measures smoothness
with respect to the manifold. The space ˜H can be shown to
be an RKHS. With the new data-dependent norm, ˜H becomes
better suited, as compared to the original function space H,
for semi-supervised learning tasks where the cluster/manifold
assumptions hold. The form of the new kernel ˜K associated
with ˜H can be derived (see [Sindhwani et al., 2005]) in terms
of the kernel function K using reproducing properties of an
RKHS and orthogonality arguments, and is given by:

p βpLp

(cid:2)

˜K(x, z) = K(x, z) − ΣT

Dx(I + M ΣDD)

(1)
where ΣDx (and similarly ΣDz) denotes the column vector:
[K(x1, x), . . . , K(xl+u, x)]
. Laplacian SVM and Lapla-
cian RLS solve regularization problems in the RKHS ˜H with
hinge and squared loss respectively.

−1M ΣDz

T

3.2 Data-dependent Conditional Prior
SSGP uses the semi-supervised kernel function ˜K deﬁned
above as a covariance function for Gaussian process learn-
ing. Thus, in SSGP the covariance between fxi and fxj not
only depends on the ambient coordinates of xi and xj , but
also on geometric properties of the set XD. In this section,
we discuss the derivation of ˜K from Bayesian considerations.
Our general approach is summarized as follows. We be-
gin with a standard Gaussian process. Given unlabeled and
labeled data XD, we deﬁne a joint probability distribution
over the associated latent process variables fD and an ab-
stract collection of random variables, denoted as G, whose

instantiation is interpreted as realization of a certain geom-
etry. Then, conditioning on the geometry of unlabeled data
through these variables, a Bayesian update gives a posterior
over the latent variables. SSGP is the resulting posterior pro-
cess, which incorporates the localized spatial knowledge of
the data via Bayesian learning.

There are many ways to deﬁne an appropriate likelihood
evaluation for the geometry variables G. One simple formu-
lation for P(G|fD) is given by:

(cid:9)

P(G|fD) =

1
Z exp

T
D M pD

p

(2)

(cid:8)
− 1
2

where pD = [P(y = 1|fx1), . . . , P(y = 1|fxl+u)]T =
[Φ(fx1), . . . , Φ(fxl+u)]T
is a column vector of conditional la-
bel probabilities for latent variables associated with XD, M
is a graph-based matrix such as the graph Laplacian in Sec-
tion 3.1, and Z is a normalization factor. P(G|fD) may be
interpreted as a measure of how much fD corroborates with
a given geometry, computed in terms of the smoothness of
the associated conditional distributions with respect to unla-
beled data. This implements a speciﬁc assumption about the
connection between the true underlying unknown conditional
and marginal distributions – that if two points x1, x2 ∈ X are
close in the intrinsic geometry of PX , then the conditional
distributions P(y|x1) and P(y|x2) are similar, i.e., as a func-
tion of x, P(y|x) is smooth with respect to PX .

Since the likelihood form (Eqn 2) leads to a non-Gaussian
posterior, a natural substitute is to use an un-normalized
Gaussian form, as commonly used with EP approximations.
The posterior is then approximated as:

(cid:9)

P (fD|G) ≈ c exp

T
D M fD
f

P(fD)/P (G)

(3)

(cid:8)
− 1
2

Such a form captures the functional dependency between the
latent and the geometry variables while rendering subsequent
computations tractable. The value of c is inconsequential in
deﬁning this approximate posterior distribution (since c can-
cels due to the normalizing term P (G)), although it is impor-
tant for evidence computations. In Section 3.3, we further
comment on the role of the c and propose the use of partial
evidence for model selection. The matrix M is approximated
by the Laplacian matrix of the data-adjacency graph, to cor-
respond to the deterministic algorithms introduced in [Sind-
hwani et al., 2005; Belkin et al., 2006]. We note that c, M
can alternatively be computed from EP calculations, leading
to a novel graph regularizer and tractable computations for
the full evidence. We outline more details in this direction
in [Chu et al., 2006].

To proceed further, we make the assumption that given fD,
G is independent of latent variables at other points, i.e., if the
data set X contains XD and a set of unseen test points XT ,
we have:

P(G|fX ) = P(G|fD)

(4)

This assumption allows out of sample extension without the
need to recompute the graph for a new dataset.

The posterior distribution of fX given G is: P(fX |G) ∝

P(G|fX )P(fX ) = P(G|fD)P(fX ).

IJCAI-07

1061

The prior distribution for fX is a Gaussian N (0, ΣXX )
(cid:11)(cid:9)
given by the standard Gaussian processes.
In the form
of block matrices, the prior P(fX ) can be written as fol-

(cid:8)(cid:10)

(cid:11)

(cid:10)

(cid:11)

(cid:10)

lows: fX =

fD
fT

∼ N

0
0

,

ΣDD ΣDT
ΣT
DT ΣT T

,

where we use the shorthand D for XD and T for XT .
The posterior distribution of fX conditioned on G can be
written as a zero-mean Gaussian distribution P(fX |G) ∝
exp(− 1

−1
XX fX ) where

T
X ˜Σ

2 f

(cid:10)

(cid:11)−1

(cid:10)

(cid:11)

˜Σ

−1
XX =

ΣDD ΣDT
ΣT
DT ΣT T

+

M 0
0
0

Proposition: Given Eqn 3, for any ﬁnite collection of
the random variables fX = {f (x)}x∈X
data points X,
conditioned on G have a multivariate normal distribution
N (0, ˜ΣXX ), where ˜ΣXX is the covariance matrix whose el-
ements are given by evaluating the following kernel function
˜K : X × X (cid:3)→ (cid:5),

˜K(x, z) = K(x, z) − ΣT

(5)
for x, z ∈ X . Here ΣDx denotes the column vector
[K(x1, x), . . . , K(xn, x)]T

Dx(I + M ΣDD)

−1M ΣDz

.

This proposition shows that the Gaussian process condi-
tioned on the geometry variable G is a Gaussian process with
a modiﬁed covariance function ˜K. A proof of this result uses
straightforward matrix algebra and is omitted for brevity.

SSGP is the posterior process obtained by conditioning the
original GP with respect to G. Note that the form of its co-
variance function ˜K is the same as in Eqn 1, which is derived
from properties of RKHS.

(cid:4)

3.3 Model Selection
Model selection for SSGP involves choosing the kernel pa-
rameters and the noise variance σn (see Section 2). The def-
inition of ˜K is based on the choice of a covariance function
K and a graph regularization matrix M . As in [Sindhwani
et al., 2005], in this paper we restrict our attention to the fol-
γI
γA Lp
lowing choices: K(x, z) = exp
˜K. The parameters γA and γI balance
and use the kernel
ambient and intrinsic covariance. The parameters related to
the computation of the graph Laplacian L are – the number
of nearest neighbors N N and the graph adjacency matrix W .
We set Wij = exp
in a N N -nearest neighbor graph and zero otherwise.

, if xi and xj are adjacent

− (cid:3)xi−xj (cid:3)2

− (cid:3)x−z(cid:3)2

, M =

(cid:5)

(cid:4)

(cid:5)

1
γA

2σ2
t

2σ2

Denote φ as the collection of model parameters. The opti-
mal values of φ are determined by maximizing the evidence
P(YL|G, φ) which is available from EP computations.

Model selection is particularly challenging in semi-
supervised settings in the presence of few labeled examples.
Popular model selection techniques like cross-validation (for
deterministic methods like Laplacian SVM/RLS) require sub-
sets of the already-scarce labeled data to be held out from
training. Moreover, for graph-based semi-supervised learn-
ing, a proper cross-validation should also exclude the held
out subset from the adjacency graph instead of simply sup-
pressing labels, in order to ensure good out-of-sample exten-
sion. This adds a major expense since the semi-supervised

Table 1: Datasets with d features, n training examples (la-
beled+unlabeled) and t test examples.

DATASET
MOONS
3VS5
SET1

PCMAC

REUTERS

d

2

256
241
7511
9930

n

120
1214
1126
1460
458

t

40401

326
374
486
152

DOMAIN

SYNTHETIC
USPS DIGIT

IMAGE RECOGNITION

20-NEWGROUPS
REUTERS-21578

kernel in Eqn 1 needs to be recomputed multiple times. Also,
since cross-validation is based on counts, it may often be un-
able to uniquely identify a good set of parameters. By con-
trast, evidence maximization neither needs to suppress expen-
sive labels nor needs to hold out labeled examples from the
graph. As a continuous function of many model parameters, it
is more precise in parameter selection (demonstrated in Sec-
tion 4) and also amenable to gradient-based techniques. In
addition to these beneﬁts, we note the novel possibility of em-
ploying unlabeled data for model selection by maximizing the
full evidence P(YL, G|φ) = P(YL|G, φ)P(G|φ). The latter
P(G|fD)P(fD)dfD.
term may be computed as P(G|φ) =
Given Eqn 3, one can immediately derive log P(G|φ) =
log c − 1
2 log det(M ΣDD + I) where I is the identity
matrix. For simplicity, in this paper we focus on compar-
ing standard evidence maximization in SSGP with standard
cross-validation in Laplacian SVM/RLS.

(cid:6)

4 Experiments

We performed experiments on a synthetic two-dimensional
dataset and four real world datasets for binary classiﬁcation
problems. The statistics of these datasets are described in
Table 1.

Synthetic Data
The toy 2-D dataset MOONS, meant to visually demonstrate
SSGP, is shown in Figure 1 as a collection of unlabeled
examples (small black dots) and two labeled examples per
class (large colored points). The classiﬁcation boundary and
the contours of the mean of the predictive distribution over
the unit square are shown for standard GP and SSGP (Fig-
ure 1(a,c)). These plots demonstrate how SSGP utilizes unla-
beled data for classiﬁcation tasks. The uncertainty in predic-
tion (Figure 1(d)) respects the geometry of the data, increas-
ing anisotropically as one moves away from the labeled ex-
amples. Figure 1(b) also demonstrates evidence-based model
selection.

Real World Data sets
For all the real world data sets (except 3VS5), we generated
a ﬁxed training/test split by randomly drawing one-fourth of
the original data set into a test set of unseen examples, and
retaining the rest as a training set of labeled and unlabeled
examples. Learning curves were plotted for varying amount
of labeled data, randomly chosen from the training set. For
3VS5, we chose the exact data splits used in [Lawrence and
Jordan, 2004] to facilitate comparison between algorithms.

To reduce the complexity of model selection, we ﬁx N N =
10 and p = 2 for 3VS5, SET1 and N N = 50 and p = 5 for

IJCAI-07

1062

Mean of the Predictive Distribution of Supervised Learning

1.5

1

0.5

0

−0.5

−1

0

0.6

0.4
0.2

0

−0.2

−0.4

−0.6

−0.6
−0.4

−0.2

(a)

0.2

0.6

0.4

0

−1.5

−1.5

−1

−0.5

0

0.5

1

1.5

2

2.5

Mean of the Predictive Distribution of Semi−supervised Learning

Logrithm of the Evidence of Semi−supervised Learning
6

(b)

4.75

3.5

t

σ

2

g
o
−

l

2.25

1

−3

−3.5

−4

−4.5

−5

1

2.25

3.5
σ

−log2

4.75

6

Entropy of the Predictive Distribution of Semi−supervised Learning

1.5

1

−0.2

0.5

0

−0.3

−0.5

−0.2

−1

0

−0.1

−0.1

−0.1

0.3

0.2

0.1

−0.4
−0.3

−0.2
0

0.1

(c)

0.2

0.4

1

0.5

0

0.3

0.1

0.2

−0.5

−1

−1.5

(d)

0.65

0.6

0.55

0.5

0.45

0.4

0.35

−1.5

−1.5

−1

−0.5

0

0.5

1

1.5

2

2.5

−1

−0.5

0

0.5

1

1.5

2

2.5

Figure 1: MOONS: The mean of the predictive distribution
of supervised GP is shown in graph (a). Based on maximum
evidence, the best settings are found at log2 σt = −3.0 and
log2 σ = −2.25, shown as a cross in graph (b). The results of
SSGP with the best settings are shown in graph (c) and (d).

PCMAC, REUTERS. These values are based on experimen-
tal experience from [Sindhwani et al., 2005] with image and
text data sets. We used weighted graphs with Euclidean dis-
tance as the graph similarity measure, and Gaussian weights
with width (σt) set to the mean distance between adjacent
vertices. The behavior of evidence-based model selection for
SSGP and cross-validation (CV) based model selection for
Laplacian SVM was investigated over a range of values for
the parameters σ, γA, γI . For each dataset, we computed the
mean length of feature vectors (σ0) in the training set, and
probed σ in the range [ σ0
σ0
2 σ0 2σ0 4σ0 ]; the range for
γI
γA was [10−6 10−4 10−2 1 100] and the choices for ratio
γA
were [0 1 10 100 1000] (note that a choice of 0 ignored un-
labeled data and reduces the algorithm to its standard super-
vised mode). The noise parameter σn in the probit model for
class labels was set to 10−4 in all experiments. Note that this
parameter allows SSGP to potentially deal with label noise.

4

1. BENEFIT OF UNLABELED DATA: In Figure 2, we plot
the mean error rates on the test sets for the four datasets as
a function of the number of labeled examples (expressed in
terms of probability of ﬁnding a labeled example in the train-
ing set) for SSGP and standard supervised GP (which ignores
unlabeled data). Figure 3 shows the corresponding curves for
performance on the unlabeled data. The solid curves show the
mean and standard deviation of the minimum error rates for
SSGP and GP over the set of parameters σ, γA, γI (γI = 0
for GP) and the dashed curves show the mean and standard
deviation of error rates at parameters chosen by maximiz-
ing evidence. The mean and standard deviations are com-
puted over 10 random draws for every choice of the amount
of labeled data. We make several observations: (a) By utiliz-
ing unlabeled data, SSGP makes signiﬁcant performance im-
provements over standard GP on both the test and unlabeled
sets, the gap being larger on the unlabeled set. This holds true
across the entire span of varying amounts of labeled data for

3vs5

set1

t

e
S

 
t
s
e
T
n
o

 

 

e

t

a
r
 
r
o
r
r

E

t

e
S

 
t
s
e
T
n
o

 

 

e

t

a
r
 
r
o
r
r

E

0.2

0.15

0.1

0.05

0.25

0.2

0.15

0.1

0.05

SSGP*
SSGP
GP*
GP

0.02

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

prob. of label present

pcmac

t

e
S

 
t
s
e
T
n
o

 

 

e

t

a
R

 
r
o
r
r

E

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

t

e
S

 
t
s
e
T
n
o

 

 

e

t

a
r
 
r
o
r
r

E

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

prob. of label present

reuters

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

prob. of label present

prob. of label present

Figure 2: Comparison of SSGP with GP on Test Data.

all datasets, except PCMAC where the gap seems to converge
faster. (b) Performance based on parameters chosen by ev-
idence based model selection is much closer to the optimal
performance for SSGP than for GP. The performance curves
for SSGP have lesser variance.

3vs5

set1

SSGP*
SSGP
GP*
GP

0.02

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

prob. of label present

pcmac

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0.2

0.15

t

l

 

e
S
d
e
e
b
a
n
U
n
o

 

l

 

t

e
a
R

 
r
o
r
r

E

t

e
S

 
t
s
e
T
n
o
e

 

 

t

l

 

e
S
d
e
e
b
a
n
U
n
o
e

 

 

l

t

a
r
 
r
o
r
r

E

t

e
S

 
t
s
e
T
n
o
e

 

 

t

a
r
 
r
o
r
r

E

0.1

0.05

t

a
r
 
r
o
r
r

E

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

prob. of label present

reuters

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

prob. of label present

prob. of label present

Figure 3: SSGP vs GP on Unlab. Data (Transduction).

2. EVIDENCE MAXIMIZATION VERSUS CV: We next
compare evidence-based model selection in SSGP with CV
based model selection in Laplacian SVM. In Figure 4, we
plot the mean error rates on the test sets for the four datasets
as a function of the number of labeled examples for SSGP
and Laplacian SVM, both of which utilize unlabeled data
through the same kernel. Figure 5 shows the correspond-
ing curves for performance on the unlabeled data. The solid
curves show the mean and standard deviation of the mini-
mum error rates for SSGP and Laplacian SVM over the set
of parameters σ, γA, γI ; and the dashed curves show corre-
sponding curves at a parameter setting chosen by evidence-
maximization and CV for SSGP and Laplacian SVM respec-
tively. The mean and standard deviations are computed over
10 random draws for every choice of the amount of labeled
data. We used 5-fold CV for each labeled set, except those
with 10 or fewer examples, in which case leave-one-out cross
validation was used. It is important to note that our CV pro-
tocol for Laplacian SVMs only suppresses labels but does
not exclude the labeled examples from the graph. We make

IJCAI-07

1063

3vs5

set1

SSGP*
SSGP
LapSVM*
LapSVM

0.02

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

prob. of label present

pcmac

0.18

0.16

0.14

0.12

0.1

0.08

0.06

t

e
S

 
t
s
e
T
n
o

 

 

e

t

a
r
 
r
o
r
r

E

0.3

0.25

0.2

0.15

0.1

0.05

t

e
S

 
t
s
e
T
n
o

 

 

e

t

a
r
 
r
o
r
r

E

t

e
S

 
t
s
e
T
n
o

 

 

e

t

a
r
 
r
o
r
r

E

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

t

e
S

 
t
s
e
T
n
o

 

 

e

t

a
r
 
r
o
r
r

E

0.3

0.25

0.2

0.15

0.1

0.05

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

prob. of label present

reuters

1

e
v
r
u
c
 
C
O
R

 
r
e
d
n
u
 
a
e
r
a

0.9

0.8

10−2

prob. of label present

10−1

IVM
NCNM
SVM
TSVM
SSGP

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

prob. of label present

prob. of label present

Figure 6: SSGP versus NCNM, TSVM, IVM and SVM

Figure 4: Comparison of SSGP with LapSVM on Test Data.

the following observations: (a) In terms of best performance
over the range of parameters (i.e the solid curves), SSGP and
Laplacian SVM return nearly identical mean error rates, ex-
cept in 3VS5, where when given very few labeled examples,
SSGP outperforms LapSVM. (b) By looking at the perfor-
mance curves at parameters chosen by their corresponding
model selection strategies (i.e the dashed curves), the superi-
ority of evidence maximization in SSGP over CV in Lapla-
cian SVM becomes quite evident. This is true for both test
and unlabeled set performance. (c) The quality of CV based
performance of Laplacian SVM is signiﬁcantly better over
unlabeled data as compared to that over test data, indicating
that CV drives the model selection towards parameters that
favor transduction at the expense of semi-supervised induc-
tion. These experiments show that evidence maximization
returns signiﬁcantly better performance than CV on both the
test and unlabeled sets.

3vs5

set1

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0.14

0.13

0.12

0.11

0.1

0.09

0.08

0.07

0.06

SSGP*
SSGP
LapSVM*
LapSVM

t

l

 

e
S
d
e
e
b
a
n
U
n
o

 

l

0.02

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

prob. of label present

pcmac

 

t

e
a
r
 
r
o
r
r

E

t

l

l

e
S
 
d
e
e
b
a
n
U
 
n
o
 
e
t
a
r
 
r
o
r
r

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.12

0.1

0.08

0.06

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

prob. of label present

reuters

E

0.04

t

l

 

e
S
d
e
e
b
a
n
U
n
o
e

 

 

l

t

a
r
 
r
o
r
r

E

t

l

l

e
S
 
d
e
e
b
a
n
U
 
n
o
 
e
t
a
r
 
r
o
r
r

E

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

0.04

0.06

0.08

0.1

0.12

0.14

0.16

0.18

0.2

prob. of label present

prob. of label present

Figure 5: SSGP vs LapSVM on Unlab. Data (Transduction).

3. COMPARISON WITH OTHER METHODS: Finally, in Fig-
ure 6, we superimpose the SSGP performance curves over the
results of the Null Category Noise Model (NCNM), Infor-
mative Vector Machine (IVM), SVM, and transductive SVM
(TSVM) plotted in [Lawrence and Jordan, 2004]. The same
experimental protocol and data splits were used. We are en-
couraged to see that SSGP outperforms all algorithms tested
in this experiment.

5 Conclusion
To conclude, we have presented semi-supervised Gaussian
process classiﬁers based on a data-dependent covariance
function that is adapted to the geometry of unlabeled data. We
have empirically demonstrated the utility of SSGP on several
learning tasks, and observed the beneﬁts of evidence-based
model selection in comparison to cross-validation techniques
in Laplacian SVM/RLS.

References
[Belkin et al., 2006] M. Belkin, P. Niyogi, and V. Sind-
hwani. Manifold regularization: A geometric framework
for learning from labeled and unlabeled examples. Journal
of Machine Learning Research (to appear), 2006.

[Chu et al., 2006] W. Chu, V. Sindhwani, Z. Gharamani, and
S. S. Keerthi. Relational learning with gaussian processes.
In Neural Information Processing Systems 19, 2006.

[Kapoor et al., 2005] A. Kapoor, Y. Qi, H. Ahn, and R. Pi-
card. Hyperparameter and kernel learning for graph-based
semi-supervised classiﬁcation. In Neural Information Pro-
cessing Systems 18, 2005.

[Krishnapuram et al., 2004] B. Krishnapuram, D. Williams,
Y. Xue, A. Hartemink, L. Carin, and M. A. T. Figueiredo.
On semi-supervised classiﬁcation. In Neural Information
Processing Systems 17, 2004.

[Lawrence and Jordan, 2004] N. D. Lawrence and M. Jor-
dan. Semi-supervised learning via Gaussian processes. In
Neural Information Processing Systems 17, 2004.

[Rasmussen and Williams, 2006] C. E. Rasmussen and
C. K. I. Williams. Gaussian Processes for Machine
Learning. MIT Press, 2006.

[Sindhwani et al., 2005] V. Sindhwani, P. Niyogi,

and
M. Belkin. Beyond the point cloud: from transductive to
semi-supervised learning. In International Conference on
Machine Learning, 2005.

[Zhu et al., 2003] X. Zhu, Z. Ghahramani, and J. Laf-
ferty. Semi-supervised learning: From Gaussian ﬁelds to
Gaussian processes. Technical Report CMU-CS-03-175,
School of Computer Science, Carnegie Mellon University,
Pittsburgh, PA, 2003.

IJCAI-07

1064


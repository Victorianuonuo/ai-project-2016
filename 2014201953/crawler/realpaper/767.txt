Higher-Order Potentialities and their Reducers: 

A Philosophical Foundation Unifying Dynamic Modelling Methods 

Tibor Bosse and Jan Treur 

Vrije Universiteit Amsterdam, Department of Artificial Intelligence 

De Boelelaan 1081a, 1081 HV Amsterdam, The Netherlands 

{tbosse, treur}@cs.vu.nl 

Abstract 

In 
the  development  of  disciplines  addressing 
dynamics, a major role was played by the assumption 
that  processes  can  be  modelled  by  introducing  state 
properties, called potentialities, anticipating in which 
respect  a  next  state  will  be  different.  A  second 
assumption  often  made  is  that  these  state  properties 
can  be  related  to  other  state  properties,  called 
reducers. The current paper proposes a philosophical 
framework 
their 
reducers 
common  philosophical 
foundation for methods in AI and Cognitive Science 
to  model  dynamics.  This  framework  provides  a 
unified  philosophical 
for  numerical, 
symbolic, and hybrid approaches. 

terms  of  potentialities  and 

to  obtain 

foundation 

in 

a 

1.  Introduction  
Dynamics  of  the  world  shows  itself  by  the  occurrence  of 
different world states, i.e., states at different  points in time 
that differ in some of their state properties. In recent years, 
within  Cognitive  Science,  dynamics  has  been  recognised 
and  emphasised  as  a  central  issue  in  describing  cognitive 
processes  [Port  and  Gelder,  1995].  Van  Gelder  and  Port 
[1995] propose the Dynamical Systems Theory (DST) as a 
new paradigm that is better suited to the dynamic aspects of 
cognition than symbolic modelling approaches. However, as 
DST  (which  subsumes  neural  networks  and  many  other 
quantitative  approaches  to  adaptive  and  control  systems), 
commits to the use of quantitative methods (differential and 
difference equations), it  is often considered less suitable to 
model  higher  cognitive  processes  such  as  reasoning  and 
language processing. DST is based on the state-determined 
system  assumption:  properties  of  a  given  state  fully 
determine the properties of future states; cf. [Ashby, 1960], 
p.  25;  [Gelder  and  Port,  1995],  p.  6.  This  means  that  for 
state  properties  that  are  different  in  a  future  state,  state 
properties  in  the  given  state  can  be  found  that  somehow 
indicate  or  anticipate  these  differing  (changed)  properties. 
This  idea  closely  relates  to  the  concept  of  potentiality  that 
goes back to Zeno and Aristotle [350 BC’]: if a potentiality 
p for a state property a occurs in a given state, then in a next 
state,  property  a  will  occur.  A  potentiality  is  a  kind  of 

anticipatory state property: a state property anticipating the 
different state property (in the changed state).  

framework 

that  address  dynamics.  This 

In the current paper the notion of potentiality is used as a 
basis  for  a  philosophical  framework  to  analyse  modelling 
methods 
is 
applicable  to  obtain  philosophical  foundations  for  both 
quantitative approaches (such as DST and neural networks) 
and  qualitative  or  symbolic  approaches  (such  as  BDI-
modelling  and  production  systems) 
to  modelling  of 
dynamics.  Given  that  the  framework  is  applicable to  DST, 
which since long has proved its value for quantifiable areas 
within  a  wide  variety  of  disciplines,  also  the  scope  of 
applicability  of  the  proposed  philosophical  framework 
covers disciplines such as Physics, Chemistry, Biology, and 
Economics.  For  the  cognitive  and  knowledge  engineering 
area,  it  is  shown  how  the  framework  can  be  applied  to 
provide a foundation for symbolic modelling methods such 
as  production  systems  and  BDI-models.  The  framework 
easily describes both mental aspects and physical aspects of 
embodied cognitive agents and their relationship. 

Below, first the notion of potentiality is briefly introduced 
(Section  2).  Next,  in  Section  3  the  notion  of  higher-order 
potentiality  is  discussed  and  illustrated  by  examples  from 
Mathematics (higher-order derivatives and Taylor series). In 
Section 4 it is shown how higher-order potentialities play a 
role  as  a  philosophical  foundation  of  basic  concepts  in 
Physics  (momentum,  kinetic  energy,  force).  Section  5 
discusses the assumption that  potentialities of some higher 
order can be reduced to lower level state properties. Sections 
6  and  7  show  how  the  framework  can  be  applied  to 
symbolic  modelling  methods:  BDI-models  and  production 
systems. Section 8 shows how it can be applied to modelling 
of  adaptive  agents.  In  Section  9 
the  philosophical 
framework  is  related  to  the  Dynamical  Systems  Theory, 
including  neural  networks  and  many  other  (quantitative) 
approaches to dynamic systems. Section 10 is a discussion. 

2.  Potentialities  
Given  a  particular  state  that  just  changed  with  respect  to 
some  of  its  state  properties,  it  is  natural  to  ask  for  an 
explanation of why these new state properties occurred. In a 
state-based  approach,  as  a  source  for  such  an  explanation, 
state properties found in the previous state  form  a  primary 

IJCAI-07262candidate. A main question becomes how to determine for a 
certain  state  that  it  is  going  to  change  to  a  different  state, 
and,  more  specifically,  how  to  determine  (on  the  basis  of 
some  of  the  state  properties  in  the  given  state)  those  state 
properties for which the new state will differ from the given 
one.  This  poses  the  challenge  to  identify  state  properties 
occurring  in  a  given  state  that  anticipate  the  next  state: 
anticipatory  state  properties.  If  such  state  properties 
(historically  sometimes  called  potentialities)  are  given, 
anticipation  to  change is  somehow  encoded  in  a  state. The 
assumption on the existence of such properties is the crucial 
factor  for  the  validity  of  the  assumptions  underlying 
dynamic modelling methods such as the Dynamical Systems 
Theory.1  Aristotle did introduce such a type of concept; he 
called  it  potentiality  (to  move),  or  movable.2  For  example, 
following Zeno, the difference between an arrow at rest and 
the snapshot of a moving arrow at time t at position P is that 
the former has no potentiality to be at P', whereas the latter 
has. This explains why at a next instant t' the former arrow 
is still where it was, at P, while the latter arrow is (assuming 
no  obstruction)  at  a  different  position  P':  Aristotle  did  not 
only consider changes of positions (due to locomotion), but 
also, for example, a young man becoming an old man, and a 
cold  object  becoming  hot.  For  each  of  these  types  of 
changes  a  specific  type  of  potentiality  is  considered;  e.g., 
the  potentiality  to  be  at  position  P',  the  potentiality  (of  a 
cold  object)  to  be  hot.  In  general,  if  the  potentiality  p 
(occurring in a state S) to have state property a has led to a 
state  S'  where  indeed  a  holds,  then  this  state property  a  of 
state  S'  is  called  the  fulfilment  or  actualisation  of  the 
potentiality  p  for  a,  occurring  in  state  S.  Notice  that 
Aristotle  considered  both  absolute  potentialities,  indicating 
a state property for the future state independent of this state 
property  in  the  present  state,  and  relative  potentialities, 
indicating  a  difference  (increase  or  decrease)  in  a  future 
state property compared to the present state. 

3.  Higher-Order Potentialities 
The effect of a potentiality on a future state can be described 
by  relating  its  occurrence  in  the  present  state  to  the 
occurrence  of  a  certain  state  property  in  the  future  state, 
usually  under  an  additional  opportunity  condition  (e.g., 
assuming  no  obstruction  by  influences  otherwise).  This 

                                                            
1  ‘As  a  working  guide,  the  scientist  has  for  some  centuries  followed  the 
hypothesis that, given a set of variables, he can always find a larger set that 
(1) includes the given variables, and (2) is state-determined. Much research 
work consists of trying  to  identify such a larger set (…). The assumption 
that  such  a  larger  set  exists  is  implicit  in  almost  all  science,  but,  being 
fundamental, it is seldom mentioned explicitly.’ [Aristotle, 350 BC’], p. 28. 
2 ‘We have now before us the distinctions in the various classes of being 
between what is full real and what is potential. (…) The fulfilment of what 
exists potentially,  in  so far as  it exists potentially,  is  motion - namely,  of 
what is alterable qua alterable, alteration: of what can be increased and its 
opposite what can be decreased (there is no common name), increase and 
decrease:  of  what  can  come  to  be  and  can  pass  away,  coming  to  be  and 
passing away: of what can be carried along, locomotion.’ 
(from [Aristotle, 350 BC], Book III, Part 1) 
 

indicates  what  it  is  a  potentiality  for.  A  more  complicated 
question  is  how  to  specify  when  (under  which  past  and 
present  circumstances)  a  potentiality  itself  will  occur.  For 
the case of empty space, where an object is assumed to have 
no  interaction  with  other  objects,  a  potentiality  to  change 
position is present because it was present at an earlier point 
in time and persisted until t (inertia of motion). However, if 
the  potentiality  in  a  new  state  is  different  from  the  earlier 
one,  a  question  becomes  why  this  is  so.  This  leads  to  the 
question  addressed  in  this  section  of  how  a  changed 
potentiality can be explained. 

The use of higher-order potentialities is an answer to this 
question.  The  idea  behind  higher-order  potentialities  is 
simple. To obtain an explanation of changed state properties 
over  time,  potentialities  were  introduced.  Potentialities  are 
also  state  properties  that  change  over  time.  Therefore  it 
would  be  reasonable  to  treat  them  just  like  any  other  state 
property  that  changes  over  time.  This  means  that  for  a 
potentiality  p(1)  a  socalled  second-order  potentiality  p(2)  is 
introduced  to  explain  why  p(1)  may  become  changed  over 
time. And of course this process can be repeated for p(2), and 
so  on.  This  leads  to  an  infinite  sequence  of  higher-order 
potentialities,  p(1),  p(2),  p(3),  p(4),  …,  where  for  each  natural 
number  n  the  potentiality  p(n)  is  called  an  n-th-order 
potentiality. Using such higher-order potentialities, the idea 
is the following: 
• 
for a certain point in time t0 the occurrence of a state property 
a  is  determined  by  the  state  at  a  previous  time  point  t1  <  t0,  in 
particular, by the occurrence of the first-order potentiality p(1) for a 
at that time point t1,  
• 
the  occurrence  of  the  first-order  potentiality  p(1)  at  t1  is 
determined  by  the  state  at  a  time  t2  <  t1,  in  particular  by  the 
occurrence  of  its  own  potentiality  which  is  the  second-order 
potentiality p(2) for a at t2, et cetera. 
This process can be visualised as depicted in Figure 1. 
 
 
 
 
 
 
 
 
 
 
 

state 
properties 
p(4) 

time points 

p(3) 

p(4) 

p(3) 

p(2) 

p(1) 

a 

p(2) 

p(1) 

a 

Figure 1.  Dynamics based on higher-order potentialities 

This  shows  how  the  concept  of  potentiality  to  explain 
change of a certain basic state property a can take the form 
of a large number  of (higher-order)  entities. Strange as  the 
idea of an infinite number of higher-order potentialities may 
seem at first sight, in mathematical context (in particular in 
calculus)  this  has  been  worked  out  well  (using  infinite 
summations).  Higher-order  potentialities  have  been 
formalised  in  the  form  of  higher-order)  derivatives  of  a 
function. The (first-order) derivative of a function at a time 
point t gives an estimated measure of how the function will 
change  its  value  in  a  next  time  point.  The  well-known 

IJCAI-07263Taylor  series  [Taylor,  1715]  for  sufficiently  smooth 
functions  (at  least  infinitely  often  differentiable  to  guantee 
the existence of the derivatives) shows how changes of the 
(within  some  given 
function  value 
neighbourhood  of 
(higher-order) 
derivatives:   

t)  depend  on  all 

from 

t 

to 

t' 

   

f(t') = f(t) + Σk  f(k)(t)(t' – t)k / k!.  

This  shows  how  the  (relative)  potentiality  at  t,  defined  by 
the 
all  higher-order  potentialities, 
determines the changed state at the future time points t'.  

combination  of 

4.  Potentialities Underlying Physics 
In  later  times,  successors  of  Aristotle,  such  as  René 
Descartes  (1596-1650),  Christiaan  Huygens  (1629-1695), 
Isaac  Newton  (1643-1727)  and  Gottfried  Wilhelm  Leibniz 
(1646-1716),  among  others,  have  addressed  the  question 
how  to  further  develop  the  phenomenon  of  dynamics  (or 
change),  in  particular  within  Physics.  They  developed 
classical  mechanics  based  on  concepts 
that  can  be 
philosophically founded as certain types of potentialities.  
  Descartes [1633] took the product of mass and velocity 
of an object for its potentiality to be in a changed position, 
or  ‘quantity  of  motion’.  Notice  that  this  anticipatory  state 
property  ‘quantity  of  motion’  is  a  relative  potentiality:  the 
actualisation of a given quantity  of motion entails being  at 
another position as specified by this quantity relative to the 
current  position.  Descartes  also  expresses  a 
law  of 
conservation for this quantity of motion. 

In  modern  physics  this  ‘quantity  of  motion’  concept  is 
called  linear  momentum,  or  just  momentum,  and  the 
conservation, for example, during elastic collisions, is called 
the ‘law of momentum conservation’. Newton incorporated 
this  notion  in  his  approach  to  motion.  This  is  one  way  in 
which a concept ‘potentiality’ (for change  of position) was 
used  as  a  philosophical  basis  to  introduce  formalised 
concepts in physics, thus providing one of the cornerstones 
of  classical  mechanics.  Also  the  concept  of  ‘quantity  of 
motion’,  describing  change  of  position,  can  change  itself; 
this  leads  to  a second-order  potentiality.  In  his  second law 
Newton  [1729]  uses  the  term  ‘impressed  motive  force’  to 
express the change of motion.3 This law expresses that  the 
concept of force used by Newton directly relates to change 
of  motion.  For  (quantity  of)  motion  he  gives  the  same 
definition  as  Descartes,  i.e.,  momentum.  For  an  impressed 
force a definition is given that refers to ‘exerted action’, and 
to the corresponding change of the object’s state of motion.4 
He  shows how  this  notion  applies  in the  particular  case of 
centripetal  (i.e.,  directed  to  one  point)  force.5  This  shows 
that  the  concept  ‘force’  used  by  Newton  as  an  addition  to 
state  ontology  can  be  given  a  definitional  relationship  to 

                                                            
3  ‘The change  of  motion  is  proportional  to  the  motive  force  impressed; 
and  is  made  in  the  direction  of  the  right  line  in  which  that  force  is 
impressed.’ [Newton, 1729] 
4  ‘An  impressed  force  is  an  action  exerted  upon  a  body,  in  order  to 
change its state ...’ [Newton, 1729] 
5  ‘The  motive  quantity  of  a  centripetal  force  is  (…)  proportional  to  the 
motion which it generates in a given time.’ [Newton, 1729] 

‘motion generated in a given time’. This ‘motion generated 
in a given time’ can be philosophical founded as a second-
order  potentiality  for  the  first-order  potentiality  ‘motion’. 
So,  within  classical  mechanics,  after 
the  concepts 
‘momentum’ and ‘kinetic energy’ which were added to the 
state ontology as specific types of concepts based on a (first-
order)  potentiality, the  concept ‘force’ can be considered a 
third anticipatory state property added to the state ontology, 
this time based on a second-order potentiality. Newton and 
also Leibniz developed mathematical techniques of calculus, 
such  as  differentiation  and 
these 
techniques, Newton’s second law is formulated as F = dp/dt 
or F = d(mv)/dt. For a mass which is constant over time this 
is equivalent with F = ma with a the acceleration dv/dt; in 
this - most known form - the law was formulated by  Euler 
65  years  after  the  Principia  appeared.  In  20th  century  text 
books such  as [Mach,  1942] the  concept  ‘moving  force’ is 
defined in terms of acceleration, which is based on a second-
order potentiality for change of position.6 

integration.  Using 

As  illustrated  by  the  examples  in  Sections  3  and  4,  the 
idea of use potentialities to analyse the change of states has 
successfully  contributed  to  the  development  of  well-
respected disciplines such as Mathematics and Physics.  

5.  Reducers Limiting a Potentiality Chain 
Apparently, the use of potentialities may lead to an infinite-
dimensional  vector  of  higher-order  potentialities.  As  this 
can be difficult to handle, it makes sense to look for ways to 
break  off  this  chain  of  higher-order  potentialities.  One 
possible  option  is  to  consider  only  changes  that  involve  a 
finite  number  of  higher-order  potentialities.  For  example, 
for  a  falling  object  within  a  constant  gravitation  field,  the 
second-order potentiality (the second-order derivative of the 
function  measuring  the  distance)  is  constant  (9.8  m/sec2), 
and  hence  no  third-  or  higher-order  potentiality  is  needed. 
However,  further  away  in  the  universe,  if  an  object  is 
approaching the earth, gravitation will increase over time, so 
this  assumption  of  constancy  will  not  always  be  fulfilled. 
However,  as  is  also  shown  by  the  Taylor  series,  often  an 
adequate  approximation  can  be  obtained  by  taking  into 
account only the terms up to some n-th order, as the terms 
substantially  decrease 
in  absolute  size;  for  example, 
assuming  all  derivatives bounded  by  some  constant  M,  the 
effect  of  the  n-th  term  is  less  then  M/n!,  so,  as  an 
approximation its contribution can be counted as zero. 
 
In the same perspective, a more general way to get rid of 
the infinite vector of higher-order potentialities is when for 
some  n  the  n-th-order  potentiality  is  equivalent  to  a 
combination  of  lower  level  potentialities  and/or  basic  state 
properties  (this  is  called  a  reducer  of  the  higher-order 
potentiality) and by means of this relation can be reduced to 
them. This is what happens in classical mechanics, and, in a 
more  general  context,  in  other  cases  where  a  differential 

                                                            
6  ‘Moving  force  is  the  product  of  the  mass  value  of  a  body  with  the 
acceleration induced in that body.’ [Mach, 1942], p. 304 
 

IJCAI-07264equation can be found that relates a higher-order derivative 
to lower order derivatives and/or basic state properties.  

The  domain  of  Physics  illustrates  this.  Analysing  the 
motion  of  planets  around  the  sun,  Newton  found  out  that 
they can only follow their orbit if a second-order potentiality 
is  assumed,  in  the  direction  of  the  sun.  Newton  calculated 
(using  his  calculus  under  development)  in  detail  that  this 
motive force was proportional to 1 divided by the square of 
the distance. For example, for an object in space with mass 
m  approaching  earth  (with  mass  M),  Newton’s  law  of 
gravitation  for the  motive  force on the object is as  follows 
(here x is the distance between the object and the earth, and 
c is a constant): F = c mM/x2. Such a relation between the 
second-order  potentiality  force  and  basic  state  properties 
mass  and  distance  shows  how  a  higher-order  potentiality 
can be reduced: in this case  c mM/x2 is a reducer for F. 

As  the  example  from  Physics  shows,  a  differential 
equation  is  a  manner  to  reduce  higher-order  potentialities. 
This  reveals  another  assumption  underlying  DST, 
in 
addition to  the state-based system  assumption,  namely  that 
for  some  n  the  n-th  order  potentiality  can  be  reduced  to 
lower  level  potentialities  and/or  basic  state  properties. 
Without  this  assumption  no  differential  equations  can  be 
found, and without them DST will not work. 

foundations 

6.  Potentialities and BDI-Models 
Although traditionally only used within disciplines  such as 
Mathematics and Physics, a natural question is whether the 
idea  of  higher-order  potentialities is  also  suitable to  obtain 
philosophical 
for  domains  such  as  AI, 
Cognitive  Science  and  Agent  Systems.  To  describe  the 
internal  dynamics  of  agents,  the  concepts  beliefs,  desires 
and  intentions  have  been  introduced;  e.g.,  [Aristotle,  350 
BC’].  From  a  historical  perspective, 
the  reason  for 
introducing  these  concepts  was  not  unlike  the  reason  for 
introducing  the  concepts  momentum  and  force  within 
classical mechanics: they were needed as abstract notions to 
explain the change of states, in this case of living creatures. 
Aristotle describes how desire plays a role similar to that of 
the  potentiality  for  an  action.  Here  ‘desire’  is  indicated  as 
the  source  of  motion  of  a living  being.  He  shows  how  the 
occurrence  of  certain  internal  (mental)  state  properties 
(desires, ‘the good’) within the living being entail or cause 
the occurrence of an action in the external world, given an 
opportunity  (‘the  possible’) to  actualise the  potentiality  for 
the action indicated by the desire.  

In  this  section  it  is  discussed  how  to  philosophically 
found concepts such as desire and intention by potentialities. 
To  start,  the  notion  of  intention  is  addressed.  An  intention 
can be founded by a potentiality for an action in the world 
(i.e., for a changed world state). Where do intentions come 
from?  A  common  view  is  that,  given  some  beliefs, 
intentions  come  from  desires,  by  some  form  of  selection 
process.  In  this  interpretation  a  desire  can  founded  by  a 
potentiality as well, but not a potentiality for some state of 
the world, but a potentiality for an intention, which itself is 
also  considered  a  potentiality,  for  an  action  (i.e.,  for  a 
changed world state). Therefore this view identifies a desire 

as  a  second-order  potentiality  (for  a  changed  world  state). 
To the question where desires come from there seems to be 
no general answer. In some cases there may be reducers for 
it in terms  of physical states,  in other  cases,  e.g., norms  or 
personality aspects may provide a third-order potentiality. 

7.  Potentialities and Production Systems 
An  often  used  method  in  AI  and  Cognitive  Science  is  to 
specify  how  a  state  in  a  system  may  change  is  production 
systems. These are collections of production rules, denoted 
as ϕ → ψ with antecedent ϕ and consequent ψ ; here: 
•  ϕ  indicates a (combined) state property for the current state 
•  ψ  indicates one or more state properties for the next state 
The idea is that if the combination of properties specified in 
the first description holds in a (current) state, then in a next 
state the properties specified by the second description will 
hold.  This  is  illustrated  by  a  simple  model  of  agent 
behaviour based on beliefs desires and intentions. Consider 
an agent walking down a street and observing an ice cream 
sign  across  the  street  he  believes  the  supermarket  sells  ice 
cream. Based on this belief (b1) the agent generates a desire 
(d) for ice cream. Given this desire, and the belief (b2) that 
the  supermarket  is  reachable  (by  crossing  the  street)  the 
agent generates the intention (i) of having ice cream. Given 
this  intention  and  the  belief  (b3)  that  no  traffic  is  on  the 
street he actually crosses the street and obtains the ice cream 
(e). In this case the state ontology is described by six basic 
state properties: b1, b2, b3, d, i, e. The production system is: 
 
Based on this a trace of subsequent states is made:  
•  Given a current state S, take the production rules for which the 
antecedent  holds  in  the  current  state.  This  is  the  set  of 
applicable rules. 
•  Collect the  consequents  of all  applicable  rules  and obtain  the 
next state S' by modifying S so that all these consequents hold 
in S' (and the rest of S is persisting). 

b2 ∧ d → i        

b3 ∧ i → e 

b1 → d      

So,  for  example,  the  subsequent  states  for  a  given  initial 
state for which the three beliefs hold are as follows: 

 

0 
1 
2 
3 

[b1, b2, b3] 
[b1, b2, b3, d] 
[b1, b2, b3, d, i] 
[b1, b2, b3, d, i, e] 

How  can  this  be  interpreted  in  terms  of  potentialities?  For 
example, consider state 1. As in the next state, state 2, state 
property i holds, in state 1 the potentiality for i to hold has 
to be present. On the other hand, i occurs in state 2 because 
of  the  second  production  rule.  Taken  together  this  means 
that  this  production  rule  can  be  interpreted  for  state  1  as 
indicating  that,  due  to  the  occurrence  of  both  b2  and  d  in 
this  state,  also  the  potentiality  p(i)  for  i  occurs  in  state  1. 
Similarly  the  other  production  rules  can  be  interpreted  as 
indications of which potentialities occur in a given state. In 
general, according to this interpretation, a production system 
specifies for each state which potentialities occur: for each 
production rule ϕ → ψ, if in a state S its antecedent ϕ holds, 
then in this state S also the potentialities p(ψ) for ψ occur. 
Thus  a  production  rule  ϕ  →  ψ  can  be  interpreted  as  an 
implication  ϕ  →  p(ψ),  describing  a  logical  relationship 

IJCAI-07265between state properties in a given state, e.g., ϕ is a reducer 
for  p(ψ).  Since  the  idea  of  production  rules  is  used  in 
various other modelling approaches (e.g., knowledge-based 
systems,  and  cognitive  architectures  such  as  ACT-R 
[Anderson  and  Lebiere,  1998]  and  SOAR  [Laird  et  al., 
1987]), 
to 
interpret  such 
approaches in terms of potentialities as well. 

in  principle 

is  possible 

it 

8.  Potentialities and Adaptive Agents 
Adaptive  agents  are  often  modelled  in  numerical  and 
algorithmic manners. In this sense modelling approaches for 
adaptive agents are in general closer to the DST modelling 
approach  than  to  symbolic  modelling  approaches  as  often 
applied  for  other  types  of  cognitive  agents  such  as  BDI 
agents.  The  potentiality-based  analysis  framework 
is 
applicable  to  obtain  a  philosophical  foundation  for  both 
types of modelling approaches. As in Section 6 and 7 it was 
shown  how  symbolic  models  can  be 
founded  by 
potentialities, in this section adaptive agents  are addressed, 
illustrated by a case study of Aplysia. Aplysia is a sea hare 
that is often used to do experiments. It is able to learn; for 
example, it performs classical conditioning in the following 
manner. This (a  bit simplified) description is mainly based 
on  [Gleitman,  1999],  pp.  155-156.  Initially  the  following 
behaviour  is  shown:  a  tail  shock  leads  to  a  response 
(contraction), and a light touch on its siphon is insufficient 
to  trigger  such  a  response.  Now  suppose  the  following 
experimental protocol is undertaken. In each trial the subject 
is touched lightly on its siphon and then, shocked on its tail 
(as  a  consequence  it  responds).  It  turns  out  that  after  a 
number of trials (assumed three in the current example) the 
behaviour has changed: the animal also responds (contracts) 
on a siphon touch. The cause of this change in behaviour is, 
in  short,  that  the  learning  trials  strengthen  the  internal 
connection between sensory nand motor neurons. To obtain 
a  potentiality-based  analysis  of  this  adaptive  agent  the 
following steps can be made. 
• 

Introduce  the  basic  world  state  properties:  siphon  touch,  tail 
shock,  contraction,  weak  connection  between  sensory  neuron 
and motor neuron. 
• 
Introduce  a  potentiality  p  for  the  contraction  (based  on  the 
opportunity that a siphon touch occurs). 
• 
Introduce a potentiality p' for p (based on the opportunity that 
both a tail shock and siphon touch occur). 
• 
Introduce a potentiality p" for p' (based on the opportunity that 
both a tail shock and siphon touch occur). 
• 
Introduce  a  potentiality  p"'  for  p"  (based  on  the  opportunity 
that both a tail shock and siphon touch occur). 
•  Specify  a  reducer  for  p"'  based  on  the  untrained  state  of  the 

connection between the relevant sensory and motor neurons. 

Notice that in the analysis of the example the intermediate 
states during the adaptation process were founded by first-, 
second- and third-order potentialities p, p', p" that were not 
reduced,  similar  to  the  not  reduced  first-order  potentiality 
momentum  in  classical  mechanics  as  an  intermediate  state 
between  the  second  order  potentiality  force,  which  is 
reduced,  and  position.  However, 
if  more  detailed 
information from the neurological area is incorporated (i.e., 

about  the  physiological  states  of  the  synapses  between 
certain neurons, e.g., sensory neuron SN2 and motor neuron 
MN,  during  the  adaptation  process),  then  all  of  the 
potentialities  can  be  reduced;  e.g.,  [Gleitman,  1999],  pp. 
155-156,  see  also  Figure  2.  Within  a  potentiality-based 
analysis this is incorporated as follows. Suppose s1, s2 and 
s3  are  reducers  of  p,  p',  p"  respectively,  then  the  (higher-
order)  potentialities  p',  p",  p"' 
  become  (first-order) 
potentialities  for  the  world  state  properties  s1,  s2  and  s3 
respectively. 
 
 
 
 
 
 

siphon 
touch

contraction 

tail 
shock 

IN 

SN

M

 

 
SN

Figure 2.  Aplysia from neurological perspective 

This  shows  how  potentialities  cover  the  foundation  of 
both  internal  agent  states  that  are  used  as  functional  states 
without  direct physical grounding, and internal agent states 
that  are  considered  as  embodied  and  embedded  in  the 
physical  world.  Moreover,  the  relationship  between  these 
functionalist  and  physicalist  perspectives  can  easily  be 
analysed  within 
the  potentiality-based  philosophical 
framework.  For reasons  of  presentation,  the  example  of  an 
adaptive agent discussed in this section was kept relatively 
simple. However, the case can be extended easily to include, 
for  example,  numerical  aspects,  or  more  precise  timing 
aspects such as in trace conditioning.  

9.  Potentialities and DST 
Within Dynamical Systems Theory (DST; cf. [Ashby, 1960; 
Port and Gelder, 1995]), techniques used are difference and 
differential  equations.  The  analysis  based  on  potentialities 
covers the DST approach in the following manner.  
•  For  the  basic  state  properties,  take  value  assignments  to  the 
•  For  any  n,  for  the  nth-order  potentiality  for  a  basic  state 
property, take the value assignment to the nth-order derivative 
of the variable.  
•  For the reduction relation of an nth-order potentiality, take the 
nth-order  differential  equation  relating  the  value  of  the  nth-
order  derivative  to  the  values  of  lower-order  derivatives  and 
basic variables. 

basic variables used to describe a phenomenon in DST.  

the object, m mass of the object, M mass of the earth. 

As an example, the gravitation case is covered as follows: 
•  Basic  state  properties:  value  assignments  to  x  the  position  of 
•  First-order  potentiality  for  basic  state  property  mx:  value 
assignment to first-order derivative p of mx (momentum  p of 
the object): p = dmx/dt. 
•  Second-order  potentiality  for  state  property  mx:  value 
assignment  to  first-order  derivative  f  of  p,  or,  equivalently, 
second-order derivative f of mx (force f): f = dp/dt = d2mx/dt2. 

•  Reduction relation of the second-order potentiality f:  
 

            f = c*mM/x2       (or d2mx/dt2 = c*mM/x2 ). 

IJCAI-07266As  neural  networks  can  be  considered  as  a  specific  use  of 
DST, the relationship of the potentiality-based view to DST 
also  covers  the  relationship  to  neural  networks  and  many 
other quantitative modelling methods. 

10.  Discussion 
Dynamical  Systems  Theory  (DST)  has  recently  been  put 
forward  within  Cognitive  Science  to  model  dynamics  of 
cognitive  processes,  and  proposed  as  an  alternative  for  the 
symbolic/computational approach; for example, [Gelder and 
Port,  1995;  Port  and  Gelder,  1995].  DST  subsumes  many 
quantitative  approaches  such  as  neural  networks  and  other 
adaptive  system  modelling  approaches.  The  notion  of  a 
state-determined  system [Ashby,  1960]  is  central  for  DST; 
such a system is based on the assumption that properties of a 
given  state  fully  determine  the  properties  of  future  states.  
Within DST the notion of dynamical system that is assumed 
is  based on world states at different points in time that  are 
conceptualised  by  (real  number)  value  assignments  to 
continuous  variables.  The  way  in  which  properties  of  a 
present  state  determine  properties  of  a  future  state  is 
expressed by difference or differential equations. This is one 
specific  way 
the  state-determined  system 
assumption, where derivatives play the role of potentialities: 
state properties that indicate how the state will change.  

to  achieve 

In  other  literature,  such  as  [Giunti,  1995],  the  notion  of 
dynamical system is more general, not (only) based on real 
number  value  assignments.  Also  in  such  systems  a  state-
determined  system  assumption  can  be  incorporated  by 
introducing potentialities to the  state  ontology that indicate 
in which respect the state is going to change. The notion of 
potentiality  goes  back  to  Zeno  and  Aristotle  and  was 
exploited  by  Descartes,  Newton,  Huygens  and  Leibniz, 
among others, to develop fundamental areas within Physics 
and Mathematics (classical mechanics and calculus).  

for  both 

foundations 

In  this  paper,  a  philosophical  framework  has  been 
introduced  to  analyse  modelling  methods  for  dynamics  by 
(first- and higher-order) potentialities and reducers for them. 
It  was  shown  how  this  framework  provides  unified 
philosophical 
and 
mathematical approaches, approaches that are often seen as 
mutually  exclusive.  As  a 
result,  hybrid  modelling 
approaches  can  be  philosophically  founded,  in  which  both 
symbolic  and  mathematical  aspects  are  covered.  Examples 
of  such  approaches  that  make  some  first  steps  in  that 
direction are cognitive modelling frameworks such as ACT-
R [Anderson and Lebiere, 1998], SOAR [Laird et al., 1987], 
and (recently) LEADSTO [Bosse et al., 2005]. 

symbolic 

Within  Mathematics  the  same  philosophical  concept  has 
been  worked  out  in  the  nth-order  derivatives  of  a  function 
for  all  n,  and  the  Taylor  series  to  calculate  changes  of  the 
function value. In many cases, after a few steps a reduction 
can be made in the sense that a higher-order potentiality is 
equivalent to a combination of basic state properties and/or 
lower  level  potentialities.  Within  DST  this  is  where  a 
differential  equation  comes  in,  reducing  an  n-th-order 
derivative  to  lower  level  properties:  the  assumption  of 
reduction is another crucial assumption underlying DST. 

The  presented  philosophical  framework  unifies 

the 
modelling  of  a  wide  variety  of  dynamic  phenomena  in  the 
natural  and  artificial  world,  from  cognitive  phenomena  to 
biological, chemical and physical phenomena. For example, 
in the Aplysia case study it was shown how modelling from 
a functionalist/mental perspective can easily be integrated at 
the  underlying  philosophical  level  with  a  perspective  from 
the  physical/neurological  level.  Thus  it  was  shown  how 
philosophical  foundations  of  agents  embodied  in  their 
physical environment are addressed in a unified manner. 

References 
[Anderson and Lebiere, 1998] Anderson, J.R., and Lebiere, 
C.  The  atomic  components  of  thought.  Lawrence 
Erlbaum Associates, Mahwah, NJ. 

[Aristotle,  350  BC]  Aristotle.  Physica  (translated  by  R.P. 

Hardie and R.K. Gaye). 

[Aristotle, 350 BC’] Aristotle. De Motu Animalium On the 
Motion of Animals (translated by A. S. L. Farquharson). 
[Ashby, 1960] Ashby,  R.  Design for a  Brain. Chapman & 

Hall, London. Revised second edition, 1960. 

[Bosse  et  al.,  2005]  Bosse,  T.,  Jonker,  C.M.,  Meij,  L.  van 
der,  and  Treur,  J.  LEADSTO:  a  Language  and 
Environment for Analysis of Dynamics by SimulaTiOn. 
In: Eymann, T. et al. (eds.), Proc. of MATES'05. LNAI, 
vol. 3550. Springer Verlag, 2005, pp. 165-178. 

[Descartes,  1633]  Descartes,  R.  The  World  or  Treatise  on 
Light.  Withdrawn  from  publication,  1633.  Also  in: 
Descartes:  The  World  and  Other  Writings  (edited  by 
Stephen Gaukroger). Cambrige University Press. 

[Gelder and Port, 1995] Gelder, T.J. van, and Port, R.F. It’s 
About Time: An Overview of the Dynamical Approach 
to Cognition. In: (Port and van Gelder, 1995), pp. 1-43. 

[Giunti, 1995] Giunti, M. Dynamical Models of Cognition. 

In: (Port and Gelder, 1995), pp. 549-572. 

[Gleitman,  1999]  Gleitman,  H.  Psychology,  W.W.  Norton 

& Company, New York. 

[Laird  et  al.,  1987]  Laird,  J.E.,  Newell,  A.,  and 
Rosenbloom,  P.S.  Soar:  an  architecture  for  general 
intelligence. Artificial Intelligence, vol. 33, pp. 1-64. 

[Mach,  1942]  Mach,  E.  The  Science  of  Mechanics:  A 
Critical  and  Historical  Account  of its  Development.  9th 
edition, trans. T.J. McCormack, LaSalle III. 

[Newton, 1729] Newton, I. The Mathematical Principles of 
Natural  Philosophy,  1729.  In:  Newton's  Principles  of 
Natural Philosophy, Dawsons of Pall Mall, 1968. 

[Port and Gelder, 1995] Port, R.F., and Gelder, T. van (eds.) 
Mind  as  Motion:  Explorations  in  the  Dynamics  of 
Cognition. MIT Press, Cambridge, Mass 

[Taylor, 1715] Taylor, B. Methodus Incrementorum Directa 

et Inversa. 

IJCAI-07267
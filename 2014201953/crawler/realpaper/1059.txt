Learning to Walk through Imitation 

Rawichote Chalodhorn, David B. Grimes, Keith Grochow, and Rajesh P. N. Rao 

Neural Systems Laboratory  

Department of Computer Science and Engineering 

University of Washington 

Seattle, WA 98195-2350 U.S.A. 

E-mail: {choppy,grimes,keithg,rao}@cs.washington.edu 

Abstract 

Programming  a  humanoid  robot  to  walk  is  a  chal-
lenging  problem 
in  robotics.  Traditional  ap-
proaches  rely  heavily  on  prior  knowledge  of  the 
robot's  physical  parameters  to  devise  sophisticated
control  algorithms  for  generating  a  stable  gait.  In
this paper,  we provide, to our knowledge,  the first
demonstration  that  a  humanoid  robot  can  learn  to 
walk  directly  by  imitating  a  human  gait  obtained 
from  motion  capture  (mocap)  data.  Training  using 
human  motion  capture  is  an  intuitive  and  flexible 
approach  to  programming  a  robot  but  direct  usage 
of  mocap  data  usually  results  in  dynamically  un-
stable  motion.  Furthermore,  optimization  using 
mocap  data in the humanoid full-body joint-space 
is  typically  intractable.  We  propose  a  new  model-
free  approach  to  tractable  imitation-based  learning
in humanoids. We represent kinematic information 
from  human  motion  capture  in  a  low  dimensional 
subspace  and  map  motor  commands  in  this  low-
dimensional  space  to  sensory  feedback  to  learn  a 
predictive  dynamic  model.  This  model  is  used 
within  an  optimization  framework  to  estimate  op-
timal motor commands that satisfy the initial kine-
matic  constraints  as  best  as  possible  while  at  the 
same  time  generating  dynamically  stable  motion. 
We  demonstrate  the  viability  of  our  approach  by 
providing examples of dynamically stable  walking 
learned  from  mocap  data  using  both  a  simulator 
and a real humanoid robot. 

1  Introduction 

Imitation is an important learning  mechanism in  many bio-
logical systems including humans [Rao and Meltzoff, 2003]. 
It is easy to recover kinematic information from human mo-
tion  using,  for  example,  motion  capture,  but  imitating  the 
motion with stable robot dynamics is a challenging research 
problem.    Traditional  model-based  approaches  based  on 
zero-moment  point  (ZMP)  [Vukobratovic  and  Borovac, 
2004],  [Kajita  and  Tani,  1996]    or  the  inverted  pendulum 
model  [Yamaguchi  et  al.,  1996]    require  a  highly  accurate 
model  of  robot  dynamics  and  the  environment  in  order  to 

achieve a  stable  walking  gait. Learning approaches such as 
reinforcement  learning  [Sutton  and  Barto,  1998]    are  more 
flexible  and  can  adapt  to  environmental  change  but  such 
methods  are  typically  not  directly  applicable  to  humanoid 
robots  due  to  the  curse  of  dimensionality  problem  engen-
dered by the high dimensionality of the full-body joint space 
of the robot.  

     

Motion tracking 

Kinematics mapping 

Low dimensional subspace 

representation 

Learning of a complex task  

Robot dynamics compensa-

tion 

Figure 1. A framework for learning human behavior by imitation 
through sensory-motor mapping in reduced dimensional spaces. 

    In  this  paper,  we  propose  a  model-free  approach  to 
achieving  stable  gait  acquisition  in  humanoid  robots  via 
imitation. The framework for our method is shown in Figure 
1. First, a motion capture system transforms Cartesian posi-
tion  of  markers  attached  to  the  human  body  to  joint  angles 
based  on  kinematic  relationships  between  the  human  and 
robot bodies. Then,  we employ dimensionality reduction to 
represent posture information in a compact low-dimensional 
subspace.  Optimization  of  whole-body  robot  dynamics  to 
match  human  motion  is  performed  in  the  low  dimensional 
space.  In  particular,  sensory  feedback  data  are  recorded 
from  the  robot  during  motion  and  a  causal  relationship  be-
tween actions in the low dimensional posture space and the 
expected sensory feedback is learned. This learned sensory-
motor  mapping  allows  humanoid  motion  dynamics  to  be
optimized. An inverse mapping from the reduced space back 
to the original joint space is then used to generate optimized 

IJCAI-07

2084

motion  on  the  robot.  We  present  results  demonstrating  that 
the proposed approach allows a humanoid robot to learn to 
walk  based  solely  on  human  motion  capture  without  the 
need for a detailed physical model of the robot. 

2  Human Motion Capture and Kinematic 

Mapping 

 

 

 

 

 

analysis (PCA) to parameterize the low-dimensional motion 

subspace X .  Although  nonlinear  dimensionality  reduction 

methods  could  be  used  (e.g.,  [MacDorman  et  al.,  2004], 
[Grochow et al., 2004]), we found the  standard linear PCA 
method to be sufficient for the classes of  motion studied in 
this paper.  

The result of linear PCA can be thought of as two linear 
-1C which map from high to low and low to 
operators C and 
high dimensional  spaces respectively. The low dimensional 
representation of the joint angle space of the HOAP-2 robot 
executing  a  walking  gait  (in  the  absence  of  gravity)  is 
shown in Figure 3. 

Figure  2.  Kinematic  mapping  used  in  our  approach  (from  left  to 
right:  human  body,  human  skeleton,  robot  skeleton,  and  robot 
body, respectively). 

In this paper, we  manually  map the joint angle data from a 
motion  capture  system  to  a  kinematic  model  for  a  Fujitsu 
HOAP-2  humanoid  robot.  To  generate  the  desired  motion 
sequence for the robot, we capture example motions from a 
human subject and map these to the joint settings of the ro-
bot.  Initially, a set of markers is attached to the human sub-
ject and the 3-D positions of these markers are recorded for 
each  pose  during  motion.   We  use  a  Vicon  optical  system 
running at 120Hz and a set of 41 reflective markers.  These 
recorded marker positions provide a set of  Cartesian points 
in the 3D capture volume for each pose.  To obtain the final 
subject poses, the marker positions are then assigned as po-
sitional constraints on a character skeleton to derive the joint 
angles using standard inverse kinematics (IK) routines. 
   As  depicted  in  Figure  2,  in  order  to  generate  robot  joint 
angles, we simply replace the human subject’s skeleton with  
a robot skeleton of the same  dimensions.  For example, the 
shoulders  were  replaced  with  three  distinct  1-dimensional 
rotating joints rather than one 3-dimensional ball joint.  The 
IK routine then directly generates the desired joint angles on 
the  robot  for  each  pose.   There  are  limitations  to  such  a 
technique  (e.g,  there  may  be  motions  where  the  robot’s 
joints  cannot  approximate  the  human  pose  in  a  reasonable 
way),  but  since  we  are  interested  only  in  classes  of  human 
motion that the robot can handle, this method proved to be a 
very  efficient  way  to  generate  large  sets  of  human  motion 
data for robotic imitation. 

3 Sensory-Motor Representations 

3.1 Low-Dimensional Representation of Postures 

Particular  classes  of  motion  such  as  walking,  kicking,  or 
reaching for an object are intrinsically low-dimensional. We 
apply  the  well  known  method  of  principal  components

4

1

4

2

0

-2

-4

5

3

0

2

-4

-2

0

2

-5

4

Figure  3.  Posture  subspace  and  example  poses.  A  3-dimensional 
reduced space representation of the postures of the HOAP-2 robot 
during a walking motion. We applied linear PCA to 25 dimensions 
of  joint  angle  data  of  the  robot  that  mapped  from  a  human  kine-
matic  configuration  as  described  in  Section  2.  Blue  diamonds 
along  the  function  approximated  trajectory represent  different 
robot  postures  during  a  single  walking  cycle.  Red  circles  mark 
various example poses as shown in the numbered images.  

We use a straightforward, standard linear PCA method to 
map between the low and high-dimensional posture spaces. 
Vectors  in  the  high-dimensional  space  are  mapped  to  the 
low-dimensional  space  by  multiplication  with  the  transfor-
mation  matrix C .  The  rows  of  C   consist  of  the  eigenvec-
tors, computed via singular  value decomposition (SVD), of 
the  motion  covariance  matrix.  SVD  produces  transformed 
vectors  whose  components  are  uncorrelated  and  ordered 
according to the magnitude of their variance.  
  vector  of  joint  angles  (the 
  vector  in  3D  space. 
= ×
3 1
p
We can calculate  p  in 3D space by using r = Cq , where  r
is  a  21 1× vector  of  all  principal  component  coefficients  of 
  transformation  matrix.    We  then 
q   and  C   is  the  21 21×
pick the first three elements of  r  (corresponding to the first 
three  principal  components)  to  be  p .  The  inverse  mapping 
can be computed likewise using the pseudo inverse of  C . 

high-dimensional  space)  and 

For  example,  let 

=

×
21 1

q

IJCAI-07

2085

3.2 Action Subspace Embedding 

High-level control of the humanoid robot reduces to select-
ing a desired angle for each joint servo motor. As discussed 
previously,  operations  in  the  full  space  of  all  robot  joint 
angles tend to be intractable. We leverage the redundancy of 
the full posture space and use the reduced dimensional sub-

(also  referred  to  as  an  action)  can  be  represented  by  a 

space X   to  constrain  target  postures.  Any  desired  posture 
point ∈a X .  
loop in X as shown in Figure 4. In the general case, we con-
⊆A X .  Non-linear  parameterization  of  the  action  space 

sider  a  non-linear  manifold  representing  the  action  space 

A periodic movement such as walking is represented by a 

allows further reduction in dimensionality. We embed a one 
dimensional  representation  of  the  original  motion  in  the 
three dimensional posture space and use it for constructing a 
constrained  search  space  for  optimization  as  discussed  in 
Section 5. Using the feature representation of the set of ini-
tial training examples 
x = C z , we first convert each point 
to  its  representation  in  a  cylindrical  coordinate  frame.  This 
is  done  by  establishing  a  coordinate  frame  with  three  basis 
directions  θ
x , y , z in  the  feature  space. The  zero point  of 
the  coordinate  frame  is  the  empirical  mean  μof  the  data 
points in the reduced space. We recenter the data around this 
new zero point and denote the resulting data ˆ ix . 

θ

θ

i

i

4

2

0

-2

-4

5

xθθθθ    

jo 
yθθθθ    
zθθθθ    

0

0

2

-5

4

-4

-2

Figure  4.  Embedded  action  space  of  a  humanoid  walking  gait. 
Training data points in the reduced posture space (shown in blue-
dots) are converted to a cylindrical coordinate frame relative to the 
x , y , z .  The  points  are  then  represented  as  a 
coordinate  frame θ
function  of  the  angle  φ ,  which  forms  an  embedded  action  space 
(shown  in  red-solid-curve).  This  action  space  represents  a  single 
gait cycle. 

θ

θ

We then compute the principal axis of rotation  θz : 

θ =
z

∑
∑

i

i

 
i

ˆ
ˆ
(x × x

 
i+1

 
i

ˆ
ˆ
(x × x

 
i+1

)

)

.   

 

 

 

(1) 

ix
Next,  θx is chosen to align with the maximal variance of 
θy is  specified  as  or-
in  a  plane  orthogonal  to θz .  Finally, 
thogonal  to θx and θz .  The  final  embedded  training  data  is 
obtained  by  cylindrical  conversion  to φ(
radial  distance,  h the  height  above  the 
φ   the  angle  in  the 
interpreted as the phase angle of the motion.  

y   plane,  and 
y plane.  The  angle  φ   can  also  be 
θ

,r,h) where r is  the 
−θ

−θ

x

x

θ

Given the loop topology of the latent training points, one 
can parameterize  r  and  h  as a function of φ . The embed-
ded action space is represented by a learned approximation 
of the function: 

[

r,h

]

g=

(φ)

                                    (2) 

≤ ≤

where  0 φ 2π
formed by using a radial basis function (RBF) network. 

.  Approximation  of  this  function  is  per-

4 Learning to Predict Sensory Consequences 

of Actions 

S = ×Z

. In general, the state space 

A central component of our proposed framework is learning 
to  predict  future  sensory  inputs  based  on  actions  and  using
this learned predictive  model  for action selection. The  goal 
is  to  predict  the  future  sensory  state  of  the  robot,  denoted 
by t+1s
product  of  the  high-dimensional  joint  space  Z   and  the 
space of other percepts Ρ . Other percepts could include, for 
example,  a  torso  gyroscope,  an  accelerometer,  and  foot 
pressure sensors as well as information from camera images. 
S(cid:2)  that maps 
The goal then is to learn a function
the current state and action to the next state. For this paper, 
we assume that  F  is deterministic. 

 is the Cartesian 

F S A

:  ×

Ρ

Often  the  perceptual  state 

ts   is  not  sufficient  for  pre-
dicting  future  states.  In  such  cases,  one  may  learn  a  higher 
order  mapping  based  on  a  history  of  perceptual  states  and 
actions, as given by an -thn

 order Markovian function: 

s = F (s
t

t-n

,...,s

t-1

,a

t-n

,...,a

t-1

)       

 

 

(3) 

We  use  a  radial  basis  function  (RBF)  approximator  to 
learn  F  from  sensory-motor  experience.  In  particular,  the 
RBF  network  approximates  F  by  learning  a  function 
F' (cid:2) : 
 β

 : α 

(

w

k

exp

β = 

K

∑

k

(α μ )
− −

T

k

∑

−

1

k

(α μ )

−

k

)

,        (4) 

where K represents  the  number  of  kernels,  μ
the  mean  and  inverse  covariance  of  the -thk

1

−Σ are 
k and 
kernel  respec-

k

IJCAI-07

2086

[

= s , s

tively.  The  output  weight  vector 

kw   scales  the  output  of 
each  kernel  appropriately,  and  the  input  and  output  are  
]
  and β =  t+1s
α 
tively. For convenience, one can instead view the RBF as a 
time  delay  network  [Lang  et  al.,  1990]  for  which  the  input 
s ,a .  The  previous  state  and  action  in-
simplifies  to
t

  respec-

α = 

,a ,a

,...,a

,...,s

t-n-1

t-n-1

t-1

t-1

]

[

t

t

t

puts are implicitly remembered by the network using recur-
rent feedback connections. 

In  this  paper,  we  use  a  second-order  (n  =  2)  RBF  net-
work  with  the  state  vector  equal  to  the  three-dimensional 
≡s ω .  As  discussed  in  the  previous 
gyroscope  signal (
section,  an  action  represents  the  phase  angle,  radius,  and 
height of the data in latent posture space (

X .  

χ≡ ∈

a

)

)

t

t

t

5  Motion Optimization using the Learned 

Predictive Model 

The  algorithm  we  present  in  this  section  utilizes  optimiza-
tion  and  sensory  prediction  to  select  optimal  actions  and 
control  the  humanoid  robot  in  a  closed-loop  feedback  sce-
nario. Figure 5 illustrates the optimization process.  

One may express the desired sensory states that the robot 
should  attain  through  an  objective  function ( )Γ s .  Our  algo-

rithm  then  selects  actions 

,...,

a such  that  the  predicted 

future states

s will be optimal with respect to   ( )Γ s : 

*
a
t

*
T

T

,...,
s
t
*  = arg min
a
t

(
FΓ

(
s

t

,...,
s

t-n

,
a

t

,...,

a

t-n

)

.     

 (5) 

)

a

t

Controller
Controller

 
 
 
 
 
 
 
 
 
 
 
 
 
 

ωmin
ωmin

Optimization
Optimization

Algorithm
Algorithm

χ′
χ′

χ
χ

Model Predictor
Model Predictor

ωp
ωp

ωa
ωa

Humanoid Robot
Humanoid Robot

Gyroscope signal
Gyroscope signal

ωmin = minimum gyroscope signal
ωmin = minimum gyroscope signal
ωa      = actual gyroscope signal
ωa      = actual gyroscope signal
ωp      = predicted gyroscope signal
ωp      = predicted gyroscope signal

χ
χ

χ′
χ′

= posture command
= posture command

= tentative posture command
= tentative posture command

 

 
Figure 5. Model predictive controller for optimizing posture stabil-
ity.  The optimization algorithm and the sensory-motor model pre-
dictor  produce  the  action  (
control  of  the  humanoid  robot.  The  resulting  gyroscope  signal  is 
fed back to the predictor for retraining. The optimization algorithm 
ω in order to optimize actions 
utilizes a predicted gyroscope signal

X which  is  used  for  posture 
)

χ≡ ∈

a

t

p

for posture stability. 
 

 
The  objective  function  used  in  this  paper  is  a  measure 
of  torso  stability  as  defined  by  the  following  function  of 
gyroscope signals:   
 

Γ

(ω) = λ ω

2
x x

+

λ ω
2
y y

+

λ ω
z

2
z

,     

 

  (6) 

 
where x

ω ,ω ,ω  

z

y

to 

refer 

signals 

gyroscope 

the x, y, z axes  respectively.  The  constants 

in 
λ ,λ ,λ allow 
one to weight rotation in each axis differently. The objective 
function  (6)  provides  a  measure  of  stability  of  the  posture 
during  motion.  For  our  second-order  predictive  function  F, 
the optimization problem becomes one of searching for op-
timal stable actions given by:  
 

z 

x

y

χ  = arg min
∈

*
t

χ

t

(

Γ

(ω , ω , χ ,χ

1
t-

t

F

)

1
t-

)

  

   (7) 

 

⎡
⎢
= ⎢
⎢
⎣

⎤
⎥
⎥
⎥
⎦

 

 

 

 

 

 

 

 (8) 

t
φ

s

r
s
h
s

To allow for efficient optimization, we restrict the search 
space to a local region in the action subspace as given by: 

   

r -
a
h -

a

 

t

s

-1

φ

φ

≤
≤

ε
+
ε

φ < φ
-1
t
ε
≤
   
r
r +
s
r
a
ε
ε
≤
≤
h
h +
s
0 < ε < 2π  
(φ )

φ
g=

h
 

   

r ,h
a
a

[

]

h

a

s

r

   

 

 

 

 

(9) 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

(10) 

(11) 

(12) 

(13) 

t

r

-1

φ

ah and ε

 
The  phase  motion  command  search  range  φ
s begins  after 
the  position  of  the  phase  motion  command  at  the  previous 
time  step
.  The  radius  search  range sr begins  from  a 
point in the action subspace embedding  A that is defined by 
(12) in both positive and negative directions from  ar  along 
r  for the distance ε
> . The search range sh  is defined in 
0
the  same  manner  as sr according  to 
h .  In  the  ex-
periments,  the  parameters  φε , ε
h   were  chosen  to 
ensure efficiency while at the same time allowing a reason-
able range for searching for stable postures. An example of 
the search space for a walking motion is shown in Figure 6. 
Selected  actions  will  only  truly  be  optimal  if  the  sen-
sory-motor  predictor  is  accurate.  We  therefore  periodically 
re-train the prediction model based on the new posture com-
mands generated by the optimization algorithm and the sen-
sory  feedback  obtained  from  executing  these  commands. 
After  three  iterations  of  sensory-motor  prediction  learning, 
an improved dynamically balanced walking gait is obtained. 
The  trajectory  of  the  optimized  walking  gait  in  the  low  di-
mensional subspace is shown in Figure 6.  

r and  ε

 

 

IJCAI-07

2087

4
4

2
2

0
0

-2
-2

-4
-4

-5
-5

0
0

 

Search space
Search space

Original data
Original data

Embed function
Embed function

Optimized data
Optimized data

7)    Repeat  steps  4  through  6  until  a  satisfactory  gait  is 

obtained. 

6 Experimental Results 

Target pattern

Final result 

Scale 0.7 

5
5

-4
-4

-2
-2

0
0

2
2

4
4

4

2

0

-2

-4

Scale 0.5 

Scale 0.3

5

0

Figure  6.  Optimization  result  for  a  walking  motion  pattern  in  a 
low-dimensional  subspace  based  on  an  action  subspace  embed-
ding. 

We  summarize  below  the  entire  optimization  and  action 
selection process: 

1)  Use PCA to represent in a reduced 3D space the ini-

tial walking gait data from human motion capture. 

2)   Employ the non-linear embedding algorithm for pa-

rameterization of the gait. 

3)  Start the learning process by projecting actions back 
to  the  original  joint  space  and  executing  the  corre-
sponding sequence of servo motor commands in the 
Webots HOAP-2 robot simulator [Webots, 2004]. 

4)  Use the sensory and motor inputs from the previous 
step  to  update  the  sensory-motor  predictor  as  de-
scribed  in  Section  4  where  the  state  vector  is  given 
by the gyroscope signal of each axis and the action
variables are  φ,r and  h  in the low-dimensional sub-
space. 

5)  Use the learned model to estimate actions according 
to  the  model  predictive  controller  framework  de-
scribed above (Figure 5).  

6)  Execute computed actions and record sensory (gyro-

scope) feedback.  

4

3

2

1

0

-1

-2

-5

-3

Figure  7.  Motion  pattern  scaling.  The  target  motion  pattern  is 
scaled down until it can produce a stable motion to start the motion 
optimization process. 

This section explains how the optimization methodology in 
the  previous  section  is  used  in  conjunction  with  the  mocap 
data.  From  our  study  of  the  motion  pattern  in  the  reduced 
subspace, we found that we can scale up and down the mo-
tion  pattern  and  get  similar  humanoid  motion  patterns  ex-
cept for changes in the magnitude of motion. When we scale 
down  the  pattern  in  the  reduced  subspace,  it  produces  a 
smaller  movement  of  the  humanoid  robot,  resulting  in  s-
maller  changes  in  dynamics  during  motion.  Our  strategy  is 
to scale down the pattern until we find a dynamically stable 
motion and start learning at that point. We apply the motion 
optimization method in Section 5 to the scaled-down pattern 
until  its  dynamic  performance  reaches  an  optimal  point; 
then  we  scale  up  the  trajectory  of  the  optimization  result 
toward  the  target  motion  pattern.  In  our  experiments,  we 
found that a scaling down of 0.3 of the original motion pat-
tern is typically stable enough to  start the learning process.  
Our  final  optimization  result  obtained  using  this  procedure 
is shown as a  trajectory of red circles in Figure 7. It corre-
sponds to about 80% of the full scale motion. 

IJCAI-07

2088

7 Conclusion 

Our results demonstrate that  a humanoid robot can learn to 
walk  by  combining  a  learned  sensory-motor  model  with 
imitation of a human gait. Our approach does away with the 
need  for  detailed,  hard-to-obtain,  and  often  fragile  physics-
based  models that previous  methods have relied on  for sta-
ble  gait  generation  in  humanoids.  Our  approach  builds  on 
several previous approaches to humanoid motion generation 
and  imitation.  Okada,  Tatani  and  Nakamura  [Okada  et  al., 
2002] first applied non-linear principal components analysis 
(NLPCA) [Kirby and Miranda, 1996] to human and human-
oid  robot  motion  data.  The  idea  of  using  imitation  to  train 
robots has been explored by a number of researchers [Hayes 
and  Demiris,  1994],  [Billard,  2001].  In  [Ijspeert  et  al., 
2001], a nonlinear dynamical system was carefully designed 
to produce imitative behaviors. The mimesis theory of [Ina-
mura et al., 2003] is based on action acquisition and action 
symbol  generation  but  does  not  address  dynamics  compen-
sation  for real-time biped locomotion. The  motion segmen-
tation  framework  in  [Jenkins  and  Mataric,  2003]  uses  di-
mensionality  reduction  and  segmentation  of  motion  data  in 
the  reduced  dimensional  space  but  without  dynamics  com-
pensation.  

The framework described in this paper has several practi-
cal applications. One scenario we are currently investigating 
is  a  general  navigation  task  involving  our  humanoid  robot. 
A  modular  architecture  could  be  used  to  switch  between  a 
set  of  learned  modules,  such  as  walking-straight,  turning-
left,  turning-right  and  stepping-backward.  Using  visual  in-
formation  as  feedback,  we  could  control  robot  direction  by 
switching  behaviors  (actions)  of  the  robot.  We  have  also 
found  that  translation  in  the  x-y  plane  in  the  reduced  3D 
latent space can be used for changing direction. Because our 
method does not depend on a model of the world, it can be 
applied  to  the  problems  of  learning  to  walk  up  and  down 
stairs,  and  walking  on  constant  slopes.  To  learn  actions 
other than walking straight, we can modify our optimization 
function in Eq. (6) or introduce additional sensory variables 
such  as  velocity  or  foot-contact  pressure.  We  are  currently 
investigating these lines of research.  

Clearly, the present framework cannot be applied directly 
to  the  problem  of  navigation  on  uneven  terrain.  To  effec-
tively  navigate  on  uneven  terrain,  we  may  need  a  higher 
degree  of  compliance  control  in  the  leg  and  foot  actuators. 
However,  a  hybrid  active-passive  actuator  will  likely  pro-
duce  even  more  complex  dynamics  than  typical  actuators 
used  in  current  humanoid  robots.  Since  our  approach  does 
not require a physics-based dynamics model for learning, it 
lends  itself  naturally  to  tackling  this  problem.  We  hope  to 
investigate  this  important  research  direction  in  the  near  fu-
ture.  

Figure  8.  Learning  to  walk  through  imitation.  The  pictures  in  the 
first row show a human subject demonstrating a walking gait in a 
motion  capture  system.  The  second  row  shows  simulation  results 
for  this  motion  before  optimization.  The  third  row  shows  simula-
tion results after optimization. The last row shows results obtained 
on the real robot.

Our simulation and experimental results are shown in Figure 
8. We performed the learning process in the simulator [We-
bots, 2004] and tested the resulting motion on the real robot. 
The  walking  gait  on  the  real  robot  is  not  as  stable  as  the 
results  in  the  simulator  because  of  differences  in  frictional 
forces  between  the  simulator  and  the  floor.  We  expect  an 
improvement  in  performance  when  learning  is  performed 
directly on the real robot. We note that the learned motion is 
indeed  dynamic  and  not  quasi-static  motion  because  there 
are only two postures in our walking gait that can be consid-
ered statically stable, namely, the two postures in the walk-
ing cycle when the two feet of the robot contact the ground. 
The remaining postures in the walking gait are not statically 
stable as the gait has a fairly fast walking speed.

IJCAI-07

2089

ing of the Second International Symposium on Imitation 
in Animals and Artifacts, pages 4-14, 2003. 

[Sutton and Barto, 1998] Richard S. Sutton and Andrew G. 
Barto. An Introduction to Reinforcement Learning. MIT 
Press, 1998.  

[Vukobratovic  and  Borovac,  2004]  Miomir  Vukobratovic 
and  Branislav  Borovac.  Zero  moment  point-thirty-five 
years of its life. International Journal of Humanoid Ro-
botics, 1(1):157–173, 2004. 

[Webots,  2004]  Webots,  “http://www.cyberbotics.com,” 
commercial  Mobile  Robot  Simulation  Software. 
[Online]. Available: http://www.cyberbotics.com 

[Yamaguchi  et  al.,  1996]  J.  Yamaguchi,  N.  Kinoshita,  A. 
Takanishi, and I. Kato. Development of a dynamic biped 
walking  system  for  humanoid:  development  of  a  biped
walking  robot  adapting  to  the  humans’  living  floor.  In 
Proceeding  of  1996  IEEE  Conference  on  Robotics  and 
Automation, pages 232–239, 1996. 

References 

[Billard,  2001]  A.  Billard.  Imitation:  a  means  to  enhance 
learning of a synthetic proto-language in an autonomous 
robot.  In  Dautenhahn,  K.  and  Nehaniv,  C.  (eds),  Imita-
tion  in  Animals  and  Artifacts,  Academic  Press,  pages 
281-311, 2001.  

[Grochow  et  al.,  2004]  Keith  Grochow,  S.  L.  Martin,  A. 
Hertzmann,  and  Z.  Popovic.  Style-based  inverse  kine-
matics.  ACM  Transaction  on  Graphics,  23(3):522–531, 
2004. 

[Hayes  and  Demiris,  1994]  Hayes,  G.  and  Demiris,  J.  A 
robot  controller  using  learning  by  imitation.  In  Interna-
tional  Symposium  on  Intelligent  Robotic  Systems,  pages 
198 204, Grenoble 

 [Ijspeert et al., 2001] Auke Jan Ijspeert, Jun Nakanishi and 
Stefan  Schaal.  Trajectory  formation  for  imitation  with 
nonlinear  dynamical 
In  Proceeding  of 
IEEE/RSJ  International  Conference  on  Intelligent  Ro-
bots and Systems, pages 752–757, 2001. 

systems. 

[Inamura  et  al.,  2003]  Tetsunari  Inamura,  Iwaki  Toshima 
and  Yoshihiko,  Nakamura.  Acquiring  motion  elements 
for bi-directional computation of motion recognition and 
generation.  In  Siciliano,  B.,  Dario,  P.,  Eds.,  Experimen-
tal Robotics VIII. Springer, pages 372–381, 2003. 

[Jenkins  and  Mataric,  2003]  Odest  C.  Jenkins  and  Maja  J. 
Mataric.  Automated derivation of behavior  vocabularies 
for autonomous humanoid motion. In Proceedings of In-
ternational Conference on Autonomous Agents and Mul-
tiagent Systems (AAMAS), pages 225–232, 2003. 

[Kajita and Tani, 1996] S. Kajita and K. Tani. Adaptive gait 
control of a biped robot based on real time sensing of the 
ground profile. In Proceeding of 1996 IEEE Conference 
on Robotics and Automation, pages 570–577, 1996. 

[Kirby  and  Miranda,  1996]  Michael  J.  Kirby  and  Rick
Miranda.  Circular  nodes  in  neural  networks.  Neural 
Computation. 8(2):390–402, 1996. 

 [Lang et al, 1990]  K. J. Lang, A. H. Waibel, and G. E. Hin-
ton.  A  time-delay  neural  network  architecture  for  iso-
lated  word  recognition.  Neural  Networks,  3(1):23–43, 
1990. 

 [MacDorman et al., 2004] Karl F. MacDorman, Rawichote 
Chalodhorn, and Minoru Asada. Periodic nonlinear prin-
cipal  component  neural  networks  for  humanoid  motion
segmentation,  generalization,  and  generation.  In  Pro-
ceedings of the Seventeenth International Conference on 
Pattern Recognition,  pages 537–540, 2004. 

 [Okada et al., 2002] Masafumi Okada, Koji Tatani and Yo-
shihiko  Nakamura.  Polynomial  design  of  the  nonlinear 
dynamics for the brain-like information processing of the 
whole  body  motion.  In  Proceeding  of  2002  IEEE  Inter-
national Conference on Robotics and Automation, pages 
1410–1415, 2002. 

[Rao and Meltzoff, 2003] Rajesh P. N. Rao and Andrew N. 
Meltzoff. Imitation  Learning  in Infants and  Robots: To-
wards  Probabilistic  Computational  Models.  In  Proceed-

IJCAI-07

2090


Modular self-organization for a long-living autonomous agent 

Bruno  SCHERRER 

scherrer@loria.fr 

LORIA, BP 239 

54506 Vandceuvre-les-Nancy 

France 

Abstract 

The aim of this paper is to provide a sound frame(cid:173)
work  for addressing a difficult  problem:  the auto(cid:173)
matic construction of an autonomous agent's mod(cid:173)
ular  architecture.  We  briefly  present  two  appar(cid:173)
ently uncorrelated frameworks:  Autonomous plan(cid:173)
ning through Markov Decision Processes and Ker(cid:173)
nel  Clustering.  Our  fundamental  idea  is  that  the 
former  addresses  autonomy  whereas  the  latter  al(cid:173)
lows  to  tackle  self-organizing  issues.  Relying 
on  both  frameworks,  we  show  that  modular  self-
organization can be formalized as a clustering prob(cid:173)
lem  in  the  space  of MDPs.  We  derive  a  modular 
self-organizing algorithm in which an autonomous 
agent learns to efficiently  spread n  planning prob(cid:173)
lems over m  initially  blank modules with rn  <  n. 

Introduction 
This  paper  addresses  the  problem  of building  a  long-living 
autonomous  agent;  by  long-living,  we  mean  that  this  agent 
has  a  large  number of relatively  complex  and  varying  tasks 
to perform.  Biology suggests some ideas about the way an(cid:173)
imals  deal  with  a  variety  of tasks:  brains  are  made  of spe(cid:173)
cialized and complementary areas/modules; skills arc spread 
over modules.  On  the  one hand,  distributing  functions and 
representations has immediate advantages:  parallel process(cid:173)
ing  implies  reaction  speed-up;  a  relative  independence  be(cid:173)
tween modules gives more robustness.  Both properties might 
clearly increase the agent's efficiency. On the other hand, the 
fact of distributing a system raises a fundamental issue:  how 
does the organization process of the  modules happen during 
the life-time ? 

There has been much research about the design of modu(cid:173)
lar  intelligent  architectures  (e.g.  [Theocharous  et al,  2000J 
[Hauskrechtef a/.,  1998]  [Kaelbling,  1993]).  It is neverthe(cid:173)
less  very  often  the  (human)  designer  who  decides  the  way 
modules  are  connected  to  each  other and  how  they  behave 
with  respect  to  the  others.  Few  works  study  the  construc(cid:173)
tion  of these  modules.  To  our knowledge,  there  are  no  ef(cid:173)
fective works about modular self-organisation except for re(cid:173)
active tasks (stimulus-response associations) (e.g.  [Jacobs et 
al.,  1991] [Digney,  1996]). 

1440 

This paper proposes an  algorithm by which the organiza(cid:173)
tion of an agent in functional modules is automatically com(cid:173)
puted.  The  most  significant  aspect  of our  work  is  that  the 
number m of modules  is fewer than the number n of tasks to 
be performed.  Therefore, the approach we propose involves 
a high-level clustering process, in which the n tasks need to 
be "properly" spread over the m modules. 

Section  1  introduces what we  consider as the  theoretical 
foundation  for  modelling  a  mono-task  autonomous  agent: 
Markov  Decision  Processes.  Section  2  presents  the  Kernel 
Clustering approach: we consider this approach as a theoret(cid:173)
ical basis for addressing self-organization. Finally, Section 3 
combines both  domains  in  order to  propose a  modular self-
organizing algorithm. 

1  Modelling A Mono-Task Autonomous Agent 
Markov  Decision  processes  [Puterman,  1994]  provide  the 
theoretical foundations of challenging problems such as plan(cid:173)
ning  under  uncertainty  and  reinforcement  learning  [Sutton 
and Barto, 1998]. They stand for a fundamental model for se(cid:173)
quential decision making and they have been applied to many 
real  worls  problems  [Sutton,  1997].  This  section  describes 
this formalism and presents a general  scheme for approach(cid:173)
ing difficult problems (that is problems in large domains). 

A Markov Decision Process (MDP) is a controlled stochas(cid:173)
tic process satisfying the Markov property with rewards (nu(cid:173)
merical values) assigned to state-control pairs.  Formally, an 
MDP is a four-tuple (S, A, T, R)  where S is the state space, 
A is the action space, T is the transition function and R. is the 
reward function. T is the state-transition probability distribu(cid:173)
tion conditioned by the control: 

(0 
is  the  instantaneous  reward  for  taking  action 

A in state S. 

The usual MDP problem consists in  finding  an optimal pol(cid:173)
icy, that is a mapping 
A  from states to actions, that 
maximises  the  following  performance criterion,  also  called 
value function of policy TT: 

: S 

(2) 

POSTER  PAPERS 

It is shown that there exists a unique optimal value function 
V*; once V* is computed, an optimal policy can immediately 
be derived (e.g. see [Puterman, 1994]). 

In  brief,  solving  an  MDP problem  amounts to computing 
the optimal value function. Well-known algorithms for doing 
so  are  Value  Iteration  and  Policy  Iteration  (see  [Puterman, 
1994]).  Their temporal  complexity dramatically  grows  with 
the number of states  [Littman  et al,  1995],  so they can only 
be applied in small domains. 

In  large  domains,  it  is  impossible  to  solve  an  MDP  ex(cid:173)
actly,  so one usually  adresses a complexity/quality compro-
mis through an approximation scheme.  Ideally, an approxi(cid:173)
mation scheme for MDPs should consist of a set of tractable 
algorithms for 

•  computing an approximate optimal value function 
•  evaluating (an upper bound of) the approximation error 
•  improving the quality of approximation (by reducing the 
approximation error) while constraining the complexity. 
The first two points are the fundamental theoretical bases for 
sound approximation.  The third one  is often  interpreted as 
a  learning  process  and  corresponds  to  what  most  Machine 
Learning researchers study. For convenience, we respectively 
call  these  three  procedures  Appproximate(),  Error  ()  and 
Learn().  Then, the practical use of an approximate scheme 
can be sketched by algorithm 1.  One successively applies the 

Given a set of kernels 
a data point x is naturally 
associated to its most representative kernel L{x), i.e.  the one 
that is the closest according to distance d: 

Conversely, a set of kernels 
partition of the data set 
each class corresponding to a kernel: 

into  m  classes   

(3) 
naturally induces a 

(4) 
Given a data space, a data set, a kernel space and a distance 
d(),  the goal of the  Kernel  Clustering problem  is to find the 
that  minimizes  the  distortion  D 
set  of kernels 
for the data set 

which is defined as follows: 

A  general  procedure  for  suboptimally  solving  this  problem 
is  known  as  the  Dynamic  Cluster algorithm  [Diday,  19731, 
which we present in an online version  in algorithm 2. 
It is 

(5) 

Lcarn()  procedure  in  order  to  minimize  the  approximation 
error;  when this  is done,  one can compute a good approxi(cid:173)
mate optimal value function. 

2  Kernel Clustering 
Before addressing the problem of modular self-organization, 
we need to present the Kernel  Clustering paradigm.  In  [Di-
day,  1973],  the  author introduces the  Kernel  Clustering  ap(cid:173)
proach  as  an  abstract  generalization  of vector quantization. 
Indeed, the author argues that, in general, a clustering prob(cid:173)
lem is based on three elements: 

• 
• 

• 

a set of data points taken from a data space A" 
a set of kernels taken from a kernel space 

A distance measure between any data 
point and any kernel.  The smaller the distance d(x, L), 
the more L is representative of the point x. 

a  very  intuitive  process:  for each  piece  of data  x,  one finds 
its  most  representative  kernel  L,  and  one updates  L  so that 
it  gets  even  more  representative  of x.  Little  by  little,  one 
might expect that such a procedure will  minimize the global 
distortion and eventually give a good clustering. 

3  Modular Self-Organization 
This final section shows how the Kernel Clustering paradigm 
can  be  used  to  formalize  a  modular self-organization prob(cid:173)
lem in the MDP framework, the algorithmic solution of which 
will  be given by the on-line Dynamic Cluster procedure (al(cid:173)
gorithm 2). 

If one carefully compares the general  learning scheme we 
have described in  order to address a  large state  space MDP 
(algorithm 1) and the on-line Dynamic Cluster procedure (al(cid:173)
gorithm 2), one can see that the former is a specific case of 
the latter. More precisely, algorithm 1 solves a simple Kernel 
Clustering problem where 

•  the data space is the space of all possible MDPs and the 

data set is a unique task corresponding to an MDP M 

POSTER  PAPERS 

1441 

•  the kernel space is the space of all possible approxima(cid:173)

tions and there is one and only one kernel:  

•  the  distance  d is  the  Error()  function. 
It  is  then  straightforward to extend  this  simple  clustering 
problem to a more general one (with n tasks/data points and 
m  approximate  models/kernels).  Given  a  set of m  approxi(cid:173)
mate models 
is naturally associ(cid:173)
ated to the approximate model 
that makes the small(cid:173)
est error: 

}, an MDP 

As  before,  a set of approximate models 
urally  induces a partition  of any  set of n  MDPs 
into  m  classes 
approximate model: 

each class corresponding to an 

(6) 

nat(cid:173)

(7) 
The transpositon of the on-line Dynamic Cluster Algorithm 
into  the  MDP  framework  (algorithm  3)  therefore  allows  to 
find  a  set  of m  approximate  models  that  globally  minimize 
the approximation error for n MDPs: 

problem in  the  space  of MDPs.  A natural  algorithmic  solu(cid:173)
tion to this clustering problem (algorithm 3) uses an on-line 
version of the Dynamic Cluster algorithm (algorithm 2).  Due 
to lack of space, we could not show any experimental evalu(cid:173)
ation;  interested readers  will  find  some in  [Scherrer, 2003bl 
and [Scherrer, 2003a]. 

References 
[Diday,  1973J  E.  Diday. 

The  dynamic  clusters  method 
and  optimization  in  non  hierarchical-clustering. 
In 
SpringerVerlag,  editor,  5th  Conference  on  optimization 
technique,  Lecture  Notes  in  Computer  Science  3,  pages 
241-258,1973. 

[Digney,  1996]  B.  Digney.  Emergent  hierarchical  control 
structures:  Learning reactive hierarchical relationships in 
reinforcement environments,  1996. 

[Hauskrecht et al., 1998] M. Hauskrecht, N. Meuleau, L. P. 
Kaelbling, T.  Dean,  and C.  Boutilier.  Hierarchical solu(cid:173)
tion  of Markov  Decision  Processes  using  macro-actions. 
In  Uncertainty  in  Artificial  Intelligence,  pages  220-229, 
1998. 

[Jacobs et  al,  1991]  R.  Jacobs,  M.  Jordan,  and  A.  Barto. 
Task  decomposition  through  competition  in  a  modular 
connectionist  architecture:  The  what  and  where  vision 
tasks.  Cognitive Science, 15:219-250, 1991. 

[Kaelbling,  1993]  L.  P.  Kaelbling.  Hierarchical  learning  in 
stochastic  domains:  Preliminary  results.  In  International 
Conference on Machine Learning, pages  167-173,  1993. 
[Littmanefa/.,  1995]  M.  L.  Littman,  T.  L.  Dean,  and L.  P. 
Kaelbling.  On the complexity of solving Markov decision 
In  Proceedings  of the  Eleventh  Annual  Con(cid:173)
problems. 
ference  on  Uncertainty  in  Artificial  Intelligence  (UAI-95), 
pages 394-402, Montreal, Quebec, Canada, 1995. 

[Puterman,  1994]  M. Puterman. Markov Decision Processes. 

Wiley, New York,  1994. 

[Scherrer, 2003a]  B.  Scherrer. 

Apprentissage  de 
representation  et  auto-organisation  modulaire  pour 
un agent autonome. PhD thesis, Universite Henri Poincare 
- Nancy 1, January 2003. 

[Scherrer, 2003b]  Bruno  Scherrer. 

Modular  Self-
long-living  autonomous  agent. 

Organization 
Technical report, INR1A, April 2003. 

for  a 

order to efficiently solve n tasks, or, as we might say,  it self-
organizes the  m  modules  in  order to  improve the  resolution 
of the n tasks. 

Conclusion 
In  this  paper,  we  have  described  a  general  scheme  for  ad(cid:173)
dressing  large  state  space Markov  Decision Processes.  We 
have then showed how such an approach could be extended 
to an interesting problem: modular self-organization. Indeed, 
we have formalized modular self-organization as a clustering 

[Sutton and Barto,  1998]  R.S. Sutton and A.G. Barto.  Rein(cid:173)
forcement Learning, An  introduction.  BradFord Book.  The 
MIT Press,  1998. 

[Sutton, 1997]  Richard  S.  Sutton.  On  the  significance  of 
In  ICANN,  pages  273-282, 

markov  decision  processes. 
1997. 

lTheocharous et al.,2000] G.  Theocharous,  K.  Rohani-
manesh, and S. Mahadevan.  Learning and planning with 
hierarchical  stochastic  models  for  robot  navigation. 
In 
ICML  2000  Workshop  on  Machine  Learning  of  Spatial 
Knowledge, Stanford University, July 2000. 

1442 

POSTER PAPERS 


Estimating the Rate of Web Page Updates

Sanasam Ranbir Singh

Indian Institute of Technology Madras

Department of Computer Science and Engineering

Chennai-36, India

ranbir@lantana.tenet.res.in

Abstract

Estimating the rate of Web page updates helps in
improving the Web crawler’s scheduling policy.
But, most of the Web sources are autonomous and
updated independently. Clients like Web crawlers
are not aware of when and how often the sources
change. Unlike other studies,
the process of
Web page updates is modeled as non-homogeneous
Poisson process and focus on determining local-
ized rate of updates. Then various rate estimators
are discussed, showing experimentally how precise
they are. This paper explores two classes of prob-
lems. Firstly the localized rate of updates is esti-
mated by dividing the given sequence of indepen-
dent and inconsistent update points into consistent
windows. From various experimental comparisons,
the proposed Weibull estimator outperforms Du-
ane plot(another proposed estimator) and other es-
timators proposed by Cho et la. and Norman Mat-
loff in 91.5%(90.6%) of the whole windows for
synthetic(real Web) datasets. Secondly, the future
update points are predicted based on most recent
window and it is found that Weibull estimator has
higher precision compared to other estimators.

1 Introduction
Most of the information available on the Web are autonomous
and are updated independently of the clients. Estimating the
rate at which Web pages are updated, is important for Web
clients like Web crawlers, Web caching etc. to improve their
performance. Since the sources are updated on their own,
such a client does not know exactly when and how often the
sources change. Web crawlers need to revisit the sources re-
peatedly to keep track of the changes and maintain the repos-
itory upto-date. Further, if a source is revisited before it is
changed, then it will be a wastage of resources such as CPU
time, ﬁle system and network bandwidth for both client and
server. Such a visit is deﬁned as a premature visit. For a client
like a Web crawler with a collection of thousands of millions
of such sources, it is important to schedule its visits optimally
such that number of unobserved changes between the subse-
quent visits and the number of premature visits are minimum.
Therefore, study of the changing behavior of web pages and

estimating their changing rate can beneﬁt in improving Web
crawler’s scheduling policy, Web page popularity measure,
preprocessing the accessed schedules etc. This is the main
motivation of this work. Another question that is attempted
to answer in this paper is the localized rate of changes of the
sources. For example, At what rate was a Web page updated
during the month of January last year? How does the rate of
updates change during the month of January for the last ﬁve
years? This paper focuses mainly on estimating localized rate
of updates rather than global update rates. Since the rate of
updates of the sources may vary with time, localized estima-
tion provides more detail, useful and accurate information.

Contrast to the previous studies [Cho and Garcia-Molina,
2003; Matloff, 2005], the process of Web page updates is
modeled as time-varying Poisson process. It is because of the
fact that sources are updated by its owner autonomously and
independently whenever there are new developments and the
rate of updates may vary with time. Therefore, it is more ap-
propriate to assume the update process as a time varying Pois-
son process rather than time homogeneous Poisson process.
The advantage of modeling the problem as non-homogeneous
Poisson process is its wide range of ﬂexibilities. It can be
noted that homogeneous Poisson process is a special case
of non-homogeneous Poisson process. A major difﬁculty in
modeling the problem as non-homogeneous Poisson process
is that it has inﬁnitely many parameters. In particular, it is
parameterized by its arrival-rate λ(t), which varies with time.
To simplify the problem, the attention is restricted to certain
assumptions of λ(t) i.e., constant, increasing or decreasing.
Such assumption help in reducing the number of parameters.
This assumption may not be correct over a long period of ob-
servations (say, over years of page update records), but rea-
sonable over appropriate subintervals (say, over few weeks).
Therefore, the whole observation space is divided into a se-
quence of windows of consistent behavior (see Section 4).
The above assumption is reasonable as this paper is focussed
more on the instanteneous page update rates.

1.1 Accessing the Web pages

One major problem in modeling the changing behavior of
Web page is that, the complete information of the sequence
of updates is not available. Web pages are accessed repeat-
edly through normal activities such as periodic crawling of
web pages. It is only possible to determine, whether a page

IJCAI-07

2874

had changed, but not possible to determine the number of
changes between two subsequent accesses, which results in
loss of information. The accuracy of the estimation of the rate
of changes depends on how small the number of unobserved
updates is.

modeling the process as a non-homogeneous Poisson process
includes the ability to determine the instantaneous update rate
i.e., λ(t). In summary, this paper makes the following contri-
butions:

-handles the last date of change and existence of change

and ith

Assuming that

the Web pages are accessed periodi-
cally with a small interval τ , let m(i) be the number of
page updates between the (i − 1)th
accesses and
{t(i,1), t(i,2), t(i,3), ..., t(i,m(i))} be the time sequence of cor-
responding updates. The changes can be observed in the fol-
lowing two ways:
-Last date of change: Some Web servers provide the in-
formation about when the page was last modiﬁed.
It is
not possible to obtain the complete update information be-
tween accesses. In such case, we only observe t(i,m(i)) and
{t(i,1), t(i,2), t(i,3), ..., t(i,m(i)−1)} are left unobserved. There
is no way to obtain the unobserved information. Assuming
that m(i) is small, we ignore the unobserved updates and
count only one update during ((i − 1)τ, iτ ] if changes occur
and consider t(i,m(i)) as the renewal point.
-Existence of change: Some Web servers do not provide
such information i.e., last date of change. In such case, it
is only possible to determine whether the page has changed
between accesses comparing with the old image of the page.
We can not even determine t(i,m(i)). It could have changed
any number of times anywhere between (i − 1)th
access and
ith

access, i > 0. If changes occur, it is counted as one.

1.2 Formulating the Process

In the studies [Cho and Garcia-Molina, 2003; Matloff, 2005],
the two classes of observing updates are handled separately.
Unlike these studies, the observed information is prepro-
cessed and a model is formulated to handle both the cases
identically simplifying the problem. For the case of last date
of change, the t(k,m(k)) is considered as the update point of
the kth
interval and the process of update time is formulated
as {t(i) | t(i) = t(k,m(k)), i = N (kτ ), i, k > 0}, where N (t)
is the number of updates during the period (0, t].

For the case of existence of change, the t(k,m(k)) is not
available. Assuming that sources are accessed periodically
with an optimal interval, we can approximate t(i), say the
middle point or the end point of the period or a point de-
termined from an appropriate distribution. Here, the t(i) is
approximated as the middle point and the process of update
time is formulated as {t(i) | t(i) = (k − 1)τ + τ /2, i =
N (kτ ), i, k > 0}, where N (t) is the number of observed up-
dates during the period (0, t].

Thus, the time sequence {t(i) : i = 0, 1, 2, ...} represents
the sequence of the update points for both the cases satisfy-
ing the properties t(0) = 0 and t(i) < t(i + 1), where t(i) is
the time of the ith
update. Since the client does not know ex-
actly when and how often the Web page updates, the sequence
{t(i) : i = 0, 1, 2, ...} is an independent and identically dis-
tributed random process, which is often modeled as a Poisson
process in several studies [Brewington and Cybenko, 2000;
Cho and Garcia-Molina, 2003; Matloff, 2005]. This study
also assumes the model as a Poisson process, but as a non-
homogeneous Poisson process. One of the advantages of

cases identically.

-models the system as a non-homogeneous Poisson pro-

cess.

-proposes two λ(t) estimators namely Weibull estimator
and Duane plot estimator and compares the estimates using
various datasets with the estimators proposed in existing lit-
eratures.

The rest of the paper is organized as follows. Section 2 dis-
cusses non-homogeneous Poisson distribution. In Section 3,
we discuss different estimators of rate of updates.
In Sec-
tion 4, we divide the whole observation space into windows of
consistent behavior. Experimental veriﬁcations are discussed
in Section 5. Then we conclude the paper in Section 6.

2 Non-homogeneous Poisson Process
A non-homogeneous Poisson process over [0, ∞] is deter-
mined by a rate function λ(t) ≥ 0, known as intensity density
of the process (see Chapter 6, Trivedi [Trivedi, 1982]). Let
N (T ) be the number of updates during the time period (0, T ].
Then, the intensity function of the process, i.e. mean number
(cid:2) ∞
of updates during the period (0, T ] is deﬁned as follows.
0 λ(t)dt = ∞

μ(T ) = E(N (T )) =

0 λ(t)dt,

(cid:2) T

for

Now, the probability that the number of updates during the
period (0, T ] equals to k, is given by

P r[N (T ) = k] =

[μ(T )]k

k!

−μ(T )

e

for k = 0, 1, 2, ... The Time homogeneous Poisson pro-
cess is a special case, where λ(t) = λ for all time instances
t ≥ 0. Now, the probability of N (T ) becoming zero is equal
to P r[N (T ) = 0] = e−μ(T )
. Thus we can write its cumula-
tive distribution function (cdf) as F (T ) = 1 − e−μ(T )
and its
probability density function (pdf) as f (T ) = μ(cid:3)(T )e−μ(T ) =

0

λ(t)e
above, the instantaneous λ(t) can be expressed as

. Using the expression of F (t) and f (t)

(cid:2) T

−

λ(t)dt

λ(t) =

f (t)

1 − F (t)

(1)

3 Estimating λ(t)
As stated in Section 1, the whole observation space is divided
into a sequence of windows of consistent behavior. A win-
dow is a sequence of update points satisfying the property that
t(i) > t(i − 1), where t(i) is the time of the ith
update. Since
the sequence of update points are independent and identically
distributed and satisﬁes the memoryless property, each win-
dow can be processed independently to determine the λ(t).
Deﬁnition 1 A window is said to be consistent, iff either one
of the followings is true.
1. λ(t(i)) < λ(t(j)), ∀i < j, j > 0 i.e., increased
2. λ(t(i)) > λ(t(j)), ∀i < j, j > 0 i.e., decreased
3. λ(t(i)) = λ(t(j)), ∀i < j, j > 0 i.e., constant

IJCAI-07

2875

Intuitive Measure: N (t)/t

3.1
If N (t) is the number of updates during the period (0, t],
the rate of updates at the time t is λ(t) = N (t)/t and in-
stantaneous mean time between updates (MTBU) at time t,
M T BU (t) = t/N (t). This estimation is suitable when the
window satisﬁes the third consistency property i.e., the rate of
update is constant. It is the biased estimation of λ(t) in a ho-
mogeneous Poisson process [Cho and Garcia-Molina, 2003].

3.2 Estimation using Weibull Distribution
If the rate of update is either increasing or decreasing or con-
stant, the Weibull distribution is a good mechanism to model
the process. The two parameter Weibull pdf is given by

(cid:3)

(cid:4)β−1

f (t) =

β
η

t
η

−(t/η)β

e

,

(2)

where β is known as shape parameter and η is known as scale
parameter and β, η > 0 (see Chapter 2, Patrick [O’Connor,
2002], [Lieblein, 1955]). Then its cdf is given by F (t) =
1 − e−(t/η)β
. Substituting equation 2 and F (t) in equation 1,
we get

(cid:3)

(cid:4)β−1

λ(t) =

β
η

t
η

(3)

From the study [Lieblein, 1955], we have (1) if β = 1, then
λ(t) = 1/η, a constant (the process reduces to the homo-
geneous Poisson process), (2) if β < 1, then λ(t) decreases
as t increases and (3) if β > 1, then λ(t) increases as t in-
creases. A window is always in one of the above three states.
Therefore, Weibull’s distribution is obviously a good choice
of estimating λ(t).

In [Tsokos, 1995], the author found the relationship be-

tween M T BU (t) and λ(t) as follows.

⎧⎪⎨
⎪⎩

= 1
λ(ti)
> 1
λ(ti)
< 1
λ(ti)

for β = 1
for β < 1
for β > 1

M T BU (ti) =

Thus, 1/λ(t) deﬁnes the bounds of M T BU (t). Now our
task is to estimate the parameters β and η. We estimate these
two parameters using maximum likelihood estimator as given
below. The conditional pdf of the ith
(cid:9)
(cid:10)β−1
event given that the
(i − 1)th
event occurred at ti−1 is given by f (ti | ti−1) =
β
η
function can be deﬁned as

, ti−1 < ti. Now, the likelihood

−tβ

ti
η

i−1

e

)

i

− 1

ηβ (tβ
n(cid:11)

n(cid:11)

(cid:3)

(cid:4)nβ

1
η

L(β, η) =

i=1

f (ti | ti−1) =

Taking log on both sides we get

log L(β, η) = n log(1/η

β

) + n log β − (

Assuming c = 1/ηβ

, we can write

i=1

β
)

+ (β − 1)

tn
η

n(cid:12)

i=1

ti

β
n + (β − 1)
log L(β, c) = n log(c) + n log β − ct

n(cid:12)

i=1

ti

IJCAI-07

2876

Taking partial derivative w.r.t. c and equating to zero, we get

ˆη =

tn
n1/β .

(4)

Again, taking partial derivative w.r.t. β and equating to zero,
we get

(cid:13)n

n

n(cid:13)n

=

ˆβ =

n ln tn −

i=1 ln ti

i=1 ln

tn
ti

(5)

Lemma 1 ˆβ is biased for small value of n and equal to

n
n−2 β

2nβ

Proof As reported in [Calabria et al., 1988], 2nβ/ ˆβ ∼ Z,
where Z is a chi-square random variable with 2(n − 1)
2nβ
degree of freedom. Then we can write ˆβ =
Z =
Γ(n−1)2n−1 tn−2e−t/2
the expectation of ˆβ can
2nβ
tn−3e−t/2dt =
be derived as E[ ˆβ] =
(n−2)Γ(n−2)2n−1 Γ(n − 2)2n−2 = n
n−2 β. If n is small, then
ˆβ is biased. 2

(cid:2) ∞

. Now,

Γ(n−1)2n−1

2nβ

0

Now, Lemma 1 suggests correcting the ML by a factor of

(n − 2)/n and thus obtain
n − 2

˜β =

n

ˆβ =

n − 2(cid:13)n

i=1 ln

.

tn
ti

ˆβ] = n−2

n E[ ˆβ] = β. Thus we get ˜η =

We can easily prove that E[ ˜β] is unbiased i.e., E[ ˜β] =
tn
E[ n−2
n1/ ˜β .
n
Substituting the value of ˜β and ˜η in equation 3, we deter-
˜β
mine the value of ˜λ(t) = n ˜βt
n. In [Calabria et al.,
1988], it is proved that ˜λ(t) is less biased i.e., E[˜λ(t)] =
λ(t)(n − 2)/(n − 3). Again, we correct ˜λ(t) by a factor
(n − 3)/(n − 2) to get an unbiased estimation and thus obtain

˜β−1/t

ˇλ(t) =

n − 3
n − 2

˜λ(t) =

(n − 3)n

n − 2

˜β−1

˜βt

˜β
n.

/t

˜λ(t)] = n−3

It can easily be proved that E[ˇλ(t)] is unbiased i.e., E[ˇλ] =
E[ n−3
n−2 E[˜λ(t)] = λ(t). We use this estimator in
n−2
our experiment. Another unbiased estimate of λ(t) is ¯λ(t) =
ˆβ
(n − 3) ˆβt
n, which is obtained by correcting ˆλ(t) by
a factor of (n − 3)/2 [Calabria et al., 1988]. Once we get
windows of consistent behavior (see Section 4), we can thus
estimate λ(t).

ˆβ−1/t

In this Subsection, we discuss another λ(t) estimator using
Duane plot [Duane, 1964].

ti
Property 1 Let M T BU (ti) =
i , i > 0 be the cumulative
mean time between updates at ith
update. If the sequence of
M T BU (ti) is either increasing or decreasing or constant as
i increases and if we plot a graph between M T BU (ti) along
y-axis and ti along x-axis on log-log graph paper, the points
tend to line up following a straight line. This plot is known as
Duane Plot.

−( tn

η )β

n

e

β

β−1
i

t

3.3 Estimation using Duane Plots

The slope of the straight line passing through points is known
as Duane plot slope. From the straight line, we can write
log M T BU (t) = log α + γ log t, where γ is the Duane plot
slope and α is the intercept when t = 1. Equating M T BU
to its expected value, we get M T BU (t) = αtγ
. Similarly,
cumulative λ(t) can be written as λ(t) = 1
α t−γ
. Now, the
slope γ can be calculated as

γ =

log M T BU (ti) − log M T BU (tj)

log ti − log tj

.

(6)

As we consider the λ(t) estimation only within the consis-
tent windows, the Duane plot can be applied to estimate
the λ(t). This estimation works ﬁne as long as both the
points (M T BU (ti), ti) and (M T BU (tj), tj) are on the best
straight line through the points. But, it is not the case in re-
ality. Therefore, the straight line which is closest to all the
observed points is considered and the γ is the slope of the ob-
tained straight line. The simple linear regression is applied to
ﬁnd the best straight line closest to all the observed points.

3.4 Estimation using Linear Regression

If we consider a consistent window with either constant or
increasing or decreasing λ(t), linear regression is an effec-
tive mechanism to estimate λ(t). Under such conditions, the
relationship between λ(t) and t can be deﬁned by a straight
line. Linear regression deﬁnes a straight line closest to the
data points. We can deﬁne the equation of the closest straight
line as y(cid:3) = a + bx, where a and b are regression coefﬁcients
and deﬁned as a = y − bx and b =

(cid:13)
(cid:13)

(xi−x)(yi−y)

. We ap-

(xi−x)2

ply simple linear regression to get the best straight line closest
to all the observed points in the Duane plot. If we consider
x = log ti and y = log M T BUc(ti), then b is equal to the
slope of the Duane plots γ and a is equal to the intercept α.

(cid:13)N (t)

3.5 Measuring error
An error at a point i is deﬁned as the difference between
the observed value and predicted value at that point i i.e.,
(cid:6)(i) = fobserved(i) − fpredicted(i). The error in measuring
λ(t) is minimum if error in measuring M T BU (t) is min-
(cid:14)
imum and vice versa. Again, overall error in M T BU (t)
(cid:15)
(cid:13)N (t)
M T BU (t(i)) −
is minimum when
i=1 (cid:6)(i) can be
(t(i) − t(i − 1))
used as a measure to compare different estimations of λ(t).
i=1 (cid:6)(i) is considered as the
The λ(t) with the minimum
(cid:13)n
best estimate. But, in the case of linear regression mechanism
i (cid:6)(i) = 0. To avoid this problem, we can use mean of
i=1 (cid:6)(i)2/n as a measure to com-

is minimum. Thus,

(cid:13)N (t)

(cid:13)N (t)

i=1 (cid:6)(i) =

(cid:13)

i=1

square error M SE =
pare different estimators.

4 Generating Consistent Window

In reality, most of the Web page update period sequence is
inconsistent.
It is found to be a sequence of constant, in-
creasing and decreasing over subsequent period of times in
any order. In this Section, a procedure to generate the consis-
tent windows (as deﬁne in Deﬁnition 1) out of a sequence
of update points is discussed. First the localized λ(t) is

DataSet
Real
Synthetic

min max mean
4
4

57
201

27
103

Std. De.

11.46
45.85

#Pages
26972
299324

Table 1: Characteristics of the datasets. (min:Minimum num-
ber of updates in a page, max: maximum number of updates
in a page, mean: mean number of updates and Std. De.: Stan-
dard Deviation)

(cid:2)
If W be the set of update points, then the aver-
deﬁned.
age number of update points within the window W can be
W λ(x)dx < ∞, where λ(x) is lo-
deﬁned as μ(W ) =
cally integrable. Now the localized λ(x) can be deﬁned as
, where N (W ) denotes the
λ(x) = limW →x
number of updates within W and x is an update point. Thus,
it is necessary for the window W to be consistent so as to be
able to predict P r(N (W ) = 1).

P r(N (W )=1)

|W |

Now, the total observation space is divided into windows
Wi of consistent M T BU i.e. either constant or increasing
the length of a window | Wi(k) |
or decreasing in nature.
is deﬁned by (tj+k − tj), j > 0, where Wi(k) starts just
after jth
update and k is the number of update points within
the window. The accuracy of the estimation depends on the
nature of the update points within the window. So, how do we
decide the window size? The important factor in deciding the
window size i.e., k is the number of consistent points within
the window, not | Wi(k) |.

Consistent window: The initial window consists of the last
(ﬁrst) three update points and extended backward (forward).
A window is always in one of the three states, either con-
stant or increasing or decreasing state. The window extends
if the next point at tj is consistent with the state of the current
window. We often do not ﬁnd consistent windows of large
size. To increase the size of the windows, we allow few in-
consistent update points within the window with a limit on
the amount of discrepancies. Even if an update point tj is not
exactly consistent with the current state, we consider the tj
as a consistent point if the amount discrepancy is below some
threshold value, otherwise we create the next window.

5 Experimental Evaluation
The comparisons of different estimators are based on both
synthetic datasets (collected from the UCLA WebArchive
project data available at http://webarchive.cs.ucla.edu/) and
real Web datasets. Synthetic datasets consist of 3,00,000
pages’ records and each record contains 200 access units. The
real Web datasets were collected locally. The 27034 number
of frequently access Web pages were identiﬁed from our lo-
cal proxy server log ﬁle. These pages were crawled every day
for two months (12th
June, 2006) to keep
track of the changes.

April, 2006 to 12th

Table1 shows the characteristics of the two datasets. The
pages having less than four update points during the ob-
servation period are ignored. The proposed estimators are
compared with the estimators proposed in [Cho and Garcia-
Molina, 2003] and [Matloff, 2005]. The λ proposed by Cho

IJCAI-07

2877

DataSet

Synthetic

Real

Estimator
λcho
λnorman
Weibull
Duane
λcho
λnorman
Weibull
Duane

Total
3.849
3.925
2.363
2.956
2.379
2.471
1.363
1.802

Const
0.639
0.745
0.2488
0.339
0.642
0.753
0.245
0.348

Incr

13.094
13.174
7.955
10.307

5.9

5.979
4.069
4.633

Decr
5.19
5.248
3.274
4.059
4.23
4.296
2.513
3.359

Table 2: Average MSE for different estimators over different
window types. Windows were generated with a Threshold
value of 3.

τ log( n−X+0.5

et al. can be stated as λcho = − 1
n+0.5 ), where n is
the number of intervals of τ and X is the number of intervals
with at least one update point. Again, the λ proposed by Mat-
loff N. can be stated as λnorman = − 1
n ), where
n is the number of intervals of τ and Mn is the number of
intervals with at least one unobserved update point. To get
the best result out of the available information, τ is set to 1.

τ log(1 − Mn

The experimental evaluations consists of two parts. In the
ﬁrst part, the localized λ(t) is estimated using the estimators
discussed in Section 3 from the sequence of data points in
the observation space. The sequence of consistent windows
are generated using the procedure discussed in Section 4.
The consistent windows with a minimum size of four update
points are considered. If the size of the window is less than
four, then one half is merged with the left window and an-
other half with the right window in the sequence introducing
some discrepancies. For each window, the λ(t) is estimated at
every update point t using different estimators. For simplic-
ity, M T BU (t) is approximated as M T BU (t) ∼ 1/λ(t) and
then regenerate the whole observation space again using the
estimated M T BU (t). The difference between actual update
point and estimated update point is considered as an error.
The errors in every estimated update points are recorded and
the average MSE for each window is determined to compare
the accuracy of different estimators.

Syn

Real

cho
nor
Wei
Dua
cho
nor
Wei
Dua

Rest

0.002%

0%

cho

0%

nor

Wei

Dua
100% 1.2% 2.3%
1% 1.4%

91.5% 98.8% 99%
8.5%

97.6% 98.5% 8.5%

91.5%

0.002%

0%

0%

90.6% 98.8% 99%
9.4%

98.4% 99.1% 9.4%

100% 1.2% 1.6%

1% 0.92%
90.6%

Table 3: Estimator in the row outperforms estimator in the
column. Windows were generated with a Threshold value
of 3. (Syn :Synthetic, Wei:Weibull, Dua: Duane, cho:λcho,
nor:λnorman)

Table 2 compares the average MSE of different estimators
in terms of different window classes. The proposed Weibull

 

E
S
M
e
g
a
r
e
v
A

 

E
S
M
e
g
a
r
e
v
A

 9

 8
 7

 6

 5
 4

 3

 2
 1

 0

 5

 4

 3

 2

 1

 0

Average MSE Vs Window Size

Weibull
Duane
cho
norman

 5

 10

 15

 20

 25

 30

Window Size

(a) Synthetic Data

Average MSE Vs Window Size

Weibull
Duane
cho
norman

 5

 10

 15

 20

 25

 30

Window Size

(b) Real Data

Figure 1: Window size versus MSE between different esti-
mators. Windows were generated with a Threshold value of
3.

estimator outperforms other estimators for all the cases. For
the case of constant state windows, all these four estima-
tors performs equally well with very small average MSE.
But, Weibull estimator outperforms the rest with signiﬁcantly
smaller average MSE for both increasing and decreasing state
windows. In Table 3, the percentage of the number of win-
dows out of the whole windows, in which row estimator
outperforms the column estimator, is shown. It shows that
Weibull estimator outperforms the rest in 91.5% of the whole
windows for synthetic datasets and in 90.6% of the whole
windows for real Web datasets. Whereas the previous esti-
mators λcho and λnorman outperforms the rest almost in 0%
for both synthetic and real datasets. Again we compare dif-
ferent estimators in terms of the average MSE for different
window sizes as shown in Figure 1.
It clearly shows that
error decreases as we increase the size of window. For the
larger size windows, all estimators perform equally well (still
Weibull outperforms by smaller extents), but for the smaller
size windows, Weibull outperforms the other estimators by
almost double.

The main motivation behind investigating localized λ(t),
rather than global λ is to use the piecewise information to im-
prove the prediction of future update points. For example, the

IJCAI-07

2878

trend in λ(t) during the month of January for last ﬁve years
can be used (or combined with recent trend) to predict future
λ(t) of next January. Due to not having enough datasets to
make use of such information, such study is left as the future
work.

However in the second part of the experiment, the effec-
tiveness of different estimators is investigated by predicting
the future update points from the past information running
a simulation program. Simulation starts with ﬁrst consistent
window in the observation space and based on this window,
the next update point is predicted. Then the window is ex-
tended to include the newly predicted point and again pre-
dict the next update point and so on. As pointed out in Sec-
tion 1, an efﬁcient estimator should be able to detect good
amount of update points with minimum amount of resource
consumption. Each premature visit is a wastage of resources
such as CPU time, ﬁle system, network bandwidth etc. (see
Section 1). To compare different estimators , a measure of
precision is deﬁned and stated as follows.

precision =

#Observed U pdate points

#T otal V isits

The precision of an estimator represents the probability of
detecting an update per visit.
It is reasonable to conclude
that estimator with the highest precision is the best estima-
tor, with the highest probability of detecting update points.
There are 744385 number of update points in real dataset and
30931883 in synthetic dataset. Table 4 shows the compar-
isons among the four estimators in terms of the percentage
of updates detected, premature visits, precision and number
of visits. Though the λcho and λnorman estimators detect
the highest number of the update points, it is very expen-
sive, almost 42% (36%) of the total visits over real (synthetic)
dataset are premature visits. Moreover, the number of vis-
its scheduled by both λcho and λnorman estimators are very
large compared to the number of visits scheduled by Weibull
estimator. Whereas Weibull estimator has the highest preci-
sion i.e. the probability of detecting updates per visit, almost
0.70 (0.76) for real (synthetic) datasets and minimum num-
ber of visits. If the number of visits is normalized, Weibull
estimator detects the highest number of update points, next
followed by Duane, then λnorman and λcho. If the number
of visits is normalized to the number of visits of Weibull es-
timator, both the estimotors λcho and λnorman detect only
around 62% (60%) and Duane estimator detects around 68%
(66%) for synthetic (real) datasets. Note that the best policy is
the one that detects the highest number of update points with
minimum resource consumption, in other word highest num-
ber of update points detected from the same number of visits.
Therefore, Weibul estimator outperforms other estimators.

6 Conclusions

This paper models the process of web page updates as non-
homogeneous Poisson process. In a scenario with inconsis-
tent rate of updates, piecewise estimation provides more de-
tail and useful information compared to global estimation.
In the study, the whole observation space is divided into in-
dependent consistent windows to estimate localized rate of

Syn

Real

λcho
λnorman
Weibull
Duane
λcho
λnorman
Weibull
Duane

%Obs %Prem %Prec %Visits
0.64
94.9% 36%
0.637
95.2% 36.3
74.1% 23.8% 0.762
91.2% 29.5% 0.7
95.1% 41.5% 0.58
95.5% 42%
0.58
72.3% 30.9% 0.69
88.6% 36.4% 0.64

154%
153%
100%
129%
155%
157%
100%
133%

Table 4: Comparison of % of observed updates, % of pre-
mature visits, precision, % of visits over Weibull estimator.
(Syn: Synthetic, Obs: Observed update points, Prem: Prema-
ture visits, Prec: Precision). Windows were generated with a
Threshold value of 3

updates. Two update rate estimation mechanisms namely
Weibull and Duane plot are proposed. Tests on both synthetic
and real datasets conﬁrm that Weibull’s estimator outper-
forms Duane plot and both the existing estimators λcho and
λnorman while estimating localized rate of updates. While
investigating the effectiveness of using these estimators to
predict future update points, Weibull estimator has highest
probability of detecting update points.

References
[Brewington and Cybenko, 2000] Brian E. Brewington and
George Cybenko. How dynamic is the web? In Proceed-
ing of WWW2000, March 2000.

[Calabria et al., 1988] R. Calabria, M. Guida, and G. Pul-
cini. Some modiﬁed maximum likelihood estimator for
the weibull process. Reliability Engineering and System
Safety, 23, 1988.

[Cho and Garcia-Molina, 2003] Junghoo Cho and Hector
Garcia-Molina. Estimating frequency of change. ACM
Transaction on Internet Technology, 3:256–290, August
2003.

[Duane, 1964] J. T. Duane. Learning curve approach to re-
liability monitoring. IEEE Transactions on Aerospace, 2,
April 1964.

[Lieblein, 1955] Julius Lieblein. On moments of order statis-
tics from the weibull distribution. Annals of Mathematical
Statistics, 26, June 1955.

[Matloff, 2005] Norman Matloff. Estimation of internet ﬁle-
access/modiﬁcation rates from indirect data. ACM Trans-
actions on Modeling and Computer Simulation, 15:233–
253, July 2005.

[O’Connor, 2002] Patrick D. T. O’Connor. Practical Relia-

bility Engineering. John Wiley, forth edition, 2002.

[Trivedi, 1982] Kishor S. Trivedi. Probability and Statistics
with Reliability, Queuing and Computer Science Applica-
tion. Prentice Hall, 1982.

[Tsokos, 1995] Chris P. Tsokos. Reliability growth: nonho-

mogeneous poisson process. CRC Press, 1995.

IJCAI-07

2879


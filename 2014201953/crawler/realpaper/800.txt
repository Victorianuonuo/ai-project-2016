Abstract Interpretation of Programs for Model-Based Debugging

Wolfgang Mayer

Markus Stumptner

Advanced Computing Research Centre

University of South Australia
[mayer,mst]@cs.unisa.edu.au

Abstract

Language
Semantics

Test-Cases

Developing model-based automatic debugging
strategies has been an active research area for sev-
eral years. We present a model-based debug-
ging approach that is based on Abstract Interpre-
tation, a technique borrowed from program analy-
sis. The Abstract Interpretation mechanism is inte-
grated with a classical model-based reasoning en-
gine. We test the approach on sample programs
and provide the ﬁrst experimental comparison with
earlier models used for debugging. The results
show that the Abstract Interpretation based model
provides more precise explanations than previous
models or standard non-model based approaches.

Program

Conformance Test

Components

Conﬂict sets

Fault

Assumptions

MBSD Engine

Diagnoses

Figure 1: MBSD cycle

1 Introduction

Developing tools to support the software engineer in locating
bugs in programs has been an active research area during the
last decades, as increasingly complex programs require more
and more effort to understand and maintain them. Several dif-
ferent approaches have been developed, using syntactic and
semantic properties of programs and languages.

This paper extends past research on model-based diagnosis
of mainstream object oriented languages, with Java as con-
crete example [14]. We show how abstract program anal-
ysis techniques can be used to improve accuracy of results
to a level well beyond the capabilities of past modelling ap-
proaches.
In particular, we discuss adaptive model reﬁne-
ment. Our model reﬁnement process targets loops in partic-
ular, as those have been identiﬁed as the main culprits for
imprecise diagnostic results. We exploit the information ob-
tained from abstract program analysis to generate a reﬁned
model, which allows for more detailed reasoning and conﬂict
detection.

Section 2 provides an introduction to model-based diag-
nosis and its application to automatic debugging. Section 3
outlines the adaption of the program analysis module of our
debugger and discusses differences between our and the clas-
sical analysis frameworks. Section 4 contains results obtained
though a series of experiments and compares the outcome
with other techniques. A discussion of related and future
work concludes the paper.

2 Model-based debugging
Model-based software debugging (MBSD) is an application
of Model-based Diagnosis (MBD) [19] techniques to locat-
ing errors in computer programs. MBSD was ﬁrst intro-
duced by Console et. al. [6], with the goal of identifying
incorrect clauses in logic programs; the approach has since
been extended to different programming languages, includ-
ing VHDL [10] and JAVA [21].

The basic principle of MBD is to compare a model, a de-
scription of the correct behaviour of a system, to the observed
behaviour of the system. Traditional MBD systems receive
the description of the observed behaviour through direct mea-
surements while the model is supplied by the system’s de-
signer. The difference between the behaviour anticipated by
the model and the actual observed behaviour is used to iden-
tify components that, when assumed to deviate from their nor-
mal behaviour, may explain the observed behaviour.

The key idea of adapting MBD for debugging is to ex-
change the roles of the model and the actual system:
the
model reﬂects the behaviour of the (incorrect) program, while
the test cases specify the correct result. Differences between
the values computed by the program and the anticipated re-
sults are used to compute model elements that, when assumed
to behave differently, explain the observed misbehaviour. The
program’s instructions are partitioned into a set of model com-
ponents which form the building blocks of explanations. Each
component corresponds to a fragment of the program’s source
text, and diagnoses can be expressed in terms of the original
program to indicate a potential fault to the programmer (see

IJCAI-07

471

Figure 1).

Each component can operate in normal mode, denoted
¬AB (·), where the component functions as speciﬁed in the
program, or in one or more abnormal modes, denoted AB (·),
with different behaviour. Intuitively, each component mode
corresponds to a particular modiﬁcation of the program.

Example 1 The program in Figure 2 can be partitioned
into ﬁve components, C1, . . . , C5, each representing a sin-
gle statement. For components representing assignments, a
possible AB mode is to leave the variable’s value undeter-
mined. Another possibility is to expand the assignment to a
set of variables, rendering their values undetermined. For
loops, the number of iterations could be altered.

The model components, a formal description of the seman-
tics of the programming language and a set of test cases are
submitted to the conformance testing module to determine
if the program reﬂecting the fault assumptions is consistent
with the test cases. A program is found consistent with a test
case speciﬁcation if the program possibly satisﬁes behaviour
speciﬁed by the test case.

In case the program does not compute the anticipated re-
sult, the MBSD engine computes possible explanations in
terms of mode assignments to components and invokes the
conformance testing module to determine if an explanation is
indeed valid. This process iterates until one (or all) possible
explanations have been found.

Formally, our MBSD framework is based on Reiter’s the-
ory of diagnosis [19], extended to handle multiple fault modes
for a component. MBSD relies on test case speciﬁcations to
determine if a set of fault represents is a valid explanation,
where each test case describes the anticipated result for an
execution using speciﬁc input values.

Deﬁnition 1 (Debugging Problem) A Debugging Problem
is a triple (cid:2)P, T, C(cid:3) where P is the source text of the pro-
gram under consideration, T is a set of test cases, and C
denotes the set of components derived from P .
The set C is a partition of all statements in P and contains
the building blocks for explanations returned by the debugger.
For simplicity of presentation, it is assumed that there is a
separate component for each program statement.

ˆ=
{AB (C1) , . . . , AB (Ck)} is a valid explanation if
the
model modiﬁed such that components Ci may exhibit
deviating behaviour, while the remaining components exhibit
normal behaviour, no longer implies incorrect behaviour.

assumptions Δ

set

of

fault

A

Deﬁnition 2 (Explanation) A fault assumption Δ is a con-
sistent explanation for a Debugging Problem (cid:2)P, T, C(cid:3) iff for
all test cases T ∈ T, it cannot be derived that all executions
of P (altered to reﬂect Δ) violate T :

∀T ˆ= (cid:2)I, O(cid:3) ∈ T [[P ⊕ Δ ⊕ O]](I) (cid:7)= ⊥.

I and O represent the input values and assertions describing
the correct output values speciﬁed in a test speciﬁcation, re-
spectively. [[·]] denotes the semantic function of the program-
ming language and the fault assumptions. ⊕ denotes the ap-
plication of the fault assumptions in Δ and test assertions O
to P . ⊥ denotes an infeasible program state.

I : n (cid:9)→ 4

1

int i = 1
int x = 1

2
3 while (i ≤ n) {
4

x = x + i
i = i + 1

“ correct is x = x ∗ i ”

5

6

}

O : assert (x == 24)

Figure 2: Example program and test speciﬁcation

Example 2 Assume component C1 representing line 1 of the
program in Figure 2 is abnormal. In this case, there is no ex-
ecution that terminates in a state satisfying x == 24. There-
fore, a fault in line 1 cannot explain the program’s misbe-
haviour. In contrast, if the value of x is left undetermined
after line 2, an execution exists where the program satisﬁes
the assertion. Therefore, C2 is a potential explanation.

While the deﬁnition of an explanation is intuitive, the pre-
cise test is undecidable and must be approximated. Differ-
ent abstractions have been proposed in the literature, ranging
from purely dependency-based representations [10] to Pred-
icate Abstraction [13]. The following section introduces an
approximation based on Abstract Interpretation [8], a tech-
nique borrowed from program analysis.

3 Model construction

Models developed for VHDL and early models for Java were
based on static analysis and represented the program as a
ﬁxed model representing the program’s structure. As test
cases were not taken into account, it was necessary to repre-
sent all possible executions and fault assumptions. While this
approach has been used successfully to debug VHDL pro-
grams, the dynamic method lookup and exception handling
lead to large models for Java programs.

Mayer and Stumptner [15] propose an approach where
the model is constructed dynamically, taking into account
only executions that are feasible given a test case. The ap-
proach is based on Abstract Interpretation [8] of programs,
generalised to take fault assumptions and test case informa-
tion into account. In the following, the basic principles are
introduced. More detailed presentations are given in [15;
16].

3.1 Abstract interpretation

The model is derived from a graph representing the effects of
individual statements of the program.

Deﬁnition 3 (Program Graph) A program graph for a pro-
gram P is a tuple (cid:2)V, S, Sω, E(cid:3) where V is a ﬁnite set of
vertices, E ⊆ V × V a set of directed edges, and S ∈ V
and Sω ∈ V denote distinct entry and exit vertices.
The vertices of the program graph represent sets of program
states; the edges represent the effects of program instructions,
transforming sets of program states into new sets of states.
The initial vertex  is associated with the initial program state
I described by the test speciﬁcation. An execution of a pro-
gram starts in the initial state in S and proceeds following the

IJCAI-07

472

S : n (cid:2)→ 4

C1 : [[i = 1]]

S1

S2

C2 : [[x = 1]]

C3 : [[i > n]]

∇

C3 : [[i ≤ n]]

S6

C4 : [[x = x + i]]

C5 : [[i = i + 1]]

S3

S4

S5

ˆS = ˆI
ˆS1 = (cid:2)[[i = 1]]( ˆS)
ˆS2 = (cid:2)[[x = 1]]( ˆS3) (cid:14) ˆS5
ˆS3 = ∇( (cid:2)[[i ≤ n]]( ˆS2), ˆS (cid:4)
3)
ˆS4 = (cid:2)[[x = x + i]]( ˆS3)
ˆS5 = (cid:2)[[i = i + 1]]( ˆS4)
ˆS6 = (cid:2)[[i > n]]( ˆS2)
ˆSω = (cid:2)[[x == 24]]( ˆS6)

Figure 3: Program graph and abstract equation system

edges until the ﬁnal vertex ω has been reached. The program
states in Sω represent the result computed by the program.

Example 3 Figure 3 depicts the program graph derived from
the program in Figure 2 (where Δ = ∅). Each program state-
ment is represented as an edge, connecting the program state
before the statement is executed to the successor state. The
loop statement in line 3 induces two possible successors: one
transition representing the case where the loop is entered, and
one transition representing loop termination. Starting in the
initial state where n (cid:9)→ 4, the ﬂow graph induces a trace
that traverses the loop four times and the stops in Sω with
x (cid:9)→ 12.
In case the initial state is not fully speciﬁed or parts of an
intermediate state become undetermined due to fault assump-
tions, a potentially inﬁnite number of paths must be followed
to determine the states in Sω.

To ensure every analysis of the graph is ﬁnite, the concrete
program states and the concrete semantics of the program
are replaced with abstract versions. Formally, the Abstract
Interpretation framework [8] uses complete lattices to repre-
ˆS) such that (cid:2)ˆS, ⊥, (cid:17), (cid:18)
sent concrete (S) and abstract states (
, (cid:14), (cid:19)(cid:3) is a safe abstraction of (cid:2)P (S) , ∅, S, ⊆, ∪, ∩(cid:3). ⊥ de-
notes infeasibility and (cid:17) represents all possible states. (cid:18) rep-
resents the abstract state ordering, and (cid:14) and (cid:19) denote the
least upper bound and greatest lower bound operator, respec-
tively.

The key component in this framework is a pair of functions
(cid:2)α, γ(cid:3) (“Galois connection”), where α maps sets of concrete
program states to their abstract representation and γ imple-
ments the reverse mapping. The semantic functions operating
(cid:3)[[·]] ˆ= α◦[[·]]◦γ.
on the abstract domain can then be deﬁned as
An approximation of the program execution can be obtained
by translating the program graph into a set of equations and
computing the ﬁxpoint solution. In case the abstract lattice is
of inﬁnite height and the program graph is cyclic, widening
operators (∇) must be applied to ensure termination. Widen-
ing operators selectively discard information from the ab-
stract states to guarantee that the computation of mutually
dependent equations eventually converges.

Similar equations systems and ﬁxpoint solutions can be de-
ˆSω an ap-

rived for backward analyses, where for any given

proximation of S is computed such that the program is guar-
anteed to terminate in a state in γ( ˆSω). In our work we apply
a variant of Bourdoncle’s bidirectional method [2] where for-
ward and backward analyses are applied repeatedly to reﬁne
approximations.

Example 4 Using backward analysis it can be derived that if
the program in Figure 2 should terminate in Sω with i (cid:9)→ 24,
then n ≤ 4 must hold in S (assuming that n has been left
undetermined in I).

3.2 Dynamic model creation
To utilise Abstract Interpretation for debugging, Deﬁnition 2
must be lifted to the abstract domain, replacing [[·]] and I
ˆI, respectively. For
with their abstract counterparts
O translation is not necessary, as it is assumed that O is rep-
resented as assertions in the programming language and can

(cid:3)[[·]] and

be evaluated using

(cid:3)[[·]].

In our framework, we apply the well-known non-relational
interval abstraction [8] to approximate concrete program
states. Further, we use the simple abstraction described in [7]
to ensure dynamically allocated data structures are repre-
sented ﬁnitely.

In contrast to purely static program analysis, we do not
assume a ﬁxed program graph modelling all possible execu-
tions. Instead, the graph is constructed dynamically, using in-
puts I and assertions O from the test case speciﬁcations. Only
the paths that may be executed when starting in a state I are
created. Paths that contain ⊥ are ignored. Once a program
state is reached where multiple paths could be followed, we
apply the conventional equation-based analysis to the remain-
ing execution. Our framework can thus be placed in the mid-
dle grounds between purely static analysis [8] and dynamic
analysis [9].

The algorithm for determining consistency for a program
P with fault assumptions Δ and test case T is outlined as
follows:

1. Add the assertions in T to P and apply Δ to P .
2. Construct the initial program graph using the forward

abstract semantics

(cid:3)[[·]] and initial state

ˆI.

3. Apply backward analysis to approximate the initial pro-
gram states not implying assertion violations later in the
execution.

4. Analyse program graph in forward direction using the
new initial program states to eliminate values that cannot
be realised.

5. Repeat from step 3 until a ﬁxpoint is reached.
6. In case the initial program state is is ⊥, no execution
satisfying all test case speciﬁcations can be found and
Δ is not a valid explanation. Otherwise, Δ is consistent
and another test case is considered.

The beneﬁts of the dynamic approach are as follows:

(i) The equation system is concise. This is a signiﬁcant
advantage when dealing with paths representing excep-
tions, as in Java runtime exceptions may be thrown by a
large number of statements. However, typically only a
few of these paths are actually followed in an execution.

IJCAI-07

473

(ii) Ambiguity in control ﬂow is reduced compared to the
purely static approach. This is an important advan-
tage for the precision of the abstraction of dynamic data
structures, which deteriorates dramatically in the pres-
ence of loops in the program graph.

(iii) Simple non-relational abstractions are sufﬁcient to ob-
tain good results if values for most related variables are
known. Expressions can be partially evaluated, leading
to tighter bounds for the remaining variables.

(iv) In case all values potentially accessed by a called
method are known in a program state and the method
does not include AB components, the ﬁxpoint compu-
tation can be replaced with native execution.

The ﬁxpoint analysis is limited to the regions of the execu-
tion where the values of abstract states critical to the region’s
execution are undetermined. As many fault assumptions af-
fect only a minor part of a program state, execution can pro-
ceed normally in unaffected regions. Consequently, many al-
ternative branches that must be considered by purely static
Abstract Interpretation can be eliminated.

the MBSD examines Δ =
Example 5 Assume that
{AB (C4)} for our running example.
In this case, execu-
tion proceeds normally until line 4 is reached. The execution
of AB (C4) leads to a state where i (cid:9)→ 1 and x is unde-
termined. As i is known, the execution of statement 5 can
proceed normally and no ambiguity in the control ﬂow arises
at line 3. In fact, the entire trace reduces to a linear chain.
Backward analysis derives that the value of x must hold af-
ter statement 4 was last executed. The remaining occurrences
of x have undetermined values. Since the initial state in the
graph is not ⊥, Δ is consistent and is considered a valid ex-
planation.

Iterative reﬁnement

3.3
The partially determined abstract states facilitate further iter-
ative model reﬁnement. Abstraction of the effects of loops
and recursive method calls can be improved if the known
states before and after a loop or method call contain conﬂict-
ing values. In this case, the execution is either inconsistent,
or the loop (or method) must be expanded further. In case
the loop (call) has been shown to terminate (for example,
through syntax-based methods presented in [16]) or a limit
on the number of iterations (recursive calls) has been set by
the user, the loop (call) can be expanded. As a result, values
lost through widening and heap abstraction may be regained,
potentially leading to a contradiction elsewhere. Similar im-
provements can be achieved through informed restructuring
of the model based on automatically derived or user-speciﬁed
properties of program regions. For space reasons we illus-
trate the reﬁnement approach in the context of our running
example and rely on [16] for more detailed discussion.

Example 6 Assume the MBSD engine examines the candi-
date explanation Δ = {AB (C3)}. As the loop condition
is assumed incorrect, the loop condition cannot be evaluated
uniquely. This implies that the number of loop iterations is
not known and the program state S6 after the loop is approx-
imated using intervals: S6 : x (cid:9)→ [1..∞), i (cid:9)→ [1..∞). The
upper bound of the intervals is due to the widening opera-
tor. The assertion x==24 may be satisﬁed in S6, leading

to Sω : x (cid:9)→ 24, i (cid:9)→ [1..∞). Backward analysis reveals
that x (cid:9)→ 24 must also hold after the loop. This state con-
ﬂicts with the program state before the loop, which contains
x (cid:9)→ 1. Subsequently, the loop is expanded in an attempt
to prove that Δ does not represent the true fault. The pro-
cess repeats and stops after six expansions, leading to a state
S6 : i (cid:9)→ [8..∞], x (cid:9)→ [29..∞]. At this point, it can be proved
that the assertion is violated and Sω : ⊥. A subsequent back-
ward pass derives S : ⊥ and eliminates Δ from the set of
potential explanations.

3.4 Fault assumptions/structural ﬂaws
Similar to model-based hardware diagnosis [1], previous
MBSD approaches have great difﬁculty with faults that mani-
fest as structural differences in the model. The dynamic mod-
elling approach described here is more ﬂexible in that fault
assumptions modifying variables that were originally not af-
fected by a component’s behaviour can now be addressed
appropriately. The modelling of dynamically allocated data
structures beneﬁts in particular, as the scope of fault assump-
tions can now be determined more precisely compared to the
crude modelling in previous MBSD approaches, which led to
a large fraction of undesirable explanations. Together with
more expressive speciﬁcations of the correct behaviour [17],
we believe that extended fault modes can deal effectively with
complex faults.

4 Evaluation
This section presents experimental results obtained from the
model described in the previous sections and compares the
results with other MBSD approaches, as well as results ob-
tained by Slicing [20]. Readers interested in a theoretical
comparison of the different models are referred to [18].

4.1 Slicing
The classic principle of Program Slicing [20] is to eliminate
all statements from a program that cannot inﬂuence the value
of a distinguished variable at a given location in the program.
The pair (cid:2)V ariable, Location(cid:3) denotes the slicing criterion.
The working hypothesis is that typically, only a small fraction
of a program contributes to a variable’s value; the rest can be
pruned away to reduce debugging effort.

Static backward slicing starts at the variable and location
mentioned in the slicing criterion and recursively collects all
statements that may inﬂuence the variable’s value in any pro-
gram execution. Further, all statements that may cause the
location to be skipped are also included. This process repeats
until a ﬁxpoint is reached. While static slices can be com-
puted quickly even for large programs, the remaining pro-
gram often remains large.

Dynamic slicing aims at reducing the number of statements
by considering only dependencies that arise between state-
ments executed for a particular test case execution. Similar to
static slicing, dependencies for a variable are followed and all
statements contributing to the variable’s value are collected.
While dynamic slices are typically much smaller than their
static counterparts, for programs with many control depen-
dencies or long data-ﬂow chains, results remain similar.

Example 7 The static slice of the program in Figure 2 w.r.t.
the slicing criterion (cid:2)i, 6(cid:3) includes statements 1, 3 and 5. The

IJCAI-07

474

remaining statements are ignored as they are not relevant for
the computation of i. For this test case dynamic slicing com-
putes the same result. The difference between the two ap-
proaches can be seen in case the loop is executed only once:
The static slice for (cid:2)x, 6(cid:3) contains all statements, while the
dynamic slice does not include statement 5.

4.2 Dependency-based MBSD

A number of dependency-based MBSD approaches have been
introduced in [21]. Wotawa [22] has shown that these ap-
proaches provide results equivalent to static or dynamic slic-
ing in case only a single abnormal variable is observed. In the
following, we limit our attention to results obtained through
slicing without explicitly stating that the same results hold for
dependency-based MBSD.

4.3 Value-based MBSD
Value-based Models (VBM) [14] extend beyond simple de-
pendency tracking and model the effects of program state-
ments explicitly. In contrast to the Abstract Interpretation-
based approach discussed here, the VBM does not abstract
from the concrete semantics and relies on statically con-
structed models. While the model is effective for programs
where dependency-based representations do not provide ac-
curate results, the poor abstraction causes the model to col-
lapse when loop iterations or recursive method calls cannot
be determined precisely.

4.4 Experimental results

A set of test programs have been used to evaluate the perfor-
mance of different debugging approaches. Some programs
were taken from the Siemens Test Suite 1 (transcribed to Java),
a debugging test bench commonly used in the debugging
community; others have been retained from earlier tests of
the VBM and dependency-based models.

A summary of he results obtained is given in Figure 4.
LoC denotes the number of non-comment lines in the pro-
gram’s source code, Comp represents the number of diagno-
sis components used to construct explanations, SSlice, DSlice
and Exec denote the number of statements in the static slice,
dynamic slice and the number of executed statements, respec-
tively. VBM and AIM denote the number of statements re-
turned as potential explanations for the VBM and the model
described in this paper, and Time is the average diagnosis time
in seconds (wall-clock time) required for the AIM. The results
of the AIM and the VBM are limited to single fault explana-
tions. Due to limitations of the implementation of the VBM,
some diagnoses listed for the AIM may not be included in the
VBM’s results. Initial experiments with multiple faults indi-
cated that the number of explanations does not differ signiﬁ-
cantly from static slicing and does not warrant the additional
overhead necessary for the AIM and VBM.

For each program, n variants with different faults were cre-
ated and tested with up to eight test cases per variant. The
test results reported are the averages for each program over
all variants and test cases. Slight improvements could be ob-
served when using two rather than a single test case; no sig-

1http://www-static.cc.gatech.edu/aristotle/

Tools/subjects/

niﬁcant differences could be detected in any of the test pro-
grams (with the exception of Adder) when using more than
two test cases. This can be attributed to the structure of the
selected programs, where a large fraction of statements and
test cases are executed for all test cases.

It can be seen that static slicing and dynamic slicing in
many cases cannot improve much compared to the entire pro-
gram and the statements executed in test cases. Similarly, dy-
namic slices often improve little compared to the executed
statements, as all statements contribute to the ﬁnal result.
Comparing VBM and AIM, it can be seen that the AIM im-
proves over the VBM in most cases. In fact, the case where
the VBM provides fewer explanations is due to explanations
missed by the VBM. Comparing AIM and slicing: the AIM
provides signiﬁcantly fewer explanations, but is computation-
ally more demanding. (Slicing approaches typically compute
solutions in a few milliseconds). Note that VBM and AIM
are not currently optimised for speed and rely on a Java inter-
preter written in VisualWorks Smalltalk.

5 Related Work
Delta Debugging [5] aims at isolating a root cause of a pro-
gram failure by minimising differences between a run that
exhibits a fault and a similar one that does not. Differences
between program states at the same point in both executions
are systematically explored and minimised, resulting in a sin-
gle “root cause” explaining why the program fails.

Model checking has recently been applied to locate
faults [11; 3] by comparing abstract execution traces leading
to correct and erroneous program states. Likely causes for
a misbehaviour can be identiﬁed by focussing on traces that
deviate only slightly from passing and failing test cases.

Error traces have also been applied to synthesise poten-
tial corrections of faulty programs, given a speciﬁcation of
the program’s correct behaviour [12]. Symbolic evaluation is
used to compare symbolic representations of program states
as computed by the program versus states necessary to satisfy
the post condition of the program. Differences in the pred-
icates allow to heuristically synthesise replacement expres-
sions correcting single faults in the program. The approach
is able to provide corrections automatically only if a formal
speciﬁcation is given, which is not required for MBSD.

6 Conclusion
We introduced the basic principle of model-based software
debugging and illustrated a model centred around abstract
simulation of programs. Differences to previous approaches
were outlined and results obtained using different debugging
strategies were compared. Notable improvements over other
approaches have been achieved, in particular for “difﬁcult”
programs where traditional debugging techniques do not per-
form well.

The Abstract Interpretation based model has been shown to
achieve considerable improvement compared to slicing and
previous model-based debugging approaches, with a reduc-
tion of the average number of explanations from 81% for
static slicing to roughly 26%. Conversely, the model is com-
putationally more demanding and will require optimisation to
be applicable to mid-sized or large programs in an interactive
setting.

IJCAI-07

475

Name
Adder
BinSearch
Binomial
BubbleSort
Hamming
Permutation
Polynom
SumPowers
TCAS
Total
%

n
5
6
7
5
6
5
9
7
8
6.4

LoC Comp
49
29
80
29
48
54
103
27
78
55.5

31
24.9
45
11
38
25
66.9
16
42
33.7

SSlice DSlice Exec VBM AIM Time (s)
24.3
24.9
32.4
11
38
25
39.8
14.8
42
27.5
81.6

29.2
20.5
32
10
30.9
24.4
37.5
12.3
35.5
25.8
76.6

9.8
6
(9)
6
11
10.3
14
10
n/a
9.5
28.2

22
20.5
27.4
10
30.9
24.4
28
11.4
35
22.7
67.4

7.6
3.6
9.8
4
5.2
8
12
7.4
20
8.9
26.4

10.9
12.2
72.6

8

218.3

79

584.4

3.8
34.5
115.5

Figure 4: Debugging results

Current ongoing and future work includes (i) broadening
fault modes to deal with a wider range of complex faults, such
as assignments to incorrect variables, missing or swapped
statements, etc., and (ii) providing simple user interaction for
incremental speciﬁcation of complex program behaviour. Re-
cent advances in automatic abstraction and program veriﬁca-
tion [4] could lead the way to an MBSD engine with a fast
but powerful adaptive abstract conformance checker.

References
[1] Claudia B¨ottcher. No faults in structure? How to diag-

nose hidden interaction. In Proc. IJCAI, 1995.

[2] Franc¸ois Bourdoncle. Abstract debugging of higher-
order imperative languages. In Proc. SIGPLAN Conf.
PLDI, pages 46–55, 1993.

[3] Sagar Chaki, Alex Groce, and Ofer Strichman. Explain-
ing abstract counterexamples. In Richard N. Taylor and
Matthew B. Dwyer, editors, SIGSOFT FSE, pages 73–
82. ACM, 2004.

[4] Edmund Clarke, Daniel Kroening, Natasha Sharygina,
and Karen Yorav. Predicate abstraction of ANSI-C pro-
grams using SAT. Formal Methods in System Design,
25(2-3):105–127, 2004.

[5] Holger Cleve and Andreas Zeller. Locating causes of
program failures. In Gruia-Catalin Roman, William G.
Griswold, and Bashar Nuseibeh, editors, ICSE, pages
342–351. ACM, 2005.

[6] Luca Console, Gerhard Friedrich, and Daniele Thesei-
der Dupr´e. Model-based diagnosis meets error diagno-
sis in logic programs. In Proc. 13th IJCAI, pages 1494–
1499, 1993.

[7]

James Corbett, Matthew Dwyer, John Hatcliff, Co-
rina Pasareanu, Robby, Shawn Laubach, and Hongjun
Zheng. Bandera: Extracting ﬁnite-state models from
Java source code. In Proc.ICSE-00, 2000.

[8] Patrick Cousot and Radhia Cousot. Abstract interpreta-
tion: A uniﬁed lattice model for static analysis of pro-
grams by construction of approximation of ﬁxpoints. In
POPL’77, pages 238–252, 1977.

[10] Gerhard Friedrich, Markus Stumptner, and Franz
Wotawa. Model-based diagnosis of hardware designs.
In Wolfgang Wahlster, editor, ECAI, pages 491–495.
John Wiley and Sons, Chichester, 1996.

[11] Alex Groce and Willem Visser. What went wrong:
In Thomas Ball and
Explaining counterexamples.
Sriram K. Rajamani, editors, SPIN, volume 2648 of
Lecture Notes in Computer Science, pages 121–135.
Springer-Verlag, 2003.

[12] Haifeng He and Neelam Gupta. Automated debugging
using path-based weakest preconditions. In Michel Wer-
melinger and Tiziana Margaria, editors, FASE, volume
2984 of Lecture Notes in Computer Science, pages 267–
280. Springer-Verlag, 2004.

[13] Daniel K¨ob, Rong Chen, and Franz Wotawa. Abstract
model reﬁnement for model-based program debugging.
In Proc. DX’05, pages 7–12, 2005.

[14] Cristinel Mateis, Markus Stumptner, and Franz Wotawa.
A Value-Based Diagnosis Model for Java Programs. In
Proc. DX’00 Workshop, 2000.

[15] Wolfgang Mayer and Markus Stumptner. Model-based
In Proc.

debugging using multiple abstract models.
AADEBUG ’03 Workshop, pages 55–70, 2003.

[16] Wolfgang Mayer and Markus Stumptner. Debugging
In Proc.

program loops using approximate modeling.
ECAI, 2004.

[17] Wolfgang Mayer and Markus Stumptner. High level ob-

servations in java debugging. In Proc. ECAI, 2004.

[18] Wolfgang Mayer and Markus Stumptner. Model-based
debugging – state of the art and future challenges. In
Workshop on Veriﬁcation and Debugging, 2006.

[19] Raymond Reiter. A theory of diagnosis from ﬁrst prin-

ciples. Artiﬁcial Intelligence, 32(1):57–95, 1987.

[20] Frank Tip. A Survey of Program Slicing Techniques.
Journal of Programming Languages, 3(3):121–189,
1995.

[21] Dominik Wieland. Model-Based Debugging of Java
Programs Using Dependencies. PhD thesis, Technische
Universit¨at Wien, 2001.

[9] Michael D. Ernst, Adam Czeisler, William G. Griswold,
and David Notkin. Quickly detecting relevant program
invariants. In Proc.ICSE-00, pages 449–458, 2000.

[22] Franz Wotawa. On the Relationship between Model-
Based Debugging and Program Slicing. Artiﬁcial Intel-
ligence, 135(1–2):124–143, 2002.

IJCAI-07

476


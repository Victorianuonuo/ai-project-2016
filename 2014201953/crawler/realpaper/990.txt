SegGen: a Genetic Algorithm for Linear Text Segmentation

S.Lamprier, T.Amghar, B.Levrat and F.Saubion

LERIA, Universit´e d’Angers

2, Bd Lavoisier 49045 Angers (France)

{lamprier,amghar,levrat,saubion}@info.univ-angers.fr

Abstract

This paper describes SegGen, a new algorithm for
linear text segmentation on general corpuses.
It
aims to segment texts into thematic homogeneous
parts. Several existing methods have been used
for this purpose, based on a sequential creation of
boundaries. Here, we propose to consider bound-
aries simultaneously thanks to a genetic algorithm.
SegGen uses two criteria: maximization of the in-
ternal cohesion of the formed segments and min-
imization of the similarity of the adjacent seg-
ments. First experimental results are promising and
SegGen appears to be very competitive compared
with existing methods.

1 Introduction

The purpose of automatic text segmentation is to identify the
most important thematic breaks in a document in order to
cut it into homogeneous units, disconnected from other ad-
jacent parts [Salton et al., 1996]. More precisely, segmen-
tation partitions a text by determining boundaries between
contiguous segments related to different topics, deﬁning so
semantically coherent parts of text that are sufﬁciently large
to expose some aspect of a given subject. Thematic segmen-
tation of texts can also be seen as a grouping process of basic
units (words, sentences, paragraphs...) in order to highlight
local semantical coherences [Kozima, 1993]. The granularity
level of the segmentation depends on the size of the units.

The increasing interest in text segmentation is mainly ex-
plained by the number of its applications such as text align-
ment, document summarization, information extraction, or
information retrieval [Baeza-Yates and Ribeiro-Neto, 1999].
Text segmentation can be indeed very useful for these tasks
by providing the structure of a document in terms of the dif-
ferent topics it covers [McDonald and Chen, 2002].

Many segmentation methods have been proposed and we
focus here on the most general and signiﬁcant methods that
rely on statistical approaches such as Text Tiling [Hearst,
1997], C99 [Choi, 2000], DotPlotting [Reynar, 2000] or Seg-
menter [Kan et al., 1998]. These methods perform an analysis
of the distribution of the words in the text, in order to deter-
mine the thematic changes by means of lexical inventory vari-

ations in ﬁxed size windows and thus create boundaries in the
text where the local cohesion is the lowest.

In this paper, we introduce SegGen a genetic algorithm to
achieve a statistical linear segmentation of texts. Section 2
presents the main motivations of our work. Our segmentation
algorithm is described in section 3. Then, section 4 describes
an experimental study of the algorithm in order to tune its pa-
rameters. Finally, section 5 evaluates SegGen by comparing
it with other segmentation systems.

2 Motivations and Preliminary Works

In most of existing segmentation approaches, the relation-
ships between sentences are usually very local. For exam-
ple, the methods addressing segmentation by means of lex-
ical chains1 mainly use the repetitions of the terms in order
to deﬁne thematic boundaries. More precisely, these meth-
ods cut the texts where the number of lexical chains is mini-
mal. Nevertheless, the context of these multiple occurrences
is not addressed neither the signiﬁcance of simultaneous lex-
ical chains at a given position in the text.

According to the preceding remark, we attempted to pro-
pose an alternative approach to the segmentation of texts by
taking into account a more complete view of the texts.
In
[Bellot, 2000], P. Bellot has shown that there exists a strong
relationship between clustering and text segmentation. His
hypothesis states that it is possible to set a boundary between
two adjacent sentences belonging to two different semantic
classes. This assumption seems too strong since text segmen-
tation not only depends on the similarity of the sentences, but
must also consider their layout in the text. Indeed, discourse
structures of documents may be very diverse and a part of a
text related to a particular topic may contain sentences de-
viating somewhat from it. These sentences are likely to be
classiﬁed differently from their neighbors. However, this cer-
tainly does not imply that a boundary has necessarily to be
set there. Moreover, this assumption is too dependent on the
chosen clustering mechanism, no existing clustering method
being fully reliable.

1Lexical chains are formed between two occurrences of a same
term [Galley et al., 2003] [Utiyama and Isahara, 2001] and occa-
sionally between synonyms and terms having statistical associations
such as generalization/specialization or part-whole/whole-part rela-
tionships [Morris and Hirst, 1991]

IJCAI-07

1647

However, we were convinced that a preliminary cluster-
ing could provide a more complete view of the sentences
relations. Therefore, we have deﬁned a ﬁrst segmentation
method, called ClassStruggle, that uses an initial clustering
of the sentences of the text based on their similarity, in or-
der to have a global view on their semantic relations. In this
approach, the resulting clusters evolve by taking into account
their proximity in the text. Considering the clusters as topics
of the text, ClassStruggle performs a linear traversal of the
document in order to determine the most appropriate class
assignment for each sentence, depending on their context of
occurrence. This process goes on as long as modiﬁcations
occur in the clusters. Finally, boundaries are created between
sentences belonging to different classes. Due to a lack of
space, ClassStruggle cannot be fully described but is never-
theless used for the evaluation of SegGen (section 5).

According to the deﬁnition of text segmentation by Salton
[Salton et al., 1996], the internal cohesion of the segments
and their semantical proximity with their neighborhood con-
stitute important factors of an efﬁcient segmentation method.
But, in the case of a sequential creation of boundaries, these
characteristics cannot be computed since, when a boundary is
created, the concerned segment is not yet entirely delimited.
Therefore, sequential methods need to introduce a concept of
window in which cohesion measures can be achieved. Nev-
ertheless, it is difﬁcult to determine the size of the window:
a too short window may lead the algorithm not to consider
some cohesion clues and a too long one may take into ac-
count some repetitions of terms that should indeed not be as-
sociated, since used in different contexts.

From this second remark, we decided to consider an ap-
proach where the boundaries are considered globally. Based
on an evaluation of the possible segmentation schemes, such a
method would have a complete view of the text and would be
able to compute coherence without using windows. Consid-
ering the segmentation problem as a combinatorial problem,
SegGen is an evolutionary algorithm that evaluates the seg-
mentations of the whole text rather than setting boundaries
incrementally. The lack of knowledge on the structure of the
text or on the number of segments to create, induces a huge
search space and leads us to consider a genetic algorithm to
cope with complexity.

3 SegGen: a Genetic Algorithm for Text

Segmentation

Inspired by the mechanisms of natural selection and evo-
lution, genetic algorithms [Holland, 1975; Goldberg, 1989]
have been successfully applied to the resolution of numerous
combinatorial optimization problems. The general principle
of genetic algorithms consists in managing a population of
individuals, which represents potential conﬁgurations of the
problem to be solved. Genetic algorithms have been used
for several text mining purposes (such as document cluster-
ing [Raghavan and Agarwal, 1987]).

As mentioned above, text segmentation can be viewed as
ﬁnding parts of text with strong intrinsic relationships, dis-
connected from other adjacent parts. Therefore, we consider
text segmentation as a multi-objective problem with two cri-

teria to optimize: the internal cohesion of the segments and
the dissimilarity between adjacent ones. As usual for multi-
objective problems, these two criteria are negatively corre-
lated.
Indeed, the internal cohesion tends to increase with
the number of boundaries whereas the dissimilarity between
adjacent segments tends to decrease with it. Following the
multi-objective optimization principles, we consider the ob-
jective functions separately in order to allow our algorithm to
preserve enough diversity in the search process and to extract
good segmentation proposals.

Our SegGen algorithm is a variant of the ”Strength Pareto
Evolutionary Algorithm” (SPEA) [Zitzler, 1999], an elitist al-
gorithm for multi-objective problems. The method uses an
¯P to memorize the non-dominated2 individ-
external archive
uals w.r.t. both criteria and a current population Pt.
Indi-
viduals are selected from these two populations in order to
generate new individuals thanks to genetic operators. These
new individuals constitute the next Pt and are used to update
¯P .
¯P constitutes, at the end of the process, a set of potential
segmentations of the text. The algorithm has then to extract
an unique solution from this set (see section 3.5). All docu-
ments do not need the same number of generations to reach a
satisfactory segmentation. Therefore, our stop criterion cor-
responds to a stagnation of the population evaluation.

Mutation

Crossing Over

Current Population Pt

Archive updating

Computation

of the fitness score

Selection

Extraction of a Solution

Temporary Population

External Archive P

Figure 1: General functioning of SegGen

Algorithm 1 Pseudo-code of the Seggen algorithm

Similarities computation and populations initialization;
while Stop Criterion not yet encountered do

- Fitness evaluation of each individual of Pt ∪ ¯P ;
- Selection, Crossover, Mutation;
- Replacement of Pt by the set of new individuals;
¯P w.r.t. the non-dominated individuals;
- Update of

end while
Selection of the best individual in

¯P .

3.1 Problem Representation
Given a text of ns sentences, the individuals are binary vec-
tors (cid:2)x of (ns − 1) elements xi; xi = 1 indicates that there
is a boundary between sentence i and i + 1. For an indi-
vidual (cid:2)x, we consider two evaluation criteria: C((cid:2)x) ∈ [0, 1],
which corresponds to the internal cohesion of its segments,

2An individual is dominated if there exists an other individual in
the populations having a better score on one criterion and at least the
same score on the others.

IJCAI-07

1648

(1)

i=1

nbseg(cid:6)
nbseg(cid:6)

i=1

C((cid:2)x) =

SumSim(segi)

(4)

N bCouples(segi)
(cid:4)
(cid:3)

(cid:4)

(cid:3)

(cid:4)

(cid:3)

(cid:3)

(cid:4)

n+1

In presence of long segments, the number of couples is
greater than if all the segments have the same size (for exam-
). Therefore, individuals hav-
ple,
ing a lower variance in their segment size will be preferred.
However, this bias is limited by the second objective function.

n−1

+

>

+

n
2

n
2

2

2

Dissimilarity Between Adjacent Segments
The similarity of a segment w.r.t. its successor is computed
as follows:

(cid:6)

(cid:6)

Sim(sj, sk)

SimSeg(seg1, seg2) =

sj ∈S(seg1)

sk∈S(seg2)

|S(seg1)| × |S(seg2)|

(5)
with segi being a segment, sj a sentence, S(segi) the set
of the sentences of the segment segi and |S(segi)| the cardi-
nality of this set. This formula allows us to deﬁne the compu-
tation of the general dissimilarity between adjacent segments
in an individual:

(cid:9) nbseg−1(cid:6)

i=1

SimSeg(segi, segi+1)

nbseg − 1

(cid:10)

(6)

D((cid:2)x) = 1 −

with nbseg being the number of segments of (cid:2)x and segi a

(2)

segment of this individual.

and D((cid:2)x) ∈ [0, 1], which evaluates the dissimilarity between
its adjacent segments. Our optimization problem consists in
approximating the set of optimizers O:

O =

(cid:2)
(cid:3)
(cid:4)
(cid:4)(cid:5)
(cid:3)
(cid:2)x ∈ {0, 1}ns−1 | (cid:4) ∃(cid:2)x(cid:2) ∈ {0, 1}ns−1,
D((cid:2)x) < D((cid:2)x(cid:2))
C((cid:2)x) < C((cid:2)x(cid:2))

∧

According to this deﬁnition, the optimum is not a unique

solution but a set of compromises, called the Pareto Front.

3.2

Initial Population Generation

Initial solutions are randomly and incrementally created but,
in order to start with an heterogeneous population, individuals
are created w.r.t.
the boundaries of others. The size of the
population is determined empirically (see section 4).

Genetic algorithms may converge easier if the individuals
of the initial population are closer to the optimum [Goldberg,
1989]. Therefore, we tried to insert in the population an indi-
vidual created by an external segmentation method. Experi-
ments w.r.t. this strategy will be described in section 5.

3.3 Fitness Function

Two criteria are used to evaluate the individuals: internal co-
hesion and external dissimilarity. Both use similarities be-
tween sentences computed beforehand. According to the vec-
torial representation of the sentences, inspired by the vectorial
model [Baeza-Yates and Ribeiro-Neto, 1999] (i.e., a vector
of weights w.r.t.
the set of meaningful terms), the similar-
ity sim(s1, s2) between two sentences s1 and s2 is computed
with a cosine measure:

Sim(s1, s2) =

(cid:6)
(cid:7)(cid:6)

t
i=1 wi,s1
×

t

i=1 w2

i,s1

(cid:6)
× wi,s2
i=1 w2

t

i,s2

where t is the number of meaningful terms, wi,sj

the

weight of the term i in the sentence sj.

Internal Cohesion
The internal cohesion of segments can be computed in two
different ways by using the sentence similarities. First, it can
be seen as the average internal cohesion of segments:

nbseg(cid:8)

i=1

C((cid:2)x) =

1

nbseg

×

SumSim(segi)
N bCouples(segi)

(3)

with nbseg being the number of segments of the individual,
segi the segment i of the individual, SumSim(segi) the sum
of the sentence similarities of segi and N bCouples(segi) the
number of possible couples of sentences in segi.

Since the distribution of similarities in the text is not likely
to be homogeneous, one may probably ﬁnd very cohesive
small areas. In this case, individuals representing small seg-
ments would have a really greater chance to obtain a good
score of cohesion. A normalization of the score could be per-
formed according to the size of the segments but may induce
some bias. Therefore, we propose to realize the sum of each
cohesion divided by the number of couples:

Fitness of the Individuals
Following [Zitzler, 1999], the computation of the ﬁtness value
of each individual is realized in two steps. A hardness value
¯P . This
H is given to each individual of the external archive
value is computed according to the number of individuals of
Pt (the current population) that an element (cid:2)x ∈ ¯P slightly
dominates on both objective functions:

H((cid:2)x) =

|{(cid:2)y/(cid:2)y ∈ Pt ∧ C((cid:2)x) ≥ C((cid:2)y) ∧ D((cid:2)x) ≥ D((cid:2)y)}|

|Pt| + 1

(7)
The ﬁtness value of an individual (cid:2)x ∈ ¯P is equal to the
inverse of its hardness value: F ((cid:2)x) = 1
H((cid:2)x) . The ﬁtness value
of an individual (cid:2)y ∈ Pt is computed by summing the hardness
scores of individuals (cid:2)x of

¯P dominating (cid:2)y:

F ((cid:2)y) =

1 +

(cid:6)

1

(cid:2)x∈ ¯

P ∧C((cid:2)x)≥C((cid:2)y)∧D((cid:2)x)≥D((cid:2)y)

H((cid:2)x)

(8)

This ﬁtness function, allowing us to select individuals for
¯P

mutation and crossover, aims to diversify the search (from
point of view).

IJCAI-07

1649

3.4 Exploration Operators

Three operators are used in the evolution process: selection,
crossover and mutation (see ﬁg.1). At each generation, the al-
gorithm select N bIndiv individuals and produces N bIndiv
new individuals to maintain a ﬁxed size of population.
¯P and Pt according to
their ﬁtness value by ”roulette wheel” [Holland, 1975]. This
method ﬁrst sums all the ﬁtness scores of the individuals.
Each individual gets a probability of selection equal to their
percentage of contribution in this sum.

The individuals are selected from

Among the selected individuals, while the number of
N bIndiv new individuals has not been reached yet, two par-
ents are randomly chosen in order to produce two new indi-
viduals (the offsprings) by crossover. Here, we use a classical
single point crossover: a position is randomly chosen in the
parents and the two corresponding parts of the parents are ex-
changed to form two children.

Two different mutation operators are applied to these chil-
dren: a mutation that replaces a parent by a randomly pro-
duced individual with a probability P ms and a mutation used
with a probability P mc that shifts a boundary of the individ-
ual to the next or the previous sentence. Both mutation oper-
ators appear complementary since the ﬁrst enables to explore
new areas while the second reﬁnes the existing solutions.

3.5 Extraction of a Solution

¯P
When the algorithm meets the stop criterion, the archive
contains a set of potential segmentations. We have then to
extract the best individual from this set. This choice can be
guided by an aggregation of both objective functions:

Agg((cid:2)x) = C((cid:2)x) + α × D((cid:2)x)

(9)

The extracted solution is the one having the best score w.r.t.
this aggregation function. The coefﬁcient α weights the sec-
ond objective compared to the ﬁrst. It acts upon the features
of the ﬁnal segmentation of the text and is thus tuned experi-
mentally in section 4.

4 Experiments

4.1 Experimental Process

The experiments were carried out on articles from the cor-
pus of texts of the international conference TREC-1 [Harman,
1993], which includes full articles from the Associated Press
published between 1988 and 1989. Those articles have been
gathered to form different sets of 350 documents. Separations
between articles constitute the segmentation of reference.
These tests based on concatenated articles may be less con-
clusive than tests performed on more homogeneous texts but
this evaluation seems to be the most commonly used in the lit-
erature (see for example [Choi, 2000]) and this kind of bound-
aries appears to be difﬁcult enough to recognize. According
to this principle, four corpuses have thus been created in order
to test the behavior of the algorithm with regards to different
kinds of text. We note T (ns, nb) a corpus composed by texts
of ns sentences and an average of nb boundaries.We use the
corpuses T (50, 2), T (50, 4), T (100, 4), T (100, 8). The seg-
mentation algorithms are evaluated w.r.t.
three criteria: the

precision (number of exact boundaries found / number of
boundaries found ), the recall (number of exact boundaries
found / number of boundaries of the reference) and a crite-
rion, called WindowDiff [Pevzner and Hearst, 2002], which
takes into account the number of boundaries between two
sentences separated from a distance k.

W inDif f (hyp, ref ) = 1

(cid:6)
N −k
i=0 (|b(refi, refi+k)
−b(hypi, hypi+k)|)
(10)
where b (xi, xj) is the number of boundaries between i and
j in a segmented text x containing N sentences, ref cor-
responds to the segmentation of reference and hyp the one
found with the method to evaluate. The size k of the window
has to be set to half of the average true segment size. This
criterion enables to consider the near-misses.

N −k

4.2 Parameters Tuning
The quality of the population is evaluated with a function
Eval( ¯P ) that returns the score of the best individual in
¯P
w.r.t the aggregation function (section 3.5):

P Agg((cid:2)x)

Eval( ¯P ) = M ax(cid:2)x∈ ¯

A study of different

(11)
¯P , issued from executions of SegGen
on multiples texts after various numbers of generations, has
shown that a weight α = 6 enables the selection of a good
individual in the majority of cases. The following tests will
use this value.

In order to adjust the probabilities of mutation P ms and
P mc, SegGen has been ran for each couple of (P ms, P mc)
between (0, 0) and (1, 1), using a step of 0.2 for each. These
experiments have been carried out on the set of 350 docu-
ments of the corpus T (50, 2), 10 times each in order to limit
random variations. The number of individuals in the popula-
tion is set to 10, value which appears to provide good results.
Results of ﬁgure 2 are averages of the population evaluation
function Eval( ¯P ) on these multiple executions.

l

a
v
E

 6.042

 6.04

 6.038

 6.036

 6.034

 6.032

 6.03

40,80
20,80
40,60
60,80
40,100
40,0
0,80

 0  50  100 150 200 250 300

Number of Generations

Figure 2: Evolution of Eval w.r.t. P ms, P mc

The ﬁgure 2 shows the evolution of Eval( ¯P ) according to
some different combinations of mutation probabilities3. We
may ﬁrst notice the inﬂuence of both mutation operators since
the curves that corresponds to a single mutation operator are

3The legends correspond to the values P ms, P mc (times 100)
and are given in the same order than the obtained score at the end of
the process (the greatest is the best).

IJCAI-07

1650

 3

 4

 5

 6

 7

 8

 3

 4

 5

 6

 7

 8

Alpha

Alpha

5.2 Results

f
f
i

D
w
o
d
n
W

i

n
o
s

i

i

c
e
r
P

 0.24

 0.235

 0.23

 0.225

 0.22

 0.215

 0.21

 0.205

 0.2

 0.64

 0.63

 0.62

 0.61

 0.6

 0.59

 0.58

 0.7

 0.68

 0.66

 0.64

 0.62

 0.6

 0.58

 0.56

l
l

a
c
e
R

s
e
i
r
a
d
n
u
o
B

 
f
o
 
r
e
b
m
u
N

 5.5

 5

 4.5

 4

 3.5

• TT: TextTiling4 [Hearst, 1997] is based on the distribu-
tion of the terms in the text, according to several criteria
(number of common words, new words, active chains).
• C99: C995 [Choi, 2000], which seems to be the most
efﬁcient existing approach, uses a ”local ranking” by de-
termining, for each pair of textual units, the number of
surrounding pairs having a lower similarity.

• CS: ClassStruggle, mentioned in section 2, is based on a

clustering process to segment the texts.

As mentioned in section 3.2, we tried to improve the ini-
tial population by inserting a good solution. Therefore, two
versions of the algorithm, S5 and SC5, have been evaluated.
In SC5, a segmentation scheme, computed by C99, has been
inserted in the initial population. Both use a coefﬁcient α = 5
(see section 4.2).

Since SegGen uses stochastic parameters in mutation and
crossover, the range of error has been evaluated by computing
the standard deviations, on ten executions, of the average of
scores obtained on each corpus. Standard deviation of Win-
dowDiff is located around 0.015, of recall around 0.01 and of
precision around 0.012. These values are really low w.r.t. the
ranges of scores given in table 1.

According to results given in table 1, ClassStruggle, having a
more complete view of the relations of the sentences thanks
to the use of a clustering process, is globally more efﬁcient
than C99 and TextTiling. However, its incremental process
reduces its ability to take into account the segment cohesion
and the similarity between adjacent segments. SegGen over-
rides this problem and obtains really better results for all ac-
curacy criteria, especially on corpuses that do not have a lot
of boundaries to retrieve (i.e. T (50, 2) and T (100, 4)). Text-
Tiling and C99 seem to have difﬁculties to adapt themselves
w.r.t the number of boundaries to retrieve, the length of the
text has a great impact on their number of detected bound-
aries. SegGen seems to better follow these variations.

However, on corpora that have a lot of boundaries to re-
trieve, i.e. T (50, 4) and T (100, 8), SegGen seems to have dif-
ﬁculties to product individuals sufﬁciently segmented. This is
due to the fact that good individuals with a lot of boundaries
are more difﬁcult to ﬁnd than others. The insertion of a good
individual in the initial population (SC5), obtained by C99,
seems to solve this problem. C99, providing segmentations
with a lot of boundaries, helps SegGen to ﬁnd solutions when
the search space is huge.

Additionally, a Student t-test6 has shown that these differ-
ences are really signiﬁcant with a 99% conﬁdence rate, all of
the values being higher than the p-value 2.57.

4Available on: www.sims.berkeley.edu/˜hearst/.
5Available on: www.freddychoi.me.uk.
6The 99% Student t-test is based on the average, the standard
deviation and the cardinality of a set of runs. It uses a p-value equal
to 2.57 to insure with a rate of conﬁdence of 99% that the difference
in means of two sets is signiﬁcant.

the lowest. The couple P ms = 0.4 and P mc = 0.8 seems to
be the best compromise.

A study on the number of individuals N bIndiv has shown
that values greater than 10 allow the algorithm to converge
more quickly in term of generations but induce a greater com-
putation cost in term of evaluation. For instance, on the
corpus T (50, 2), with N bIndiv = 20, the algorithm needs
only 150 generations to reach the score of evaluation of 6.04
against 250 with 10 individuals. However, the number of gen-
erated individuals is greater (250 × 10 < 150 × 20). On the
other hand, a lower number of individuals leads to the oppo-
site problem, the number of generations is too high, which
implies a loss of time.

Concerning the stop criterion, long texts may need more
time to be segmented but the quality of segmentation can be
the same than for shorter ones. A value of 100 iterations with-
out any improvement seems to be sufﬁcient enough to reach
a good solution.

 3

 4

 5

 6

 7

 8

 3

 4

 5

 6

 7

 8

Alpha

Alpha

Figure 3: Results of experiments on α

Concerning the value of the α parameter used in the aggre-
¯P ,
gation function in order to extract a good individual from
we have to remark that the ﬁrst objective function C favorizes
individuals that generate a lot of boundaries, small segments
having more chances to be cohesive. At the contrary, the
second objective function D is in favor of individuals hav-
ing few boundaries, since long segments have more chances
to own sentences faraway the following. The parameter α
has thus an inﬂuence on the number of detected boundaries
(ﬁgure 3). The best results according to WindowDiff are ob-
tained around α = 5. Nevertheless, an higher value enables
to obtain a better precision and a lower a better recall. This
value may be set between 4 and 6 w.r.t. the frequency of the
required segmentation.

5 Evaluation
5.1 Experimental Process
In order to evaluate the methods, we use benchmarks created
similarly to those described in section 4. Of course, in order
not to skew the results, the selected articles are different from
those used for tuning purposes. SegGen has been compared
to the following methods to evaluate its efﬁciency:

IJCAI-07

1651

WinDiff

T(50,2)
σ

μ

T(50,4)
σ

μ

T(100,4)
μ
σ

T(100,8)
μ
σ

TT

C99

CS

S5

SC5

1.02

0.89

0.36

0.22

0.22

0.71

0.75

0.42

0.36

0.37

0.40

0.38

0.27

0.19

0.18

0.26

0.31

0.20

0.17

0.18

0.97

0.54

0.33

0.20

0.20

0.66

0.45

0.29

0.22

0.23

0.37

0.33

0.28

0.22

0.18

0.16

0.16

0.13

0.12

0.11

Recall

T(50,2)
σ

μ

T(50,4)
σ

μ

T(100,4)
μ
σ

T(100,8)
μ
σ

TT

C99

CS

S5

SC5

Prec.

TT

C99

CS

S5

SC5

Nb.

TT

C99

CS

S5

SC5

0.54

0.63

0.62

0.69

0.69

0.38

0.38

0.38

0.37

0.37

0.49

0.59

0.58

0.60

0.64

0.26

0.27

0.30

0.27

0.27

0.59

0.64

0.60

0.61

0.64

0.25

0.26

0.27

0.27

0.27

0.56

0.59

0.56

0.56

0.65

0.18

0.20

0.22

0.20

0.21

T(50,2)
σ

μ

T(50,4)
σ

μ

T(100,4)
σ
μ

T(100,8)
σ
μ

0.26

0.32

0.54

0.61

0.61

0.22

0.22

0.36

0.39

0.39

0.49

0.48

0.60

0.68

0.68

0.26

0.23

0.29

0.28

0.28

0.27

0.40

0.52

0.62

0.63

0.14

0.19

0.26

0.29

0.29

0.47

0.53

0.57

0.67

0.69

0.17

0.17

0.21

0.21

0.19

T(50,2)
σ

μ

T(50,4)
σ

μ

T(100,4)
μ
σ

T(100,8)
μ
σ

4.21

4.22

2.62

2.44

2.47

1.39

1.42

1.40

1.32

1.36

4.31

4.94

3.91

3.49

3.74

1.40

1.31

1.54

1.40

1.51

9.52

6.81

5.23

4.19

4.48

2.33

2.09

2.29

1.87

2.10

9.82

9.15

7.86

6.50

7.75

2.03

2.23

2.45

1.80

2.11

Table 1: Averages (μ) and Standard deviations between docu-
ments (σ) of WindowDiff (WinDiff), recall, precision (Prec.)
and number of detected boundaries (Nb.)
for each tested
method on each corpus.

6 Conclusion

The experiments have shown that our approach appears to
realize a really more accurate segmentation of the texts than
existing incremental methods. Moreover, SegGen seems to
adapt itself much better than other methods according to the
number of boundaries to retrieve.

Nevertheless, the ﬁtness function can be improved, in par-
ticular by a more sophisticated calculus of similarity, over-
passing the sole repetitions of words. Finally, SegGen will be
at term integrated as a preliminary step in an ongoing project
of information retrieval that aims to furnish composite docu-
ments as responses to user’s queries.

Acknowledgments

This work was supported by ”Angers Loire Metropole”.

References
[Baeza-Yates and Ribeiro-Neto, 1999] R. Baeza-Yates and
B. Ribeiro-Neto. Modern Information Retrieval. ACM
Press/Addison-Wesley, 1999.

[Bellot, 2000] P. Bellot. M´ethodes de classiﬁcation et de seg-
mentation locales non supervis´ees pour la recherche doc-
umentaire. PhD thesis, Universit´e d’Avignon, 2000.

[Choi, 2000] F.Y.Y. Choi. Advances in domain independent
linear text segmentation. In Proc. of the ACL, pages 26–33,
2000. Morgan Kaufmann Publishers Inc.

[Galley et al., 2003] M. Galley, K. McKeown, E. Fosler-
Lussier, and H. Jing. Discourse segmentation of multi-
party conversation. In ACL ’03:, pages 562–569, 2003.

[Goldberg, 1989] D.E. Goldberg. Genetic Algorithms in
Search Optimization and Machine Learning. Addison-
Wesley, Reading, MA, USA, 1989.

[Harman, 1993] D. Harman. Overview of the ﬁrst trec con-

ference. In SIGIR ’93, pages 36–47, 1993. ACM Press.

[Hearst, 1997] M.A. Hearst. Texttiling: segmenting text into
multi-paragraph subtopic passages. Comput. Linguist.,
23(1):33–64, 1997.

[Holland, 1975] J.H. Holland. Adaptation in Natural and Ar-
tiﬁcial Systems. University of Michigan Press, Ann Arbor,
MI, USA, 1975.

[Kan et al., 1998] M. Kan, J. Klavans, and K. McKeown.
Linear segmentation and segment signiﬁcance.
In 6th
Workshop on Very Large Corpora (WVLC-98), pages 197–
205. ACL SIGDAT, 1998.

[Kozima, 1993] H. Kozima. Text segmentation based on
similarity between words. In ACL, pages 286–288, 1993.
[McDonald and Chen, 2002] D. McDonald and H. Chen.
Using sentence-selection heuristics to rank text segments
in txtractor. In JCDL ’02, pages 28–35, 2002. ACM Press.

[Morris and Hirst, 1991] J. Morris and G. Hirst. Lexical co-
hesion computed by thesaural relations as an indicator of
the structure of text. Comp. Ling., 17(1):21–48, 1991.

[Pevzner and Hearst, 2002] L. Pevzner and M.A. Hearst. A
critique and improvement of an evaluation metric for text
segmentation. Comp. Ling., 28(1):19–36, 2002.

[Raghavan and Agarwal, 1987] V. Raghavan and B. Agar-
wal. Optimal determination of user-oriented clusters: an
application for the reproductive plan. In Proc. of GA, pages
241–246, New York, NY, USA, 1987. ACM Press.

[Reynar, 2000] J.C. Reynar. Topic Segmentation: Algorithms
and applications. PhD thesis, University of Pennsylvania,
Seattle, WA, 2000.

[Salton et al., 1996] G. Salton, A. Singhal, C. Buckley, and
M. Mitra. Automatic text decomposition using text seg-
ments and text themes.
In Hypertext ’96, pages 53–65.
ACM, 1996.

[Utiyama and Isahara, 2001] M. Utiyama and H. Isahara. A
statistical model for domain-independent text segmenta-
tion. In ACL, pages 491–498, 2001.

[Zitzler, 1999] E. Zitzler. Evolutionary Algorithms for Mul-
tiobjective Optimization: Methods and Applications. Phd
thesis, Swiss Federal Institute of Technology (ETH)
Zurich, December 1999.

IJCAI-07

1652


Heuristic Selection of Actions in Multiagent Reinforcement Learning

∗

Reinaldo A. C. Bianchi

Carlos H. C. Ribeiro

FEI University Center

Technological Institute of Aeronautics

Electrical Engineering Department
S˜ao Bernardo do Campo, Brazil.

Computer Science Division
S˜ao Jos´e dos Campos, Brazil.

rbianchi@fei.edu.br

carlos@comp.ita.br

Anna H. R. Costa
Escola Polit´ecnica

University of S˜ao Paulo

S˜ao Paulo, Brazil.

anna.reali@poli.usp.br

Abstract

This work presents a new algorithm, called Heuris-
tically Accelerated Minimax-Q (HAMMQ), that al-
lows the use of heuristics to speed up the well-
known Multiagent Reinforcement Learning algo-
rithm Minimax-Q. A heuristic function H that in-
ﬂuences the choice of the actions characterises the
HAMMQ algorithm. This function is associated
with a preference policy that indicates that a cer-
tain action must be taken instead of another. A
set of empirical evaluations were conducted for the
proposed algorithm in a simpliﬁed simulator for
the robot soccer domain, and experimental results
show that even very simple heuristics enhances sig-
niﬁcantly the performance of the multiagent rein-
forcement learning algorithm.

1 Introduction

Reinforcement Learning (RL) techniques are very attractive
in the context of multiagent systems. The reasons frequently
cited for such attractiveness are: many RL algorithms guar-
antee convergence to equilibrium in the limit [Szepesvari and
Littman, 1996], they are based on sound theoretical founda-
tions, and they provide model-free learning of adequate sub-
optimal control strategies. Besides that, they are also easy to
use and have been applied to solve a wide variety of control
and planning problems when neither an analytical model nor
a sampling model is available a priori.

In RL, learning is carried out on line, through trial-and-
error interactions of the agent with its environment, receiv-
ing a reward (or penalty) at each interaction.
In a multia-
gent system, the reward received by one agent depends on the
behaviour of other agents, and a Multiagent Reinforcement
Learning (MRL) algorithm needs to address non-stationary
scenarios in which both the environment and other agents
must be represented. Unfortunately, convergence of any RL
algorithm may only be achieved after extensive exploration
of the state-action space, which can be very time consuming.
The existence of multiple agents increases by itself the size

of the state-action space, worsening the RL algorithms per-
formance when applied to multiagent problems. Despite that,
several MRL algorithms have been proposed and success-
fully applied to some simple problems, such as the Minimax-
Q [Littman, 1994], the Friend-or-Foe Q-Learning [Littman,
2001] and the Nash Q-Learning [Hu and Wellman, 2003].

An alternative way of increasing the convergence rate of
an RL algorithm is to use heuristic functions for selecting
actions in order to guide the exploration of the state-action
space in an useful way. The use of heuristics to speed up re-
inforcement learning algorithms has been proposed recently
by Bianchi et al. [2004], and it has been successfully used in
policy learning for simulated robotic scenarios.

This paper investigates the use of heuristic selection of ac-
tions in concurrent and on-policy learning in multiagent do-
mains, and proposes a new algorithm that incorporates heuris-
tics into the Minimax-Q algorithm, the Heuristically Acceler-
ated Minimax-Q (HAMMQ). A set of empirical evaluation
of HAMMQ were carried out in a simpliﬁed simulator for the
robot soccer domain [Littman, 1994]. Based on the results of
these simulations, it is possible to show that using even very
simple heuristic functions the performance of the learning al-
gorithm can be improved.

The paper is organised as follows: section 2 brieﬂy reviews
the MRL approach and describes the Minimax-Q algorithm,
while section 3 presents some approaches to speed up MRL.
Section 4 shows how the learning rate can be improved by
using heuristics to select actions to be performed during the
learning process, in a modiﬁed formulation of the Minimax-
Q algorithm. Section 5 describes the robotic soccer domain
used in the experiments, presents the experiments performed,
and shows the results obtained. Finally, Section 6 provides
our conclusions.

2 Multiagent Reinforcement Learning

Markov Games (MGs) – also known as Stochastic Games
(SGs) – are an extension of Markov Decision Processes
(MDPs) that uses elements from Game Theory and allows the
modelling of systems where multiple agents compete among
themselves to accomplish their tasks.

∗The support of FAPESP (Proc. 2006/05667-0), CNPq (Proc.
305558/2004-8) and CAPES/GRICES (Proc. 099/03) is acknowl-
edged.

Formally, an MG is deﬁned by [Littman, 1994]:
• S: a ﬁnite set of environment states.

IJCAI-07

690

• A1 . . . Ak: a collection of sets Ai with the possible ac-

tions of each agent i.

• T : S × A1 × . . . × Ak → Π(S): a state transition func-
tion that depends on the current state and on the actions
of each agent.

• Ri : S × A1 × . . . × Ak → (cid:4): a set of reward functions

specifying the reward that each agent i receives.

Solving an MG consists in computing the policy π : S ×
A1 ×. . .×Ak that maximizes the reward received by an agent
along time.

This paper considers a well-studied specialization of MGs
in which there are only two players, called agent and oppo-
nent, having opposite goals. Such specialization, called zero-
sum Markov Game (ZSMG), allows the deﬁnition of only
one reward function that the learning agent tries to maximize
while the opponent tries to minimize.

A two player ZSMG [Littman, 1994] is deﬁned by the

quintuple (cid:5)S, A, O, T , R(cid:6), where:

• S: a ﬁnite set of environment states.
• A: a ﬁnite set of actions that the agent can perform.
• O: a ﬁnite set of actions that the opponent can perform.
• T : S × A × O → Π(S): the state transition function,
where Π(S) is a probability distribution over the set of
states S. T (s, a, o, s(cid:2)) deﬁnes a probability of transition
from state s to state s(cid:2) (at a time t + 1) when the learn-
ing agent executes action a and the opponent performs
action o.

• R : S × A × O → (cid:4): the reward function that speciﬁes
the reward received by the agent when it executes action
a and its opponent performs action o, in state s.

The choice of an optimal policy for a ZSMG is not trivial
because the performance of the agent depends crucially on
the choice of the opponent. A solution for this problem can
be obtained with the Minimax algorithm [Russell and Norvig,
2002]. It evaluates the policy of an agent regarding all possi-
ble actions of the opponent, and then choosing the policy that
maximizes its own payoff.

To solve a ZSMG, Littman [1994] proposed the use of a
similar strategy to Minimax for choosing an action in the Q-
Learning algorithm, the Minimax-Q algorithm (see Table 1).
Minimax-Q works essentially in the same way as Q-Learning
does. The action-value function of an action a in a state s
when the opponent takes an action o is given by:

(cid:2)

Q(s, a, o) = r(s, a, o) + γ

s(cid:2)∈S

T (s, a, o, s(cid:2))V (s(cid:2)),

(1)

and the value of a state can be computed using linear pro-
gramming [Strang, 1988] via the equation:

(cid:2)

V (s) = max
π∈Π(A)

min
o∈O

a∈A

Q(s, a, o)πa,

where the agent’s policy is a probability distribution over ac-
tions, π ∈ Π(A), and πa is the probability of taking the action
a against the opponent’s action o.

An MG where two players take their actions in consecutive
turns is called an Alternating Markov Game (AMG). In this

IJCAI-07

691

Initialise ˆQt(s, a, o).
Repeat:

Visit state s.
Select an action a using the  − Greedy rule (eq. 4).
Execute a, observe the opponent’s action o.
Receive the reinforcement r(s, a, o)
Observe the next state s(cid:2).
Update the values of ˆQ(s, a, o) according to:

ˆQt+1(s, a, o) ← ˆQt(s, a, o)+

s ← s(cid:2).

α[r(s, a, o) + γVt(s(cid:2)) − ˆQt(s, a, o)].

Until some stop criterion is reached.

Table 1: The Minimax-Q algorithm.

case, as the agent knows in advance the action taken by the
opponent, the policy becomes deterministic, π : S × A × O
and equation 2 can be simpliﬁed:

V (s) = max

a∈A

min
o∈O

Q(s, a, o).

(3)

In

this

case,

the

arg maxa mino Q∗(s, a, o).
rule to be used is the standard  − Greedy:

optimal

policy

≡
A possible action choice

is π∗

(cid:3)

π(s) =

arg max
arandom

a

min

o

ˆQ(s, a, o)

if q ≤ p,
otherwise,

(4)

where q is a random value with uniform probability in [0,1]
and p (0 ≤ p ≤ 1) is a parameter that deﬁnes the explo-
ration/exploitation trade-off: the greater the value of p, the
smaller is the probability of a random choice, and arandom is
a random action selected among the possible actions in state
s. For non-deterministic action policies, a general formu-
lation of Minimax-Q has been deﬁned elsewhere [Littman,
1994; Banerjee et al., 2001].

Finally, the Minimax-Q algorithm has been extended to
cover several domains where MGs are applied, such as
Robotic Soccer [Littman, 1994; Bowling and Veloso, 2001]
and Economy [Tesauro, 2001].

One of the problems with the Minimax-Q algorithm is that,
as the agent iteratively estimates Q, learning at early stages is
basically random exploration. Also, update of the Q value is
made by one state-action pair at a time, for each interaction
with the environment. The larger the environment, the longer
trial-and-error exploration takes to approximate the function
Q. To alleviate these problems, several techniques, described
in the next section, were proposed.

3 Approaches to speed up Multiagent

(2)

Reinforcement Learning

The Minimax-SARSA Algorithm [Banerjee et al., 2001] is a
modiﬁcation of Minimax-Q that admits the next action to be
chosen randomly according to a predeﬁned probability, sep-
arating the choice of the actions to be taken from the update
of the Q values. If a(cid:2) is chosen according to a greedy policy,
Minimax-SARSA becomes equivalent to Minimax-Q. But if

a(cid:2) is selected randomly according to a predeﬁned probabil-
ity distribution, the Minimax-SARSA algorithm can achieve
better performance than Mimimax-Q [Banerjee et al., 2001].
The same work proposed the Minimax-Q(λ) algorithm,
by combining eligibility traces with Minimax-Q. Eligibility
traces, proposed initially in the T D(λ) algorithm [Sutton,
1988], are used to speed up the learning process by track-
ing visited states and adding a portion of the reward received
to each state that has been visited in an episode. Instead of
updating a state-action pair at each iteration, all pairs with
eligibilities different from zero are updated, allowing the re-
wards to be carried over several state-action pairs. Banerjee
et al. [2001] also proposed Minimax-SARSA(λ), a combi-
nation of the the two algorithms that would be more efﬁcient
than both of them, because it combines their strengths.

A different approach to speed up the learning process is to
use each experience in a more effective way, through tempo-
ral, spatial or action generalization. The Minimax-QS algo-
rithm, proposed by Ribeiro et al. [2002], accomplishes spa-
tial generalization by combining the Minimax-Q algorithm
with spatial spreading in the action-value function. Hence, at
receiving a reinforcement, other action-value pairs that were
not involved in the experience are also updated. This is done
by coding the knowledge of domain similarities in a spread-
ing function, which allows a single experience (i.e., a single
loop of the algorithm) to update more than a single cost value.
The consequence of taking action at at state st is spread to
other pairs (s, a) as if the real experience at time t actually
was (cid:5)s, a, st+1, rt(cid:6).

4 Combining Heuristics and Multiagent

Reinforcement Learning: the HAMMQ
Algorithm

The Heuristically Accelerated Minimax Q (HAMMQ) algo-
rithm can be deﬁned as a way of solving a ZSMG by making
explicit use of a heuristic function H : S × A × O → (cid:4) to
inﬂuence the choice of actions during the learning process.
H(s, a, o) deﬁnes a heuristic that indicates the desirability of
performing action a when the agent is in state s and the op-
ponent executes action o.

The heuristic function is associated with a preference pol-
icy that indicates that a certain action must be taken instead of
another. Therefore, it can be said that the heuristic function
deﬁnes a “Heuristic Policy”, that is, a tentative policy used
to accelerate the learning process. The heuristic function can
be derived directly from prior knowledge of the domain or
from clues suggested by the learning process itself and is used
only during the selection of the action to be performed by the
agent, in the action choice rule that deﬁnes which action a
should be executed when the agent is in state s. The action
choice rule used in HAMMQ is a modiﬁcation of the standard
 − Greedy rule that includes the heuristic function:

(cid:5)

(cid:3)

(cid:4)

π(s) =

min

arg max
arandom otherwise,

a

o

ˆQ(s, a, o) + ξHt(s, a, o)

if q ≤ p,

(5)
where H : S ×A×O → (cid:4) is the heuristic function. The sub-
script t indicates that it can be non-stationary and ξ is a real

variable used to weight the inﬂuence of the heuristic (usually
1).

As a general rule, the value of Ht(s, a, o) used in HAMMQ
should be higher than the variation among the ˆQ(s, a, o) val-
ues for the same s ∈ S, o ∈ O, in such a way that it can
inﬂuence the choice of actions, and it should be as low as
possible in order to minimize the error. It can be deﬁned as:

(cid:3)

H(s, a, o) =

ˆQ(s, i, o) − ˆQ(s, a, o) + η if a = πH (s),

i

max
0 otherwise.

(6)
where η is a small real value (usually 1) and πH (s) is the
action suggested by the heuristic policy.

As the heuristic function is used only in the choice of the
action to be taken, the proposed algorithm is different from
the original Minimax-Q in the way exploration is carried out.
Since the RL algorithm operation is not modiﬁed (i.e., up-
dates of the function Q are the same as in Minimax-Q), our
proposal allows that many of the theoretical conclusions ob-
tained for Minimax-Q remain valid for HAMMQ.

Theorem 1 Consider a HAMMQ agent learning in a deter-
ministic ZSMG, with ﬁnite sets of states and actions, bounded
rewards (∃c ∈ (cid:4); (∀s, a, o), |R(s, a, o)| < c), discount factor
γ such that 0 ≤ γ < 1 and with values used in the heuristic
function bounded by (∀s, a, o) hmin ≤ H(s, a, o) ≤ hmax.
For this agent, ˆQ values will converge to Q∗, with probability
one uniformly over all states s ∈ S, provided that each state-
action pair is visited inﬁnitely often (obeys the Minimax-Q
inﬁnite visitation condition).

Proof: In HAMMQ, the update of the value function approxi-
mation does not depend explicitly on the value of the heuristic
function. Littman and Szepesvary [1996] presented a list of
conditions for the convergence of Minimax-Q, and the only
condition that HAMMQ could put in jeopardy is the one that
depends on the action choice: the necessity of visiting each
pair state-action inﬁnitely often. As equation 5 considers an
exploration strategy – greedy regardless of the fact that the
value function is inﬂuenced by the heuristic function, this vis-
itation condition is guaranteed and the algorithm converges.
q.e.d.

The condition that each state-action pair must be visited an
inﬁnite number of times can be considered valid in practice
– in the same way that it is for Minimax-Q – also by using
other strategies:

• Using a Boltzmann exploration strategy [Kaelbling et

al., 1996].

• Intercalating steps where the algorithm makes alternate

use of the heuristic function and exploration steps.

• Using the heuristic function during a period of time

shorter than the total learning time.

The use of a heuristic function made by HAMMQ explores
an important characteristic of some RL algorithms: the free
choice of training actions. The consequence of this is that
a suitable heuristic function speeds up the learning process,
otherwise the result is a delay in the learning convergence
that does not prevent it from converging to an optimal value.

IJCAI-07

692

Initialise ˆQt(s, a, o) and Ht(s, a, o).
Repeat:

Visit state s.
Select an action a using the modiﬁed −Greedy rule
(Equation 5).

Execute a, observe the opponent’s action o.
Receive the reinforcement r(s, a, o)
Observe the next state s(cid:2).
Update the values of Ht(s, a, o).
Update the values of ˆQ(s, a, o) according to:

ˆQt+1(s, a, o) ← ˆQt(s, a, o)+

s ← s(cid:2).

α[r(s, a, o) + γVt(s(cid:2)) − ˆQt(s, a, o)].

Until some stop criterion is reached.

Table 2: The HAMMQ algorithm.

The complete HAMMQ algorithm is presented on table
It is worth noticing that the fundamental difference be-
2.
tween HAMMQ and the Minimax-Q algorithm is the action
choice rule and the existence of a step for updating the func-
tion Ht(s, a, o).

5 Robotic Soccer using HAMMQ

Playing a robotic soccer game is a task for a team of multi-
ple fast-moving robots in a dynamic environment. This do-
main has become of great relevance in Artiﬁcial Intelligence
since it possesses several characteristics found in other com-
plex real problems; examples of such problems are: robotic
automation systems, that can be seen as a group of robots in
an assembly task, and space missions with multiple robots
[Tambe, 1998], to mention but a few.

In this paper, experiments were carried out using a sim-
ple robotic soccer domain introduced by Littman [1994] and
modelled as a ZSMG between two agents. In this domain,
two players, A and B, compete in a 4 x 5 grid presented in
ﬁgure 1. Each cell can be occupied by one of the players,
which can take an action at a turn. The action that are allowed
indicate the direction of the agent’s move – north, south, east
and west – or keep the agent still.

The ball is always with one of the players (it is represented
by a circle around the agent in ﬁgure 1). When a player ex-
ecutes an action that would ﬁnish in a cell occupied by the
opponent, it looses the ball and stays in the same cell. If an
action taken by the agent leads it out the board, the agent
stands still.

When a player with the ball gets into the opponent’s goal,
the move ends and its team scores one point. At the beginning
of each game, the agents are positioned in the initial position,
depicted in ﬁgure 1, and the possession of the ball is randomly
determined, with the player that holds the ball making the
ﬁrst move (in this implementation, the moves are alternated
between the two agents).

To solve this problem, two algorithms were used:
• Minimax-Q, described in section 2.
• HAMMQ, proposed in section 4.

B

A

Figure 1: The environment proposed by Littman [1994]. The
picture shows the initial position of the agents.

Figure 2: The heuristic policy used for the environment of
ﬁgure 1. Arrows indicate actions to be performed.

The heuristic policy used was deﬁned using a simple rule:
if holding the ball, go to the opponent’s goal (ﬁgure 2 shows
the heuristic policy for player A of ﬁgure 1). Note that the
heuristic policy does not take into account the opponents po-
sition, leaving the task of how to deviate from the opponent to
the learning process. The values associated with the heuristic
function are deﬁned based on the heuristic policy presented
in ﬁgure 2 and using equation 6.

The parameters used in the experiments were the same for
the two algorithms, Minimax-Q and HAMMQ. The learning
rate is initiated with α = 1.0 and has a decay of 0.9999954
by each executed action. The exploration/ exploitation rate
p = 0.2 and the discount factor γ = 0.9 (these parameters
are identical to those used by Littman [1994]). The value of
η was set to 1. The reinforcement used was 1000 for reach-
ing the goal and -1000 for having a goal scored by the op-
ponent. Values in the Q table were randomly initiated, with
0 ≤ Q(s, a, o) ≤ 1. The experiments were programmed in
C++ and executed in a AMD K6-II 500MHz, with 288MB of
RAM in a Linux platform.

Thirty training sessions were run for each algorithm, with
each session consisting of 500 matches of 10 games. A game
ﬁnishes whenever a goal is scored by any of the agents or
when 50 moves are completed.

Figure 3 shows the learning curves (average of 30 train-
ing sessions) for both algorithms when the agent learns how
to play against an opponent moving randomly, and presents
the average goal balance scored by the learning agent in each

IJCAI-07

693

l

s
a
o
G

10

8

6

4

2

0

Minimax−Q
HAMMQ

50

100

150

200

250

300

350

400

450

500

Matches

16

14

12

10

8

6

4

2

0

T module
5% Limit
0.01% Limit

50

100

150

200

250

300

350

400

450

500

Matches

Figure 3: Average goal balance for
the Minimax-Q
and HAMMQ algorithms against a random opponent for
Littman’s Robotic Soccer.

Figure 5: Results from Student’s t test between Minimax-Q
and HAMMQ algorithms, training against a random oppo-
nent.

l

s
a
o
G

7

6

5

4

3

2

1

0

−1

−2

Minimax−Q
HAMMQ

0

50

100

150

200

250

300

350

400

450

500

12

10

8

6

4

2

0

T Module
5% Limit
0.01% Limit

50

100

150

200

250

300

350

400

450

500

Matches

Matches

Figure 4: Average goal balance for the Minimax-Q and
HAMMQ algorithms against an agent using Minimax-Q for
Littman’s Robotic Soccer.

Figure 6: Results from Student’s t test between Minimax-
Q and HAMMQ algorithms, training against an agent using
Minimax-Q.

match. It is possible to verify that Minimax-Q has worse per-
formance than HAMMQ at the initial learning phase, and that
as the matches proceed, the performance of both algorithms
become similar, as expected.

Figure 4 presents the learning curves (average of 30 train-
ing sessions) for both algorithms when learning while playing
against a learning opponent using Minimax-Q. In this case, it
can be clearly seen that HAMMQ is better at the beginning of
the learning process and that after the 300th match the perfor-
mance of both agents becomes similar, since both converge to
equilibrium.

Student’s t–test [Spiegel, 1998] was used to verify the hy-
pothesis that the use of heuristics speeds up the learning pro-
cess. Figure 5 shows this test for the learning of the Minimax-
Q and the HAMMQ algorithms against a random opponent
(using data shown in ﬁgure 3). In this ﬁgure, it is possible to

see that HAMMQ is better than Minimax-Q until the 150th
match, after which the results are comparable, with a level
of conﬁdence greater than 5%. Figure 6 presents similar re-
sults for the learning agent playing against an opponent using
Minimax-Q.

Finally, table 3 shows the cumulative goal balance and ta-
ble 4 presents the number of games won at the end of 500
matches (average of 30 training sessions). The sum of the
matches won by the two agents is different from the total
number of matches because some of them ended in a draw.
What stands out in table 4 is that, due to a greater number
of goals scored by HAMMQ at the beginning of the learning
process, this algorithm wins more matches against both the
random opponent and the Minimax-Q learning opponent.

IJCAI-07

694

Training Section

Cumulative goal balance

Minimax-Q × Random
HAMMQ × Random

(3401 ± 647) × (529 ± 33)
(3721 ± 692) × (496 ± 32)
Minimax-Q × Minimax-Q (2127 ± 430) × (2273 ±209 )
HAMMQ × Minimax-Q
(2668 ± 506) × (2099 ± 117)

Table 3: Cumulative goal balance at the end of 500 matches
(average of 30 training sessions).

Training Section

Minimax-Q × Random
HAMMQ × Random

Matches won

(479 ± 6) × (11 ± 4)
(497 ± 2) × (1 ± 1)

Minimax-Q × Minimax-Q (203 ± 42) × (218 ± 42)
HAMMQ × Minimax-Q
(274 ± 27) × (144 ± 25)

Table 4: Number of matches won at the end of 500 matches
(average of 30 training sessions).

6 Conclusion

This work presented a new algorithm, called Heuristically
Accelerated Minimax-Q (HAMMQ), which allows the use of
heuristics to speed up the well-known Multiagent Reinforce-
ment Learning algorithm Minimax-Q.

The experimental results obtained in the domain of robotic
soccer games showed that HAMMQ learned faster than
Minimax-Q when both were trained against a random strat-
egy player. When HAMMQ and Minimax-Q were trained
against each other, HAMM-Q outperformed Minimax-Q.

Heuristic functions allow RL algorithms to solve problems
where the convergence time is critical, as in many real time
applications. This approach can also be incorporated into
other well known Multiagent RL algorithms, e.g. Minimax-
SARSA, Minimax-Q(λ), Minimax-QS and Nash-Q.

Future works include working on obtaining results in more
complex domains, such as RoboCup 2D and 3D Simulation
and Small Size League robots [Kitano et al., 1997]. Prelim-
inary results indicate that the use of heuristics makes these
problems easier to solve. Devising more convenient heuris-
tics for other domains and analyzing ways of obtaining them
automatically are also worthwhile research objectives.

References
[Banerjee et al., 2001] Bikramjit Banerjee and Sandip Sen
and Jing Peng. Fast Concurrent Reinforcement Learners.
In: Proceedings of the 17th International Joint Confer-
ence on Artiﬁcial Intelligence (IJCAI’01), pages 825–830,
2001.

[Bianchi et al., 2004] Reinaldo A. C. Bianchi, Carlos H. C.
Ribeiro and Anna H. R. Costa. Heuristically Accelerated
Q-Learning: a New Approach to Speed Up Reinforce-
ment Learning. Lecture Notes in Artiﬁcial Intelligence,
3171:245–254, 2004.

[Bowling and Veloso, 2001] Michael H. Bowling

and
Manuela M. Veloso. Rational and Convergent Learn-
ing in Stochastic Games.
In: Proceedings of the 17th
International Joint Conference on Artiﬁcial Intelligence
(IJCAI’01), pages 1021–1026, 2001.

[Hu and Wellman, 2003] Junling Hu and Michael P. Well-
man.
Nash Q-Learning for General-Sum Stochastic
Games. Journal of Machine Learning Research, 4:1039–
1069, 2003.

[Kaelbling et al., 1996] Leslie P. Kaelbling, Michael L.
Littman and Andrew W. Moore. Reinforcement Learn-
ing: A survey. Journal of Artiﬁcial Intelligence Research,
4:237–285, 1996.

[Kitano et al., 1997] Hiroaki Kitano, Minoru Asada, Yasuo
Kuniyoshi, Itsuki Noda and Eiichi Osawa. RoboCup: A
Challenge Problem for AI. AI Magazine, 18(1):73–85,
1997.

[Littman, 1994] Michael L. Littman Markov games as a
framework for multi-agent reinforcement learning.
In:
Proceedings of the 11th International Conference on Ma-
chine Learning (ICML’94), pages 157–163, 1994.

[Littman, 2001] Michael L. Littman

Friend-or-Foe Q-
learning in general-sum games.
In: Proceedings of
the 18th International Conference on Machine Learning
(ICML’01), pages 322–328, 2001.

[Littman and Szepesvari, 1996] Michael L. Littman and
Csaba Szepesv´ari. A Generalized Reinforcement Learn-
ing Model: Convergence and Applications. In: Procs. of
the 13th International Conference on Machine Learning
(ICML’96), pages 310–318, 1996.

[Ribeiro et al., 2002] Carlos H. C. Ribeiro, Renˆe Pegoraro
and Anna H. R. Costa. Experience Generalization for Con-
current Reinforcement Learners: the Minimax-QS Algo-
rithm. In: Proceedings of the 1st International Joint Con-
ference on Autonomous Agents and Multi-Agent Systems
(AAMAS’02), pages 1239–1245, 2002.

[Russell and Norvig, 2002] Stuart Russell and Peter Norvig.
Artiﬁcial Intelligence: A Modern Approach. Prentice Hall,
Upper Saddle River, 2nd edition, 2002.

[Spiegel, 1998] Murray R. Spiegel. Statistics. McGraw-Hill,

New York, 1998.

[Strang, 1988] Gilbert Strang. Linear algebra and its appli-
cations. Harcourt, Brace, Jovanovich, San Diego, 3rd edi-
tion, 1988.

[Sutton, 1988] Richard S. Sutton. Learning to Predict by
the Methods of Temporal Differences. Machine Learning,
3(1):9–44, 1988.

[Szepesvari and Littman, 1996] Csaba

and
Michael L. Littman.
Generalized Markov Decision
Processes: Dynamic-Programming and Reinforcement-
Learning Algorithms. Technical Report CS-96-11, Brown
University, 1996.

Szepesv´ari

[Tambe, 1998] Milind Tambe. Implementing Agent Teams
in Dynamic Multiagent Environments. Applied Artiﬁcial
Intelligence, 12(2-3):189–210, 1998.

[Tesauro, 2001] Gerald Tesauro.

Pricing in Agent
Economies Using Neural Networks and Multi-agent
Q-Learning.
Lecture Notes in Computer Science,
1828:288–307, 2001.

IJCAI-07

695

